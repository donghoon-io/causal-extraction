<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diagram-Driven Course Questions Generation</title>
				<funder>
					<orgName type="full">Shaanxi Provincial So-</orgName>
				</funder>
				<funder ref="#_r638cYe #_ss3Ze5G #_en9bEkt #_wM8B9D4 #_3DBDa9Q #_prTmFQj #_r8F2Eje #_bdZQeRY #_uQtGvt9 #_pdYknK7 #_tSpTgHt">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_puRJbKV">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-09-06">6 Sep 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
							<email>zhanglling@xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanrui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muye</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaowei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shaanxi Province Key Laboratory of Big Data Knowledge Engineering</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ministry of Education Key Laboratory of Intelligent Networks and Network Security</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">IHPC</orgName>
								<orgName type="department" key="dep2">Agency for Science, Technology and Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">College of Computing and Data Science</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shaanxi Province Key Laboratory of Big Data Knowledge Engineering</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diagram-Driven Course Questions Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-09-06">6 Sep 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2411.17771v5[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual Question Generation (VQG) research focuses predominantly on natural images while neglecting the diagram, which is a critical component in educational materials. To meet the needs of pedagogical assessment, we propose the Diagram-Driven Course Questions Generation (DDCQG) task and construct Dia-gramQG, a comprehensive dataset with 15,720 diagrams and 25,798 questions across 37 subjects and 371 courses. Our approach employs course and input text constraints to generate course-relevant questions about specific diagram elements. We reveal three challenges of DDCQG: domain-specific knowledge requirements across courses, long-tail distribution in course coverage, and high information density in diagrams. To address these, we propose the Hierarchical Knowledge Integration framework (HKI-DDCQG), which utilizes trainable CLIP for identifying relevant diagram patches, leverages frozen vision-language models for knowledge extraction, and generates questions with trainable T5. Experiments demonstrate that HKI-DDCQG outperforms existing models on DiagramQG while maintaining strong generalizability across natural image datasets, establishing a strong baseline for DDCQG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Generation (VQG) represents a pivotal and promising research domain with significant educational applications <ref type="bibr" target="#b53">(Xie et al., 2025;</ref><ref type="bibr" target="#b38">Luo et al., 2024)</ref>. While VQG focuses on automatically generating questions from visual inputs, current research predominantly addresses natural images while neglecting diagrams <ref type="bibr" target="#b51">(Wang et al., 2024;</ref><ref type="bibr" target="#b60">Zhang et al., 2025;</ref><ref type="bibr">Wang et al., 2025a)</ref>, a fundamental component of educational materials. This critical gap impedes the advancement of deep learning technologies in educational contexts.  The diagram plays a crucial role in pedagogical assessment by facilitating structured information presentation and evaluating students' comprehension of knowledge <ref type="bibr" target="#b19">(Cook, 2006)</ref>. Therefore, we propose the Diagram-Driven Course Questions Generation (DDCQG) task, which aims to encourage models to generate questions based on diagrams for specific courses. These questions are essential for evaluating students' abilities to explain, analyze, and apply course knowledge based on the diagrams <ref type="bibr">(Lambertus et al., 2008)</ref>. Furthermore, current VQG research extensively employs various text constraints-including answers <ref type="bibr" target="#b53">(Xie et al., 2025;</ref><ref type="bibr" target="#b40">Mi et al., 2024)</ref>, answer types <ref type="bibr" target="#b27">(Krishna et al., 2019)</ref>, image regions <ref type="bibr" target="#b47">(Uehara et al., 2018)</ref>, question types <ref type="bibr" target="#b22">(Fan et al., 2018)</ref>, and knowledge triples <ref type="bibr" target="#b46">(Uehara and Harada, 2023)</ref>. However, these constraints face significant limi-tations when applied to DDCQG task. Research demonstrates that existing approaches frequently produce context-independent questions <ref type="bibr">(Liu et al., 2024a)</ref>, fail to align with intended assessment objectives <ref type="bibr" target="#b46">(Uehara and Harada, 2023)</ref>, or merely generate superficial expansions lacking pedagogical depth <ref type="bibr" target="#b40">(Mi et al., 2024)</ref>, highlighting the need of specific text constraint for DDCQG task.</p><p>To address these issues, we present DiagramQG, a comprehensive dataset comprising 15,720 diagrams and 25,798 questions spanning 6 disciplines, 37 subjects, and 371 courses. We propose a novel text constraint with course and input, as shown in Figure <ref type="figure" target="#fig_1">1</ref>, where course constraint ensure question relevance to specific educational contexts, and input constraint guide question generation around targeted diagram elements. Through analysis of DiagramQG, we identify three fundamental challenges of DDCQG: domain-specific knowledge requirement, with models needing to possess specialized knowledge across various disciplines unlike existing VQG datasets that rely primarily on general common sense; long-tail distribution phenomenon, where course coverage ranges from abundant to limited, challenging models to generalize across both well-represented and underrepresented courses; and high object information density, as diagrams contain concentrated visual information that complicates content interpretation and risks overlooking critical details.</p><p>To address these challenges, we propose the Hierarchical Knowledge Integration framework for DDCQG task (HKI-DDCQG) as a strong baseline. This framework employs a trainable CLIP to identify relevant multi-scale diagram patches, utilizes vision-language models like BLIP <ref type="bibr" target="#b30">(Li et al., 2022)</ref> and Qwen2.5-VL <ref type="bibr">(Bai et al., 2025)</ref> for knowledge extraction, and implements T5 for filtering extracted knowledge to generate the question based on text constraints. The framework then integrates these filtered insights with text constraints and multi-scale diagram patches for question generation. Notably, this frame freezes the VLM's parameters and trains only the remaining parts, thus improving scalability and cost-efficiency.</p><p>We evaluate HKI-DDCQG against existing VQG and vision-language models on our DiagramQG dataset and validate its generalizability through experiments on four natural image VQG datasets. Our primary contributions include the following:  <ref type="bibr" target="#b16">(Bi et al., 2022)</ref>, constrained methods have shown success through various strategies, including answer guidance <ref type="bibr" target="#b56">(Xu et al., 2020;</ref><ref type="bibr">Liu et al., 2024c)</ref>, knowledge enhancement <ref type="bibr" target="#b54">(Xie et al., 2022;</ref><ref type="bibr" target="#b17">Chen et al., 2023)</ref>, crosstopic models <ref type="bibr">(Liu et al., 2024b)</ref>, and contrastive learning <ref type="bibr" target="#b40">(Mi et al., 2024)</ref>. There are also some studies <ref type="bibr" target="#b53">(Xie et al., 2025)</ref> on the generation of visual problems with controllable difficulty. However, challenges persist in balancing question diversity with effective visual information utilization, necessitating innovative constraint mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diagram Question Answering</head><p>Diagram Question Answering (DQA) often demands enhanced reasoning capabilities and domain knowledge. Prior research has focused on improving diagram comprehension through pre-training methods (Gómez-Pérez and Ortega, 2020; <ref type="bibr" target="#b39">Ma et al., 2022;</ref><ref type="bibr" target="#b55">Xu et al., 2023)</ref> and utilizing Large Language Models via advanced prompting techniques <ref type="bibr" target="#b36">(Lu et al., 2022;</ref><ref type="bibr">Zhang et al., 2024b;</ref><ref type="bibr">Yao et al., 2024a;</ref><ref type="bibr" target="#b51">Wang et al., 2024;</ref><ref type="bibr">Huang et al., 2025b,a)</ref>. The effective integration of visual diagrammatic information with background knowledge remains absolutely crucial for improving DQA performance.</p><p>3 DiagramQG Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>The data collection process consisted of three distinct phases. First, we gathered diagrams and related questions from existing diagram-related  datasets <ref type="bibr" target="#b26">(Kembhavi et al., 2017;</ref><ref type="bibr" target="#b49">Wang et al., 2022;</ref><ref type="bibr">Zhang et al., 2024a;</ref><ref type="bibr" target="#b36">Lu et al., 2022;</ref><ref type="bibr" target="#b59">Yue et al., 2024;</ref><ref type="bibr" target="#b18">Chen et al., 2024)</ref>, supplemented by diagrams available for academic use from platforms such as Hugging Face and Roboflow. This initial phase yielded a substantial raw dataset containing over 25,000 diagrams and 60,000 questions. Subsequently, we organized the collected diagrams and questions into six primary disciplines and further categorized them into 37 specific subjects. This structuring process involved mapping questions to their corresponding courses, resulting in the identification of 371 distinct courses.</p><p>Finally, highly experienced subject-specific annotators with relevant knowledge backgrounds annotated input text constraints for each diagramquestion pair within their specialized domains. A separate group of subject annotators evaluated all samples based on their educational utility using a 100-point scale. Samples scoring below the thresh-  As illustrated in Figure <ref type="figure">3</ref>, the DiagramQG dataset encompasses 6 disciplines, 37 subjects, and 371 courses across multiple academic domains, with an average of 17.45 words per question. The dataset follows a hierarchical structure, with samples first classified by discipline, then divided into specific subjects (e.g., Biology), and ultimately categorized into courses (e.g., Ecological interactions). For further research, we split DiagramQG into train, val, and test sets with a ratio of 70:5:25. Considering that some courses contain fewer than 5 samples, we prioritized allocating these courses to the test set to ensure a comprehensive evaluation.</p><p>As illustrated in Figure <ref type="figure" target="#fig_6">4</ref>, the ratio of questions and diagrams for each course exhibits a pronounced long-tail distribution. This asymmetric distribution indicates that the DiagramQG dataset covers a wide range of courses, some courses have notably limited samples. However, this is a common characteristic in educational scenarios that accurately reflects typical resource allocation patterns in real-world educational environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Comparisons to Other Datasets</head><p>Table <ref type="table" target="#tab_1">1</ref> presents a comprehensive comparison between DiagramQG and existing VQG-related and diagram-related datasets, highlighting Dia-gramQG's distinctive characteristics. DiagramQG encompasses 25,798 questions and 15,720 unique diagrams, substantially surpassing the scale of existing multidisciplinary datasets (such as M3CoT), including those that incorporate both natural images and diagrams. Unlike conventional VQG datasets that primarily focus on image caption and common-sense reasoning, DiagramQG is specifically engineered for educational applications.</p><p>DiagramQG demonstrates three significant advantages. First, it exhibits unprecedented educational breadth, spanning 37 distinct academic courses, which exceeds the domain coverage of existing datasets. Second, the dataset introduces a novel constraint that integrates both input phrases and course-specific contextual information, transcending traditional constraint formats. Third, each diagram contains an average of 10.5 objects, representing significantly higher complexity compared to existing datasets in the field. This unique combination of comprehensive course coverage, sophisticated constraint mechanisms, and enhanced Chemistry ( <ref type="formula">1421</ref>) Geography ( <ref type="formula">1396</ref>) Physics ( <ref type="formula">4000</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Challenges in DiagramQG Dataset</head><p>Our comparative analysis reveals three distinctive challenges in DiagramQG that set it apart from existing visual question generation datasets:</p><p>• Domain-specific knowledge requirement: Unlike other existing VQG datasets that focus on general common sense, DiagramQG dataset consistently requires models to possess and apply different courses across various subjects to generate meaningful and practical questions. • Long-tail distribution phenomenon: The inherent complex long-tail distribution in DiagramQG dataset, where course samples range from abundant to limited, challenges the generalization and performance of models across both sample-rich and sample-limited courses. • High object information density: The significantly high density of object information in the diagrams complicates content interpretation and risks overlooking critical details, demanding models to possess capabilities in capturing and processing complex visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Definition</head><p>The Diagram-Driven Course Questions Generation (DDCQG) task aims to generate a pedagogically appropriate question q given a diagram d, an input text t, and a course text c specifically. The generated questions are used to effectively assess students' understanding of the specified course c. This task can be formulated as a conditional generation problem, represented as p(q|d, t, c), where a multimodal model coherently maps visual and textual information into a joint embedding space before decoding questions that satisfy both the text constraint and the course assessment requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture</head><p>We propose the Hierarchical Knowledge Integration (HKI-DDCQG) framework as a baseline for the DDCQG task. This framework is designed to be compatible with any vision-language foundation model and implements a three-stage pipeline for question generation: HierKnowExtract, KnowSelect, and KnowFusionQG, as shown in Figure <ref type="figure" target="#fig_7">5</ref>. The HierKnowExtract stage extracts knowledge K s from multi-scale diagram patches P s using vision-language models with frozen parameters.</p><p>The KnowSelect stage selects the m most relevant knowledge sentences K (t,c) based on text t and course c. The KnowFusionQG stage integrates t, c, P s , and K (t,c) to generate the final question q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">HierKnowExtract</head><p>The HierKnowExtract stage obtains diagram patches P s of different scales related to the input &amp; course and uses vision-language models to extract the knowledge K s contained in all patches. This process begins with a hierarchical decomposition of diagram d into ordered patches P d across an n-layered pyramid structure, followed by visual encoding using the CLIP Image Encoder <ref type="bibr">(Radford et al., 2021)</ref> to get F l , as formulated in Equation <ref type="formula">1</ref>.</p><formula xml:id="formula_0">               F d = {F l } n l=1 P d = {p l i,j | l ∈ [1, n], i, j ∈ [1, l]} F l = {f l i,j = CLIP(p l i,j ) | p l i,j ∈ P d } p l i,j = d i -1 l H : i l H, j -1 l W : j l W (1)</formula><p>where H and W denote the height and width of the input diagram, respectively. For textual components, the T5 Encoder (Raffel et al., 2020) is employed to process both the input text t and course text c independently. We introduce a learnable linear transformation W h to ensure seamless compatibility between the CLIP Image Encoder <ref type="bibr">(Radford et al., 2021)</ref> and T5 Encoder <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref> feature spaces. This effectively facilitates unified similarity computation and subsequent operations in the KnowFusionQG phase. The process ultimately culminates in the selection of the most semantically relevant patch from each hierarchical layer to form the patch set P s , as shown in Equation <ref type="formula">2</ref>.</p><formula xml:id="formula_1">         e t = T5 enc (t), e c = T5 enc (c) s l i,j = sim(W h f l i,j , e t ) + sim(W h f l i,j , e c ) P s = {p l i * ,j * | (i * , j * ) = argmax i,j s l i,j , l ∈ [1, n]}</formula><p>(2) The selected patches P s , input text t, and course text c are carefully fed into a large-scale visionlanguage model (VLM) like Qwen2.5-VL <ref type="bibr">(Bai et al., 2025)</ref> or BLIP <ref type="bibr" target="#b30">(Li et al., 2022)</ref> to obtain diverse knowledge which is related with patch, input and course. The resulting knowledge set K s is systematically generated according to Equation <ref type="formula">3</ref>.</p><formula xml:id="formula_2">K s = {k l | l ∈ [1, n]} k l = VLM(p l i * ,j * , t, c), ∀p l i * ,j * ∈ P s (3)</formula><p>where k l is the text paragraph retrieved by the VLM, containing several knowledge sentences. To optimize the learning process, the CLIP Image Encoder <ref type="bibr">(Radford et al., 2021)</ref>, T5 Encoder <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref>, and the linear transformation W h employed in this phase share parameters with their counterparts in the third phase, where gradient updates are effectively propagated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">KnowSelect</head><p>The KnowSelect stage selects the m most relevant knowledge sentences K (t,c) from the extensive knowledge set K s based on input text t and course text c. The process begins with encoding the knowledge set K s and the t, c text constraints using the T5 Encoder <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref>, as formulated in the following Equation <ref type="formula" target="#formula_3">4</ref>.</p><formula xml:id="formula_3">H K = T5 enc (K s ) H t,c = T5 enc (Prompt(t, c))<label>(4)</label></formula><p>where Prompt(t, c) combines t and c into a prompt following the template: Given the input text t, identify key knowledge related to the course c.</p><p>For the semantic relevance between the knowledge set (K (t,c) ) and text constraints (t and c), we employ a scaled dot-product attention mechanism. This computes attention scores between the knowledge tokens and the text constraint, followed by knowledge selection, as formulated in Equation <ref type="formula" target="#formula_4">5</ref>:</p><formula xml:id="formula_4">     A = softmax H K H T t,c √ d k K (t,c) = top-m(AH K )<label>(5)</label></formula><p>where d k denotes the dimensionality of the hidden representations. The top-m() operator selects the m most semantically relevant knowledge sentences based on attention scores, creating a knowledge set K (t,c) that best matches the text constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">KnowFusionQG</head><p>The KnowFusionQG stage integrates the selected diagram patches P s , input text t, course text c, and knowledge set K (t,c) to generate the diagramdriven course question through a multimodal fusion mechanism. This process begins with encoding the visual and textual inputs into language and vision representations, as shown in Equation <ref type="formula" target="#formula_5">6</ref>.</p><formula xml:id="formula_5">H t = T5 enc (Prompt[t; c; K (t,c) ]) H v = W h • CLIP(P s )<label>(6)</label></formula><p>where W h is also used in HierKnowExtract phase. This Prompt[t; c; K (t,c) ] synthesizes t, c and K (t,c) into a coherent prompt following the template: Generate the question including input t to assess course c with the knowledge K (t,c) . To capture the intricate relationships between textual and visual representations, a cross-modal attention mechanism followed by a gated fusion network is employed, as formulated in Equation <ref type="formula">7</ref>.</p><formula xml:id="formula_6">         H attn v = Softmax( H t H T v √ d k )H v λ = W t H t + W v H attn v H f use = H t + tanh(λ) • H attn v (7)</formula><p>Finally, the fused output H f use is fed into the T5 decoder <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref> to predict the input question q. This integration of visual and text information through cross-modal attention and gated fusion enables the model to generate questions that are contextually relevant and conceptually focused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>The experimental framework uses T5-Base and T5-Large architectures along with the CLIP (ViT-B/32) model. Optimization is performed with the AdamW optimizer, applying a learning rate of 1e-5 for CLIP and 5e-5 for others, running through 10 epochs of fine-tuning. Key parameters include a maximum input sequence length of 256, an output sequence length of 64, and a batch size of 32, with experiments conducted on one NVIDIA A800 80G GPU. The DiagramQG is split into training, validation, and testing sets with a ratio of 70:5:25, ensuring no overlap of diagrams and questions across these sets. Additionally, each course is represented in both validation and testing sets, despite some courses having relatively few samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>VQG approaches can be classified into two main categories: Fine-tuned and In-Context Learning methods. Fine-tuned methods use models like BERT <ref type="bibr" target="#b21">(Devlin, 2018)</ref>, T5 <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref>, or GPT-2 <ref type="bibr" target="#b44">(Radford et al., 2019)</ref> for question generation, with examples including IM-VQG <ref type="bibr" target="#b27">(Krishna et al., 2019)</ref>, K-VQG (Uehara and Harada, 2023), Patch-TRM <ref type="bibr" target="#b37">(Lu et al., 2021)</ref>, ConVQG <ref type="bibr" target="#b40">(Mi et al., 2024)</ref>, LV2-Net <ref type="bibr">(Liu et al., 2024c)</ref>, and KC-VQG <ref type="bibr">(Liu et al., 2024a)</ref>. In-Context Learning methods utilize large vision-language models to extract relevant information from diagrams and generate questions using three reference questions. This includes open-source models like Qwen2.5-VL <ref type="bibr">(Bai et al., 2025)</ref>, MiniCPM-V <ref type="bibr">(Yao et al., 2024b)</ref>, DeepSeek-VL <ref type="bibr" target="#b2">(Lu et al., 2024)</ref>, and InternVL2.5 <ref type="bibr">(Wang et al., 2025b)</ref>, as well as closed-source models such as GLM4-V pro, Claude-3.5 Sonnect, and GPT-4o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>We evaluate our model using established language generation metrics, including BLEU <ref type="bibr" target="#b42">(Papineni et al., 2002)</ref>, Bert Score, METEOR (Denkowski and Lavie, 2014), CIDEr <ref type="bibr" target="#b48">(Vedantam et al., 2015)</ref>, and ROUGE <ref type="bibr" target="#b31">(Lin, 2004)</ref>. We evaluate model performance using the pycocoevalcap package to measure alignment between generated and ground truth questions. At the same time, in order to evaluate the relevance of generated questions to diagrams, we introduce the FLEUR evaluation index <ref type="bibr" target="#b29">(Lee et al., 2024)</ref> to supplement the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We comprehensively evaluate various models on our DiagramQG dataset, as shown in Table <ref type="table">2</ref>. Among the in-context learning methods that utilize Vision-Language Models (VLMs), models with larger parameter counts generally perform significantly better, indicating their abundant knowledge across different subjects. Interestingly, Qwen2.5-VL 3B achieves impressive and competitive results despite having fewer parameters, making it a suitable base model for further comprehensive experiments. Tests conducted with GPT-4o and Claude-3.5-Sonnet using only text inputs result in reduced generation performance, highlighting the critical importance of incorporating diagrams in the question generation process. Among the fine-tuned methods, KVQG and Patch-TRM do not leverage VLMs for additional knowledge, while ConVQG and KC-VQG do. The latter models demonstrate superior performance, with KC-VQG showing im-  proved results as VLM parameters scale up. By observing the Fleur metrics, we find that T5-Base and T5-Large do not show significant improvement in image information understanding, indicating that VLM's contribution to the correlation between the image and the question is more obvious than T5. HKI-DDCQG framework achieves the best performance across all quantitative evaluation metrics, regardless of whether it is based on T5-Base or T5-Large, particularly when using the larger-scale Qwen2.5-VL <ref type="bibr">(Bai et al., 2025)</ref> model. These impressive results convincingly validate HKI-DDCQG as an effective baseline for future research by demonstrating the value of incorporating subject knowledge in the DDCQG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on Other Dataset</head><p>We evaluate HKI-DDCQG's generalization capabilities on four natural image VQG datasets: VQG-COCO, OK-VQA, A-OKVQA, and K-VQG (Tables 3 and 4). Our model consistently outperforms existing approaches, particularly in CIDEr and ME-TEOR metrics, indicating superior semantic comprehension and contextual coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation experiment</head><p>Since KnowSelect inherently builds upon Hier-KnowExtract, we cannot evaluate it in isolation. Ablation results (Table <ref type="table" target="#tab_4">5</ref>) demonstrate that knowledge incorporation systematically enhances ques-  tion generation quality. KnowSelect shows greater impact than KnowFusionQG, indicating its effectiveness in filtering and retaining relevant information from HierKnowExtract's output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Parameter Sensitive Study</head><p>We examine two parameters: the number of pyramid layers (n) for diagram division and the number of knowledge sentences (m) selected for question generation. Results in Table <ref type="table" target="#tab_5">6</ref> show optimal performance at n = 3, with finer divisions adding noise in knowledge extraction. However, performance continues to improve after m = 3, although the marginal gains come at the cost of increased input length and computational overhead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present DiagramQG, the first diagram-specific question generation dataset, and HKI-DDCQG, a framework leveraging hierarchical knowledge integration for diagram-driven course question gen-eration. Experimental results demonstrate HKI-DDCQG's superior performance existing VQG and vision-language models, advancing intelligent education <ref type="bibr" target="#b41">(Misra et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitation</head><p>Our DiagramQG dataset and HKI-DDCQG framework, while advancing educational question generation, face two main limitations. First, our approach generates questions without multiple-choice options. Adding the capability to generate both questions and plausible distractors would enhance the system's educational value, as multiple-choice questions effectively assess student understanding across knowledge levels. However, DiagramQG also establishes the groundwork for future research in the simultaneous generation of diagram-based questions and distractor options. Second, the longtail distribution across 371 courses poses challenges for uniform performance. Models may show lower effectiveness for underrepresented courses with fewer training examples. However, this distribution reflects real-world educational patterns, and our framework maintains strong generalization across diverse domains. Despite these constraints, DiagramQG significantly advances educational AI research, providing a robust foundation for diagram-driven course question generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Ethical Statement</head><p>In developing DiagramQG, we prioritized ethical considerations throughout the entire process. Our data collection strictly adhered to copyright guidelines, utilizing only publicly available educational resources with appropriate permissions or fair use provisions. All data comply with CC BY-SA 4.0, CC BY-NC-SA, and MIT licenses and have been reviewed to ensure educational value and accuracy. We will publicly release complete datasets under appropriate licenses, both reducing unnecessary carbon footprint and optimizing processing pipelines to lower computational overhead. We fully recognize the broad impacts of automated question-generation systems. Our work aims to assist educators, with these systems designed to complement. To address representation bias, we constructed a diverse dataset spanning 6 disciplines,</p><p>A comprehensive benchmark towards physics-based reasoning. In Proceedings of the 63rd Annual Meetthe Association for Computational Linguistics (Volume 1: Long Papers), pages 16593-16615, Vienna, Austria. Association for Computational <ref type="bibr">Lin</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Appendix</head><p>In this supplementary material, we provide more details on our implementation and experiments as follows:</p><p>• Section A: More details on implementation;</p><p>• Section B: More details on baselines;</p><p>• Section C: More details on DiagramQG;</p><p>• Section D: More case studies on Diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More details on implementation</head><p>In our experimental setup, we developed a framework based on the T5 architecture, utilizing both its Base and Large variants respectively, alongside CLIP (ViT-B/32) for visual understanding capabilities. The framework's trainable parameters, excluding pre-trained components, were initialized following a normal distribution (µ = 0, σ = 0.02).</p><p>For optimization, we implemented the AdamW optimizer with a dual learning rate strategy: a conservative 1e -5 for CLIP components to maintain visual feature integrity, and a higher 5e-5 for remaining components. The training process spanned 20 epochs, incorporating linear learning rate warmup during the initial 2 epochs followed by cosine decay. Our implementation featured specific configurations including 256 tokens for maximum input sequence length, 32 tokens for output sequence length, a batch size of 32, 4 gradient accumulation steps, 0.01 weight decay, and 0.1 dropout rate, with FP16 mixed precision training enabled. All experimental procedures were executed on a hardware setup consisting of two NVIDIA A100 80G GPUs, utilizing CUDA 11.8 and PyTorch 1.13.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More details on baselines</head><p>Visual Question Generation (VQG) approaches can be classified into two main categories: fine-tuningbased methods and large vision-language models. For Fine-tuning-based methods, using the same data split approach as our method, the dataset is divided into training, validation, and test sets in a ratio of 70:5:25. Models are trained on the training set, validated on the validation set, and final results are reported on the test set.</p><p>K-VQG <ref type="bibr" target="#b46">(Uehara and Harada, 2023)</ref> integrates UNITER, a multi-modal transformer, to encode both visual features (extracted via Faster R-CNN) and masked knowledge triplets. The model processes features with positional information and combines them with tokenized knowledge triplets through a BART-based decoder, generating questions Patch-TRM <ref type="bibr" target="#b37">(Lu et al., 2021)</ref> employs a hierarchical patch-based transformer that processes diagrams by decomposing them into meaningful patches through a pyramid layout. The model combines ResNet and vision Transformer for visual processing, using attention mechanisms to fuse visual and textual features, enabling effective capture of both local details and global relationships for diagram-focused question generation.</p><p>ConVQG <ref type="bibr" target="#b40">(Mi et al., 2024</ref>) introduces a contrastive learning framework with dual modalityspecific objectives for visual question generation. By contrasting image-specific and text-guided features, the model generates diverse questions that are both image-specific and controllable through flexible textual constraints such as answers, captions, or knowledge triplets, enabling precise control over question content while maintaining visual grounding.</p><p>KC-VQG <ref type="bibr">(Liu et al., 2024a)</ref> presents a knowledge-guided framework that combines topicaware visual attention and Large Language Model (LLM) generated knowledge for question generation. The model integrates three components: a topic-aware visual feature extractor, a knowledge extractor with discriminative filtering, and a GPT-2 based decoder, enabling it to generate questions that incorporate both explicit visual information and implicit commonsense knowledge about specified topics.</p><p>LV2-Net <ref type="bibr">(Liu et al., 2024c)</ref> integrates logical verification into both knowledge acquisition and question generation processes, hence its designation as LV^2-Net. By performing a dual logical structure check-examining the relationships between visual content (V), attributes (A), knowledge (K), ground-truth answers, and the generated questions (Q) at two distinct stages within the knowledge-based visual question generation (KB-VQG) pipeline-LV2-Net is capable of producing a diverse range of insightful knowledge-driven visual questions.</p><p>For large vision-language models (VLMs), whether open-source or closed-source, we selected three reference questions for each course. These reference questions, along with target and course textual constraints, are incorporated into the prompt during the VLM's question generation process to obtain final outputs.</p><p>Open-Source Large vision-language models: Qwen2.5VL <ref type="bibr">(Bai et al., 2025</ref>) is a multimodal model with key enhancements in document parsing, object grounding, and video understanding. It boasts powerful omnidocument parsing capabilities, excelling in diverse document types and languages, including complex elements like tables, charts, and formulas. The model offers precise object grounding with support for various coordinate formats, enabling advanced spatial reasoning. Its ultra-long video understanding is enhanced by dynamic resolution in the temporal dimension, allowing for comprehension of hours-long videos and fine-grained event localization. Furthermore, Qwen2.5VL features improved agent functionality for computer and mobile devices through enhanced grounding, reasoning, and decision-making. Architecturally, it incorporates dynamic FPS sampling and temporal mRoPE for video understanding, along with a streamlined vision encoder utilizing window attention, SwiGLU, and RMSNorm. Available in 3B, 7B, 32B, and 72B parameter versions, its streaming architecture ensures efficient processing of various inputs and strong performance on diagram-related tasks.</p><p>MiniCPM-V <ref type="bibr">(Yao et al., 2024b)</ref> presents a compact yet effective vision-language model that combines SigLip-400M visual encoder with Qwen2-7B language model, totaling 8B parameters. The model offers efficient multi-image and video understanding capabilities while maintaining competitive performance on vision-language tasks through its streamlined architecture and parameter-efficient design.</p><p>DeepSeek-VL <ref type="bibr" target="#b2">(Lu et al., 2024)</ref> introduces a balanced approach to vision-language modeling that builds upon the DeepSeek language model series. The model emphasizes maintaining strong language capabilities while developing visual understanding, featuring high-resolution processing capabilities and a carefully curated training strategy. Through its systematic scaling methodology from 1B to 7B parameters, it achieves competitive performance in practical applications while maintaining efficiency in multi-modal processing.</p><p>InternVL 2.5 <ref type="bibr">(Wang et al., 2025b)</ref>     <ref type="figure">7</ref>), manifests when the generated questions fail to maintain fidelity to the target text, resulting in contextually inconsistent question generation. Our analysis suggests that this phenomenon stems from limitations in the model's instruction processing capabilities. Specifically, the neural architecture appears to inadequately preserve and integrate the target text constraints during the generation pipeline, leading to divergent outputs that, while potentially coherent, deviate from the intended textual context.</p><p>The second challenge, Diagram Interpretation Bias (Figure <ref type="figure">8</ref>), reveals a notable gap between the model-generated questions and ground truth questions in terms of course application depth. While the generated questions demonstrate basic assessment of students' course comprehension, they often exhibit simplified or superficial understanding of course concepts. To investigate this phenomenon, we conducted a visualization analysis of the knowledge retrieval process during question generation, as illustrated in Figure <ref type="figure">8</ref>. Our findings reveal a significant limitation: the retrieved background knowledge corpus lacks crucial content related to key concepts such as "filter blood," thereby constraining the model's ability to generate sophisticated, course-relevant questions.</p><p>The third challenge, Course Ambiguity (Figure <ref type="figure" target="#fig_10">9</ref>), represents a important limitation in current DDCQG systems. In these cases, the generated questions demonstrate engagement with course-specific content, instead defaulting to surface-level expansions of the target text. This suggests a deeper architectural limitation in connecting diagram elements with relevant course concepts and pedagogical objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Details of human annotators</head><p>For data annotation and evaluation, we engaged six graduate students (including both PhD and Master's students) from engineering disciplines who are also co-authors of this paper. All annotators possessed strong backgrounds in different subjects, making well-qualified for this task. Since the annotators were co-authors actively involved in the research, no formal recruitment process or compensation was required, and they were fully aware of how the data would be used in the study. The annotation process focused solely on content evaluation and did not involve collecting any personal identifying information or expose annotators to any risks. As this research involved co-authors analyzing academic content rather than external human subjects, it was determined to be exempt from formal ethics review board approval. The annotation work was conducted as part of regular academic research activities within our institution.</p><p>No protected or sensitive demographic information was collected or used in this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Details of Ai Assistants In Research Or Writing</head><p>We used Claude-3.5-Sonnet, o1, o3-mini-high, and Deepseek-R1 to help us write code and polish the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Knowledge triple: (Bivalves, impact, [mask]) (6) Input &amp; Course: Bivalves &amp; Ecological interactions Question: If the number of Bivalves suddenly decreases, what will happen to the number of Geese and Mute Swans?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of different text constraints for an example in DiagramQG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Discipline: Basic sciences • Subject: Biology • Course: Ecological interactions • Target Text: Bivalves • Question: If the number of Bivalves suddenly decreases, what will happen to the number of Geese and Mute Swans? • Discipline: Earth and environmental sciences • Subject: Climate Science • Course: Climate zones and biomes • Target Text: Negative acceleration • Question: In which biome is tree growth stunted by low temperatures and a short growing season? • Discipline: Humanities and social sciences • Subject: History • Course : Age of exploration • Target Text: Columbian Exchange • Question: Based on the definition of the Columbian Exchange, which arrow could show a part of the Columbian Exchange? • Discipline: Engineering and technology • Subject: Computer Science • Course: Network topology • Target Text: Topology • Question: What is the type of topology in the graph?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Four different examples of different subjects in DiagramQG dataset.Humanities and social sciences (10.6%) ◼ Art Theory Aesthetics, Printmaking, Sculpture, … ◼ Civics Government, Reconstruction, … ◼ Cognitive Science Abstract Tangram Recognition, … ◼ Grammar Sentences, fragments, and run, … ◼ History European history, The silk road, … ◼ Literature Children's literature, Drama, Fiction, … ◼ Music Music, … ◼ Psychology Abnormal psychology, Clinical psychology, … ◼ Sociology Social economics, Sociology theory, … ◼ Writing Persuasive strategies, …</figDesc><graphic coords="3,295.29,141.02,98.92,55.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>old of 70 are removed, and only the highest-scoring set of text constraints is retained for each diagramquestion pair. The resulting DiagramQG dataset contains 25,798 samples associated with 15,720 unique diagrams. Examples from four subjects in the DiagramQG dataset are shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of diagrams, questions ratios across different courses in DiagramQG. diagram complexity establishes DiagramQG as the first extensive dataset specifically designed for diagram question generation across diverse educational domains. The dataset facilitates the development of more robust and versatile questiongeneration systems for educational applications.</figDesc><graphic coords="4,306.23,258.97,217.85,169.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) Process the diagram into a pyramid structure of patches. (b) Our DiagramQA baseline, HKI-DDCQG, consists of three distinct stages: HierKnowExtract, KnowSelect, and KnowFusionQG. In this framework, orange modules indicate trainable parameters, blue modules represent fixed parameters, and gray modules denote components without learnable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 6: Comparison between HKI-DDCQG and GPT-4o using two examples from the DiagramQG dataset.</figDesc><graphic coords="8,324.96,221.50,71.93,56.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6</head><label>6</label><figDesc>Figure 6 compares HKI-DDCQG with GPT-4o on DiagramQG examples. While GPT-4o generates fluent questions, HKI-DDCQG demonstrates superior capability in producing course-relevant questions that better assess student understanding. More cases are provided in supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>InputFigure 9 :</head><label>9</label><figDesc>Figure 9: Three different cases (Course Ambiguity) in DiagramQG dataset.</figDesc><graphic coords="15,97.89,80.61,101.90,75.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparing characteristics of other datasets and DiagramQG, where Q. and I. mean question and image. Num. of Q. Num. of I. Object on I.</figDesc><table><row><cell>Images</cell><cell>Text Constraint Subjects</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on K-VQG dataset.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">BLEU-4 CIDEr METEOR</cell></row><row><cell></cell><cell>IM-VQG</cell><cell>11.44</cell><cell>17.07</cell><cell>0.26</cell></row><row><cell></cell><cell>K-VQG</cell><cell>18.84</cell><cell>22.79</cell><cell>1.31</cell></row><row><cell>K-VQG</cell><cell>ConVQG</cell><cell>20.01</cell><cell>22.66</cell><cell>1.53</cell></row><row><cell></cell><cell>HKI-DDCQG B</cell><cell>25.34</cell><cell>26.52</cell><cell>1.82</cell></row><row><cell></cell><cell>HKI-DDCQG Q</cell><cell>32.12</cell><cell>30.12</cell><cell>2.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on three VQG datasets.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">BLEU-1 CIDEr METEOR</cell></row><row><cell></cell><cell>MDN</cell><cell>36.0</cell><cell>23.4</cell><cell>0.51</cell></row><row><cell></cell><cell>MC-BMN</cell><cell>40.7</cell><cell>22.6</cell><cell>0.50</cell></row><row><cell>VQG-COCO</cell><cell>ConVQG</cell><cell>50.2</cell><cell>26.4</cell><cell>0.56</cell></row><row><cell></cell><cell>HKI-DDCQG B</cell><cell>55.6</cell><cell>43.4</cell><cell>0.74</cell></row><row><cell></cell><cell>HKI-DDCQG Q</cell><cell>58.7</cell><cell>47.6</cell><cell>1.08</cell></row><row><cell></cell><cell>IM-VQG</cell><cell>36.47</cell><cell>30.25</cell><cell>0.15</cell></row><row><cell></cell><cell>KVQG</cell><cell>27.18</cell><cell>55.38</cell><cell>0.13</cell></row><row><cell>OK-VQA</cell><cell>LV2-Net</cell><cell>29.90</cell><cell>92.17</cell><cell>0.15</cell></row><row><cell></cell><cell>HKI-DDCQG B</cell><cell>31.51</cell><cell>112.61</cell><cell>0.20</cell></row><row><cell></cell><cell>HKI-DDCQG Q</cell><cell>33.82</cell><cell>128.31</cell><cell>0.24</cell></row><row><cell></cell><cell>IM-VQG</cell><cell>39.30</cell><cell>22.11</cell><cell>0.12</cell></row><row><cell></cell><cell>KVQG</cell><cell>30.56</cell><cell>40.97</cell><cell>0.13</cell></row><row><cell>A-OKVQA</cell><cell>LV2-Net</cell><cell>32.11</cell><cell>60.06</cell><cell>0.14</cell></row><row><cell></cell><cell>HKI-DDCQG B</cell><cell>33.21</cell><cell>78.42</cell><cell>0.18</cell></row><row><cell></cell><cell>HKI-DDCQG Q</cell><cell>35.31</cell><cell>93.56</cell><cell>0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on DiagramQG with Qwen2.5-VL 3B &amp; T5-Base, where H.K.E, K.S and K.F mean Hier-KnowExtract, KnowSelect and KnowFusionQG.</figDesc><table><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>40.90</cell><cell>47.61</cell><cell>3.74</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell>38.85</cell><cell>45.42</cell><cell>3.48</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell>35.57</cell><cell>44.02</cell><cell>2.95</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell>34.04</cell><cell>42.52</cell><cell>2.82</cell></row><row><cell></cell><cell></cell><cell>✓</cell><cell>14.32</cell><cell>25.64</cell><cell>0.48</cell></row></table><note><p>H.K.E K.S K.F BLEU-4 METEOR CIDEr</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on DiagramQG under different settings. We employ HKI-DDCQG (Qwen2.5-VL 3B &amp; T5-Base) to obtain the results.</figDesc><table><row><cell>Setting</cell><cell cols="3">BLEU-4 METEOR CIDEr</cell></row><row><cell>n=1,m=4</cell><cell>35.45</cell><cell>41.70</cell><cell>2.98</cell></row><row><cell>n=2,m=4</cell><cell>38.13</cell><cell>46.01</cell><cell>3.28</cell></row><row><cell>n=4,m=4</cell><cell>40.63</cell><cell>47.25</cell><cell>3.72</cell></row><row><cell>n=3,m=1</cell><cell>37.14</cell><cell>44.32</cell><cell>3.35</cell></row><row><cell>n=3,m=2</cell><cell>38.85</cell><cell>45.63</cell><cell>3.50</cell></row><row><cell>n=3,m=4</cell><cell>41.16</cell><cell>47.74</cell><cell>3.77</cell></row><row><cell>n=3,m=5</cell><cell>41.41</cell><cell>48.12</cell><cell>3.78</cell></row><row><cell>n=3,m=3</cell><cell>40.90</cell><cell>47.61</cell><cell>3.74</cell></row></table><note><p><p>GT</p>Our If the number of Bivalves suddenly decreases, what will happen to the number of Geese and Mute Swans? If Bivalves suddenly decline in numbers, what would happen to the populations of Sea Ducks?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>is an advanced open-source Multimodal Large Language Model (MLLM) series, building on InternVL 2.0 with enhanced training, testing, and data quality. It rivals top commercial models like GPT-4o and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>25% for testing purposes. The comprehensive statistical distribution across these partitions, along with the aggregate dataset metrics, is presented in Table7. It can be seen that some images have intersections, but this does not affect the experimental results, because its course and ques-tion, and can effectively detect overfitting, because overfitting will cause the generated questions to be inconsistent with the current requirements D More case studies on Diagram Through extensive examination of the DDCQG (Diagram-based Question Generation) task, we have identified and analyzed three critical bottlenecks: Text Constraint Lost, Diagram Interpretation Bias, and Course Ambiguity. Detailed case studies illustrating these challenges are presented in Figures 7, 8, and 9, respectively. The first challenge, Text Constraint Lost (Figure</figDesc><table><row><cell>Statistic</cell><cell>Number</cell></row><row><cell>Total Diagram</cell><cell>15,720</cell></row><row><cell>Total Question</cell><cell>25,798</cell></row><row><cell>Total Discipline</cell><cell>6</cell></row><row><cell>Total Subject</cell><cell>37</cell></row><row><cell>Total Course</cell><cell>371</cell></row><row><cell>Train Diagram</cell><cell>11,817</cell></row><row><cell>Train Question</cell><cell>17,880</cell></row><row><cell>TotTrainal Discipline</cell><cell>6</cell></row><row><cell>Train Subject</cell><cell>37</cell></row><row><cell>Train Course</cell><cell>351</cell></row><row><cell>Val Diagram</cell><cell>1,151</cell></row><row><cell>Val Question</cell><cell>1,104</cell></row><row><cell>Val Discipline</cell><cell>6</cell></row><row><cell>Val Subject</cell><cell>33</cell></row><row><cell>Val Course</cell><cell>310</cell></row><row><cell>Test Diagram</cell><cell>6,767</cell></row><row><cell>Test Question</cell><cell>5,565</cell></row><row><cell>Test Discipline</cell><cell>6</cell></row><row><cell>Test Subject</cell><cell></cell></row><row><cell>Test Course</cell><cell>371</cell></row><row><cell cols="2">Table 7: Main statistics in DiagramQG</cell></row><row><cell cols="2">sophisticated visual comprehension abilities. Sec-</cell></row><row><cell cols="2">ond, they need comprehensive domain knowledge</cell></row><row><cell cols="2">across multiple academic disciplines. Following</cell></row><row><cell cols="2">established methodological practices in machine</cell></row><row><cell cols="2">learning, we implemented a stratified partitioning</cell></row><row><cell cols="2">of the dataset, allocating 70% for training, 5% for</cell></row><row><cell>validation, and</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="37" xml:id="foot_0"><p>subjects, and 371 courses. While natural imbalances exist in educational content availability, our dataset provides broader coverage than previous work, enabling contextually appropriate question generation across various educational domains. As an important resource driving AI capabilities in educational reasoning, DiagramQG maintains high standards for data quality and ethical considerations. In all experiments, we strictly comply with all licensing requirements for models and data.</p></note>
		</body>
		<back>

			<div type="funding">
<div><head n="7">Acknowledgements</head><p>This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFC3303600</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62137002</rs>, <rs type="grantNumber">62293550</rs>,  <rs type="grantNumber">62293553</rs>, <rs type="grantNumber">62293554</rs>, <rs type="grantNumber">62450005</rs>, <rs type="grantNumber">62437002</rs>,  <rs type="grantNumber">62477036</rs>, <rs type="grantNumber">62477037</rs>, <rs type="grantNumber">62176209</rs>, <rs type="grantNumber">62192781</rs>,  <rs type="grantNumber">62306229</rs>), '<rs type="projectName">LENOVO-XJTU' Intelligent Industry Joint Laboratory Project</rs>, the <rs type="funder">Shaanxi Provincial So-</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_puRJbKV">
					<idno type="grant-number">2022YFC3303600</idno>
				</org>
				<org type="funding" xml:id="_r638cYe">
					<idno type="grant-number">62137002</idno>
				</org>
				<org type="funding" xml:id="_ss3Ze5G">
					<idno type="grant-number">62293550</idno>
				</org>
				<org type="funding" xml:id="_en9bEkt">
					<idno type="grant-number">62293553</idno>
				</org>
				<org type="funding" xml:id="_wM8B9D4">
					<idno type="grant-number">62293554</idno>
				</org>
				<org type="funding" xml:id="_3DBDa9Q">
					<idno type="grant-number">62450005</idno>
				</org>
				<org type="funding" xml:id="_prTmFQj">
					<idno type="grant-number">62437002</idno>
				</org>
				<org type="funding" xml:id="_r8F2Eje">
					<idno type="grant-number">62477036</idno>
				</org>
				<org type="funding" xml:id="_bdZQeRY">
					<idno type="grant-number">62477037</idno>
				</org>
				<org type="funding" xml:id="_uQtGvt9">
					<idno type="grant-number">62176209</idno>
				</org>
				<org type="funding" xml:id="_pdYknK7">
					<idno type="grant-number">62192781</idno>
				</org>
				<org type="funded-project" xml:id="_tSpTgHt">
					<idno type="grant-number">62306229</idno>
					<orgName type="project" subtype="full">LENOVO-XJTU&apos; Intelligent Industry Joint Laboratory Project</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Closed-Source Large vision-language models: GLM4-V is a large-scale multimodal language model developed by ZhiPu, featuring outstanding visual understanding and language generation capabilities. It employs a unified pre-training framework, capable of handling multiple modalities such as text, images, and audio. GLM4-V demonstrates strong performance in tasks like visual question answering, image description, and cross-modal reasoning, and can quickly adapt to new scenarios through few-shot learning. The model supports both Chinese and English, excelling in multilingual understanding and generation. Claude-3.5-Sonnet is a next-generation multimodal assistant developed by Anthropic, exhibiting exceptional performance in both visual and language understanding. It utilizes an innovative neural network architecture that allows for deep comprehension of image content and complex reasoning. The model has strict controls in terms of safety and ethics, capable of identifying and filtering inappropriate content. A notable feature is its strong contextual understanding and coherent conversational abilities.</p><p>GPT-4o is the latest large language model developed by OpenAI, possessing powerful multimodal understanding and generation capabilities. It can process image and text inputs and perform complex reasoning and knowledge integration. This model showcases remarkable few-shot learning abilities, quickly mastering new tasks with only a few examples. GPT-4o also excels in creativity, generating high-quality text, code, and other creative content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More details on DiagramQG</head><p>The DiagramQG dataset is a comprehensive collection of questions covering 6 disciplines, 37 subjects, and 371 courses, consisting of 25,798 questions and 15,720 diagrams. This dataset aims to encourage models to generate questions that assess students' course understanding by leveraging the provided input &amp; course text constraints and diagrams, as shown in Figure <ref type="figure">2</ref>. Accomplishing this task requires two key capabilities from computational models. First, they must demonstrate</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Table 2: Results on DiagramQG, where GPT-4o (T) and Claude-3.5-Sonnet (T) are meant to generate questions using only text, where, Q-3B, Q-7B, T5-B, T5-L mean, Qwen2.5-VL-3B, Qwen2.5-VL-7B, T5-Base and T5-Large. Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 Bert-Score METEOR CIDEr ROUGE-L Fleur In</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Wang et al., 2025b</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Minicpm-V</forename></persName>
		</author>
		<editor>Yao et al.</editor>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepseek-Vl ;</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
		<editor>Qwen2.5-VL</editor>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
		<editor>Qwen2.5-VL</editor>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bai</surname></persName>
		</author>
		<editor>Qwen2.5-VL</editor>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fine-Tuned K-Vqg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Uehara and Harada</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Convqg</surname></persName>
		</author>
		<author>
			<persName><surname>Blip) (mi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kc-Vqg (blip) (</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Kc-Vqg</surname></persName>
		</author>
		<editor>Liu et al.</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m">Ours HKI-DDCQG</title>
		<meeting><address><addrLine>BLIP, T5-B</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hki-Ddcqg</surname></persName>
		</author>
		<imprint>
			<pubPlace>B, T5-B</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hki-Ddcqg</surname></persName>
		</author>
		<imprint>
			<pubPlace>B, T5-B</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hki-Ddcqg</surname></persName>
		</author>
		<editor>BLIP, T5-L</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hki-Ddcqg</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Q-7B, T5-L</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.13923</idno>
		<title level="m">Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inferential visual question generation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4164" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deconfounded visual question generation with causal inference</title>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5132" to="5142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">M3cot: A novel benchmark for multi-domain multi-step multi-modal chainof-thought</title>
		<author>
			<persName><forename type="first">Qiguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8199" to="8221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual representations in science education: The influence of prior knowledge and cognitive load theory on instructional design principles</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cook</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science education</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1073" to="1091" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">Lavie</forename><surname>Michael Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A question type driven framework to diversify visual question generation</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4048" to="4054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Isaaq-mastering textbook questions with pre-trained transformers and bottom-up and top-down attention</title>
		<author>
			<persName><forename type="first">José</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gómez-Pérez</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Raúl</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5469" to="5479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2025a. Evochart: A benchmark and a self-training approach towards real-world chart understanding</title>
		<author>
			<persName><forename type="first">Muye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3680" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2025b. Vprochart: Answering chart question through visual perception alignment agent and programmatic solution reasoning</title>
		<author>
			<persName><forename type="first">Muye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3689" to="3696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4999" to="5007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Information maximizing visual question generation</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2008" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Amanda Jane Lambertus and 1 others. 2008. Students&apos; understanding of the function concept: Concept images and concept definitions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fleur: An explainable reference-free evaluation metric for image captioning using a large multimodal model</title>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imseong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungjoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3732" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2024a. Knowledgeguided cross-topic visual question generation</title>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)<address><addrLine>Torino, Italia</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA and ICCL</publisher>
			<biblScope unit="page" from="9854" to="9864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2024b. Knowledgeguided cross-topic visual question generation</title>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</meeting>
		<imprint>
			<biblScope unit="page" from="9854" to="9864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2024c. Look before you leap: Dual logical verification for knowledge-based visual question generation</title>
		<author>
			<persName><forename type="first">Xumeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)<address><addrLine>Torino, Italia</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA and ICCL</publisher>
			<biblScope unit="page" from="10802" to="10812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoshu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05525</idno>
		<title level="m">Yaofeng Sun, and 1 others. 2024. Deepseekvl: towards real-world vision-language understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2507" to="2521" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13214</idno>
		<title level="m">Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chain-of-exemplar: Enhancing distractor generation for multimodal educational question generation</title>
		<author>
			<persName><forename type="first">Haohao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.432</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7978" to="7993" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for textbook question answering</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7378" to="7388" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convqg: Contrastive visual question generation with multimodal guidance</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syrielle</forename><surname>Montariol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javiera</forename><surname>Castillo Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi-Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="4207" to="4215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning by asking questions</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Pmlr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and 1 others</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">K-vqg: Knowledge-aware visual question generation for common-sense acquisition</title>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4401" to="4409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual question generation for class acquisition of unknown objects</title>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Tejero-De-Pablos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="481" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Computer science diagram understanding with topology parsing</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">2025a. Alignment-guided self-supervised learning for diagram question answering</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2024.3521744</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2141" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cogdqa: Chain-of-guiding learning with large language models for diagram question answering</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12386</idno>
		<title level="m">Jianfei Gao, and 1 others. 2025b. Internvideo2. 5: Empowering video mllms with long and rich context modeling</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Explicitly guided difficulty-controllable visual question generation</title>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guimin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="25552" to="25560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Knowledge-based visual question generation</title>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingbao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7547" to="7558" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Moca: Incorporating domain pretraining and cross attention for textbook question answering</title>
		<author>
			<persName><forename type="first">Fangzhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qika</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudai</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2023.109588</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">109588</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Radial graph convolutional network for visual question generation. IEEE transactions on neural networks and learning systems</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1654" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">2024a. Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01800</idno>
		<title level="m">Zhihui He, and 1 others. 2024b. Minicpm-v: A gpt-4v level mllm on your phone</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9556" to="9567" />
		</imprint>
	</monogr>
	<note>Yuxuan Sun, and 1 others</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanrui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyou</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2025.acl-long.811</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
