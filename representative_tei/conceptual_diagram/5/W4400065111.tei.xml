<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Compositional Interpretability for XAI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
							<email>sean.tull@quantinuum.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>17 Beaumont Street</addrLine>
									<settlement>Quantinuum Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robin</forename><surname>Lorenz</surname></persName>
							<email>robin.lorenz@quantinuum.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>17 Beaumont Street</addrLine>
									<settlement>Quantinuum Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
							<email>steve.clark@quantinuum.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>17 Beaumont Street</addrLine>
									<settlement>Quantinuum Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilyas</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>17 Beaumont Street</addrLine>
									<settlement>Quantinuum Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
							<email>bob.coecke@quantinuum.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>17 Beaumont Street</addrLine>
									<settlement>Quantinuum Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Compositional Interpretability for XAI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial intelligence (AI) based on machine learning, while highly successful in many applications, currently relies largely on black-box models which lack interpretability. The developing field of eXplainable AI (XAI) strives to address this major concern, being most critical in high-stakes areas such as the financial, legal and health sectors.</p><p>We present an approach to defining AI models and studying their interpretability based on category theory. For this we take a compositional viewpoint, employing the notion of a compositional model, which sees a model in terms of formal string diagrams which capture its abstract structure together with its concrete implementation. This view is comprehensive and incorporates deterministic, probabilistic and quantum models. We demonstrate explicitly how a wide range of AI models can be seen as compositional models, ranging from linear and rule-based models, to (recurrent) neural networks, transformers, VAEs, and causal and DisCoCirc models.</p><p>This analysis provides the grounds for a meaningful comparison of different models, along with a definition of interpretation of a model in terms of its compositional structure. We demonstrate how one may analyse the interpretability of a model and use this to clarify common themes in XAI within this broad compositional perspective. In particular, the approach recognises the standard notion of 'intrinsically interpretable' models as essentially compositional, finding that what makes these models so transparent is brought out most clearly diagrammatically. This leads us to the new, more general notion of what we call compositionally-interpretable (CI) models, which in addition to linear and rule-based models include, for instance, causal models, conceptual space models, and DisCoCirc models.</p><p>We explicitly demonstrate the explainability benefits that CI models can offer, based on their rich compositional structure, none of which are available for black-box architectures. Firstly, their structure may allow the computation of other quantities of interest in terms of their components, and may correspond to structure in the phenomenon being modelled, facilitating inference about the world from the model. Secondly, we show that CI models allow for several forms of diagrammatic explanations for their behaviour, respectively in terms of influence constraints, diagram surgery and the novel notion of rewrite explanations using graphical equations. Finally, we discuss many directions for a further exploration of the approach and overarching vision, in particular raising the question of how to learn such meaningfully structured models in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial intelligence (AI) based on machine learning (ML) has achieved enormous practical successes over the past decade, but today is faced with major concerns about its lack of interpretability. Indeed, the explainability of AI has become an increasingly prevalent theme in public discussions around its safety and role in our society. Ultimately, this can be said to stem from the black-box nature of ML models: though neural networks can be trained effectively to solve a given task, their lack of explicit meaningful structure means that the manner in which they do so is largely inaccessible to us from the outside. The absence of interpretability is a particular concern in sensitive areas, such as the financial, legal and health sectors, where accountability and a strong kind of transparency are required and an ethical and responsible use of AI needs to be established [Rud19, GSC + 19, XUD + 19].</p><p>In response, the growing field of eXplainable AI (XAI) is attempting to solve the interpretability problem, largely through the exploration of so-called 'post-hoc' techniques which take a trained AI model and aim to give explanations for either its overall behaviour, or individual outputs. Examples include saliency maps for images [SVZ13, ZF14, SCD + 17], Shapley values which rank the importance of input features <ref type="bibr" target="#b88">[LL17]</ref>, and counterfactual explanations of outputs <ref type="bibr" target="#b146">[WMR17]</ref>.</p><p>However, the field of XAI has numerous issues to overcome <ref type="bibr" target="#b47">[FK23a]</ref>, and several authors including Rudin <ref type="bibr" target="#b113">[Rud19]</ref> have pointed out the problematic nature of many of the 'explanations' produced by such methods. Rather than relying on after-the-fact, and by their nature limited, explanations of standard AI models, these authors argue that in sensitive high-stakes areas, such as in decisions on bail, loans or hiring, one should instead make use of models which are intrinsically interpretable: that is, come with explicit structure which is meaningful to us from the outside. Such intrinsically interpretable models do not require the post-hoc methods of XAI, serving instead as their own explanation, and one of a deeper kind, which is manifest, principled and not approximate. Crucially, in many cases they can perform just as well as state-of-the-art black-box models <ref type="bibr" target="#b113">[Rud19]</ref> and hence should not be dismissed in critical high-stakes situations. To address these concerns, it would be desirable to have more theoretical and foundational tools which allow us to assess the interpretability of a given model. Further, one would ideally hope to broaden the scope and applicability of intrinsically interpretable models beyond the simple examples of rule-based and linear models which are typically considered.</p><p>In this work, we present a theoretical framework for both defining AI models and analysing their interpretability. Our approach is based on category theory, a mathematical language for describing processes and their composition, which has in recent years found a number of applications in ML [FST19, CGG + 22, SGW21]. A benefit of the categorical approach is its use of the simple but formal graphical calculus which allows the description of processes in terms of intuitive string diagrams <ref type="bibr" target="#b118">[Sel11,</ref><ref type="bibr" target="#b109">PZ23]</ref>. We show how a wide range of AI models can be described in terms of string diagrams, in order to demonstrate the broad applicability of the categorical approach, and introduce categories to those with an ML background. Amongst many others we discuss linear and rule-based models, neural networks (NNs), recurrent NNs <ref type="bibr" target="#b44">[Elm90]</ref> and transformers [VSP + 17], DisCoCirc and DisCoCat models in natural language processing (NLP) <ref type="bibr" target="#b35">[CSC10,</ref><ref type="bibr" target="#b32">Coe21b,</ref><ref type="bibr" target="#b145">WMLC23]</ref>, conceptual space models [BCG + 19, TSZC24] and causal models [Pea09, <ref type="bibr" target="#b76">JKZ19,</ref><ref type="bibr" target="#b90">LT23]</ref>. These string diagrams generalise several existing graphical approaches, including decision trees, computational graphs for neural networks, DAGs for causal models, and even circuit diagrams in quantum computation, whilst also upgrading these approaches to allow for formal reasoning about the models themselves.</p><p>The diagrammatic approach provides a unified perspective in which to both compare AI models, and readily analyse the interpretability of a model in terms of its components. In doing so we see that the explainability of the classic examples of intrinsically interpretable models, namely linear and rule-based models, is evident diagrammatically. Beyond these examples, our analysis highlights the explainability benefits offered by models coming with rich interpretable compositional structure, which we term compositionallyinterpretable (CI) models. This encourages us to broaden our perspective beyond the standard examples of intrinsically interpretable models, which form special cases, to more general CI models. Prominent examples of CI models include causal models, which are actively studied in the growing field of causal ML [Sch22, SLB + 21a] and where compositional structure is given by causal structure, and DisCoCirc models in NLP <ref type="bibr" target="#b32">[Coe21b,</ref><ref type="bibr" target="#b145">WMLC23]</ref>, where compositional structure derives from grammatical structure.</p><p>For most black-box models, the only manifestly interpretable components are the inputs and outputs, and thus XAI explanations often focus on input-output behaviour. CI models instead allow one to inter-pret and reason about their internal components. We make the interpretability benefits of such CI models explicit by showing how they allow for three specific forms of explanation: no-influence arguments as to their input-output influences; diagram surgery arguments which generalise causal interventions; and finally the new notion of rewrite explanations, which provide guarantees on, and explanations for, the outputs of compositional models satisfying certain interpretable equations.</p><p>Beyond these arguments, we also use our framework to give precise definitions that clarify and disentangle aspects of common intuitions in XAI. These include: when a model itself is interpretable rather than just affords (approximate) explanations of particular outcomes; notions of intrinsic interpretability; the distinction between 'abstract' (structure-based) and 'concrete' (semantics-based) interpretations; and the relation between structure in 'the model' and structure in 'the world'. Causal models also form a prime example of interpretable compositional models, and we aim to help clarify the prominent role of causal intuitions in XAI, where for explanatory purposes it is crucial to distinguish between causal structure 'just' induced by an NN-based model and that corresponding to the phenomena that the model is about.</p><p>The central contributions of this work are:</p><p>• The presentation of AI models as compositional models, providing a formal basis to compare different kinds of model on a common ground and study their interpretability.</p><p>• The clarification of certain concepts in XAI using the compositional perspective.</p><p>• The notion of compositionally-interpretable models, generalising intrinsically interpretable models.</p><p>• The kind of diagrammatic explanations as facilitated by compositionally-interpretable models.</p><p>Another noteworthy benefit of a categorical treatment is the ability to accommodate not only classical deterministic and probabilistic models, but also quantum models such as those based on quantum machine learning (QML) and implemented on quantum computers. Indeed, formal diagrammatic reasoning has historically been heavily motivated by the study of quantum processes and information <ref type="bibr" target="#b0">[AC04,</ref><ref type="bibr" target="#b25">CK18]</ref>, and quantum models are often described compositionally in terms of quantum circuits. Hence a categorical approach such as that proposed here can be seen as necessary in order to analyse interpretability of future quantum AI models.</p><p>Whether classical or quantum, the toolkit for analysing AI models and interpretability presented here naturally leads one to consider the explainability benefits of models which make use of rich compositions of interpretable components. In future work, we hope to further explore the space of such compositional models, including the difficult problem of how and to what extent compositional structure can be learned from data (generalising both causal representation learning for causal models [SLB + 21a] and learning conceptual domain structure [HMP + 16, TSZC24]), as well as further sharpen our explainability techniques such as the notion of rewrite explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>Let us now give a brief overview of the article and our diagrammatic treatment of AI models and interpretability as a whole. Our formalisation is based on category theory, the mathematics of processes and their composition, which allows us to describe the structure of models at an abstract level using string diagrams.</p><p>Formally, a category consists of a collection of objects A, B, . . . and morphisms (processes) between them. For example, consider a model which consists of a process M from a collection of inputs X i to a collection of outputs Y j . In diagrams, this would be depicted as below, where inputs and outputs (objects) are depicted as wires and the process as a box, read from bottom to top.</p><formula xml:id="formula_0">M X 1 . . . . . . Xn Y 1 Ym<label>(1)</label></formula><p>By choosing which category we are working in, we can consider different kinds of processes and thus models.</p><p>For example, M may describe a deterministic function, such as a neural network, a probability channel as in a Bayesian network, or even a quantum process as used in quantum computation.</p><p>In any case, to gain more understanding of the model we will need to 'open up' this process further, by decomposing it as a string diagram of internal components, and this is where the categorical language becomes useful. For example, we may know that the process M in fact takes the following form.</p><formula xml:id="formula_1">f g . . . . . . . . . . . . X 1 X k Xn W z Z Y 1 Y j Ym Y j+1 X k+1</formula><p>(2) This string diagram factorises M in terms of internal 'variables' W and Z, processes f and g, and the state z. The diagram itself can be understood as describing the abstract structure of the model. This structure is then given specific semantics when it is implemented via a specific choice of category, along with specific choices of objects and processes for the wires and boxes. For example, for a semantics based on neural networks, we would assign a dimension to each wire, a vector to z, and neural networks to the boxes f , g.</p><p>We can formalise this view of a model in categorical terms with the notion of a compositional model M, which consists of the following. Firstly, a collection G of abstract variables (wires in the diagram) and generators (the boxes), as well as any equations that these may satisfy. From these we can generate an entire structure category S, consisting of all abstract string diagrams which can be built from these components, such as the diagram in (2). Secondly, the model includes a choice of semantics category C, along with a representation V , f in C of each variable V and generator f, respectively. Formally this is equivalent to specifying a functor (mapping between categories):</p><p>-: S → C which gives semantics to any diagram built from the variables and generators. This view of a model as a functor between categories of structure (or 'syntax') and semantics has a long history in categorical logic <ref type="bibr" target="#b84">[Law63a,</ref><ref type="bibr" target="#b85">Law63b]</ref>. In practice, we usually define a compositional model implicitly by simply drawing one or several string diagrams which encode the variables and generators, such as that above in (2).</p><p>A large number of AI and ML models can be described string diagrammatically as compositional models in this way, and we cover many in this article. In typical cases the model essentially consists of a single input-output process, decomposed as a diagram such as (2). Examples include decision tree models, linear models, neural networks, as well as specific architectures such as the transformer (with fixed input length), and causal models.</p><p>String diagrams for these models are shown in Figure <ref type="figure" target="#fig_0">1</ref>. In each case these diagrams in fact correspond to well-known graphical representations of such models, namely as trees, computational graphs, or DAGs. String diagrams upgrade these graphical systems to also capture semantics and thus the ability to reason about the model. For example, the string diagram of a causal model allows us to both represent it (as for a DAG) and categorically carry out causal and probabilistic reasoning with the model <ref type="bibr" target="#b76">[JKZ19,</ref><ref type="bibr" target="#b48">FK23b,</ref><ref type="bibr" target="#b90">LT23]</ref>. Another common form of model does not distinguish any particular input-output process but instead makes use of a collection of processes and diagrams, formally within the structure category S. For example, this is common for some structured models of language, where one may have a specific diagram for each input text which captures its representation (semantics) in the model. These include recurrent neural networks (RNNs), bag of words models, or the more categorically oriented DisCoCat and DisCoCirc models. Examples of string diagrams representing text meanings for some of these models are shown in Figure <ref type="figure" target="#fig_1">2</ref>. Now, while a diagrammatic account can provide insights into a model, it doesn't yet tell us that a model is interpretable. For example, in the string diagram above for a neural network the individual neurons may have no meaning to us from the outside. Hence any interpretation which exists must be provided in addition to the formal structure of the model.</p><p>Here we will define an interpretation of a compositional model semi-formally, as a partial mapping from variables and related processes into a collection H of 'human-friendly' terms and concepts, which assigns them meaning. In more detail, an interpretation consists of two parts. Firstly, the abstract interpretation assigns meanings in H to the abstract structure of the model, namely its variables and generators. For example, we may say that a variable V in a model related to images corresponds to the concept 'brightness'. Secondly, a concrete interpretation assigns meanings to aspects of the semantics, i.e. processes in C, such as individual states. For example, a concrete interpretation could assert that the state V = 0 corresponds to 'dark' and V = 1 to 'bright'. Typically, both mappings are only partial, since most processes in C will not be interpreted, and neither may some of the variables or generators, such as those which are 'latent' or hidden.</p><p>Using this setup, we can analyse many examples of AI models compositionally, and discuss their interpretability. For example, while transformer models can be drawn as string diagrams, their internal components (e.g. attention heads) typically lack any prior interpretation. In contrast, models typically considered to be intrinsically interpretable, such as rule-based models, come with concrete interpretations, and moreover the components of their associated string diagrams precisely show the manner in which they are usually deemed to be interpretable.</p><p>This viewpoint suggests that we consider not only the standard examples of 'intrinsically interpretable' model, namely linear and rule based models, but more general forms of interpreted compositional models. Moreover, when these models come with rich compositional structure, involving many interpreted processes which may be composed and re-combined in a number of ways, the interpretability benefits become yet more apparent. As already mentioned, we term such models as compositionally-interpretable (CI). Examples of CI models include (interpreted) causal models, as explored in the field of Causal ML [SLB + 21a, Sch22], as well as DisCoCirc models in NLP <ref type="bibr" target="#b32">[Coe21b,</ref><ref type="bibr" target="#b145">WMLC23]</ref>.</p><p>One way to make this notion of 'rich' compositional structure precise is what we refer to as the framework of a model, which captures the way that the compositional structure of the model is actually used, i.e. which meaningful string diagrammatic calculations we can perform. For example, the framework of causal models is famously richer than that of plain statistical models, allowing us to compute not only conditional probabilities, but also interventions and counterfactuals <ref type="bibr" target="#b90">[LT23]</ref>, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. In order to make the proposed interpretability benefits of CI models more explicit, we can consider what forms of explanation they can provide over generic black-box models. For the latter, typically it is only the inputs and outputs which have a concrete interpretation, and so most post-hoc XAI techniques view the model simply at the 'outside' level of input-output behaviour. For example, counterfactual explanations (CFEs) involve searching for minor alterations x ′ to an input x for the model M which produce a given output y ′ ; i.e. given that M • x = y, which is the 'nearest' input x ′ with M • x ′ = y ′ ?</p><p>In contrast, for a CI model the internal components are themselves interpretable, allowing us to provide richer explanations and constraints on their behaviour. We discuss several forms of explanations available for CI models in Section 9. The most simple are (no-)influence arguments, illustrated in Figure <ref type="figure" target="#fig_3">4a</ref>, which allow us to reason about which variables can affect some particular outputs of a model. Often these influence constraints can be simply read off from the connectivity of the string diagram for a model, when it has nontrivial compositional structure. In contrast, the diagrams for typical neural networks are fully connected, offering no such constraints.</p><p>Next, given a string diagram for a model, we can consider altering or acting on it to examine how a given output is affected, or to inspect the model further at certain locations. Such diagram surgery, illustrated in Figure <ref type="figure" target="#fig_3">4b</ref>, can be seen to generalise interventions on causal models, as well as CFEs, by extending them beyond simply altering inputs to now also altering the internal processes of a model. Finally, the strongest constraints are provided by a novel form of explanation we call rewrite explanations, applicable only to models with interpretable compositional structure. These require knowledge about a model in the form of diagrammatic equations, which may either be discovered empirically after training, encouraged in training via a loss function, or imposed in the definition of a model. From such equations, a rewrite explanation consists of a diagrammatic argument proving that a given diagram, such as that describing the model applied to some input, is (approximately) equal to another, such as a given output value. Examples of rewrite explanations are shown in Figure <ref type="figure" target="#fig_4">5</ref>. Crucially, for these to qualify as explanations, the processes featured in the argument must themselves be interpretable. Because of this, and the need for non-trivial diagrammatic structure, typical AI models -unlike CI models -will not allow for any non-trivial rewrite explanations of their behaviour. (a) A DisCoCirc type model, where we explain why Alice is with Bob in the Garden, where is Alice? returns as its answer the location of the garden. The equations used in the rewriting express that if X is in/with Y then the answer to Where is X is simply Y. (b) A conceptual space type model, using information that yellow bananas are typically sweet to explain why they are output as tasty. The equation implicit in the first rewrite captures that a yellow banana is also sweet; in the second it states that sweetness on its own ensures tastiness.</p><p>Overall, we hope that the compositional perspective can help to elucidate the interpretability of a wide range of AI models, whilst enabling the definition of new inherently categorical forms of model coming with explainability benefits. The compositional viewpoint can also help to clarify various notions from across XAI, including the role of causal concepts, since causal models are a special case of compositional models (see Section 7).</p><p>Quantum models. A strength of the categorical view is the ability to accommodate both classical and quantum AI models uniformly, by choice of the semantics category. In this way the same abstract structure, such as that of a DisCoCirc NLP model illustrated below, may be implemented either classically in terms of neural networks (left), or as quantum circuits (right). In Section 10 we discuss aspects of interpretability related specifically to quantum AI models. In fact, the compositional perspective has been relatively widely adopted in quantum information, and many developments in applied category theory have their origins in the study of quantum processes. In addition, quantum AI models are often defined compositionally, in terms of circuit diagrams. Hence we claim that a compositional framework capable of accommodating both classical and quantum models uniformly, such as that offered here, will be essential to assess the interpretability of quantum models. While features such as entangled states may make it more challenging to assign specific concrete interpretations to all of the states of a quantum model, the remainder of our compositional approach to interpretability, including all of the diagrammatic forms of explanation outlined above, applies just as straightforwardly to quantum models as to classical.</p><p>Structure of the article. We begin in Section 2 by giving as context for this work a short summary of the current state of XAI, and listing further related research. Following this, in Section 3 we introduce the necessary mathematical background, introducing categories and string diagrams. In Section 4 we give our central definitions, namely that of a compositional model, and an interpretation of a model. To convey these abstract definitions, in Section 5 we describe a comprehensive list of AI models as compositional models, and discuss their interpretability, including decision trees, neural networks, transformers, recurrent neural networks, variational autoencoders, causal models, and DisCo-style models in NLP. In Section 6 we then introduce the further theoretical notion of a compositional framework, which can articulate the use of models with 'rich' and 'meaningful' compositional structure. We make use of our compositional perspective to discuss various issues in conventional XAI in Section 7, including the model-vs-world distinction, comparing CFEs and Pearlian counterfactuals, and causal abstraction. We then take stock of all we have covered and make our observations on the relation between compositionality and interpretability in Section 8. In Section 9 we sharpen these observations by giving explicit forms of explanations which can be offered by interpretable compositional models, based on signalling, diagram surgery, and the notion of rewrite explanations. In Section 10 we discuss aspects of interpretability related specifically to quantum AI models. Finally, in Section 11 we discuss future directions for research, including the exploration of further forms of compositional model and the key question of how compositional structure can be learned from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The context</head><p>In this section we briefly review the background context of XAI, its issues, and further related research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional XAI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">A rough overview</head><p>The field of XAI is concerned with the explainability and interpretability of ML models. What exactly these two terms are supposed to refer to and how they relate to each other varies, but roughly speaking, XAI studies what can be said about a model in human intelligible terms so as to make understandable how and why that model works and gives particular outputs. Typical questions to introduce and illustrate the field are "How does the model make predictions?" [GDC + 23], "For situation X, why was the outcome Y and not Z?" [GWE + 19] and "Was it a specific feature that caused the decision made by the model?" [MKG + 20].</p><p>The reasons why we would like to answer such and related questions, and strive for better explainability of ML models, can be as broad as: justifying a model or its predictions, e.g. to avoid unfair bias and build trust; controlling and debugging a model; improving a model in terms of accuracy through better explainability; and discovering scientific knowledge through the model <ref type="bibr" target="#b16">[CBC23,</ref><ref type="bibr" target="#b92">MCB20]</ref>. Arguably, the first reason drives most of the efforts in XAI, especially with a view to so called high-stakes decision problems, where lives are directly impacted by the model's output. High-stakes areas include the healthcare sector (e.g. medical image analysis for diagnosis and treatment recommendations), the financial sector (e.g. automated loan approval and investment advice), the legal sector (e.g. bail decisions), and the educational sector (e.g. assessing university applications) <ref type="bibr" target="#b68">[HDR18,</ref><ref type="bibr" target="#b56">Fre22]</ref>.</p><p>The lack of interpretability of a model can be due to a lack of knowledge of the identity of the overall input-output function, be it because it is proprietary or because of some other epistemic restriction on the model parameters. We can refer to this as the black-boxness of the model. Alternatively, the identity of the model may be known, but it may lack interpretability because it is, loosely speaking, too complex or obscure to be 'made sense of'. We can refer to this instead as the opacity of a model; that is, a white-box, the opposite of a black box, may still be opaque.</p><p>The XAI literature is large, somewhat piecemeal and in most parts not guided by an agreed-upon conceptual foundation with clear definitions (see below). The following intends to only touch on some of the main themes in XAI for the unfamiliar reader, who can otherwise refer to the the many existing review articles for a more detailed background; see for instance [GDC + 23, PLM + 21, MCB20, ADRDS + 20, DR20, DVK17].</p><p>Post-hoc vs intrinsic. A main distinction is that between post-hoc and intrinsic (or ante-hoc) interpretability. Some models are regarded as manifestly interpretable due to the properties they have by design and are thus referred to as intrinsically interpretable models. A precise definition of the concept is never given in the literature, but the canonical set of examples include linear (regression) models and rule-based models (also called if-then or logical models) such as decision lists, decision trees, and scoring systems. Any of these qualify as intrinsically interpretable only subject to size conditions like sparsity. The intuitions behind intrinsic interpretability will be discussed in more detail in Secs. 5 and 8. In contrast, post-hoc interpretability is the kind achieved through XAI methods applied to a given model after training, when it is opaque and lacks any intrinsic interpretability. Post-hoc methods are usually referred to as producing 'explanations' of the model of some sort and the vast majority of the XAI literature is concerned with the toolbox of such methods.</p><p>Typical post-hoc methods and dimensions of distinction. Post-hoc methods are commonly compared and classified with respect to various dimensions. Explanations are either local or global, where 'local' means that the explanandum concerns a particular prediction, i.e. input-output pair -like 'why was person X predicted to default on the loan?' -and 'global' tends to refer to some form of averaging or combining of local explanations, which is then considered to be explanatory of the model and its workings as a whole. A method may be model-agnostic, in which case it can be applied to a black box, i.e. it only relies on access to the input-output behaviour; or the method may be model-specific, i.e. rely on facts about the given model, such as the parameters to compute gradients for back-propagation. Molnar <ref type="bibr" target="#b92">[MCB20]</ref> moreover distinguishes methods in three (not mutually exclusive) ways, by whether they: analyse the components of the model (model-specific as reliant on a useful decomposition of the model), study the sensitivity of the model (investigating the input-output behaviour), or use a surrogate model (an intrinsically interpretable model trained to approximate the given black-box or opaque model in a specific local input region).</p><p>The majority of methods study the sensitivity of the model in some form or another. For instance, a counterfactual explanation (CFE) of a given input-output pair (x, y), an idea first introduced in [WMR17], may be roughly paraphrased as an alternative input x ′ , minimally different from x and such that the model produces some (usually desired, but in any case predefined) output y ′ ̸ = y. Note that CFEs are in general highly non-unique, and see <ref type="bibr" target="#b56">[Fre22]</ref> for other pitfalls and a generally insightful analysis of CFEs, as well as Sec. 7.2 for further details.</p><p>Many of the other, non-CFE techniques fall into the broad category of feature attribution methods. The features may be pixels, tabular data or words, and the basic idea of feature attribution is that it 'explains' a particular output by assigning each input feature a number that is interpreted as a weight or contribution towards the output (negatively or positively). Prominent examples are LIME and SHAP. Local Interpretable Model-agnostic Explanations (LIME), first introduced in <ref type="bibr" target="#b112">[RSG16]</ref>, is a local method that fits a linear regression model -as an intrinsically interpretable surrogate model -in feature space within the vicinity of the concrete data point to be 'explained'. The output is the list of coefficients, understood as approximate feature importance weights. Shapley Additive exPlanations (SHAP), first introduced in [LL17], is a local method that also fits a surrogate model, though such that its coefficients (locally) approximate analogues of game-theoretic Shapley values, again interpreted as the different features' contributions to a given outcome.</p><p>Terminology, desiderata and definitions. The basic intuitions, and what exactly interpretability and explainability mean, differ significantly across XAI. For instance, "is it an approximation of a complex model", "an assignment of causal responsibility", "a set of human intelligible features contributing to a model's prediction", or "a function mapping a less interpretable object into a more interpretable one"? [PLM + 21]. Is it about "means to engender trust [...], faith in a model's performance, [...] or a low-level mechanistic understanding of our models"? <ref type="bibr" target="#b87">[Lip18]</ref>. In light of this, review articles have collated and compared different definitions (see, e.g. [GDC + 23, PLM + 21, DR20]) with some attempting to provide an overarching definition to 'capture them all' and to give guidance. Seeing as the field is broad, it is perhaps not surprising that one then ends up with rather vague and high-level suggestions. For instance, in [GDC + 23] "an AI system is interpretable if it is possible to translate its working principles and outcomes in human-understandable language without affecting the validity of the system". In [PLM + 21] one finds "an explanation is the process of describing one or more facts, such that it facilitates the understanding of aspects related to said facts (by a human consumer)" and that an "interpretation is the assignment of meaning (to an explanation)", so that "for ML, the assigned meaning refers to notions of the high-level task for which the explanans is provided as evidence. An interpretation is therefore bridging the gap between under-specified non-functional requirements of the original task and its representation in formal, low-level primitives."</p><p>Certainly, it is important to be explicit about what the explanandum, the explanans and the explanatory link between the two is, as voiced for example in [FK23a, PLM + 21, Cre20]. Rather than searching for a distinguished, overarching definition though, one may also attempt to disentangle and clearly name different aspects of 'interpretability'. There will not be one unique way to do this, but for instance Lipton <ref type="bibr" target="#b87">[Lip18]</ref> distinguishes : (1) simulatability that focuses on a simplicity aspect, which calls "a model transparent if a person can contemplate the entire model at once"; (2) decomposability that demands that "each part of the model -each input, parameter, and calculation -admits an intuitive explanation"; and (3) algorithmic transparency, which could be given by e.g. proven guarantees that training converges to a unique solution. This work's focus is closest to the second.</p><p>An aspect which has attracted considerable attention is the role of the 'human element' in XAI. This includes the common attitude of admitting some degree of subjectivity in desiderata for satisfying explanations among (non-expert) humans (see, e.g., [GDC + 23, MHS17]), but also includes calls to turn to the social sciences for theories of explanation (see, e.g., <ref type="bibr" target="#b93">[MHS17,</ref><ref type="bibr" target="#b129">Sul22a]</ref>). Note the call is not for scientific explanations, but of how humans generally give explanations to each other, and proposals to explicitly model the human in an interpretability framework <ref type="bibr" target="#b99">[MPT23]</ref>.</p><p>Role of causality in XAI. Causality plays a prominent and increasingly explicit role in XAI, though one that is multifaceted and that can cause confusion. See <ref type="bibr" target="#b16">[CBC23]</ref> for a review covering some of its aspects.</p><p>As is well-known, many ML models can be seen to (implicitly) induce a causal model. The details of how exactly and what kind of causal model we leave to Sec. 7, but this observation gives context to how many posthoc methods are in fact ways to uncover causal properties of the model and to make causal statements about it. Obviously, CFEs have a decidedly causal flavour (though as we argue in Sec. 7.2, unlike often claimed, a CFE is not a counterfactual in the Pearlian causal model sense). Also, feature attribution methods are essentially methods which vary the input features in order to study the (causal) effect on outputs. Some have indeed argued that ideal explanations in XAI are in fact causal explanations. "Explanations can be established by pointing to associations between the explanans and the explanandum [...]. Usually, however, the relationship we are interested in is causal, that is, the explanans makes a difference for the explanandum" <ref type="bibr" target="#b47">[FK23a]</ref>; also see <ref type="bibr" target="#b68">[HDR18,</ref><ref type="bibr" target="#b16">CBC23]</ref>. A crucial but easily overlooked distinction in the context of striving for causal explanations in XAI is the distinction between causal structure of 'just' the model vs that of the phenomenon in the world that the model is about. This point, which has important consequences for the purposes of interpretability, has been raised by many authors [Bec22, FK23a, FKMTC22, MCB20, Sul22b, Wat22, CBC23] and will be discussed in more detail in Section 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The issues</head><p>Despite the undoubtedly many useful methods and insights that the field of XAI has produced, there also are serious issues to point out -beyond just the fact that the field appears to lack a conceptually well-grounded framework and a consensus on the definition of interpretability. Two particularly clear and insightful pieces in this regard are the ones by Rudin <ref type="bibr" target="#b113">[Rud19]</ref> and Freiesleben et al. <ref type="bibr" target="#b47">[FK23a]</ref>, to which we refer readers for the details and a more comprehensive list of references, while here we only briefly mention those issues that are most pertinent to motivating this work's approach; see also [Lip18, Wat22, MHS17, MKH + 20, Cre20, Sul22a, DVK17].</p><p>Arbitrariness and falsehood of XAI explanations. Many authors have pointed out that post-hoc explanations may be untrustworthy, due to two related facts. Firstly, they are based on approximations to the original model which are inherently 'unfaithful', and secondly they are often non-unique and might even seem adjustable at will [Rud19, GDC + 23, FK23a]. Rudin argues that "[Post-hoc] explanations must be wrong. [...] If the explanation was completely faithful to what the original model computes, the explanation would equal the original model" <ref type="bibr" target="#b113">[Rud19]</ref>, noting that this would do away with the need for the original model in the first place, while an explainer who is correct only 90% of the time would be wrong in 10% of cases, which is unacceptable in certain high-stakes situations.</p><p>The myth of learning human concepts. The belief that deep neural networks must automatically learn the concepts which humans use to reason about the data and task, Freiesleben et al. argue, underlies many explanation techniques, but is one of which we should be sceptical. Indeed, why should a model learn human concepts when just optimised on a particular task like classification or sequence prediction? The underlying assumption of course is that the good performance that a model achieves on some task is only conceivable if it indeed uses our concepts and hence all one needs to do is "use XAI techniques like activation maximization or network dissection to discover/reveal which nodes in the network stand for which concept, and thentada -we have a fully transparent model where every part of the model stands for something" <ref type="bibr" target="#b47">[FK23a]</ref>. Crucially, they argue that while certain nodes in the network may co-activate when certain concepts are present (at least to some degree), they lack the causal role that the corresponding concept would have for us, and moreover point out that learning techniques such as drop-out would actually incentivise models to not learn concepts as localised in specific neurons.</p><p>The what vs the how. Rudin points out that typical methods like saliency maps may tell us what was relevant, but not how it was actually used by the model. "Knowing where the network is looking within the image does not tell the user what it is doing with that part of the image [...]. In fact, the saliency maps for multiple classes could be essentially the same; in that case, the explanation for why the image might contain a Siberian husky would be the same as the explanation for why the image might contain a transverse flute." <ref type="bibr" target="#b113">[Rud19]</ref> The inadequacy of probing a model. Perturbing data to then study the model's outputs on these additional inputs is very common in XAI, but involves applying the model to inputs outside the given data distribution (that is, outside the training and test data). Yet, guarantees for a model to correctly generalise to an out-of-distribution setting are notoriously hard to come by [SLB + 21a]. Conclusions about how a perturbation-based method apparently makes the model more transparent should be seen critically. Put yet more strongly: "In extrapolation regions, models disagree even when fitted to exactly the same data and achieve similar high performance on a test set. Asking an ML model to extrapolate is like asking a five-yearold kid who hasn't gone to school about her insights into algebraic topology. You might get an answer, but that answer will not really help you." <ref type="bibr" target="#b47">[FK23a]</ref> The accuracy-transparency trade-off, or myth thereof. A prevalent attitude consists in the belief that 'perfect' accuracy and interpretability are mutually exclusive and that one generally has to trade off one for the other to some degree. Rudin debunks this belief as a myth that has had much influence on the types of models considered and research undertaken <ref type="bibr" target="#b113">[Rud19]</ref>. While opaque models such as ones built from deep NNs may perform with higher accuracy on a larger class of tasks and domains, this does not imply the impossibility of vanilla transparency for an equally accurate model if one is willing to 'pay the cost' of finding models tailored to the specific problem at hand -costs being time, effort and possibly expert knowledge. Rudin finds that "particularly when the data are structured, with a good representation in terms of naturally meaningful features" there is little difference in performance between complex models and classic examples of intrinsically interpretable models. Crucially, many of the high-stakes decision problems, where interpretability arguably matters the most, are in fact such data science problems. Rudin lists a series of concrete examples, where interpretable models with state-of-the-art accuracy do exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Compositionality</head><p>The term 'compositionality' has appeared in various fields, but often has a distinct meaning and relates to distinct questions in each setting. Here we give a brief summary of various uses of the term, to clarify what compositionality as understood in this work does and does not refer to.</p><p>In linguistics and philosophy. There is a long tradition of studying notions of compositionality in linguistics and philosophy, including formal treatments as pioneered by Frege <ref type="bibr" target="#b54">[Fre92,</ref><ref type="bibr" target="#b55">Fre14]</ref> and Montague <ref type="bibr" target="#b98">[Mon70]</ref>. A focus lies on the principle of compositionality, which roughly states that "the meaning of a complex expression is determined by its structure and the meanings of its constituents" <ref type="bibr" target="#b133">[Sza22]</ref>. This principle aims to explain phenomena such as the productivity and systematicity of language, which allow us to understand sequences of words we have never encountered before. An important question concerns the interplay between this 'bottom-up' view of meaning determination and the contrasting 'top-down' view expressed by the so-called context principle.<ref type="foot" target="#foot_0">foot_0</ref> See <ref type="bibr" target="#b133">[Sza22]</ref> and <ref type="bibr" target="#b144">[WHM12]</ref> for encyclopaedic overviews of the field.</p><p>In cognitive science. The language of thought hypothesis (LOTH) is the proposal that our thought processes happen in a mental language, Mentalese, and plays a prominent role in cognitive science, going back to the influential works of Fodor [F + 75, Fod83, FP88] (in turn building on a long history in philosophy). Natural language and mental language differ in multiple ways; crucially, however, they also resemble each other in important ways according to LOTH. In particular, Mentalese features sentences and words with the meanings of sentences being built from word meanings in a systematic way. Indeed, Fodor argued that Mentalese is a compositional language which explains the compositional nature of thought processes. Assuming that mental events that represent mental (intentional) states are instantiated neurologically, one question to study, for instance, is how the compositional structure of the state is reflected in the instantiation -are there individuated instantiations of the separately meaningful components of a mental state, or thought process? See <ref type="bibr" target="#b110">[Res24]</ref> for an overview.</p><p>In NLP and AI more generally. Assuming that there indeed is a principle of compositionality behind human language use, and more generally behind human reasoning, there are questions concerning AI which have attracted much attention, especially in light of recent successes of deep NN-based NLP models. How is it that models generalise so well, despite not being systematic or rule-based systems? And can they genuinely generalise compositionally? [Bar20, Nef20, LB18] Some have argued that part of the difficulty in answering such questions is the lack of clarity on what 'generalising compositionally' should mean for modern, connectionist NLP models. This has led to a focus on the identification of datasets and compositional tasks that are designed to test a model's capability to generalise compositionally, based on the conviction that such a task requires a compositional approach to reach the correct output [SPW + 13, HDMB20, DLS + 24]. Broadly speaking, we can refer to the engagement of AI research with compositionality in this sense -studying model performance on compositional tasks -as the study of the compositional behaviour of models. This is conceptually distinct from questions of the 'modularity' of the model, i.e. the compositional structure of the model at an architectural or implementation level, which is closer to our focus here. However, some works have explored the design of a model's structure with a view to improving its compositional behaviour [SPW + 13, YC11, CvSS21, DBM + ng]. For an overview on the 'challenge of compositionality in AI' also see <ref type="bibr" target="#b96">[MM22]</ref>.</p><p>In category theory. Finally, our use of the term 'compositionality' is rooted in category theory, and especially the tradition of the Applied Category Theory (ACT) community. It refers to the formal study of compositional structure, that is, how systems and processes compose and how structural constraints for wellbehaved composition can guide abstract reasoning about them. This aproach can be applied in many contexts including theoretical computer science, fundamental physics, quantum computing, AI, and engineering. See <ref type="bibr" target="#b118">[Sel11,</ref><ref type="bibr" target="#b58">FS18,</ref><ref type="bibr" target="#b109">PZ23,</ref><ref type="bibr" target="#b31">Coe21a,</ref><ref type="bibr" target="#b13">BS11]</ref> for some introductory pieces, Section 2.3 for the works most closely related to ours, and Section 3 for the formal details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related work</head><p>Alongside the articles introducing the many AI models referenced throughout the text, there is a wide range of work related to our approach.</p><p>Compositional Intelligence. Broadly speaking, this work can be seen as part of the research programme of Compositional Intelligence, which explores applications of compositional models in (quantum) AI, and theories of intelligence more broadly. In particular, this work aims to make precise the common intuition that categorically structured models come with interpretability benefits over black-box models. Another aim is to build on the proposed definitions of 'meaningful' compositional structure put forward by Coecke <ref type="bibr" target="#b31">[Coe21a]</ref>.</p><p>Aside from interpretability, one could argue for further benefits of such models as a desired 'middle ground' between 'too strict' symbolic models and 'too loose' connectionist models, similarly to the aims of neuro-symbolic AI <ref type="bibr" target="#b72">[HS22]</ref>. In our case, symbolic aspects appear through explicit diagrammatic structure, and connectionist aspects through the implementation. These benefits could include robust reasoning and increased efficiency from training on smaller representational spaces, with the latter perhaps being beneficial for QML [MBS + 18].</p><p>Rodatz et al. [RLH + 24] develop an approach to defining and training ML models similarly to compositional models in our sense, in terms of generators whose behaviour is specified by string diagrammatic equations which constrain loss functions. These would also allow for the rewrite explanations in our sense, and we hope to connect with this approach in future work. Also related is a recent quantum implementation of the DisCoCirc framework in [DBM + ng], in which the authors discuss the compositional interpretability of the trained model, making use of diagrammatic axioms similar to those considered for our rewrite explanations.</p><p>Compositional models. Our notion of a compositional model is the standard one in categorical logic <ref type="bibr" target="#b84">[Law63a,</ref><ref type="bibr" target="#b85">Law63b]</ref>, and is used for example in DisCoCat <ref type="bibr" target="#b35">[CSC10]</ref>, and categorical treatments of causal models <ref type="bibr" target="#b51">[Fon13,</ref><ref type="bibr" target="#b76">JKZ19,</ref><ref type="bibr" target="#b90">LT23]</ref>. For those wanting a more formal treatment, we recommend Giovanni de Felice's DPhil thesis <ref type="bibr">[dF22]</ref>, which describes a number of the compositional models explored here, with a focus on NLP.</p><p>The discopy python library may be seen as a software implementation of compositional models <ref type="bibr" target="#b39">[dFTC20]</ref>, allowing for a range of semantics, such as neural networks or quantum circuits. The related lambeq toolkit [KFY + 21] handles the special case of quantum and tensor-network-based compositional models in NLP, discussed in Sections 5.7, 5.8 and 5.9.</p><p>While we focus on 'strictly' compositional (i.e. strong monoidal) models, recent work has explored graded notions based on obstructions to compositionality, and related properties such as emergence [PHGC23, BFG + 24] and it would be interesting to explore connections to this area in future work.</p><p>Categories for AI. The categorical perspective on ML has been developed in recent years by a number of authors, usually with a focus on viewing the training setups of models as 'bidirectional transformations', or lenses. These capture the forward pass of a trained model along with its backward 'parameter updating' direction; see for example <ref type="bibr" target="#b59">[FST19,</ref><ref type="bibr" target="#b122">SGW21]</ref>. Parameterised lenses have been used to formalise the general setup of gradient-based learning in [CGG + 22], and this is applied to models which produce 'explanations' for their outputs in [BFG + 23].</p><p>Closely related is the field of categorical cybernetics <ref type="bibr" target="#b22">[CGHR21,</ref><ref type="bibr" target="#b124">Smi21b]</ref> which observes that bidirectional transformations also capture probabilistic inference. This has even been used to make new proposals for the general forms of AI systems in [SNK + 22]. Here we focus on the (string diagrammatic) structure of a model as given, rather than the training process. Related ideas within the field of human cognition are theories of the Bayesian brain such as Predictive Processing and Active inference [PPF22, SFW22], with the latter having received various recent treatments in terms of category theory <ref type="bibr" target="#b123">[Smi21a,</ref><ref type="bibr" target="#b125">Smi22]</ref> and a related string diagrammatic treatment <ref type="bibr" target="#b135">[TKS23]</ref>.</p><p>Our account of causal models is based on the flourishing area of string diagrammatic approaches to probability theory <ref type="bibr" target="#b23">[CJ19,</ref><ref type="bibr" target="#b57">Fri20]</ref>, and causality <ref type="bibr" target="#b51">[Fon13,</ref><ref type="bibr" target="#b76">JKZ19,</ref><ref type="bibr" target="#b48">FK23b]</ref>, and in particular the string diagrammatic account of the Pearlian causal model framework in <ref type="bibr" target="#b90">[LT23]</ref>.</p><p>String diagrammatic formalisms for neural networks, and architectures such as transformers, have been presented informally and semi-formally [ENO + 21] while the upcoming article <ref type="bibr" target="#b79">[KLLWM24]</ref> aims to give a systematic diagrammatic view on a wide range of attention-based architectures. Another relatively structurally focused field of ML lies in geometric deep learning <ref type="bibr" target="#b5">[BBCV21]</ref>, which has recently been given a categorical generalisation [GLD + 24], and which we hope to connect to in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Categories and string diagrams</head><p>To describe compositionality formally, we will make use of the mathematics of category theory and its associated graphical language of string diagrams <ref type="bibr" target="#b28">[Coe06,</ref><ref type="bibr" target="#b109">PZ23,</ref><ref type="bibr" target="#b118">Sel11]</ref>. These diagrams provide a rigorous reasoning system which is often equivalent to working with traditional equations, but is more intuitive, allowing one to skip many mathematical details which are automatically accounted for by graphical rules.</p><p>A category C consists of a collection of objects A, B, . . . and morphisms f : A → B between them, which we can compose in sequence. Such a morphism is also referred to as a process f from input A to output B. We write C(A, B) for the collection of morphisms from A to B in C. In string diagrams an object A is depicted as a wire labelled by A, and a morphism f : A → B as a box with lower input wire A and upper output wire B, read from bottom to top.</p><formula xml:id="formula_2">f B A</formula><p>For any morphisms f : A → B and g : B → C whose respective outputs and inputs match, we can form the sequential composite g • f : A → C, depicted as follows.</p><formula xml:id="formula_3">g • f = A A C C f g B</formula><p>Each object A also comes with an identity morphism id A : A → A depicted as a plain wire:</p><formula xml:id="formula_4">id A = A A A A</formula><p>The identity acts as a unit for composition, with id</p><formula xml:id="formula_5">B • f = f = f • id A for any morphism f : A → B.</formula><p>We will work with categories with the following extra structure, allowing us to also compose processes 'side by side'. In formal terms, a symmetric monoidal category (SMC) is a category C coming with a functor ⊗ : C × C → C, distinguished object I and natural transformations which express that ⊗ is suitably associative and symmetric, with I as a unit <ref type="bibr" target="#b28">[Coe06]</ref>. Spelling out what this means is most naturally done in string diagrams, as follows.</p><p>Firstly, for any pair of objects A, B we can form their parallel composite or tensor A ⊗ B. In diagrams (the identity morphism on) A ⊗ B appears as a wire labelled by A alongside one labelled by B.</p><formula xml:id="formula_6">A ⊗ B = A B</formula><p>This allows us to have boxes with multiple inputs and outputs in diagrams. For example, a morphism</p><formula xml:id="formula_7">f : A ⊗ B ⊗ C → D ⊗ E is drawn as below. f A ⊗ B ⊗ C D ⊗ E = f A D C E B</formula><p>This parallel composition also extends to morphisms. Given morphisms f : A → C and g : B → D we can form their tensor f ⊗ g : A ⊗ B → C ⊗ D, depicted as below.</p><formula xml:id="formula_8">f ⊗ g A ⊗ B C ⊗ D = f A C g B D</formula><p>The tensor is symmetric meaning that we can also 'swap' pairs of wires past each other, such that swapping twice gives the identity (left-hand below), and boxes can move along the swaps (right-hand below).</p><formula xml:id="formula_9">A B = A B g f D A B C g f B C D A =</formula><p>In a monoidal category there is also a distinguished unit object I, which is depicted simply as 'empty space'.</p><formula xml:id="formula_10">I I =<label>(3)</label></formula><p>Intuitively, tensoring any object by I simply leaves it invariant. <ref type="foot" target="#foot_1">2</ref> The unit object lets us now have special kinds of morphisms 'without' inputs and/or outputs. A morphism ω : I → A is called a state of A, and is depicted with no input. An effect is a morphism e : A → I, depicted with no output.</p><formula xml:id="formula_11">ω A e A</formula><p>A scalar is a morphism r : I → I, depicted with no inputs or outputs. Scalars can move 'freely' around diagrams, and also can be multiplied together via r • s := r ⊗ s = r • s = s • r, i.e.:</p><formula xml:id="formula_12">r s = r • s = r s = s r</formula><p>We denote the 'empty space' scalar (3) by 1 := id I . By definition 1 • r = r for all scalars r. The composition operations in a category satisfy numerous axioms which we mostly omit here but which are self-evident in the graphical language. For example, in any category associativity of composition:</p><formula xml:id="formula_13">(h • g) • f = h • g • f = h • (g • f )</formula><p>is automatically apparent in string diagrams, with each expression given simply by drawing f , g, and h composed sequentially along a line. As another example, functoriality of ⊗ means that we always have:</p><formula xml:id="formula_14">(f ⊗ g) • (f ′ ⊗ g ′ ) = (f • f ′ ) ⊗ (g • g ′ )</formula><p>In diagrams this is automatic, shown left-hand below, with a special case being the right-hand 'interchange law' which lets us freely slide boxes along wires.</p><formula xml:id="formula_15">f g ′ f ′ g = f g ′ f ′ g f g = f g = f g</formula><p>The categories we consider in this article typically come with some extra structure allowing us to 'ignore' or 'throw away' objects.</p><p>Definition 1. <ref type="bibr" target="#b29">[Coe08,</ref><ref type="bibr" target="#b30">Coe14]</ref> A discard-category is an SMC in which every object comes with a chosen discarding morphism, depicted as a ground symbol:</p><p>A Moreover we require the choice of discarding morphisms to be 'natural' in that the following hold.</p><formula xml:id="formula_16">A ⊗ B = A B I = 1 (4) A morphism f : A → B is called a channel when it preserves discarding: 3 f = A B A</formula><p>In particular, we call a state ω normalised when the following holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ω = 1</head><p>3 Channels are also sometimes called 'causal' <ref type="bibr" target="#b30">[Coe14]</ref> or 'total' <ref type="bibr" target="#b24">[CJWW15]</ref>.</p><p>The channels in C form their own SMC C channel in which is now the unique effect on any object<ref type="foot" target="#foot_2">foot_2</ref> . A useful rule, which follows from (4), is that any channel with multiple inputs f satisfies: f . . . = . . . As we will see, categories with discarding provide a very general language for reasoning about composable processes of many kinds. One useful benefit is that discarding allows one to 'ignore' certain outputs of morphisms. Given any morphism f from A to B ⊗ C, its marginal A → B is the following morphism.</p><formula xml:id="formula_17">f B C A</formula><p>The final piece of extra structure we will use is the following ability to 'copy' information. This is present for most but not all categories we will consider, being a signature of 'classical' (deterministic or probabilistic) processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. [CJ19]</head><p>A cd-category ( copy-discard category) is a discard-category in which every object comes with a distinguished copying morphism depicted as: and satisfying the following:</p><formula xml:id="formula_18">= = = =</formula><p>Formally, this says that copying and discarding form a 'commutative comonoid'. The copying morphisms are moreover required to be 'natural' in that the following holds for all objects A, B.</p><formula xml:id="formula_19">A ⊗ B = A B<label>(5)</label></formula><p>A cd-category in which every morphism is a channel is called a Markov category <ref type="bibr" target="#b57">[Fri20]</ref>.</p><p>Let us now introduce our main example categories for this article. These include deterministic processes, as used in classical computation and neural networks, probabilistic processes as used in Bayesian and causal models, and quantum processes as used in quantum computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deterministic processes</head><p>Deterministic classical computation, as used in AI models based on neural networks, can be described by simply taking our processes to be functions, as follows.</p><p>In the cd-category Set the objects are sets X, Y , . . . and morphisms are functions f : X → Y . Composition g • f is the usual composition of functions, with id X being the identity map x → x. The monoidal structure is the Cartesian product with A ⊗ B = A × B and f ⊗ g = f × g, with I = {⋆} given by a singleton set. A state x : I → X of X can be equated with an element x ∈ X via x = x(⋆). Discarding on X is given by the unique function : X → I, and copying : X → X × X as expected by x → (x, x).</p><p>Set is a Markov category, so every morphism is a channel. Moreover every morphism f is in fact deterministic, which means that the following holds.</p><formula xml:id="formula_20">= f f f<label>(6)</label></formula><p>In particular every state x is deterministic, also called sharp, meaning it is copied by the copy morphism:</p><formula xml:id="formula_21">X X x = x x X X<label>(7)</label></formula><p>In diagrams we draw such sharp states as triangles, as above.</p><p>Variants of this category are useful in AI in allowing us to model neural networks. We define the category NN just like Set but with objects being only sets of the form V = R n for some n.<ref type="foot" target="#foot_3">foot_3</ref> Morphisms in NN are thus arbitrary (not necessarily linear) functions f : R n → R m . A state v of V corresponds to a vector v ∈ R n . The monoidal product is now often denoted V ⊕ W as it constitutes the direct sum of vector spaces, satisfying R n ⊕ R m = R n+m . More broadly, it can be useful to include discrete sets in this setup. To do so, we define the category NN * just like Set but with objects being sets of the form X 1 × • • • × X n , where each X j is either a finite discrete set or of the form R n for some n.</p><p>String diagrams in these categories can precisely capture the 'computational graphs' usually used to depict neural networks, as well as linear and rule based models, as we will see in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic processes</head><p>Recent work has demonstrated how cd-categories and Markov categories may be used to describe much of probability theory in string diagrams <ref type="bibr" target="#b23">[CJ19,</ref><ref type="bibr" target="#b57">Fri20]</ref>. In particular, these provide a diagrammatic account of Bayesian networks and causal models <ref type="bibr" target="#b51">[Fon13,</ref><ref type="bibr" target="#b23">CJ19,</ref><ref type="bibr" target="#b90">LT23]</ref>. For simplicity we will focus on probability with finite sets, but note that continuous probability theory can be treated in just the same way via suitable categories of 'Markov kernels' <ref type="bibr" target="#b57">[Fri20]</ref>.</p><p>In the cd-category Mat R + , the objects are finite sets X, Y , . . . and morphisms M : X → Y are functions M : Y × X → R + , where R + := {r ∈ R | r ≥ 0}. We think of such a morphism as a positive 'X × Y matrix', and following probabilistic notation denote its values by M (y</p><formula xml:id="formula_22">| x) := M (y, x). M Y X :: (y, x) → M (y | x)</formula><p>Sequential composition N • M is given by the matrix product, i.e. summation over internal wires.</p><formula xml:id="formula_23">M Z X N Y :: (z, x) → y∈Y N (z | y)M (y | x)</formula><p>The tensor ⊗ is again given on objects by X ⊗ Y = X × Y , and on morphisms now by the Kronecker (tensor) product of matrices.</p><formula xml:id="formula_24">M W X N Z Y :: (w, z, x, y) → M (w | x)N (z | y)</formula><p>The unit object is the singleton set I = {⋆}, so that states ω and effects e on X correspond to positive functions on X:</p><formula xml:id="formula_25">6 ω X :: x → ω(x) e X :: x → e(x)</formula><p>A scalar is precisely a positive real r ∈ R + , and composing scalars amounts to multiplication r⊗s = r•s = r•s in R + . The copy map on X is (y, z | x) = δ x,y,z with value 1 iff x = y = z and 0 otherwise. Discarding on X is given by the function with x → 1 for all x ∈ X.</p><p>A state ω is normalised precisely when it forms a normalised probability distribution over X, i.e. x∈X ω(x) = 1. More generally, M : X → Y is a channel precisely when it forms a probability channel, or equivalently the matrix is Stochastic, meaning that it sends each x ∈ X to a normalised distribution over Y :</p><formula xml:id="formula_26">M Y X a channel ⇐⇒ y∈Y M (y | x) = 1 ∀x ∈ X.</formula><p>The category of channels in Mat R + is the Markov category FStoch of finite Stochastic matrices, describing probability channels between finite sets. Note that in both Mat R + and FStoch most states are not deterministic, i.e. in general we have:</p><formula xml:id="formula_27">X X x ̸ = x x X X</formula><p>In fact a normalised state x is deterministic (sharp) precisely when it is given by a point distribution δ x for some x ∈ X. In general, deterministic morphisms f : X → Y in FStoch correspond precisely to functions</p><formula xml:id="formula_28">f ′ : X → Y , via f • x = f ′ (x)</formula><p>for each sharp state x. Hence we can see deterministic processes (on finite sets) as living 'inside' FStoch.</p><p>String diagrams in FStoch can describe many notions from probability theory. For example, given any state or general process with outputs Y , Z, the marginal on Y corresponds to taking the marginal in the usual sense, i.e. by summation over the set corresponding to the discarded wire:</p><formula xml:id="formula_29">M Y Z X :: (x, y) → z∈Z M (y, z | x)</formula><p>We will see more aspects of probability theory cast as cd-diagrams later; in particular in Section 5.11 we will see how they can capture the structure of Bayesian networks and causal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantum processes</head><p>The categorical approach allows us to consider very general kinds of processes, including those used in quantum computation. Here we briefly sketch the categorical treatment of quantum processes; for more details see <ref type="bibr" target="#b0">[AC04,</ref><ref type="bibr" target="#b25">CK18]</ref>.</p><p>In the category Quant the objects are finite-dimensional Hilbert spaces (equivalently, finite-dimensional complex vector spaces) H, K, . . . , with H ⊗ K given by the usual tensor product of vector spaces, and I = C being the one-dimensional space. We write L(H) for the space of linear operators on</p><formula xml:id="formula_30">H. A morphism H → K is a linear map f : L(H) → L(K)</formula><p>which is completely positive (CP). This means that f is positive, sending positive operators a ∈ L(H)that is, ones for which there is some b ∈ L(H) such that a = b † b -to positive operators f (a) in L(K), and moreover is such that each operator f ⊗ id H ′ is also positive, for any other Hilbert space H ′ . Here b † denotes the Hermitian conjugate of the operator b, i.e. the complex conjugate of the transpose.</p><p>In general, an effect e on H may be equated with a positive operator a ∈ L(H) via e(b) = Tr(ab), where Tr denotes the trace, with discarding given by the map : a → Tr(a) and corresponding to the identity operator 1. Similarly, a state ω of H may also be equated with a positive operator ρ ∈ L(H) on H via ρ = ω(1).</p><p>A normalised state is then precisely a density matrix (trace one positive operator), i.e. a quantum state:</p><formula xml:id="formula_31">ρ H is normalised ⇐⇒ ρ ∈ L(H) is a density matrix</formula><p>In general, a channel is precisely a CP trace-preserving (CPTP) map, the standard notion of a quantum operation. Such a channel maps density matrices on its input to density matrices on its output.</p><formula xml:id="formula_32">f K H is a channel ⇐⇒ f : L(H) → L(K) is a CPTP map</formula><p>Taking the marginal of a CP map is given by the partial trace operation.</p><formula xml:id="formula_33">f K L H = Tr L (f )</formula><p>Of particular interest are those CP maps f : L(H) → L(K) which are pure, meaning they are of the form f (a) = f • a • f † for some linear map f : H → K. 7 The normalised pure states are those of the form ψ = |ψ⟩ ⟨ψ|, where |ψ⟩ ∈ Hilb is a unit vector and ⟨ψ| the corresponding dual linear functional on H.</p><p>String diagrams in Quant can be used to describe the quantum circuits featured in quantum computation, including the preparation of states (density matrices), applications of unitary gates Û for a unitary matrix U : H → H, discarding of systems, and post-selective measurements given by effects, as depicted below. Each individual quantum wire typically corresponds to a 'qubit', i.e. 2-dimensional system H = C 2 . U V 0 0 0 T post-selection unitary pure states 7 Formally, the pure maps are the morphisms in the image of a monoidal 'embedding' FHilb → Quant given by f → f . This mapping equates any two linear maps only when they are equal up to global phase, i.e. f = ĝ ⇐⇒ f = e iθ • g for some θ ∈ [0, 2π).</p><p>The compositional perspective has been particularly fruitful for modelling quantum processes due to the inherently compositional nature of quantum circuits, and thanks to the existence of a complete graphical language for describing pure quantum processes, the ZX-calculus (and related ZW and ZXW calculi) [CD08, vdW20, PWS + 23, HNW18].</p><p>Remark 3. Full measurement setups, including the representation of classical measurement outcomes, can similarly be described in the broader category of finite-dimensional C*-algebras, which now includes both quantum and finite classical systems; for more details, see <ref type="bibr" target="#b25">[CK18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Compositional models</head><p>We now introduce the general notion of a compositional model, through which we will later understand AI models. Intuitively, such a model consists of an abstract 'structure', consisting of a collection of (usually learned) basic variables and processes between them, called 'generators', which we may then compose to describe further processes. The model then specifies semantics for this structure in a particular category, e.g that of neural networks, or quantum circuits. We will start by giving all the formal notions needed for a model at once, before illustrating them together in an example. Definition 4. A (monoidal) signature G consists of:</p><p>• a set G ob of 'objects', which we call variables;</p><p>• a set G mor of 'morphisms' f, g, . . . , called generators, whose inputs and outputs are lists of objects<ref type="foot" target="#foot_5">foot_5</ref> ;</p><p>• a set G eq of equations of the form D 1 = D 2 where D 1 , D 2 are string diagrams built from the generators.</p><p>Given a signature we depict its variables as wires, and generators as boxes with their input and output variables as input and output wires, just as for morphisms in a category. A generator with an empty list of inputs is again called a state and depicted with no input wires.</p><p>Typically a signature is thought of as a collection of basic objects and processes which we combine together to generate a category, as follows. Given some fixed class of string diagrams (e.g. monoidal or cd-diagrams), we write Free(G) for the category whose objects are lists (A i ) n i=1 of variables in G ob , and morphisms D : (A i ) n i=1 → (B j ) m j=1 are such string diagrams D built from G mor with inputs (A i ) n i=1 and outputs (B j ) m j=1 .<ref type="foot" target="#foot_6">foot_6</ref> We consider two diagrams D, D ′ equal as morphisms if one can be rewritten to the other in finitely many steps using equations in G eq and standard rules for such string diagrams.</p><p>We now reach our main definition.</p><p>Definition 5. A compositional model M = (G, C, -) consists of:</p><p>• a signature G, which induces a structure category S = Free(G);</p><p>• a semantics category C;</p><p>• a representation functor:</p><formula xml:id="formula_34">-: S → C</formula><p>For any object or morphism A, f in S we call A , f its semantics or representation in C.</p><p>In the above S and C are always categories of some explicitly specified type (e.g. monoidal or cd-categories) where S = Free(G) consists of all diagrams of this kind, and -is then a functor which preserves this structure (e.g. a monoidal or cd-functor).</p><p>The definition is the standard notion of model from categorical logic <ref type="bibr" target="#b84">[Law63a,</ref><ref type="bibr" target="#b85">Law63b]</ref>. We refer to such a model M as a (compositional) model of S in C. Often we don't write -explicitly but distinguish objects and morphisms in S from their representation in C by using different fonts, so that objects and morphisms A, f in S are mapped to A := A and f := f in C.</p><p>Since the structure category is freely built, specifying such a model simply amounts to specifying the representation of each variable and generator. That is, we specify an object A = A of C for each variable A ∈ G ob and a morphism f in C of the appropriate type, as below, for each generator f: f . . . . . .</p><formula xml:id="formula_35">B 1 Bm A 1 An → f . . . . . . B 1 Bm A 1 An -<label>(8)</label></formula><p>such that -preserves the equations in G eq . For a reader not familiar with above notion of a model from categorical logic, and with a view to AI models, the upshot of the separation between syntax and semantics is to be able to clearly distinguish between, and yet relate to each other, the high-level abstract structure of a model and its concrete instantiation or implementation. The benefits of this distinction will be illustrated through numerous examples in Sec. 5. Rather than labelling the wires and boxes abstractly as A, f, at times we may simply label them via their representations A = A , f = f in C, thus capturing the whole model (structure + semantics) at once in a single diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models from</head><p>The following toy example now illustrates together the notions of signature, model, and capturing a model in a diagram.</p><p>Example 6. Let C be a cd-category. Suppose that M is a compositional model in C, in the language of cd-categories, of the following form:</p><formula xml:id="formula_36">a A B b C E D c D (9)</formula><p>This means that the signature G consists of variables A, B, C, D, E and generators:</p><formula xml:id="formula_37">a A B b C E D D c D D , ,<label>(10)</label></formula><p>The structure category S = Free(G) consists of all cd-diagrams that can be built from these variables and generators, including for example the following diagrams as morphisms: Maps of signatures. Our representation functor can also be defined while referring only to the signature G, using the following notion which will be helpful in what follows. Given signatures G, G ′ , by a map of signatures F : G → G ′ we mean a map of variables F : G ob → G ′ ob along with a map F : G mor → G ′ mor which preserves types, i.e. of the form:</p><formula xml:id="formula_38">A A , b E D C , B B B , b E c a A B C c D D C . . . a A B b C E D c D ,<label>(11)</label></formula><formula xml:id="formula_39">f . . . . . . B 1 Bm A 1 An → F (f) . . . . . . F (B 1 ) F (Bm) F (A 1 ) F (An)</formula><p>and which also maps equations in G eq to equations in G ′ eq .<ref type="foot" target="#foot_7">foot_7</ref> As a special case, one can in fact view any monoidal category C as a signature with a variable for every object, generator for every morphism, and every valid equation in its equation set. Then a (strong monoidal) functor -: Free(G) → C as in a compositional model ( <ref type="formula" target="#formula_35">8</ref>) is the same as a map of signatures -: G → C.</p><p>We will also need to consider partial such mappings. We define a partial map of signatures F in the same way, only requiring the maps G ob → G ′ ob and G mor → G ′ mor to be partial functions, such that if f is a generator and F (f) is defined then so is F (V) whenever V is an input or output of f. Moreover, we no longer require any map on the equations of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interpretations of compositional models</head><p>Having introduced compositional models fully formally, in this section we will propose a notion of interpretability of a compositional model. This assigns meanings to (some of) the components and processes within a compositional model, from a collection of 'human-friendly' concepts.</p><p>Before doing so, we briefly introduce one more notion. To interpret processes in the semantics category C, one must know precisely what variables are being viewed as their inputs and outputs. To capture this, we define the category C G to have as objects lists of variables (A i ) n i=1 with A i ∈ G ob and with a morphism f :</p><formula xml:id="formula_40">(A i ) n i=1 → (B j ) m j=1 in C G for each morphism f : n i=1 A i → m j=1 B j in C.</formula><p>Composition operations are the same as in C.</p><p>Hence morphisms in C G are those of C but with their inputs and outputs indexed by variables from G ob . For example, states of variables A, B are regarded as states of distinct objects in C G , even if the variables' semantics coincide so that A = B in C.</p><p>We now reach our definition.</p><p>Definition 7 (Interpretation). An interpretation I = (H, I A , I C ) for a compositional model M = (G, C, -) consists of:</p><p>• a signature H of human-friendly terms;</p><p>• partial maps of signatures I A , the abstract interpretation, and I C , the concrete interpretation, such that the following diagram commutes:</p><formula xml:id="formula_41">11 G H C G - I A I C (12)</formula><p>We call the image in H of a variable V, generator f, or morphism g in C G its interpretation. We say a variable, generator or entire signature has an abstract interpretation when I A is defined on it. Finally, we will say that a variable V itself has a concrete interpretation when I C (ω :</p><formula xml:id="formula_42">I → V) is defined for every state ω of V in C.</formula><p>Let us now unpack this definition in detail, before making it more vivid in an example shortly. The definition is not fully formal in that we have not specified what makes a term 'human-friendly'. Formally, we have only asked that these are organised into a signature H. This simply means we can separate the terms into collections of interpretations H ob for variables and objects, and H mor for processes, and the above mappings respect these, explicitly taking the form:</p><formula xml:id="formula_43">f . . . . . . B 1 Bm A 1 An → I A (f) . . . . . . I A (B 1 ) I A (Bm) I A (A 1 ) I A (An) g . . . . . . B 1 Bm A 1 An → I C (g) . . . . . . I A (B 1 ) I A (Bm) I A (A 1 ) I A (An)<label>(13)</label></formula><p>The commutativity condition (12) means that the abstract and concrete interpretations coincide on variables and generators; that is, I C (V) = I A (V) for all variables V and I C ( f ) = I A (f) for all generators f, in each case whenever the former is defined. First, let us consider a model with an abstract interpretation I A : G → H only. This assigns terms from H to (some of) the variables and generators of the model. We can think of this as simply 'labelling' them in human-friendly terms. For example we may say that a variable V corresponds to 'brightness'. Note that an abstract interpretation depends only on the structure (G, S) of the model, being independent from its semantics (C, -). <ref type="foot" target="#foot_9">12</ref>It is common however to also wish to interpret additional parts of the semantics of a model. For example, given our 'brightness' variable with semantics V = R, we may wish to interpret each of its states as a degree of brightness, e.g. 0 → 'Dark', 1 → 'Bright' etc. This is the role of a concrete interpretation I C . Explicitly, given lists of variables such as A i and B j , this provides a partial map allowing us to interpret morphisms g in C from these inputs to outputs, as shown right-hand above. An important special case is that of states, where a variable V is said to have a concrete interpretation when we can assign an interpretation I C (ω) to every state ω of V .</p><p>The fact that I C is indexed by variables, and so defined on C G rather than simply on C, is essential as we must know what variables are being considered to apply an interpretation. For example, consider variables V 1 = V 2 = R interpreted as 'brightness' and 'size' respectively. A particular value r = 0.5 forms a state of R in C but can only be interpreted once a variable is chosen, i.e. as a state in C G of either V 1 (a brightness) or V 2 (a size).</p><p>Note that typically the mappings of an interpretation are only partial, in that some variables or generators may have a clear meaning, while others may not. We say a model has a complete interpretation when every variable and generator has an abstract interpretation, and a complete concrete interpretation when moreover every variable has a concrete interpretation.</p><p>Example 8. Consider a compositional model predicting how slippery the floor will typically be outside a house, depending on the season, generated by the diagram below. <ref type="foot" target="#foot_10">13</ref>Se</p><formula xml:id="formula_44">f Sp g R h W k Sl (14)</formula><p>Hence our signature contains variables Se, R, Sp,W, Sl and generators f, g, h, k as above.</p><p>Let us now equip this structure with the intended abstract interpretation. Our collection of human-friendly concepts H will have as its set of interpretations for objects H ob := {'Season', 'Rain', 'Sprinkler', 'Wetness', 'Slipperiness'}. The interpretation of variables I A : G ob → H ob acts as follows:</p><formula xml:id="formula_45">Se → 'Season' R → 'Rain' Sp → 'Sprinkler' W → 'Wetness' Sl → 'Slipperines'</formula><p>On generating morphisms, the mapping I A : G mor → H mor acts as below, where H mor consists precisely of all the morphisms on the right-handsides, with inputs and outputs as shown. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>→ induces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slipperiness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wetness</head><p>In each case the morphism in H has a label giving a short-hand for our understanding of the concept. For example ' avg. rainfall' reflects our interpretation of g as the mapping from seasons to their average rainfall.</p><p>In this scenario, we might expect to furthermore make use of a concrete interpretation. Suppose our semantics is in the category C = Set of sets and functions, with the specific semantics V = V for each variable V given by Se = {w, sp, su, a}, Sp = {0, 1}, R = W = Sl = R. The states of each variable in Set are then its elements as a set. Suppose we expand H to include states of 'Sprinkler' called 'on' and 'off '. A concrete interpretation for Sp could then be given by the mapping:</p><formula xml:id="formula_46">0 Sp → off Sprinkler 1 Sp → on</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sprinkler</head><p>Similarly, a concrete interpretation for Se could be given by mapping the states w, a, sp, su of Se to states of 'Season' in H called 'winter', 'autumn', 'spring' and 'summer', respectively.</p><p>To give a concrete interpretation of R, we could include states of 'Rain' in H called 'no rain' and 'r-mm' for each r &gt; 0, and then specify the following mapping from states of R (i.e. real numbers) to states in H:</p><formula xml:id="formula_47">r R →                    no rain Rain r ≤ 0 r mm Rain r &gt; 0</formula><p>Similarly, each state r ∈ R of W could denote the total mm of water on the ground, with r ≤ 0 denoting dry, while each state r ∈ [0, 1] of S could denote a degree of slipperiness, with all other states of S lacking an interpretation. Note that, even if we interpret all states of all variables, I C is still only a partial map of signatures. For example we have not defined interpretations for any other morphisms (i.e. functions) Se → Sp or any morphisms W → R.</p><p>Remark 9 (Specifying interpretations in practice). For clarity, we have in Example 8 gone to great lengths to spell out the interpretation explicitly. In practice, and in the remainder of this article, we typically specify it much more briefly. For example we may simply say 'the variables Se, R, Sp, W, Sl have concrete interpretations in terms of season, rain, sprinkler settings, wetness, and slipperiness, and f , g, h, k have interpretations as sprinkler activation, average rainfall, total moisture and inducing slipperiness, respectively'. Typically for brevity we will also not specify the signature H or mappings I explicitly, but only implicitly by directly using human-friendly concepts as names for generators and for morphisms in C. Thus we may instead describe the model with a diagram such as: Remark 10 (Functorial interpretations). From a category-theoretic perspective, the view of H as a signature may suggest that it should come with its own composition operations also, and thus form a (monoidal) category. Though we will not require this notion here, we can call an interpretation functorial when H itself forms a monoidal category and I C a (partial) monoidal functor. In this case, I A extends freely to a functor on S = Free(G) and the following is a commutative diagram of (partial) functors:</p><formula xml:id="formula_48">S H C G - I A I C</formula><p>This more richly structured form of interpretation is appealing categorically, and may be found to be the natural one from a compositional perspective, making it worth exploring in future work. However, we do not make this functorial notion of interpretation our default here since the majority of models relevant to XAI do not come with such a well-structured collection of terms, and so their H does not obviously form a category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Examples of compositional models</head><p>In this section we will describe numerous examples of our notion of compositional model, from across AI. This serves to illustrate the definitions of compositional model and interpretation from the previous section, and also demonstrate how familiar (and perhaps less familiar) models may be cast diagrammatically. Each model will be defined in terms of string diagrams, which implicitly specify the signature G and the language of string diagrams it is based on, from which the structure category S = Free(G) is in turn defined. For example, for most models the diagrams contain copying morphisms, meaning that S and C are cd-categories and -: S → C is a cd-functor. We emphasise that it is not essential for a reader to understand every example in detail. The reader is invited to pursue the list of examples from the Contents and choose those which interest them most. In Section 8 we will summarise our main conclusions from these examples regarding the role of compositionality in interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification models</head><p>As a simple example of a compositional model in our sense, let us consider a basic classification problem. Suppose we are given labelled data coming from a distribution Data(x, l) over inputs x ∈ X and labels l ∈ L. For simplicity assume X, L are both finite sets, so that Data forms a normalised state in FStoch. A classification model is then a compositional model of the Data in FStoch of the following form:</p><formula xml:id="formula_49">Data X L = X L f Data L X (15)</formula><p>Thus the model has variables X, L, and generators f, Data. By the choice of category f = f : X → L is a channel (and Data is the normalised distribution underlying the dataset). In standard notation, the above equation states that:</p><formula xml:id="formula_50">Data(x, l) = f (l | x) l ′ Data(x, l ′ )</formula><p>If each data point x has a unique label l = l(x) for which Data(x, l) is non-zero, then it follows that f (l | x) = δ l,l(x) , so that f chooses the correct label. We may then apply f to classify new inputs. In practice one is unlikely to have a perfect classifier, and so equation (15) may only hold approximately, with f given by the outcome of a training procedure.</p><p>Interpretation. In a classification model, typically the space of labels L is a (finite) set with a concrete interpretation, so that each label l ∈ L comes with a specific interpretation. The inputs X may either be given in terms of interpretable features X 1 , . . . X n , as in typical data science scenarios, or as an abstract latent space in a broader ML model, lacking a concrete interpretation. The generator Data is interpreted as the distribution underlying the dataset. The map f has a simple interpretation as mapping each input to its correct label, but without further decomposition of X and f we may not be able to interpret how this is done any further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Encoder-decoder models</head><p>For our next example, we consider the common form of model in which data from some input space is encoded via a latent space. By a strict encoder-decoder model we mean a compositional model M in the language of cd-categories, of the following form. The model features an input space variable X, a representation space variable Z, and the following generators:</p><formula xml:id="formula_51">d Z X e Z X σ Z Data X</formula><p>Here d is called the decoder, e the encoder, σ the prior over Z, and Data is equal to a given data distribution over inputs X. To ensure that these do behave as one would expect of an encoder and decoder, we include equations stating that all of these are channels, and that the following holds:</p><formula xml:id="formula_52">d σ Z X = e Data Z X<label>(16)</label></formula><p>This equation relates the distribution over X underlying the data to our prior distribution over Z, via the encoder and decoder. In fact it states precisely that the decoder forms the Bayesian inverse of the encoder with respect to this prior σ. In particular, given such a model we can sample from Z via the prior σ and apply the decoder, to give a distribution which generates new inputs from X:</p><formula xml:id="formula_53">σ d X Z</formula><p>Marginalizing out Z in the equation ( <ref type="formula" target="#formula_52">16</ref>) tells us that the above state is equal to Data , i.e. that this newly generated data will match the original distribution. If we include an extra equation specifying that d is deterministic, one may then prove graphically the desirable property that encoding and then decoding leaves all data points invariant, i.e. the following holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><formula xml:id="formula_54">X X = Data X X e d<label>(17)</label></formula><p>VAEs. Practical encoder-decoders such as the Variational Autoencoder (VAE) <ref type="bibr" target="#b82">[KW13]</ref> do not require equation ( <ref type="formula" target="#formula_52">16</ref>) to hold strictly, instead using a loss function to minimize some distance between both sides. 14 For a model in FStoch, let us define the structural loss L(V) as the Kullback-Leibler divergence D KL between the distributions over Z, X given by the LHS and RHS of (16). Then minimizing this loss would minimize D KL between their marginals on X, so that generating new inputs via d • σ returns points of X distributed closely to Data. In practice, this structural loss is impractical to compute directly, and so a VAE is instead trained by minimizing the ELBO loss defined by:</p><formula xml:id="formula_55">ELBO(V) := E x∼Data D KL (e(x), σ) -E z∼e(x) log d(x | z)<label>(18)</label></formula><p>Indeed one may prove directly that L(V) ≤ ELBO(V), so by minimizing the latter we can reduce L(V). For more details on VAEs and the ELBO loss see for instance <ref type="bibr" target="#b41">[Doe16]</ref>.</p><p>Encoder-decoder models can be extended in various ways, such as by including a collection of 'concepts' over the representation space, as in the Conceptual VAE <ref type="bibr" target="#b136">[TSZC24]</ref> outlined in Appendix A.7, and related to conceptual space models considered in Section 5.10.</p><p>Interpretation. Encoder-decoder models can appear within models of many kinds. A common case is where each data point in the space X has a concrete interpretation, for example, X = [0, 1] 3×n×m may be the space of images given by n × m many pixels represented as RGB values in [0, 1] 3 , and the data a collection of images. In contrast, Z is often an uninterpreted hidden state space used to compress the data. As a result the only interpretation for the encoder and decoder is simply to say that they are indeed an encoder and decoder, with their precise workings uninterpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Linear models</head><p>The simple example of a linear model is based on the use of specific morphisms available in the category C = NN. Any object V = R n in this category comes with the following distinguished morphisms:</p><formula xml:id="formula_56">+ V V V ⋆ R V V (<label>19</label></formula><formula xml:id="formula_57">)</formula><p>corresponding to addition and scalar multiplication respectively. As expected, addition is the function (v, w) → v + w and scalar multiplication is (r, v) → r • v. For any state r of R, the operation v → r • v of 'multiplication by r' is then given by:</p><formula xml:id="formula_58">⋆ r V V R r V V :=<label>(20)</label></formula><p>One may draw string diagrams expressing the usual relations satisfied by each of these operations, for example that r</p><formula xml:id="formula_59">• (v + w) = r • v + r • w. For more details, see Appendix A.1.</formula><p>Using the above structure we can now specify linear models as string diagrams. A linear model is a compositional model in C = NN of the form:</p><formula xml:id="formula_60">+ . . . w 1 w n b X 1 Xn Y R (<label>21</label></formula><formula xml:id="formula_61">)</formula><p>14 In a VAE it is also typically assumed that all channels (aside from the data distribution) take a particular Gaussian form.</p><p>where we assert that</p><formula xml:id="formula_62">X 1 = • • • = X n = Y = R</formula><p>, each box w j denotes multiplication by w j ∈ R as in (20) and + is the standard addition map above. Explicitly, this means the model has variables X 1 , . . . , X n , Y, (all with representation R) and generators given by the weights and bias:</p><formula xml:id="formula_63">w 1 X 1 w n Xn b Y . . .</formula><p>The model comes with a distinguished morphism given by the diagram above, which we denote (the representation of) by L(w, b). Concretely, as a function this maps each x = (x 1 , . . . , x n ) ∈ R n to the 'linear' (in fact affine) combination:</p><formula xml:id="formula_64">L(w, b)(x) = ⟨x, w⟩ + b with weights w = (w 1 , . . . , w n ) ∈ R n , bias b ∈ R and ⟨ , ⟩ the inner product in R n . 15</formula><p>Interpretation. While they could be applied to uninterpreted data, or within a broader model such as a neural network, a linear model used in isolation is usually given with a complete concrete interpretation. Indeed, linear models are a canonical example of an intrinsically interpretable model. Thus the variables X 1 , . . . , X n , Y typically come with abstract interpretations as to what they represent, as well as concrete interpretations for each of their possible values in R. Each generator w i has an interpretation as the 'strength of association' between variable X i and Y and the generator b as a bias within the space of outputs Y. The diagram (21) provides an internal view of the model showing all of the usually interpreted components, with the inputs, outputs, weights and biases each appearing.</p><p>In practice, however, note that a linear model is typically only asserted to be truly interpretable when it satisfies a sparsity requirement, meaning that only a 'small' number of weights w j are non-zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Rule-based models</head><p>Aside from linear models, the classic examples of intrinsically interpretable models are rule-based models, of which there are several variants, each admitting a natural diagrammatic description. These require the use of discrete sets, as well as continuous spaces R n , and so we work in the category NN * .</p><p>Within this category is the object corresponding to the set B = {1, 0} of Boolean truth values, understood as 'true' and 'false'. A rule on data X is then a function R : X → B, returning either true or false on each input. We can return a fixed output r ∈ R whenever the rule is 'true' by composing with the function also denoted r : B → R given by 0 → 0 and 1 → r, which in diagrams we denote as a box labelled r (similarly to (20)).</p><p>One can go on to define logical operations on rules, such as conjunctions and disjunctions, in diagrams also. For example, often we may define a rule as a conjunction of conditions c j , so that the rule is satisfied iff every condition is satisfied. Such a conjunction is depicted as follows: R . . .</p><formula xml:id="formula_65">B = c 1 c k B ∧ X 1 X k X 1 X k . . . B B</formula><p>Here ∧ is conjunction in B, returning 1 iff all inputs are 1.</p><p>Scoring systems. A scoring system is a model which, given inputs X 1 , . . . , X n , passes each to a number of rules R j , each of which maps to a fixed score value s j . These scores are then totalled to give the final output in Y . Equivalently, the final output is an s-weighted linear combination of outputs from each rule, where rule R j contributes s j to the sum whenever R j (x) = 1:</p><formula xml:id="formula_66">score(x) = j s j • R j (x)<label>(22)</label></formula><p>For example, in <ref type="bibr" target="#b113">[Rud19]</ref> the following example of a score-based model is discussed as an alternative to the proprietary COMPAS deep neural network in predicting subsequent convictions in the two years following prison release.<ref type="foot" target="#foot_12">foot_12</ref> </p><p>Prior Arrests ≥ 2 1 point Prior Arrests ≥ 5 1 point Prior Arrests for Local Ordinance 1 point Age at Release between 18 to 24 1 point Age at Release ≥ 40 -1 point Score = ... Writing variables P for the number of priors, L whether the subject has a local prior (1 or 0), and A for the age at time of release, we can represent this as a compositional model in Set of the following form:</p><formula xml:id="formula_67">P A ≥ 2 ≥ 5 18-24 ≥ 40 -1 + L Y 1 1 1 1 B B</formula><p>where each wire and rule box takes the semantics indicated above.</p><p>Decision lists. A closely related variant are decision lists, which are functions of the form 'if R 1 return s 1 else if R 2 return s 2 . . . ' where the R j are rules and s j their associated outputs. Such a model is defined just like a scoring system but replacing the final addition morphism with the function 'first' which returns the first of its inputs (x 1 , . . . , x n ) which is non-zero, and returns zero if all inputs are zero. Also in <ref type="bibr" target="#b113">[Rud19]</ref>   Decision trees. A final variant of rule-based models are decision trees. Each decision tree can be thought of as a finite series of yes/no questions, where each answer leads either to an output (represented by a leaf node in the tree) or to a further question. As an overall function, such a tree is a special case of a score-based system (22) with a rule R j for each of its leaf nodes, true whenever all the conditions in the unique path which leads to this leaf are true. These form a partition on the inputs, so that precisely one output s j contributes to the sum on any input. For example, consider the following decision tree, adapted from <ref type="bibr" target="#b97">[Mol20]</ref>, which determines an expected number of bike rentals from four possible output values o 1 , o 2 , o 3 , o 4 , given input data X consisting of the current temperature T and number of days D since 2011.</p><p>≤ 400 days since 2011? ≤ 100 days since 2011?</p><p>Temperature ≥ 11?</p><formula xml:id="formula_68">Y N Y N Y N o 1 o 2 o 3 o 4 X * + O<label>(25)</label></formula><p>The tree first asks whether it has been ≤ 400 days since 2011. If yes, it asks whether it has in fact been ≤ 100 days 2011, and if no asks whether the temperature is ≥ 11 degrees. Each of the four answer sets yields a disinct output o 1 , . . . , o 4 . Note that the intuitive diagram one would draw for such a tree corresponds precisely to (the lower portion of the) corresponding string diagram, read from bottom to top. At the top of the diagram, we then sum over the outputs from each leaf. As we will see, on any input only one leaf j is reached, with all other leafs contributing zero to the sum, yielding o j as the final output.</p><p>Let us now explain how such a string diagram can indeed be given a semantics which yields the inputoutput function of the decision tree. <ref type="foot" target="#foot_14">18</ref> In the diagram (25), Y , N are merely convenient labels (for 'yes', 'no'), with all wires below the output boxes o j on the same variable X * with X * = X × B, where X is the space of inputs. We view each question as a rule Q : X → B, and then each of the 'Q?' boxes above is of the left-hand form below, from one to two copies of X * , labelled Y and N . Here ¬ is the 'not' function which swaps 0 and 1.</p><formula xml:id="formula_69">Q? Y N := X Q B B X X X * ∧ ∧ ¬ B B o j R := o j R B X X * (26)</formula><p>Applied to an input (x, b) ∈ X * , the value of x is simply copied to both outputs. The Y output will carry true iff the 'control' input b is true and Q(x) is true, and N will carry true iff the b is true and Q(x) is false. If b is false both output Booleans will be false. Each output box o j is of the right-hand form above, returning its output o j if its Boolean input is true. By construction, one may verify that the overall diagram then computes the function on X represented by the decision tree.</p><p>It is clear that any form of decision tree can be represented by a string diagram in just the same way. In appendix A.2 we give an alternate string diagram for the tree which explicitly shows the dependencies of each condition on only a subset of inputs.</p><p>Interpretation. As for linear models, rule-based models are considered standard examples of intrinsically interpretable models. Rule-based models are typically applied where all of their input and output variables come with a concrete interpretation, so that we know what each variable, and each value of each variable, represents. Moreover each generator, i.e. each process in the diagram, has an interpretation as a simple logical function on these inputs. For example, in the diagram (24) for a decision list, each state of A is interpreted as a specific age, and we can concretely interpret the box '18-20' as simply asking whether an age falls within this range. Similarly the box '400 days since 2011' is understood to ask the question of whether for this input it has been 400 days since the year 2011, and the outputs interpreted as yes or no. We note that the components by which we would typically interpret a rule-based model (e.g. the rows in the table (23)) are the same components as in their string diagram (e.g. the boxes in (24)), and even the branching structure of a decision tree is apparent from its diagram (25).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Neural networks</head><p>A typical neuron may be described as a compositional model in NN given by composing the output of a linear model with a (usually non-linear) fixed activation function σ, taking the form:</p><formula xml:id="formula_70">+ σ . . . w 1 w n b . . . = Y Y R X 1 Xn X 1 Xn</formula><p>where as for linear models each wire is represented by R, and each w j box is a scalar multiplication by w j . This implements the function f (x) = σ( n i=1 w i x i + b) on inputs x ∈ R n , with generators given by the weights w ∈ R n , bias b and activation function σ.</p><p>A feed-forward neural network, also known as a multi-layer perceptron (MLP), is a compositional model in NN given by sequentially composing layers of such neurons, each layer defining a function R n → R m for some n, m, and with the generators of the compositional model given by the union of generators for all of its neurons, i.e. their collective set of weights, biases, and choices of activation function. Typically all neurons in a layer use the same activation function and so each such neuron is determined by its weights and bias. An entire neural network can be drawn as a single (typically huge) string diagram, in which a copy morphism passes the inputs of each layer to all of its neurons. These string diagrams are essentially equivalent to the computational graphs commonly used to describe neural networks.</p><p>In the examples below each grey dot is a single neuron, with its own weights and bias. The left-hand diagram shows the general shape of a many-layered network with n inputs and k outputs. For example, the lower input layer has n inputs and m outputs, with neurons a 1 , . . . , a m , where each a j encodes both the weights and bias for a neuron. Similarly, the upper layer has neurons b 1 , . . . , b k . The right-hand diagram shows a network with input size 3 and layers of size 3, 2, 3.</p><formula xml:id="formula_71">. . . . . . a 1 a m a 2 . . . . . . . . . b 1 b k b 2 X 1 X 2 Xn Y 1 Y 2 Y k d e a b c f g h Interpretation.</formula><p>Neural networks can vary in their degree of interpretability. Typically, the inputs to the network come with a concrete interpretation, related to the training data, as do the outputs of the final layer (e.g. as a classification or piece of generated data such as text). However, variables internal to the network typically lack any interpretation. Hence we can conclude that neural networks usually lack a (complete) interpretation.</p><p>Various XAI methods exist for analysing the internal components of neural networks to attempt to provide a given neuron or layer either with an abstract or concrete interpretation, but these methods have been subject to criticism (see Sections 2.1.2 and 7). Despite these issues, one may use a neural network to implement a more high-level model, whose variables (e.g. the inputs and outputs of the network) do come with an interpretation. An example would be a neural implementation of a conceptual space model, such as the conceptual VAE (see Section 5.10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Transformers</head><p>Similarly to neural networks, common architectures such as the transformer can be viewed as compositional models. For simplicity we will focus on the encoder of a transformer, but the same approach can be applied to the whole architecture from [VSP + 17]. For a more detailed diagrammatic account of the transformer see <ref type="bibr" target="#b79">[KLLWM24]</ref>, and also [ENO</p><formula xml:id="formula_72">+ 21].</formula><p>A transformer is a compositional model in NN which specifies, for each sentence length s ∈ N, a diagram of the following form.</p><formula xml:id="formula_73">L s E s X s h 1 h n + . . . MLP + feed forward attention heads positional embedding X s X s + pos +<label>(27)</label></formula><p>Here we write Y s (resp. f s ) as a short-hand for drawing s-many parallel copies of a wire Y (resp. process f ). Within such an encoder, an input sentence (w 1 , . . . , w s ) of length s consisting of word labels from w i ∈ L is converted to a vector in X s where X = R d for some d, by first applying an embedding map E and adding positional information, summing with n attention heads, and then adding to the output of an MLP. Furthermore, the model comes with variables Q, K, V represented as linear spaces Q = K = R d1 and V = R d2 , for some d 1 , d 2 such that each attention head h i takes the following form, for learned linear maps</p><formula xml:id="formula_74">q i , k i , v i , o i . h i X s X s = X s q s i k s i v s i o s i V s Q s V s K s X s • R s×s σ 1 √ d R s×s Mult (28)</formula><p>In the above, σ represents a softmax, 'Mult' represents the 'matrix multiplication' map and • the map which takes all pairs of inner products, using the fact that Q = K. We spell these out explicitly in Appendix A.1, where we also show how to represent each in elementary terms as diagrams built from our earlier addition and scalar multiplication maps (19). In summary, to actually specify a transformer model one must specify the set of labels L, (the dimensions of) spaces</p><formula xml:id="formula_75">X = R dx , Q = K = R d1 , V = R d2 and the linear generators (q i , k i , v i , o i ) n</formula><p>i=1 , as well as the generators E and MLP (where the latter is further specified by a feed-forward NN, decomposed as a compositional model as we saw for neural networks in section 5.5), and along with a specific choice of positional encoding vector pos on X s for each sentence length s. From these components one defines the heads h i as in (28) and for each sentence length s the encoder map in (27), with the meaning of all other components fixed.</p><p>Interpretation. Transformers face the same interpretability issues as neural networks, and their lack of interpretability underlies current safety issues with large language models (LLMs) based on them. Typically, for a transformer model as above, the labelled inputs L have a concrete interpretation, in that each word w ∈ L is understood to correspond to a specific word in natural language. <ref type="foot" target="#foot_15">19</ref> Beyond this, the individual spaces X, Q, K, V and q, k, v have no fixed meaning or interpretation by default. Nonetheless, as for neural networks, via post-hoc analysis one may hope to understand the meaning of each space to some extent. In this case one could provide, say, the space V of a particular head with an abstract (or concrete) interpretation, bringing the model closer to an interpretable one. For example, it has been argued that an individual attention head in an NLP model may at times pick up on certain grammatical features <ref type="bibr" target="#b69">[HM19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">RNNs and text sequence models</head><p>In NLP models, compositionality can arise in the composing of the meanings of words to produce meanings for a sentence or text. Perhaps the simplest form of such composition, used for example in Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b44">[Elm90]</ref>, is given by sequentially composing the meanings of words w 1 , . . . , w n to yield a composite meaning for the sequence.</p><p>Definition 11. Let Σ be a lexicon consisting of a set of words, e.g. Alice, Bob, plays, . . . . A sequence model of Σ is a compositional model<ref type="foot" target="#foot_16">foot_16</ref> with a single variable X, and for each word w ∈ Σ a generator:</p><formula xml:id="formula_76">w X X (29)</formula><p>For any such model, given a sequence of words (w 1 , . . . , w n ) we can define a morphism (in S) given by their sequential composite</p><formula xml:id="formula_77">w n • • • • • w 1 .</formula><p>In diagrams, such a sequence appears as below, with composition being simply sequential.</p><formula xml:id="formula_78">w 1 , . . . , w n X X = w 1 w n . . . X X (30)</formula><p>The model represents the sequence in C as a composition w n • • • • • w 1 of their representations w j = w j in the same way. Note that since diagrams are read bottom to top, we read the text sequence from bottom to top also. Associativity of composition in a category means that we are free (in principle) to parse a given sequence by parsing any sub-sequences and composing these together. For example, the phrase Alice says hello is represented equivalently by any of the following. This associativity is a characteristic property of sequence models, not common to all structured models in NLP.</p><p>Remark 12. Formally, we can consider sequence models to just use the language of plain categories, i.e. sequential composition only. The structure category S may be identified with Σ * , the free monoid generated by the symbols Σ. <ref type="foot" target="#foot_17">21</ref> Morphisms of S thus may be equated with finite sequences s = (w 1 , . . . , w n ) from Σ, with composition s 1 • s 2 given by concatenation of sequences, and with 1 = () being the empty sequence.</p><p>This simple form of model is well-studied in NLP, where the structure Σ * is also known as a sequence algebra and the model itself as a parser <ref type="bibr" target="#b11">[BL22]</ref>. Indeed a sequence model is equivalent to specifying an object X of C and a monoid homomorphism Σ * → C(X, X), with the latter being the usual definition of a parser. Such a morphism amounts to specifying a morphism on X for each w ∈ Σ, since the homomorphism property enforces that each (w 1 , . . . ,</p><formula xml:id="formula_79">w n ) is mapped to w n • • • • • w 1 , just as above.</formula><p>In practice, one often wishes to obtain a specific state (e.g. vector) for the meaning of a word or phrase, rather than a process. By a sequence model with an initial state we mean one with an additional generator given by a state ⋆ on X. For such a model, the state representation of a word sequence is defined as the following state:</p><formula xml:id="formula_80">w 1 , . . . , w n = ⋆ w 1 , . . . , w n<label>(31)</label></formula><p>Henceforth we assume all sequence models to come with such an initial state. Various forms of these models have appeared in the NLP literature, including the following well-known example.</p><p>RNNs. A sequence model implemented in the category C = NN of neural networks is called a Recurrent Neural Network (RNN) <ref type="bibr" target="#b44">[Elm90]</ref>. This means that X = R d for some dimension d, and the model sends each word w ∈ Σ to a typically non-linear function w : X → X. The space X is referred to as the space of 'hidden states' of the network.</p><p>Equivalently the whole RNN is often viewed as a single transition function f : X × Σ → X, where Σ is the discrete set of the lexicon, which we may view as a morphism in NN * . Each function f (-, w) represents the way a hidden state is updated by the word w. For any sequence the resulting update is given as below.</p><formula xml:id="formula_81">f w 1 Σ X 0 X 1 f w n Σ Xn . . . w 1 , . . . , w n = X X X n-1<label>(32)</label></formula><p>The representation of a word or sequence from an RNN is then defined by composing with a fixed initial state vector as in (31), such as ⋆ = 0.</p><p>Unitary RNNs. A related variant of sequence models is the Unitary RNN, which is argued to come with several interpretability benefits in <ref type="bibr" target="#b11">[BL22]</ref>. A Unitary RNN is a sequence model in which for each word w its morphism w on X = R d is linear, and moreover represented by an orthogonal matrix w, so that w T w = ww T = id X . Equivalently, it may be viewed as a sequence model in the category Orthog of orthogonal matrices. <ref type="foot" target="#foot_18">22</ref> Since orthogonal matrices are closed under composition, the process representation of each word sequence (32) is also given by an orthogonal matrix and so can be directly stored within R d×d . The space Orthog(d, d) of d-dimensional orthogonal matrices also has a linear structure (forming a real Hilbert space) which is argued to allow for ready analysis. In <ref type="bibr" target="#b11">[BL22]</ref> it is argued that this ability to efficiently store and analyse the representation of subsequences makes the unitary RNN inherently interpretable. The name 'unitary' in URNN refers to the fact that an orthogonal matrix is a special case of a unitary matrix between Hilbert spaces, and indeed we can consider replacing neural networks with a quantum implementation. We could define a quantum URNN to be a sequence model of Σ in the category Quant, such that each w = w is both a channel and an isomorphism, and hence unitary. Equivalently, it consists of a Hilbert space H and unitary U (s) on H for each s ∈ Σ. The model then specifies a unitary for any sequence via U (s 1 , . . . , </p><formula xml:id="formula_82">s n ) = U (s 1 ) • • • • • U (s n ). Quantum</formula><formula xml:id="formula_83">w 1 w n ⋆ . . . X * X X * X (a) w 1 w 2 g w n g . . . X (b) + w 1 w n . . .<label>(c)</label></formula><p>Firstly, (a) shows a cups reader, a model on X = R d where each process representation w is linear. Equivalently, this is a sequence model in the category C = FVec of finite-dimensional real-vector spaces. As in this diagram, a word w is however typically not treated as a morphism X → X, but instead equivalently as a state of X * ⊗ X, by applying 'caps' (which we return to in Section 5.8).</p><p>Next, (b) shows a stairs reader, a form of sequence model in which the representation is given by repeatedly composing the state representations of each word w with some (typically linear) stairs function g. In fact, such a model may be seen as a special case of a (linear) RNN; see Appendix A.3. Finally, a bag of words model is the special case of a stairs reader where g is simply given by vector addition. Then the representation of a word sequence is simply the vector sum w 1 + • • • + w n , shown in (c), and in particular is commutative, ignoring word order.</p><p>Interpretation. A sequence model forms perhaps the simplest example of an NLP model with interesting interpretable compositional structure. Here, the object X comes with only a (somewhat vague) abstract interpretation simply as a 'meaning' space, and ⋆ as an 'initial' or 'undetermined' meaning. Each generator for a word w however has a clearer abstract interpretation, namely as (the meaning of) that word. A diagram describing a text consists only of these interpretable components, making it an 'interpreted diagram' in the sense of Section 9.3, where we will see that this enables one to give interpreted (rewrite) explanations for the output of such a model.</p><p>Note that these benefits exist despite the absence of a concrete interpretation for the space X, which is typically a generic representation space. It is essentially argued in <ref type="bibr" target="#b11">[BL22]</ref> that those sequence models for which the space X is lower dimensional, and the processes w more efficiently representable, are more interpretable. An example is an URNN where the representation is given by an n × n (orthogonal) matrix, for a space X of dimension n. This however would only yield more interpretability in our sense when each dimension, and thus the space X, comes with a specific (concrete) interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">DisCoCat models</head><p>So far we have considered natural language simply as sequences of un-typed words. To provide a more meaningful form of composition we can introduce grammar, where words are given specific types (such as nouns or verbs) and composed via grammatical rules which determine, for example, whether a sentence is valid. A form of categorial grammar is provided by Lambek's notion of a 'pregroup' <ref type="bibr" target="#b83">[Lam08]</ref>. Compositional models of pregroup grammars form the heart of the DisCoCat formalism for (Q)NLP, which we now introduce.</p><p>Formally, a pregroup is a partially ordered set (G, ≤) with a compatible monoid structure (1, •) such that for each element x there is a distinguished left adjoint x l and right adjoint x r , satisfying the following:</p><formula xml:id="formula_84">x l • x ≤ 1 ≤ x • x l x • x r ≤ 1 ≤ x r • x (33)</formula><p>Suppose we have a set B of basic grammatical types, e.g. B = {n, s}, for 'nouns' and 'sentences', respectively. This generates a free pregroup G(B), whose elements are terms t such as n, s, n r • s • n l . . . . A pregroup grammar is then a lexicon Σ consisting of a set of words w : t, each with a given type t ∈ G(B), such as Alice : n, plays : n l • s • n r , or football : n. A sequence of words with types t 1 , . . . , t n is considered to yield a grammatical sentence when their resulting type t = t 1 • • • • • t n satisfies t ≤ s. For example Alice plays football yields a valid sentence since:</p><formula xml:id="formula_85">n • (n r • s • n l ) • n ≤ 1 • s • 1 = s (34)</formula><p>To give semantics to pregroups, we must consider their generalisation to a special form of category.<ref type="foot" target="#foot_19">foot_19</ref> A monoidal category C is rigid when every object A comes with a left adjoint A l and a right adjoint A r , each respectively equipped with distinguished states and effects depicted as cups and caps as below, satisfying the so-called 'yanking equations':</p><formula xml:id="formula_86">24 A A r A = A A A A l A =</formula><p>Any pregroup grammar Σ over a free pregroup G(B) generates a free rigid monoidal category Free Rigid (Σ) with G(B) as its objects. For types t, t ′ we have t ⊗ t ′ = t • t ′ , and for any type t ∈ G(B), the elements t l , t r form the left and right adjoints to t as expected. Along with these cups and caps, for each word, type pair w : t in Σ there is a generator given by a state w of t, such as:</p><formula xml:id="formula_87">plays Alice football s n n n r n l , ,<label>. . .</label></formula><p>Morphisms in this category can describe pregroup derivations of sentences (and other types). For example, a pregroup derivation of the sentence 'Alice plays football' would appear as:</p><formula xml:id="formula_88">plays Alice football s n n n r n l<label>(35)</label></formula><p>Any such diagram for a sentence can be read as a lower part consisting of the words, and an upper part encoding the grammatical structure, as a reduction of types t ≤ s in the pregroup (in this example, precisely the derivation (34)). Now we have seen how to describe pregroup grammars in terms of a free structure category, we can consider models which give semantics to these words and derivations. Explicitly then, the structure category contains diagrams such as (35). The model amounts to specifying in C a representation for each basic type, e.g. n = n, and a state w = w of t for each word of this type, e.g. a state likes of n r ⊗ s ⊗ n l . Since -is a strong monoidal functor it automatically preserves adjoint objects, cups and caps, and so the representation of a sentence diagram such as (35) is given by 'the same diagram' in C. <ref type="foot" target="#foot_21">25</ref>Example 14. A commonly used semantics category for DisCoCat models is the category C = FVec of finite-dimensional real vector spaces and linear maps f : V → W . Here ⊗ is the tensor product, with I = R the one-dimensional space. Each state v of V may be equated with a vector v = v(1) ∈ V .</p><p>This category is compact, meaning it is symmetric monoidal and that left and right adjoints coincide. For a space V we have that V l = V r = FVec(V , R), the dual space of V . Given a choice of basis {|i⟩} n i=1 for V we can further equate the dual space with V itself, thanks to finite dimensionality. Each cup is given by the state n i=1 |i⟩ ⊗ |i⟩ of V ⊗ V , while the caps are:</p><formula xml:id="formula_89">V V :: |i⟩ ⊗ |j⟩ → 1 i = j 0 otherwise</formula><p>In such a DisCoCat model, a word such as Plays corresponds to a vector of the tensor product n ⊗ s ⊗ n, also simply called a tensor. For example, the sentence (35) is mapped to a vector of s given by the tensor contraction:</p><formula xml:id="formula_90">|Alice plays football⟩ k = n i=1 n j=1 |Alice⟩ i |plays⟩ i,k,j |football⟩ j</formula><p>It is also common to take DisCoCat semantics in the compact category FHilb of finite-dimensional (complex) Hilbert spaces and linear maps f : H → K. Again ⊗ is the tensor product, now with I = C, and cups and caps are defined as above only now using the complex dual space of linear maps H → C. This latter category provides the semantics for quantum models of DisCoCat, as explored for example in [MTdFC23, LPM + 23].</p><p>Example 15. Further choices for the semantics category include compact categories of relations, such as the category ConvRel of convex sets and convex relations. In [BCG + 19] this is used to give a DisCoCat model of the conceptual spaces framework, which we return to in Example 18. Here each noun wire factorises in terms of domains such as colour or taste, and the meaning of a sentence is given by relational composition.</p><p>Various additions to DisCoCat models have been explored, including the modelling of relative pronouns through the use of Frobenius structures, which may be thought of as 'copying information' <ref type="bibr" target="#b115">[SCC13]</ref>. These satisfy various equations which allow for simplification of diagrams, such as: More recently, a higher-order variant of DisCoCat has been introduced, which uses a free cartesian closed (rather than merely rigid) category as both its structure and target category, which describes higher-order operations on diagrams in the language of closed categories <ref type="bibr" target="#b134">[TdF23]</ref>.</p><p>Interpretation. In a DisCoCat model, the variables t for each pregroup type have an abstract interpretation as representing that type; for example n corresponds to nouns. Similarly the generator w for each word has an abstract interpretation as representing (the meaning of) that word. Since the 'caps' encode grammatical relationships, we can interpret every component of a diagram for a sentence such as (35) (making it an interpreted diagram in the sense of Section 9.3), which we can say has an interpretation itself as the meaning of the sentence, with this specific grammatical structure. The fact that we can see how the meaning of a sentence is constructed grammatically, may provide additional explanatory benefits. For example, the sentence in (36) using Frobenius structures reduces to the readily inspectable composite form on the RHS. Specific DisCoCat models may have their interpretability enhanced by furthermore coming with concrete interpretations; for example the conceptual space models in Example 15 are such that each noun space n has a concrete interpretation via a conceptual space factored in terms of colour, taste and texture.</p><p>CCG Models. Beyond pregroup grammars, one may define compositional models based on further forms of categorial grammar, notably Combinatory Categorial Grammar (CCG) <ref type="bibr" target="#b128">[Ste00]</ref>. We describe CCG models in detail in Appendix A.4. In fact they are closely related to DisCoCat models; see Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">DisCoCirc models</head><p>An alternative form of compositional structure in NLP is offered by the more recently developed DisCoCirc models, originally from <ref type="bibr" target="#b32">[Coe21b]</ref> and treated more fully in <ref type="bibr" target="#b145">[WMLC23]</ref>. In this approach, the semantic space now decomposes in terms of multiple wires corresponding to the relevant nouns or discourse referents in a text <ref type="bibr" target="#b80">[KR93]</ref>. The text itself, including its grammatical structure, is encoded in the shape of a string diagram describing a process on these wires. For example, a sentence such as Alice laughs and sees Bob who likes Charlie could be modelled as a text circuit of the following form, acting on wires representing Alice, Bob and Charlie.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sees likes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alice</head><p>Bob Charlie</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>laughs</head><p>The DisCoCirc framework allows for a still richer class of string diagrams encoding more features of language and grammatical types. In particular it makes use of 'higher order' processes which we can depict as a 'box with a hole' into which we may insert a diagram to be modified. An example is when a verb is modified by inserting it into a higher order box representing an adverb. This allows us to produce diagrams known as text circuits, such as the following representation of the sentence Alice who is sober, sees Bob, who is drunk, dance clumsily and laughs at him (from <ref type="bibr" target="#b145">[WMLC23]</ref>).</p><p>Alice Bob sober drunk sees clumsily dance laughs at (37)</p><p>These diagrams naturally allow the treatment of entire texts, rather than single sentences, and require only the structure of a monoidal category, on which such 'higher order processes' may be defined.</p><p>Text circuits. Let us now introduce text circuits in more detail. Let Σ be a set of typed words, where the pair of a word w with a type t is denoted w : t. We will now list the small set of example types from <ref type="bibr" target="#b145">[WMLC23]</ref>, motivated by the types from a CCG grammar, along with their graphical representation. <ref type="foot" target="#foot_22">26</ref> A text circuit is a string diagram with n input wires and n output wires, which ultimately will each be labelled with distinct discourse referents from the text. Adjectives (e.g. drunk : ADJ) and intransitive verbs (e.g. laughs : IV) appear as boxes modifying a single noun wire, while transitive verbs act on two wires representing their subject and object. Thus for each adjective, intransitive verb or transitive verb in Σ we have a graphical box of the corresponding form below.</p><formula xml:id="formula_91">ADJ IV TV<label>(38)</label></formula><p>Adverbs (e.g. quickly : ADV), and other higher order types, are represented graphically as boxes with a hole into which we can insert a text circuit of the appropriate shape, to build a new text circuit, for example:</p><formula xml:id="formula_92">run → run quickly</formula><p>The various word types represented as higher-order maps are shown in (39). For each adverb in Σ we have a box for each of the two forms shown, which act on intransitive or transitive verbs respectively. Next, for each adposition (e.g. at : ADP) we have a box as shown, with a hole into which a verb is inserted and which returns a verb along with an additional noun.</p><p>Next are verbs with a sentential complement. For each verb v : SC.V, we have a generator as depicted below. For example, for sees : SC.V we insert a text circuit for the sentence of what is seen, introducing an extra noun wire, as in Alice sees Bob dance. Finally, conjunctions allow us to combine pieces of text which overlap in some fragment of their nouns. For each conjunction we have an infinite family of generators of text circuits of the right-hand form below, for each possible circuit shape of its two arguments. </p><p>Now for each n ∈ N let TextCirc Σ (n) denote the collection of string diagrams that can be built in finitely many steps by composing and inserting these generators into holes, leaving no open holes remaining. Text circuits can be composed in sequence and parallel and so these form the morphisms n → n of an SMC TextCirc Σ .</p><p>We formalise this notion of 'box with holes' more thoroughly in Appendix A.6. Roughly, by a 'higher order morphism' on a category we mean a function which produces a new morphism from a set of input morphisms, each of fixed types. By a h.o.-category we mean a monoidal category C with a distinguished set H C of higher order morphisms. As expected, TextCirc Σ forms a h.o.-category with the generators (39) as its distinguished higher order morphisms. Finally, a 'h.o.-functor' is a monoidal functor F : C → D between h.o.-categories along with a compatible mapping H F : H C → H D between their higher order morphisms.</p><formula xml:id="formula_94">Definition 16. A DisCoCirc model of lexicon Σ is a compositional model of S = TextCirc Σ in a h.o.- category C, where -is a h.o.-functor.</formula><p>Equivalently, a DisCoCirc model amounts to specifying a single object n = n in C (for 'noun') along with morphisms as in (38) for each word w of type ADJ, IV, TV, and higher order morphisms as in (39) on C for each possible hole shape and each word w of type ADV, ADP, SC.V or CNJ, as appropriate.</p><p>Remark 17 (Labelled text circuits). The wires in actual text circuits should ultimately be labelled with nouns (or noun phrases) corresponding to the discourse referents in a text. Given a set of possible labels L, e.g. Alice, Bob, Charlie, . . . , we write TextCirc Σ,L for the family of labelled text circuits, where now each wire is given a distinct label from L, such that the inputs and outputs to every circuit are identical (and by convention aligned) as in (37). Any DisCoCirc model of Σ extends, by simply 'forgetting' wire labels, to give a h.o-functor TextCirc Σ,L → C, which we may call a labelled DisCoCirc model. By construction, the semantics it provides to any text circuit does not depend on the choice of labels on its wires.</p><p>We will meet several examples of (labelled) text circuits in DisCoCirc in the context of explainability later in Section 9.</p><p>Implementations. The theory of text circuits requires the semantics category C to only be a symmetric monoidal category, allowing us to define DisCoCirc models very broadly. In particular, the same text circuit could be implemented either via neural networks, or quantum circuits, as depicted in Figure <ref type="figure" target="#fig_5">6</ref>. Taking C = NN gives neural DisCoCirc models, in which a wire such as Alice is represented by a space R n and each basic gate such as plays by a neural network. Taking C = Quant instead gives quantum DisCoCirc models, in which the wires are quantum systems, such as collections of qubits, and the basic gates are CP maps (e.g. unitary gates). In [DBM + ng] such a quantum DisCoCirc model is trained, with its interpretabiliy benefits discussed, and it is shown that the resulting quantum text circuits are hard to simulate classically.</p><p>Interpretation. DisCoCirc models come with a natural abstract interpretation for their components. For any labelled DisCoCirc model, there is a variable for each possible wire label, corresponding to a noun representing this discourse referent in the text. This variable has an abstract interpretation as representing the meaning state of this noun. The (higher order) generator morphism for each typed word w has an abstract interpretation as the meaning of that word, or rather the process of how it updates the meanings of its input words.</p><p>As well as this, DisCoCirc models come with a rich compositional structure from the nature in which text circuits are constructed, combined with the fact that each of their components has an abstract interpretation as described above. In Section 9 we will see that this provides various explainability benefits, without requiring a concrete interpretation. A specific DisCoCirc model may come with further interpretability still by giving its noun space a concrete interpretation; for example by making it an interpreted conceptual space similarly to Example 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Conceptual space models</head><p>Our next form of model takes its inspiration from cognitive science. Gärdenfors' conceptual spaces provide a framework for cognition in humans and AI, in which a cognitive space is viewed as a convex space <ref type="bibr" target="#b60">[Gar04,</ref><ref type="bibr" target="#b61">Gar14]</ref>, which factorises in terms of elementary simpler spaces known as domains, such as colour, taste, or sound. Each such space (or domain) comes with a collection of concepts, such as red, salty, or loud, modelled as convex subsets. Various formalisations and generalisations of the approach have been given, including compositional treatments based on string diagrams in [BCG + 19, Tul21, TSZC24], which we give a simplied view of here. <ref type="foot" target="#foot_23">27</ref>A conceptual space model is a compositional model with a variable Z, called the conceptual space, and variables Z 1 , . . . , Z n called the domains, along with the equation</p><formula xml:id="formula_95">Z = Z 1 ⊗ • • • ⊗ Z n .</formula><p>Additionally, it comes with a collection of generators, called the concepts, of the form:</p><formula xml:id="formula_96">C dom(C) . . . (<label>40</label></formula><formula xml:id="formula_97">)</formula><p>where dom(C) ⊆ {Z 1 , . . . , Z n } is a subset of domains. For example, a domain Z j interpreted as colour may have a concept red; more generally a concept can be defined over multiple domains. We can extend any such concept to the whole of Z by discarding the remaining unused domains. More generally, given any collection (C 1 , . . . , C k ) of concepts for which the subsets dom(C i ) are all disjoint, we define a product concept C = (C 1 , . . . , C k ) over all of Z as follows:</p><formula xml:id="formula_98">C C 1 dom(C 1 ) dom(C k ) . . . := C k Z 1 Zn Z ′ . . .<label>(41)</label></formula><p>where Z ′ consists of all remaining domains. For example, we may have a conceptual space Z for images of simple 2d shapes, with domains colour, size, shape, position. Given concepts red, large, square on the first three domains, we can define a concept large red square via their product (applying discarding to the unused position domain).</p><p>In addition to these generators, any specific conceptual space model should come with a distinguished collection of normalised states ψ ∈ Inst(Z i ) on each domain Z i = Z i , called the instances of the domain [CLvG + 21]. We then define an instance ψ ∈ Inst(Z) of the overall space Z to be a product of instances ψ i ∈ Inst(Z i ) on each factor, i.e. a state of the form:</p><formula xml:id="formula_99">ψ Z 1 Zn . . . = ψ 1 Z 1 Zn . . . ψ n</formula><p>Given any such instance, and any concept C on Z, we can compose its representation with the instance to yield a scalar C • ψ in C, which we think of as representing 'how well' the instance fits the concept:</p><formula xml:id="formula_100">C ψ Z<label>(42)</label></formula><p>Depending on the semantics category, this scalar can take values in {0, 1}, for simply 'yes' or 'no', or [0, 1] or R + to model 'fuzzy' or 'graded' concepts.</p><p>Beyond this simple treatment here, one can imagine enriching the setup further to include processes relating domains and representing conceptual reasoning, as suggested in <ref type="bibr" target="#b137">[Tul21]</ref>.  Instances on these domains correspond simply to elements of their underlying sets; for example there is an instance of colour for each RGB value in the colour cube. Thus an instance of a food space is given by a triple of a particular taste, colour, and texture. The scalar obtained by applying a concept to an instance is 1 if the instance belongs to the subset or 0 otherwise.</p><p>In <ref type="bibr" target="#b137">[Tul21]</ref> an alternative semantics is given which allows fuzzy concepts C : Z → [0, 1] which now map instances to values in [0, 1]. For this we use the category C = ConSp of (measurable) convex spaces and log-concave probability channels. In particular, concepts (40) are now log-concave measurable functions Z → [0, 1].</p><p>Finally, in <ref type="bibr" target="#b136">[TSZC24]</ref> quantum conceptual models are explored, by taking semantics in the category C = Quant of quantum processes. Now each domain is associated with a Hilbert space Z i = H i , and a conceptual space with their tensor product H 1 ⊗• • •⊗H n . Instances are defined as non-entangled pure states</p><formula xml:id="formula_101">ψ 1 ⊗• • •⊗ψ n . By definition concepts are then positive operators C on H, with C • ψ = Tr(C |ψ 1 ⟩ ⊗ • • • ⊗ |ψ n ⟩).</formula><p>Interpretation. Conceptual space models are typically applied with a clear abstract interpretation for each of their components. Each Z i is abstractly interpreted as a given cognitive domain (e.g. colour), and the overall space Z as a semantic space related to some class of entity (e.g. food). Each generator C is interpreted as a concept related to the domains on which it is relevant. These interpretations of concepts allow them to be related to training data, by labelling data points with appropriate concepts. Indeed training a conceptual space model on a latent space Z can be seen as a way to partially interpret it; even if each point of Z has no direct interpretation, we can understand it to some extent by saying how well it applies to each (interpreted) concept. Equipping latent spaces with conceptual structure in this way can make them more interpretable, a benefit of (classical and quantum) approaches such as in <ref type="bibr" target="#b136">[TSZC24]</ref>. However, in conceptual space models from cognitive science each domain usually does come with a concrete interpretation, allowing us to interpret each of its instances also. For example each instance of the domain colour would be interpreted as a specific colour value.</p><p>Applying conceptual spaces to data. Using conceptual space models in practice requires the training of an encoder e : X → Z from a space of inputs X into a latent conceptual space Z, along with a set L of concept labels applied to training data. A quantum version of this setup is explored in <ref type="bibr" target="#b136">[TSZC24]</ref>. Additionally, a decoder d : Z → X can allow concepts to be used as states on Z to generate new inputs in X, as in the Conceptual VAE also defined in <ref type="bibr" target="#b136">[TSZC24]</ref>. For details of such conceptual encoder-decoder models, see Appendix A.7.</p><p>Disentangled representations. The factorisation of a conceptual space as a product of domains is closely related to the notion of a 'disentangled representation' in structured ML, as formalised in [HAP + 18]. We discuss such representations as compositional models in Appendix A.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Causal models</head><p>One class of model that plays an important role both in AI and interpretability is the class of causal models. The framework of causal models has been developed over the past four decades, originally with a perspective on AI that predates ML and partly comes from the philosophy of causation. A main goal has been to devise a framework that formalises and guides causal reasoning in a principled and scientific way, delineating it from mere Bayesian reasoning with statistical models. Pioneered by Pearl and his collaborators [Pea09], as well as Spirtes, Glymour and Scheines <ref type="bibr" target="#b121">[SGS00]</ref>, there is now a rich framework and a vast literature.</p><p>In recent years, it has also gained traction in NN-based ML and a number of authors have argued that the traditional statistical models learned by AI are not sufficient to carry out meaningful 'human-like' reasoning, necessitating ML models equipped with explicit causal structure. Such causal ML systems are argued to gain robustness to adversarial attacks, greater generalisation skills and more efficient training, as well as increased interpretability [Sch22, HAP + 18]. In fact, concepts and terminology from the causal model literature feature prominently in standard XAI methods.</p><p>Later in Section 7 we will discuss the various roles causality has to play in interpretability. Here we will first introduce the formal notion of a causal model as a compositional model, and give a short overview of the diagrammatic account of the causal model framework from <ref type="bibr" target="#b90">[LT23]</ref>, which builds on the previous works <ref type="bibr" target="#b51">[Fon13,</ref><ref type="bibr" target="#b76">JKZ19,</ref><ref type="bibr" target="#b48">FK23b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11.1">Causal Bayesian networks</head><p>Causal models are typically introduced through the notion of a Causal Bayesian Network (CBN). This consists of a finite set of variables V = {X 1 , . . . X n }, which form the vertices of a Directed Acyclic Graph (DAG) G representing the direct-cause relations. Each variable X i takes a (finite) set of values X i and is associated with a probability channel P (X i | Pa(X i )), its causal mechanism, describing the probability distributions of its values given those of its parents Pa(X i ) in G. The family of mechanisms induce a joint distribution over V , which, by construction, satisfies the causal Markov condition: <ref type="foot" target="#foot_24">28</ref> In addition, it is common for only a subset O ⊆ V of the variables in a model to be considered as outputs, which we think of as 'observed' variables, with the remaining variables referred to as 'hidden' or 'latent'.</p><formula xml:id="formula_102">P (V ) = n i=1 P (X i | Pa(X i )).</formula><p>Example 19. Let S be a person's choice to smoke, L whether or not they develop lung cancer, A their age and B a set of relevant background conditions like socio-economic status, education etc. A plausible causal model consists of the DAG below, where the vertices corresponding to output variables are circled, along with the specification of each of the probability channels listed to the right, which give the mechanisms. . . . along with labellings on the wires, such that any wires not connected by a sequence of copy maps are given distinct labels, and each label appears as an output at most once and as an input to any given box at most once.</p><p>Examples of such diagrams will follow shortly in Ex. 22 and 23. For G a DAG with vertices V and O ⊆ V a specified subset of output vertices, the pair (G, O) is equivalent to a network diagram over wires V with outputs O, and no inputs (see Ex. 22). The correspondence works as follows: each vertex X ∈ V is associated with a single-output box with |Pa(X)| many input wires and an output wire labeled by X. This wire is followed by a copy map that connects it with the boxes associated with X's children Ch(X), and which also copies the wire one more time if it is an output, X ∈ O. More broadly, a general network diagram, which may have inputs, is equivalent to an open DAG (G, I, O), which now may come with a subset I ⊆ V of inputs, such that each input has no parents in the DAG.</p><p>While a network diagram thus specifies a causal structure, an actual causal model also specifies causal mechanisms, by modelling the diagram in some suitable cd-category C, as follows. in C is given by specifying objects X 1 , . . . , X 5 along with channels a, b, c of the corresponding type.</p><formula xml:id="formula_103">X 1 X 4 X 2 X 3 X 5 ⇐⇒ X 1 X 2 X 3 X 5 a b c X 3 X 4</formula><p>Open causal models with inputs, while not as common in the traditional causal model literature, arise naturally when studying neural networks, as well as interventions on models, as we will see later.</p><p>Interventions. A CBN is in particular a Bayesian network, encoding conditional independence relations in the probability distribution over all variables (see Sec. 6 for more details). What distinguishes a causal Bayesian network, and causal models more generally, are interventions. An intervention is thought of as a 'reaching in' that changes the causal influences by altering some of the mechanisms of a model. The ability to do so is justified by the assertion that the channels c X : Pa(X) → X are causal mechanisms, and so in particular have the property of 'autonomy'. Here we briefly sketch the string diagrammatic view of interventions, referring the reader to <ref type="bibr" target="#b90">[LT23]</ref> for more details.</p><p>The simplest kind of intervention most commonly discussed is that of a do-intervention. A do-intervention do(X = x) replaces the mechanism for X in the given (open) causal model M with a fixed sharp state x on X, representing a 'reaching in' and forcing the variable to take a certain value independent of what the direct causes of X in M are. <ref type="foot" target="#foot_25">29</ref> In compositional model terms, it thus is a meta-operation that alters the signature of the model M by replacing the mechanism c X for X with a mechanism of the right-hand form below, such that</p><formula xml:id="formula_104">x = x in C. c X X Pa(X) . . . → do(X = x) x X<label>(44)</label></formula><p>Suppose M has input variables I and output variables O. Common notation is to write P (O|I) for its induced channel M . Then do(X = x) induces a causal model M ′ , whose induced channel is now commonly denoted P (O|I; do(X = x)).</p><p>Example 24. Consider the causal model from Ex. 22. For an intervention do(S = s) -say, forcing someone to (not) smoke -the transition between models yields the following channel P (S, L, A; do(S = s)) in C:</p><formula xml:id="formula_105">P (S, L, A; do(S = s)) L = A S c B c A A c L S L s = c B c A A c L S L s s</formula><p>Various more general kinds of interventions have been considered in the literature such as soft and conditional interventions. They all fall under the following most general notion, which was first proposed in <ref type="bibr" target="#b15">[CB20]</ref> and then studied in string diagrammatic language in <ref type="bibr" target="#b90">[LT23]</ref>. An intervention σ on variable X of (open) causal model M replaces its mechanism c X with any other channel c ′ X , subject to the condition of preserving acyclicity of the overall causal structure of the resultant new model σ(M). where the channel η may be deduced from the above expression for c ′ S . This intervention yields a new model with the following output state:</p><formula xml:id="formula_106">c B c A A c ′ S c L S L = c B c A A η S c L S L c S P (S, L, A; σ) S A L := B B</formula><p>We emphasise that causal models, and the causal reasoning performed on them such as interventions, are most naturally presented in string diagrams. This applies also to further aspects of the causal model framework, discussed in Section 6.</p><p>Interpretation. A causal model does not a priori come with an interpretation in our sense, be it abstract or concrete. However, in the contexts where CBNs tend to be used and where much of the causal model literature sees their intended purpose, usually all (at least observed) variables have an abstract and a concrete interpretation. This is because the models are applied to 'data science problems' where there is observational or interventional (experimental) data involving variables with concrete interpretations. These variables are the aspects we consider causally relevant to the phenomenon being studied and given from the outset. For instance, in Ex. 19 all variables have an abstract and, with the exception of B, also a concrete interpretation. Note that a non-output variable X, i.e. a hidden or latent variable, might be part of the model simply because one cannot exclude the possibility of some confounding common cause unknown to us and it thus lacks any interpretation. However, also latent variables may have an abstract and even concrete interpretation, since being unobserved may just signify the lack of empirical data, despite being a specific variable that we 'understand'.</p><p>What does necessarily come with stating a causal model is the status of the variables as causal variables (causal relata) and the morphisms as causal mechanisms. In our setup, that status -essential for the causal model framework with its many concepts and rules -is best treated however using our general notion of a compositional framework, introduced in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11.2">Functional causal models</head><p>In the causal model framework, there is also an important 'more refined' form of causal model known as a Functional Causal Model (FCM), or perhaps more commonly a Structural Causal Model (SCM) [Pea09]. Formally speaking, an SCM is a special case of a CBN, where each mechanism factorises in terms of an underlying 'true' deterministic functional component, along with an additional 'noise' variable with no parents, encoding our uncertainty about the underlying state of the world. By treating causal relations fundamentally as functional dependencies, formalised via SCMs, one can then go on to derive the rest of the causal model framework for CBNs as theorems, rather than via ad hoc reasoning. In particular every CBN can be seen to arise (non-uniquely) from some SCM through marginalisation. From an interpretability viewpoint, SCMs are especially important in that they are the kind of causal model most pertient to studying neural network-based models, and allow us to define counterfactuals, which we treat in Section 6.</p><p>Recall the definition of a deterministic morphism (6), which generalises the notion of a function. This allows us to define an FCM in any cd-category. Definition 26. A Functional Causal Model (FCM) in a cd-category C is a causal model M in C whose variables are partitioned into exogenous variables U = {U i } n i=1 , where each U i is hidden and has no parents, having a mechanism of the form:</p><formula xml:id="formula_107">λ i U i (45)</formula><p>and endogenous variables V = {X i } n i=1 , where each X i has a mechanism of the form:</p><formula xml:id="formula_108">f i X i . . . U i Pa ′ (X i ) (46) where Pa ′ (X i ) ⊆ V and f i = f i is deterministic.</formula><p>Formally, the signature for an FCM includes equations of the form (6) specifying that each generator f i is deterministic. The usual kind of FCM considered in the literature is one with C = FStoch, and we reserve the term SCM for these. An SCM is thus given by finite sets X 1 , . . . , X n , U 1 , . . . , U n , describing values of the endogenous and exogenous variables, respectively, along with a distribution λ i over each U i and for each X i a function</p><formula xml:id="formula_109">f i : Pa ′ (X i ) × U i → X i for a subset Pa ′ (X i ) ⊆ {X 1 , . . . , X n }.</formula><p>Example 27. An SCM with variables V = {B, S, L, A} and U = {U B , U S , U L , U A } and the same causal structure amongst V as in Ex. 22 is given by a representation in FStoch of the network diagram below, such that each f X is deterministic, for X ∈ V. The pairs f X , λ X for X ∈ V may be chosen such that their composite recovers precisely the corresponding channel c X from our example CBN in Ex. 22.</p><formula xml:id="formula_110">f B f A A f S f L S L λB λA λS λA</formula><p>FCMs are closely related to a final form of causal model which is most relevant to the study of (deterministic) ML models. We call an (open) causal model deterministic when it is an (open) causal model but where every mechanism is a deterministic morphism. Thus it is given by a network diagram where every box is deterministic. We can thus arrive at a deterministic open causal model by starting from an FCM and 'removing' the input state λ i for each 'noise' variable U i , turning them into input variables of the new model. Many of the models we met in Section 5, such as linear models, neural networks or decision trees, are specified by network diagrams represented as functions, and so indeed may also be viewed as deterministic open causal models. When doing so, we view each of the morphisms + and ⋆ as mechanisms themselves, with a single output. This causal view on deterministic models is in fact common in XAI and will be discussed further in Sec. 7.</p><p>Interpretation. Just as for generic causal models, an FCM a priori has no interpretation, abstract or concrete. In the contexts where SCMs have been studied in the original causal model literature, they tend to play the role of a fine-graining or refinement of a CBN, and are supposed to capture the true (albeit usually unknown) underlying functional model in data science problems. Here the endogenous variables usually have an abstract and concrete interpretation. The exogenous 'noise variables' however tend not to. In ML contexts where SCMs, or more precisely deterministic open causal models (see above), appear frequently, they do not always have either an abstract or concrete interpretation. Variables in such a model might include computational units, e.g. a neuron's weights, which are not readily interpretable. See Sec. 7 for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Compositional frameworks</head><p>The discussion in Section 5 demonstrated how a wide range of AI models can be formally seen as compositional models, and that our definition of an interpretation (Definition 7) lets us analyse a model's interpretability in terms of its components. However, there is a further aspect of the intuition of a model's interpretability that is not captured by this definition, but which features in XAI discussions, and which the compositional perspective can again help us to disentangle. The further aspect is what we call the compositional framework of the model. Independently from having an (abstract or concrete) interpretation, a model may live in a particular framework that licences certain types of reasoning with its components. Roughly speaking, the framework answers the question of how useful the explicit structure of a model is once seen as a compositional model. That is, what kind of reasoning and what meaningful computations can be done using this structure?</p><p>Here we will focus on examples of frameworks, revisiting several classes of models which share the same well-defined framework. We will not define the general notion of a framework formally here, with the question of whether there is an insightful such formalisation being left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Framework of input-output models</head><p>Our first example can be seen as a 'trivial' notion of a framework. The framework of input-output models applies to any model which comes with an associated overall input-output map M in the semantics category. This includes any model generated by a single diagram D M , where M = D M is the resulting morphism in C from inputs to outputs. Examples include linear, rule-based, and transformer (on fixed input length) models.</p><formula xml:id="formula_111">M . . . . . . Y 1 Y k X 1 Xn</formula><p>What can we do with such a mapping? Firstly, one can consider different input states and obtain corresponding output states. Indeed, this is the way in which many models are intended to be used. In Set or NN any state necessarily is a product state, i.e. just a list of values for the corresponding variables, as in the left-hand diagram below. More generally, the input state s could be a non-product state, as on the right-hand side, and for FStoch it may in particular be the probability distribution underlying the given data in a training scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M</head><formula xml:id="formula_112">. . . . . . Y 1 Y k s 1 s n . . . Y 1 Y k = t 1 t k M . . . . . . Y 1 Y k t . . . Y 1 Y k = s</formula><p>Many models have just one single output variable, but if the output is given by k distinct variables as above, one can also compute the marginal on a subset of outputs, and apply this to input states as above. Below we depict the induced marginal over R ⊆ {Y 1 , ...,</p><formula xml:id="formula_113">Y k }. M . . . . . . R X 1 Xn . . .</formula><p>While it may seem rather trivial to regard the input-output behaviour as a framework, this is all that can be done with some models. Consider for instance a (sparse) linear regression model for a simple data science problem. Such a model is 'perfectly interpretable' with each component interpreted and appearing in the diagram. However, the structure of the model has no obvious further use. There is no framework that would tell us how to use the components on their own for further reasoning or computing quantities of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Framework of statistical models</head><p>Our next framework considers models which allow for statistical reasoning about their variables, including conditioning. By an (open) statistical model we will mean any compositional model M that has a distinguished morphism M and semantics in C channel , where C is a cd-category that has suitable further structure to model probabilistic reasoning. For the details on this structure see <ref type="bibr" target="#b90">[LT23]</ref>, which builds on categorical probability theory [CS12, CJ19, Fri20, DLR23]. For simplicity here we restrict to C = Mat R + with C channel = FStoch, i.e. probability spaces over finite sets. The most common statistical models are without inputs, defining a probability distribution M over the output variables, and we will focus on these. However, one can treat open models, where M is a probability channel with inputs, in the same way.</p><p>What does the framework of statistical models allow? Firstly, statistical models live in particular in the framework of input-output models: feeding in an input distribution yields an output distribution and one can compute marginal probability distributions and channels. In addition, however, one can compute conditionals. In conventional notation, given a distribution M = P (Y , X) over sets X and Y , P (Y |X = x) denotes the distribution over Y conditional on X = x, and P (Y |X) denotes the induced conditional stochastic map. In string diagrams, the former can be represented by the expression on the left below and the latter by the one on the right.</p><formula xml:id="formula_114">M Y X P (Y |X = x) Y := x M X P (Y | X) Y X := Y</formula><p>To understand precisely the formal meaning of these dashed boxes we refer to <ref type="bibr" target="#b90">[LT23]</ref>, but roughly the meaning of the blue dashed box is to turn a morphism in Mat R + into a correctly normalised probability channel. 30 In particular it turns a process with no input into a probability distribution by mapping P (Y , X = x) to P (Y ,X=x) P (X=x) . The cap (bent down wire) on the right corresponds to a Kronecker-delta. 31 These two 30 Technically it yields a partial probability channel, which on any input is either normalised or 'undefined'. 31 Formally, Mat R + is a compact category with the Kronecker-delta as a cap in the sense of Section 5.8.</p><p>diagrams for conditioning on a point X = x are related by the following equalities:</p><formula xml:id="formula_115">M P (Y | X) Y = Y x x = M Y x = M Y x P (Y |X = x) Y = (47)</formula><p>The framework of statistical models can also include updating our knowledge with respect to fuzzy evidence given by a distribution ω, rather than just sharp facts like X = x. Interestingly, there are in fact multiple meaningful ways in which we may update our knowledge with such fuzzy evidence. In contrast to the equality in (47), for a generic state ω (i.e. not a point distribution) and its corresponding effect, the left and right-hand sides of the inequality below show two distinct yet natural ways to apply updating, called Jeffrey updating and Pearl updating, respectively, as discussed by Jacobs <ref type="bibr" target="#b73">[Jac19,</ref><ref type="bibr" target="#b74">Jac21]</ref>.</p><formula xml:id="formula_116">M Y ̸ = M Y = M Y ω ω ω (48)</formula><p>The diagram associated with M may of course have internal structure. A particularly informative case is when it is a network diagram (see Def. 20), in which case the model is a Bayesian network. The structure then encodes conditional independence relations as formalised through the notion of d-separation [GVP90, VP90]. Both conditional independence relations and d-separation have been generalised within categorical probability theory and can be treated diagrammatically [CJ19, LT23, FK23b], illustrated with a simple example in Figure <ref type="figure" target="#fig_20">9</ref>. In summary, the framework of (open) statistical models allows the computation of any combination of conditioning, marginalising and fixing distributions for some inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Framework of causal models</head><p>Section 5.11 covered the notion of a causal model, but barely touched on the many concepts and results of its rich framework, which the literature indeed often refers to as the framework of causal models. Being well established, for a full account of this framework we refer to <ref type="bibr" target="#b90">[LT23,</ref><ref type="bibr" target="#b76">JKZ19]</ref> for the compositional model view and to [Pea09, SGS00, BCII22] for a conventional presentation. Here we only mention a few of its key aspects, with a view to the discussion of causal concepts in XAI, in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interventions.</head><p>A core aspect of the causal model framework are interventions, which we met in Section 5.11. An intervention, be it a basic do-intervention or a more general kind, is a well-defined operation relative to any causal model M which transforms it to a new one M ′ with altered causal structure and semantics (by altering causal mechanisms). The overall process M ′ of the new model is at times also referred to as the 'post-intervention distribution' and as we saw denoted P (Y ; do(X)) for a do-intervention.</p><p>The ability to compute interventions, and thus quantities that refer to a causal scenario distinct from the original one, is a feature of causal models over statistical ones. Formally, one is of course free to intervene on a general Bayesian network just as for a CBN. However, the resulting network would generally have nothing to do with the phenomenon or data generating process that was described by the original network, unless the structure is asserted as having a causal status.</p><p>Counterfactuals. For a functional causal model the framework is richer still, allowing the computation of well-defined 'counterfactuals'. The literature is not always clear about what is and what is not a counterfactual, and so for clarity we state a general definition below, after the following simple example.</p><p>Example 28. Consider the counterfactual question: 'Had Mary taken an aspirin last night, would she still have woken up with a headache today?' <ref type="bibr" target="#b126">[SP08]</ref>. To answer this, suppose an SCM of the below form is given with variables A and H, denoting whether Mary takes an aspirin and gets a headache, respectively, each represented by {y, n} for 'yes', 'no':</p><formula xml:id="formula_117">f g λ ′ H A λ U H U A</formula><p>Here U A , U H are the respective exogenous variables with distributions λ, λ ′ . To evaluate the counterfactual, we intuitively wish to restrict to those (albeit unknown) background conditions that led to the event which occurred, namely that A = n, H = y, but now intervene so that an aspirin is being taken, and determine the resulting distribution over headaches. The counterfactual distribution is thus represented by the string diagram on the LHS below, which is then simplified via rewrite rules (see Sec. 8.1 of <ref type="bibr" target="#b90">[LT23]</ref> for the details).</p><formula xml:id="formula_118">f g λ ′ λ f g H * y y n y = g λ ′ g H * y y n A H A *</formula><p>We can think of the LHS diagram in terms of 'parallel worlds', which share the same background variables (through perfectly correlated copies of U A and U H ), where the left-hand world shows (via the conditioning) what actually occurred, and the right-hand one a hypothetical world where aspirin was taken, given by the intervention.</p><p>The key ideas are that interventions allow the construction of hypothetical worlds, that functional dependencies allow the background conditions to be kept the same and 'stitch the worlds together', and that conditioning incorporates the facts that obtained in the actual world. These lead to the following definition of counterfactuals from <ref type="bibr" target="#b90">[LT23]</ref>, which matches (but slightly generalises) the notion from the conventional causal model literature [Pea09, <ref type="bibr" target="#b103">Pea11,</ref><ref type="bibr" target="#b6">BCII21,</ref><ref type="bibr" target="#b71">HP05]</ref>.</p><p>Given an FCM M, consider its factorisation into a model L, consisting only of its ('latent') exogenous variables with the product state over them (45), and the model F consisting of its endogenous variables with their deterministic mechanisms (46). Each computable counterfactual is given by a diagram in C of the following form:</p><formula xml:id="formula_119">L U σ 1 (F) • • • C 1 D 1 E 1 c 1 C k D k E k c k σ k (F)</formula><p>Here the exogenous variables U are copied to n many 'parallel worlds', each given by applying a dointervention σ j to the deterministic part F of the original FCM, as well as conditioning on some variables via c j and marginalising some D j .<ref type="foot" target="#foot_26">foot_26</ref> Counterfactuals are thus another quantity of interest computable for a particular class of models, determined by the status of their compositional structure, but in principle independent from an interpretation in our earlier sense. We return to discuss their relation to counterfactual explanations in XAI in Section 7.2.</p><p>Identifiability problems and the causal hierarchy. In data science problems, one is rarely confident as to what exactly the causal structure is. There may be unknown common causes, and even if one is aware of all of them they may lack empirical data. As a result much of the causal model literature is concerned with causal identifiability problems: without a fully specified SCM or CBN, under which conditions on the causal structure and the available empirical data can a certain quantity still be inferred unambiguously? Such quantities include post-intervention distributions and counterfactuals.</p><p>For many identifiability problems, complete algorithmic solutions have been developed which, given (partial) causal knowledge, some empirical data and a query, output whether the quantity is identifiable and if so what it is [Pea09, <ref type="bibr" target="#b126">SP08,</ref><ref type="bibr" target="#b15">CB20]</ref>. The results establish that there is a strict separation in the identifiability of various quantities, forming what is known as the causal hierarchy [Pea09, BCII21, PM18], with the identification of purely statistical quantities, causal effects and counterfactuals forming three levels, each of which requires strictly more detailed causal knowledge than the preceding one. Note that many identifiability problems can be treated string diagrammatically <ref type="bibr" target="#b90">[LT23,</ref><ref type="bibr" target="#b76">JKZ19]</ref>, in which case the answer to a query is produced through a series of diagrammatic rewrites, licensed by both the status and properties of the compositional structure (e.g. d-separation conditions for the well-known do-calculus rules). The causal hierarchy has implications for AI that have been discussed in many works, e.g. [Pea18, SLB + 21b]. In our terminology we can say that the causal hierarchy is independent from the presence of an interpretation, but manifests in the difference between the frameworks of statistical models, CBNs and FCMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Frameworks for NLP models</head><p>For our last example, we consider how we might define the frameworks for NLP type models, which do not come with a single distinguished input-output process. These include transformers (now with varying input length), and sequence models, as well as CCG, DisCoCat and DisCoCirc models. What are the meaningful processes we can compute with models of this kind?</p><p>Most importantly, for any such model, when given a phrase w 1 , . . . , w n we can compute its representation as a state in the semantics category C:</p><formula xml:id="formula_120">w 1 , . . . , w n</formula><p>Explicitly, for a sequence model such as an RNN this is the state representation (31). For a DisCoCat model (or CCG model) the phrase is first sent to a parser which assigns it a grammatical structure equivalent to a diagram, from which we obtain a state such as <ref type="bibr">(35)</ref>. For a DisCoCirc model, the parser returns a text circuit for the phrase, to which we apply some fixed initial states ⋆ to each noun wire. A transformer model applies the network (27) for input length s = n to the token labels.</p><p>However, computing word or phrase representations alone will not be of much use in practice. For actual applications, the framework for an NLP model must also allow us to compute useful outputs from this representation. To do so, each of these models is applied with processes which map this text representation to some output. One way to think of such output processes is as questions</p><formula xml:id="formula_121">(Q i ) n</formula><p>i=1 we can ask of the representation, each yielding an answer from some set A i of answers:</p><formula xml:id="formula_122">w 1 , . . . , w n Q i ? A i</formula><p>For example, the question could ask for the likely next word, as in a generative language model. Alternatively each question could return a class such as a sentiment label for the phrase. Or we could have a DisCoCat model followed by a process which assigns a sentiment from A = {positive, negative} to a movie review as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>is movie esoteric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N N S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>good?</head><p>A Thus, in general, the framework for any NLP model will include at least the representation states for phrases (w 1 , . . . , w n ) and a set of 'question' processes Q i , allowing us to compute outputs as above.</p><p>For models where the representation space of a phrase has interesting compositional structure, however, these questions may themselves be structured, leading to a richer framework. For example, a DisCoCirc model is typically trained along with processes for a set of questions, each of which acts on a set of nouns to which it is relevant. A question Does Alice know Bob? would be represented by a process of the following form, with discards on the irrelevant nouns: Does Alice know Bob? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋆ ⋆ ⋆ (49)</head><p>A quantum DisCoCirc model trained with questions of this form is explored in [DBM + ng]. Combining the structure in (49) with the fact that the text circuit itself is compositionally structured, it can be seen that DisCoCirc models come with a richer framework that allows for reasoning about the answer to such a query. We will see this explicitly when we consider the explanations offered by diagrams of this kind in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Aspects of conventional XAI</head><p>In this section we discuss various aspects of conventional approaches to XAI from our compositional perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Influence, causal structure and feature importance</head><p>A basic consideration which may help us better understand a model is that of which inputs can affect which outputs. Most models we discussed contain channels (Def. 1), where the input and possibly also the output often have a product structure of several variables. For such processes, we can reason about the relations of influence between inputs and outputs, also known as signalling relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence and interventions. Given a channel</head><formula xml:id="formula_123">c : X 1 , X 2 → Y 1 , Y 2 ,</formula><p>we say that variable X 2 does not influence (signal to) Y 1 iff the marginal channel into Y 1 factorises as follows for some channel d:</p><formula xml:id="formula_124">c = d Y 1 Y 2 X 1 X 2 X 1 Y 1 X 2 (50)</formula><p>The terminology of 'no-signalling' comes from how interventions on an input variable propagate through caltering X 2 won't affect Y 1 , and hence no signal can pass from the former to the latter. More precisely, no matter what state s 2 is fed into X 2 , the marginal state at Y 1 does not depend on s 2 :</p><formula xml:id="formula_125">c = d Y 1 Y 2 Y 1 s 2 = d Y 1 s 1 s 2 s 1 s 1</formula><p>Note that, seeing as X 1 , X 2 , Y 1 and Y 2 may have further product sub-structure, we can define (no-)influence from any collection of inputs to a collection of outputs in the same way.</p><p>Causal structure in NNs. One way to situate several approaches in conventional XAI is that they are based on the idea that a given neural network-based model implicitly is, or induces, a causal model. This can be understood in several ways, which it will be helpful to clarify. Firstly, observe that any such model defines a function, i.e. deterministic morphism f :</p><formula xml:id="formula_126">X 1 × • • • × X n → Y 1 , × • • • × Y k ,</formula><p>which we can view as an open deterministic causal model (Section 5.11.2) as follows. Just like any function, f factorises into k component morphisms f j from its inputs to each output Y j . Moreover, each f j will in general only depend on a subset S j of the inputs. (This subset S j is determined by excluding those inputs for which f satisfies no-influence to Y j .) Hence we can rewrite the function f as below, where each X i is only copied to f Yj if X i ∈ S j . The resulting diagram is a network diagram, and thus defines an open deterministic causal model whose variables are simply the inputs and outputs, in which Y j is a child of X i in the causal structure iff X i ∈ S j .</p><p>. . .</p><formula xml:id="formula_127">f . . . Y 1 Y k X f . . . X = X = Y k . . . Y 1 f f . . . Y 1 . . . . . . Y k = . . . X 1 Xn f Y1 f Y k . . . Y 1 Y k . . . . . . . . . S 1 . . . S k . . . . . . . . .</formula><p>There is also a second, more immediate way in which to view a neural network-based model as an open deterministic causal model. By construction, any such model will be specified as a computational graph, either down to the level of neurons, as in neural network diagrams (Section 5.5), or at a higher level of abstraction, as in the diagram for a transformer (Section 5.6). As we have seen, any such graph can be seen as a string diagram, and in fact a network diagram. Thus the diagram explicitly represents the function as an open deterministic causal model, now including intermediate variables along with the inputs and outputs.</p><p>Post-hoc methods approximating causal properties. Seeing an NN-based model as inducing an (open deterministic) causal model in this way is increasingly common in the context of XAI methods. With the structure and specific properties of this implicit causal model typically unknown, the goal of many of the post-hoc XAI methods can be seen as trying to reveal facts about it.</p><p>One causal aspect to explore is the set of influence relations. In practice, however, for a trained deep NN there will typically not be any no-influence relations that hold exactly -that is, no strict equality as in Eq. 50 -but at most approximate ones, due to the full connectivity of the string diagram. As a result, interest focuses on the 'strength' of influences between variables. The many variations of feature importance (feature attribution) methods -be they gradient-based [CMSB19, ZF14, SGK17, BML + 16] or perturbation-based [FOSR21, PCJ + 20, VGB + 20, DCSHT21, ADF + 22, CGAGD + 22] -essentially study the causal effect of features on the output. (Also see the discussion in [GWL + 22].) Here features may be inputs or neurons of intermediate layers, and one means of exploring the causal effects is through interventions on these corresponding input or intermediate variables. Note that freely choosing an input state x, as is common in 'extrapolation' or 'data augmentation' techniques, is a special case of this, and in general has to be seen as an act of intervention.<ref type="foot" target="#foot_27">foot_27</ref> A form of explanation with an explicitly causal flavour, and based on such intervening on inputs, are counterfactual explanations, which we discuss in more detail in Section 7.2.</p><p>While one can always study the properties of a deterministic model through interventions on input or intermediate variables, arguably the resulting dependence statements only help with the interpretability of the model when the concerned variables have an (at least abstract) interpretation. This is usually the case for the input and output variables, while intermediate variables such as individual neurons in middle layers tend not to have an interpretation. Yet a seemingly common attitude in XAI is that a successfully trained model must have implicitly learned 'meaningful' intermediate variables, and by probing or intervening on them, one may discover their interpretation. See Sec. 2.1.2 and <ref type="bibr" target="#b47">[FK23a]</ref> for a critical assessment of this view.</p><p>Whichever methods are applied, note that, while viewing a model as a causal one in this way has a clarificatory benefit, it does not provide anything formally new from the perspective of the causal model framework. We are 'just' studying a model with all variables 'observed', rather than using a causal model to make predictions about hypothetical interventions that cannot be performed experimentally.</p><p>The explanatory role of causal notions. To the degree that the XAI methods outlined above are explanatory, they explain the model. The implicit causal structure that is being studied is that of the model, and a priori has nothing to do with the one in the world -see Sec. 7.3 for a more detailed discussion. Nonetheless, some argue that the most important way in which humans understand any phenomenon is in causal terms, and that their preferred explanations are causal explanations, and so ideal explanations of an AI model (in particular of input-output pairs) are causal explanations also <ref type="bibr" target="#b47">[FK23a,</ref><ref type="bibr" target="#b68">HDR18]</ref>. This attitude naturally leads to seeing a model that has fully explicit causal structure and a complete interpretation simply as 'its own explanation' <ref type="bibr" target="#b16">[CBC23]</ref>. Such a model has a kind of 'intrinsic interpretability' that, due to the explanatory significance of autonomous causal mechanisms, goes beyond the transparency and traceability aspects of the classic examples of intrinsically interpretable models. Such an attitude is well aligned with this work's argument for the benefits of explicit and interpreted compositional structure, and we will return to it in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Counterfactual explanations</head><p>As mentioned in Section 2.1, among the most common post-hoc methods in XAI are ones that produce local explanations called counterfactual explanations (CFEs) <ref type="bibr" target="#b146">[WMR17]</ref>. Roughly speaking, given a deterministic model with distinguished process M , which has input variables X = X 1 . . . X n and output variable Y ,<ref type="foot" target="#foot_28">foot_28</ref> a CFE of input-output pair (x, y) is an alternative pair (x ′ , y ′ ) with y ̸ = y ′ (and y ′ usually of particular interest to the explainee), and such that x ′ is as similar to x as possible given that M</p><formula xml:id="formula_128">• x ′ = y ′ : M Y Y = X x y M Y x ′ Y = X y ′ (51)</formula><p>Here similarity is taken with respect to some chosen notion of distance d(x, x ′ ). For example, x and x ′ might only differ in as few dimensions as possible, say only in the ith variable: . . .</p><formula xml:id="formula_129">X = X 1 x1 xn x xi . . . X n X i ; x ′ x ′ i . . . = x1 xn . . . X X1 Xn Xi (52)</formula><p>The non-trivial aspects of CFE methods consist in picking a suitable notion of similarity and in providing a solution to the practical problem of finding CFE pairs. For the sake of concreteness, consider the following much discussed kind of loan example.</p><p>Example 29. (Adjusted from <ref type="bibr" target="#b56">[Fre22]</ref>) Suppose a person applies for a loan with a bank that decides deterministically whether to grant it based on a model M L that takes age, salary, capital, number of open loans, and number of pets as input features. Also suppose the application is rejected and moreover that, on the basis of a CFE method, they are told by the bank that "had they had a £5k higher salary and two pets, they would have got the loan".</p><p>It is understandable that this kind of 'explanation' is popular -it appeals to the link between counterfactual dependencies and causal relations, and the explanatory significance these have, even in everyday life. There are, however, crucial issues with CFEs, such as their lack of uniqueness. See, for example, <ref type="bibr" target="#b56">[Fre22]</ref> for insightful discussions. Here we focus on two points: in what sense CFEs are counterfactuals, and then in Section 7.3 how the loan example relates to the 'model-vs-world' distinction.</p><p>CFEs and Pearlian counterfactuals. The presentation of CFEs in the literature often claims that a CFE is a counterfactual in the causal model sense (an exception is <ref type="bibr" target="#b16">[CBC23]</ref>). However, simply varying the inputs of a function does not, in an obvious way, look like a special case of the definition of a counterfactual in Eq. (6.3). Of course, the idea of a function, some of whose inputs are changed through intervention, does play an important role in the definition of a counterfactual. However, this alone is not enough to justify the use of the term in the sense of the causal model framework. For the details see App. B, but the crux of the argument is this: without further causal assumptions beyond just M , it does not make sense to regard the alternative state x ′ , which differs from x only in X i -or more generally, some subset S of the input variables -as arising through only intervening on X i . And yet this is supposedly the "had X i been different" part (and all else the same) of the counterfactual's antecedent. As a result, the pair (x ′ , y ′ ) does not constitute a well-defined counterfactual relative to just the given model M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">An important distinction -model vs world</head><p>Any form of interpretability should be informed by the specific question and purpose that the model is supposed to address <ref type="bibr" target="#b47">[FK23a]</ref>. A key distinction is whether we need to reach a degree of transparency and explainability of the model, or whether we are concerned with the world, or phenomenon that the model is about. The importance of distinguishing concerns of model vs world in XAI has been stressed in various works <ref type="bibr" target="#b47">[FK23a,</ref><ref type="bibr" target="#b8">Bec22,</ref><ref type="bibr" target="#b49">FKMTC22,</ref><ref type="bibr" target="#b130">Sul22b,</ref><ref type="bibr" target="#b142">Wat22,</ref><ref type="bibr" target="#b16">CBC23,</ref><ref type="bibr" target="#b56">Fre22]</ref>. From a compositional perspective, a pertinent question is whether a model's structure is asserted to correspond directly to structure in the phenomenon the model is about or not. The answer has a bearing on the types of questions and interpretability concerns that can be addressed with the model.</p><p>While not formal definitions, we introduce the following terminology to aid clarity.</p><p>M-type Designates questions and purposes, as well as corresponding explanations and notions of interpretability, that concern only the model itself.</p><p>W-type Designates questions and purposes, as well as corresponding explanations and notions of interpretability, that (explicitly or implicitly) refer to the world, phenomenon or process that is being modelled.</p><p>One might argue that any notion of interpretability is of M-type -it can only concern the interpretability of a model. Whereas questions that invoke the world, like asking what intervention will achieve some goal, can only concern what one does with a model's interpretability. However, it may be useful to extend the distinction to notions of interpretability and explanations. Explicitly declaring a post-hoc explanation as M-type may help to prevent misuse, such as users confusing M-type statements like 'prediction p was caused by feature x' for ones that can guide actions. Freiesleben writes "we need to be clear about whether we want to [explain] the model or the modeled process. [...] We can only move from a model [explanation] to a process [explanation] if the model itself, and also the translation of our inputs, preserve the essential structure of the process" <ref type="bibr" target="#b56">[Fre22]</ref>.</p><p>The distinction is particularly relevant for causal structure. Suppose we have the ideal case of a causal model with a complete interpretation, either given explicitly or somehow induced by a neural model as discussed in Section 7.1. Even in this case, the causal structure is usually simply the structure of the model (M-type), and a priori has nothing to do with the causal structure between the variables in the world (Wtype). While this seems clear enough, the causal flavour of the common XAI explanations can easily be misunderstood as causal statements about the world and thus lead to inadvertent misuse. Consider for instance the following situation.</p><p>Example 30. Suppose a model predicts the probability of suffering a stroke within 5 years based on the input features of age, profession, exercise habits and BMI. Also suppose that a person is predicted to have a stroke with probability 15% and that some version of LIME outputs the feature importances for this prediction as -0.3 for age, +0.2 for profession, +0.2 for exercise habits, -0.1 for BMI. Assume also that the model in fact perfectly follows the ground-truth distribution, and so 15% is indeed the true probability of a stroke given these features. Even then, the output of LIME may be interesting in telling us (local, approximate) dependencies in the model, but it would be foolish to infer actions in the world to reduce the risk of stroke on its basis. This is because altering any of these variables in reality may well affect the others, and the influence between some feature and stroke likelihood in the model might have picked up on a spurious correlation, while the influence in the world is mediated via other variables; the model has simply not been trained to address interventional questions on how to reduce stroke likelihood. This has nothing to do with a limitation of LIME, but is the result of a category mistake.</p><p>The care needed in distinguishing between purely associational questions and causal queries is by now an old and well understood point -see Sec. 6.3 for the causal hierarchy. Also, the role of action-guiding explanations has specifically been emphasised in XAI. Beckers argues that "an important goal of [XAI] is to compensate for [the] mismatch by offering explanations about the predictions of an ML-model which ensure that they are reliably action-guiding" and that "action-guiding explanations are causal explanations, [which require] knowledge of an additional model, namely a causal model of the target system" <ref type="bibr" target="#b8">[Bec22]</ref>. A further typical XAI example where the model-vs-world distinction matters, but is rather subtle, is our earlier loan application example.</p><p>Example 31 (CFEs and the world). Consider again Example 29 of a bank's loan application system and the CFE for an applicant's refusal to receive a loan. When the model-vs-world distinction is discussed in the literature, this sort of example on loan giving is often presented as an example where the explanations from XAI methods, such as a CFE, are about the world and not just the model. Indeed the model is by definition the mechanism in the world by which the bank does in fact decide on the loan. However, while the model's relations between inputs and loan prediction are correct by definition, they don't capture the causal relations that actually obtain in the world, both amongst the input features and also between inputs and whether or not someone actually defaults on a loan (as opposed to getting a loan).</p><p>This structural mismatch can easily lead to wrong conclusions and abuse. First, as is well-known, a customer may exploit the wrong causal relations in order to receive a loan when really nothing has changed that would make them more likely to actually pay it back -say, by increasing the number of pets <ref type="bibr" target="#b56">[Fre22]</ref>. Second, CFEs cannot be taken to guide actions in the world. As we have seen above, an alternative input x ′ that appears in the antecedent of a CFE may differ from the original input only in a subset S of features, making it look as if the effect is achievable by suitably intervening on S. However, in the world this will generally change other variables, too, and not lead to the assignments as in x ′ . <ref type="foot" target="#foot_29">35</ref>The M-type vs W-type distinction is not only relevant to causal structure. For instance, in models of the DisCoCat and DisCoCirc frameworks (Secs. 5.8 and 5.9) the compositional structure is determined by the (grammatical) structure in the phenomenon of natural language. There may well be tasks and concerns that, analogously to the case of causal structure, require getting this structure right. We leave it for future work to identify further examples that demonstrate this form of structure-correspondence aside from the causal one.</p><p>In summary, while there has been some focus on the distinction between associational and causal reasoning (e.g. <ref type="bibr" target="#b8">[Bec22]</ref>), this doesn't acknowledge the fact that many XAI methods are closely related to causal explanations, and instead the key distinction is the status of the model's structure (causal or otherwise)does it correspond to the phenomenon or not? A formalisation of structure such as that offered in this article can help make this distinction sharper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Observations on compositionality and interpretability</head><p>Having seen the basic ingredients of our compositional approach (Sections 4 and 6), numerous examples of AI models in this light (Section 5), as well as the discussion of pertinent themes in conventional XAI (Section 7), let us now extract and summarise the key observations on the relationship between the two central concepts -compositionality and interpretability.</p><p>Many AI models can be represented in string diagrams. We have seen that a wide range of AI models can be formally described as compositional models, which in practice means representing them as string diagrams. This also allows us to distinguish in a formal, yet very intuitive way a model's high-level structure (syntax) and the concrete instantiation (semantics). This is of independent interest aside from the interpretability angle this work focuses on, and helps reason about models and their properties.</p><p>Forms of compositionality are familiar in ML. While the language of categories and composition may seem novel, we have seen that some forms of composition are in fact ubiquitous in ML and data science. At the most basic level, any situation in which we can describe a component of a model as consisting of various aspects or factors is usually an instance of compositionality. In this case, the composition is simply depicted as separate 'wires in parallel', with a wire for each aspect. For example, suppose we have a representation space X which we know factorises in terms of n dimensions or features X 1 , . . . , X n . In diagrams this appears as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=</head><p>. . .</p><formula xml:id="formula_130">X X 1 Xn<label>(53)</label></formula><p>In a neural network model, this means that X = X 1 × • • • × X n is given by a Cartesian product ('direct sum') of the features X i ; for example, each X i could simply be given by R with X = R n . <ref type="foot" target="#foot_30">36</ref>As another example, suppose we have a model trained on a data table of staff members, consisting of columns for 'age', 'gender' and 'role', which means the input space factorises in terms of wires for each of these. The data itself is given by a distribution over these factors, and so appears as below.</p><formula xml:id="formula_131">= Input Age Gender Role Data Data<label>(54)</label></formula><p>Beyond decomposing a wire (a single space), more general string diagrams, which allow us to decompose a process, such as an entire input-output model, also feature in ML. For example, the computational graphs used to describe neural networks can be seen as string diagrams, as we saw in Section 5.</p><p>Explicit compositional structure underlies, but doesn't imply, interpretability. In general, drawing a string diagram can be seen as a first step towards 'making sense of' a model, but does not necessarily provide it with an interpretation. Once we have a diagrammatic account of a model, we then can ask whether or not its components do in fact have an interpretation in our sense (Sec. 4.1). Such an interpretation may be partial (concerning only a subset of the components), abstract (only assigning meaningful names to the respective components), or concrete (essentially, also interpreting the variables' state spaces).</p><p>For some models the diagram will consist of meaningful interpretable processes, such as causal mechanisms or the conditions in a decision tree. However, for most ML models this diagram will consist of components which are not a priori necessarily interpretable. This applies both to models whose diagrams are very low level, such as neurons in a network, or more high level, as in a transformer diagram which contains attention heads whose function is not interpreted. For example, the decomposition (53) need not necessarily make the space X more interpretable; it may or may not be that the features X i have an interpretation to us. In contrast, in (54) we would expect each variable such as 'Age' to come with both an abstract and concrete interpretation, with each specific value interpreted as a specific age.</p><p>So whether or not a model has an interpretation is not part of its definition as a compositional model. Indeed, one and the same kind of model, be it a linear model or some deep NN architecture, may be interpretable in one situation, but not in another. An interpretation is extra data -wherever it may come from. Moreover, the definition of an interpretation (Def. 7) is broad with the signature H of human-friendly terms not prescribed. It is down to the researcher, user or explainee to decide what they regard as giving meaningful interpretations in the given context -in keeping with the generally acknowledged degree of subjectivity in judging a model's interpretability.</p><p>Standard XAI methods -interpreting models from the 'outside'. Even standard black-box models do in fact typically come with some interpretable compositional structure, namely that of their inputs and outputs. Indeed these relate to training data, which itself is often concretely interpretable. If so, the overall input to the model thus factorises in terms of individual concretely interpreted input factors, as does the output similarly. Hence, for any such model, the level at which we may draw an interpretable diagram is simply from this 'outside' perspective, as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black box Out 1</head><p>Out n</p><p>In 1 In m . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>concretely interpreted (55)</head><p>For example, this might be age, gender and role as in (54). For a transformer trained on text, from this perspective the inputs would be the individual word labels (rather than their embeddings), while the output could be the predicted next word. Since the diagram in (55) is often the only one of the model we may draw whose variables are immediately interpretable, and seeing as 'explanations' are only explanatory if referring to interpreted terms, it is also the level at which standard XAI methods tend to be applied. Indeed, as Section 7 argued, most kinds of explanations provided by post-hoc XAI methods can be stated solely in terms of inputs and outputs and usually convey (or rather, approximate) causal properties of the model -essentially, 'how does input j affect output k?' Examples are methods that use Shapley values, in order to answer 'which input wire is most important to this output?', or that produce counterfactual explanations. However, several criticisms of the 'explanations' provided by such methods have been given; see <ref type="bibr" target="#b113">[Rud19,</ref><ref type="bibr" target="#b47">FK23a]</ref> and Section 2.1.2 for a summary. Because of such limitations, we argue that richer, more principled explanations will require us to look inside the model, requiring a model with interpretable internal compositional structure.</p><p>Intrinsic interpretability is diagrammatic. We have seen that the standard examples of intrinsically interpretable models, namely rule-based and linear models, can be naturally depicted in string diagrams (Secs. 5.3 and 5.4). The components made visually explicit are the very aspects that are also considered to make the model interpretable. For example, the diagram for a decision tree contains boxes for each of its conditions, which are readily interpretable functions (e.g. 'Age &lt; 20'), while the diagram for a linear model contains the weights, interpreted as relevance.</p><p>Note that the interpretation is not formally a part of the definition of either linear or rule-based models, but it is understood implicitly that the contexts where they are used ensure the existence of a complete and concrete interpretation. For example, a linear regression model is trained relative to a data science problem with all variables having a concrete interpretation as a given, while a rule-based model is understood to have rules that individually do make sense to us. Furthermore, intrinsically interpretable models are usually only considered to be such subject to a sparsity constraint, meaning they are not too 'large' for humans to easily trace decisions. This condition though is not part of an interpretation in our sense. Obviously, it is an important question whether a model is simple enough so that a human gets it 'at one glance', but from this work's perspective this is a question of sparsity and simplicity and not about interpretability as such. Even if a rule-based model is 'huge', as long every variable and process has a concrete interpretation, in the worst case one may just need an analysis tool to help inspect and trace decisions, but the interpretability is just as manifest and inherent as with a small model.</p><p>The literature actually does not give a definition of intrinsic interpretability, but only lists canonical examples. In our terms they are models with a complete, concrete interpretation subject to the additional sparsity condition. Hence, our notion of interpretation is consistent with the one prevalent in the XAI community, for the case of these models, but extends it to further kinds of interpreted compositional models.</p><p>Compositional frameworks -interpretability beyond interpreted structure. A further aspect of a model's compositional structure that contributes to how (usefully) interpretable it is, but that is not captured by the mere presence of an interpretation in our sense, is the status of the structure. This structure may or may not license computing quantities of interest by combining its components in novel ways. This aspect is captured by the kind of compositional framework in which a model lives, independently from its interpretation.</p><p>The main examples, discussed in Section 6, are: input-output models, which despite a possibly explicit and complex compositional structure only allow for the production of outputs given inputs; statistical models, which additionally allow for the computation of conditionals; and causal models, which allow for interventions, computing counterfactuals and treating causal identifiability problems. Indeed, a sparse linear regression model for a loan prediction problem may be a vanilla version of being intrinsically interpretable, but the components themselves have no meaningful relevance to us on their own -unlike a causal model for the same problem.</p><p>Model vs world -compositional structure of what? An increasingly popular perspective sees standard post-hoc methods couched in the language of causal models. Indeed, as discussed in Sec. 7.1, any deterministic model induces a deterministic open causal model and many XAI methods are aimed at revealing causal properties of the given model. However, even if the causal structure was explicit and perfectly known, it is causal structure of the model, which a priori (and indeed typically) has nothing to do with the causal structure in the world or phenomenon the model is about. One must not confuse the two, and only when there is a structural correspondence, can XAI explanations, and the model more generally, be action-guiding in the world.</p><p>Related to the model-vs-world distinction are a line of arguments claiming that when a model has picked up relevant modular structure in the phenomenon (such as causal structure), then this structure should stay largely unaltered between closely related problem instances, and the model itself should be more robust, train efficiently and exhibit strong generalisation [SLB + 21b].</p><p>Compositionally-interpretable models. Bringing together our observations so far, we claim the following.</p><p>Most ways one can 'make sense of' an AI model amount to providing it with a string diagrammatic description, i.e. viewing it as a compositional model. Interpreting the AI model then amounts to providing the compositional model with an interpretation in our sense. Models with a complete interpretation, which we refer to as compositionally-interpretable (CI) models, generalise the notion of intrinsically interpretable models and include in particular causal models. Compositional interpretability is especially useful when the interpretation is moreover concrete and the interpreted structure corresponds to structure in the phenomenon.</p><p>Starting from an AI model we choose a 'level of abstraction' at which to understand it as a compositional model (e.g. from neurons at the lowest level, to potentially a single black box at the highest), which amounts to expressing it in terms of string diagrams. This does not itself make a model interpretable; for example we can always draw any neural network as a (huge) string diagram. However, if the components of the diagram(s) for a model are interpretable, we can say that the model itself is.</p><p>If the interpretation is a concrete one and complete, i.e. covers all generators, one may see the interpreted diagram describing the model as an 'explanation' in itself. This view on what we call a CI model is consistent with the perspective that an (interpreted) causal model is 'its own explanation', as well as our observation that rule-based and linear models are manifestly interpretable from their own diagrams.</p><p>In fact, this view is also consistent with much existing work in the XAI literature, in which a model is specified in terms of a computational graph, and attempts to understand it amount to attempting to interpret the components of this diagram. For example, methods exist to attempt to assign an interpretable 'concept' to each neuron in a neural network, or attention head in a transformer <ref type="bibr" target="#b69">[HM19]</ref>, both of which can be understood as aiming to assign abstract interpretations to these components of the diagram.</p><p>If it can be made possible to assign rich such interpretations post-hoc to trained networks, then this would indeed yield CI models, and arguably solve the main problems for XAI. However, this is unlikely due to various limitations for example pointed out in <ref type="bibr" target="#b47">[FK23a]</ref>, including the fact that there is no reason for neural networks to independently use the same concepts as humans, as well as that features such as dropout encourage global, not local, representations of information in neural networks, and so concepts are unlikely to be located in specific parts of a network. Practically, this process of assigning meanings post-hoc following training is also highly effort intensive.</p><p>While it is arguably uncontroversial that CI models have an ideal form of interpretability, they are not easy to obtain. Yet, there are at least two options for employing the concept in practice. First, one may weaken the requirement that the model that solves a certain task of interest must itself be a CI model, but rather ensure that it behaves approximately as such, for instance through causal abstraction approaches (Sec. C.2).</p><p>Second, and maybe most importantly, following the voice of, e.g., Rudin <ref type="bibr" target="#b113">[Rud19]</ref>, there are situations in practice where the stakes are high enough to justify simply biting the bullet. In these cases, the cost and effort of building a case-specific CI model -combining expert knowledge, causal discovery algorithms, and whatever else is required -is worth it and should be investigated more often than currently done so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Explanations from diagrams</head><p>Our analysis of a range of compositional models and aspects of interpretability led us in the previous section to consider the class of compositionally-interpretable models. These come with a complete interpretation, and ideally a concrete one, and include the classic intrinsically interpretable models, as well as causal models. <ref type="foot" target="#foot_31">37</ref>While such models can be regarded as 'their own explanation', it is important to ask: how exactly can the internal diagrammatic structure of a model provide explanations for the specific outputs it produces?</p><p>In this section we make this concrete by discussing several ways in which interpreted diagrams allow reasoning about the behaviour of a model. These are structural influence constraints, which allow reasoning about which inputs affect which outputs; diagram surgery, which allows us to act on, or alter the model directly; and finally rewrite explanations, which use diagrammatic reasoning to give direct accounts of, and constraints on, particular outputs of a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">No-influence arguments</head><p>As we have seen, perhaps the most prevalent and simple form of composition is the factorisation of a wire as a product of factors. Whenever the inputs (and possibly also the outputs) of a process can be decomposed in this manner, we can then reason about which input factors can affect which output factors. Recall that a channel c from A, B to C, D exhibits no-influence from B to C, so that interventions on B cannot influence C through c, when the following holds:</p><formula xml:id="formula_132">c = d C D A B A C B</formula><p>The presence of no-influence relations of a (process within a) model provides constraints on which inputs affect which outputs, which can greatly simplify the search for explanations or be explanatory themselves. For diagrams consisting of channels, no-influence relations may in particular be found by simply examining the connectivity structure of the diagram. Indeed, whenever an input variable X has no directed path (reading bottom up) to output variable Y in the diagram, then X cannot influence Y in the overall channel represented by that diagram. This fact becomes evident diagrammatically by 'letting discards fall through' [KHC17] as in the following example, where there is no directed path from X 4 to Y 1 :</p><formula xml:id="formula_133">h g f m h g f = g f = X1 X2 X3 X4 Y1 Y1 Y1 X1 X2 X3 X4 X1 X2 X3 X4<label>(56)</label></formula><p>As a result, models with non-trivial compositional structure come with the added benefit of non-trivial influence relations that can simply be read off the structure of the diagram.</p><p>A classic example are causal models, where the influence constraints from the absence of paths in the DAG (or equivalently, network diagram) provide great reasoning ability. Indeed, these constraints are part of the intuitive and explanatory role such causal structure is understood to have. While the concept of no-influence cannot be applied directly to a conventional (closed) causal model without inputs, there are closely related no-influence type of statements that can be inferred from the structural properties of such a model, using the same sort of 'absence of path' argument as above. For instance, whenever X is not an ancestor of Y in G, then in the corresponding network diagram there is no directed path from X to Y and one can easily see, analogously to Eq. 56, how an intervention on X, say a do-intervention do(X = x), cannot have any effect on the output of Y .<ref type="foot" target="#foot_32">foot_32</ref> See the following example for an illustration.</p><p>Example 32. Given a causal model with network diagram as on the left below (also studied in <ref type="bibr" target="#b90">[LT23,</ref><ref type="bibr" target="#b126">SP08]</ref>) consider a do-intervention do(X = x) and marginalise all variables but Y . As is straightforward to see through the below simplifications, the absence of a path from X to Y in the network diagram yields that do(X = x)</p><formula xml:id="formula_134">has no effect on Y . 39 Z Y W X D cX cW cZ cY c D c R → Y cX cW cZ cY c D c R x = Y cX cW cY c D c R x = Y cY c D</formula><p>A further example of no-influence argument of a different kind is the following.</p><p>Example 33. Consider a DisCoCirc model, with a verb such as likes in the phrase Alice likes Bob, represented by a channel. The word 'likes' will act only on the Alice and Bob wires, and as the identity on any other discourse referents, with the following representation. The resulting channel will only allow signalling between the Alice and Bob wires with all other wires only signalling to themselves. For certain forms of text input, this can induce desirable properties of the model. Given a collection of facts relating several agents, and arranged in a chronological order, such as Alice hired Bob and then spoke to Claire, we can consider a DisCoCirc model of the form below, which describes the resulting updates of information about each agent. If we wish to understand the final state of Bob, we see that this will not depend on that of Claire, due to the absence of signalling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>hired by speak</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bob</head><p>Alice Claire</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= hired by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bob Alice Claire</head><p>Intuitively, this is because the interaction between Alice and Claire happens after the final action relevant to Bob, and so should not affect them. <ref type="foot" target="#foot_34">40</ref>Recall from Section 7.1 that for conventional architectures such as fully connected neural networks, there typically are not any no-influence constraints that are enforced structurally. Hence, the kind of reasoning in terms of strict no-influence arguments is a benefit of models with non-trivial compositional structure, that is, where the connectivity in the diagram is not all-to-all.</p><p>We will see more examples of no-influence arguments shortly, combined with the forms of explanation we consider next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Diagram surgery</head><p>One way to understand the utility of a compositional structure is to see each internal wire in a diagram as a point where we may 'inspect' or 'intervene' on a process, in order to learn more about it. More generally, we can learn more about how a diagram leads to a particular output by acting on and altering fragments of the diagram, either to see how this affects the output or, broadly speaking, to inspect the process itself.</p><p>Following the terminology of <ref type="bibr" target="#b76">[JKZ19]</ref>, given any diagram D in a category C, by an act of diagram surgery we mean the generation of a new related string diagram D ′ in C, where some fragments of D have changed. When D is itself seen as describing a compositional model, then this amounts to an alteration to a new (typically closely related) model. A common case of surgery is that which alters a single process (box) in our original diagram, which we may illustrate as follows.</p><formula xml:id="formula_135">→ f f ′</formula><p>Note that in the above illustration the blank boxes may themselves have complex internal structure, and that the new box f ′ may have distinct input and output types from f , as may the overall diagram D ′ from D. Alterations to a diagram can provide various forms of explanations about a model, and the kinds they provide will vary depending on the level of interpretability of the variables on which the surgery acts. Let us now discuss several useful forms of diagram surgery.</p><p>Local surgery. Suppose we have some variable V occurring within a diagram, which lacks a concrete or even abstract interpretation, but which we would like to learn more about. While we cannot interpret the variable directly, we can instead relate it to further variables C i via some channels of the form (c i :</p><formula xml:id="formula_136">V → V ⊗ C i ) n i=1</formula><p>, where now each variable C i does have a concrete interpretation for each of its states. Applying each such channel amounts to the kind of local surgery on our diagram depicted below.</p><formula xml:id="formula_137">c i C i V → V V (57)</formula><p>In general, the channels c i may alter or 'disturb' the state of V ; for example this will typically be the case for local surgery in quantum models, which we focus on in Section 10. However, in typical classical models, we are free to copy (sharp) states without disturbing them. Hence we may simply observe any variable through channels c i of the following form, which amount to copying the variable and applying some channel</p><formula xml:id="formula_138">d i : V → C i . c i V C i V = V C i V d i<label>(58)</label></formula><p>For a representation space V inside a neural network, the channels d i may be given by a set of classifier networks, where each classifier d i maps a state v of V to one d i (v) of a finite set of interpreted classes C i , which are taken to describe aspects of V .</p><p>Example 34. Consider an RNN, which makes use of an uninterpreted representation space X and produces an output by applying a classifier from X to a finite set of interpreted classes C. More generally, we may train the same representation to come with a family of classifier maps C i each of which is used in some task. To understand how the state of X evolves as the RNN is applied, we can probe the wire using any of these classifiers d i as below.</p><formula xml:id="formula_139">movie not good ⋆ d i C i d i C i d i C i</formula><p>Now rather than only receiving a class for the final output, we can see how the class is altered as each word of the text is read, to indirectly interpret the model's representation. For example, we may see how the overall sentiment C i = {positive, negative} of the movie review, produced at the final output, is altered by each word such as 'not' and 'good'.</p><p>Interventions. Another major form of surgery lies in deliberately altering the value of a variable, to inspect the changes this causes to the model. Firstly, given any variable V, we can intervene by applying some fixed channel c i to V , replacing the V wire in our diagram by c i . <ref type="foot" target="#foot_35">41</ref> Provided an interpretation for this intervention c i itself is given, we can observe the resultant changes to V and to the entire model in order to attempt to interpret V .</p><formula xml:id="formula_140">→ c i V V V V</formula><p>Indeed we already met interventions in the context of causal models in Sec. 5.11. The special case of a do-intervention do(V = v), in which the variable V is set to some particular fixed state v, can be seen to be equivalent to the following choice for c i , in which we simply discard V and replace it with v.</p><formula xml:id="formula_141">c i V V = V V v</formula><p>More broadly, we saw how we can go beyond do-interventions, defining an intervention on a causal model to be any replacement of the mechanism for a variable (or multiple variables) so long as the overall model remains a valid causal model. At the level of the network diagram describing the causal model, this involves a form of diagram surgery <ref type="bibr" target="#b76">[JKZ19]</ref> in which we replace the box for mechanism c i for variable X i with another for a new mechanism c ′ i :</p><formula xml:id="formula_142">c i . . . X i Pa(X i ) → c ′ i . . . X i Pa ′ (X i )<label>(59)</label></formula><p>We then connect all inputs of c ′ i to the new parents of X i via copy maps. For a detailed discussion of interventions on causal models in string diagrammatic terms, see <ref type="bibr" target="#b90">[LT23]</ref>.</p><p>By analogy, we can think of any diagram surgery that replaces boxes within a diagram as a form of 'intervention' on the model. This general notion of intervention is in spirit the same as that of the causal model framework, but now the class of diagrams and (hence models) is generalised, no longer restricted to network diagrams (i.e. causal models).</p><p>Example 35 (Intervening on inputs and CFEs). A special case of do-style interventions is simply the altering of the inputs to a given process. This forms the basis of many standard XAI methods, such as the CFEs discussed in Section 7.2. Modifying the inputs to a process f is the following basic form of surgery:</p><formula xml:id="formula_143">f x 1 x n . . . → f x ′ 1 x ′ n . . .</formula><p>In this light we can thus see explanations that involve altering internal components of a model as generalisations of such explanations.</p><p>Diagram surgery as such can always be applied to a model. What makes it explanatory is when the surgery concerns variables (wires) or larger fragments that have an interpretation and are replaced with new fragments that also have an interpretation. This distinguishes compositionally-interpretable models from black-box models with uninterpreted intermediate components. In particular, we may combine diagram surgery with influence arguments to provide explanations for the outputs of CI models, as in the following example.</p><p>Example 36 (Influence constraints and Surgery). Let us return to our DisCoCirc model of the text Alice hired Bob and then spoke to Claire from Example 33. Suppose we follow with a question Is Bob employed? and as expected receive the answer yes, and would like to explain how this answer was reached. As before, the structure of the diagram tells us that there will be no-signalling from Claire to Bob and so restrict analysis to the left of the diagram. To check that the model arrives at the answer in the way we would expect, we can apply diagram surgery by either removing the process for hired or by replacing it with another word such as fired, and verify that these lead to a distinct answer.</p><p>employed? Bob = no = fired by employed?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bob Alice</head><p>How does this analysis compare with simply inserting the alternate texts as new inputs to a fixed black box? Firstly, the influence constraints have provided guarantees that certain parts of the model won't affect the output (the Claire wire). Secondly, we may as a result restrict to the sub-diagram above, which provides a localised explanation and may be more efficient to analyse than the entire model.</p><p>Example 37. Consider a model with inputs X 1 , X 2 and output O which on inputs (x 1 , x 2 ) gives output o.</p><p>The standard form of (counterfactual) explanations may for instance be of the form 'if we change the value of x 2 to x ′ 2 we get the output o ′ instead'. When the model comes with further interpreted compositional structure, we can give a richer form of explanation still. Suppose the model factorises as an open causal model via intermediate variables V 1 , V 2 as below, which have an interpretation. Also suppose that inputting (x 1 , x ′ 2 ) yields in particular V 2 = v and suppose that through intervening on V 2 we discover that regardless of</p><formula xml:id="formula_144">V 1 in fact V 2 = v necessarily yields o ′ : O X 1 X 2 V 1 V 2 → v O V 1 V 2 X 1 X 2 = o ′ O X 1 X 2</formula><p>Thus we now have a more fine-grained explanation, telling us that the change of the output to o ′ really came down to changing the variable V 2 , which happens to depend on X 2 .<ref type="foot" target="#foot_36">foot_36</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Rewrite explanations</head><p>Our final notion of explanation from diagrams is the most direct, but also the most speculative. It is based on diagrammatic 'rewriting', a central aspect of the theory of monoidal categories, in which one applies successive equations between pieces of a diagram to prove that it is equal to another. We will aim to demonstrate how rewriting can be applied to models with interpreted compositional structure to provide explanations for their outputs, which we call rewrite explanations. Note however that the ability to give such arguments in practice remains to be demonstrated; we return to this at the end of the section.</p><p>Let us now motivate this approach. Consider a situation where a compositional model specifies an overall process f from inputs X 1 , . . . , X n to outputs Y . Suppose that given input x 1 , . . . , x n it produces output y, which we would like to explain. To do so, we first consider the 'internal structure' of the process f given by the compositional model, as in the first equality below. Next, suppose we know of some equations which we may apply to sub-diagrams of this diagram, as in the second equality below. By continually applying such rewriting we find we can arrive at the output y.</p><formula xml:id="formula_145">f X 1 Xn Y . . . = Y . . . x 1 x n x 1 x n = Y y x j . . . = Y . . . x 1 x j = (<label>60</label></formula><formula xml:id="formula_146">)</formula><p>Any compositional model could potentially allow for diagrammatic rewriting from the input to the output in this way. However, for this to qualify as an explanation for the output y, the structure in these diagrams must itself be interpreted . That is, the inputs X j and output Y should be (at least abstractly) interpretable, the states x 1 , . . . , x n concretely interpretable, and all of the components of the diagrams and sub-diagrams used in the above argument should be interpretable also. If that is the case, we consider the argument above as a rewrite explanation. Let us now spell this out formally. Explicitly, an interpreted diagram consists of wires labelled by variables V of the model for which the abstract interpretation I A (V) is defined, and boxes f :</p><formula xml:id="formula_147">V 1 , . . . , V n → W 1 , . . . , W m corresponding to morphisms f : V 1 ⊗ • • • ⊗ V n → W 1 ⊗ • • • ⊗ W m in C</formula><p>for which the concrete interpretation I C (f ) is defined. <ref type="foot" target="#foot_37">43</ref>We can now define rewrite explanations themselves. For any diagram D in C, we denote the induced morphism in C from its inputs to outputs by D = D .</p><p>Definition 39 (Rewrite Explanation). Given a compositional model and two diagrams D, D ′ in C, a rewrite explanation for (D = D ′ ) is given by a list of equations between interpreted diagrams</p><formula xml:id="formula_148">D j = D ′ j n j=1</formula><p>along with a rewriting proof showing that these imply that D = D ′ . Similarly, a rewrite explanation for an approximate equality (D ≈ D ′ ) is given by a rewrite proof using a collection of approximate equations between interpreted diagrams (D j ≈ D ′ j ) n j=1 .</p><p>The basic idea of a rewrite explanation is as outlined above in (60), but we will see many more forms of example shortly. Before this, we note two aspects. Firstly, the method involves looking 'inside' a model's overall process and so requires compositional structure. As such this method is unique to compositional models and cannot be applied to a complete black box in the sense of Sec. 2.1.1, that is, when the underlying process (model parameters) is unknown (e.g. in a proprietary system). Secondly, to justify the term 'explanation' we have required that the intermediary diagrams D j , D ′ j are themselves fully interpretable. Indeed, otherwise one could for example view an entire uninterpreted neural network as a diagram and write (hundreds of) diagrams tracing the input through each neuron in each layer, and this would count as an 'explanation' for the eventual output. Thirdly, note that at a diagrammatic level rewrites can be seen to involve surgery. However, in Sec. 9.2 surgery was used to reason about an inequivalent diagram or model, while in rewrite explanations diagrammatic fragments are replaced with ones that are equationally guaranteed to (approximately) represent the same morphism in C.</p><p>Remark 40 (Approximate equalities of diagrams). As well as strict equations D = D ′ we will consider approximations of diagrams D ≈ D ′ . The latter require the semantics category C to come with a notion of approximation f ≈ g for morphisms f , g : X → Y , which should moreover be quantified, and respected by composition, so that approximate rewrites of sub-diagrams can be extended to an entire diagram. For simplicity we will treat these informally here, but note that these approximations and their properties can be made formal, for example as done for quantum processes in <ref type="bibr" target="#b81">[KTW17]</ref> 44 .</p><p>Let us now meet a suite of toy examples to illustrate the idea of rewrite explanations.</p><p>Example 41. Consider the decision list model (24) from Section 5.4. Suppose that on a given input (s, a, p), consisting of sex s, age a, and number of priors p, the model predicts arrest within 2 years, i.e. outputs 'yes'. An explanation for this output could take as its sub-diagrams the following:</p><formula xml:id="formula_149">21-23 a = y 2-3 p = y</formula><p>These diagrams have an interpretation as the statements 'age is between 21-23' and 'has 2-3 prior arrests'. An explanation is then given by the following rewrite proof (using the standard properties of the ⋆ and 'first' boxes) that they lead to the output 'yes': This argument simply follows the logical structure of the model to reach the output. Hence in this case the rewrite explanation corresponds to the standard sense of 'intrinsic interpretability' for rule-based models, capturing how they intuitively give explanations by simply inspecting how they arrive at their conclusion.</p><p>Tracing through the inputs in other rule-based models such as decision trees provides simple examples of rewrite explanations in just the same way. Like rule-based models, interpreted causal models also allow for simple rewrite explanations by tracing inputs through the diagram, as in the following example. 44 One natural approach, used in <ref type="bibr" target="#b81">[KTW17]</ref>, is to have that each homset C(X, Y ) embeds into a normed space, with ∥g • f ∥ ≤ ∥f ∥∥g∥ and ∥f ⊗ g∥ ≤ ∥f ∥∥g∥. We then write f ϵ ≈ g whenever ∥f -g∥ ≤ ϵ.</p><p>Example 42. Consider the causal model for slipperiness of the floor outlined in Example 8. Here for simplicity we label the variables and boxes with their interpretations (see Rem. 9). Suppose that for the season 'autumn' as an input, the model predicts that the floor will be slippery. An example rewrite explanation could be given as follows. Rather than listing the sub-diagrams used we will merely give the rewriting argument, from which they may be read off. The following non-example shows how rewrite explanations are typically not available for black-box models whose internal components are not interpreted, such as typical neural networks or transformers.</p><p>Example 43 (Non-Example). Suppose we have a language model for deciding whether to grant a loan, which is only interpretable at the level of overall inputs and outputs. We observe that the model seems to consider employed homeowners to be 'reliable' and that 'reliable' applications should be given loans, in the sense that the following approximate equalities hold. employed homeowner ≈ reliable reliable loan?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≈ yes</head><p>Despite this knowledge, we cannot derive a proof that an employed homeowner should be given a loan, because the corresponding approximate equality relating all components may or may not hold in general for this sort of model: 45 employed homeowner ̸ ≈ yes loan?</p><p>Alternatively, one might instead try to give a rewrite explanation for a model such as a transformer, by starting from the diagram (27) showing the attention heads, as in Section 5.6, and tracing the inputs through to the output. However, without interpretations for the components of this diagram this could not yield a rewrite explanation in our sense.</p><p>In contrast, more richly compositionally structured NLP models do allow for such reasoning. The following example shows that even sequential composition, as featured in an RNN, can produce rewrite explanations.</p><p>Example 44. Consider a text sequence model as in Section 5.7 whose representation is used to determine whether an applicant is successful in applying for a loan. Suppose the input text states that the applicant is an employed homeowner, and suppose that their application is successful. A rewrite explanation for this output could include the following, where we find that following the information that the applicant is an employed homeowner, the model's representation is approximately that for a 'reliable' person. Secondly, we find that a 45 Structurally, this stems from a lack of relation between the 2-input and 3-input instances of the model. Example 45. Consider a text sequence model whose wires are internally structured in the manner of conceptual space models from Section 5.10, containing representations of foods in terms of the domains of colour C and taste T . Suppose the model answers that a yellow banana is not bitter. We might provide an explanation for this output by finding that the following equations hold, interpreted roughly as 'yellow is a colour', 'a yellow banana is sweet' and 'sweet is not bitter'. The first states that when an agent follows another they are in the same location. The second states that if an entity is in another entity, and we ask where the first one is, the answer is given by the location of the second one. These equations can give a rewrite explanation for why the model will always correctly answer that Alice is in the kitchen, as follows. It is natural to ask: where would the rules in (61) come from? One answer is that the model can be trained in a manner to encourage them to hold, that is 'enforce' them approximately by construction. Another is that they may simply be found to hold experimentally following training. Alternatively, they may be 'hard-coded' in an exact sense into the structure of the model, for example as follows. Suppose that each wire factorises in terms of a 'representation' wire, depicted in bold, and a 'location' wire depicted as a dashed line. Then the rules (61) follow automatically from the following structure for our keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>in</head><p>= where = follows =</p><p>Example 47. Consider a DisCoCirc model of the text describing a western film plot from <ref type="bibr" target="#b32">[Coe21b]</ref>, shown below, involving the characters Harmonica, Claudio, Frank and Snaky. Suppose that we ask the question Is Claudio alive?, and receive the answer n for 'no'. An explanation can be given as follows, where for brevity we only depict the rewrite argument itself, and for this example depict the initial states ⋆ supplied to each wire. In the first and third step we use the fact that the boxes are channels. In the second step we apply a is stored for each generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>follows in</head><p>Alice garden</p><formula xml:id="formula_150">= 0 0 R(θ A ) R(θ K ) R(θ i ) R(ϕ i ) + R(θ f ) R(ϕ f ) +</formula><p>The utility of such quantum AI models, practically trained in quantum machine learning (QML) setups, remains a subject of ongoing research. Suppose though there is a range of tasks in AI for which quantum models do prove valuable, then their interpretability may become an increasing concern just as for classical models.</p><p>More specifically, we claim that a compositional framework for interpretability, such as that offered here, will be helpful and in fact necessary to study interpretability of quantum models. Indeed, comparison against classical AI will be necessary and so a model-agnostic approach is required . Moreover, quantum models are naturally defined compositionally, being typically specified in terms of (quantum circuit) diagrams, and in fact formal diagrammatics has roots in quantum information and foundations <ref type="bibr" target="#b0">[AC04,</ref><ref type="bibr" target="#b25">CK18]</ref>. Finally, it has been argued that training compositionally offers a way to circumvent serious obstacles to training quantum AI models arising from the Barren plateaus in typical parameter landscapes. To train compositionally here means to train local components in simulation, and combine these into larger circuits at test time. An example of just such a compositional training setup is the recent quantum implementations of the DisCoCirc framework for QNLP [DBM + ng].</p><p>Interpreting quantum models. What, then, can our setup tell us about interpreting quantum models? Firstly, while one may attempt to argue that for specific domains, such as cognition or NLP, quantum semantics is especially natural to use and thus in a sense may be regarded as more interpretable, we will not do so here (for arguments for quantum models of cognition see <ref type="bibr" target="#b4">[BB12]</ref>). Instead we will simply claim that while a quantum implementation might provide computational efficiency, it is a model's compositional structure that underlies its interpretability in our sense.</p><p>Indeed, many of our arguments for the interpretability benefits of rich compositional structure apply equally to quantum models as to classical ones, as can any arguments which only assume semantics based on symmetric monoidal categories. <ref type="foot" target="#foot_39">48</ref> For example, the notion of abstract interpretation, and explanation methods based on influence constraints, diagram surgery, and rewrite explanations may all be carried over to quantum models, so that many examples from Section 9 could be taken with quantum semantics. Examples in the literature of quantum models with abstract interpretations include the QNLP models in [MTdFC23, LPM + 23], as well as the quantum conceptual space models in <ref type="bibr" target="#b136">[TSZC24]</ref> (see Example 18).</p><p>Despite this, one aspect of interpretability which may pose more challenges for quantum models is the notion of a concrete interpretation. Recall that a variable has such an interpretation when we can assign specific meanings to each of its states. For a variable V with classical semantics in R, abstractly interpreted as some feature or concept, if V is said to also have a concrete interpretation then it is usually obvious and uncontroversial what it means to interpret each state r ∈ R -this could be the 'degree' to which this feature is present, the spatial position of an object, a time stamp or a person's age in years etc. In contrast, for a variable V with quantum semantics given by a d-dimensional complex Hilbert space H, its set of states are all corresponding density operators (i.e. trace-1 positive operators on H, see Sec. 3.3). Hence, even if V is abstractly interpreted as some feature or concept, it is not clear what a corresponding concrete interpretation of V 's states is in general. <ref type="foot" target="#foot_40">49</ref> For the most elementary quantum system given by a single qubit, i.e. where d = 2, each state can be parametrised by three real numbers, which can then be visualised as the two angles and the radius of a point in the so called 'Bloch ball'. Now, while some variables may have a natural Bloch ball interpretation, such as the example of an RGB colour sphere as in <ref type="bibr" target="#b136">[TSZC24,</ref><ref type="bibr" target="#b149">YLH21]</ref>, in general providing an interpretation for each point of this ball is not obvious. Note, however, that such interpretations are provided for a number of states of a qubit in the quantum DisCoCirc model in [DBM + ng].</p><p>The challenge of concrete interpretations in the quantum case has a further aspect, namely when considering composites of variables. For classical deterministic models, we can interpret a state (v, w) of a composite V × W as simply a pair of interpreted aspects corresponding to v and w.<ref type="foot" target="#foot_41">foot_41</ref> However, the combination of quantum systems H, K is described by their tensor product H ⊗ K. A typical state of this composite is entangled, meaning it cannot be written as a product of states of each factor independently. <ref type="foot" target="#foot_42">51</ref> Even when provided with concrete interpretations for (the individual states of) all variables of a model, how are we to interpret such entangled states of their composite?</p><p>Despite this problem with concrete interpretations, it may simply be that the other aspects of interpretability listed above, nonetheless suffice to deem a quantum model interpretable. In particular the special case of diagram surgery given by the application of 'local surgery' from Section 9.2, may be especially helpful for quantum models. Given a quantum variable V = H we can imagine training a model to come with a family of channels m i : H → H ⊗ C i , where C i is a finite set of concretely interpreted classical outcomes. A channel of this form is called a quantum instrument. These would allow us to 'probe' the quantum system, whose states may not be concretely interpretable, by applying such an instrument and observing the classical outcomes C i . For example, we may have a variable representing images and probes relating this system to interpretable aspects of the image such as colour, brightness, or shape, encoded as classical variables. This notion of 'probing' a wire in a compositional model as a form of mediated interpretation may be useful in endowing quantum models with interpretations, by relating them to classical attributes, and would be interesting to explore in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusions and future directions</head><p>In this article we have presented a compositional viewpoint on AI models and their interpretability, employing the formal notion of a compositional model. Essentially any AI model, including deterministic, probabilistic or quantum models, can be seen as a compositional model, providing both the grounds for meaningful comparison of different models, and the basis for our notion of an interpretation -namely, the components of its compositional structure.</p><p>While any model can be analysed from this viewpoint, it also naturally leads to the stronger notion of compositionally-interpretable models -ones with a complete, abstract interpretation in our sense. Standard neural networks, transformers and other black-box models do not (as such) yield CI models, but classic instances of intrinsically interpretable models, as well as causal models, amongst other kinds, do. The interpretability of CI models may be further distinguished and made stronger in a number of ways, all of which are rooted in the compositional view: a model's interpretation may be concrete; the status of its compositional structure may license certain rules for computing with the components (the compositional framework); the structure may or may not directly correspond to structure in the phenomenon; it may allow for explanations in terms of structural influence constraints; or may facilitate rewrite explanations in terms of diagrammatic equations.</p><p>Overall, we suggest that taking a compositional view may help to broaden both the search for interpretable AI models, and the kinds of explanations they can provide. There are many directions in which one may continue this research in the pursuit of interpretable compositional models.</p><p>Finding more CI models. Broadly, we can say that the space of CI models remains to be explored in full detail. Our paradigm examples include intrinsically interpretable models (see Secs. 5.3, 5.4 and 8), but also causal models (Sec. 5.11) and DisCo models in NLP (Sec. 5.9), where compositional structure comes respectively from causal and grammatical structure. However, further kinds of compositional models should be developed in the future. Potential examples may include cognitively-inspired models such as conceptual spaces (Section 5.10), which in particular should be enriched beyond possessing merely states (instances) and effects (concepts) but also 'conceptual processes' between domains.</p><p>In Section 10 we discussed aspects of interpretability related to quantum models. For now, perhaps the most pressing area in quantum AI is simply the development of effective models, which may become more widespread as quantum hardware improves. Ideally, from our perspective, one would develop quantum models whose compositional structure is directly related to human intelligible terms so as to yield a CI model. Research towards such models within QNLP includes for example [DBM + ng].</p><p>Learning and relating compositional structures. A major question in the pursuit of compositionallyinterpretable models is the following: where does the compositional structure come from? There are cases in which one may indeed specify the signature and hence structure category S of a model 'by hand'; for example in a data science scenario with given interpreted variables, in which one explores either intrinsically interpretable models for a particular task or specific causal relations between them. In some other cases it is known how to apply structure to the data, such as parsers which overlay raw text with grammatical structure, which may then be converted to string diagrams.</p><p>However, for many applications in AI one hopes to instead train a model from unstructured data. If the resulting model is to possess interpretable variables and processes on them, how should these arise? In answering this question there may be a spectrum of approaches in how much supervision we give the model. It is perhaps naive to expect a model to learn human interpretable variables with no input of human concepts or knowledge of some form or other in the training, but one aims to reduce this as much as possible. We note that a special case of this problem is in the learning of causal variables and their causal structure from (to some extent) unstructured data, which in causal ML is known as causal representation learning [SLB + 21b]. Solving this, and now the broader problem of compositional representation learning, remains a holy grail for structured AI, and a major research goal for the future.</p><p>While we have largely focused on defining and giving examples of compositional models and their interpretations, there is much more mathematical machinery that could be developed to allow us to relate compositional models to one another. This would be interesting to explore in future work, building on standard notions of model relations from categorical logic. An example of the kind of interesting relation one may study is one that relates models at different levels of abstraction or fine-grainedness. App. C.1 indicates some first steps in this direction, by introducing the notions of morphisms of compositional models and the related refinement of diagrams. The latter formalises the intuitive idea of refining a model or process, viewed as a diagram, to another typically more detailed one with further variables and processes. Special cases include specifying a (causal) model of some distribution or process, refining a causal model to an FCM, or specifying a precise computational implementation of a given model.</p><p>Related to both the refinement of models and the learning of interpretable structure is the concept of causal abstraction, discussed in App. C.2. This concerns an abstraction relation between causal models, which does not just require equality of diagrams and models, but also constrains how interventions at the different levels relate to each other. In future work it would be interesting to further develop the categorical theory behind causal abstraction, and relate it to our theory of refinements of models. More broadly, is there an interesting notion of 'abstraction' for general compositional models, based on some form of diagram surgery?</p><p>The potential of diagrammatic equations. In Section 9.3 we introduced the new notion of rewrite explanations as an explainability technique applicable to CI models. More work is needed to demonstrate that such explanations can be used in practice. In particular, how can we train models coming with the kinds of equations used in rewrite explanations? Should these equations be imposed to hold strictly in the setup of the model, encouraged to hold approximately via minimising a loss function, or simply found experimentally after training? As well as this, more work is needed to demonstrate the use of rewrite explanations on trained models, and develop the technique to a fully-fledged explainability tool if this is viable, complete with software for generating and examining explanations.</p><p>A related idea in this spirit of transparency from equations is to develop transparent models by specifying their behaviours through equations that their components are trained to (approximately) satisfy. Rather than considering equations that merely feature in the explanation of particular outcomes, here one considers them as making the entire compositional model more functionally transparent. An example we have already met are the equations ( <ref type="formula" target="#formula_52">16</ref>) and (17) for encoder-decoder models, which the loss functions of VAEs enforce approximately in training. Another example are the get-put rules developed for lenses in database theory, which are now widely studied categorically [BPV06, FGM + 07, JRW10]. These consist of a 'get' map, which given a variable X returns values of some type C, and a 'put' map which allows to 'insert' a new state for this value. These satisfy various equations which define their mutual behaviour and interactions, including the following:</p><formula xml:id="formula_151">put X C X get X = X X put X C get C = C C X</formula><p>The article [RLH + 24] aims to give a compositional approach to specifying ML models using equations which constrain loss functions, just as in these examples. Equational constraints such as these could help to align a model to have desirable properties, for example by removing harmful biases and allowing one to reason about the way the model will behave, as well as possibly facilitating rewrite explanations of specific outcomes in our sense.</p><p>Promising links to related fields. As discussed in Section 2.2, perhaps the currently most prominent area of AI research that is centered on 'compositionality' is in the exploration of compositional behaviour of ML systems. These include the ability of a trained model to compositionally generalise; for example to generalise from few training examples to many new unseen ones, by combining or composing known examples. It is natural to expect compositionally (i.e. categorically) structured models to come with built-in compositional behavioural abilities, at least in ways which relate to their explicit structure. For example, DisCoCirc models may be able to apply strict logical reasoning to large texts even when trained only on small fragments, as demonstrated in the quantum model of [DBM + ng]. However, spelling out precisely in what ways explicit compositional structure leads to compositional behaviour remains a major direction for future work.</p><p>Our perspective should also be tied in future to a number of other structural perspectives on AI, including the area of geometric deep learning <ref type="bibr" target="#b5">[BBCV21]</ref>, which could provide further examples of structured compositional models. In this approach the notion of 'structure' is largely present in symmetry groups on the training data, which are respected by the model (through 'equivariance'); as in the classic example of translational equivariance of a Convolutional Neural Network (CNN). In fact, recently a categorical perspective and generalisation of geometric deep learning has been offered [GLD + 24] which would be interesting to compare with our work.</p><p>A complementary area of applied category theory may be called categorical learning, consisting of categorical descriptions of ML systems and their training setup, including the learning process, parameter updating and optimization. A number of recent works have provided categorical accounts of machine learning in terms of bidirectional transformations [FST19, SGW21, CGG + 22]. In future work, it would be interesting to explicitly connect our viewpoint with these approaches which take the training process into account. model, which explicitly shows the variables used in each question: 400 days since 2011? 100 days since 2011? Temperature ≥ 11?</p><formula xml:id="formula_152">Y N Y N Y N o 1 o 2 o 3 o 4 + O D T</formula><p>Here the first question is passed 'true' to its control wire, omitted from the diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Stairs readers</head><p>A stairs reader may be equivalently described as an RNN where the transition function takes the form:</p><formula xml:id="formula_153">f Σ X X = g Σ X X e X where e := X Σ f Σ X ⋆</formula><p>for the, usually linear, stairs function g. For any RNN, the function e above sends each word w to its state representation and may be called the embedding map. From this the state representation of a sequence (w 1 , . . . , w n ) shown in section 5.7 follows. Hence a representation of a phrase is given by repeatedly applying the stairs map to the vector embeddings of each word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 CCG models</head><p>In this section we consider compositional models based on the linguistic formalism Combinatory Categorial Grammar (CCG). This approach is due to <ref type="bibr" target="#b148">[YK21]</ref>, to which we refer readers for an introduction to CCG and more details.</p><p>Let Σ be a lexicon consisting of words w : t where each is assigned a type t within a CCG built from a set of basic types such as n (noun), s (sentence), etc. For example we may have Alice: n, sleeps : n ▷ s. Given a sequence of typed words, one may apply the rules of CCG, such as forward application (FA) or backward application (BA), to specify its grammatical structure and derive whether it forms a valid sentence.</p><p>For example, we can depict a derivation of how 'Alice likes Bob' forms a sentence as below. </p><p>Now let us see how to define a suitable structure category to give formal meaning to such a diagram. This is based on the observation that the rules of CCG are closely related to the structure of the following form of category <ref type="bibr" target="#b148">[YK21]</ref>. A monoidal category B is said to be biclosed when it is both left and right-closed. 53 This means for each pair of objects A, B there are objects A ⇐ B and A ⇒ B, such that morphisms with inputs A, B and output C are in bijection with those of either of the left or right-hand types below.</p><formula xml:id="formula_155">f ⇐ A C ⇐ B ⇐⇒ f A C B ⇐⇒ f ⇒ B A ⇒ C</formula><p>As shown in <ref type="bibr" target="#b148">[YK21]</ref>, all the basic rules of CCG exist in any biclosed category where, given types corresponding to objects t 1 , t 2 , the CCG type t 1 ▷ t 2 corresponds to the object suggested by the notation, and similarly for ◁. In particular, any CCG lexicon Σ generates a free bi-closed category FreeBi(Σ). The variables are the basic types n, s, from which all further types are constructed using ⇐, ⇒, and there is a generator state w of t for each word w of type t in Σ. For example, there are generators given by the state Alice of n and likes of (n ⇒ s) ⇐ n as in (62). Then using the bi-closed structure, any diagram for a CCG derivation, such as (62), becomes a morphism in this category.</p><p>To now give semantics to CCG derived sentences, we must give a model of this structure, as follows.</p><p>Definition 48. Let Σ be a CCG lexicon. By a CCG model of Σ we mean a compositional model where S = FreeBi(Σ), C is biclosed, and -is a biclosed monoidal functor.</p><p>The fact that -is bi-closed means t 1 ⇒ t 2 = t 1 → t 2 and t 1 ⇐ t 2 = t 1 ⇐ t 2 for all types. Explicitly then, to give a CCG model amounts to specifying a biclosed category C, an object for each basic type, e.g. n = n , and a state w of the appropriate object in C for each w : t in the lexicon, e.g. a state Alice of n and a state likes of (n ⇒ s) ⇐ n, in C. Then all other CCG types and rules come with an automatic meaning in C from the bi-closed structure, and so a diagram such as (62) in S is mapped to the 'same' form of diagram in C.</p><p>Example 49. One commonly used semantics is the category C = FVec of finite-dimensional real vector spaces and linear maps, which we saw in Example 14 was compact. In fact, any compact category is bi-closed with A ⇒ B = A * ⊗ B and A ⇐ B = A ⊗ B * . Alternatively, the category Set of sets and functions is an example of a bi-closed category that provides a 'functional' and 'truth-theoretic' form of semantics for CCG models.</p><p>DisCoCat and CCG models are closely related, since rigid monoidal categories are a special case of biclosed ones, which we outline in Appendix A.5.</p><p>CCG phrase structure models We can weaken the requirement that our categories are bi-closed to more readily obtain models from a CCG lexicon Σ, at the expense of having models which no longer obey the combinatorial rules of CCG. Models of the following kind have been studied in the CCG case by Hermann et al. <ref type="bibr" target="#b66">[HB13]</ref> and in the more general phrase-structure case by Socher et al. <ref type="bibr" target="#b114">[SBMN13]</ref>.</p><p>By a CCG phrase structure model of Σ we mean a compositional model with a variable t for each CCG type, a generator state w of t for each w : t in Σ, and a generator for each possible CCG rule and set of types to which it may be applied. Thus, by construction, we have all the generators needed to give meaning to a diagram such as (62), without requiring the category to be bi-closed.</p><p>Any CCG model induces a phrase structure model via its bi-closed structure, where now t 1 ▷ t 2 = t 1 ⇒ t 2 etc. For a general CCG phrase structure model, however, there is no longer any enforced relation between a type such as n ▷ s and n, s, with the former being 'just a name'.</p><p>Example 50. A CCG phrase structure model in NN amounts to specifying a vector space R d[t] (i.e. a dimension) for each CCG type t, a vector w ∈ R d [t] for each w : t in Σ, and a neural network for each typed CCG rule. A sentence such as 'Alice likes Bob' is first mapped to a CCG derivation by a parser, and then to the resulting state given by the composite (62).</p><p>53 Formally, there are natural isomorphisms</p><formula xml:id="formula_156">B(A, C ⇐ B) ≃ B(A ⊗ B, C) ≃ B(B, A ⇒ C).</formula><p>Since specifying a morphism (e.g. neural network) for every typed CCG rule may be cumbersome, it is natural to weaken this structure further. Firstly, one can ignore the types on words, so that there is a single variable and every input or output to a generator is of this type, with a single generator for each rule type ('FA', 'BA' etc.). Then diagrams appear as in the left-hand below. Secondly, one may replace all rule generators with a single generator denoted 'UNIBOX', so that diagrams appear as in the right-hand below. Interpretation Similarly to DisCoCat models, in a CCG model the variable t for each CCG type has an abstract interpretation as that type, and the generator w for each word has an abstract interpretation as (the meaning of) that word. Each generator FA, BA, . . . for a CCG rule has an abstract interpretation as encoding this form of grammatical composition. Combining these, we can informally say that we interpret a sentence diagram such as (62) as encoding the meaning of the sentence, complete with grammatical structure.</p><p>A CCG phrase structure model similarly has an abstract interpretation for each word and composition rule, though now all variables t receive the same interpretation (as simply 'a word'), since all word types coincide. Since multiple CCG rules are mapped to the same semantics, their interpretations also coincide, in the case of a 'UNIBOX' model reducing to a single interpretation (as 'word composition').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Relation between DisCoCat and CCG models</head><p>Any rigid monoidal category is a special case of a biclosed category, allowing DisCoCat models to also be used as CCG models. Here we sketch this briefly; for more details see <ref type="bibr" target="#b148">[YK21,</ref><ref type="bibr" target="#b38">dF22]</ref>. Given any CCG lexicon Σ we can specify a corresponding pre-group grammar Σ ′ by mapping each type t 1 ▷ t 2 to t l 1 • t 2 and t 1 ◁ t 2 to t 1 • t r 2 . Similarly, any rigid category forms a biclosed category by setting A ⇒ B = A l ⊗ B and A ⇐ B = A ⊗ B r . For example, the morphism in a biclosed category corresponding in CCG to the forward application rule in CCG is equal to the following. (63) Relations such as the above can be seen as a major advantage of both CCG models and DisCoCat models, which make use of categorical composition. In contrast, generic CCG phrase structure models will typically not be faithful to such structural features of grammar; for example, the two representations in (63) will not coincide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Higher order morphisms in DisCoCirc</head><p>Here we make the notion of higher order morphisms used in DisCoCirc models more precise. Higher order maps have been studied in various forms <ref type="bibr" target="#b143">[WC22,</ref><ref type="bibr" target="#b111">Rom20]</ref>, and in particular they are often allowed to be applied not only to morphisms of some fixed type f : A → B but also to morphisms with any extra wires such as g : A ⊗ C → B ⊗ D, leading to further naturality conditions that they should satisfy. However, this ability is not required for DisCoCirc, and so we use only the following simpler notion.</p><p>In any category C, by a higher order morphism we mean a function h :</p><formula xml:id="formula_157">n i=1 C(A i , B i ) → C(C, D)<label>(64)</label></formula><p>for some objects A i , B i , for i = 1, . . . , n, and C, D. Thus, given morphisms f i : satisfying H F (h)(F (f 1 ), . . . , F (f n )) = F (h(f 1 , . . . , f n )) for all inputs f 1 , . . . , f n . Then a DisCoCirc model may be defined as a h.o.-functor as in Definition 16.</p><formula xml:id="formula_158">A i → B i</formula><p>Example 51. TextCirc Σ forms a h.o.-category as expected, with the generators of shape (39) as its higher order morphisms. Here each generator takes the appropriate shapes of text circuits as inputs and returns as output the text circuit in which they have been inserted into the box.</p><p>Example 52. Any closed SMC forms a h.o.-category where higher order morphisms (64) correspond to morphisms h ′ : (A 1 ⇒ B 1 ) ⊗ • • • ⊗ (A n ⇒ B n ) → (C ⇒ D), by setting h(f 1 , . . . , f n ) to be the morphism induced by the state h</p><formula xml:id="formula_159">′ • (f ′ 1 ⊗ • • • ⊗ f ′ n ) of C ⇒ D,</formula><p>where f ′ j is the state of A j ⇒ B j induced by f j : A j → B j . In this way any Cartesian closed category (such as Set) or any compact category is a h.o.-category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Conceptual encoders and decoders</head><p>In this section we will explore how concepts, similar to those of the conceptual space models of Section 5.10, can be implemented practically in encoder-decoder type setups as in Section 5.2. This is achieved by equipping a latent space Z with a collection of concepts, represented on Z in one of two ways: either as states, or as effects. Practical implementations are given by the Conceptual VAE and quantum concepts models introduced in <ref type="bibr" target="#b136">[TSZC24]</ref> (and in each of these models Z additionally takes on the structure of a conceptual space model, factorising in terms of domains).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts as generative.</head><p>Let us introduce the first such approach. We can enrich an encoder-decoder model by equipping our latent space Z with a collection of concepts (C i ) n i=1 , each given by a state of Z and with a name from some set of labels L = {1, . . . , n}. By a conceptual encoder-decoder model we mean a compositional model in a cd-category with variables X, Z as well as an additional concept labels variable L, and generators shown as below, such that all are represented as channels, and that is trained to approximately satisfying the following equation, similarly to an encoder-decoder model. Thus we have generators d, e for the decoder and encoder as before. The data is now labelled, given as a distribution over both inputs X and concept labels L, which typically is represented by a finite set. Rather than a single prior over Z, the channel C maps each concept label i ∈ L to a normalised state (i.e. distribution, for a model in FStoch) of Z, which we call the generative concept with label i:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><formula xml:id="formula_160">C i Z := C Z i L</formula><p>Passing this distribution to the decoder d : Z → X then yields a distribution over inputs X, from which we may sample to generate new instances of the concept. Note that marginalizing Z in (65) tells us that we can recover the original data by choosing a concept label i, according to their distribution in the data, and then generating instances using d • C i , in this way.</p><p>In the case where L is discrete and each concept does appear in the data, such a model is equivalent to providing an encoder-decoder model 'for each concept'. That is, for each label i, (X, Z, d, e, C i , Data| i ) forms an encoder-decoder model as in (16). Here Data| i is given by conditioning the data to those with concept label i, and the prior over Z is now given by C i .</p><p>Concepts as classifiers. Alternatively, as in a conceptual space model (Section 5.10) we can treat concepts as effects, and so use them as classifiers, i.e. maps which send each latent point z to a value C(z), indicating how well it fits the concept. From such values one may determine which concept best fits an instance. From this perspective we only require an encoder from some input space X into a latent space Z.</p><p>By a classification concept model we mean a model with variables X, Z, L and generators Data and e of the same form as a conceptual encoder-decoder model (but not necessarily with a decoder), and where the representation map is now given by an effect on Z, L as below, so that plugging in a concept label i ∈ L yields a concept C i as an effect on Z:</p><formula xml:id="formula_161">C i Z = C Z i L</formula><p>We can pass any data point x ∈ X to the encoder to produce a representation e • x over Z, and then apply each concept to obtain a scalar C i • e • x, representing how well each concept fits, similarly to (42). We typically assume such a model comes with a generator giving us a prior distribution over concept labels, intuitively telling us how likely each concept is. When X and L are classical spaces, we can then carry out conditioning to produce a classification map X → L which sends each input x in X to a distribution over possible concept (labels): For an input x ∈ X, this outputs the probability distribution P (C i |x) over concepts C i . Here the dashed box denotes probabilistic conditioning, treated in more detail in Section 6.2.</p><p>Connecting both views of concepts. In a classical classification concept model, the classificatory and generative views of concepts can be related by the following, where µ is a given measure over Z (e.g. the Lebesgue measure, when using a suitable category of 'continuous' probability channels). Example 53 (Conceptual VAE). In [TSZC24] a model is presented which is both a conceptual encoderdecoder model and concept classifier model, called the Conceptual VAE. The model makes use of both an encoder and decoder, relating a latent space Z to an input space of images X. The model is trained on data given by simple 2d shapes generated from the 'Spriteworld' software, such as the following:</p><p>The conceptual VAE uses a classical probabilistic semantics where each channel takes a Gaussian form. <ref type="foot" target="#foot_44">55</ref>Each concept C i may be seen either as a state of Z, given by a Gaussian distribution, or as an an effect on Z, via the corresponding density function, as in (67).</p><p>The latent space Z factorises as a conceptual space, with domains Z 1 = shape, Z 2 = colour, Z 3 = position and Z 4 = size, where an instance corresponds to a single domain. Each domain is taken to be one-dimensional, with Z i = R. Examples of concepts include single domain concepts such as red, square, large, as well as as 'entangled' concepts which depend on multiple domains.</p><p>The training is achieved by a variation of the standard ELBO loss for VAEs (18) to take into account the concept C i for each training instance as a prior over Z. This relates to minimising a KL-divergence for each concept in just the same way that the standard ELBO loss (18) relates to a KL-divergence between the two sides of (16).</p><p>Example 54 (Quantum concepts model). In <ref type="bibr" target="#b136">[TSZC24]</ref> it is shown how one may use the same dataset as the Conceptual VAE, consisting of 2d images from the Spriteworld software, to train a concept classifier model with a quantum semantics. Again Z forms a conceptual space with domains Z 1 = shape, Z 2 = colour, Z 3 = position and Z 4 = size, and the same concept labels. Semantics is now taken in C = Quant, so that each domain is associated with a Hilbert space Z i = H i , and the overall space Z = H 1 ⊗ • • • ⊗ H 4 with their tensor product. The encoder sends each input image in X to a pure quantum state (instance) over the Hilbert space Z. Each concept is represented as a positive operator, and is tested against the instance by applying an appropriate quantum measurement as in (42). The model is implemented using qubits via (simulated) quantum circuits.</p><p>Interpretation In a conceptual encoder-decoder model, or classification concept model, the input space X typically has a concrete interpretation, as does the set of concept labels L, both being applicable to the training data. In general Z has only a (rather broad) abstract interpretation as a representation space, e, d as an encoder and decoder, and C as sending each label in L to its concept. Each specific state or effect C i has an interpretation as that specific concept. The classification map (66) has a concrete interpretation as assigning each input to its likelihoods of being interpreted as each concept (label). In the examples above, the space Z has the further structure of a conceptual space, factorising in terms of domains Z 1 , . . . , Z n which each come with a specific abstract interpretation.</p><p>As remarked for conceptual space models, endowing an encoder-decoder with a set of concepts C i (either states or effects) can help us interpret the latent space Z further (while not necessarily providing a full concrete interpretation), by allowing us to understand a point z by seeing how similar it is to each concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Disentangled representations</head><p>A common approach to attempting to provide a latent space Z with further structure is to ask that it factorises as a product Z = Z 1 ⊗ • • • ⊗ Z n of suitably 'independent' and 'meaningful' factors Z i , similarly to the domains in a conceptual space. A literature in ML exists exploring the requirements and learnability of such 'disentangled representations'. In [HAP + 18] it is suggested that a disentangled representation can be identified by endowing each factor with a group of symmetries, with the product of these groups acting on Z.</p><p>To treat this in our setup, first observe that any group G may be viewed as a category with a single object ⋆ and a morphism g : ⋆ → ⋆ for each g ∈ G. Composition is group multiplication g • h , and the identity is the unit 1 ∈ G. Here every morphism is an isomorphism. Now a disentangled representation may be defined as a compositional model with variables Z 1 , . . . , Z n , Z and a generator g i : Z i → Z i for each group element g i ∈ G i , subject to all equations which hold in the group 56 , and the equation stating that Z = Z 1 ⊗ • • • ⊗ Z n . The model therefore specifies objects Z i in the semantics category C, whose composite Z comes with a G-action for the product group G = G 1 × • • • × G n . That is, for each g = (g 1 , . . . , g n ) we have an isomorphism:</p><formula xml:id="formula_162">g 1 Z 1 Z 1 g n Zn Zn . . . g Z Z =</formula><p>(68) such that the following hold:</p><formula xml:id="formula_163">h • g Z Z = g Z Z h 1 Z Z = Z Z</formula><p>56 More efficiently, one may merely specify a sufficient set of generators and relations for each group G i in the usual grouptheoretic sense, and these will generate the same structure category S.</p><p>Through the choice of semantics category one can impose further structure; for example, the category C = Vec of vector spaces and linear maps imposes that each group action is linear, forming a group representation [HAP + 18].</p><p>Interpretation Typically a disentangled representation comes with an abstract interpretation for each factor and symmetry group, for example that Z 1 is an image space with G 1 as translation, while Z 2 is a colour space with G 2 as rotations on the color wheel. In [HAP + 18] these symmetries are understood as 'in the world', known to us, and the representation manifests them in the model. This interpretation may or may not be concrete; for example we may know that Z 2 represents colour but not precisely the colour corresponding to each element z ∈ Z 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CFEs vs Pearlian counterfactuals</head><p>As mentioned in Sec. 7.2, although in the literature a CFE is often said to be a counterfactual in the causal model sense, just varying over the possible inputs of a function, on the face of it, does not look like a special case of the definition of a counterfactual in Eq. ( <ref type="formula" target="#formula_20">6</ref>.3) -a definition relative to a closed SCM. In addition to this immediate observation, it is worth having a closer look to see whether it perhaps holds true in a less obvious way.</p><p>In the definition of a counterfacutal, the conditioning on the observed facts that obtained in the actual world, effectively updates the probability distribution over the exogenous 'noise' variables U i , which in the context of data science problems typically are unobserved. However, if one knew the exact values of the U i that led to the observed facts, then one can of course equivalently just consider the hypothetical world 'on its own' with these corresponding values for the U i 's fed in. For instance, in Example 28, if one knew that it was precisely U A = u A and U H = u H that led to A = n, H = y then the counterfactual becomes a 'one-world' computation with the intervention in the world where U A and U H have been fixed correspondingly.</p><p>One might then wonder whether this is how a CFE is to be seen as a counterfactual. The model M in Eq. ( <ref type="formula">51</ref>) induces an open deterministic causal model -a very simple one with one single mechanism M for Y with parents X 1 , ..., X n . Crucially, the input variables X i are generally not statistically independent from one another -in contrast to the exogenous variables U i in an SCM -and there typically is a data distribution D over X 1 , ..., X n which has x in its support and which does not factorise. To try and make sense of the claim, one might suppose there is also an unknown SCM M X which generates D = M X , and such that if one made it explicit, then the alternative pair (x ′ , y ′ ) would always become a counterfactual in the strict sense relative to the combination of M X and M L . That is, one may wonder whether y ′ would (assuming M X and M L ) necessarily be the answer to the counterfactual question: "given that X = x and Y = y in the actual world, what would Y have been had X = x ′ ?". However, this is not the case as shown in Eq. ( <ref type="formula">69</ref>), where we factorised the SCM M X into the deterministic part F X and the state L X on its exogenous variables (see Sec. 6.3). The inequality is essentially due to the following. The question is whether the alternative input x ′ , differing from x only in component x ′ i , can arise through intervening only on X i while allowing all other input variables X j to be untouched and evolve according to their usual deterministic evolution. The answer is that this is not always possible, depending on M X , and in particular on whether X i has descendants in the causal structure of M X , i.e. on causal assumptions not part of the model M L . As a result, the pair (x ′ , y ′ ) does not constitute a well-defined counterfactual relative to just M L -neither on the face of it, nor can one fill in the details by envoking an implicit model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head≯ =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Relating models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Refinements of models</head><p>A first kind of relation between compositional models, which is natural to consider, is that of refining, that is the refining of variables and/or processes between the variables. Refinements include, for example, the notions of more fine-grained (causal) explanations and concrete (neural) implementations. The notion of refinement in particular formalises the following intuitive notion. Consider a morphism f in C with specified inputs and outputs, viewed as a diagram: Explicitly, this means that D is again a diagram in C with D = f . When variable-preserving it futher means that its inputs (X ′ i ) n i=1 have X ′ i = X i = X i , and similarly for its ouputs. Hence the diagram D has 'the same' inputs and outputs as f but has potentially further structure 'internally'.</p><p>Example 56. Suppose we have an input-output model given by a morphism f from inputs X to outputs Y , and then specify that these factorise in terms of variables X 1 , . . . , X n to Y 1 , . . . , Y m . Then we obtain a (non variable-preserving) refinement of diagrams expressed by the first equality below. , and suppose we refine this to an FCM F with V as its endogenous variables in the usual sense. This corresponds to a morphism M → F where we identify the X i and their semantics X i in both models and map each mechanism c i to the composite of f i and λ i as syntax, such that the semantics also matches as expected:</p><formula xml:id="formula_164">f i X i λ i . . . U i c i X i . . . = Pa(X i ) Pa(X i )</formula><p>In other words, the left-hand diagram in C is refined to that on the right, for each mechanism in M.</p><p>We can depict the whole refinement through an equality of diagrams where we understand this to mean that variables labelled the same on both sides are identified and each dashed box on the left corresponds to the respective dashed box on the right. Again returning to our earlier smoking example from Ex. 19 such a refinement may be depicted as: </p><formula xml:id="formula_165">f B f A A f S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Causal abstraction</head><p>Rather than, as with refinements, going down in the level of abstraction or fine-grainedness, let us now consider a prominent example of a relation that concerns the other direction. As we saw in Sec. 7 and also Sec. 8, an ideal kind of model is one with explicit causal structure between interpreted variables which reflects that of the phenomenon in the world. Yet this poses a challenge: such models are notoriously hard to come by. An interesting approach to arriving at such models is via causal abstraction. Importantly, and as we will see shortly, this is not just 'looking at refinement from the other direction', but a much more constrained and hence stronger notion. Causal abstraction is a relation between two causal models, originally defined independently from ML in the philosophy of causation and causal modeling [CEP16, CEP17, BH19, BEH20]. While there are various versions of the concept, roughly a 'high-level' causal model H is a causal abstraction of a 'low-level' causal model L when there is an ('abstraction') map from the low to the high-level that plays well with the causal structure, so that interventions on H have the same effect as on L once suitably translated across levels. The intuition is that if L is an NN model trained on some task, seen as a deterministic open causal model, and H captures our causal knowledge of the phenomenon that underlies the task, then the causal abstraction relation gives a strong kind of interpretability for L. <ref type="foot" target="#foot_46">58</ref></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: String diagrams for (a) decision tree, (b) neural network with layers of size 3, 2, (c) simplified transformer, (d) causal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagrams for text representations in (a) recurrent neural network (b) DisCoCat model (c) DisCoCirc model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Meaningful processes in various frameworks. (a) Applying inputs to a simple input-output model. (b) Conditional probability P (Y |X, Z) in a statistical model. (c) Do-intervention on a causal model. (d) Counterfactual distribution for a functional causal model M = F • L, where F and L denote the deterministic part for the endogenus variables and the product distribution over the exogenous variables, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Example argument showing that A cannot influence D for a model of the left-hand form. (b) Illustration of diagram surgery in which we replace the component f of a diagram with f ′ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Toy examples of rewrite explanations, in which the equations used are implicit in the rewrite steps.(a) A DisCoCirc type model, where we explain why Alice is with Bob in the Garden, where is Alice? returns as its answer the location of the garden. The equations used in the rewriting express that if X is in/with Y then the answer to Where is X is simply Y. (b) A conceptual space type model, using information that yellow bananas are typically sweet to explain why they are output as tasty. The equation implicit in the first rewrite captures that a yellow banana is also sweet; in the second it states that sweetness on its own ensures tastiness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Neural (left) and quantum (right) implementations of a text circuit (centre).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Diagrams. Often one can specify a model simply by drawing a single string diagram, where the variables and generators correspond to the wires and boxes. To make this precise, let D be a string diagram of a given kind. By a compositional model of D in C we mean a compositional model M in C whose signature G D has the wires in D as its variables and the boxes in D as its generators. The model M is understood to come with the diagram D itself as a distinguished morphism in S. We will also refer to the pair of a diagram D with a given model in C as a diagram in C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Furthermore the model comes</head><label></label><figDesc>with the fourth diagram above as a distinguished morphism in S. Specifying the semantics of the generators G in (10), i.e. the representation functor -, amounts to specifying corresponding objects A, B, C, D, E of C and morphisms in C of the form: A = A and a = a etc. The functor -then extends this mapping to give any morphism (i.e. diagram) D in S a representation as a morphism D in C. For example, if D is the fourth diagram in (11) then D is equal to the morphism (9) in C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Definition 13.<ref type="bibr" target="#b35">[CSC10]</ref> Let Σ be a pregroup grammar over some set of basic types B. A DisCoCat model of Σ is a compositional model of S = Free Rigid (Σ) in a rigid monoidal category C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Example 18 .</head><label>18</label><figDesc>In [BCG + 19] conceptual spaces are studied via the category C = ConvRel of convex relations. Here the objects are convex sets, i.e. sets equipped with operations (x, y) → p • x + (1 -p) • y for p ∈ [0, 1], with A ⊗ B = A × B and I a singleton set. Morphisms R : A → B are convex relations, i.e. subsets R ⊆ A × B which are convex, meaning they are closed under convex combinations. In particular, effects, i.e. concepts as in (40), correspond to convex subsets of a product of domains, just as concepts are defined by Gärdenfors [Gar04].The authors give examples of the domains Z 1 = taste, Z 2 = colour and Z 3 = texture. The domain taste is represented by a simplex, and colour by an RGB colour cube [0, 1] 3 , as below. The right-hand image shows an example concept yellow on the colour domain. A toy example of a conceptual space Z for food is then defined as a product of these domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A taste domain with concept sweet highlighted, a colour domain, and a concept yellow. Images from [BCG + 19].</figDesc><graphic coords="45,118.57,611.66,84.75,75.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>This data defines a marginal distribution over the outputs: P (S, L, A) = B P (L|S, B, A)P (S|B)P (B|A)P (A).It turns out that such a model can be understood as a compositional model induced by a single diagram. This is based on a correspondence between DAGs and the following class of string diagrams. Definition 20. A network diagram is a string diagram D built from single-output boxes, copy maps and discarding effects:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Definition 21. [LT23] An open causal model (resp. causal model) M in a cd-category C is a compositional model of a network diagram D M in C channel (resp. with no inputs). We call the network diagram D M , or the equivalent open DAG G M = (G, I, O), the causal structure of the model, and the variables which form the inputs (resp. outputs) of D M the inputs (resp. outputs) of the model. We call each generator of the model a (causal) mechanism. A causal model M in FStoch is hence equivalent to a CBN. The induced state in FStoch over all variables is equal to the distribution P (V ) = n i=1 P (X i | Pa(X i )), and M to the marginal P (O) over outputs. By definition, a causal model amounts to a network diagram D and cd-functor Free(G D ) → C channel . This functorial view was noted by Jacobs, Kissinger and Zanasi in [JKZ19], building on Fong's account of Bayesian networks [Fon13]. Example 22. For the DAG G from example (19) the equivalent network diagram D G is shown below. model is given by a model of this diagram in a cd-category C. Thus such a model has variables A, B, L, S and generators c A , c B , c S , c L , represented by corresponding objects A, B, S, L and channels c L , c S , c B , c A in C as above. For C = FStoch this is a CBN as in Ex. (19). Example 23. The diagram below depicts an open DAG over V = {X 1 , . . . , X 5 } with inputs I = {X 2 , X 3 } and outputs O = {X 3 , X 5 }, as well as the equivalent network diagram. An open causal model M of this form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>In particular the parents of X may change. See<ref type="bibr" target="#b90">[LT23]</ref> for a discussion of many interesting special cases other than do-interventions.Example 25. Returning once more to the smoking causal model from Ex. 22, consider an intervention σ which represents a public health policy such that all people under 21 years of age have a 90% probability of not smoking, while all those above 21 years are unaffected by the policy<ref type="bibr" target="#b15">[CB20]</ref>. This replaces the mechanism for S with a new mechanism c ′ S of the form: δ S,0 + 0.1 δ S,1 ∀A &lt; 21 c S (B) ∀A ≥ 21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>PFigure 9 :</head><label>9</label><figDesc>Figure 9: (a) Basic example of a BN: the fork Y ← X → Z; (b) Diagrammatic rewriting, using a variant of (7) and properties of the normalisation box [LT23], to show how the structure of the network diagram can be used to infer the conditional independence implied by the fork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Given a text circuit, we can obtain a distribution over possible answers A by applying the text circuit to the initial states, as below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Definition 38 (</head><label>38</label><figDesc>Interpreted Diagram). Let M be a compositional model with generators G and interpretation I. An interpreted diagram is a string diagram D in C G (see Def. 7), for which I is defined on all variables and boxes in the diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>'≈</head><label></label><figDesc>reliable' person is typically provided with a loan. diagrams together, we can apply rewriting to explain the outcome above, as follows.In the next example, we make use of additional factorisation structure on the representation space of a sequence model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Together these imply that asking whether a yellow banana is bitter yields the output 'no', as follows.Text circuit models, such as DisCoCirc models, provide further examples of models with factorisation structure, where now the factorisation is over discourse referents rather than domains. These allow for rewrite explanations which relate agents in a text, as in the following examples.Example 46. Consider a DisCoCirc model of the text Bob is in the kitchen. Claire is in the Garden. Alice follows Bob. Where is Alice?'. Suppose that the words in our model satisfy the following rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>FA</head><label></label><figDesc>specifies a biclosed monoidal functor FreeBi(Σ) → Free Rigid (Σ ′ ), so that any DisCoCat model of Σ ′ lifts to a CCG model of Σ. As noted in<ref type="bibr" target="#b148">[YK21]</ref>, this mapping can be useful in allowing us to readily inspect the equality of apparently distinct CCG parsings of the same sentence, such as the following (where FC denotes the function composition rule):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>in C, for i = 1 . . . , n, it returns a morphism h(f 1 , . . . , f n ) : C → D. As defined earlier, a h.o.-category is a monoidal category C with a distinguished set H C of higher order morphisms. By a h.o.-functor we mean a monoidal functor F : C → D between h.o.-categories along with a function H F : H C → H D , such that each higher order morphism h of type (64) in H C , is mapped to one of type H F (h) : n i=1 D(F (A i ), F (B i )) → D(F (C), F (D))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>each effect C i forms a density for the state C i with respect to the measure µ. In a quantum model, one instead associates a concept effect C (i.e. positive operator) with a normalised state (density matrix) ρ C = norm(C) = 1 Tr(C) C. Let us now meet two examples of conceptual encoder-decoder setups, both from [TSZC24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Definition 55 .</head><label>55</label><figDesc>Let M = (S, -, C) and M ′ = (S ′ , -′ , C) be compositional models in C. A morphism of models R : M → M ′ is a functor 57 as below such that the following commutes.We say R is variable-preserving if R(V) is a variable of M ′ , for every variable V of M, and faithful ifV → R(V) is injective. By a refinement of a diagram D in C we mean a diagram D ′ in C along with a morphism R : M D → M D ′ of their induced compositional models in C, such that R(D) = R(D ′ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>By a compositional model of f we mean a diagram D in C which forms a faithful refinement of this diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Z=</head><label></label><figDesc>Suppose we outline the further factorisation in terms of an 'encoder' and 'decoder' over Z as in the second model. This amounts to a refinement of diagrams corresponding to the second equation. Here the refinement is faithful and variable-preserving and has also introduced a new variable Z.Example 57. An open causal model M of a channel c is precisely one whose diagram D M in C forms a faithful variable-preserving model of c. For example, consider a distribution ω over variables S, L, A corresponding to the left-hand diagram below in Stoch, where, following our earlier Ex. 19, the variables S, L and A may for instance stand for a person's choice to smoke, whether or not they develop lung cancer, and their age, respectively. A causal model of ω in the above sense then amounts to a diagram in C with the same output variables, such that the following equality holds, that is, it is precisely the usual notion of a 'causal model for a distribution', i.e. a model that is a candidate causal explanation of that distribution.Example 58. Consider a causal model M in C with variables V = {X i } n i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head></head><label></label><figDesc>Example 59. A computational implementation of a diagram D in C can often be described as a (typically non-variable preserving) refinement to a diagram D ′ consisting of elementary computational units (e.g. wires of the form R and neurons, or qubits and ZX-diagrams).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the following decision-list model is claimed to give equal performance to the COMPAS model, despite its simplicity:</figDesc><table><row><cell cols="2">explicitly shown each rule as a conjunction. 17</cell><cell></cell></row><row><cell>IF</cell><cell>age 18-20 and sex is male</cell><cell>THEN predict arrest (within 2 years)</cell></row><row><cell cols="3">ELSE IF age 21-23 and 2-3 prior offences THEN predict arrest ELSE IF more than three priors THEN predict arrest</cell><cell>(23)</cell></row><row><cell>ELSE</cell><cell>predict no arrest.</cell><cell></cell></row><row><cell cols="4">We can represent this model diagrammatically as follows, where S, A, P denote sex, age and number of priors</cell></row><row><cell cols="4">respectively, and the output O = B is either true (predict arrest) or false (predict no arrest). Here we have</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>variants of the RNN are applied to a classification problem in computational biology in [LBX + 23]. Further sequence models. Further forms of sequence model appear in the lambeq python toolkit for compositional NLP [KFY + 21]. For each model, the state representation of a word sequence (w 1 , . . . , w n ) is shown below.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Thus a compositional model of the left-hand kind in NN consists of a single vector space with vectors for each word and neural networks for each rule type, while the latter kind have the single network 'UNIBOX'. Models of each kind are implemented in the lambeq software [KFY + 21].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54</cell></row><row><cell></cell><cell>BA</cell><cell></cell><cell cols="2">UNIBOX</cell><cell></cell></row><row><cell></cell><cell>FA</cell><cell></cell><cell></cell><cell cols="2">UNIBOX</cell></row><row><cell>Alice</cell><cell>likes</cell><cell>Bob</cell><cell>Alice</cell><cell>likes</cell><cell>Bob</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that principles of compositionality are often attributed to Frege incorrectly [Nef20, Jan01, Sza14, Pel01].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Formally this is expressed via 'coherence isomorphisms' A ⊗ I ≃ A ≃ I ⊗ A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Formally this means that I is a terminal object in C channel .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>It is also common to define NN to only contain differentiable functions, to allow for the study of the training process, but we will not do so here and so for simplicity allow arbitrary functions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Precisely, ω(x) := ω(x | ⋆) and e(x) := e(⋆ | x).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>Formally, these come with functions in, out : Gmor → G * ob sending each generator to a list of input variables and output variables, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Composition is given simply by composing diagrams 'on the page', the empty list is the monoidal unit and each identity morphism is the diagram of all plain wires.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>Formally, the mapping equations condition is expressed as follows. Write Ĝ, Ĝ′ respectively for signatures G, G ′ with their equations removed. Then F induces a functor F ′ : Free( Ĝ) → Free( Ĝ′ ). For each equation D 1 = D 2 in Geq we require that F is such that F ′ (D 1 ) = F ′ (D 2 ) in Free( Ĝ′ ). Then F extends to a functor Free(G) → Free(G ′ ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>Formally, each arrow represents a partial map of signatures, where we view the category C G as a special case of a signature, and -denotes as expected the (total) map V → V = V , f → f = f .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>Informally, we may also think of an abstract interpretation as often coming with assigned meanings for the composition operations (•, ⊗, I) of S in relation to the concepts of H. This allows one to extend the interpretation to one of diagrams in S.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10"><p>This is a classic example of an (open) causal model, treated more generally in Section 5.11.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11"><p>More generally, one may define multi-linear models by a string diagram in which the inputs X 1 , . . . , Xn are copied to m different linear models, each with a distinct output wire Y j , for j = 1, . . . , m.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12"><p>Following the calculation of the score, the model then includes a further function converting each score to a final output probability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_13"><p>Note that in this example all the rules return the same output value s j = 1 and so the 'first' box can be equivalently replaced with an 'or' function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_14"><p>To the authors' knowledge, this approach is novel here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_15"><p>In practice, a label would correspond to a token which can in fact be a word or a word fragment, produced by a statistically driven tokenizer. Some of these tokens themselves may lack a clear interpretation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_16"><p>For a sequence model one does not require ⊗ and so may take C, S to be plain (non-monoidal) categories with a distinguished object I. However, here for consistency we assume both are monoidal with I as the monoidal unit.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_17"><p>Recall that a monoid is a category with a single object; equivalently it is a set S along with a binary operation a, b → a • b satisfying associativity (a • b) • c = a • (b • c) and with a unit 1 such that a • 1 = a = 1 • a for all a ∈ S.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_18"><p>Orthog is defined just like Mat R + but with matrices having values in R and being orthogonal only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_19"><p>A pregroup is precisely a rigid monoidal category which forms a partially ordered set, i.e. with at most a unique morphism x → y for each pair of objects x, y, denoted ≤ when it exists, and such that x ≤ y ≤ x =⇒ x = y. In this setting the existence of cups and caps correspond to the equations of pregroups (33).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_20"><p>Additionally there are equations expressing id A l and id A r by yanking similarly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_21"><p>That is, by composing the word representations with the caps in C in the same way.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_22"><p>For a practical NLP application a larger set of types would be required for the particular language in question.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_23"><p>In<ref type="bibr" target="#b136">[TSZC24]</ref> a more detailed notion of this model is introduced, which for brevity we have simplified, which makes use of 'projection' morphisms which allow instances to be defined intrinsically and each conceptual space to form only a 'subspace' of the product of domains, i.e. the domain of a projection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_24"><p>Note that the P (X i | Pa(X i )) are given stochastic maps. As the notation suggests, they may in practice often be computed as conditionals from a joint distribution; however this need not be the case. Going between a joint distribution P (plus DAG) and the causal mechanisms P (X i | Pa(X i )) is only uniquely possible in both directions assuming full support of P . The definitional primitives of causal models are the causal mechanisms P (X i | Pa(X i )).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_25"><p>A 'sharp state' captures the idea of a fixed value x of X. As a general technical term it is defined in Sec. 3, though in FStoch a sharp state simply is a point distribution.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_26"><p>Further we require that for some j ̸ = j ′ we haveC j ̸ = ∅ ̸ = E j ′ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_27"><p>Given data distribution D over X, feeding x into some model with input X can only be seen as conditioning on x (or sampling from D), if x is in the support of D. In contrast, whatever causal model M D might be behind the generation of D, intervening on all variables via do(X 1 = x 1 , ..., Xn = xn) can produce any x = (x 1 , ..., xn). In this sense, then, although freely choosing the input state x conceptually speaking has to be regarded as of interventional status, it does not rely on any specific causal knowledge and is part of the framework of input-output models in Sec. 6.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34" xml:id="foot_28"><p>Y could also have product structure (but that has no bearing on the discussion of CFEs in this section).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35" xml:id="foot_29"><p>Note that this is distinct from the observation that a CFE explanation may not amount to actionable advice due to operational limitations because one can't intervene on, for instance, age (see<ref type="bibr" target="#b56">[Fre22]</ref>).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="36" xml:id="foot_30"><p>In a quantum model a diagram such as the above would mean X = X 1 ⊗ • • • ⊗ Xn is given by the tensor product of Hilbert spaces X i and in a statistical model by the tensor product of (classical) probability spaces X i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="37" xml:id="foot_31"><p>That is, a causal model in the W-type sense (Sec. 7.3) as originally understood in the causal model literature; to be distinguished from the causal model that any NN model can be seen to induce, which does not a priori yield a CI model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="38" xml:id="foot_32"><p>This fact can also be seen to be a special case of the third rule of the do-calculus rules [Pea09].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="39" xml:id="foot_33"><p>In the causal model literature this would be stated as the causal effect quantity P (Y ; do(X)) in fact not depending on X and just reducing to the marginal P (Y ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_34"><p>Note however that this behaviour may not be desirable for arbitrary text input which is not of this 'sequence of facts' kind, in which the sentence or word order in a text may be considered irrelevant. For such situations, channels may not be appropriate as representations for each word in the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="41" xml:id="foot_35"><p>Note this is a special case of local surgery with C i = I.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="42" xml:id="foot_36"><p>Note that the need for care in distinguishing causal structure in the model vs in the 'world', especially with a view to action-guidance (as pointed out in Sec. 7.3), applies here just as much as with explanations in terms of inputs and outputs only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="43" xml:id="foot_37"><p>More formally, for which I C (f ) is defined on the corresponding morphism f :(V i ) n i=1 → (W j ) m j=1 in C G .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_38"><p>n ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆ ⋆Rewrite explanations in practice. For intrinsically interpretable models, rewrite explanations include the familiar notion of passing an input through the model, with every step being interpretable and thus yielding an overall explanation. For further kinds of compositionally-interpretable models, however, they are more novel but typically also require additional equations to hold between processes within the model. It remains to be shown in practice that one may train sufficiently rich CI models coming with such equations.As discussed in Example 46, this could be demonstrated either by training useful CI models, which have equations as built-in structure or explicitly encouraged in the training function. Otherwise it would require showing that one can find such equations experimentally from a given compositional model which has not been trained to make the equations hold. Each of these approaches suggest directions for further experimental work in future, which would demonstrate that rewrite explanations can be useful in practice outside of the case of intrinsically interpretable models.10 Quantum modelsThe categorical approach can capture models using a wide range of semantics categories, including that of quantum processes. In this section we focus our attention on quantum models and their relation to interpretability. We have seen several examples of quantum models, including quantum conceptual space models (Section 5.10), quantum variants of RNNs including unitary RNNs (Section 5.7), and DisCoCat and DisCoCirc models in QNLP (Sections 5.8 and 5.9). For concreteness, recall that a quantum compositional model will take its semantics in a category such as Quant, the category of finite-dimensional Hilbert spaces and completely positive maps (see Section 3.3).46  We may implement such a model using a quantum computer, where each variable is represented by some number of qubits, and each generator by a channel decomposed in terms of states, unitary gates, and discarding.47   For example, the diagram for a text circuit similar to Example 46 can be given a quantum implementation by representing each generator (word) by a parameterised quantum circuit, where a separate set of parameters46  Alternatively, a broader setting such as the category FCStar of finite-dimensional C*-algebras allows one to also include finite classical systems, which are often used to record measurement outcomes.47 For certain models like DisCoCat (see Sec. 5.8) the generators include maps like the 'cap', which are not channels, but instead non-trace-preserving completely positive maps leading to a quantum implementation, which additionally requires post-selection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="48" xml:id="foot_39"><p>As opposed to cd-categories, which assume classical 'copying' structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="49" xml:id="foot_40"><p>Note that, at least in the first instance, this has nothing to do with the more than a century old debate in physics about the appropriate 'interpretation of quantum theory'. In quantum theory each density operator has a completely uncontroversial role within the formalism for computing probabilities of physical events, but the issue here is that not even this kind of immediate reading of the mathematical state is generally available when using quantum semantics for classical variables in AI.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="50" xml:id="foot_41"><p>Probabilistic classical models may in fact possess correlated states (distributions), which cannot be written as product states. However any such state can be written as a probability distribution over product states, and interpreted as an 'uncertain' product state.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="51" xml:id="foot_42"><p>Importantly, it also cannot be written as a probability distribution over product states -unlike in the case of classical correlation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="54" xml:id="foot_43"><p>By definition, each of these structures is weaker so that a model of an earlier structure specialises to a later one. Formally these specify structure categories S Notypes and S UNI related to the generic CCG structure category S CCG by quotient functors S CCG → S Notypes → S UNI so that any model of S UNI forms a model of S Notypes and any such model forms one of S.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="55" xml:id="foot_44"><p>In particular it takes semantics in the continuous setting of the category ConSp introduced in Example 18, rather than merely FStoch.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="57" xml:id="foot_45"><p>Here we assume that R is the same kind of functor as the models (e.g. monoidal, cd-functor).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="58" xml:id="foot_46"><p>An obvious question may be: why this is interesting if we are already given a high-level causal model -why not just work with that in the first place? The point is that L takes as input data not directly related to the variables of H -the nature of</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="12">Acknowledgements</head><p>We would like to thank <rs type="person">Giovanni De Felice</rs>, <rs type="person">Gabriel Matos</rs>, <rs type="person">Vincent Wang-Maścianica</rs>, <rs type="person">Caterina Puca</rs>, <rs type="person">Konstantinos Meichanetzidis</rs> and the rest of the <rs type="institution">Oxford team of Quantinuum</rs>, as well as <rs type="person">Lachlan McPheat</rs>, for feedback and many helpful and in-depth discussions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sub-diagram stating that Alive? always returns 'no' when following hanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Harmonica Claudio</head><p>Frank Snaky </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further compositional model examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Linear models</head><p>Here we show to how to describe linear structure in categories such as NN diagrammatically, in a manner closely related to the approach of 'graphical linear algebra' <ref type="bibr" target="#b14">[BSZ17]</ref>.</p><p>Let C be a cd-category with a distinguished object R. Let us call an object V R-linear when it comes with distinguished morphisms as in ( <ref type="formula">19</ref>), but now with R replaced by R, called addition and scalar multiplication, as well as a distinguished state 0 of V , which together satisfy the following conditions.</p><p>Firstly, R is itself R-linear, coming with its own addition and scalar multiplication maps, as well as distinguished state denoted by 1. Secondly, each of the morphisms (19) is a deterministic channel. Next, addition is associative and commutative, with unit zero:</p><p>For any state r of R we define a r-multiplication morphism on V as in (20). We require that scalar multiplication satisfies the following.</p><p>Finally, we require that addition and multiplication are related by the following, which intuitively encode the relations that rv + sv = (r + s)v and r(v + w) = rv + rw respectively, for for all v, w ∈ V and r, s ∈ R.</p><p>The key example is the category NN where R = R and on each object V = R n we have that addition is given by (v, w) → v + w and scalar multiplication by (r, v) → r • v, as expected. Then for each r ∈ R, (20) is given by v → r • v.</p><p>From these maps one may go on to define further linear operations in string diagrammatic terms. For example, consider the inner product and matrix multiplication morphisms that feature in the transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mult</head><p>These decompose as elementary string diagrams as follows.</p><p>Mult</p><p>where each • denotes the map (v, w) → ⟨v, w⟩. 52 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Decision trees</head><p>An alternative string diagram describing the tree (25), which shows explicitly which variable each question depends on, is the following. For any question represented by Q : V → B let us define an alternative form of 'controlled Q' box:</p><p>Now the Y , N wires are simply the Booleans B. Again the Y ('yes') wire carries true iff the 'control' B is true and Q(v) is true, and N carries true iff the control is true and Q(v) is false. We may then represent the same decision tree as a function of T (temperature) and D (days since 2011) by the following computational 52 One may consider adding the inner product as another elementary generator on each linear object in the language of computational models. Note, however, that on any fixed choice of Q = K = R d , it can be written as a string diagram in terms of only + and multiply.</p><p>There is a series of works exploring this idea. In [GLIP21, GWP + 24] Geiger et al. study, given models L and H, how to find and check alignment maps with respect to which the given models may stand in a causal abstraction relation, while in [GWL + 22] it is studied how to use a given model H during training to induce a model L such that the former is a causal abstraction of the latter.</p><p>Abstraction in diagrams. Causal abstraction is well aligned with a compositional perspective, and naturally discussed in diagrammatic terms. Let L and H be two deterministic open causal models; the 'low-level' model L with input (exogenous) variables R and non-input (endogenous) variables V L ; and 'high level' model H with inputs U and non-inputs V H = {X 1 , . . . , X n }. 59 The particular semantics category is irrelevant, but for concreteness let's assume NN.</p><p>For each high level variable X ∈ V H , suppose we are given a subset π(X) ⊆ V L , such that π(X)∩π(Y ) = ∅ for X ̸ = Y (the 'localist' assumption), and a surjective map τ X : π(X) → X Suppose we also have a surjective map: 60 τ : R → U</p><p>In slight abuse of notation, for any S ⊆ V H , denote the corresponding product variable π(S) = X∈S π(X), and τ S the corresponding product function. In the strongest sense, H is a causal abstraction of L with respect to these alignment maps iff ∀S ⊆ V H : . . . . . .</p><p>Here we used the notion of opening a causal model from <ref type="bibr" target="#b90">[LT23]</ref>: open X essentially 'drops' the causal mechanism of X so that a do-intervention on X can be seen as the composition of first opening at X and then feeding in a state of choice; i.e. for any state x the corresponding do-intervention is given by do</p><p>In practice, though, with L a deep NN, it is unrealistic to even check every such equality of processes, which would require equality for all input states on R and all possible weights of neurons in π(X) for all X. Instead, Geiger et al. propose a weaker abstraction condition that quantifies over only the set of input states that can arise through L from the actual data distribution on R. Let D ⊆ St(R) denote the set of input states that come with the problem. In the weaker sense, H is a causal abstraction of L iff the LHS and RHS of (71) are equal whenever fed in any state of the following form, for</p><p>the task means that 'our' causal model H cannot be directly applied to it, while one would also like to leverage the power of NN-based ML.</p><p>59 Here V L and V H are all considered outputs of the respective models, but this assumption is just for a simpler presentation and could easily be dropped. 60 Here π(X), R and U in the respective (co-)domains each refer to the corresponding product variables (rather than the sets of variables).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretability from abstraction.</head><p>What are the interpretability benefits of such an abstraction relation? Suppose that the high-level model H does come with a complete interpretation I H (as is the point here). If a causal abstraction relation obtains as above, then morally this induces a (partial) abstract interpretation for L by 'factoring through' H, i.e. by assigning the interpretation I H (X) to the set of variables (neurons) π(X) for each X ∈ V H . Note, though, that this is not formally an interpretation of L itself, which would have to concern individual generators of L. 61  While causal abstraction thus won't generally yield an explicit interpreted causal structure for the model L itself, it provides a yet richer form of 'induced' causal model than discussed in Sec. 7, and we agree in spirit with the judgement in [GWL + 22] that it "is not a story about the reasoning a neural network might use to achieve its behaviour, but instead is an intervention-based method that determines how it does, in fact, achieve its behaviour. We can interpret the semantic content of neural representations using the high-level variables they are aligned with, and understand how those neural representations are composed using the high-level parenthood relation." [GWL + 22] Moreover, Geiger et al. argue that many standard techniques in XAI can be seen as causal abstraction analysis, turning the concept into a general approach to "providing faithful and interpretable explanations of AI models" [GLIP21, GWL + 22]. One might also argue that causal abstraction answers a key question that comes with causal representation learning [SLB + 21b], namely 'how much' causal knowledge and in 'what form' does one have to provide to the model design and training so as to guarantee the learning of meaningful causal relata.</p><p>There are notable generalisations of causal abstraction, including approximate ones and those where the localist assumption is dropped. We leave a full categorical presentation, including possible extensions from causal to more general compositional models, for future work; also see Sec. 11. 61 Nor does the partitioning of V L induced by π generally induce a correspondingly coarsened causal model L with variables (π(X)) X∈V H in a straightforward sense, which then could have a partial abstract interpretation induced by I H in the formal sense. Given causal model M with variables V and causal structure G M , a partition (Y 1 , ..., Y k ) of V , i.e. ∪ k j=1 Y j = V , does not in general induce a causal model with coarsened variables Y 1 , ..., Y k , by just lumping together mechanisms to form new ones. This only works if the partitioning plays well with the causal structure -roughly speaking, if it corresponds to a way of 'partitioning' the network diagram D M into fragments. Moreover, even if a partitioning does, formally speaking, define a coarsened causal model M, the latter generally becomes detached from the phenomenon the original M is describing, for in reality one cannot intervene on the new coarsened variables independently -generically, coarsening destroys the autonomy of mechanisms.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A categorical semantics of quantum protocols</title>
		<author>
			<persName><forename type="first">Samson</forename><surname>Abramsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science</title>
		<meeting>the 19th Annual IEEE Symposium on Logic in Computer Science</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cebab: Estimating the causal effects of realworld concepts on NLP model behavior</title>
		<author>
			<persName><forename type="first">Karel D'</forename><surname>Eldar D Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Oosterlinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxuan</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17582" to="17596" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible ai</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Barredo Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siham</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Benjamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ADRDS + 20</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linguistic generalization and compositionality in modern artificial neural networks</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page">20190307</biblScope>
			<date type="published" when="1791">1791. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Quantum models of cognition and decision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Busemeyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bruza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Interacting conceptual spaces I: Grammatical composition of concepts. Conceptual spaces: Elaborations and applications</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Joe</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Bolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><surname>Piedeleu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<idno>BCG + 19</idno>
		<imprint>
			<date type="published" when="2019">2021. 2019</date>
			<biblScope unit="page" from="151" to="181" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On Pearl&apos;s hierarchy and the foundations of causal inference</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">David</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duligur</forename><surname>Ibeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Icard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On Pearl&apos;s hierarchy and the foundations of causal inference</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duligur</forename><surname>Ibeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and causal inference: the works of judea pearl</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="507" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal explanations and XAI</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="90" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How Nice is this Functor? Two Squares and Some Homology go a Long Way</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Barbiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Fioravanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Giannini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Tonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lavore</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14094</idno>
	</analytic>
	<monogr>
		<title level="m">Categorical foundations of explainable AI: A unifying formalism of structures and semantics</title>
		<editor>
			<persName><forename type="first">Merlin</forename><surname>Benjamin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Bumpus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Fairbanks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Caterina</forename><surname>Genovese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Puca</surname></persName>
		</editor>
		<editor>
			<persName><surname>Rosiak</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2023. 2024. 2024</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of Applied Category Theory</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abstracting causal models</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2678" to="2685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Bernardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2016: 25th International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2022. September 6-9, 2016. 2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
	<note>Algebraic Structures in Natural Language. Proceedings. Part II 25</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational lenses: a language for updatable views</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bohannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting>the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="338" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Physics, topology, logic and computation: a Rosetta Stone</title>
		<author>
			<persName><forename type="first">John</forename><surname>Baez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Stay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interacting hopf algebras</title>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Pawe L Sobociński</surname></persName>
		</author>
		<author>
			<persName><surname>Zanasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pure and Applied Algebra</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="184" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A calculus for stochastic interventions: Causal effect identification and surrogate experiments</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10093" to="10100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Carloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Colantonio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.09901</idno>
		<title level="m">The role of causality in explainable artificial intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interacting quantum observables</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="298" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-level cause-effect systems</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal feature learning: an overview</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="164" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal scrubbing: A method for rigorously testing interpretability hypotheses</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Goldowsky-Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Greenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Nitishinskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansh</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buck</forename><surname>Shlegeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Thomas</surname></persName>
		</author>
		<idno>CGAGD + 22</idno>
	</analytic>
	<monogr>
		<title level="m">AI Alignment Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1828" to="1843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Categorical foundations of gradient-based learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Cruttwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Gavranović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Zanasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Programming</title>
		<imprint>
			<publisher>Springer International Publishing Cham</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Capucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Gavranović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jules</forename><surname>Hedges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eigil Fjeldgren</forename><surname>Rischel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06332</idno>
		<title level="m">Towards foundations of categorical cybernetics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disintegration and Bayesian inversion via string diagrams</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Structures in Computer Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="938" to="971" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bas</forename><surname>Westerbaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Westerbaan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05813</idno>
		<title level="m">An introduction to effectus theory</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Picturing quantum processes: A first course on quantum theory and diagrammatic reasoning</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleks</forename><surname>Kissinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diagrammatic Representation and Inference: 10th International Conference</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-18">2018. June 18-22, 2018. 2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
	<note>Diagrams</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Tamara Von Glehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Tanburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matko</forename><surname>Dashevskiy</surname></persName>
		</author>
		<author>
			<persName><surname>Bosnjak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05125</idno>
		<title level="m">Formalising concepts as grounded abstractions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural network attributions: A causal perspective</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyushi</forename><surname>Manupriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="981" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introducing categories to the practicing physicist</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">What is category theory</title>
		<imprint>
			<publisher>Polimetrica Publishing Milan</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="45" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Axiomatic description of mixed states from Selinger&apos;s CPM-construction</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Notes in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3681</idno>
		<title level="m">Terminality implies non-signalling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05327</idno>
		<title level="m">Compositionality as we see it, everywhere around us</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<title level="m">The mathematics of text structure. Joachim Lambek: The Interplay of Mathematics, Logic, and Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="181" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transparency in complex computational systems</title>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">A</forename><surname>Creel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="568" to="589" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Picturing classical and quantum Bayesian inference</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Spekkens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="651" to="696" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="345" to="384" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Are neural nets modular? Inspecting functional modularity through differentiable weight masks</title>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sjoerd</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><surname>Dbm + Ng] Tiffany</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saskia</forename><surname>Duneau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriel</forename><surname>Bruhn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tuomas</forename><surname>Matos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katerina</forename><surname>Laakkonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Saiti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Pearson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bob</forename><surname>Meichanetzidis</surname></persName>
		</editor>
		<editor>
			<persName><surname>Coecke</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Scalable and interpretable quantum natural language processing: an implementation on trapped ions. 2024 (Forthcoming</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06837</idno>
		<title level="m">Sparse interventions in language models with differentiable masking</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felice</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2212.06636</idno>
		<title level="m">Categorical tools for natural language processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felice</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Toumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><surname>Discopy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02975</idno>
		<title level="m">Monoidal categories in python</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faith and fate: Limits of transformers on compositionality</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lavore</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mario Román ; Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Sclar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Le Bras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12989</idno>
	</analytic>
	<monogr>
		<title level="m">Evidential decision theory via partial markov categories</title>
		<imprint>
			<date type="published" when="2023">2023. 2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>DLS + 24</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Opportunities and challenges in explainable artificial intelligence (XAI): A survey</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11371</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards a rigorous science of interpretable machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A mathematical framework for transformer circuits</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Jeffrey L Elman ; Nelson Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2021/framework/index.html" />
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990. 2021</date>
			<publisher>Transformer Circuits Thread</publisher>
		</imprint>
	</monogr>
	<note>Finding structure in time</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The language of thought</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Harvard university press</publisher>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combinators for bidirectional tree transformations: A linguistic approach to the viewupdate problem</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Michael B Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems (TOPLAS)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dear XAI community, we need to talk! fundamental misconceptions in current XAI research</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Freiesleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>König</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04292</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The d-separation criterion in categorical probability</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Klingler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Freiesleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Tejero-Cantero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05487</idno>
		<title level="m">Scientific inference with interpretable machine learning: Analyzing models to learn about real-world phenomena</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The modularity of mind</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Causal theories: A categorical perspective on Bayesian networks</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.6201</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Causalm: Causal model explanation through counterfactual language models</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="386" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: A critical analysis</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenon</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Über sinn und bedeutung</title>
		<author>
			<persName><forename type="first">Gottlob</forename><surname>Frege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Philosophie und philosophische Kritik</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="25" to="50" />
			<date type="published" when="1892">1892</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Gottlob</forename><surname>Frege</surname></persName>
		</author>
		<title level="m">Letter to Jourdain. Philosophical and mathematical correspondence</title>
		<imprint>
			<date type="published" when="1914">1914</date>
			<biblScope unit="page" from="78" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The intriguing relation between counterfactual explanations and adversarial examples</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Freiesleben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="109" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A synthetic approach to markov kernels, conditional independence and theorems on sufficient statistics</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Mathematics</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="page">107239</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Seven sketches in compositionality: An invitation to applied category theory</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Spivak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05316</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Backprop as functor: A compositional perspective on supervised learning</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Spivak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémy</forename><surname>Tuyéras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 34th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Conceptual spaces: The geometry of thought</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gardenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The geometry of meaning: Semantics based on conceptual spaces</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gardenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">GLD + 24] Bruno Gavranović, Paul Lessard, Andrew Dudzik, Tamara von Glehn, João GM Araújo, and Petar Veličković. Categorical deep learning: An algebraic theory of architectures</title>
		<author>
			<persName><forename type="first">Mara</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><surname>Dutkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Calvaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Pereira Amorim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Yordanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Vered</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Henriques</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Blanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>Pulignano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15332</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3473" to="3504" />
			<date type="published" when="2023">2023. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">XAI-explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanson</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts ; David Gunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stefik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang-Zhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2021. 2019</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7120</biblScope>
		</imprint>
	</monogr>
	<note>Causal abstractions of neural networks</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Finding alignments between interpretable causal variables and distributed neural representations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanson</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rozner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxuan</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/1904.07451</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Christopher Potts, Thomas Icard, and Noah Goodman</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="1990">1990. 2019. 2022. 2024</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="160" to="187" />
		</imprint>
	</monogr>
	<note>Causal Learning and Reasoning</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<title level="m">Towards a definition of disentangled representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Hinrich</forename><surname>Schuetze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Compositionality decomposed: How do neural networks generalise</title>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verna</forename><surname>Dankers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathijs</forename><surname>Mul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="757" to="795" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Causal learning and explanation of deep neural networks via autoencoded activations</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Harradon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Druce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ruttenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">June 2019. 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
	<note>International conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Two complete axiomatisations of pure-state qubit quantum computing</title>
		<author>
			<persName><forename type="first">Amar</forename><surname>Hadzihasanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanlong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM/IEEE symposium on logic in computer science</title>
		<meeting>the 33rd annual ACM/IEEE symposium on logic in computer science</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Causes and explanations: A structural-model approach. Part I: Causes. The British journal for the philosophy of science</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="843" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Neuro-symbolic artificial intelligence: The state of the art</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Kamruzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarker</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The mathematics of changing one&apos;s mind, via Jeffrey&apos;s or via Pearl&apos;s update rule</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="783" to="806" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Learning from what&apos;s right and learning from what&apos;s wrong</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14045</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Frege, contextuality and compositionality</title>
		<author>
			<persName><forename type="first">Theo Mv</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic, Language and Information</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="115" to="136" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Causal inference by string diagram surgery</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleks</forename><surname>Kissinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Zanasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOSSACS 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-04-06">2019. April 6-11, 2019. 2019</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="313" to="329" />
		</imprint>
	</monogr>
	<note>ETAPS</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Algebras and update strategies</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Rosebrugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard J Wood ; Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richie</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04236</idno>
	</analytic>
	<monogr>
		<title level="m">lambeq: An Efficient High-Level Python Library for Quantum NLP</title>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Felice</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Meichanetzidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bob</forename><surname>Clark</surname></persName>
		</editor>
		<editor>
			<persName><surname>Coecke</surname></persName>
		</editor>
		<meeting><address><addrLine>Anna Pearson, Robin Lorenz, Alexis Toumi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2021</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="729" to="748" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KFY + 21</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Aleks</forename><surname>Kissinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matty</forename><surname>Hoban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04118</idno>
		<title level="m">Equivalence of relativistic causal structure and process terminality</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">On the anatomy of attention</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Laakkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Wang-Maścianica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">From discourse to logic: Introduction to model-theoretic semantics of natural language, formal logic and discourse representation theory</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Picture-perfect quantum key distribution</title>
		<author>
			<persName><forename type="first">Aleks</forename><surname>Kissinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bas</forename><surname>Westerbaan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08668</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">From Word to Sentence: a computational algebraic approach to grammar</title>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Lambek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Polimetrica sas</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Functorial Semantics of Algebraic Theories</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Lawvere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Functorial semantics of algebraic theories</title>
		<author>
			<persName><forename type="first">Lawvere</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="869" to="872" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Charles London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sezen</forename><surname>Vatansever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>James Langmead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Meichanetzidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15696</idno>
	</analytic>
	<monogr>
		<title level="m">Peptide binding classification on quantum computers</title>
		<imprint>
			<date type="published" when="2018">2018. 2023</date>
			<biblScope unit="page" from="2873" to="2882" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery</title>
		<author>
			<persName><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">QNLP in practice: Running compositional models of meaning on a quantum computer</title>
		<author>
			<persName><surname>Lpm + 23] Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Meichanetzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1305" to="1342" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Causal models in string diagrams</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07638</idno>
		<idno>MBS + 18</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Barren plateaus in quantum neural network training landscapes</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Jarrod R Mcclean</surname></persName>
		</author>
		<author>
			<persName><surname>Boixo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vadim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4812</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Interpretable machine learning-a brief history, state-of-the-art and challenges</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="417" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piers</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liz</forename><surname>Sonenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00547</idno>
		<title level="m">Explainable AI: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Causal interpretability for machine learning-problems, methods and evaluation</title>
		<author>
			<persName><forename type="first">Mansooreh</forename><surname>Mkg + 20] Raha Moraffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrienne</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">General pitfalls of model-agnostic interpretation methods for machine learning models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Herbinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Freiesleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Dandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">A</forename><surname>Scholbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Grosse-Wentrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<idno>MKH + 20</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The challenge of compositionality for AI</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Millière</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Interpretable machine learning</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lulu.com</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Universal Grammar</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Philosophy: Selected Papers of Richard Montague</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Richmond</surname></persName>
		</editor>
		<editor>
			<persName><surname>Thomason</surname></persName>
		</editor>
		<meeting><address><addrLine>New Haven, CT</addrLine></address></meeting>
		<imprint>
			<publisher>Yale University Press</publisher>
			<date type="published" when="1970">1970. 1974</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="222" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Interpretability is in the mind of the beholder: A causal framework for human-interpretable representation learning</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Marconato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Passerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Teso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Grammaraware sentence classification on quantum computers</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Meichanetzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Toumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>De Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantum Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><surname>Nefdt</surname></persName>
		</author>
		<title level="m">A puzzle concerning compositionality in machines. Minds and Machines</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="47" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12919</idno>
		<title level="m">Causal effects of linguistic properties</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>PCJ + 20</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The algorithmization of counterfactuals</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Theoretical impediments to machine learning with seven sparks from the causal revolution</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04016</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">Jeffry</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><surname>Pelletier</surname></persName>
		</author>
		<title level="m">Did Frege believe Frege&apos;s principle? Journal of Logic, Language and information</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="87" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">XAI handbook: towards a unified framework for explainable AI</title>
		<author>
			<persName><forename type="first">Caterina</forename><surname>Puca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Hadzihasanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke ; Sebastian Palacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Lucieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsin</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheraz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.14461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2023. 2021</date>
			<biblScope unit="page" from="3766" to="3775" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Obstructions to compositionality</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The Book of Why. Basic Books</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Completeness for arbitrary finite dimensions of ZXW-calculus, a unifying calculus</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Pezzulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Friston ; Boldizsár Poór</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razin</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lia</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richie</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12135</idno>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Active inference: the free energy principle in mind, brain, and behavior</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">An introduction to string diagrams for computer scientists</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Piedeleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Zanasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08768</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">The Language of Thought Hypothesis</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Benjamin Rodatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Laakkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">John</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Wang-Maścianica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">N</forename><surname>Zalta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uri</forename><surname>Nodelman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
		<respStmt>
			<orgName>Metaphysics Research Lab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Summer 2024 edition. RLH + 24</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">Mario</forename><surname>Román</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04526</idno>
		<title level="m">Open diagrams via coend calculus</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The frobenius anatomy of word meanings I: subject and object relative pronouns</title>
		<author>
			<persName><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1293" to="1317" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Causality for machine learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Causal Inference: The Works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="765" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A survey of graphical languages for monoidal categories</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New structures for physics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="289" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">A step-by-step tutorial on active inference and its application to empirical data</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical psychology</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">102632</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shiebler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Gavranović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07032</idno>
	</analytic>
	<monogr>
		<title level="m">Category theory in machine learning</title>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="612" to="634" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Toward causal representation learning</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">Toby</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clere</forename><surname>Smithe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04461</idno>
		<title level="m">Compositional active inference I: Bayesian lenses. statistical games</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">Toby</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clere</forename><surname>Smithe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10483</idno>
		<title level="m">Cyber kittens, or some first steps towards categorical cybernetics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">SNK + 22] Jerry Swan</title>
		<author>
			<persName><forename type="first">Toby</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clere</forename><surname>Smithe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12173</idno>
	</analytic>
	<monogr>
		<title level="m">Eric Nivel, Neel Kant, Jules Hedges, Timothy Atkinson, and Bas Steunebrink. The road to general intelligence</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Compositional active inference II: Polynomial dynamics. approximate inference doctrines</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Complete identification methods for the causal hierarchy</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1941" to="1979" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Inductive risk, understanding, and opaque machine learning models</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1065" to="1074" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Understanding from machine learning models</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal for the Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="133" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Zoltán Gendler Szabó. Problems of compositionality. Routledge</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Gendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szabó</forename></persName>
		</author>
		<title level="m">The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Zalta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uri</forename><surname>Nodelman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Metaphysics Research Lab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Fall 2022 edition</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Higher-order discocat (Peirce-Lambek-Montague semantics)</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Toumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felice</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Active inference in string diagrams: A categorical account of predictive processing and free energy</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clere</forename><surname>Smithe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00861</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">From conceptual spaces to quantum concepts: formalising and learning structured conceptual models</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razin</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabrina Zemljic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Quantum Machine Intelligence</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">A categorical semantics of fuzzy concepts in conceptual spaces</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Tull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05985</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">ZX-calculus for the working quantum computer scientist</title>
		<author>
			<persName><forename type="first">John</forename><surname>Van De Wetering</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Causal mediation analysis for interpreting neural NLP: The case of gender bias</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vgb + 20]</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simas</forename><surname>Nevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Sakenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12265</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Causal networks: Semantics and expressiveness</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine intelligence and pattern recognition</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
	<note>VSP + 17</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Conceptual challenges for interpretable machine learning</title>
		<author>
			<persName><surname>David S Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">A mathematical framework for transformations of physical processes</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Chiribella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04319</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">The Oxford handbook of compositionality</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Werning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Hinzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Machery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>OUP</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Wang-Mascianica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.10595</idno>
		<title level="m">Distilling text into circuits</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box: Automated decisions and the GDPR</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangzhou</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing: 8th CCF International Conference</title>
		<meeting><address><addrLine>Dunhuang, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2019. October 9-14, 2019. 2019</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="563" to="574" />
		</imprint>
	</monogr>
	<note>Harv. JL &amp; Tech.</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">A CCG-based version of the discocat framework</title>
		<author>
			<persName><forename type="first">Richie</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07720</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">QHSL: A quantum hue, saturation, and lightness color model</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaoru</forename><surname>Hirota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="page" from="196" to="213" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 13</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
