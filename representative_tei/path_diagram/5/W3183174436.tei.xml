<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topology-Guided Path Planning for Reliable Visual Navigation of MAVs</title>
				<funder>
					<orgName type="full">Institute of Information &amp; Communications Technology Planning &amp; Evaluation</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
				<funder ref="#_pAgwtXR">
					<orgName type="full">Korea government(MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-19">19 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Dabin</forename><surname>Kim</surname></persName>
							<email>dabin404@snu.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Gyeong</forename><forename type="middle">Chan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Youngseok</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">Jin</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Mechanical and Aerospace Engineering Department</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">with the Aerospace Engineering Department</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topology-Guided Path Planning for Reliable Visual Navigation of MAVs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-19">19 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.08616v1[cs.RO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual navigation has been widely used for state estimation of micro aerial vehicles (MAVs). For stable visual navigation, MAVs should generate perception-aware paths which guarantee enough visible landmarks. Many previous works on perception-aware path planning focused on samplingbased planners. However, they may suffer from sample inefficiency, which leads to computational burden for finding a global optimal path. To address this issue, we suggest a perceptionaware path planner which utilizes topological information of environments. Since the topological class of a path and visible landmarks during traveling the path are closely related, the proposed algorithm checks distinctive topological classes to choose the class with abundant visual information. Topological graph is extracted from the generalized Voronoi diagram of the environment and initial paths with different topological classes are found. To evaluate the perception quality of the classes, we divide the initial path into discrete segments where the points in each segment share similar visual information. The optimal class with high perception quality is selected, and a graphbased planner is utilized to generate path within the class. With simulations and real-world experiments, we confirmed that the proposed method could guarantee accurate visual navigation compared with the perception-agnostic method while showing improved computational efficiency than the samplingbased perception-aware planner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Fully automated robotic operation requires a perception module that recognizes surrounding environment and estimates the robot state. In particular, visual odometry (VO) and simultaneous localization and mapping (SLAM) using vision sensors have been conducted for self-localization of micro aerial vehicles (MAVs) due to low weight, cost, and small size of the sensors while capturing abundant visual information of surrounding environments. The integration of motion planning and perception modules poses significant issues for consideration, one of which is that the robot's localization capabilities are dependent on the path chosen by the robot. Therefore, it is necessary to take the perception module into account at the motion planning level, and this approach is called perception-aware motion planning.</p><p>In general, the performance of visual navigation is affected by the number and distribution of salient keypoints in the Fig. <ref type="figure">1</ref>. Snapshot from the experiment. For robust visual navigation, a micro aerial vehicle should decide a path which can guarantee enough visible features from multiple path candidates. observed images. For example, if the generated trajectory passes through a texture-less area, it may become difficult to execute given missions due to accumulated error of state estimation. Therefore, instead of conventional planners that are agnostic to the performance of the perception module, perception-aware motion planning is needed to lead stable visual navigation of MAVs.</p><p>To this end, the previous perception-aware planners have focused on approximation of navigation performance by inserting a perception-related cost on existing motion planning algorithms. Especially, sampling-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> have been mainly used for perception-aware planners. However, since the evaluation of perception quality itself is usually computationally heavy <ref type="bibr" target="#b3">[4]</ref>, difficulty arises when the planner suffers from sample inefficiency in large environments.</p><p>Instead of covering the entire environment with randomly drawn samples, exploiting higher-level information about the environment improves efficiency of global planning. The topology of the environment is determined from the geometrical distribution of obstacles. This fact leads to an important heuristic in the perspective of visual navigation. The visibility of the landmarks is limited by the relative pose between MAV and obstacles, thus the topological class of path restricts the maximum obtainable visual information <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Therefore, taking topological information of paths into account helps to find whether the path is advantageous for visual navigation. In addition, once a reference path is generated, it cannot be updated to belong to a different topology through gradient-based optimization <ref type="bibr" target="#b6">[7]</ref>. As a result, finding topological classes of paths with enough visual features gives important prior information for perceptionaware path planning.</p><p>In this paper, a topological perception-aware planner is suggested to prevent situations difficult to obtain accurate state estimation due to limited visual information. To create a path which attains both short path length and good perception quality, we propose the process of generating paths belonging to distinct topology classes and evaluate each path's quality with respect to path length and visual information. According to the authors' knowledge, this work is the first research on perception-aware motion planning that incorporates visual information with topological planning. The proposed planner can be exploited as an reference path to generate a feasible trajectory via trajectory optimization, or to provide prior information for a lower-level global planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Perception-aware Planning</head><p>Perception-aware motion planning refers to the algorithms that generate motion by considering the localization quality of the onboard navigation system. This study focuses specifically on motion planning algorithms that are applicable to visual navigation systems. In order to find a path from the start to the goal point, most perception-aware global planning algorithms are based on sampling-based methods. <ref type="bibr" target="#b7">[8]</ref> suggested the Rapidly-exploring random belief tree (RRBT) for planning in belief space with a linear estimator based on a sampling-based method. As an extension of RRBT for vision systems, <ref type="bibr" target="#b0">[1]</ref> applied local bundle adjustment (BA) in offline to estimate the covariance of future pose, and <ref type="bibr" target="#b1">[2]</ref> designed perception score based on the feature numbers in images as an approximation of localization quality. <ref type="bibr" target="#b2">[3]</ref> utilized the photometric information of images for dense VO and biased the path toward texture-rich region. Unlike aforementioned methods, our work suggests a global planning method which can utilize the environment's topological information as a heuristic for perception quality.</p><p>Another focus of perception-aware motion planning is improving perception quality of the trajectory via considering tracking and triangulation of local landmarks. <ref type="bibr" target="#b8">[9]</ref> formulated an optimal control problem which simultaneously minimizes the energy and velocity of the point of interest in image plane. <ref type="bibr" target="#b9">[10]</ref> suggested differentiable cost for keeping visible features inside the Field of View (FoV) of the camera. <ref type="bibr" target="#b10">[11]</ref> applied feature triangulation and covisibility-related costs to gradient-based trajectory optimization, encouraging the tracking of local landmarks. <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> used recedinghorizon method with designed planning cost regarding perception quality, which is evaluated from local landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Topological Planning</head><p>Among other global planning methods such as samplingbased methods and search-based methods, topological planning methods obtain paths from the topological graph of the environment. Topological planning is widely used in motion planning in the sense of reducing planning dimension with topological constraints <ref type="bibr" target="#b5">[6]</ref>, storing and searching for previsited regions <ref type="bibr" target="#b13">[14]</ref>, and finding the global optimal path from distinctive topologies <ref type="bibr" target="#b14">[15]</ref>. Our work also evaluates paths from distinctive topologies, though we focus on path planning with consideration about the path's perception quality.</p><p>To create a traversable topological graph structure from a given environment, PRM-based methods and generalized Voronoi diagram (GVD) can be used. However, as pointed out in <ref type="bibr" target="#b14">[15]</ref>, GVD is more beneficial than PRM-based methods in global planning since GVD guarantees coverage of the environment. GVD is a form of a roadmap structure, which can be computed online via the Euclidean signed distance field (ESDF) with low computational cost <ref type="bibr" target="#b15">[16]</ref>. Although topology in 3D spaces can be computed to better describe MAV flight as in <ref type="bibr" target="#b16">[17]</ref>, in this study, we use a 2D topological graph for the sake of efficient computation under the assumption that the flight altitude would not drastically change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM DESCRIPTION</head><p>In this paper, we suggest a planner that allows MAV to move from the start to the goal point while maintaining good self-localization. The MAV can observe surrounding environments and estimate ego-motion using a vision sensor. Although proposed method can also be adapted to multicamera systems, but single camera is used for demonstration.</p><p>The overall structure for the perception-aware motion planning is described in Fig. <ref type="figure" target="#fig_0">2</ref>. To represent the environment, we used an occlusion-aware feature map, which is an integrated map of volumetric and landmark maps. This allows to obtain information about visibility of map points and occupancy of the environment, which is required for evaluating the perception quality and generating a collision-free path. The global planner first constructs a sparse topological graph via GVD. Then initial paths belonging to distinct topologies are extracted from the graph. For each initial path, it is divided into multiple segments based on visibility information. Pose samples are generated near segments, and perception quality of each sample is evaluated. Then the path which can obtain the maximum perception information is generated via graph search. Among the generated paths from distinctive topologies, the best path with respect to the path length and perception quality is selected as an output of the global planner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MAP REPRESENTATION</head><p>In perception-aware planning, it is required to compute visibility information of landmarks from arbitrary poses to evaluate perception quality at candidate waypoints. In this paper, we consider visual SLAM algorithms which represent the map using point features observed in keyframes <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. While sparse pointcloud map allows efficient query of visible landmark candidates using adjacent keyframes, it does not provide the capability to consider geometry of the scene and exclude occluded landmarks. This may limit the accuracy of the queried visibility information, especially in an obstacle-filled environment where occlusion of landmarks occurs frequently. To tackle this issue, we have considered 3D volumetric map representation which allows more accurate reasoning on 3D geometry of the scene.</p><p>3D volumetric map representations have been developed to enable robots to differentiate between the traversable and occupied spaces. By modeling 3D space with probabilistic occupancy grid <ref type="bibr" target="#b19">[20]</ref> or ESDF <ref type="bibr" target="#b20">[21]</ref>, these map representations inherently provide the ability to reason about scene geometry. We devise a method to integrate SLAM map with volumetric map representation to create occlusion-aware feature map representation. Our method is similar to the method proposed in <ref type="bibr" target="#b2">[3]</ref>, where the authors stored texture information for each occupied voxel's surface. In our case, instead of storing photometric information, we embed each landmark's information in the occupied voxel corresponding to its location. Example of this map representation is illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>To construct the integrated map of an environment, we first construct a volumetric map and a SLAM map separately from series of measurements by RGB-D camera. Then the respective global coordinate frames of the maps are aligned and each landmark's information is embedded into the voxel at its location. Finally, to allow raycasting-based visibility query, landmark information is shifted to the nearest surface voxel which is visible from reference keyframe's pose. In this work, we used ORB-SLAM2 <ref type="bibr" target="#b17">[18]</ref> and Voxblox <ref type="bibr" target="#b20">[21]</ref> for SLAM map and volumetric map representation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TOPOLOGICAL GLOBAL PLANNING</head><p>Based on the integrated map, we generate a global path which serves as an initial reference for a low-level planner.</p><p>From the observation that visual information and relative position between MAV and obstacles are strongly related, we devise a method to generate multiple global paths with distinctive topologies and to select the path with respect to the perception quality and path length. This section is configured as follows. In Sec. V-A, we will give a detailed explanation on why it would be beneficial for a perceptionaware planner to consider topological properties of the path. The process of perception-aware planning will be covered in Sec. V-B ∼ V-E. The overall process of global planner is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref> and Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Homology Classes and Perception Quality</head><p>In this subsection, the close relationship between topological planning and perception quality is explained. To express topological equivalence of trajectories, the concept of homology class is widely used. Two trajectories with fixed start and goal points in 2D belong to the same homology class if the cycle formed by them does not include or intersect any obstacle. For a more formal definition of homology class, refer to <ref type="bibr" target="#b21">[22]</ref>.</p><p>During trajectory generation for MAV flight, the local planner refines the reference path from global planner to smooth and feasible trajectory. In order to maintain visual navigation stable, the global planner should generate a path such that navigation does not fail, not only in a reference path but also in a path refined by the low-level planner. However, the local planner module updates the path with collision avoidance constraint. For example, some algorithms restrict the search space into free, convex region as a hard constraint <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and other algorithms which use gradientbased optimization update paths to the opposite direction of obstacles <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Therefore, it is difficult for local planners to update a path to jump over obstacles and change its homology class, thus reference and refined paths are topologically equivalent.</p><p>On the other hand, a path's homology class affects visual information which MAV can obtain by following the path. Selecting the homology class fixes relative topology of the path to the obstacles, which determines visible surfaces of obstacles. In the vision-based navigation system, features are detected on the surface of obstacles. If feature-rich surfaces are hindered by occlusion, we can conclude that corresponding homology class is disadvantageous for visual navigation. Thus, the homology class which can guarantee visibility of feature-rich surfaces is preferred. The relationship between homology class and perception quality of the path can be observed in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>Therefore, searching distinctive homology classes can be a helpful heuristic for finding a reference path for reliable visual navigation. In addition, it also enables to boost computation by searching each homology class in parallel, exploiting multi-process CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Topological Graph Generation</head><p>The methodology that we present aims to find a global path through the topological structure of the environment. To this end, it is necessary to generate topological graphs in a given environment and extract homology classes. To generate a topological graph, we create a 2D GVD with the given reference height. Construction of GVD follows the method suggested by <ref type="bibr" target="#b15">[16]</ref>, in which the ESDF map is used to obtain a 2D voxel map of GVD. Since the graph structure of GVD is needed, vertices and edges are extracted from the GVD. A voxel is a vertex if 1 or more than 2 neighboring voxels are elements of GVD. And a connected set of voxels is an edge if it connects two vertices. By classifying each voxel as a vertex or an edge, a bi-directional topological graph is Pose graph search</p><formula xml:id="formula_0">P * t = (n 1 k1 , n 2 k2 , • • • , n N k N ) 9:</formula><p>Evaulate quality of the path q(P * t ) (7) 10: end for 11: P * = argmax</p><formula xml:id="formula_1">q(P * t ) {P * 1 , • • • P * T }</formula><p>Fig. <ref type="figure">6</ref>. Illustration of path segment extraction process. Green cross represents landmarks, and the co-visible candidate set is marked in purple circles. A path is iteratively divided until there are significant portion of co-visible landmarks at both ends of every segment.</p><p>acquired. By adding the start and goal points to the graph, connected paths from the start to the goal can be obtained from this graph. By adopting the methodology from <ref type="bibr" target="#b14">[15]</ref>, we can find 'initial paths', which are sets of consecutive vertices, from distinct homology classes using breadth first search on the topological graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extracting Path Segments</head><p>After initial paths are generated in different homology classes, each path and corresponding homology class are evaluated with respect to perception quality. It is important to maintain co-visibility to the landmarks along the path for vision-based localization. Thus, we evaluate the perception quality of the path based on co-visible landmarks while traveling along the path. However, especially in environments with multiple obstacles, visible landmarks change as the robot travels through the path, which makes it difficult to find a set of globally co-visible landmarks along the whole path.</p><p>Thus, we seek to split the initial path into smaller 'path segments' where points in the same segment share a significant amount of visible landmarks. This is inspired by the previous study on co-visibility based regional segmentation <ref type="bibr" target="#b26">[27]</ref>, where different positions in the map are grouped based on the similarity of their observations. While the method in <ref type="bibr" target="#b26">[27]</ref> was devised to obtain a topological map of the environment, our objective is to divide a path based on the similarity of predicted observation along the path. The algorithm is depicted in Fig. <ref type="figure">6</ref>.</p><p>We defined the 'visibility candidate set' at point p ∈ R 3 , V (p), as the set of the landmarks which are within the vertical FoV and sensing range. Given two points p, q ∈ R 3 , the 'co-visible candidate set' CV (p, q) and 'co-visibility ratio' CR(p, q) are respectively defined as</p><formula xml:id="formula_2">CV (p, q) = V (p) ∩ V (q)</formula><p>(1)</p><formula xml:id="formula_3">CR(p, q) = |CV (p, q)| |V (p) ∪ V (q)|<label>(2)</label></formula><p>As in Fig. <ref type="figure">6</ref>, we begin with evaluating co-visibility ratio of the start and goal point: CR(s, g). If it is larger than the threshold η ∈ [0, 1], we consider the path as a path segment s(s, g) and assign CV (s, g) as the covisible candidate set for the path segment. Otherwise, we query the visibility candidate set at the midpoint p of the path and evaluate CR(s, p) and CR(p, g). We iteratively divide the path until the co-visibility ratio of the end points for each segment becomes larger than η, or length of the segment reaches the minimum length min . As a result, an initial path is transformed into multiple path segments s(s, p 1 ), s(p 1 , p 2 ), • • • , s(p m , g), where the points in the same segment share significant co-visible landmarks. Furthermore, the previously stored co-visible candidate set of the path segment is used during the evaluation of perception quality to avoid redundant querying of visible landmarks, which include time-consuming raycasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pose Graph Construction &amp; Graph Search</head><p>While a path over sparse graph can represent the homology class, directly using it as global path precludes the existence of a path with better perception quality within the homology class. Also, since GVD is extracted from the 2D plane at the reference height, 3D obstacles cannot be considered in the sparse graph. From this need, we construct a dense graph from given sequence of path segments as in Fig. <ref type="figure" target="#fig_3">5 (c)</ref>. Each node of the dense graph represents a 4 Degree of Freedom (DoF) pose including the position and yaw angle of the MAV (x, y, z, and θ), and graph search is performed to find the optimal 4 DoF path.</p><p>The 4 DoF pose graph is constructed as follows. First we divide the initial path into intervals of length which is determined by nominal speed v nom and time step T s. Each of these intervals represents a layer of the graph; only the nodes within consecutive layers are allowed to form edges. We denote layers as L 1 , L 2 , • • • L N where N is the total number of the intervals obtained by splitting the entire path. For each layer L j , waypoint candidates are randomly sampled from the plane passing through the starting point of the interval and perpendicular to the edge of the interval. Samples are generated within a distance R sample centered around the path. On top of this, yaw angles are sampled at equal intervals. The sampled waypoint candidates and yaw angles are combined to form nodes of the layer {n j 1 , • • • n j m }, where n j k = (x j k , y j k , z j k , ψ j k ). Nodes in the consecutive layers are connected only if the difference of yaw angles between the nodes is smaller than a certain limit ψlim to prevent an abrupt yaw change.</p><p>To find the path with maximum visual information, each node's perception quality needs to be evaluated. We use the Fisher information matrix (FIM) as the metric for perception quality, which quantifies the information that can be obtained about the desired state through measurement. By modeling the camera as a bearing sensor as in <ref type="bibr" target="#b27">[28]</ref>, FIM of measurement on a landmark located at l observed from pose x can be formulated as</p><formula xml:id="formula_4">FIM(l; x) = 1 σ 2 (J(l; x)) T J(l; x)<label>(3)</label></formula><formula xml:id="formula_5">J(l; x) = 1 ||l c || I 3 - 1 ||l c || 3 l c (l c ) T I 3 [l w ] ×<label>(4)</label></formula><p>where l c , l w are the position of point seen from camera frame and global frame respectively, and σ is standard deviation of measurement noise. For each node, we compute the visible landmarks from previously stored co-visible candidate set along the path segment containing the layer node is in. For the j-th node, we evaluate the FIM of the visible landmarks at pose n j kj and denote it as I(n j kj ). We perform a graph search over the pose graph to find the 4 DoF path with the smallest value of combined distance cost and perception cost. Given a path P as a sequence of connected nodes</p><formula xml:id="formula_6">P = (n 1 k1 , n 2 k2 , • • • , n N k N )</formula><p>, we can formulate the graph search problem as</p><formula xml:id="formula_7">P * = arg min k1,k2,••• ,k N λ d c d (P) -λ p c p (P),<label>(5)</label></formula><p>where λ d , λ p ∈ R are weights for distance cost and perception cost, the distance cost c d (P) can be defined straightforward as the length of total path. The perception cost of the path c p (P) is formulated as</p><formula xml:id="formula_8">c p (P) = 1 N N i=1 log(det(I(n i ki )).<label>(6)</label></formula><p>To perform graph search, a layered structure of the graph can be exploited. Since each layer is arranged in time order, the graph is a directed acyclic graph. Also, topological sorting is used for graph search by dynamic programming. It can find an optimal path with less computation than Dijkstra search <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Selection of the best path</head><p>We design the cost function to quantify the quality of the generated path P * for each homology class as</p><formula xml:id="formula_9">q(P * ) = η d d d min -1 -η p f (c p,min -c p,thr ),<label>(7)</label></formula><formula xml:id="formula_10">c p,min = min(c p (s(s, p 1 )), • • • , c p (s(p m , g)))<label>(8)</label></formula><p>where f is defined as f (x) = 1/(1 + exp(x)), d is the distance of the path, d min is the length of the shortest  path among the generated path candidates, and c p,thr is the parameter to indicate the required information for robust visual navigation. As in <ref type="bibr" target="#b7">(8)</ref>, the perception quality of the path is determined by the minimum c p value among the segments of the path. It enables to avoid selecting the path passes through feature-poor region, which would result in high estimation error. The reason for using the sigmoid function is because effect of FIM on the localization performance degrades if there is enough information. Among the selected best path within each homology class, the path with the smallest cost is selected as the global path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. VALIDATION</head><p>To validate the proposed topological global planner, simulations and experiments were performed. The global planner was connected to local trajectory optimization to obtain a smooth and kinodynamically feasible trajectory. We used the gradient-based optimization method proposed in <ref type="bibr" target="#b25">[26]</ref> as an example of local planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation</head><p>We used the Unreal engine with AirSim plugin <ref type="bibr" target="#b29">[30]</ref> to perform high-fidelity simulations in photo-realistic environments. For visual navigation, a front-looking RGB-D camera was used and we conducted experiments in two realistic environments, storage and gallery, as in Fig. <ref type="figure" target="#fig_6">7</ref>. Simulations were performed on a desktop with 8 core Intel i7 3.2GHz CPU and 32GB RAM.</p><p>Four metrics were used for evaluation of the path quality. The first is the total length of the path and the second is the translational estimation error of VO algorithm, which is for quantifying the perception quality of the path. Third metric is the ratio of successful runs of VO without loss in feature tracking, which is related to the reliability of the path for stable navigation. Lastly, computation time was calculated to check computational efficiency of the algorithms. We used ORB-SLAM2 <ref type="bibr" target="#b17">[18]</ref> as an example for VO algorithm.</p><p>We compared the performance of the proposed method with two other methods. 1) Perception-Agnostic Planner (AP), which is a topological planner but without consideration about perception quality, 2) perception-aware sampling-based planning based on RRT* similar to <ref type="bibr" target="#b1">[2]</ref>, with some modification to fit in our settings. In perception-aware RRT*, at each sample, visible landmarks are queried, and the perception cost is evaluated using landmarks co-visible from the sample pose and its parent's pose. The criterion for choosing the best path is comparing weighted sum of the distance cost and the perception cost, similar to the proposed planner. Three different numbers of samples were used for the sampling-based planner. Perception-aware RRT* with N samples are shortened as 'R-N '. Parameters used in the simulations are noted in Table <ref type="table" target="#tab_0">I</ref>.</p><p>We ran each algorithm 10 times for each scenario, and the resulting trajectories are presented on Fig. <ref type="figure" target="#fig_6">7</ref>. Also, absolute trajectory error (ATE) for each path and success rates of VO are presented in Fig. <ref type="figure" target="#fig_7">8</ref>. For clear visualization, ATE values for only three trials are visualized for each planner.</p><p>In the first environment, storage, there are two selectable homology classes, distinguished by the wall in the middle. The class at upper side passes through a texture-less region and the lower side has many objects with abundant visual information. As in Fig. <ref type="figure" target="#fig_6">7</ref>(a), the proposed algorithm selected the lower side in every trials although path length became longer than choosing the upper side. On the contrary, AP selected the upper side's homology class and estimation error became higher than the proposed method's trajectory. For sampling-based planner, we chose 300, 1000, and 2000 samples respectively. Even though the sampling-based planner is designed to prefer feature-rich paths, planner with low sample numbers (300, 1000) often failed to find homology class beneficial for visual navigation and failed to maintain VO during flight. With large samples (2000), it can generate path toward feature-rich region and result in small estimation error, but it takes more computation time compared to the proposed method.</p><p>The second environment, gallery, contains two major texture-poor regions: a horizontal corridor to the right of the center and a longitudinal corridor to the upper middle. In contrast, many texture-rich objects are distributed along the walls surrounding the environment, especially the walls of left and upper side. Similar to the previous environment, AP chose the homology class with the shortest length among 10 distinct homology classes. However, since the selected path traverses through texture-poor region, it resulted in a higher odometry error and a lower success rate (3/10) compared to the proposed method. For sampling-based planners, since the environment is even larger and more complicated than the previous one, sample efficiency dropped and required more samples to find a visually advantageous path. With 1000 and 8000 samples, the planner often selected path traversing texture-poor regions. Sampling-based planner with abundant sample number (15000) generated paths with lowest estimation error, but it took over 100 s to generate the path at each trial. On the contrary, the proposed algorithm generated paths traversing texture-rich area in all trials, within a much shorter computation time. Results of each simulation environment are summarized in Table <ref type="table" target="#tab_1">II</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-world Experiment</head><p>For experiments, S-500 frame quadrotor equipped with forward-facing Realsense D435 camera was used. We used Pixhawk4 flight controller and Intel i7 NUC computer.</p><p>As in Fig. <ref type="figure" target="#fig_8">9</ref>(a), the environment has two obstacles parallel to each other, with only a small number of features visible in the middle region and feature-rich objects visible from the rear sides. Two algorithms, the proposed algorithm and the perception-agnostic planner were mounted on MAV. We used OptiTrack motion capture system to provide state information to the controller in order to prevent control failure. It also provided the groundtruth poses. Estimated trajectories from visual odometry were compared with the groundtruth trajectories. As Fig. <ref type="figure" target="#fig_8">9(b</ref>). shows, the proposed planner created a path with a low odometry error through the region, but the perception-agnostic planner showed failure on VO since it did not take the visual information of the environment into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS &amp; FUTURE WORKS</head><p>Based on the observation that the homology class affects the visual information which MAVs can obtain, we proposed a topological path planner for perception-aware navigation. By creating a sparse topological graph from 2D GVD, multiple paths within distinctive homology classes are obtained. Then, the best path within each homology class is searched using graph search. Finally, among the paths selected within each homology, the path with minimum travel cost and perception cost is selected. We validated the effectiveness of our planner in multiple simulation environments and experiment. Future works would include an extension to online planning in unknown environments, reducing the need of tuning parameters (i.e. perception quality threshold c p,thr ) via data-driven methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed method.</figDesc><graphic coords="2,319.32,50.08,232.57,165.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Occlusion-aware feature map representation of the environment drawn in Fig. 4. The darker the voxel color is, the more landmark information it contains. The arrows in the figure indicate the orientation with maximum visible landmarks from sampled position, and the color of each arrow represents the number of visible landmarks from the respective pose.</figDesc><graphic coords="3,55.22,50.08,242.36,164.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of dependence of perception quality on homology classes. Tracking a trajectory belonging to homology class marked in red leads to bad localization because feature-rich surfaces are occluded by the wall in the middle. Therefore, homology class of blue path is preferred in terms of perception quality.</figDesc><graphic coords="3,325.44,50.08,220.32,163.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Process of the proposed planner (a) Topological graph generation from GVD (b) For each homology class, path segments are extracted based on feature co-visibility. (c) Pose samples are generated and optimal paths are found. (d) The best path is selected with respect to the perception quality and path length.</figDesc><graphic coords="4,314.22,258.21,242.76,149.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 7 :</head><label>17</label><figDesc>Topological perception-aware path planner 1: Input: Map M , Start s, Goal g 2: Output: Global Path P * 3: Topological Graph Generation G = (V, E) 4: Generate Initial Paths from Distinct Homology Classes H = {h 1 , • • • h T } 5: for h t ∈ {h 1 • • • h T } do 6: Extract Path Segments h t → s(s, p 1 ), s(p 1 , p 2 ), • • • , s(p m , g) Generate 4DoF pose samples (n j k ) &amp; evaluate perception quality I(n j k ) 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Resulting trajectories from the tested planners in (a) storage and (b) gallery environments.</figDesc><graphic coords="7,321.12,261.30,236.87,123.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Absolute trajectory error of VO with respect to travel distance, and success rate for each planner at (a) storage and (b) gallery environment. Only 3 successful trials for each planner are shown for clear visualization.</figDesc><graphic coords="7,54.00,259.86,236.88,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Result from hardware experiments. (a) MAV generates a path pass to move through two waypoints and arrives at the goal point. (b) The resulting trajectories with the proposed planner (left) vs. the perception-agnostic planner (right). Blue lines indicate the groundtruth trajectory of MAV, and red lines are estimated position trajectory. With the perception-agnostic planner, the position estimate could be obtained during a part of the trajectory only, because of the early VO failure.</figDesc><graphic coords="8,61.86,159.50,232.56,106.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Parameter List for Simulation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Simulation</figDesc><table><row><cell></cell><cell>Planner</cell><cell>Length</cell><cell>Goal Error</cell><cell>Comp. Time</cell></row><row><cell></cell><cell>Proposed</cell><cell>15.9 m</cell><cell>0.118 m</cell><cell>1.58 s</cell></row><row><cell></cell><cell>AP</cell><cell>14.5 m</cell><cell>0.391 m</cell><cell>0.402 s</cell></row><row><cell>storage</cell><cell>R-300</cell><cell>9.86 m</cell><cell>0.083 m</cell><cell>3.04 s</cell></row><row><cell>(12m×10m)</cell><cell>R-1000</cell><cell>13.0 m</cell><cell>0.136 m</cell><cell>9.31 s</cell></row><row><cell></cell><cell>R-2000</cell><cell>14.3 m</cell><cell>0.082 m</cell><cell>19.5 s</cell></row><row><cell></cell><cell>Proposed</cell><cell>35.8 m</cell><cell>0.235 m</cell><cell>7.92 s</cell></row><row><cell></cell><cell>AP</cell><cell>33.9 m</cell><cell>0.664 m</cell><cell>1.66 s</cell></row><row><cell>gallery</cell><cell>R-1000</cell><cell>36.8 m</cell><cell>0.619 m</cell><cell>9.52 s</cell></row><row><cell>(22m×20m)</cell><cell>R-8000</cell><cell>36.4 m</cell><cell>0.598 m</cell><cell>90.2 s</cell></row><row><cell></cell><cell>R-15000</cell><cell>36.7 m</cell><cell>0.163 m</cell><cell>164.1 s</cell></row></table><note><p>results for storage and gallery environments. Mean values of length of the groundtruth trajectory, distance between the goal and the estimated goal position, and computation time are measured. Note that goal estimation error was evaluated only for successful trials.</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by <rs type="funder">Institute of Information &amp; Communications Technology Planning &amp; Evaluation(IITP)</rs> grant funded by the <rs type="funder">Korea government(MSIT)</rs> (No. <rs type="grantNumber">2019-0-00399</rs>, Development of A.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pAgwtXR">
					<idno type="grant-number">2019-0-00399</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion-and uncertainty-aware path planning for micro aerial vehicles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lynen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="676" to="698" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature-rich path planning for robust navigation of mavs with monoslam</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chutskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jungic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3870" to="3875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting photometric information for planning under uncertainty</title>
		<author>
			<persName><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An experiment in integrated exploration</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Makarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bourgault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A qualitative approach to localization and navigation based on visibility information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fogliaroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wallgrün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clementini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tarquini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spatial Information Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enabling topological planning with monocular vision</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1667" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Planning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lavalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rapidly-exploring random belief trees for motion planning under uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on robotics and automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pampc: Perceptionaware model predictive control for quadrotors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Falanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptionaware trajectory generation for aggressive quadrotor flight using differential flatness</title>
		<author>
			<persName><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Spasojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 American Control Conference (ACC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perception-aware path planning for uavs using semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Pinto</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>IROS 2020</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perception-aware receding horizon navigation for mavs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2534" to="2541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Navigation-assistant path planning within a mav team</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient planning for high-speed mav flight in unknown environments using online sparse topological graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrated online trajectory planning and optimization in distinctive topologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rösmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bertram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="142" to="153" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient grid-based spatial representations for robot navigation in dynamic environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sprunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1116" to="1130" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse 3d topological graphs for micro-aerial vehicle planning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oleynikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Svo: Fast semi-direct monocular visual odometry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Octomap: An efficient probabilistic 3d mapping framework based on octrees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Wurm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous robots</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voxblox: Incremental 3d euclidean signed distance fields for onboard mav planning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oleynikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1366" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topological constraints in search-based robot path planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Likhachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="290" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polynomial trajectory planning for aggressive quadrotor flight in dense indoor environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient multi-agent trajectory planning with feasibility guarantee using relative bernstein polynomial</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="434" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chomp: Covariant hamiltonian optimization for motion planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pivtoraiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klingensmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Dellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="1164" to="1193" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust and efficient quadrotor trajectory generation for fast autonomous flight</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3529" to="3536" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Consistent observation grouping for generating metric-topological maps that improves robot localization</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Fernandez-Madrigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2006 IEEE International Conference on Robotics and Automation</title>
		<meeting>2006 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006. 2006</date>
			<biblScope unit="page" from="818" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03324</idno>
		<title level="m">Fisher information field: an efficient and differentiable map for perception-aware planning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Airsim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and service robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="621" to="635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
