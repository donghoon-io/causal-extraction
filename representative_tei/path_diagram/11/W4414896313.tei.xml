<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-06-02">2 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manan</forename><surname>Suri</surname></persName>
							<email>manans@umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Puneet</forename><surname>Mathur</surname></persName>
							<email>puneetm@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Primary Research Mentor</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-02">2 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2506.01344v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Flowcharts are a fundamental tool for representing structured decision-making processes. Used across domains such as software engineering, business process modeling, and instructional design, flowcharts provide a visual roadmap of logical operations, guiding both human users and automated systems <ref type="bibr" target="#b2">(Charntaweekhun and Wangsiripitak, 2006;</ref><ref type="bibr" target="#b23">Perols and Perols, 2024;</ref><ref type="bibr">Zimmermann</ref>   <ref type="bibr">et al., 2024;</ref><ref type="bibr" target="#b5">Ensmenger, 2016)</ref>. Their structured yet visual nature makes them an effective medium for conveying procedural logic. However, interpreting flowcharts accurately is challenging due to their nonlinear structures (branching and loopbased control flow), where meaning emerges from the interplay between textual content, visual arrangement, and logical dependencies. Ambiguities in flowchart interpretation arise from diverse notational conventions, implicit relationships, and misinferred steps, making precise attribution of information sources difficult <ref type="bibr" target="#b6">(Eppler et al., 2008)</ref>.</p><p>Recent advancements in Vision Language Models (VLMs) have enabled substantial progress in flowchart processing <ref type="bibr" target="#b27">(Singh et al., 2024)</ref>. These models leverage both textual and visual information, allowing them to extract structural relationships, recognize decision nodes, and generate answers based on flowchart content. However, despite their capabilities, VLMs struggle with hallucination: the tendency to generate information that is not grounded in the input <ref type="bibr" target="#b10">(Huang et al., 2024;</ref><ref type="bibr" target="#b8">Guan et al., 2024)</ref>. In the context of flowcharts, hallucination can manifest as misidentifying decision nodes, producing incorrect logical pathways, or fabricating connections that do not exist in the original structure. This issue severely impacts the reliability of automated flowchart reasoning, particularly in high-stakes applications such as healthcare, software verification and process automation.</p><p>Although VLMs have made significant progress in understanding flowcharts, prior work has mainly concentrated on flowchart parsing <ref type="bibr" target="#b0">(Arbaz et al., 2024)</ref>, conversion <ref type="bibr" target="#b26">(Shukla et al., 2023;</ref><ref type="bibr" target="#b16">Liu et al., 2022)</ref>, and question-answering <ref type="bibr" target="#b27">(Singh et al., 2024;</ref><ref type="bibr" target="#b30">Tannert et al., 2023)</ref>, while overlooking the critical aspect of fine-grained attribution. While existing attribution methods <ref type="bibr" target="#b11">(Huo et al., 2023;</ref><ref type="bibr" target="#b3">Chen et al., 2023)</ref> focus on textual grounding, attributing responses to visual-textual elements like flowcharts presents unique challenges. It involves not just text recognition, but also interpreting the interconnected decision nodes, hierarchical structures, and conditional pathways that define flowchart semantics. Attribution serves as a crucial mechanism for mitigating hallucination by explicitly tracing the paths in the flowchart that ground a particular response, enabling rigorous evaluation of the model's fidelity to the flowchart's logic, as illustrated in Fig 1 . Such fine-grained attribution is fundamental for ensuring reliability, particularly when these systems are deployed in domains where verifiable decision-making is crucial. Main Results. We introduce Flowchart Attribution task aimed at identifying the optimal path within a flowchart that grounds the model's response. The optimal path aims to extract the most relevant sequence of nodes and edges that directly support the model's reasoning, encompassing all the key decision points and actions involved in the prediction. To facilitate the evaluation of this task, we propose FlowExplainBench, a novel benchmark that features a diverse set of flowcharts with varying styles, domains, and question types.</p><p>We introduce FlowPathAgent, a neurosymbolic agent specifically designed to perform fine-grained as a post-hoc flowchart attribution. Instead of relying solely on text-based or vision-based cues, FlowPathAgent integrates symbolic reasoning by using an agentic interface to interact with the flowchart as a graph object. FlowPathAgent begins with segmenting flowcharts into distinct components, followed by constructing symbolically operable flowchart representations. These graphbased representations have direct correspondence to visual regions of the flowchart, enabling the model to interoperate between the visual and symbolic representations. We leverage graph tools to extract and manipulate these representations, allowing for identification of relevant nodes and edges. Our methodology facilitates precise attribution of the model's reasoning steps to specific decision points within the flowchart, providing accurate and interpretable explanations of the model's output. Experimental results demonstrate that FlowPathAgent significantly outperforms strong baselines <ref type="bibr" target="#b13">(Lai et al., 2024;</ref><ref type="bibr" target="#b22">Peng et al., 2023;</ref><ref type="bibr">Yuan et al., 2025</ref>) by 10-14% on FlowExplainBench. Our main contributions<ref type="foot" target="#foot_0">foot_0</ref> are:</p><p>• We introduce a new task of Fine-grained Flowchart Attribution where the goal is to identify the optimal path within a flowchart diagram that grounds the LLM text response.</p><p>• FlowExplainBench -a novel evaluation benchmark consisting of 1k+ high quality attribution annotations over flowchart QA with diverse styles, domains, and question types.</p><p>• FlowPathAgent -a neurosymbolic agent capable of performing fine-grained post-hoc attribution for flowchart QA. FlowPathAgent uses a VLM-based agentic approach to perform graph-based reasoning and symbolic manipulation to accurately trace the decision process within flowcharts.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Flowchart Understanding</head><p>Research in flowchart understanding has evolved from basic image processing to complex reasoning tasks. Modern deep learning approaches, such as FR-DETR <ref type="bibr">(Sun et al., 2022a)</ref>, have significantly improved symbol and edge detection through endto-end architectures that combine CNN backbones with multi-scale transformers. The emergence of LLMs has led to benchmarks like FlowchartQA <ref type="bibr" target="#b30">(Tannert et al., 2023)</ref>, FlowLearn <ref type="bibr" target="#b21">(Pan et al., 2024)</ref>, SCI-CQA <ref type="bibr" target="#b25">(Shen et al., 2024)</ref>, and FlowVQA <ref type="bibr" target="#b27">(Singh et al., 2024)</ref>, which assess geometric understanding, spatial reasoning, and logical capabilities of models for question-answering on flowchart images . Recent work like <ref type="bibr">(Ye et al., 2024)</ref> has begun exploring alternatives to end-to-end VLMs; <ref type="bibr">(Ye et al., 2024)</ref> introduced intermediate textual representations between visual processing and reasoning steps for Flowchart QA; <ref type="bibr" target="#b16">(Liu et al., 2022;</ref><ref type="bibr" target="#b26">Shukla et al., 2023)</ref> explored code generation from flowcharts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attribution in LLMs</head><p>Large Language Models (LLMs) are challenged with factual accuracy <ref type="bibr" target="#b14">(Zhang et al., 2023)</ref>. While various solutions have emerged, including citationaware training <ref type="bibr" target="#b7">(Gao et al., 2023)</ref> and tool augmentation <ref type="bibr">(Ye et al., 2023)</ref>, ensuring reliable attributions remains crucial. Three primary attribution strategies have emerged in literature:</p><p>(1) Direct model-driven attribution generates answers and attributions simultaneously <ref type="bibr" target="#b24">(Peskoff and Stewart, 2023;</ref><ref type="bibr">Sun et al., 2022b)</ref>.</p><p>(2) Post-retrieval answering retrieves information before answering <ref type="bibr">(Ye et al., 2023;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b11">Huo et al., 2023;</ref><ref type="bibr" target="#b3">Chen et al., 2023)</ref>.</p><p>(3) Post-hoc attribution generates answers first and then searches for supporting references <ref type="bibr">(Li et al., 2023a)</ref>. Our work falls in the scope of Post-hoc attribution, as it serves as a modular approach integrable with existing system, without accessing the response generation mechanism.</p><p>Recent work has expanded attribution capabilities to handle diverse data formats. While MATSA <ref type="bibr" target="#b19">(Mathur et al., 2024)</ref> explored fine-grained attribution for tables through a multi-agent approach, VISA <ref type="bibr" target="#b18">(Ma et al., 2024)</ref> advanced visual attribution by leveraging vision-language models to highlight specific regions in document screenshots. Ours is the frst work on flowchart QA attribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Post-hoc Flowchart Attribution</head><p>We formalize fine-grained post-hoc Flowchart Attribution as follows: Given a dataset D consisting of a set of flowchart images F, each flowchart image c i ∈ F, c i = I w×h×3 corresponds to a logical graph representation</p><formula xml:id="formula_0">G i = (V i , E i ), where V i rep-</formula><p>resents the set of nodes and E i represents the edges between them. Each node corresponds to a logical operation or directive statement, and the edges represent the flow between these operations. Additionally, the input includes a flowchart-referring statement s i , which is a natural language description of a process or action to be grounded in the flowchart image. The underlying goal is to find a path in the image that grounds the statement s i . This path may be disjoint, but it should correspond to a set of regions in the flowchart image. The regions are the physical abstraction that corresponds to the logical nodes in the graph. Formally, the task can be represented as a mapping function:</p><formula xml:id="formula_1">F : (c i , s i ) → R s i ,</formula><p>where F maps the flowchart image c i and the statement s i to a set of regions R s i in the image. R s i = {r i1 , r i2 , . . . , r in } represents the sequence of regions in the image that correspond to a path of logical nodes, and the edges included between consecutive nodes v i1 , v i2 , . . . , v in in the graph G i , grounding the statement s i . The path may be disjoint, but it should satisfy the following criteria:</p><p>1. Optimality: The path should be the shortest sequence of regions that ground the statement s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Contextual Alignment:</head><p>The path should correspond to the relevant actions and decisions described in s, matching the flow of the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Exclusivity:</head><p>No additional regions outside of R s i are necessary to fully explain the statement s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FlowExplainBench</head><p>To enable systematic evaluation of flowchart attribution, we introduce FlowExplainBench, a comprehensive benchmark designed with four key criteria: diverse visual styles, varied question types, multiple flowchart domains, and faithful groundtruth attributions (see Table <ref type="table">1</ref>). Each entry in the dataset consists of the following components: the flowchart image c, a statement s (which, in this context, is a Question-Answer pair), a set of attributed logical nodes v 1 , v 2 , . . . , v n , and their corresponding visual regions R s = {r 1 , r 2 , . . . , r n }. These visual regions represent the physical abstractions of the logical nodes, which are mapped from the flowchart image c as discussed in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sources</head><p>FlowExplainBench is constructed using the test split of the FlowVQA dataset <ref type="bibr" target="#b27">(Singh et al., 2024)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attribution Annotation</head><p>The attribution annotation process is as follows:</p><p>Step 1: Automatic Labeling. We use GPT-4 to perform the initial attribution for corresponding QA pairs directly in the Mermaid source code. By analyzing the nature of different question types, we generalize the attribution patterns and provide GPT-4 with few-shot examples in the prompt.</p><p>Step 2: Human Verification. Two human evaluators are involved in the next stage, where they interact with an attribution platform that allows them to select nodes in the flowchart to be attributed. The inter-annotator agreement, measured using Cohen's Kappa (κ), shows a high level of agreement both between the two annotators (κ = 0.89) and between the annotators and the initial GPT-4-generated labels (κ = 0.72, 0.80). More details on human annotation in Appendix Sec. C.3.</p><p>Step 3: Multi-step Question Filtering. We applied a filtering srategy to get rid of trivial and low-quality QA pairs. Questions related to node and edge count were excluded, as they required trivially attributing the entire graph rather than reasoning over its fine-grained individual components. This excluded 1792 samples from the annotation exercise described above. Subsequently, for each flowchart image, questions with the highest agreement among annotators were selected, prioritizing cases where both human annotators concurred. This yielded an initial set of 953 samples.</p><p>To achieve balance across three domains and four question types, additional high-agreement samples were selected from underrepresented categories, resulting in a final benchmark of 1,238 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FlowPathAgent</head><p>FlowPathAgent (Fig 2 ), is a neurosymbolic agent designed for structured reasoning over flowcharts for fine-grained flowchart attribution. The approach consists of three key stages: Chart Component Labeling, which segments and labels chart components; Graph Construction, which constructs a symbolic graph from the labeled flowchart; and Neurosymbolic Agent-based Analysis, which uses graph-based tools to interact with the symbolic flowchart to generate attributed paths. Each stage plays a critical role in bridging the gap between visual representations and symbolic reasoning over structured workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Chart Component Labeling</head><p>We identify and label individual flowchart components, ensuring an explicit correspondence be- tween visual elements and the symbolic representations generated in subsequent steps. FlowMask2Former. To achieve flowchart component recognition, we construct a synthetic dataset using the training split of FlowVQA <ref type="bibr" target="#b27">(Singh et al., 2024)</ref>, incorporating style diversification techniques similar to those described in Section 4.2, but with different color schemes. Further the node content is replaced with randomized text. These augmentations improve domain generalization and ensure robust performance across diverse flowchart styles. We fine-tune Mask2Former (?) on this dataset for instance segmentation, specifically targeting node recognition. The fine-tuned model, FlowMask2Transformer, generates segmentation maps, from which individual nodes are sequentially labeled using alphabetical identifiers, rendered in red text on the flowchart image, to serve as visual anchors for graph construction, reasoning, and node referencing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Construction</head><p>Flowcharts inherently encode structured logical processes, making graph-based representations ideal for symbolic reasoning. By converting visual flowcharts into symbolic graph structures, we eliminate reliance on visual recognition for every reasoning step, ensuring robust handling of distant relationships that visual models often misinterpret. The symbolic graph facilitates efficient graph-based operations such as traversal, topologi-cal analysis, and conditional evaluation. This structured representation also enhances interpretability and enables automated verification of logical consistency. Moreover, as flowchart complexity increases, our method avoids the compounding errors seen in purely visual models by explicitly encoding edge conditions and node relationships, enabling reliable and scalable path tracing.</p><p>Flow2Mermaid VLM. To convert the labeled flowchart to a symbolic graph representation, we first convert the visual flowchart to a Mermaid code, and then parse the Mermaid code to generate the symbolic graph. For the Flowchart to Mermaid transformation, we fine-tune Qwen2-VL(7B) <ref type="bibr" target="#b31">(Wang et al., 2024)</ref> using supervised finetuning (SFT) on a style-diversified projection of the FlowVQA <ref type="bibr" target="#b27">(Singh et al., 2024)</ref> training set, with marked alphabetic node labels sourced from SVG metadata. Flow2Mermaid VLM is trained to generate Mermaid flowchart code directly from flowchart images, using the alphabetical node labels as anchors to maintain consistency between visual and symbolic representations. We perform fine-tuning to improve the ability to generate accurate and semantically robust Mermaid syntax, minimize structural inconsistencies that could affect graph analyses, and adapt to the varied aspect ratios and visual styles found in flowcharts.</p><p>The generated Mermaid representation is then parsed into a symbolic graph, tailored to capture the specific properties of flowcharts, including boolean conditional edges and node-level statement mappings. Additionally, we define a comprehensive suite of tools to operate on this symbolic graph, enabling structured function calls for reasoning over the flowchart's logical structure. The list of tools and their API is described in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neurosymbolic Agent</head><p>FlowPathAgent employs a neurosymbolic reasoning approach to attribute relevant nodes based on an input statement. In this context, it combines neural models i.e. VLMs to plan, reason and attribute in a discrete token space, based on observations made via tool use over a symbolic graph representing the flowchart. The agent operates on a sequence of interdependent steps: 1. Node Selection: During the initial planning stage, our agent identifies nodes to be explored by referencing their corresponding labels in the flowchart image. Additionally, it clarifies the expectation and underlying rationale for each node selection. This is the only step where the labeled flowchart image is passed to the underlying VLM. 2. Tool Selection: Our agent employs reasoningbased prompting to determine the necessary symbolic tools and their respective functional parameters for the selected nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tool Execution:</head><p>The selected tools are executed on the symbolic graph representation to extract relevant insights. Multiple sequential cycles of Tool Selection and Tool Execution may occur, with each cycle selecting and executing a single tool. 4. Tool Response Analysis: The agent interprets observations from tool-use, in relation to the given statement, generating a path of nodes in the flowchart that attribute the statement. 5. Mapping to Original Flowchart: Finally, the attributed path's node labels are mapped back onto the flowchart image using the segmentation regions obtained during the labeling stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation</head><p>To map the segmented regions to the ground truth nodes, we apply an Intersection over Union (IoU) threshold of 0.7 to ensure high fidelity between ground-truth and predicted nodes. The groundtruth node with the maximum overlap is selected as the reference for the segmented node. This process is crucial for fine-grained attribution, where accurate identification of individual flowchart components is required. For each baseline, we collect the nodes identified by the model and treat the set of ordered nodes as the attributed path. We then compute the micro-averaged Precision, Recall, and F1 scores to assess the model's performance. More extensive experimental details have been provided in Appendix Sec. A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Discussion</head><p>Baseline Comparison.</p><p>FlowPathAgent demonstrates a significant improvement over all baseline models when evaluated on the FlowExplainBench, outperforming them by a margin of 6-65 percentage points, as shown in Table <ref type="table" target="#tab_2">2</ref>. Visual grounding models, including Kosmos-2, LISA, and SA2VA, exhibit suboptimal performance. This is primarily due to their limited ability to process visual logic, which is crucial for fine-grained flowchart attribution. These models struggle to correctly map logical relationships between elements in the flowchart, resulting in less accurate attributions.</p><p>Among the baselines, GPT4o Zero Shot Bounding Box shows the poorest performance. This model lacks inherent capabilities for mask generation, and instead generates bounding boxes in the textual token space, which is not well-suited for the task of flowchart attribution.   Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p><p>Step 5</p><p>Step 6</p><p>Step 7</p><p>Step 8 In contrast, GPT4o SoM achieves a comparatively stronger performance. This can be attributed to the effective segmentation abilities of Flow-Mask2Former, which ensures that the elements to be attributed are accurately captured in the candidate set available to the model. Additionally, the reasoning capabilities of GPT4o contribute to improved performance by leveraging these segmented components in a logical manner. VisProg relies on weaker visual back-bones which do not understand images with text, leading to low detection rates, including none in FEBench-Wiki.</p><p>Further analysis of performance trends reveals an interesting behavior when we examine the per-formance of different models against the number of nodes in the flowchart, as illustrated in Fig 3 . 
As the complexity of the flowchart increases (i.e., as the number of nodes decreases), a performance dip is observed across all methods. This is likely due to the increasing difficulty in processing larger, more complex flowcharts, especially for models relying heavily on the visual presentation of the flowchart. In contrast, FlowPathAgent maintains a more consistent performance, with a relatively smaller dip in performance despite the increased complexity. This can be attributed to the model's ability to treat flowchart elements as logical entities, rather than solely relying on their visual representation. By leveraging its neurosymbolic approach, FlowPathAgent is able to more effectively process and attribute complex flowchart structures, providing robust and reliable attributions even in the long tail of node distributions. Qualitative Analysis. Fig 5 presents a qualitative comparison between FlowPathAgent and various baseline models. The GPT4o Zero Shot Bounding Box baseline fails to generate bounding boxes that overlap with or match the shape and dimensions of any flowchart nodes. On the other hand, LISA tends to overgeneralize by attributing the entire flowchart image, producing small, noisy masks that cover irrelevant areas, which reduces the clarity and precision of its attributions. Kosmos-2 also struggles with segmenting the nodes associated with the statement; it segments a single irrelevant node. SA2VA, while performing better than the other visual grounding models, still exhibits limitations. It generates low IoU masks around some correct nodes. Additionally, it sometimes produces extraneous masks that are not relevant to the flowchart's logical structure. GPT4o with SoM shows some improvement, but tends to over-attribute by including steps that are further ahead in the flowchart than necessary. In contrast, FlowPathAgent excels by accurately detecting and attributing the entire flowchart path, Question: A landscape architect, Sophia, is working on a garden design that was initially measured in yards. However, the international team she's collaborating with uses the metric system. Sophia opts for an online calculator to convert the measurements but is unsure whether altitude plays a role in this scenario. Should Sophia make any adjustments before finalizing the conversion?</p><p>Answer: Sophia should continue without altitude adjustment unless the specific context of her work requires it.  Error Propagation: An inherent limitation in modular agentic systems is that inaccuracies in one component can affect downstream results. On the full benchmark dataset, FlowMask2Former achieved a high overall Jaccard Similarity (IoU &gt; 0.5) of 0.98, and Flow2Mermaid VLM obtained a Word F1 score of 0.89. To better understand the relationship between segmentation quality and transcription fidelity, we performed a binned analysis in Table <ref type="table">3</ref>, grouping samples by segmentation IoU. Complementing this, Fig. <ref type="figure" target="#fig_7">6</ref> visualizes the correlation between IoU and Word Overlap F1 for individual data points, colored by overall </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statement Flowchart (with attributions)</head><formula xml:id="formula_2">B -&gt; C -&gt; F -&gt; G -&gt; J -&gt; M -&gt;N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced the task of Flowchart Attribution and proposed FlowExplainBench for evaluating fine-grained visual grounding in flowchart QA. We presented FlowPathAgent, a neurosymbolic agent that leverages graph-based reasoning to accurately identify the optimal path underpinning LLM responses. Experimental results demonstrate significant improvements over existing baselines, highlighting the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations</head><p>While our approach demonstrates strong performance, there are areas for further improvement. First, although FlowPathAgent effectively integrates symbolic reasoning, it builds on FlowMask2Former for segmentation and Flow2Mermaid VLM for converting visual flowcharts to mermaid code. As with any modular system, potential errors in these components may influence overall performance. However, our framework remains flexible, allowing for seamless integration of alternative models better suited to specific scenarios. Second, our benchmark, FlowExplainBench, captures a diverse range of flowchart structures but does not yet encompass all real-world variations, such as hand-drawn diagrams. The primary challenge lies in the availability of high-quality datasets with comprehensive annotations. While existing methods address hand-drawn flowchart segmentation, scaling them for attribution remains an open area of research. Future work could explore semisupervised or automated annotation strategies to enhance coverage.</p><p>Lastly, our approach is designed for static flowcharts, and extending it to dynamic or interactive systems presents an opportunity for further research. Many real-world applications involve evolving decision-making processes, which could benefit from models that handle sequential updates and conditional dependencies.</p><p>Future work could address these limitations by improving segmentation robustness, expanding the benchmark to include more diverse flowchart types, and developing models capable of handling dynamic and interactive flowcharts. Additionally, integrating reinforcement learning or self-supervised learning techniques could enhance model adaptability and generalization across various flowchart formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Ethics Statement</head><p>In this study, we utilize the publicly accessible FlowVQA dataset, which is distributed under the MIT License<ref type="foot" target="#foot_1">foot_1</ref> . We ensure that the identities of human evaluators remain confidential, and no personally identifiable information (PII) is used at any stage of our research. This work is focused exclusively on applications for fine-grained visual flowchart attribution and is not intended for other use cases. We also recognize the broader challenges associated with large language models (LLMs), including potential risks related to misuse and safety, and we encourage readers to consult the relevant literature for a more detailed discussion of these issues <ref type="bibr">(Kumar et al., 2024;</ref><ref type="bibr" target="#b4">Cui et al., 2024;</ref><ref type="bibr" target="#b17">Luu et al., 2024)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>The facebook/mask2former-swin-tiny-coco -instance model is fine-tuned for 20 epochs for FlowMask2Former, employing a learning rate of 1 × 10 -5 with a cosine annealing scheduler.</p><p>A batch size of 4 is used, and gradient accumulation occurs over 4 steps to address memory constraints. Training is conducted using 16-bit precision to improve computational efficiency. In the case of the Mermaid2Graph Vision-Language Model (VLM), fine-tuning is performed on the unsloth/Qwen2-VL-7B-Instruct checkpoint, focusing on vision, language, attention, and MLP layers. This model is trained for 3 epochs with a batch size of 1 and gradient accumulation over 5 steps. A learning rate of 2 × 10 -4 is applied with a linear scheduler. To optimize memory usage, the model is loaded in 4-bit precision, and AdamW is used as the optimizer with a weight decay of 0.01. The baseline models are initialized as follows: LISA from xinlai/LISA-13B-llama2-v1, Kosmos-2 from microsoft/kosmos-2-patch14-22, and Sa2VA from ByteDance/Sa2VA-8B. Default settings and parameters are used for all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Computational Budget</head><p>Table <ref type="table" target="#tab_6">4</ref> shows the computational budget for this paper, broken down by associated tasks.     mance ranking, they have been deliberately chosen to highlight specific limitations and failure cases of each method. This selection aims to provide insights into the scenarios where certain approaches struggle, offering a clearer understanding of their weaknesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Agent Analysis B.1 API Description</head><p>Fig. <ref type="figure" target="#fig_19">18</ref> shows the class diagram of the data structure used to represent Nodes, Edges, and the Flowchart. The FlowChart class serves as the primary structure, managing a collection of Node objects, each identified by a unique ID and containing a statement. Nodes are interconnected through Edge objects, which define directed relationships with optional conditions (Yes, No, or unconditional).</p><p>Table <ref type="table" target="#tab_8">6</ref> summarises the API for the tools provided to FlowPathAgent.</p><p>Except for final_answer which returns the final answer and reasoning involved, all other tools operate on a global FlowChart object initiated from mermaid code generated by Flow2Mermaid VLM. We analyze the distribution of tool calls by question type in Fig 8 . Topological questions show the most diversity in terms of tool calls, since they require interpreting structural aspects of the FlowChart. get_statement is the most common tool for the other question types. This is because all the other questions require content from inside the flowchart, and often involve multiple calls of get_statement in a single agentic run. For flowreferential questions, get_neighbours is a popular tool, since this tool allows downstream flow analysis from an anchor node. We implemented  FlowPathAgent using HuggingFace's smolagents 3 library. We patched the library to ensure that visual tokens are only used in the planning step (node selection step), and removed from the conversation template thereafter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Prompts and Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Benchmark Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Human Annotation</head><p>We employed two graduate student annotators, aged 22-25. The annotators were proficient in English, and were exposed to flowchart QA samples from the training set before the annotation exercise, to make them comfortable with the flowcharts involved. The annotators were fairly compensated at the standard Graduate Assistant hourly rate, following their respective graduate school policies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional analysis on hand-constructed charts</head><p>We conducted a supplemntary case study to analyze FlowPathAgent's generalization to realworld handwritten charts. Data: Given the lack of Question-Answer datasets for hand-drawn charts, we used the FC_BScan <ref type="bibr" target="#b1">(Bresler et al., 2016)</ref> dataset for hand-drawn flowchart component recognition. We randomly selected 50 samples from the test set, and used prompted GPT4o with example questions from <ref type="bibr" target="#b27">(Singh et al., 2024)</ref> to generate Question-Answer pairs. An annotator manually annotated ground-truth attributions for the selected samples. We used the train set to train FlowMask2Former for this domain.</p><p>Table <ref type="table" target="#tab_10">7</ref> compares results from the chosen baselines.  This example represents Tolopolgical and Flow Referential questions, and has a style type 3 (mermaid default).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Precision Recall F1 Kosmos-2 <ref type="bibr" target="#b22">(Peng et al., 2023)</ref> 7.34 3.12 4.43 LISA <ref type="bibr" target="#b13">(Lai et al., 2024)</ref> 16.52 30.13 21.50 SA2VA <ref type="bibr">(Yuan et al., 2025)</ref> 22.12 5.13 8.25 VisProg <ref type="bibr" target="#b9">(Gupta and Kembhavi, 2022)</ref> 0.00 0.00 0.00     You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can. The task you have been asked to solve is performing flowchart attribution. This tasks takes as input a flowchart image, and some text (like question-answer pair), and finds out which flowchart nodes explain, and are relevant to the text. You need to find the minimum set of nodes, that are directly associated with the statements.</p><p>To do so, you have been given access to some tools.</p><p>The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an "observation".</p><p>This Action/Observation can repeat N = 8 times, you should take several steps when needed.</p><p>Here are the rules you should always follow to solve your task:</p><p>1. ALWAYS provide a tool call, else you will fail. You need to call tools even if you think you know the answer already. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes No</head><p>Your task is to attribute nodes in the flowchart, which are relevant to the provided statements.</p><p>To do this, you will need to perform some graph operations, using the tools provided for you. These tools mostly operate on a node level, by referencing nodes with their identifiers, such as 'A', 'G' or 'AF' (letter based, labeled on the flowchart).</p><p>You will now decide a plan for this task. To do so, you will have to read the task and identify things that must be discovered in order to successfully complete it. Don't make any assumptions. For each item, provide a thorough reasoning. Here is how you will structure this survey:</p><p>1. Nodes that need to be explored List the specific nodes you want to explore with different tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Facts to look up</head><p>List here any facts that we may need to look up. Also list how to find these: which tools, what nodes and arguments you will call on these tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reasoning</head><p>The reasons for picking the specific tools, and choosing the nodes to explore. Note that some functions do not need you to explore nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Answer:      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attribution (represented by •-•-•) with FlowPathAgent ensures logical consistency in flowchart-based reasoning. FlowPathAgent uses a neurosymbolic approach to generate attribution paths ( ➊ &amp; ➋) in the flowchart. This enhances interpretability and reliability in flowchart driven automated decision-making.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of FlowPathAgent. FlowPathAgent processes a flowchart image through segmentation-based component labeling, constructs a symbolic graph representation using Mermaid, and employs a neurosymbolic agent, that treats the flowchart as a symbolic graph to attribute nodes based on an input statement. The agent interacts with predefined tools to analyze and traverse the flowchart structure, producing attributions as interpretable mappings of relevant nodes back onto the original flowchart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the long tail of node distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison of FlowPathAgent against baselines demonstrates superior effectiveness across long-tail distribution of node count in flowcharts.</figDesc><graphic coords="7,73.79,207.20,215.29,143.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Flow diagram of the sequence of tools used by FlowPathAgent on FlowExplainBench. Each Step refers to a cycle of Tool Selection + Call.</figDesc><graphic coords="7,96.45,431.64,167.60,76.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>A pair involves the process of using an online calculator for conversion and considering altitude adjustment.-**Node B** is where the input of yards is initiated.-**Node C** checks the availability of an online calculator.-**Node F** involves using the online calculator.-**Node G** asks if altitude adjustment is needed.-**Node J** indicates continuing without altitude adjustment.-**Node M** is where the converted value is obtained.-**Node N** marks the end of the process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative comparison of FlowPathAgent with baseline methods. The flowchart illustrates attributions generated by various baselines, highlighting the agentic trace of FlowPathAgent. We contrast its output with the next strongest baseline, GPT-4o+SoM, to showcase differences in attribution quality and interpretability..</figDesc><graphic coords="8,72.00,80.21,87.89,139.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Scatter plot of segmentation IoU versus Word Overlap F1 for individual samples, color-coded by overall task F1. Clustering in the high-performance region indicates minimal error propagation across the pipeline.</figDesc><graphic coords="8,338.71,296.68,151.99,124.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Box-plot distribution of time taken in each tool call, in seconds.</figDesc><graphic coords="12,306.14,72.00,217.12,158.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Heatmap of time taken by different tools to execute, binned by number of nodes in the flowchart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Heatmap of duration of tool call execution, arranged by agentic step.</figDesc><graphic coords="12,306.14,280.62,217.14,83.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>ExamplesFigure 12 :</head><label>12</label><figDesc>Figure 12: Overview of training split used for FlowMask2Former, and Flow2Mermaid VLM. The figure demonstrates the style options, color palettes used, and distinction between both training sets.</figDesc><graphic coords="13,326.05,287.40,177.77,101.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig</head><label></label><figDesc>Fig 19 and 20 represent the system prompt template, and planning prompt template used by FlowPathAgent.We implemented</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>C. 1</head><label>1</label><figDesc>Fig 21 represents the prompt template used to perform automatic annotations, in step 1 of our ground truth annotation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig</head><label></label><figDesc>Fig 23 shows a summary of the annotator guidelines, and Fig 24 shows the annotation platform used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 13: Example from FlowExplainBench-Instruct.This example represents an Applied Scenario question, and has a style type 1 (single color).</figDesc><graphic coords="15,72.00,153.48,89.20,481.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc> represent qualitative examples of FlowPathAgent's performance. We observe, that due to the neuro-symbolic approach used by our agent, it is able to generalize across style variations and is is robust to errors in intermediate steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure 15: Example from FlowExplainBench-Instruct.This example represents Tolopolgical and Flow Referential questions, and has a style type 3 (mermaid default).</figDesc><graphic coords="16,72.40,132.39,100.51,522.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Question:</head><label></label><figDesc>Figure 17: (Continued) Qualitative comparison of FlowPathAgent with baselines via examples.</figDesc><graphic coords="18,74.42,468.56,441.90,178.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Class Diagram of the FlowChart data structure representing directed graphs with conditional edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>2.Figure 19 :</head><label>19</label><figDesc>Figure 19: System prompt template provided to FlowPathAgent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Planning prompt template provided to FlowPathAgent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 :Figure 22 :</head><label>2122</label><figDesc>Figure 21: Prompt Template used for initial automatic ground truth annotation using GPT4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: FlowPathAgent attributed E correctly. The blocks and labels represent FlowMask2Former annotations.</figDesc><graphic coords="23,72.00,468.59,217.13,239.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: FlowPathAgent attributed D correctly. The blocks and labels represent FlowMask2Former annotations.</figDesc><graphic coords="23,306.14,80.68,217.13,255.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: FlowPathAgent attributed B, and E correctly. The blocks and labels represent FlowMask2Former annotations.</figDesc><graphic coords="23,338.71,406.99,152.00,312.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: FlowPathAgent attributed A, B, C, E. The blocks and labels represent FlowMask2Former annotations.</figDesc><graphic coords="24,72.00,270.40,217.14,259.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,74.43,380.00,446.58,136.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of FlowPathAgent with baselines on FlowExplainBench. Best and second-best results have been highlighted.</figDesc><table><row><cell>Baseline</cell><cell cols="2">Overall Precision Recall</cell><cell>F1</cell><cell cols="2">FEBench-Code Precision Recall</cell><cell>F1</cell><cell cols="2">FEBench-Wiki Precision Recall</cell><cell>F1</cell><cell cols="3">FEBench-Instruct Precision Recall F1</cell></row><row><cell>Kosmos-2 (Peng et al., 2023)</cell><cell>37.14</cell><cell>1.76</cell><cell>3.36</cell><cell>41.41</cell><cell>6.45</cell><cell>11.16</cell><cell>20.69</cell><cell>0.31</cell><cell>0.60</cell><cell>38.30</cell><cell>1.64</cell><cell>3.14</cell></row><row><cell>LISA (Lai et al., 2024)</cell><cell>18.01</cell><cell cols="2">14.34 15.97</cell><cell>35.36</cell><cell cols="2">19.18 24.87</cell><cell>14.09</cell><cell cols="2">11.74 12.81</cell><cell>18.45</cell><cell cols="2">16.18 17.24</cell></row><row><cell>SA2VA (Yuan et al., 2025)</cell><cell>66.36</cell><cell>9.88</cell><cell>17.20</cell><cell>79.35</cell><cell cols="2">19.34 31.10</cell><cell>58.47</cell><cell>7.40</cell><cell>13.14</cell><cell>65.99</cell><cell>8.82</cell><cell>15.56</cell></row><row><cell>VisProg (Gupta and Kembhavi, 2022)</cell><cell>45.95</cell><cell>0.46</cell><cell>0.91</cell><cell>46.88</cell><cell>2.30</cell><cell>4.49</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>25.00</cell><cell>00.09</cell><cell>0.18</cell></row><row><cell>GPT4o Bounding Box</cell><cell>58.82</cell><cell>1.90</cell><cell>3.68</cell><cell>80.00</cell><cell>1.89</cell><cell>3.69</cell><cell>53.19</cell><cell>1.29</cell><cell>2.51</cell><cell>57.89</cell><cell>3.00</cell><cell>5.70</cell></row><row><cell>GPT4o SoM</cell><cell>74.10</cell><cell cols="2">67.69 70.75</cell><cell>67.32</cell><cell cols="2">70.28 68.77</cell><cell>74.55</cell><cell cols="2">65.03 69.47</cell><cell>77.84</cell><cell cols="2">70.91 74.22</cell></row><row><cell>FlowPathAgent</cell><cell>77.19</cell><cell cols="2">77.21 77.20</cell><cell>74.18</cell><cell cols="2">80.62 77.27</cell><cell>76.29</cell><cell cols="2">74.21 75.23</cell><cell>80.28</cell><cell cols="2">80.19 80.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>JianweiYang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023a. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>videos, combining SAM-2 for segmentation and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LLaVA for vision-language tasks, enabling robust</cell></row><row><cell></cell><cell></cell><cell></cell><cell>performance in referring segmentation.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VisProg (Gupta and Kembhavi, 2022) is an agent</cell></row><row><cell cols="3">Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023b. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint</cell><cell>that generates interpretable visual programs by de-composing queries into executable steps, enabling modular and explainable visual reasoning.</cell></row><row><cell cols="2">arXiv:2309.17421, 9(1):1.</cell><cell></cell><cell>GPT4o + FlowMask2Former SoM Prompting,</cell></row><row><cell cols="3">Junyi Ye, Ankan Dash, Wenpeng Yin, and Guil-</cell><cell>as an ablation study, we incorporate this baseline</cell></row><row><cell>ing Wang. 2024.</cell><cell cols="2">Beyond end-to-end vlms:</cell><cell>where GPT-4o utilizes the segmented flowchart</cell></row><row><cell cols="3">Leveraging intermediate text representations for</cell><cell>generated by FlowMask2Former, and applies Set-</cell></row><row><cell cols="2">superior flowchart understanding. arXiv:2412.16420.</cell><cell>Preprint,</cell><cell>of-Marks (SoM) (Yang et al., 2023a) prompting to guide the model's predictions.</cell></row><row><cell cols="3">Xi Ye, Ruoxi Sun, Sercan Ö Arik, and Tomas Pfis-</cell></row><row><cell cols="3">ter. 2023. Effective large language model adap-</cell></row><row><cell cols="3">tation for improved grounding. arXiv preprint</cell></row><row><cell>arXiv:2311.09533.</cell><cell></cell><cell></cell></row><row><cell cols="3">Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang,</cell></row><row><cell cols="3">Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Ji-</cell></row><row><cell cols="3">ashi Feng, and Ming-Hsuan Yang. 2025. Sa2va:</cell></row><row><cell cols="3">Marrying sam2 with llava for dense grounded</cell></row><row><cell cols="3">understanding of images and videos. Preprint,</cell></row><row><cell>arXiv:2501.04001.</cell><cell></cell><cell></cell></row><row><cell cols="3">Anthony E Zimmermann, Ethan E King, and Dipti-</cell></row><row><cell cols="3">man D Bose. 2024. Effectiveness and utility of</cell></row><row><cell cols="3">flowcharts on learning in a classroom setting: A</cell></row><row><cell cols="3">mixed-methods study. American Journal of Pharma-</cell></row><row><cell cols="2">ceutical Education, 88(1):100591.</cell><cell></cell></row><row><cell>A Further Details</cell><cell></cell><cell></cell></row><row><cell>A.1 Baselines</cell><cell></cell><cell></cell></row><row><cell cols="3">Zero-shot GPT-4o Bounding Box We use GPT-</cell></row><row><cell cols="3">4o (OpenAI, 2024) to predict normalized bounding</cell></row><row><cell cols="3">box coordinates for chart components based on text</cell></row><row><cell cols="3">and the visual chart, following established methods</cell></row><row><cell cols="3">for zero-shot localization (Yang et al., 2023b).</cell></row><row><cell cols="3">Kosmos-2:(Peng et al., 2023) is a multimodal</cell></row><row><cell cols="3">large language model that combines text-to-visual</cell></row><row><cell cols="3">grounding, supporting tasks like referring expres-</cell></row><row><cell cols="3">sion interpretation and bounding box generation</cell></row><row><cell cols="2">by linking objects in images with text.</cell><cell></cell></row><row><cell cols="3">LISA: (Li et al., 2023b) is a model for generating</cell></row><row><cell cols="3">segmentation masks from textual queries, extend-</cell></row><row><cell cols="3">ing VLM capabilities to segmentation tasks, and</cell></row><row><cell cols="3">excels in zero-shot performance with minimal fine-</cell></row><row><cell cols="2">tuning on task-specific data.</cell><cell></cell></row><row><cell cols="3">SA2VA(Yuan et al., 2025) is a unified model for</cell></row><row><cell cols="3">dense grounded understanding of both images and</cell></row></table><note><p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren's song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Computational Budget for experiments in the paper</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Tools provided to FlowPathAgent.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note><p>Case Study: Performance comparison on Handwritten Charts .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code and data will be released on acceptance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/flowvqa/flowvqa?tab= MIT-1-ov-file</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question: A landscape architect, Sophia, is working on a garden design that was initially measured in yards. However, the international team she's collaborating with uses the metric system. Sophia opts for an online calculator to convert the measurements but is unsure whether altitude plays a role in this scenario. Should Sophia make any adjustments before finalizing the conversion?</p><p>Answer: Sophia should continue without altitude adjustment unless the specific context of her work requires it.</p><p>Question: AImagine you're working on a financial application where users can add transactions to their accounts. When a user attempts to add a transaction for an existing item, what initial value is assigned to 'old' before the transaction value is added to it?</p><p>Answer: The initial value of 'old' is the value retrieved using 'UserDict.__getitem__'."</p><p>Question: While refactoring the plotting capabilities in a data analysis library, Marissa is iterating over an array of plotter objects. She comes across an object with a 'psy.plotter' attribute that is neither nonexistent nor None. What should Marissa do to this plotter object before moving to the next one?</p><p>Answer: Set the object's 'psy.plotter.disabled' property to True. Annotator Guidelines: Flowchart Attribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Task Overview</head><p>Flowchart attribution involves identifying and selecting the relevant nodes in a flowchart that correspond to a given natural language statement. Annotators will interact with an attribution platform to highlight the appropriate nodes that form a logical path grounding the statement in the flowchart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Annotation Process</head><p>Step 1: Understanding the Statement</p><p>• Carefully read the natural language statement provided.</p><p>• Identify key actions, decisions, or processes described in the statement.</p><p>Step 2: Examining the Flowchart</p><p>• Analyze the structure of the flowchart to understand the logical flow.</p><p>• Identify nodes that contain relevant operations or directives that match the statement.</p><p>Step 3: Selecting the Attributed Nodes</p><p>• Highlight nodes that directly contribute to fulfilling the statement's described process.</p><p>• Ensure the selected nodes follow a logical sequence, even if they are not directly connected.</p><p>• Use the following criteria to determine node inclusion: ○ Optimality: Select the minimal set of nodes required to fully attribute the statement. ○ Contextual Alignment: Ensure the selected nodes accurately represent the described actions or decisions. ○ Exclusivity: Avoid selecting unnecessary nodes that do not contribute to the statement's meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rubric for Flowchart Attribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Criterion</head><p>Description Score (0-2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimality</head><p>The minimal set of nodes required to ground the statement is selected. 0 = Excessive or missing nodes, 1 = Some unnecessary nodes, 2 = Only essential nodes chosen</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Alignment</head><p>The selected nodes logically represent the statement's described actions or decisions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Answer Pair</head><p>What is compared with 'self.mode' to find a match in the 'MODES' dictionary?</p><p>The 'id' from each item is compared with 'self mode'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B C</head><p>Selected Nodes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B C</head><p>Instructions:</p><p>1. Select node types from the dropdown (you can select multiple)</p><p>2. Selected nodes will appear as pills. You can change your selection.</p><p>3. Refer to the annotator guidelines to attribute and score samples. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Genflowchart: parsing and understanding flowchart using generative ai</title>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Arbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meikang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing off-line flowcharts by reconstructing strokes and using on-line recognition techniques</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Průša</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Václav</forename><surname>Hlaváč</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICFHR.2016.0022</idno>
	</analytic>
	<monogr>
		<title level="m">2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual programming using flowchart</title>
		<author>
			<persName><forename type="first">Kanis</forename><surname>Charntaweekhun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somkiat</forename><surname>Wangsiripitak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 International Symposium on Communications and Information Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1062" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11859</idno>
		<title level="m">Complex claim verification with evidence retrieved in the wild</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanpu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zujie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05778</idno>
		<title level="m">Risk taxonomy, mitigation, and assessment benchmarks of large language model systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The multiple meanings of a flowchart</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ensmenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; Culture</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="351" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Seven types of visual ambiguity: On the merits and risks of multiple interpretations of collaborative visualizations</title>
		<author>
			<persName><forename type="first">Jeanne</forename><surname>Martin J Eppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Mengis</surname></persName>
		</author>
		<author>
			<persName><surname>Bresciani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 12th International Conference Information Visualisation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="391" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Enabling large language models to generate text with citations</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14627</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models</title>
		<author>
			<persName><forename type="first">Fuxiao</forename><surname>Tianrui Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongxia</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14375" to="14385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual programming: Compositional visual reasoning without training</title>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno>ArXiv, abs/2211.11559</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual hallucinations of multimodal large language models</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minxin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14683</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrieving supporting evidence for generative question answering</title>
		<author>
			<persName><forename type="first">Siqing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region</title>
		<meeting>the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagarika</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12273</idno>
		<title level="m">Shiv Vignesh Murty, and Swathy Ragupathy. 2024. The ethics of interaction: Mitigating security threats in llms</title>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lisa: Reasoning segmentation via large language model</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9579" to="9589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zetian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshuo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiguo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.03731</idno>
		<title level="m">A survey of large language models attribution</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changtai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyue</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07838</idno>
		<title level="m">Tianxiang Sun, and Xipeng Qiu. 2023b. Llatrieval: Llm-verified retrieval for verifiable generation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Code generation from flowcharts with texts: A benchmark dataset and an approach</title>
		<author>
			<persName><forename type="first">Zejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzheng</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-emnlp.449</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6069" to="6077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Context-aware llm-based safe control against latent risks</title>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Quan Khanh Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yorie</forename><surname>Van Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Nakahira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.11863</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visa: Retrieval augmented generation with visual source attribution</title>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.14457</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matsa: Multi-agent table structure attribution</title>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexa</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="250" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/hello-gpt-4o/" />
		<title level="m">Hello, gpt</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flowlearn: Evaluating large vision-language models on flowchart understanding</title>
		<author>
			<persName><forename type="first">Huitong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Dragut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2024</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2024-01">Jan Latecki. 2024</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Kosmos-2: Grounding multimodal large language models to the world</title>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14824</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The impact of auditors creating flowcharts on auditors&apos; understanding of the flow of transactions and internal control evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><forename type="middle">L</forename><surname>Perols</surname></persName>
		</author>
		<author>
			<persName><surname>Perols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Managerial Auditing Journal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="779" to="798" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Credible without credit: Domain experts assess generative language models</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Peskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking comprehensive benchmark for chart understanding: A perspective from scientific literature</title>
		<author>
			<persName><forename type="first">Lingdong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.12150</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards making flowchart images machine interpretable</title>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajwal</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="505" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FlowVQA: Mapping multimodal logic in visual question answering with flowcharts</title>
		<author>
			<persName><forename type="first">Shubhankar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purvi</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yerram</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranshu</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vatsal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-acl.78</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics ACL 2024</title>
		<meeting><address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1330" to="1350" />
		</imprint>
	</monogr>
	<note>and virtual meeting</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2022a. Frdetr: End-to-end flowchart recognition with precision and robustness</title>
		<author>
			<persName><forename type="first">Lianshan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2022.3183068</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="64292" to="64301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01296</idno>
		<title level="m">Recitation-augmented language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FlowchartQA: The first large-scale benchmark for reasoning over flowcharts</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tannert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><forename type="middle">G</forename><surname>Feighelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmina</forename><surname>Bogojeska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Arbelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anika</forename><surname>Staar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Linguistic Insights from and for Multimodal Language Processing</title>
		<meeting>the 1st Workshop on Linguistic Insights from and for Multimodal Language Processing<address><addrLine>Ingolstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Lingustics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="34" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
