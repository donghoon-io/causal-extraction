<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-28">28 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
							<email>matthew.vowels@unil.ch</email>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Rochat</surname></persName>
							<email>mathieu.rochat@unil.ch</email>
						</author>
						<author>
							<persName><forename type="first">Sina</forename><surname>Akbari</surname></persName>
							<email>sina.akbari@epfl.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">The Sense Innovation and Research Center</orgName>
								<orgName type="institution">CHUV</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="institution">University of Lausanne</orgName>
								<address>
									<addrLine>Switzerland. Centre for Vision</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Psychology</orgName>
								<orgName type="institution">University of Lausanne</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-28">28 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.14485v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial Neural Networks (ANNs), including fullyconnected networks and transformers, are highly flexible and powerful function approximators, widely applied in fields like computer vision and natural language processing. However, their inability to inherently respect causal structures can limit their robustness, making them vulnerable to covariate shift and difficult to interpret/explain. This poses significant challenges for their reliability in realworld applications. In this paper, we introduce Causal Fully-Connected Neural Networks (CFCNs) and Causal Transformers (CaTs), two general model families designed to operate under predefined causal constraints, as specified by a Directed Acyclic Graph (DAG). These models retain the powerful function approximation abilities of traditional neural networks while adhering to the underlying structural constraints, improving robustness, reliability, and interpretability at inference time. This approach opens new avenues for deploying neural networks in more demanding, real-world scenarios where robustness and explainability is critical.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning has been broadly concerned with the discovery of powerful function approximation techniques which are capable of mapping from a set of inputs to a set of outputs. Among the two most popular approaches for doing so are the fully-connected neural network, and the transformer <ref type="bibr" target="#b83">[84]</ref> both of which are Artificial Neural Networks (ANNs). Variations of these methods have seen widespread success in the domains of computer vision <ref type="bibr" target="#b24">[25]</ref>, audio <ref type="bibr" target="#b64">[65]</ref>, reinforcement learning <ref type="bibr" target="#b85">[86]</ref>, natural language processing Figure <ref type="figure">1</ref>. Illustrating the lack of covariate shift robustness associated with conventional machine learning models like random forests <ref type="bibr" target="#b6">[7]</ref>, multilayer perceptrons (MLPs), and transformers <ref type="bibr" target="#b83">[84]</ref>, compared with our CFCN and CaT. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> and others <ref type="bibr" target="#b39">[40]</ref>. Despite their efficacy, ANNs inherently rely on statistical and data-driven mechanisms, and do not generally integrate prior knowledge about the underlying Data Generating Process (DGP).</p><p>Previous work has highlighted that models which do not respect the underlying DGP can exhibit sensitivity to covariate shift <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b68">69]</ref>. Models that capitalize on non-causal associations prevalent in the training data-associations that do not have mechanistic links-may underperform when exposed to new data where these relationships are altered and irrelevant to the primary outcomes (see e.g. Fig. <ref type="figure">1</ref>). A well-known example of this concerns the classification of camels and cows from a set of images, whereby images of camels feature dry, sandy regions and images of cows feature lush, green, countryside <ref type="bibr" target="#b1">[2]</ref>. A model trained non-causally on such data might indiscriminately use the background as a feature, though it actually represents a confounding variable relative to the true label. Consequently, non-causal approaches like transformers tend to entangle conceptually distinct factors, adversely affecting their robustness and sensitivity to shifts in these extraneous features-for instance, misclassifying a camel in a green landscape as a cow.</p><p>In addition, one of the principal goals of science concerns an understanding of causality <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b69">70]</ref>. Specifically, the task of causal inference involves estimation of the causal effect of a particular action (such as taking a drug) in the absence of experimental data. This task is inherently challenging because causal effects are often confounded by other influences. For example, observations of disease outcomes for people who have chosen surgery or medication are confounded by factors such as age -older people are more likely to select medication over surgery, but are less likely to recover. In such a case, a naive evaluation of the differences in outcomes between people under the two treatments yields a biased estimate of the true effect. Models without an appropriate structural/causal constraint invariably lead to biased estimates in tasks involving causal inference <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b92">93]</ref> In order to disentangle the effect of interest from these other spurious associations, a causal approach is required, one that is not granted by what we might refer to as 'purely statistical' or 'correlational' approaches in machine learning.</p><p>The non-causal nature of most ANNs does not therefore facilitate the kind of causal reasoning relevant to empirical researchers. As such, we make two principal contributions in this work, which concern the integration of structural/causal knowledge with ANNs. Firstly, we extend the concept proposed in MADE <ref type="bibr" target="#b27">[28]</ref> to create a mask which constrains the interactions of neurons within a fullyconnected network according to a Directed Acyclic Graph (DAG) representing the underlying causal structure. These we refer to as Causal Fully Connected Networks (CFCNs). Secondly, we propose a similar crucial modification to the transformer architecture and refer to these as Causal Transformers (CaTs). Such constraints reduce sensitivity to covariate shift, and widen their applicability as powerful function approximation techniques across other domains concerned with causality, including medical imaging <ref type="bibr" target="#b11">[12]</ref>, policy making <ref type="bibr" target="#b43">[44]</ref>, medicine <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b62">63]</ref>, advertisement <ref type="bibr" target="#b5">[6]</ref>, social science <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b92">93]</ref> and many others.</p><p>In summary, the principal contributions of this work are <ref type="bibr" target="#b0">(1)</ref> The Causal Fully-Connected Network (CFCN), and (2) the Causal Transformer (CaT). Both methods are constrained according to causal knowledge and therefore widen the direct applicability of these approaches across empirical domains. Furthermore, the causal transformers can handle inputs of arbitrary dimensionality, enabling causal structures to everything from tabular data to high-dimensional, multimodal embeddings.</p><p>Note -the goal of this paper is not to present state-of-the-art networks for specific tasks (such as causal inference), but rather to present two general modelling approaches which can integrate structural or causal inductive biases into two popular neural network architectures which have already been proven: fully connected networks and transformers. This integration aims to enhance model interpretability, fairness <ref type="bibr" target="#b48">[49]</ref>, improve data efficiency <ref type="bibr" target="#b93">[94]</ref>, and facilitate domain-specific applications by leveraging unique causal structures inherent to different fields. By facilitating the incorporation of these biases, we seek to promote robustness and generalization in models, making them more resilient to changes in data distribution and practical for real-world scenarios <ref type="bibr" target="#b1">[2]</ref>. Furthermore, these models serve as a foundation, enabling researchers to explore and innovate upon these architectures, tailoring them to their specific needs and advancing the field of neural network research. This contribution is therefore not intended to be evaluated in terms of task specific accuracy, but the degree to which it provides a versatile framework that enhances the adaptability and applicability of neural networks through the integration of causal or structural constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There exists a long history of inductive biases being used to design effective machine learning models. This inductive bias can be used in varying degrees: (a) one may constrain a model according to a causal structure which puts a restrictive prior on the possible interactions between variables, dimensions, or inputs. This can be done to aid in tasks which require disentanglement of generative factors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b97">98]</ref>, or in causal inference for estimating the effect of an intervention <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b70">71]</ref>. (b) Inductive bias can be used to restrict the class of functions which can be learned as part of a function approximation task. The latter are often more 'general' than structural inductive biases and include the utilization of, for instance, convolution operations for vision tasks, or weight regularization to encourage function smoothness <ref type="bibr" target="#b29">[30]</ref>. Functional inductive biases can be used to project an input onto a very specific space for purposes of a downstream task (see registration in medical images <ref type="bibr" target="#b25">[26]</ref>, or DAGs for predicting emotion <ref type="bibr" target="#b71">[72]</ref>).</p><p>Our contributions primarily fall under the first category: structural inductive bias. Within this category, examples of models which build structural inductive bias directly into the fitting process for the purposes of estimating a causal effect have been presented in the form of multi-task deep neural networks <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b72">73]</ref> and Variational AutoEncoders <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b103">104]</ref>, and variations of these methods have been used with different objective functions, including adversarial <ref type="bibr" target="#b101">[102]</ref> and semi-parametric / efficient influence function <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b91">92]</ref> objectives for causal inference. Besides neural network based approaches, Structural Equation Modeling <ref type="bibr" target="#b42">[43]</ref> and its non-linear variants <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b82">83]</ref> represent popular so-lutions for estimating causal effects within DGP embedded within a set of equations, some of which have the added benefit of providing statistical inference (confidence intervals, standard errors, etc.). It is worth nothing that it is possible to design estimators which yield unbiased estimates of causal effects and which simply integrate predictive models (such as MultiLayer Perceptrons or a Random Forests <ref type="bibr" target="#b6">[7]</ref>). This requires a careful selection of predictor variables according to, for example, the backdoor criterion <ref type="bibr" target="#b58">[59]</ref>.</p><p>Most of the causal inference methods are concerned with estimation of causal effects from tabular data. However, it is worth noting that top-level structural inductive biases can also be used to guide reasoning in vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b95">96]</ref> where, for instance, knowledge about which factors change in a video sequence (e.g. pose/position) versus which remain static (e.g. identity/appearance) can be used to disentangle these generative factors. Our proposed CaT method can also be used in the context of such multimodal, multidimensional embeddings. Recent work <ref type="bibr" target="#b79">[80]</ref> found that such missing inductive biases leave generative transformers fragile and unable to build reliable world models, even if they otherwise succeed at the particular task on which they were trained, highlighting the need for a means to integrate such constraints into the transformer.</p><p>Perhaps the most similar work to our causal fullyconnected network (CFCN) is the Masked Autoencoder for Density Estimation (MADE) <ref type="bibr" target="#b27">[28]</ref>. MADE <ref type="bibr" target="#b27">[28]</ref> incorporates masked weights in each layer such that the reconstructed values at the output of the network depend only on inputs which precede them in time (autoregressive property). For example, for three variables in an autoregressive structure A, B, C, the masks constrain the weights at each layer (by setting them to zero) such that the reconstructions are Ĉ|(B, A), B|A, and Â|∅.</p><p>In terms of our CaT, there have been a number of attempts to make transformers 'causal'. Perhaps the least concentrated effort resulted in what is generally known as causal masking, which is a form of masking which only allows inputs to 'attend' to other inputs which precede them. Whilst this has proven to be beneficial in non-causal applications <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101]</ref>, the only constraint enforced is that the future cannot be used to predict the past. As such, the structural constraint is relatively 'weak' (closer to 'Granger causality' <ref type="bibr" target="#b30">[31]</ref>) and cannot be used to guide high-level factor disentanglement, causal inference, or models robust to covariate shift. For transformers integrating strict causal inductive bias, work by <ref type="bibr" target="#b56">[57]</ref> utilizes separate transformers which each handle the treatment, covariate, and outcome history to undertake inference regarding patient outcomes. The limitation with this approach is that it requires grouping of these three variable types in relation to a specific target estimand. In contrast, CaT is general and can be applied to estimate any causal effect within a supplied DAG, and as such does not require separate subnetworks but is an 'allin-one' solution.</p><formula xml:id="formula_0">x3 |x 1 , x 2 x2 |x 1 x 1 x 1 x 3 x 2 x 2 x 3 x1 l 3 l 2 l 1 x 1 x 1 x 3 x 2 x 2 x 3 x1 l 3 l 2 l 1 x 4 x2 |x 1 , x 3 x3 x4 |x 3 , x 2 x 4 Ex.1 Ex.2</formula><p>Finally, methods incorporating DAGs into the transformer in some way have been proposed, but their goals are notably different from ours. For instance <ref type="bibr" target="#b53">[54]</ref> include bidirectional links (thereby violating causal ordering) for predicting outcomes unrelated to the nodes in the DAG used to constrain the attention, and <ref type="bibr" target="#b26">[27]</ref> predict topological ordering from graphs. It is worth mentioning that even though functional inductive biases (convolution, weight decay, etc.) are extremely common, in general neural networks are not built to enforce a priori structural constraints, rather the opposite. The challenge in designing such an approach is to enforce these constraints in systems which are otherwise designed to facilitate maximal interactions between inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup and Background</head><p>In this work we address two main problem setups. We consider dataset</p><formula xml:id="formula_1">D = {[Y i , D i , L set,i ]} N</formula><p>i=1 , whereby N is the sample size, and treatment D, outcome Y, and covariates L set = {L 1 , ..., L |L set | } are multidimensional embeddings (possibly of different dimensionality). CaT is concerned with this setup, but also functions in the case where all variables are univariate. CFCN functions in the univariate case. We make no assumptions about the parametric or distributional form of each uni-or multi-dimensional variable.</p><p>In both scenarios we are concerned with building a model encoding a set of conditional independencies encoded by a Directed Acyclic Graph (DAG) G. The DAG represents the factorization of a joint distribution P(Y, D, L set ) of all variables using |Z| corresponding nodes z ∈ Z and edges (j, k) ∈ E, where (j, k) indicates the presence of a directed edge from node j to node k. The acyclicity indicates the absence of any cyclic paths (such that there are no directed paths between a node and itself). DAGs are a popular tool for representing causal structures or DGPs. For instance, the DAG D → M → Y tells us that D causes variable M which itself is a mediator between D and Y where Y here would be the outcome of the causal chain. DAGs can be represented as a square adjacency matrix. With each row representing a possible parent (i.e. direct causal predecessor), and each column representing a possible child (i.e., a variable which is affected by its parent). The adjacency matrix representation will be a key component in enforcing the causal constraints in our CFCNs and CaTs.</p><p>The end goal is to be able to reason causally, under the constraints established by the DAG. One associated task is the estimation of the average treatment effect τ , which can be expressed as follows:</p><formula xml:id="formula_2">τ := E Y∼P (Y|do(D=1)) [Y] -E Y∼P (Y|do(D=0)) [Y] = E L set ∼P [E Y∼P (Y|do(D=1),L set ) [Y] -E Y∼P (Y|do(D=0),L set ) [Y]].<label>(1)</label></formula><p>Here, the do operator <ref type="bibr" target="#b58">[59]</ref> denotes an intervention, simulating the case whereby D = 0 and D = 1, even though in the joint distribution P, D takes on values according to the underlying DGP. This quantity is therefore hypothetical, but can be estimated unbiasedly under the right conditions, and these conditions can usually be derived from the DAG itself <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b61">62]</ref>. In practice, causal discovery techniques, domain knowledge and inductive biases can be used for the specification of a DAG, although in this work we assume the DAG is known a priori. One of the key advantages of models that respect the underlying causal structure (or, more loosely, respect some underlying properties of the DGP) is that predictions are less sensitive to distributional shift. Conventional machine learning models which leverage all available correlations make predictions which fluctuate according to changes in spurious or confounding factors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b87">88]</ref>. Causal models, on the other hand, may have less predictive power under a stable joint distribution, but maintain this predictive power under covariate shift precisely because they leverage only those relationships which are invariant under such shift. Interested readers are directed to surveys and introductions by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref> for more information on causality, and Sec. Section S8 for further preliminaries.</p><p>Note that if we relax the requirement for unbiased estimates for causal effects, one can use the DAG to loosely constrain the model according to relevant factors and to make predictions for arbitrary interventions (more on this below). As such, CFCNs and CaTs are not only useful for causal inference, but also as models which are robust to covariate shift in proportion to the strength of the constraints imposed via the DAG, and the coherence between this DAG and the true, underlying DGP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal Fully-Connected Networks (CFCNs)</head><p>The CFCN is constructed similarly to an autoencoder, but only insofar as the inputs are also the targets (see Figure <ref type="figure" target="#fig_0">2</ref>). Targets cannot be predicted from themselves, but only by their structural/causal parents. The output is therefore reflective of the conditional distribution implied by the DAG. For each layer r, where R is the total number of layers, we initialize a mask M r of zeros of shape q r × p r , where p r is the number of neurons in the layer before layer r, and q r is the number of neurons in the current layer. We assert that the number of neurons in each layer is never less than the dimensionality of the input to avoid bottleneck issues resulting in a dropout of signal along the depth of the network. Furthermore, we assume that the width of each layer is an integer multiple or factor of the preceding layer size, such that when growing or shrinking the network across its depth, the number of neurons capable of interacting with a particular input is always equal across inputs and always greater than or equal to 1. Taking the adjacency matrix we reshape/expand/shrink it to reflect the shape of the underlying weight matrices for each layer, whilst respecting the original dependencies encoded by the associated DAG.</p><p>The masking process is applied similarly to <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_3">o r = σ(o r-1 (W r • M r ) + b r ),<label>(2)</label></formula><p>where σ is an activation function, o r is the vector output, W r is the weight matrix, b r is a bias vector, and M r is a mask for a given layer r, respectively. • is the Hadamard product. An example is shown in Sec. S3 and Fig. <ref type="figure">5</ref> in the supp. mat. One key element illustrated with two examples in Figure <ref type="figure" target="#fig_0">2</ref> is the introduction of the identity diagonal to the DAG for all layers greater than 1. As such, the first layer acts as the 'barrier' to prevent inputs from attending to themselves, but once an intermediate (and causally valid) interaction has occurred, any interactions can then be passed on to the output. The identity cannot be introduced into the first layer, because this would enable the network to simply pass through the input to the output, and also violate the fact that variables without parents are exogenous. In contrast, without the introduction of the identity in the second layer, no signals could be propagated from intermediate, causally valid interactions. In Sec. S4 we present CFCN 'mod', which is an alternative approach which we find to perform similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Causal Transformers (CaTs)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Motivation</head><p>One of the key limitations with other approaches to causality which leverage ANNs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b101">102]</ref> or other-  wise <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b96">97]</ref> is that they can only interpret causality uni-dimensionally. That is to say, other methods require each variable to be represented by a single value, preventing these approaches from operating directly on highdimensional embeddings, such as a set of skeleton joints from a pose estimation method, the embedding of a person's voice, or multi-modal embeddings from, for instance, images and language embeddings together. In contrast, our transformer-based approach conceives of the input sequence to the transformer as a set of d-dimensional embeddings and as such can be used both with tabular data (where each variable is unidimensional) or with embeddings or other multimodal data from video, text, audio, etc. This is particularly useful in applied domains such as psychology or medicine, where researchers may either be concerned with the use of multi-item scales, the use of multimodal data for understanding human behavior <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>, or trials involving multidimensional outcome measures. Indeed, the well-known NEO Personality Inventory has 240 items <ref type="bibr" target="#b15">[16]</ref>, which are grouped as subscales according to each of the Big-5 personality traits. Researchers generally have to compress these scales 'bluntly' by taking the average across the subsets to arrive at 5 numbers representing each of the 5 traits. With CaTs, researchers can represent each input as a multidimensional vector comprising the items from the associated subscale, thereby avoiding the need to discard information unnecessarily.</p><formula xml:id="formula_4">softmax(A T • QK T √ hs ) • V O r Block Heads × Blocks D Y L L T D T Y T |Z| × dE |Z| × |Z| B × |Z| × dE B × |Z| × C B × |Z| × C L T D T Y T D Y L D L Y B × |Z| × dE skip</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Causal Multi-Head Attention</head><p>For a brief review of self-and cross-attention mechanisms in typical transformers, please see Sec. S5. A top-level depiction of the core change to the transformer attention architecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. The complete process for the single causally-masked cross-attention head of CaT is given below. First, we embed the input X from shape</p><formula xml:id="formula_5">(B × |Z| × C) into X E with dimensionality (B × |Z| × d E ).</formula><p>Crucially, this is done using |Z| independent linear layers. It is also important that d E is at least as large as C and greater than 1. In practice, we find that even if C = d E , but d E = 1, the network struggles to disentangle the contributions from the separate variables for all but trivial cases. This stage can also be used to unify the dimensionality of all input embeddings in the case where the different variables do not have the same dimensionality (this may be the case with multimodal data, for instance).</p><p>Next, we randomly initialize a learnable embedding γ, which is of size |Z| × d E . This parameter is combined using cross-attention, with the embedded input X E throughout the network and according to the interactions implied by the DAG. For cross-attention, the keys, queries and values are computed as follows:</p><formula xml:id="formula_6">K = X E W K + b K , Q = γW Q + b Q , V = X E W V + b V .<label>(3)</label></formula><p>Importantly, note that whilst the keys and values are functions of the embedded input X E , the query is a function of the learnable embedding γ in the first block, and a function of the output of the previous block for subsequent blocks (see Eq. 7).</p><p>We then mask the attention between Q (which is a function of γ) and K ⊤ (which is a function of X E ). The mask is applied via a Hadamard product (indicated by •) between the transposed, topologically sorted, adjacency matrix A and the attention matrix. We perform the softmax operation, and matrix multiply this with V:</p><formula xml:id="formula_7">O = softmax(A ⊤ • QK ⊤ √ h s ) • V.<label>(4)</label></formula><p>Importantly, and in contrast with the CFCN, we do not modify A to include self-connections / an identity diagonal after the first layer/block (see below). The self-connections are not needed in CaT. Overall, the process presented in Equation 4 is undertaken in parallel according to the number of heads in the Causal Multi-Head Cross-Attention (CMCA), and these outputs are concatenated whilst maintaining the independence between the input variables:</p><formula xml:id="formula_8">O CMCA = CMCA(γ, X E ) = σ(Dropout(Proj(Conc([O 1 (γ, X E ), O 2 (γ, X E ), ..., O H (γ, X E )])))),<label>(5)</label></formula><p>where Conc is a concatenation operation (concatenating along the last dimension corresponding with the dimensionality of each head), H is the number of heads in the CMCA, and Proj is a linear projection layer which projects the output to the same dimensionality as the expected head input.</p><p>The CMCA forms part of a causal block ('CBlock'), which includes two residual/skip connections:</p><formula xml:id="formula_9">O r, ′ CMCA = BN(CMCA(O r-1 Block , X E ) + O r-1 Block ),<label>(6)</label></formula><formula xml:id="formula_10">O r Block = BN(F F (O r, ′ CMCA ) + O r, ′ CMCA ),<label>(7)</label></formula><p>where BN is (optional) batch normalization <ref type="bibr" target="#b38">[39]</ref>, and F F is a two-layer perceptron with dropout and a Swish <ref type="bibr" target="#b65">[66]</ref> activation function. At each layer, the embedded input X E is always fed to the head(s), whilst the learned embedding γ is used only for the head(s) in the first layer, i.e. O r Block = γ for r = 0. Finally, we stack a number of such blocks in sequence (corresponding with the number of 'layers') and include one last linear layer before the final output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Key Modifications :</head><p>The key modifications to the transformer architecture are:</p><p>• The application of the cross-attention mask to constrain the network to respect the underlying DGP / DAG. • The initialization of the learned embedding γ which forms the query vector. • Feeding X E at every layer and every block so that the downstream γ (which becomes O r Block ) can attend to both its prior state and the observed embedded input. This allows the model to account for the surprise in the observed value compared to the current estimate. Usually, cross-attention is between two different but informative embeddings. In our case, it is between a source of information X E and an empty embedding γ which becomes the output as it propagates through the network. Feeding in the input at each layer enables CaT to 'compare' the current O r Block against the input to see what it needs to attend to in the input (whilst constraint according to the DAG) to produce a useful output. • We do not include layer normalization <ref type="bibr" target="#b3">[4]</ref>. Despite its successful integration with transformers, layer normalization impacts the capacity to estimate precise causal effects, because the value of one input can, via the normalization process, impact the encoded value of another input, even if these inputs should be completely independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions for CFCN and CaT</head><p>We use per-variable loss functions, and users are required to specify the variable type as continuous (mean-squared error) or binary (binary cross-entropy). If embeddings are used, the variables are likely to be continuous, but for more specific tasks we provide the option to tailor the loss function to the variable. Importantly, the loss functions can be determined independently of the causal constraint, and other loss functions (e.g. with custom regularizers) can be used as long as the independence constraints implied by the DAG are not violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Interventions and Recursive Inference</head><p>Importantly, note that M is not included. Its exclusion is a form of transitive reduction <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b93">94]</ref>, whereby intermediate variables/effects can be ignored if their exclusion has no impact on the conditional independencies relevant for the estimation of a particular effect. In fact, it is essential that M be ignored, because Y |D = 1, L, M renders D unimportant as a predictor in a model incorporating the more proximal cause M . Indeed, the graph D → M → Y can be simplified to D → Y for the purposes of estimating the effect of D on Y . CFCN and CaT enable the estimation of multiple simultaneous and arbitrary interventions. To estimate the effect of an intervention, one takes the original dataset and replaces the values of each desired scalar or vector intervention variable with the desired intervention values. This dataset is fed to a recursive prediction algorithm which, using the DAG, iteratively estimates the impact of the intervention on each of the downstream children of the intervention variables. The algorithm for this recursive estimation of the effect of interventions is presented in Sec. S6 and Alg. 1 in supp. mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We provide results for five experiments: (a) a simulated dataset motivating the need for CFCN and CaT, (b) causal inference benchmark datasets Jobs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b74">75]</ref>, Twins <ref type="bibr" target="#b0">[1]</ref> and (c) a real-world application of CaT to psychology data during the COVID-19 pandemic <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b86">87]</ref> highlighting its flexibility in leveraging multi-dimensional inputs. Note that owing to space constraints some of these deatils concerning these evaluations are presented in Sections S10 and S11 in supp. mat. Importantly, no hyperparameter tuning was undertaken for these experiments -again, the goal is not to provide a model which excels at a particular task, but to provide two general modeling classes (MLPs and transformers) which facilitate the integration of structural/causal constraints across a range of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivating Example</head><p>We start with a simple motivating example adapted from <ref type="bibr" target="#b36">[37]</ref> which highlights the advantage of causal constraints. We start by generating data according to the graph in Fig- <ref type="figure">ure 4a</ref>. The goal is to estimate the total effect of D on Y , and to evaluate the absolute error on this estimate (eATE) and the Mean Squared Error (MSE) of the model in predicting the outcome Y under three different conditions corresponding with the use of the correct graph (Figure <ref type="figure">4a</ref>), a graph corresponding with typical assumptions made in machine learning models (Figure <ref type="figure">4b</ref>), and a graph with a missing and a reverse edge (Figure <ref type="figure">4c</ref>). We run CFCN and CaT on these data, alongside a standard transformer <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b83">84]</ref>, random forest <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">61]</ref>, and multilayer perceptron, and present the results in Table <ref type="table">1</ref> and<ref type="table"></ref>  was undertaken on any of the models for the generation of these results (including CaT and CFCN).</p><p>There are three important things to note in the results. Firstly, CaT and CFCN provide the lowest eATE when using the true graph. This is to be expected as these networks respect the DGP. Secondly, the correct causal graph yields the lowest MSE or best predictive performance. Indeed, these graphs were chosen to highlight this -in general the use of non-causal predictors may improve the performance over a causal structure that matches reality. Notably, CaT and CFCN achieve similar MSE to the other methods when the incorrect graph is used. Thirdly, and relatedly, in Fig. <ref type="figure">1</ref> we see that even if non-causal models can achieve a lower MSE than their causal counterparts, they are sensitive to covariate shift. This advantage holds in general, outside of causal inference tasks. In this example we applied shift to a downstream, non-causal variable L 2 . As such, both CFCN and CaT are, by design, robust to such shift, whereas the other models exhibit high sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Causal Inference Tasks</head><p>Results for the three causal inference benchmark dataset evaluations are shown in Table <ref type="table" target="#tab_1">2</ref>. Note that recent authors have highlighted the limitations associated with the use of benchmark datasets in the context of causal inference <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b91">92]</ref>, and so we discount our interpretations accordingly (see also Sec. S12). Details concerning the evaluation metrics are described in Sec. S11 in supp. mat. Lower scores are better across all metrics. Note that the majority of comparison methods are designed and optimised especially for causal inference. Hence we do not expect CaT or CFCN to do well, and yet they nonetheless demonstrate comparable performance.</p><p>Real-World Psychology: We conducted a secondary analysis of UK data from wave 2 of the COVID-19 Psychological Research Consortium Study <ref type="bibr" target="#b55">[56]</ref>, collected during April/May 2020 under social distancing measures. The study used quota sampling to recruit a nationally representative sample of UK adults, with 895 participants included after excluding those with missing attachment style data. The original study estimated the impact of attachment styles on social distancing compliance and mental health outcomes during the pandemic. We followed the approach in <ref type="bibr" target="#b86">[87]</ref> to estimate the causal effect of shifting attachment styles on depression, but using the complete, multi-dimensional set of questionnaire items. We compare a 'naive' bivariate linear model, a specialized targeted learning (TL) estimator incorporating semi-parametric techniques, and our method, CaT. The DAG from <ref type="bibr" target="#b86">[87]</ref>, derived from causal discovery and domain expertise, was applied (see Sec. S11 for details). Both the alternative methods used the 'boileddown'/aggregated dataset (they have no other option). Using secure attachment as the reference, we estimated the effect of shifts to fearful, anxious, and avoidant attachment styles on depression. While TL and the 'naive' model provided standard errors directly, we used bootstrapping (100 iterations) to derive standard errors for CaT. Despite CaT not being specialized for causal inference, its results aligned reasonably with TL. Unlike TL's cross-validated Super Learner approach, CaT did not involve hyperparameter search. Note that it is difficult to draw conclusions from such a comparison with this real-world application (other than as a demonstration of real-world application), because no ground truth is available. Such is the nature of causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations &amp; Discussion</head><p>One principal limitation of CFCNs and CaTs concerns the recursive inference procedure. For DAGs which include long mediation structures, and when the intervention node(s) is(are) early in the causal ordering, the recursion can lead to compounding error as the prediction for each mediator is fed back into the model for the prediction of the next descendant in the chain. This issue can be alleviated by undertaking transitive reduction <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b93">94]</ref> to remove nodes which are not principally important for the inference or for downstream tasks.</p><p>Additionally, in many application areas, it may not be possible to specify a DAG because either the phenomenon is too complex, or because not enough is yet known about the DGP. However, in such cases, users can either leverage causal discovery techniques <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b90">91]</ref> and work with a putative graph, and/or otherwise err on the side of caution by including more rather than fewer edges in the DAG (the removal of an edge represents stronger assumption than an inclusion). Furthermore, CFCNs and CaTs are flexible in that they can be used both for causal inference tasks requiring confident knowledge of the DGP, but equally for higherlevel modeling/prediction tasks, where the incorporation of some relatively generic inductive biases can be beneficial in building more robust models.</p><p>Finally, in the absence of pre-existing datasets, CaT has yet to be scaled to and tested on more complex computer vision tasks with pre-defined DAGs (such as those relating to autonomous driving or human-to-human interactions). Regardless, we currently see no reason why the approach would not work, being agnostic, as it is, to the integration of various scaling techniques relating to efficient crossattention (including Flash Attention <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, the Hedgehog &amp; the Porcupine <ref type="bibr" target="#b102">[103]</ref>, and Performer <ref type="bibr" target="#b14">[15]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Traditional neural networks and transformers, whilst representing powerful function approximation techniques, are not robust to covariate shift and cannot integrate structural inductive biases. This limits their reliability in real-world or scientific applications, where robustness and explainability are paramount. In this work we proposed key modifications to fully-connected neural networks with our CFCN, and the transformer architecture with our CaT. As mentioned, the goal of CaTs and CFCNs is not uniquely as causal inference approaches (although they are competitive in this regard), but to provide a means to combine the powerful function approximation capabilities of transformers, with structural constraints dictated by domain knowledge in the form of a Directed Acyclic Graph. We achieve this goal whilst being agnostic to the integration of techniques like Flash attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Overview</head><p>This supplementary material accompanies the paper CaTs and DAGs. Note that the full code, including the code for all experiments, is provided in supplementary material, and an open repository will also be released to an open repository upon acceptance. We begin this supplementary material by providing a list of notation used throughout, in Section S2.</p><p>We then provide some additional illustrations and information concerning the masking mechanisms used in CFCN in Section S3 and describe a variation of CFCN called CFCN 'mod' in Section S4. We provide an overview of the typical self-and cross-attention mechanisms used in transformers in Section S5, and present the algorithm used for recursive inference in CaTs and CFCNs in Section S6. We describe an approach called Isomorphic Shuffling in Section S7, although this was not found to yield good results in practice.</p><p>In Section S8 we present some preliminaries relating to causal inference including the notion of one-step-ahead potential outcomes (which motivates the recursive inference algorithm in Section S6). Then, in Section S9 we provide the hyperparameters used for the evaluations. In Section S10 we provide additional information and results for the motivating simulations which were presented in the main text. In Section S11 we provide additional experimental results for other causal inference tasks involving the benchmark Jobs and Twins benchmark datasets as well as the real-world psychology dataset.</p><p>Finally, in Section S12 we present a curious result for how the same scikit-learn <ref type="bibr" target="#b60">[61]</ref> random forest <ref type="bibr" target="#b6">[7]</ref> and transformer used in the motivating experiments perform on the causal inference benchmarks.  . Simple example sequence of matrix transformations used to generate masks for CFCN in a three variable, three layer case, where the DAG includes X 1 → X 2 → X 3 mediation as well as a direct path X 1 → X 3 , and the number of neurons in each layer is 3, 6, and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Notation</head><formula xml:id="formula_11">• D = dataset • N = sample size • z = node in set of nodes in graph • Y = univariate outcome • Y = multivariate outcome • L = univariate covariate • L = multivariate covariate • L set = set</formula><formula xml:id="formula_12">X 1 X 2 X 3 X 1 0. 1. 1. X 2 0. 0. 1. X 3 0. 0. 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adj. Matrix</head><p>The transition from Layer 1 to Layer 2 includes the introduction of the diagonal 'pass-through' which is absent in the first layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. CFCN Masking</head><p>The masking process in CFCN is applied similarly to <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_13">o r = σ(o r-1 (W r • M r ) + b r ), (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where σ is an activation function, o r is the vector output, W r is the weight matrix, b r is a bias vector, and M r is a mask for a given layer r, respectively. • is the Hadamard product. An example is shown in Figure <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4. CFCN Mod</head><p>The CFCN Mod is a variation to the previously introduced CFCN described in Sec. 3.2, wherein each layer has direct access to the input through a linear connection. The computation of the layers is given by</p><formula xml:id="formula_15">y 0 = σ(x(W 0 • M 0 ) + b 0 ), y r = σ(y r-1 (W * r • M * r ) + x(W r • M r ) + b r ),<label>(9)</label></formula><p>where x is the input vector, y r is an output vector, W r , W * r are weight matrices for layer r, M r , M * r are the corresponding masks, b r is a bias vector, and • is the Hadamard product.</p><p>A key distinction from the CFCN architecture lies in the handling of masks for the weight matrices. Specifically, the masks M * r include the identity diagonal, ensuring that each layer maintains self-connections. Conversely, the mask for M r does not include this diagonal, preventing direct propagation of the input to the output, and enforcing the DaG constraints.</p><p>These modifications ensure that each intermediate layer has access to the input, thus mitigating the risk of input signal degradation throughout the network. Additionally, this design facilitates a meaningful comparison between the intermediate output features and the original input. For instance, if we consider a model predicting weight from height, the difference between the predicted weight at an intermediate layer and the true weight (derived from the input) could serve as a valuable indicator of body composition, such as identifying obesity. We hypothesize that this structure enhances the model's ability to retain essential input information while allowing richer feature interactions across layers.</p><p>Table <ref type="table" target="#tab_3">4</ref> show the performance of the CFCN and CFCN mod on the Twins and Jobs benchmark datasets. Whilst we believe CFCN 'mod' is a valuable alternative to CFCN, our current evaluations indicate similar performance across the two variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5. Conventional Multi-Head Self-and Cross-Attention</head><p>The core components of transformers <ref type="bibr" target="#b83">[84]</ref> concern the way inputs are processed by the multi-head attention (MHA). Consider an input of |Z| = 3, C-dimensional embeddings a, b, and c (boldfont to indicate vectors). Together these variables constitute an input matrix X, which is B×|Z|×C, where B is the batch size. Each of the three constituent inputs in X are processed independently, and by three linear transformations, yielding the 'query' Q, 'key' K, and 'value' V embeddings. Self-attention, derived via S = QK ⊤ h 0.5 s where h s is the dimensionality of the embeddings, creates a 3x3 interaction matrix, describing the extent to which each pair from the embeddings of a, b and c are similar to each other. This attention matrix represents the key opportunity for causal masking with an adjacency matrix, and we discuss how this is done below. The attention matrix is then matrix multiplied with V such that the different components of V interact with a strength proportional to the similarities encoded in S.</p><p>In addition to self-attention, transformers may also employ cross-attention mechanisms, which can be useful in contexts where the input and targets differ. Cross-attention processes two distinct sets of embeddings: a query matrix Q from the target sequence and key-value matrices K and V from the input sequence. Suppose we have a target matrix T of size B × |Z ′ | × C, where |Z ′ | is the number of target embeddings. The query Q is obtained from T, while the key K and value V are derived from the input matrix X. Cross-attention then computes the similarity between the target queries and the input keys using S = QK ⊤ h 0. resulting in a |Z ′ | × |Z| interaction matrix. This interaction matrix represents how much each target embedding in T attends to the input embeddings in X. As with self-attention, S is then used to reweight V, allowing information from the input to be incorporated into the target sequence based on the learned interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6. Inference Algorithm</head><p>The algorithm for performing recursive inference with CaT or CFCN is shown in Alg. 1. This algorithm can be read as an implementation of recursive substitution (see Section S8.)</p><p>Algorithm 1 Estimating the effects of interventions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7. Isomorphic Shuffling</head><p>We develop a means to shuffle the ordering of the variables and the masks such that the topology of causal structure remains the same whilst the order of the adjacency matrix is permuted randomly during training. This approach is inspired by the order agnostic training presented by <ref type="bibr" target="#b27">[28]</ref>. These permutations are isomorphic with respect to the DAG used to specify the adjacency matrix, and to the weights/parameters of the CFCN and CaT which are not shuffled. The function approximation process is thereby forced to yield a solution which respects the constraints of the underlying DAG, whilst nonetheless being forced to fulfil this role under permutations of this DAG.</p><p>More formally, assume that A is the adjacency matrix for the DAG representing the DGP. A is |Z| × |Z| if the DAG has |Z| nodes, where A ij = 1 if there is an edge between vertex i and vertex j. We generate a random ordering p from 1 to |Z|, and use this to construct a permutation matrix P, which is also |Z| × |Z|. The ith row of P is assigned a 1 in the jth column if, according to the random ordering p, vertex i should be mapped to vertex j in the permutation. The permuted (but isomorphic to the original graph) adjacency matrix A ′ = PAP ⊤ .</p><p>Note that in practice, we find that this shuffling worsened both predictive metrics (like MSE) and widened the standard errors for the ATE estimates. However, it found positive application in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b78">[79]</ref>, so it may be found to be useful for specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8. Causal Preliminaries</head><p>We consider the set of variables Z set , where Y, D ⊂ Z set denote the (possibly multi-dimensional and/or multivariate) outcome and treatment, respectively, and L set = Z set \ Y, D are covariates. We assume a causal ordering O among these variables, and that the causal relations between them are represented by a DAG G. Let Pa Zi denote the set of vertices (corresponding to variables) that have a directed edge to Z i in G (direct causes, or parents). G conforms to the ordering O in the sense that all parents of Z i precede Z i in the ordering O -this indeed guarantees that G is a DAG and no cycles are formed.</p><p>Following <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b73">74]</ref>, we let the one-step-ahead potential outcome variables Z i pa Zi denote the value of Z i had the set of its parents Pa Zi been set to values pa Zi , possibly contrary to fact. The latter can also be defined as the output of a structural equation model (SEM) where a function f i (•) maps the values of Pa Zi along with an exogenous noise ϵ i to values of Z i . We assume the existence of all one-stepahead potential outcomes. Given these one-step-ahead potential outcomes, for any subset D ⊂ Z set , the potential outcome Z i (d), which denotes the value of Z i had the variables in D been set to values d, is defined through recursive substitution as follows:</p><formula xml:id="formula_16">Z i (d) = Z i (d Pa Z i , Pa D Zi (d)),<label>(10)</label></formula><p>where Pa D Zi = Pa Zi \ D. When predicting the values of Z i (d) in Alg. 1, we apply recursive substitution by first predicting the values of Pa D Zi (d) and plugging them into Eq. <ref type="bibr" target="#b9">(10)</ref> for each variable Z i .</p><p>Equivalent to what we described above, Z i (d) can be defined as the random variable induced by a modified SEM where the functions f j (•) for those variables in D are replaced by constant functions setting their value to d. Authors denote this modified SEM using do(D = d) following Pearl's do notation <ref type="bibr" target="#b58">[59]</ref>. In particular, the distribution of Z i (d) is shown by</p><formula xml:id="formula_17">P (Z i |do(D = d)).</formula><p>For outcome Y and treatment D, the average treatment effect is defined as:</p><formula xml:id="formula_18">τ := E Y∼P (Y|do(D=1)) [Y] -E Y∼P (Y|do(D=0)) [Y].</formula><p>For any subset L sub ⊆ L set such that Y(d) is independent of D conditioned on L sub (equivalently an adjustment set in Pearl's terminology) <ref type="foot" target="#foot_1">1</ref> , we have</p><formula xml:id="formula_19">τ = E L sub ∼P E Y∼P (Y|do(D=1),L sub ) [Y] -E Y∼P (Y|do(D=0),L sub ) [Y] = E L sub ∼P E Y∼P (Y|D=1,L sub ) [Y] -E Y∼P (Y|D=0,L sub ) [Y] .<label>(11)</label></formula><p>In practice, we might estimate this quantity using an empirical estimator (like a random forest) Q as follows:</p><formula xml:id="formula_20">τ ( Q; L sub ) = 1 N n i=1 ( Q(D = 1, L sub = L sub,i ) -Q(D = 0, L sub = L sub,i )),<label>(12)</label></formula><p>where the hatˆnotation indicates the value is estimated empirically rather than population/true quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9. Experimental Setup and Hyperparameters</head><p>The hyperparameters for CaT and CFCN (and, identically, CFCN mod) were the same for the motivating experiment, Twins, Jobs, and the real-world dataset experiments presented in the main text, and are shown in Table <ref type="table" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10. Motivating Example</head><p>We generate data from three Data Generating Processes (DGPs), the Directed Acyclic Graphs (DAGs) for which are shown in Figure <ref type="figure">4</ref> in the main text. The structural equations for the DGPs for all three graphs are the same (the data  are generated according to the correct graph). In contrast, no graphs are given to the transformer, random forest (RF), or multilayer perceptron (MLP). These do not accept graph constraints, whereas CFCN and CaT are evaluated under the three different sets of constraints. The structural equations are as follows:</p><formula xml:id="formula_21">U D ∼ N (0, 1), U l1 ∼ N (0, 1), U l2 ∼ N (0, 1), U Y ∼ N (0, 1), D = U D , L 1 = 0.8D + U l1 , Y = 0.8D + 0.4L 1 + U y , X 2 = 0.7Y + 0.6D + U l2 ,<label>(13)</label></formula><p>and as such, the total average treatment effect (ATE) of D on Y is (0.8 + (0.8 * 0.4) = 1.12.</p><p>The RF and MLP are implemented in scikit-learn <ref type="bibr" target="#b60">[61]</ref>, where as the transformer utilizes the TransformerEncoder module from pytorch <ref type="bibr" target="#b57">[58]</ref>, and is given the same hyperparameters as CaT. Given the simplicity of the DGP (4 unidimensional variables in total, linear functional form, Gaussian error) we use the default hyperparameters in the MLP and RF. Indeed, the default parameters in the scikit-learn RF have been shown to work well across a wide range of tasks <ref type="bibr" target="#b63">[64]</ref>. The hyperparameters for the transformer and the CaT are as follows: batch size 300, training iterations 20k, number of heads 3, number of layers 3, head size 4, input embedding dimension 4, 'internal' embedding dimension 4, and learning rate of 0.0002. For CFCN, the batch size was 50, we used 4, 8, and 4 neurons in each of the hidden layers, and applied a learning rate of 0.001. Both CFCN and CaT were trained with an AdamW optimizer <ref type="bibr" target="#b50">[51]</ref> with learning rate scheduling.</p><p>The sample size is set to 10,000 to avoid issues with random sampling variation, and generate data and undertake the analysis for 30 simulations of these data. The MSEs Figure <ref type="figure">6</ref>. Illustrating the absolute error on the Average Treatment Effect estimation associated with conventional machine learning models, random forest <ref type="bibr" target="#b6">[7]</ref> (RF), multilayer perceptron (MLP), a transformer <ref type="bibr" target="#b83">[84]</ref>, and our CFCN and CaT networks. We show the results for CFCN and CaT under three different graphs (one correct, and two false).</p><p>presented in the main text are computed over the validation set which represented a 20% split of the data, and the ATEs were computed over the full training-validation combination, once the models had been trained on thet training data. This paradigm (training on the training set but evaluating the ATE across the full dataset) is normal in the context of causal inference, where the optimization of the network during training does not represent the same task as in conventional supervised learning. In any case, very similar results are obtained if only the validation split is used for evaluating the ATE estimation.</p><p>The results for the error on the ATE estimation across all methods are shown in <ref type="bibr">Fig 6.</ref> This plot highlights how the lowest error on the estimation of the ATE is provided by CaT and CFCN when the correct DAG is used, whereas all other approaches fail to yield good estimates. At best, when a partially correct graph is used, such as DAG 2 in the simulation ('partially' insofar as it more closely aligns with the true DGP than DAG 1 in terms of the relationships crucial for estimating the target ATE), the results are closer than with no DAG at all, which is the case for the typical machine learning approaches seen with the use of an random forest, transformer, or MLP. The key takeaway from this is that specifying a graph, even if it is incorrect, provides a transparent means to represent the assumptions concerning the underlying DGP, and may yield more meaningful results than failing to specify any constraints at all. Finally, the complete results for the MSE on the estimation on the outcome variable Y in the validation set are shown in Fig. <ref type="figure">7</ref>. One can see that the use of the correct causal graph actually results in the worst estimation performance (despite having the best robustness and ATE estimation performance). Figure <ref type="figure">7</ref>. Illustrating the out-of-sample mean squared error for the main outcome Y associated with conventional machine learning models, random forest <ref type="bibr" target="#b6">[7]</ref> (RF), multilayer perceptron (MLP), a transformer <ref type="bibr" target="#b83">[84]</ref>, and our CFCN and CaT networks. We show the results for CFCN and CaT under three different graphs (one correct, and two false). One can see that the correct causal graph has the highest MSE, whereas under all other conditions / with all other approaches the MSEs are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11. Causal Inference Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6.1. Jobs</head><p>Following descriptions in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b94">95]</ref> we also use the job outcomes dataset (Jobs) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b74">75]</ref> (available at <ref type="url" target="https://users.nber.org/">https://users.nber.org/</ref> ˜rdehejia/data/ .nswdata2.html). Unlike the IHDP dataset, Jobs is real-world data with a binary outcome. We follow a similar approach to <ref type="bibr" target="#b70">[71]</ref>, who used the Dehejia-Wahba <ref type="bibr" target="#b21">[22]</ref> sample along with a PSID comparison group. This dataset includes 260 treated samples and 185 control samples, in addition to the 2,490 samples from the PSID comparison group. It combines both observational and randomized control trial (RCT) data. Following the methodology of <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b70">71]</ref>, we apply a 56/24/20 train/validation/test split and conduct 100 runs with different random split assignments to estimate the average performance and standard error. Note that we use the same random seed across models for both initialization and dataset splitting, ensuring that the variance due to these factors is consistent across experiments. As in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b98">99]</ref>, we evaluate our network on the Jobs dataset (where only partial effect supervision is available) by measuring the Average Treatment Effect on the Treated (ATT) error:</p><formula xml:id="formula_22">AT T = ||D 1 | -1 i∈D1 Y i -|D 0 | -1 j∈D0 Y j -|D 1 | -1 i∈D1 ( Q(1, L set,i ) -Q(0, L set,i ))| (14)</formula><p>where, in a slight abuse of notation D = D 1 ∪ D 0 constitutes all individuals in the RCT, and the subscripts denote whether or not those individuals were in the treatment (subscript 1) or control groups (subscript 0). The first two terms in Eq. 14 comprise the true ATT, and the third term the estimated ATT. We may use the policy risk as a proxy for the Precision in the Estimation of the Heterogeneous Effect (PEHE -see Eq. 18):</p><formula xml:id="formula_23">R pol = 1 -(E D 1 [Y (1)|π(L set ) = 1]p D (π(L set ) = 1) +E D 0 [Y (0)|π(L set ) = 0]p D (π(L set ) = 0))<label>(15)</label></formula><p>where π(L set,i ) = 1 is the policy to treat when Ŷi (1) -Ŷi (0) &gt; α, and π(L set,i ) = 0 is the policy not to treat otherwise <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b98">99]</ref>. α is a treatment threshold. This threshold can be varied to understand how treatment inclusion rates affect the policy risk. We set α = 0, as per <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b70">71]</ref>. The subscripts for the expectation operator E and probability p indicate the specific population to which these notations apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6.2. Twins</head><p>We access the Twins <ref type="bibr" target="#b0">[1]</ref> dataset from this package / repository: https : / / github . com / bradyneal / realcause The dataset is based on births of twins in the USA between 1989 and 1991. We designate the heavier twin as the treatment group (t = 1) and the lighter twin as the control (t = 0). The outcome measured is 1-year mortality. For each pair of twins, 30 features were collected, including details about the parents, the pregnancy, and the birth, such as marital status, race, residence, previous births, pregnancy risk factors, care quality during pregnancy, and the number of gestational weeks.</p><p>For the Twins dataset, we provide an estimate for the average error on the estimated Average Treatment Effect (eATE) both within-and out-of-sample:</p><formula xml:id="formula_24">eATE = |τ -τ |<label>(16)</label></formula><p>or</p><formula xml:id="formula_25">eATE = τ - 1 n n i=1 Ŷ1 (i) -Ŷ0 (i) (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>where τ is the true average treatment effect and Ŷ1 and Ŷ0 are the estimated potential outcomes under treatment and no treatment, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6.3. Real-World Dataset</head><p>We performed a secondary analysis of UK data from wave 2 of the COVID-19 Psychological Research Consortium Study (C19PRCS), a longitudinal, internet-based survey. Comprehensive details about the methodology can be found in <ref type="bibr" target="#b55">[56]</ref>), and the data are openly available on the OSF (<ref type="url" target="https://osf.io/v2zur/">https://osf.io/v2zur/</ref>). In brief, the UK portion of the C19PRC Study was conducted in April/May 2020 for Wave 2. During this wave, strict social distancing The motivation for these particular analyses of these data stems from how the COVID-19 pandemic posed numerous challenges, including balancing health protection with maintaining a fulfilling life. For some individuals, the mental health impact of the pandemic has been more severe than for others. The original study aimed to build a theoretical causal model to better understand how individual differences in attachment styles affect both social distancing compliance and mental health outcomes (loneliness, depression, and anxiety) during the pandemic.</p><p>We follow closely the process in <ref type="bibr" target="#b86">[87]</ref> for estimating the causal effect of shifting from one category of attachment style to another on depression. We also report the results for a subset of their analyses in Table <ref type="table" target="#tab_2">3</ref>, which use a 'naive' estimator (comprising the bivariate linear model between the categorical treatment 'attachment' and the two outcomes), a targeted learning estimator specialized for causal inference which incorporates semi-parametric techniques <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b91">92]</ref>, and our results using CaT. Note that there may be some mi-Figure <ref type="figure">8</ref>. The DAG used in the real-world psychology example -reconstructed from the causal discovery and domain expertise results presented in <ref type="bibr" target="#b86">[87]</ref>. Treatment is attachment style 'attachment' (also highlighted in orange) and the two outcomes of interest at the measures of depression (highlighted in green).  <ref type="table">7</ref>. Results ± standard deviations for the Twins and Jobs benchmark datasets comparing with non-causal estimators. eATE -absolute error on the estimation of the Average Treatment Effect; Risk -Policy risk; eATT -absolute error on the estimation of the Average Treatment effect on the Treated; ws -within sample; os -out of sample.</p><p>nor differences in their data preprocessing which we were not able to reproduce. In particular, for each node in the DAG, the original authors reduced the dimensionality of the construct to be uni-dimensional by taking the sum of the scores for each of the individual items. In contrast, we padded all input variables so that they were the same dimensionality as the node with the highest dimensionality. For instance, social distancing 'social dist' was found to have 16 items, so loneliness, which has only 3 items, was zero-padded to have 16 dimensions. The enables us to use all available information in the input. The dimensionalities / number of items for each construct are shown in Table reftab:realworlddimensions. We also use the DAG presented in <ref type="bibr" target="#b86">[87]</ref> which was the result of a causal discovery process alongside domain expertise, this DAG is reproduced in Figure <ref type="figure">8</ref>.</p><p>For estimating the causal effect of a change in attachment style, we treat secure attachment as the reference group, and the three other categories (fearful, anxious, and avoidant) as the comparison categories. We estimate the effect of changing groups by setting the value of attachment to each of the 4 categories and record the shift in depression. Note the anxiety measure has 7-items (GAD-7; <ref type="bibr" target="#b76">[77]</ref>) and the depression measure has 9 (PHQ-9; <ref type="bibr" target="#b45">[46]</ref>) we take the average over the 9 dimensions to estimate the final effect on depression.</p><p>Whilst standard errors can be readily obtained from the targeted learning ('TL') and bivariate linear modeling ('naive') approaches, we used boostrapping to obtain estimates of the standard errors for the estimates from CaT. To do this, we repeated the estimation process 100 times using a sampling-with-replacement process and a sampling fraction of 0.9 of the full dataset.</p><p>In Table <ref type="table" target="#tab_2">3</ref> we see that, despite CaT not being specialized to causal inference, the results are reasonably close to those of the TL estimator. Note that whilst our method uses the full dimensionality of each construct, the comparison methods (naive and TL) use the aggregated scales -they have no other option but to do so. The standard errors are notably wider, which may be related to the use of bootstrapping for deriving them; in contrast to TL, which can be fit once and used to derive standard errors, the CaT is initialized and optimized from fresh for each bootstrap simulation, and trained on a smaller subsample of the full dataset than TL. As such, we expect that this method for estimating the effect size would yield less tight intervals than TL. Furthermore, and as mentioned in the limitations section of the main text, the presence of 'loneliness' in the graph maybe also contribute estimation variance as the recursive inference process has to provide intermediate predictions for this mediator before using these predictions to, in turn, predict the outcome. Finally, the TL estimator includes the use of a Super Learner <ref type="bibr" target="#b80">[81]</ref>, which involves a cross-validation approach to finding the optimal set of candidate learners for the plug-in-estimator prediction task, whereas we did not undertake any equivalent form of hyperparameter search for CaT, making TL a challenging approach to compete with. Regardless, it is difficult to make a comparison with this real-world application, because no ground truth is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PEHE ws PEHE os RF <ref type="bibr" target="#b6">[7]</ref> 1.84 ± 0.08 1.79 ± 0.03 CT <ref type="bibr" target="#b2">[3]</ref> 4.81 ± 0.18 4.96 ± 0.21 CF <ref type="bibr" target="#b96">[97]</ref> 2.16 ± 0.17 2.18 ± 0.19 BART <ref type="bibr" target="#b13">[14]</ref> 2.13 ± 0.18 2.17 ± 0.15 CFR <ref type="bibr" target="#b70">[71]</ref> 2.05 ± 0.18 2.18 ± 0.20 DR-CFR <ref type="bibr" target="#b32">[33]</ref> 2.44 ± 0.20 2.56 ± 0.21 GANITE <ref type="bibr" target="#b101">[102]</ref> 2.78 ± 0.56 2.84 ± 0.61 CEVAE <ref type="bibr" target="#b51">[52]</ref> 3.12 ± 0.28 3.28 ± 0.  <ref type="bibr" target="#b23">[24]</ref> including results presented in <ref type="bibr" target="#b103">[104]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12. A Curious Result</head><p>Table <ref type="table">7</ref> presents a curious result for how the same scikitlearn <ref type="bibr" target="#b60">[61]</ref> random forest (RF) <ref type="bibr" target="#b6">[7]</ref> and transformer <ref type="bibr" target="#b83">[84]</ref> used in the motivating experiments perform on the two causal inference benchmarks used in this work. Specifically, the RF and transformer (using default/untuned hyperparameters) perform very competitively with a set of methods specifically designed for causal inference. Further work is required to explore this result, but it echos a message from <ref type="bibr" target="#b16">[17]</ref> that these benchmarks for causal inference should be reconsidered.</p><p>We also provide results for the Atlantic Causal Inference Challenge (ACIC) 2016 <ref type="bibr" target="#b23">[24]</ref> which is available here: <ref type="url" target="https://github.com/vdorie/aciccomp/tree/master/2016">https://github.com/vdorie/aciccomp/ tree / master / 2016</ref> and follow the procedure and present relevant results from <ref type="bibr" target="#b103">[104]</ref>. This dataset comprises 77 different data generating processes, each with 58 variables and 4802 observations. We undertake a 60/30/10% train/validation/test split, although we discard the 30% validation set for hyperparameter tuning as such tuning is unrealistic in real-world causal inference. The results for the Precision in the Estimation of the Heterogeneous Effect (PEHE) are given in Table <ref type="table" target="#tab_9">8</ref>. This table include results from the Causal Tree (CT; <ref type="bibr" target="#b2">[3]</ref>), Disentangled Counterfactual Regression (DR-CFR; <ref type="bibr" target="#b32">[33]</ref> and Treatment Effect Estimation with Disentangled Variational Autoencoder (TED-VAE; <ref type="bibr" target="#b103">[104]</ref>). The PEHE is defined as:</p><formula xml:id="formula_27">ϵ PEHE = n -1 n i=1 (τ i -τi ) 2 , (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>where τi is the effect for observation i from the dataset estimated by the model.</p><p>For these results, the hyperparameters for CaT were: the number of layers n layers = 3, input embedding dimension = 20, head size = 10, number of attention heads = 5, feed-forward embedding / projection dimension = 10, and dropout = 0.0. We used the same hyperparameters for CFCN as in Table <ref type="table" target="#tab_6">5</ref>.</p><p>Once again, the RF also performs well on the ACIC 2016 benchmark as can be seen from Table <ref type="table" target="#tab_9">8</ref>. As successful causal inference requires one to know the underlying DAG, the causal inference benchmarks make unbiased estimation possible with a set of default / traditional estimators (like the random forest), because all of them assume the same graph (i.e. one treatment variable, a set of possible confounders or precision variables, and an outcome). At least based on this preliminary result, we recommend researchers to at least double-check the performance of standard estimators on these evaluation datasets, to validate that progress is indeed being made. Future benchmarks might, for instance, include an element of causal discovery or causal feature selection, in order to really distinguish the challenge of causal inference from a regular supervised learning task. One of the advantages of CaT and CFCN is that it makes the inference question flexible -one can estimate the causal effect between any pair of nodes in the graph. Furthermore, this graph is specified explicitly and could, for example, represent the output of upstream causal discovery.</p><p>Based on these results, we are not confident that causal inference benchmark datasets are really highlighting the key performance differences between proposed methods. We once again re-iterate that CaT and CFCN are not tuned for causal inference, although this represents a promising direction. When evaluating CaT and CFCN on standard causal inference benchmarks which assume a basic DAG structure constituting a set of covariates, a treatment, and an outcome, the estimation problem is reduced to a simple choice of estimator, and using CaT is similar to using a typical transformer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Two examples demonstrating how the delayed introduction of the identity diagonal at layer 2 and onwards (a) prevents inputs attending to themselves at the first layer and (b) allows other intermediate predictions of these inputs to be used for the prediction other variables. Without the introduction of identity, signals from e.g. x1 used for predicting x2 would be blocked, and x2 itself could not be predicted. Applies to both CFCN and CaT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A top-level depiction of how the masking and routing is applied in CaT to an input with a batch B of |Z| = 3 × C-dimensional embeddings and a corresponding causal DAG. The input parameter γ, which is initially random, is recursively fed as an input to the causal Heads, 'extracting' information from the embeddings of the input via the causal cross-attention operation, according to the constraints imposed by the adjacency matrix A. Best viewed in color. See main text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 . 2 Figure 4 .</head><label>124</label><figDesc>Figure 4. Motivating example. The data for the experiment are generated from DAG (a), whilst (b) and (c) represent to misspecified DAGs used for estimation. Dashed curved lines in (b) emphasise possible endogeneity of D, L1 and L2, and the absence of independence assumptions in typical non-causal machine learning approaches. The thick directed edges depict the target causal relationship of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of multivariate covariates • |L| = number of covariates • D = univariate treatment • P = joint observational distribution • P(Y |D, L) = conditional observational distribution • τ = treatment effect • G = topologically sorted DAG • A = adjacency matrix for DAG • Z = set of nodes in DAG • |Z| = number of nodes in DAG • O = causal ordering for each variable in topologically sorted DAG • F = causal descendants of intervention nodes • D ′ = dataset updated with interventions and predictions of estimated effects • CFCN = Causal Fully Connected Network • r = layer or block number in CFCN or CaT, respectively • R = total number of layers or blocks in CFCN or CaT, respectively • W r = weight/parameter matrix in layer r • σ = activation function • b r = bias vector for layer r • o r = vector output of CFCN for layer r • M r = CFCN parameter mask for layer r • CaT = Causal Transformer • Q = query embeddings in CaT and conventional transformer • K = key embeddings in CaT and conventional transformer • V = value embeddings in CaT and conventional transformer • S = 'similarities' in conventional transformer • X = input to CaT • X E = 're-embedded' input X in CaT • C = embedding dimension for CaT input before 'reembedding' • d E = dimensionality after 're-embedding' • γ = learnable embedding • H = number of parallel heads in each block • h s = head size • CMCA = Causal Multi-head Cross Attention • O CMCA = output of CMCA • O 1 , ...O H = output of parallel heads • O r Block = output from CaT layer/block r • P = permutation matrix • I = set of all intervention value and variable pairs (µ, z) • (µ, z) = an intervention value and variable pair, respectively • M = Trained CaT or CFCN model • Q = empirical estimator (e.g. random forest or CaT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 5. Simple example sequence of matrix transformations used to generate masks for CFCN in a three variable, three layer case, where the DAG includes X 1 → X 2 → X 3 mediation as well as a direct path X 1 → X 3 , and the number of neurons in each layer is 3, 6, and 3. The transition from Layer 1 to Layer 2 includes the introduction of the diagonal 'pass-through' which is absent in the first layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,99.61,72.00,396.00,297.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results ± standard deviations for the Twins and Jobs benchmark datasets. eATE -absolute error on the estimation of the Average Treatment Effect; Risk -Policy risk; eATT -absolute error on the estimation of the Average Treatment effect on the Treated; ws -within sample; os -out of sample. .02 .05 ± .01 .01 ± .02 Anxious .12 ± .02 .05 ± .02 .02 ± .03 Avoidant .01 ± .01 .01 ± .01 .03 ± .04</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Twins</cell><cell></cell><cell></cell><cell>Jobs</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell>eATE ws</cell><cell>eATE os</cell><cell>Risk ws</cell><cell>Risk os</cell><cell>eATT ws</cell><cell>eATT os</cell></row><row><cell></cell><cell>CaT</cell><cell></cell><cell>.0110 ± .0061</cell><cell>.0133 ± .0078</cell><cell>.25 ± .02</cell><cell>.27 ± .06</cell><cell>.016 ± .01</cell><cell>.086 ± .06</cell></row><row><cell></cell><cell>CFCN</cell><cell></cell><cell>.0098 ± .0048</cell><cell>.0102 ± .0092</cell><cell>.25 ± .02</cell><cell>.26 ± .06</cell><cell>.016 ± .01</cell><cell>.084 ± .06</cell></row><row><cell></cell><cell cols="2">GANITE [102]</cell><cell>.0058 ± .0017</cell><cell>.0089 ± .0075</cell><cell>.13 ± .01</cell><cell>.14 ± .00</cell><cell>.01 ± .01</cell><cell>.06 ± .03</cell></row><row><cell></cell><cell cols="2">CFRwass [71]</cell><cell>.0112 ± .0016</cell><cell>.0284 ± .0032</cell><cell>.17 ± .00</cell><cell>.21 ± .00</cell><cell>.04 ± .01</cell><cell>.09 ± .03</cell></row><row><cell></cell><cell>BART [14]</cell><cell></cell><cell>.1206 ± .0236</cell><cell>.1265 ± .0234</cell><cell>.23 ± .00</cell><cell>.25 ± .02</cell><cell>.02 ± .00</cell><cell>.08 ± .03</cell></row><row><cell></cell><cell cols="2">C Forest [97]</cell><cell>.0286 ± .0035</cell><cell>.0335 ± .0083</cell><cell>.19 ± .00</cell><cell>.20 ± .02</cell><cell>.03 ± .01</cell><cell>.07 ± .03</cell></row><row><cell></cell><cell cols="2">CEVAE [52]</cell><cell>-</cell><cell>-</cell><cell>.15 ± .00</cell><cell>.26 ± .00</cell><cell>.02 ± .00</cell><cell>.03 ± .01</cell></row><row><cell></cell><cell>TVAE [95]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>.16 ± .00</cell><cell>.16 ± .00</cell><cell>.01 ± .00</cell><cell>.01 ± .00</cell></row><row><cell>Group</cell><cell>Naive</cell><cell>TL</cell><cell>CaT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fearful</cell><cell>.13 ±</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Bootstrapped effect size estimates ± standard errors for the real-world psychology dataset for impact of change of attachment style on levels of depression. Compares naive (bivariate) correlation, Targeted Learning (TL), and CaT. The naive and TL based approaches use averages of the construct/scale/questionnaire items, whereas CaT uses the full questionnaire without aggregation of questionnaire items.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results ± standard deviations for the CFCN 'mod' approach described in Sec. S4 for the Twins and Jobs benchmark datasets. eATE -absolute error on the estimation of the Average Treatment Effect; Risk -Policy risk; eATT -absolute error on the estimation of the Average Treatment effect on the Treated; ws -within sample; os -out of sample.</figDesc><table><row><cell>s</cell><cell>5</cell><cell>,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Hyperparameters for CFCN, CaT, and Training Settings. Note that 's' is the input size for CFCN which is determined by the dimensionality of the input for the dataset being used.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Number of items / dimensionality of each construct used in the real-world data analysis.</figDesc><table><row><cell>Construct</cell><cell>Dimensionality</cell></row><row><cell>Attachment Style</cell><cell>1</cell></row><row><cell>Loneliness</cell><cell>3</cell></row><row><cell>Chronic Illness</cell><cell>1</cell></row><row><cell>Relationship Status</cell><cell>1</cell></row><row><cell>Social Distancing</cell><cell>16</cell></row><row><cell>Depression</cell><cell>9</cell></row><row><cell>Anxiety</cell><cell>7</cell></row><row><cell>Age</cell><cell>1</cell></row><row><cell>Gender</cell><cell>1</cell></row><row><cell>Keyworker Status</cell><cell>1</cell></row><row><cell>Adults in Household</cell><cell>1</cell></row><row><cell>Children in Household</cell><cell>1</cell></row><row><cell>Change in Income</cell><cell>1</cell></row><row><cell>Covid Anxiety</cell><cell>1</cell></row><row><cell>Risk Taking in the Past Month</cell><cell>1</cell></row><row><cell cols="2">measures were enforced. Quota sampling ensured the re-</cell></row><row><cell cols="2">cruitment of a nationally representative panel of UK adults,</cell></row><row><cell cols="2">based on age, gender, and household income. Participants</cell></row><row><cell cols="2">had to be at least 18 years old, UK residents, and capable</cell></row><row><cell cols="2">of completing the survey in English. After giving consent,</cell></row><row><cell cols="2">participants completed the survey online and were com-</cell></row><row><cell cols="2">pensated by Qualtrics for their time. Ethical approval was</cell></row><row><cell cols="2">granted by a UK university psychology department (Refer-</cell></row><row><cell cols="2">ence: 033759). In Wave 2, 1406 individuals took part, but</cell></row><row><cell cols="2">participants who did not report their attachment styles were</cell></row><row><cell cols="2">excluded from the analysis, and after listwise deletion of</cell></row><row><cell cols="2">rows with other missing values, we arrived at a sample size</cell></row><row><cell>for this wave of 895 participants.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Comparison of methods based on within-sample (ws) PEHE and out-of-sample (os) PEHE for the ACIC 2016 causal inference challenge</figDesc><table><row><cell></cell><cell>35</cell></row><row><cell cols="2">TEDVAE [104] 1.75 ± 0.14 1.77 ± 0.17</cell></row><row><cell>CaT</cell><cell>4.22 ± 0.25 4.26 ± 0.23</cell></row><row><cell>CFCN</cell><cell>4.14 ± 0.13 4.17 ± 0.10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>The use of the DAG to constrain the model not only enables one to fit models which are robust to covariate shift, but also facilitates (as mentioned above) the estimation of the effects of interventions. Consider the mediating graph D → M → Y (D is treatment, Y is an outcome, and M is an intermediate treatment-effect mechanism), and also include the paths D ← L → Y (imagine L is age, influencing the choice of treatment but also the likelihood of recovery). The latter paths including L 'confound' the direct estimation of the effect of treatment D on outcome Y . The estimation of Y |do(D = 1) (i.e., the effect of actually intervening on D and setting it to 1) can be achieved, via the rules of do-calculus<ref type="bibr" target="#b58">[59]</ref>, by estimating Y |D = 1, L. The rules of do-calculus fall beyond the scope of this work, but in this example, conditioning on L enables us to simulate the case where no edge from L → D exists in the graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note that such a subset always exists, e.g., L sub = Pa D .</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The costs of low birth weight</title>
		<author>
			<persName><forename type="first">D</forename><surname>Almond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893v3</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2004">2020. 1, 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recursive partitioning for heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences of the USA</title>
		<meeting>of the National Academy of Sciences of the USA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">113</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arxiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912v2</idno>
		<title level="m">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: the example of computational advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001. 1, 3, 7, 5, 8</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bringing behavioral observation of couples into the 21st century</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bodenmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Family Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599v1</idno>
		<title level="m">Understanding disentangling in Beta-VAE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Smoke and mirrors in causal downstream tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cadei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lindorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lo-Catello</surname></persName>
		</author>
		<idno>arxiv:2405.17151</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08142v1</idno>
		<title level="m">Causality matters in medical imaging</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">DynaNet: Neural Kalman dynamical model for motion estimation and prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03918v1</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BART: Bayesian additive regression trees</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno>CoRR, abs/2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The SAGE handbook of personality theory and assessment, chapter The revised neo personality inventory (neo-pi-r)</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SAGE Publications</publisher>
			<biblScope unit="page" from="179" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Really doing great at estimating CATE? a critical look at ML benchmarking practices in treatment effect estimation. 35th Conference onf Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Curth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weatherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">NeurIPS 2021</date>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FlashAttention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal confusion in imitation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding and misundertstanding randomized controlled trials</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cartwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science and Medicine</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2" to="21" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Propensity score-matching methods for nonexperimental causal studies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Dehejia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805v2</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Non-parametrics for causal inference</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dorie</surname></persName>
		</author>
		<ptr target="https://github.com/vdorie/npci" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic architectures of medical images revealed by registration and fusion of multiple modalities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rakic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phillipakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv preprint</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural topological ordering for computation graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rainone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Joen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bondesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zappi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MADE: Masked autoencoder for distribution estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inductive biases for deep learning of higher-level cognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2022. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<date type="published" when="2266">2266</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Testing for causality: A personal viewpoint</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="329" to="352" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The taboo against explicit causal inference in nonexperimental psychology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thoemmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for counterfactual regression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal structure learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The c-word: scientific euphemisms do not improve causal inference from observational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Public Health</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="625" to="626" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A critical evaluation of theories and their empirical testing methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hilpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSF psyarxiv preprint</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What can be learned from couple research: Examining emotional co-regulation processes in face-to-face interactions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hilpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Brick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Flueckiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceuleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuppens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Counseling Psychology</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167v3</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A nonlinear structural equation mixture modeling approach for nonnormally distributed latent predictor variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kelava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nagengast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="468" to="481" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114v10</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Principles and practice of structural equation modeling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Kline</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Guilford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kreif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diazordaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00402v1</idno>
		<title level="m">Machine learning in policy evaluation: new tools for causal inference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.0512v2</idno>
		<title level="m">Deep kalman filters</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The phq-15: validity of a new measure for evaluating the severity of somatic symptoms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychosomatic Medicine</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="258" to="266" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evaluating the econometric evaluations of training programs with experimental data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the 35th International Conference on Machine Learning</title>
		<meeting>on the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abbati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13662v1</idno>
		<title level="m">On the fairness of disentangled representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359v3</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Causal effect inference with deep latentvariable models. 31st Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2017. 2, 4, 8, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<idno>2023. 3</idno>
		<title level="m">Transformers over Directed Acyclic Graphs. 37th Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A potential outcomes calculus for identifying conditional path-specific effects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Monitoring the psychological, social, and economic impact of the covid-19 pandemic in the population: Context, design and conduct of the longitudinal covid-19 psychological research consortium (c19prc) study</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shevlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gibson-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Levita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V A</forename><surname>Stocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vallieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karatzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Valiente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bentall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Methods in Psychiatric Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2021" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Causal transformer for estimating counterfactual outcomes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Melnychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frauen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feurerriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th International Conference on Machine Learning</title>
		<meeting>39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2009. 2, 3, 4, 6</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The book of why</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Penguin Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2008">2825-2830, 2011. 7, 1, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2017. 2, 4</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Association of implementation of a universal testing and treatment intervention with HIV diagnosis, receipt of antiretroviral therapy, and viral suppression in East Africa</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Balzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kwarsiima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chamie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ayieko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kabami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owaraganise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mwangwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kadede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Medical Association</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2196" to="2206" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hyperparameters and tuning strategies for random forest</title>
		<author>
			<persName><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wires Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust speech recognition via largescale weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="28492" to="28518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Nested Markov properties for Acyclic Directed Mixed Graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06686v2</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rohrer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Causality for machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<idno>arXiv:1911</idno>
		<imprint>
			<date type="published" when="2004">10500v1, 2019. 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2007">2017. 2, 4, 8, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Directed Acyclic Graph Network for conversational emotion recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">59th Annual Meeting for the</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adapting neural networks for the estimation of treatment effects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2019. 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Multivariate Counterfactual Systems and Causal Graphical Models</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="813" to="852" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Does matching overcome LaLonde&apos;s critique of nonexperimental estimators</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A brief measure for assessing generalized anxiety disorder: The gad-7</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Löwe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fitting nonlinear structural equation models in R with package nlsem</title>
		<author>
			<persName><forename type="first">N</forename><surname>Umbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kelava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<title level="m">A deep and tractable density estimator. 31st International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rambachan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03689</idno>
		<title level="m">Evaluating the world model implicit in a generative model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Polley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Super Learner. Statistical Applications of Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Entering the era of data science: targeted learning and the integration of statistics and computational data analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J C M</forename><surname>Starmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Statistics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fleixble extensions to structural equation models using computation graphs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Kesteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Oberski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="247" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<title level="m">Attention is all you need. 31st Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2017. 1, 3, 7, 2, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Conf. on Uncertainty in Artificial Intelligence</title>
		<meeting>6th Conf. on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Toward a causal link between attachment styles and mental health during the COVID-19 pandemic</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Carnelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Millings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gibson-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Trying to outrun causality with machine learning: Limitations of model explainability techniques for identifying predictive variables</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09875</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">SLEM: Machine learning for path modeling and causal inference with super learner equation modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.04365</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A causal research pipeline and tutorial for psychologists and social scientists</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods, Advanced Online Publication</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">D&apos;ya like DAGs? A survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A free lunch with influence functions? an empirical evaluation of influence functions for average treatment effect estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Misspecification and unreliable interpretations in psychology and social science</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Prespecification of structure for increasing research transparency and for the optimization of data collection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Collabra: Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2023. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Targeted VAE: Structured inference and targeted learning for causal parameter estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SMDS</title>
		<imprint>
			<date type="published" when="2005">2021. 2, 4, 8, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Vdsm: Unsupervised video disentanglement with state-space modeling and deep mixtures of experts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Estimation and inference on heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">CausalVAE: disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08697v4</idno>
		<imprint>
			<date type="published" when="2020-02">2020. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Generative pre-trained transformer: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yenduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Selvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Supriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K R</forename><surname>Maddikunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gadekallu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54608" to="54649" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04779v1,2024.3</idno>
		<title level="m">Stablemask: Refining causal masking in decoderonly transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">GANITE: Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2007">2018. 2, 4, 8, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">The hedgehog and the porcupine: Expressive linear attentions with softmax mimicry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kumbong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Treatment effect estimation with disentangled latent factors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
