<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Contextual Bandits with Adaptive Context</title>
				<funder ref="#_4gAtgEg">
					<orgName type="full">Walmart Center for Tech Excellence</orgName>
				</funder>
				<funder ref="#_zSzKHwP">
					<orgName type="full">SERB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-02">2 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Madhavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IISc Bangalore</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Siddharth Barman</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country>IISc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurghya</forename><surname>Maiti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IISc Bangalore</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Siddharth Barman</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country>IISc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Sinha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IISc Bangalore</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Siddharth Barman</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country>IISc</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Contextual Bandits with Adaptive Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-02">2 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.18626v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a variant of causal contextual bandits where the context is chosen based on an initial intervention chosen by the learner. At the beginning of each round, the learner selects an initial action, depending on which a stochastic context is revealed by the environment. Following this, the learner then selects a final action and receives a reward. Given T rounds of interactions with the environment, the objective of the learner is to learn a policy (of selecting the initial and the final action) with maximum expected reward. In this paper we study the specific situation where every action corresponds to intervening on a node in some known causal graph. We extend prior work from the deterministic context setting to obtain simple regret minimization guarantees. This is achieved through an instance-dependent causal parameter, λ, which characterizes our upper bound. Furthermore, we prove that our simple regret is essentially tight for a large class of instances. A key feature of our work is that we use convex optimization to address the bandit exploration problem. We also conduct experiments to validate our theoretical results, and release our code at the project GitHub Repository.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen an active interest in causal bandits from the research community <ref type="bibr" target="#b24">(Lattimore et al., 2016;</ref><ref type="bibr">Sen et al., 2017a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b27">Lee &amp; Bareinboim, 2018;</ref><ref type="bibr" target="#b50">Yabe et al., 2018;</ref><ref type="bibr" target="#b28">Lee &amp; Bareinboim, 2019;</ref><ref type="bibr" target="#b29">Lu et al., 2020;</ref><ref type="bibr" target="#b35">Nair et al., 2021;</ref><ref type="bibr" target="#b30">Lu et al., 2021;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b32">Maiti et al., 2022;</ref><ref type="bibr" target="#b47">Varici et al., 2022;</ref><ref type="bibr" target="#b45">Subramanian &amp; Ravindran, 2022;</ref><ref type="bibr" target="#b49">Xiong &amp; Chen, 2023)</ref>. In this setting, one assumes an environment comprising of causal variables that are random variables that influence each other as per a given causal (directed, and acyclic) graph. Specifically, the edges in the causal DAG represent causal relationships between variables in the environment. If one of these variables is designated as a reward variable, then the goal of a learner then is to maximize their reward by intervening on certain variables (i.e., by fixing the values of certain variables). The rest of the variables, that are not intervened upon, take values as per their conditional distributions, given their parents in the causal graph. In this work, as is common in literature, we assume that the variables take values in {0, 1}. Of particular interest are causal settings wherein the learner is allowed to perform atomic interventions. Here, at most one causal variable can be set to a particular value, while other variables take values in accordance with their underlying distributions.</p><p>It is relevant to note that when a learner performs an intervention in a causal graph, they get to observe the values of multiple other variables in the causal graph. Hence, the collective dependence of the reward on the variables is observed through each intervention. That is, from such an observation, the learner may be able to make inferences about the (expected) reward under other values for the causal variables <ref type="bibr" target="#b39">(Peters et al., 2017)</ref>. In essence, with a single intervention, the learner is allowed to intervene on a variable (in the causal graph), allowed to observe all other variables, and further, is privy to the effects of such an intervention. Indeed, such an observation in a causal graph is richer than a usual sample from a stochastic process. Hence, a standard goal in causal bandits is to understand the power and limitations of interventions. This goal manifests in the form of developing algorithms that identify intervention(s) that lead to high rewards, while using as few 2. Interestingly, the intervention complexity of our algorithm depends on an instance dependent structural parameter-referred to as λ (see equation ( <ref type="formula">3</ref>))-which may be much lower than nk, where n is the number of interventions and k is the number of contexts.</p><p>3. Notably, our algorithm uses a convex program to identify optimal interventions. Unlike prior work that uses optimization to design exploration (for example see <ref type="bibr" target="#b50">Yabe et al. (2018)</ref>), we show (in Appendix Section E) that the optimization problem we design is convex, and is thus computationally efficient. Using convex optimization to design efficient exploration is in fact a distinguishing feature of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We provide lower bound guarantees showing that our regret guarantee is tight (up to a log factor) for a large family of instances (see Section 4 and Appendix Section F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>We demonstrate using experiments (see Section 5) that our algorithm performs exceeding well as compared to other baselines. We note that this is because λ ≪ nk for n causal variables and k contexts.</p><p>In conclusion, we provide a novel convex-optimization based algorithm for Causal MDP exploration. We analyze the algorithm to come up with an instance dependent parameter λ. Further, we prove that our algorithm is sample efficient (see Theorems 1 and 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Additional Related Work</head><p>Ever since the introduction of the causal bandit framework by <ref type="bibr" target="#b24">Lattimore et al. (2016)</ref>, we have seen multiple works address causal bandits in various degrees of generality and using different modelling assumptions. <ref type="bibr">Sen et al. (2017a)</ref> addressed the issue of soft atomic interventions using an importance sampling based approach. Soft interventions in the linear structural equation model (SEM) setting was addressed recently by <ref type="bibr" target="#b47">Varici et al. (2022)</ref>. <ref type="bibr" target="#b50">Yabe et al. (2018)</ref> proposed an optimization based approach for non-atomic interventions. This work was extended by <ref type="bibr" target="#b49">Xiong &amp; Chen (2023)</ref> to provide instance dependent regret bounds. They also provide guarantees for binary generalized linear models (BGLMs). The question of unknown causal graph structure was addressed by <ref type="bibr" target="#b30">Lu et al. (2021)</ref>, whereas <ref type="bibr" target="#b35">Nair et al. (2021)</ref> study the case where interventions are more expensive than observations. <ref type="bibr" target="#b32">Maiti et al. (2022)</ref> addressed simple regret for graphs containing hidden confounding causal variables, while cumulative regret in general causal graphs was addressed by <ref type="bibr" target="#b29">Lu et al. (2020)</ref>. A notable work by <ref type="bibr" target="#b31">Lu et al. (2022)</ref> formulates the framework for causal MDPs, and they provide cumulative regret  We summarize the main works in this thread in Table <ref type="table">1</ref> and provide a more detailed set of related works in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Preliminaries</head><p>We model the causal contextual bandit with adaptive context as a contextual bandit problem with a causal graph corresponding to each context. The actions at each context are given by interventions on the causal graph. Additionally, we have a causal graph at the start state, and the context is stochastically dependent on the intervention on the causal graph at the start state. For ease of notation, we will call the start state of the learner as context 0. The agent starts at context 0, chooses an intervention, then transitions to one of k contexts [k] = {1, . . . , k}, chooses another intervention, and then receives a reward; see Figure <ref type="figure" target="#fig_1">2(a)</ref>.</p><p>Assumptions on the Causal Graph: Formally, let C be the set of contexts {0, 1, . . . , k}. Then, at each context, there is a Causal Bayesian Network (CBN) represented by a causal graph; see Figure <ref type="figure" target="#fig_1">2(b)</ref>. In particular, at each context i ∈ C, the causal graph is composed of n variables {X i 1 , . . . , X i n }. Each X i j takes values from {0, 1}, with an associated conditional probability (of being equal to 0 or 1), given the other variables in the causal graph. We make the following mild assumptions on the causal graph at each context. </p><formula xml:id="formula_0">X i j ∈ {0, 1} for all i ∈ [k], j ∈ [n] do(•)</formula><p>An atomic intervention of the form do(), do(X i j = 0) or do(X </p><formula xml:id="formula_1">P (a,i) = P{i | a} a∈A 0 ,i∈[k] p+ Transition threshold p+ = min{P (a,i) | P (a,i) &gt; 0} π : C → A Policy, a map from contexts to interventions. i.e. π(i) ∈ Ai for i ∈ {0} ∪ [k] E [Ri | π(i)]</formula><p>Expectation of the reward at context i given intervention π(i)</p><p>Interventions: Furthermore, we are allowed atomic interventions, i.e., we can select at most one variable and set it to either 0 or 1. We will use A i to denote the set of atomic interventions available at context i ∈ {0, . . . , k}; in particular,</p><formula xml:id="formula_2">A i = {do()}∪ do(X i j = 0), do(X i j = 1) for j ∈ [n]</formula><p>. We note that do() is an empty intervention that allows all the variables to take values from their underlying conditional distributions. Also, do(X i j = 0) and do(X i j = 1) set the value of variable X i j to 0 and 1, respectively, while leaving all the other variables to independently draw values from their respective distributions. Note that for all i ∈ [k], we have |A i | = 2n + 1. Write N := 2n + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward:</head><p>The environment provides the learner with a {0, 1} reward upon choosing an intervention at context i ∈ [k], which we denote as R i . Note that R i is a stochastic function of variables X i 1 , . . . , X i n . In particular, for all j ∈ [n] and each realization</p><formula xml:id="formula_3">X i j = x j ∈ {0, 1}, the reward R i is distributed as P{R i = 1 | X i 1 = x 1 , . . . , X i n = x n }. Given such conditional probabilities, we will write E[R i | a] to denote the expected value of reward R i when intervention a ∈ A i is performed at context i ∈ [k].</formula><p>Here the expectation is over the parents of the variable R i in the causal graph, with the intervened variable set at the required value. Note that these parents (of R i ) may in turn have conditional distributions given their parents. The leaf nodes of the causal graph are considered to have unconditional Bernoulli distributions. For instance,</p><formula xml:id="formula_4">E[R i | do(X i j = 1)</formula><p>] is the expected reward when variable X i j is set to 1, and all the other variables independently draw values from their respective (conditional) distributions. Indeed, the goal of this work is to develop an algorithm that maximizes the expected reward at context 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Observational Threshold:</head><p>We denote by m i , the causal observational threshold<ref type="foot" target="#foot_3">foot_3</ref> from <ref type="bibr" target="#b32">Maiti et al. (2022)</ref> at context i. This is computed as follows. Let q i j = min Parents(X i j ),x∈{0,1} P{X i j = x | Parents(X i j )}. Further, let S i τ = { q i j : ( q i j ) c &lt; 1/τ } be sets parameterized by τ for every τ ∈ [2, 2n], where c indicates the c-component size. Then</p><formula xml:id="formula_5">m i = min{τ such that |S i τ | ≤ τ }.</formula><p>The existence of such a threshold at each context is guaranteed by the assumptions we made on the CBNs. In addition, let M ∈ N k×k denote the diagonal matrix of m 1 , . . . , m k .</p><p>Transitions at Context 0: At context 0, the transition to the intermediate contexts <ref type="bibr">[k]</ref> stochastically depends on the random variables {X 0 1 , . . . , X 0 n }. Here, P{i | a} denotes the probability of transitioning into context i ∈ [k] with atomic intervention a ∈ A 0 ; recall that A 0 includes the do-nothing intervention. We will collectively denote these transition probabilities as matrix P := P (a,i) = P{i | a} a∈A0,i∈ <ref type="bibr">[k]</ref> . Furthermore, write the transition threshold p + to denote the minimum non-zero value in P . Note that matrix P ∈ R |A0|×k is fixed, but unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy:</head><p>A map π : {0, . . . , k} → A, between contexts and interventions (performed by the algorithm), will be referred to as a policy. Specifically, π(i) ∈ A i is the intervention at context i ∈ {0, 1, . . . , k}. Note that, for any policy π, the expected reward, which we denote as µ(π), is equal to</p><formula xml:id="formula_6">k i=1 E [R i | π(i)] • P{i | π(0)}. Maximizing expected reward, at each intermediate context i ∈ [k],</formula><p>we obtain the overall optimal policy π * as follows. For i ∈ [k]:</p><formula xml:id="formula_7">π * (i) = arg max a∈Ai E [R i | a] π * (0) = arg max b∈A0 ( k i=1 E [R i | π * (i)] • P{i | b})</formula><p>Our goal then is to find a policy π with (expected) reward as close to that of π * as possible.</p><p>Simple Regret: Conforming to the standard simple-regret framework, the algorithm is given a time budget T , i.e., the learner can go through the following process T times - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Algorithm and its Analysis</head><p>We now provide the details relating to our main Algorithm, viz. ConvExplore. </p><formula xml:id="formula_8">P M 1/2 P ⊤ f •-1 2 .</formula><p>5: Estimate the optimal action at each intermediate context π(i) ∀i ∈ [k] based on R. Let the estimate of optimal reward be R( π(i)). 6: Estimate the optimal action at the start context π(0), based on the transition probabilities P and the optimal reward estimates R( π(i)). 7: return π = { π(0), π(1), . . . , π(k)} .</p><p>a Computation of f * is efficient as we show that the problem is Convex. b We show detailed Algorithms for estimation of transition probabilities P (line 2), estimation of causal observational threshold M (line 3), and estimation of rewards R (line 4) in Appendix B</p><p>The algorithm can be described by five main steps. In the first step, we estimate the transitions to intermediate contexts. In the second step, we estimate the causal observational thresholds at these contexts. In the third step, we estimate the rewards upon doing interventions at these contexts. With good reward estimates and transition probability estimates, the computation of a good policy at the intermediate contexts (step 4) and at the start state (step 5) is straightforward. This Algorithm relies on three subroutines which are detailed in Section B of the Appendix. The key aspect of this algorithm is in designing the exploration of interventions (at the start state and at the intermediate contexts) to be regret-optimal -i.e. trading off exploration time between different interventions such that the policy eventually obtained has near-optimal reward.</p><p>Our algorithm (ConvExplore) uses subroutines to estimate the transition probabilities, the causal parameters, and the rewards. From these, it outputs the best available interventions as its policy π. Given time budget T , the algorithm uses the first T /3 rounds to estimate the transition probabilities (i.e., the matrix P ) in Algorithm 2. The subsequent T /3 rounds are utilized in Algorithm 3 to estimate causal parameters m i s. Finally, the remaining budget is used in Algorithm 4 to estimate the intervention-dependent reward R i s, for all intermediate contexts i ∈ [k].</p><p>To judiciously explore the interventions at context 0, ConvExplore computes frequency vectors f ∈ R |A0| . In such vectors, the ath component f a ≥ 0 denotes the fraction of time that each intervention a ∈ A 0 is performed by the algorithm, i.e., given time budget T ′ , the intervention a will be performed f a T ′ times. Note that, by definition, a f a = 1 and the frequency vectors are computed by solving convex programs over the estimates. The algorithm and its subroutines throughout consider empirical estimates, i.e., find the estimates by direct counting. Here, let P denote the computed estimate of the matrix P and M be the estimate of the diagonal matrix M . We obtain a regret upper bound via an optimal frequency vector f * (see Step 4 in ConvExplore).</p><p>Recall that for any vector x (with non-negative components), the Hadamard exponentiation • -0.5 leads to the vector y = x •-0.5 wherein y i = 1/ √ x i for each component i. We next define a key parameter λ that specifies the regret bound in Theorem 1 (below). At a high-level, parameter λ captures the "exploration efficacy" in the MDP, that takes into account the transition probabilities P and the exploration requirements M at the intermediate layer. </p><formula xml:id="formula_9">P M 0.5 P ⊤ f •-0.5 2 ∞</formula><p>Furthermore, we will write f * to denote the optimal frequency vector in equation (3). Hence, with vector ν := P M 0.5 (P ⊤ f * ) •-0.5 , we have λ = max a ν<ref type="foot" target="#foot_4">foot_4</ref> a . Note that Step 4 in ConvExplore addresses an analogous optimization problem, albeit with the estimates P and M . Also, we show in Lemma 11 (see Section E in the supplementary material) that this optimization problem is convex and, hence, Step 4 admits an efficient implementation.</p><p>To understand the behaviour of λ, we first note that whenever the m i values at the contexts i ∈ [k] are low, the λ value is low. Specifically, the m i values can go as low as 2 (when the q i j s are all 1 2 ), removing the dependence of λ on n. The upper-bound on λ is nk. We see this by first upper-bounding each m i by n. Then, note that whenever max a∈A P {i|a} ≥ 1/k, then ∃f such that </p><formula xml:id="formula_10">P ⊤ f = u where u = { 1 k , . . . , 1 k }. Now we can compute that ||P • u •-0.5 || 2 ∞ = k,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of the Lower Bound</head><p>Since ConvExplore solves an optimization problem, it is a priori unclear that a better algorithm may not provide a regret guarantee better than Theorem 1. In this section, we show that for a large class of instances, it is indeed the case that the regret guarantee we provide is optimal. We provide a lower bound on regret for a family of instances. For any number of contexts k, we show that there exist transition matrices P and reward distributions (E[R i | a]) such that regret achieved by ConvExplore (Theorem 1) is tight, up to log factors.</p><p>Theorem 2. For any q i j corresponding to causal variables at contexts i ∈ [k], there exists a transition matrix P , and probabilities q 0 j corresponding to causal variables {X 0 j } j∈ <ref type="bibr">[n]</ref> , and reward distributions, such that the simple regret achieved by any algorithm is</p><formula xml:id="formula_11">Regret T ∈ Ω λ T</formula><p>We provide the details of the proof of Theorem 2 in Section F in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first list a few baseline algorithms that we compare ConvExplore with. This is followed by a complete description of our experimental setup. Finally, we present and discuss our main results.</p><p>Uniform Exploration: This algorithm uniformly explores the interventions in the instance. It first performs all the atomic interventions a ∈ A 0 at the start state 0 in a round robin manner. On transitioning to any context i ∈ [k], it performs atomic interventions b ∈ A i in a round robin manner. UnifExplore achieves a regret upperbounded by Õ( nk/T ), which is also the optimal lower bound for non-causal algorithms. Hence it serves as a good comparison as it achieves an optimal non-causal simple regret. We plot the comparison with this non-causal regret optimal exploration in Figure <ref type="figure" target="#fig_4">3</ref>. We plot the regret with respect to (A) the number of rounds of exploration and (B) with the λ values of our instance. Notice that at extremely high λ values ConvExplore does not perform well, as such an instance does not particularly benefit from the causal structure.</p><p>Even so, with further tuning of constants in our Algorithm, we should achieve a performance similar to UnifExplore.  </p><formula xml:id="formula_12">m i = m ∀i ∈ [k].</formula><p>As in experiments in prior work, we set q i j = 0 for j ≤ m i and 0.5 otherwise. Let k = n here. At state 0, on taking action a = do(), we transition uniformly to one of the intermediate contexts. On taking action do(X 0 i = 1), we transition with probability 2/k to context i and probability 1/k -1/(k(k -1)) to any of the other k -1 contexts.</p><p>We perform two experiments in this setting. In the first one, we run ConvExplore and UnifExplore for time horizon T ∈ {1000, . . . , 25000}. In the second experiment, we run ConvExplore and UnifExplore for a fixed time horizon T = 25000 with λ varying in the set {50, 75, . . . , 625}. To vary λ, we vary m i for the intermediate contexts in the set {2, 3, . . . , 25}. We average the regret over 10000 runs for each setting. We use CVXPY <ref type="bibr" target="#b15">(Diamond &amp; Boyd (2016)</ref>) to solve the convex program at Step 4 in ConvExplore. We release our code in entirety in our anonymized GitHub project repository, for the community to use and improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of comparison with UnifExplore:</head><p>In Figure <ref type="figure" target="#fig_9">3a</ref>, we compare the expected simple regret of ConvExplore vs. UnifExplore. Our plots indicate that ConvExplore outperforms UnifExplore and its regret falls rapidly as T increases. In Figure <ref type="figure" target="#fig_9">3b</ref>, we plot the expected simple regret against λ for ConvExplore and UnifExplore that was obtained in Experiment 2, and empirically validate their relationship that was proved in Theorem 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of comparison withother baselines:</head><p>We find that ConvExplore significantly outperforms baselines other than UnifExplore. Specifically Thompson samplling and UCB are not well tuned to the exploration problem, and hence perform poorly in both the metrics of (1) simple regret as well as (2) probability of finding the best intervention. A mixture of round-robin at the start state with these alternatives at the intermediate context also perform poorly with respect to ConvExplore for this particular exploration problem. In Figure <ref type="figure" target="#fig_5">4</ref> we plot the metrics with exploration budget. In Figure <ref type="figure">5</ref> we plot the metrics of interest with the number of contexts at the intermediate stage. Finally, in Figure <ref type="figure" target="#fig_6">6</ref>, we plot the simple regret as well as probability of finding the best intervention with our parameter λ, while keeping the number of intermediate contexts the same. The results of these experiments and full details can be found here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We studied extensions of the causal contextual bandits framework to include adaptive context choice. This is an important problem in practice and the solutions therein have immediate practical applications. The setting of stochastic transition to a context accounted for non-trivial extensions from Subramanian &amp; Ravindran (2022) who studied targeted interventions. We developed a Convex Exploration algorithm for minimizing simple regret under this setting. Furthermore, while <ref type="bibr" target="#b32">Maiti et al. (2022)</ref> studied the simple causal bandit setting with unobserved confounders, our work addresses causal contextual bandits with adaptive contexts, under the same constraint of allowing unobserved confounders (assuming identifiability). We identified an instance dependent parameter λ, and proved that the regret of this algorithm is Õ( 1 T max{λ, m0 p+ }). The current work also established that, for certain families of instances, this upper bound is essentially tight. Finally, we showed through experiments that our algorithm performs better than uniform exploration in a range of settings. We believe our method of converting the exploration in the causal contextual bandit setting is novel, and may have implications outside the causal setting as well.</p><p>Possible generalizations of this work include extensions to non-binary reward settings. Another natural extension would be to derive bounds for L-layered MDPs, extending from the adaptive contextual bandit setting we consider. It would be interesting to see whether that problem reduces to convex exploration as well. Finally, extending convex exploration methods from this paper to other more general simple regret problems may also be a promising avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>In our work, we draw from prior literature from causality as well as from multi-armed bandits. We will briefly cover these two in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Multi-armed bandits:</head><p>The stochastic Multi-Armed Bandit (MAB) setup is a standard model for studying the explorationexploitation trade-off in sequential decision making problems <ref type="bibr" target="#b23">(Kuleshov &amp; Precup, 2014;</ref><ref type="bibr" target="#b10">Bubeck et al., 2012)</ref>. Such trade-offs arise in several modern applications, such as ad placement, website optimization, recommendation systems, and packet routing <ref type="bibr" target="#b8">(Bouneffouf et al., 2020)</ref> and are thus a central part of the theory relating to online learning <ref type="bibr" target="#b43">(Slivkins et al., 2019;</ref><ref type="bibr">Lattimore &amp; Szepesvári, 2020)</ref>.</p><p>Traditional performance measures for MAB algorithms have focused on cumulative regret <ref type="bibr" target="#b6">(Auer et al., 2002;</ref><ref type="bibr" target="#b1">Agrawal &amp; Goyal, 2012;</ref><ref type="bibr" target="#b5">Auer &amp; Ortner, 2010)</ref>, as well as best-arm identification under the fixed confidence <ref type="bibr">(Even-Dar et al., 2006)</ref> and fixed budget <ref type="bibr" target="#b3">(Audibert et al., 2010)</ref> settings. In some settings however, one may be interested in optimizing the exploration phase. Another variant of regret that has been considered is the mini-max regret <ref type="bibr" target="#b7">(Azar et al., 2017)</ref> which focuses on the worst case over all possible environments. However, as a metric for pure exploration in MABs, simple regret has been proposed as a natural performance criterion <ref type="bibr" target="#b9">(Bubeck et al., 2009)</ref>. In this setting, we allow for some period of exploration, after which the learner has to choose an arm. The simple regret is then evaluated as the difference between the average reward of the best arm and the average reward of the learner's recommendation. We focus on simple regret in this work.</p><p>Each of these performance metrics come with their own lower bounds <ref type="bibr" target="#b36">(Orabona et al., 2012;</ref><ref type="bibr" target="#b37">Osband &amp; Van Roy, 2016;</ref><ref type="bibr" target="#b10">Bubeck et al., 2012)</ref>, which are naturally the benchmarks for any algorithms proposed. The lower bound on simple regret is known to be O( n/T ) for a stochastic multi-armed bandit problem with n arms. This bound is obtained from the lower bound for pure exploration provided by <ref type="bibr" target="#b34">Mannor &amp; Tsitsiklis (2004)</ref>.</p><p>Note that, a naive approach to the causal bandit problem which simply treats an intervention on each of exponentially many combinations of the nodes as an arm, may thus incur an exponential regret. We now review some of the literature from Causality, which helps in addressing the causal aspects of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Causality:</head><p>There are three broad threads in causality related to our work. These are causal graph learning, causal testing and causal bandits. We address relevant works in these areas below.</p><p>Learning Causal Graphs: <ref type="bibr" target="#b46">Tian &amp; Pearl (2002)</ref> laid the grounds for analysing functional functional constraints among the distributions of observed variables in a causal Bayesian networks. Similarly, <ref type="bibr" target="#b20">Kang &amp; Tian (2006)</ref> derive such functional constraints over interventional distributions. These two seminal works lead to a great interest in the problem of learning causal graphs.</p><p>There have been several studies that provide algorithms to recover the causal graphs from the conditional independence relations in observational data <ref type="bibr" target="#b38">(Pearl &amp; Verma, 1995;</ref><ref type="bibr" target="#b44">Spirtes et al., 2000;</ref><ref type="bibr" target="#b2">Ali et al., 2005;</ref><ref type="bibr" target="#b52">Zhang, 2008)</ref>. Subsequent work considered the setting when both observational and interventional data are available <ref type="bibr" target="#b16">(Eberhardt et al., 2005;</ref><ref type="bibr" target="#b18">Hauser &amp; Bühlmann, 2014)</ref>. <ref type="bibr">Kocaoglu et al. (2017a)</ref> extend the causal graph learning problem to a budgeted setting. <ref type="bibr" target="#b42">Shanmugam et al. (2015)</ref> uses interventions on sets of small size to learn the causal structure. <ref type="bibr">Kocaoglu et al. (2017b)</ref> provide an efficient randomized algorithm to learn a causal graph with confounding variables.</p><p>Testing over Bayesian networks: Given sample access to an unknown Bayesian Network <ref type="bibr" target="#b11">(Canonne et al., 2017)</ref>, or Ising model <ref type="bibr" target="#b13">(Daskalakis et al., 2019)</ref>, one may wish to decide whether an unknown model is equal to a known fixed model, and analyse the sample complexity of this hypothesis test. <ref type="bibr" target="#b0">Acharya et al. (2018)</ref> address this question by introducing the concept of covering interventions. These covering interventions allow us to understand the behaviour of multiple interventions (that are covered) simultaneously. We utilize the concept of covering interventions from <ref type="bibr" target="#b0">Acharya et al. (2018)</ref> towards our question of finding the optimal intervention in a causal bandit. The area of reinforcement learning over causal bandits has also been studied in <ref type="bibr" target="#b53">Zhang (2020)</ref>.</p><p>Apart from these areas in causality, our primary problem of causal bandits have been addressed by <ref type="bibr" target="#b24">Lattimore et al. (2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithms in Detail</head><p>In this section, we outline the three algorithms that are used as helpers in ConvExplore. The first that we outline now, Algorithm 2, would be used to estimate the transition probabilities out of context 0 on taking various actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Estimate Transition Probabilities</head><formula xml:id="formula_13">1: Input: Time budget T ′ 2: For time t ← {1, . . . , T ′ 2 } do 3: Perform do() at context 0. Transition to i ∈ [k] 4: Count number of times context i ∈ [k] is observed 5:</formula><p>Update q 0 j = P X 0 j = 1 end 6: Using q 0 j s, estimate m0 and the set Am Next we estimate the causal parameters at all contexts i ∈ [k] through Algorithm 3. Then we will use Algorithm 4 to estimate the rewards on various interventions at the intermediate contexts.</p><p>For estimating the causal parameters, we use a variant of SRM-ALG from <ref type="bibr" target="#b32">Maiti et al. (2022)</ref>, which estimates the causal observational threshold m i , under the setting of unobserved confounders and identifiability. We note that even in the presence of general causal graphs with hidden variables, SRM-ALG is able to efficiently estimate the rewards of all the arms simultaneously using the observational arm pulls. As mentioned in Section 3 of <ref type="bibr" target="#b32">Maiti et al. (2022)</ref>, the challenge is to identify the optimal number of arms with bad estimates during the initial phase of the algorithm, such that these arms can be intervened upon at the later phase. The q i (x) parameter is the minimum conditional probability of X = x, given different configurations of the parents of X. Once we have these estimates, the remaining algorithm can proceed as per usual. Perform a ∈ A0 and transition to some i ∈ [k].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>At context i, perform do() and observe X i j s 7:</p><p>Update q i j = min Parents(X i j ),x∈{0,1} P X i j = x | Parents(X i j ) end end 8: Using q i j s, estimate mi values for each context i ∈ [k] 9: return M , the diagonal matrix of the mi values a We choose actions a ∈ A 0 such that we visit the contexts i ∈ [k] approximately equally, in expectation. b On each visit to a context i ∈ [k], we perform do(). From these we can estimate q j i values, which may be used to estimate m i values.</p><p>c Based on these do() interventions at each context i ∈ [k], we get estimates of m i and the intervention sets Am i such that (I) |Am i | = m i and (II) interventions in Am i are observed with probability less than 1/m i .</p><p>Note that in Algorithm 4 there are two phases. In the first phase, we carry out estimates for interventions that have high probability of being observed on the do() intervention. In the second phase, we specifically perform interventions which have not been observed often enough. This is similar to Algorithm 2 where we carry out the two phases of interventions at context 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Estimate Rewards</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Observe variables X i j 's and rewards Ri. end end 7: Find the set Am i ∀i ∈ [k] using q i j estimates.</p><formula xml:id="formula_14">8: Estimate mean reward R (b,i) = E [Ri | b] for each b ∈ A c m i 9: For intervention a ∈ A0 at context 0 10: For time t ← {1, . . . f (a) • T ′ /2} 11:</formula><p>Perform a ∈ A0 and transition to some i ∈ [k].</p><p>12: c Note that we round robin over the interventions b ∈ Am i across visits in the second half of the algorithm.</p><formula xml:id="formula_15">Iteratively perform b ∈ Am i . Observe Ri end end 13: Estimate mean reward R (b,i) = E [Ri | b] for each b ∈ Am i 14: return R = R (b,i) i∈[k],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 1</head><p>In this section, we restate Theorem 1 and provide its proof, along with all the lemmas that are used in the proof.</p><p>Theorem. Given number of rounds T ≥ T 0 and λ as in equation ( <ref type="formula">3</ref>), ConvExplore achieves regret</p><formula xml:id="formula_16">Regret T ∈ O max λ T , m 0 T p + log (N T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Theorem 1</head><p>To prove the theorem, we analyze the algorithm's execution as falling under either good event or bad event, and tackle the regret under each.</p><p>Definition 1. We define five events, E 1 to E 5 (see Table <ref type="table">3</ref>), the intersection of which we call as good event, E, i.e., good event E := i∈[5] E i . Furthermore, we define the bad event</p><formula xml:id="formula_17">F := E c . Table 3: Table enumerating Good Events Event Condition Explanation E1 k i=1 | P (a,i) -P (a,i) | ≤ p + 3 ∀a ∈ A0</formula><p>for every intervention a ∈ A0, the empirical estimate of transition probability in each of Algorithms 2, 3 and 4 is good, up to an absolute factor of p+/3</p><formula xml:id="formula_18">E2 m0 ∈ [ 2 3 m0, 2m0]</formula><p>our estimate for causal parameter m0 for state 0 is relatively good in Algorithm 2.</p><formula xml:id="formula_19">E3 mi ∈ [ 2 3 mi, 2mi] ∀i ∈ [k]</formula><p>our estimate for causal parameter mi for each context i ∈ [k] is relatively good in Algorithm 3.</p><formula xml:id="formula_20">E4 i∈[k] | P (a,i) -P (a,i) | ≤ ζ, ∀a ∈ A0</formula><p>The error in estimated transition probability in Algorithm 2 sums to less than ζ where</p><formula xml:id="formula_21">ζ := 150m 0 T p + log 3T k E5 E [Ri | a] -R (a,i) ≤ ηi ∀i ∈ [k], a ∈ Ai</formula><p>The error in reward estimates in Algorithm 4 is bounded<ref type="foot" target="#foot_5">foot_5</ref> by ηi where ηi =</p><formula xml:id="formula_22">27 m i T ( P ⊤ f * ) i log (2T N )</formula><p>Considering the estimates P and M , along with frequency vector 2 f * (computed in Step 4), we define random variable</p><formula xml:id="formula_23">λ := P M 1/2 P ⊤ f * •-1 2 2 ∞ .</formula><p>Note that λ is a surrogate for λ. We will show that under the good event, λ is close to λ (Lemma 3).</p><p>Recall that Regret T := E[ε(π)] and here the expectation is with respect to the policy π computed by the algorithm. We can further consider the expected sub-optimality of the algorithm and the quality of the estimates (in particular, P , M and λ) under good event (E).</p><p>Based on the estimates returned at Step 4 of ConvExplore, either the good event holds, or we have the bad event. We obtain the regret guarantee by first bounding sub-optimality of policies computed under the good event, and then bound the probability of the bad event.</p><p>Lemma 1. For the optimal policy π * , under the good event (E), we have</p><formula xml:id="formula_24">i∈[k] P (π * (0),i) E [R i | π * (i)] -P (π * (0),i) R (π * (i),i) ≤ O max{ λ, m 0 /p + }/T log (N T )</formula><p>Proof. We add and subtract i∈[k] P (π * (0),i) R (π * (i),i) and reduce the expression on the left to:</p><formula xml:id="formula_25">i∈[k] P (π * (0),i) (E [R i | π * (i)] -R (π * (i),i) ) + i∈[k] R (π * (i),i) (P (π * (0),i) -P (π * (0),i) ).</formula><p>We have: (a) R (π</p><formula xml:id="formula_26">* (i),i) ≤ 1 (as rewards are bounded) (b) i∈[k] | P (π * (0),i) -P (π * (0),i) | ≤ ζ (by E 4 ) and (c) E [R i | π * (i)] -R (π * (i),i) ≤ η i (by E 5 ).</formula><p>The above expression is thus bounded above by i∈[k] P (π * (0),i) η i + ζ Furthermore, it follows from E 1 (See Corollary 2 in Section D.1 in the supplementary material) that (component-wise) P ≤ 3 2 P . Hence, the above-mentioned expression is bounded above by</p><formula xml:id="formula_27">3 2 i∈[k] P (π * (0),i) η i + ζ. Note that the definition of λ en- sures i∈[k] P (π * (0),i) η i = O( λ/T log(N T )). Further, ζ = O( m 0 /(T p + ) log(T /k)). Hence, i∈[k] P (π * (0),i) η i + ζ = O( max{ λ, m 0 /p + }/T log (N T ))</formula><p>, which establishes the lemma.</p><p>We now state another similar lemma for any policy π computed under good event.</p><p>Lemma 2. Let π be a policy computed by ConvExplore under the good event (E). Then,</p><formula xml:id="formula_28">i∈[k] P ( π(0),i) R ( π(i),i) -i∈[k] P ( π(0),i) E [R i | π(i)] ≤ O max{ λ, m 0 /p + }/T log (N T )</formula><p>Proof. We can add and subtract i∈[k] P ( π(0),i) R ( π(i),i) to the expression on the left to get:</p><formula xml:id="formula_29">i∈[k] R ( π(i),i) ( P ( π(0),i) -P ( π(0),i) ) + i∈[k] P ( π(0),i) ( R ( π(i),i) -E [R i | π(i)]). Analogous to Lemma 1, one can show that this expression is bounded above by ζ + i∈[k] 3 2 P ( π(0),i) η i = O( max{ λ, m 0 /p + }/T log (N T )).</formula><p>We can also bound λ to within a constant factor of λ. Lemma 3. Under the good event E, we have λ ≤ 8λ.</p><p>Proof. Event E 1 ensures that 2 3 P ≤ P ≤ 4 3 P (see Corollary 2 in Appendix section D.1). In addition, note that event E 3 gives us M ≤ 2M . From these observations we obtain the desired bound: λ = P M 0.5 ( P ⊤ f * ) •-0.5 ≤ P M 0.5 ( P ⊤ f * ) •-0.5 ≤ 8P M 0.5 (P ⊤ f * ) •-0.5 = 8λ; here, the first inequality follows from the fact that f * is the minimizer of the λ expression, and for the second inequality, we substitute the appropriate bounds of P and M .</p><p>Recall that:</p><formula xml:id="formula_30">π * (i) = arg max a∈Ai E [R i | a] π * (0) = arg max b∈A0 ( k i=1 E [R i | π * (i)] • P{i | b})</formula><p>We will now define ε(π), denoting the sub-optimality of a policy π, as the difference between the expected rewards of π * and π. i.e. ε(π) Proof. Since ConvExplore selects the optimal policy (maximizing rewards with respect to the estimates),</p><formula xml:id="formula_31">= k i=1 E [R i | π * (i)]•P{i | π * (0)}- k i=1 E [R i | π(i)]•P{i | π(0)}.</formula><formula xml:id="formula_32">P (π * (0),i) R (π * (i),i) ≤ P ( π(0),i) R ( π(i),i) .</formula><p>Combining this with Lemmas 1 and 2, we</p><formula xml:id="formula_33">get i∈[k] P (π * (0),i) E [R i | π * (i)] -i∈[k] P ( π(0),i) E [R i | π(i)] = O( max{ λ, m 0 /p + }/T log (N T )) under good event.</formula><p>The left-hand-side of this expression is equal to ε( π). Using Lemma 3, we get that ε( π) = O max{λ, m 0 /p + }/T log (N T ) .</p><p>Corollary 1 shows that under the good event, the (true) expected reward of π * and π are within O max{λ, m 0 /p + }/T log (N T ) of each other. In Lemma 10 (see Section D.5 in the supplementary material) we will show<ref type="foot" target="#foot_7">foot_7</ref> that P{ i∈[5] ¬E i } = P {F } ≤ 5k/T whenever T ≥ T 0<ref type="foot" target="#foot_8">foot_8</ref> .</p><p>The above-mentioned bounds together establish Theorem 1 (i.e., bound the regret of ConvExplore):</p><formula xml:id="formula_34">Regret T = E[ε(π)] = E[ε( π) | E]P {E} + E[ε(π ′ ) | F ]P {F }.</formula><p>Since the rewards are bounded between 0 and 1, we have ε(π</p><formula xml:id="formula_35">′ ) ≤ 1, for all policies π ′ . But P{E} ≤ 1 giving us Regret T ≤ E[ε(π) | E] + P{F }.</formula><p>Therefore, Corollary 1 along with Lemma 10, leads to guaran-</p><formula xml:id="formula_36">tee Regret T = O max{λ, m 0 /p + }/T log (N T ) + 5k/T = O max{λ, m 0 /p + }/T log (N T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Bounding the Probability of the Bad Event</head><p>Recall that the good event corresponds to i∈5 E i (see Definition 1). Write F := ¬ i∈5 E i and note that, for the regret analysis, we require an upper bound on P{F } = P ¬( i∈5 E i ) = P i∈5 ¬E i . Towards this, in this section we address P{¬E i }, for each of the events E 1 -E 5 , and then apply the union bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Bound on ¬E 1</head><p>The next lemma upper bounds the probability of ¬E 1 . Lemma 4. In each of Algorithms 2, 3 and 4 and for all interventions a ∈ A 0 , we have</p><formula xml:id="formula_37">P{¬E 1 } = P k i=1 | P (a,i) -P (a,i) | &gt; p+ 3 &lt; k T whenever T ≥ max 1620N p 3 + , 2025N p 2 + log 9N T k .</formula><p>Proof. On performing any intervention a ∈ A 0 at context 0, the intermediate context that we visit follows a multinomial distribution. Hence, we can apply Devroye's inequality (for multinomial distributions) to obtain a concentration guarantee; we state the inequality next in our notation.</p><p>Lemma 5 (Restatement of Lemma 3 in <ref type="bibr" target="#b14">Devroye (1983)</ref>). Let T a be the number of times intervention a ∈ A 0 is performed in context 0. Then, for any η &gt; 0 and any T a ≥ 20s η 2 , we have</p><formula xml:id="formula_38">P k i=1 | P (a,i) -P (a,i) | &gt; η ≤ 3 exp -Taη 2 25 .</formula><p>Here, s is the support of the distribution (i.e., the number of contexts that can be reached from a with a nonzero probability).</p><p>Note that each intervention a ∈ A 0 is performed at least T a = T 9N times across Algorithms 2, 3 and 4. Setting η = p+ 3 and T a = T 9N above, we get that for each intervention a ∈ A 0 , in each subroutine,</p><formula xml:id="formula_39">P k i=1 |P (a,i) -P (a,i) | &gt; p+ 3 ≤ 3 exp - T p 2 + 9N •9•25 = 3 exp - T p 2 + 2025N .</formula><p>Note that to apply the inequality, we require T 9N ≥ 180s</p><formula xml:id="formula_40">p 2 + , i.e., T ≥ 1620sN p 2 + .</formula><p>In the current context, the support size s is at most 1 p+ ; this follows from the fact that on performing any intervention a ∈ A 0 , at most 1 p+ contexts can have P (a,i) ≥ p + . Hence, the requirement reduces to T ≥ 1620N</p><formula xml:id="formula_41">p 3 + .</formula><p>Next, we union bound the probability over the N interventions (at state 0) and the three subroutines, to obtain that, for any intervention a ∈ A 0 and in any subroutine,</p><formula xml:id="formula_42">P k i=1 |P (a,i) -P (a,i) | &gt; p+ 3 ≤ 3N • 3 exp - T p 2 + 2025N = 9N exp - T p 2 + 2025N .</formula><p>Note that 9N exp -</p><formula xml:id="formula_43">T p 2 + 2025N ≤ k T , for any T ≥ 2025N p 2 + log 9N T k . Hence, for any T ≥ max 1620N p 3 + , 2025N p 2 + log 9N T k , we have P[¬E 1 ] ≤ 9N exp - T p 2 + 2025N ≤ k T .</formula><p>This completes the proof of the lemma.</p><p>We state below a corollary which provides a multiplicative bound on P with respect to P , complementing the additive form of E 1 .</p><p>Corollary 2. Under event E 1 , we have 2 3 P (a,i) ≤ P (a,i) ≤ 4 3 P (a,i) , for all interventions a ∈ A 0 and contexts</p><formula xml:id="formula_44">i ∈ [k]. Proof. Event E 1 ensures that k i=1 | P (a,i) -P (a,i) | ≤ p+ 3 , for each interventions a ∈ A and contexts i ∈ [k]</formula><p>. This, in particular, implies that for each intervention a ∈ A 0 and context i ∈ [k] the following inequality holds: | P (a,i) -P (a,i) | ≤ p+ 3 . Note that if P (a,i) = 0, then the algorithm will never observe context i with intervention a, i.e., in such a case P (a,i) = P (a,i) = 0. For the nonzero P (a,i) s, recall that (by definition), p + = min{P (a,i) | P (a,i) &gt; 0}. Therefore, for any nonzero P (a,i) , the above-mentioned inequality gives us | P (a,i) -P (a,i) | ≤ 1 3 P (a,i) . Equivalently, P (a,i) ≤ 4 3 P (a,i) and P (a,i) ≥ 2 3 P (a,i) . Therefore, for all P (a,i) s the corollary holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Bound on Events ¬E 2 and ¬E 3</head><p>In this section, we bound the probabilities that our estimated m i s are far away from the true causal parameters m i s.</p><p>Lemma 6. For any</p><formula xml:id="formula_45">T ≥ 144m 0 log T N k , in Algorithm 2, P[¬E 2 ] = P m 0 / ∈ [ 2 3 m 0 , 2m 0 ] ≤ k T .</formula><p>Proof. We allocate time T 3 to Algorithm 2. Lemma 8 of <ref type="bibr" target="#b24">Lattimore et al. (2016)</ref> ensures that, for any δ ∈ (0, 1) and T 3 ≥ 48m 0 log( N δ ), we have m 0 ∈ [ 2 3 m 0 , 2m 0 ], with probability at least (1 -δ). Setting δ = k T , we get the required probability bound.</p><p>Next, we address P{¬E 3 | E 1 }.</p><p>Lemma 7. For any T ≥ 648 max(mi)N p+ log (2N T ), in each of Algorithms 3 and 4, we have</p><formula xml:id="formula_46">P ∃i ∈ [k], m i / ∈ [ 2 3 m i , 2m i ] E 1 ≤ k T . Proof. Fix any reachable context i ∈ [k].</formula><p>Corresponding to such a context, there exists an intervention α ∈ A 0 such that P (α,i) ≥ p + . Event E 1 (Corollary 2) implies that P (α,i) ≥ 2 3 P (α,i) ≥ 2 3 p + . Now, write T i to denote the number of times context i ∈ [k] is visited by the Algorithms 3 and 4. Recall that in the subroutines we estimate P (α,i) by counting the number of times context i was reached and simultaneously intervention α observed. Furthermore, note that we allocate to every intervention at least T 9N time (See Steps 2 in both the subroutines). In particular, intervention α was necessarily observed T 9N times. Therefore, P (a,i) ≤ Ti ( T 9N )</p><p>. This inequality leads to a useful lower bound: T i ≥ T 9N P (a,i) ≥ T 2p+ 27N . We now restate Lemma 8 from <ref type="bibr" target="#b24">Lattimore et al. (2016)</ref></p><formula xml:id="formula_47">: Let T i be the number of times context i ∈ [k] is observed. Then, P m i / ∈ [ 2 3 m i , 2m i ] ≤ 2N exp -Ti 48mi .</formula><p>Since T i ≥ 2T p+ 27N , this guarantee of <ref type="bibr" target="#b24">Lattimore et al. (2016)</ref> </p><formula xml:id="formula_48">corresponds to P m i / ∈ [ 2 3 m i , 2m i ] ≤ 2N exp -T p+ 648N mi ≤ 2N exp - T p+ 648N max(mi) .</formula><p>Union bounding over all contexts i ∈ [k] and the two Algorithms 3 and 4, we obtain</p><formula xml:id="formula_49">P ∃i ∈ [k] in Algorithms 3, 4 with m i / ∈ [ 2 3 m i , 2m i ] ≤ 2N k exp - T p+ 648N max(mi) .Finally, substituting the value of T ≥ 648 max(mi)N p+ log (2N T ), gives us P ∃i ∈ [k] in Algorithms 3, 4 with m i / ∈ [ 2 3 m i , 2m i ] ≤ 2N k exp - p+ 648N max(mi) • 648 max(mi)N p+ log (2N T ) = k T .</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Bound on E 4 :</head><p>The following lemma provides an upper bound for</p><formula xml:id="formula_50">P{¬E 4 | E 2 }. Lemma 8. Let ζ := 150m0 T p+ log 3T k . Then, P{¬E 4 | E 2 } = P i∈[k] P (a,i) -P (a,i) &gt; ζ E 2 ≤ k T .</formula><p>Proof. As in the proof of Lemma 4, we will use Devroye's inequality. Write T a to denote the number of times intervention a ∈ A 0 is observed (in state 0) in Algorithm 2. For any η ∈ (0, 1) and with</p><formula xml:id="formula_51">T a ≥ 20s η 2 , Devroye's inequality gives us P k i=1 | P (a,i) -P (a,i) | &gt; η ≤ 3 exp -Taη 2 25 .</formula><p>Here, s is the size of the support of the multinomial distribution.</p><p>We first show that T a is sufficiently large, for each intervention a ∈ A 0 . Recall that we allocate time T 3 to Algorithm 2. Furthermore, we observe each intervention in state 0, at least T 3 m0</p><p>times, either as part of the do-nothing intervention or explicitly in Step 9 of Algorithm 2. Now, event</p><formula xml:id="formula_52">E 2 ensures that m 0 ∈ [ 2 3 m 0 , 2m 0 ]. Hence, each intervention a ∈ A 0 is observed T a ≥ T 3 m0 ≥ T 3•2m0 = T 6m0</formula><p>times. Substituting this inequality for T a in the above-mentioned probability bound, we obtain</p><formula xml:id="formula_53">P k i=1 | P (a,i) -P (a,i) | &gt; η ≤ 3 exp -T η 2 150m0 when T ≥ 120sm0 η 2</formula><p>. As observed in Lemma 4, the support size s is at most 1 p+ . Therefore, the requirement on T reduces to T ≥ 120m0 η 2 p+ .</p><p>Setting η = 150m0 T p+ log 3T k gives us</p><formula xml:id="formula_54">P k i=1 | P (a,i) -P (a,i) | &gt; 150m 0 T p + log 3T k ≤ 3 exp   -T 150m 0 150m 0 T p + log 3T k 2   ≤ k T . Therefore P k i=1 | P (a,i) -P (a,i) | &gt; η ≤ k T ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and this probability bound requires</head><formula xml:id="formula_55">T ≥ 120m0 η 2 p+ . That is, η ≥ 120m0 T p+ .</formula><p>This inequality is satisfied by our choice of η. Hence, the lemma stands proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Bound on ¬E 5</head><p>The next lemma bounds</p><formula xml:id="formula_56">P{¬E 5 | E 1 , E 3 }. Lemma 9. Let η i = 27 mi T ( P ⊤ f * )i log (2T N ). Then, P{¬E 5 | E 3 , E 1 } ≤ k T .</formula><p>In other words:</p><formula xml:id="formula_57">P ∃i ∈ [k] and a ∈ A i such that E [R i | a] -R (a,i) &gt; η i | E 3 , E 1 ≤ k T . Proof. For intermediate contexts i ∈ [k],</formula><p>we denote the realization of the causal parameters m i and the transition probabilities P in Algorithm 4, as m i and P , respectively. The estimates in the previous subroutines are denoted by m i and P .</p><p>Event E 1 gives us P (a,i) ∈ [ 3 4 P (a,i) , 3 2 P (a,i) ]and P (a,i) ∈ [ 2 3 P (a,i) , 4 3 P (a,i) ]. Hence, the estimates across the subroutines are close enough:</p><formula xml:id="formula_58">P (a,i) ∈ [ 1 2 P (a,i) , 2 P (a,i) ]. Similarly, event E 3 gives us m i ∈ [ 1 3 m i , 3 m i ].</formula><p>Write T i to denote the number of times context i ∈ [k] was visited in Algorithm 4. For all contexts i ∈ [k], we first establish a useful lower bound on T i , under events E 1 and E 3 . The relevant observation here is that the estimate P (α,i) was computed in Algorithm 4 by counting the number of times context i was visited with intervention α ∈ A 0 (at state 0). By construction, in Algorithm 4 each intervention α ∈ A 0 was performed at least</p><formula xml:id="formula_59">f * α 3</formula><p>T 3 times. Furthermore, given that P (α,i) was computed via the visitation count, we get that context i is visited with intervention α ∈ A 0 at least P (α,i)</p><formula xml:id="formula_60">T f * α 9 times. Therefore, T i ≥ α∈A0 P (α,i) T f * α 9 = T 9 ( P ⊤ f * ) i ≥ T 18 ( P ⊤ f * ) i .</formula><p>Here, the last inequality follows from the above-mentioned proximity between P and P . Now, note that, at each context i ∈ [k], Algorithm 4 (by construction) observes every intervention a ∈ A i at least Ti mi times. Write T (a,i) to denote the number of times intervention a ∈ A i is observed in this subroutine. Hence,</p><formula xml:id="formula_61">T (a,i) ≥ T i m i ≥ 1 m i T 18 ( P ⊤ f * ) i ≥ 1 3 m i T 18 ( P ⊤ f * ) i For each context i ∈ [k] and intervention a ∈ A i , define the event ¬E 5 (a, i) as |E [R i | a]-R (a,i) | &gt; η i .</formula><p>Hoeffding's inequality gives us</p><formula xml:id="formula_62">P {¬E 5 (a, i) | E 1 , E 3 } ≤ 2 exp -2 T (a,i) η 2 i ≤ 2 exp -T ( P ⊤ f * )i η 2 i 27 mi</formula><p>.</p><p>The last inequality is obtained by substituting Equation D.4.</p><p>Recall that η i =</p><formula xml:id="formula_63">27 mi T ( P ⊤ f * )i log (2T N ).</formula><p>Hence, the previous inequality corresponds to</p><formula xml:id="formula_64">P {¬E 5 (a, i) | E 1 , E 3 } ≤ 2 exp -T ( P ⊤ f * )i 27 mi • 27 mi T ( P ⊤ f * )i log (2T N ) 2 = 1 T N .</formula><p>Note that ¬E 5 = i∈[k] a∈Ai E 5 (a, i). Taking a union bound over all contexts i ∈ [k] and interventions a ∈ A i , we obtain</p><formula xml:id="formula_65">P{¬E 5 | E 1 , E 3 } ≤ kN T N = k T .</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Bound on bad event (F):</head><p>Write T 0 := O N max(mi)</p><formula xml:id="formula_66">p 3 + log (2N T ) = O N max(mi) p 3 + .</formula><p>Lemma 10. P{F } ≤ 5k T for any T &gt; T 0 .</p><p>Proof. We summarize the statements of Lemmas 4, 6, 7, 8 and 9 as follows. When T ≥ T 0 where T 0 = max 1620N P ⊤ f . Then, finding f is an LP</p><formula xml:id="formula_67">p 3 + , 2025N p 2 + log 9N T k , 144m 0 log T n k , 864 max(mi)N p+ log (2nT ) = O N max(mi) p 3 + log (2N T ) , we obtain P{F } = P i∈[5] ¬E i ≤ P{¬E 1 } + P{¬E 2 } + P{¬E 3 | E 1 } + P{¬E 4 | E 2 } + P{¬E 5 | E 3 , E 1 } ≤ 5k T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Nature of the Optimization Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Lower Bounds</head><p>This section establishes Theorem 2. We will identify a collection of instances for causal bandits with intermediate feedback and show that, for any given algorithm A, there exists an instance in this collection for which A's regret is Ω λ T .</p><p>First we describe the collection of instances and then provide the proof.</p><p>For any integer k &gt; 1, consider n = (k -1) causal variables at each context i ∈ {0, 1, . . . , k}. The transition matrix P is set to be deterministic. Specifically, for each i ∈ [n], we have P{i | do(X 0 i = 1)} = 1. For all other interventions at context 0, we transition to context k with probability 1. Such a transition matrix can be achieved by setting q 0 i = 0 for all i ∈ [k -1]. As before, the total number of interventions</p><formula xml:id="formula_68">N := 2n + 1 = 2k -1. Now consider a family of N k + 1 instances 6 {F 0 } ∪ F (a,i) i∈[k],a∈Ai .</formula><p>Here, F 0 and each F (a,i) is an instance of a causal bandit with intermediate feedback with the above-mentioned transition probabilities. The instances differ in the rewards at the intermediate contexts. In particular, in instance F 0 , we set the reward distributions such that E[R i | a] = 1 2 for all contexts i ∈ [k] and interventions a ∈ A i . For each i ∈ [k] and a ∈ A i , instance F (a,i) differs from F 0 only at context i and for intervention a. Specifically, by construction, we will have E[R i | a] = 1 2 + β, for a parameter β &gt; 0. The expected rewards under all other interventions will be 1/2, the same as in F 0 .</p><p>Given any algorithm A, we will consider the execution of A over all the instances in the family. The execution of algorithm A over each instance induces a trace, which may include the realized transition probabilities P , the realized variable probabilities q i j for i ∈ [k] and j ∈ [n] and the corresponding m i s, and the realized rewards R. Each of such realizations (random variables) has a corresponding distribution (over many possible runs of the algorithm). We call the measures corresponding to these random variables under the instances F 0 and F (a,i) as P 0 and P (a,i) , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Proof of Theorem 2</head><p>For any algorithm A and given time budget T , we first consider the A's execution over instance F 0 . As mentioned previously, P 0 denotes the trace distribution induced by the algorithm for F 0 . In particular, write r i to denote the expected number of times context i is visited,</p><formula xml:id="formula_69">r i := E P0 [state i is visited] /T . Recall that m i := max{j | q i (j) &lt; 1 j } and A mi := {do(X i (j) = 1) | q i (j) &lt; 1 j },</formula><p>where the Bernoulli probabilities of the variables at context i are sorted to satisfy q i</p><formula xml:id="formula_70">(1) ≤ q i (2) ≤ • • • ≤ q i (n) .</formula><p>Note that these definitions do not depend on the algorithm at hand. The algorithm, however, may choose to perform different interventions different number of times. Write N (a,i) to denote the expected (under P 0 ) number of times intervention a is performed by the algorithm at context i. Furthermore, let random variable T (a,i) denote the number of times intervention a is observed at context i. Hence, E P0 [T (a,i) ] is the expected number of times intervention a is observed<ref type="foot" target="#foot_10">foot_10</ref> .</p><p>Using the expected values for algorithm A and instance F 0 , we define a subset of A mi as follows:</p><formula xml:id="formula_71">J i := a ∈ A mi : N (a,i) ≤ 2 T ri mi .</formula><p>The following proposition shows that the size of J i is sufficiently large.</p><p>Proposition F.1. The set J i is non-empty. In particular,</p><formula xml:id="formula_72">m i /2 ≤ |J i | ≤ m i .</formula><p>Proof. The upper bound on the size of subset J i follows directly from its definition: since</p><formula xml:id="formula_73">J i ⊆ I mi we have |J i | ≤ |A mi | = m i .</formula><p>For the lower bound on the size of J i , note that T r i is the expected number of times context i is visited by the algorithm. Therefore, </p><formula xml:id="formula_74">a∈Am i N (a,i) ≤ T r i Furthermore,</formula><formula xml:id="formula_75">E P0 [T (a,i) ] ≤ 3T r i m i .</formula><p>Proof. Any intervention a ∈ J i ⊆ A mi may be observed either when it is explicitly performed by the algorithm or as a random realization (under some other intervention, including do-nothing). Since a ∈ A mi , the probability that a is observed as part of some other intervention is at most 1 mi . Therefore, the expected number of times that a is observed by the algorithm-without explicitly performing it-is at most T ri mi ;<ref type="foot" target="#foot_11">foot_11</ref> recall that the expected number of times context i is visited is equal to T r i .</p><p>For any intervention a ∈ J i , by definition, the expected number of times a is performed N (a,i) ≤ 2T ri mi . Therefore, the proposition follows:</p><formula xml:id="formula_76">E[T (a,i) ] ≤ T r i m i + N (a,i) ≤ 3T r i m i .</formula><p>We now state two known results for KL divergence.</p><p>Bretagnolle-Huber Inequality (Theorem 14.2 in Lattimore &amp; Szepesvári (2020)) : Let P and P ′ be any two measures on the same measurable space. Let E be any event in the sample space with complement E c . Then, Substituting this in the Bretagnolle-Huber Inequality, we obtain, for any event E in the sample space along with all contexts i ∈ [k] and all interventions a ∈ J i :</p><formula xml:id="formula_77">P P {E} + P P ′ {E c } ≥<label>1</label></formula><formula xml:id="formula_78">P P (a,i) {E} + P P0 {E c } ≥ 1 2 exp -18 T r i β 2 m i</formula><p>We now define events to lower bound the probability that Algorithm A returns a sub-optimal policy. In particular, write π to denote the policy returned by algorithm A. Note that π is a random variable.</p><p>For any ℓ ∈ [k] and any intervention b, write G 1 (b, ℓ) to denote the event that-under the returned policy π-intervention b is not chosen at context ℓ, i.e., G 1 (b, ℓ) := { π(ℓ) ̸ = b}. Also, let G 2 (ℓ) denote the event that policy π does not induce a transition to ℓ from context 0, i.e., G 2 (ℓ</p><formula xml:id="formula_79">) := { π(0) ̸ = ℓ}. Furthermore, write G(b, ℓ) := G 1 (b, ℓ) ∪ G 2 (ℓ). Note that the complement G c (b, ℓ) = G c 1 (b, ℓ) ∩ G c 2 (ℓ) = { π(ℓ) = b} ∩ { π(0) = ℓ}.</formula><p>Considering measure P 0 , we note that for each context ℓ ∈ [k] there exists an intervention α ℓ ∈ J ℓ with the property that</p><formula xml:id="formula_80">P P0 {G c 1 (α ℓ , ℓ)} = P P0 { π(ℓ) = α ℓ } ≤ 1 |J ℓ |</formula><p>. This follows from the fact that a∈J ℓ P P0 { π(ℓ) = a} ≤ 1. Therefore, for each context ℓ ∈ [k] there exists an intervention α ℓ such that P P0 {G c (α ℓ , ℓ)} ≤ 1 |J ℓ | . This bound and inequality F.1 imply that for all contexts ℓ ∈ [k] there exists an intervention α ℓ that satisfies</p><formula xml:id="formula_81">P P (α ℓ ,ℓ) {G(α ℓ , ℓ)} ≥ 1 2 exp -18 T r ℓ β 2 m ℓ - 1 |J ℓ | We will set β = min    1 3 , ℓ∈[k] m ℓ 18T    Therefore β takes value either ℓ∈[k] m ℓ 18T</formula><p>or 1 3 . We will address these over two separate cases.</p><p>Case</p><formula xml:id="formula_82">1: β = ℓ∈[k] m ℓ 18T .</formula><p>We wish to substitute this β value in Equation F.1. Towards this, we will state a proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition F.3. There exists a context</head><formula xml:id="formula_83">s ∈ [k] such that m s 18T r s ≥ ℓ∈[k] m ℓ 18T</formula><p>Proof. First, we note the following claim considering all vectors ρ = {ρ 1 , . . . , ρ k } in the probability simplex ∆.</p><p>Claim F.1. For any given set of integers m 1 , m 2 , . . . , m k , we have min Note that the last inequality lower bounds the to probability of selecting a non-optimal policy when the algorithm A is executed on instance F αs,s . Furthermore, in instance F αs,s , for any non-optimal policy π we have ε( π) = 1 2 + β -1 2 = β. Therefore, we can lower bound A's regret over instance F αs,s as follows: </p><formula xml:id="formula_84">(ρ1,ρ2,...,ρ k )∈∆ max ℓ∈[k] m ℓ ρ ℓ ≥ ℓ∈[k] m ℓ Proof. Assume, towards a contradiction, that for all ℓ ∈ [k], we have m ℓ ρ ℓ &lt; ℓ∈[k] m ℓ . Then, ρ ℓ &gt; m ℓ ℓ∈[k] m ℓ , for all ℓ ∈ [k]. Therefore, ℓ∈[k] ρ ℓ &gt; ℓ∈[k] m ℓ ℓ∈[k] m ℓ = 1.</formula><formula xml:id="formula_85">P ⊤ f =       f 1 f 2 . . . f k-1 f k + . . . + f N      </formula><p>From here, we can compute the following:</p><formula xml:id="formula_86">P M 1/2 P ⊤ f •-1 2 = m 1 f 1 , . . . , m k-1 f k-1 , m k f k + . . . + f N , . . . , m k f k + . . . + f N ⊤</formula><p>That is, for all ℓ ∈ [k -1], the ℓth component of the vector P M 1/2 P ⊤ f •-1 2 is equal to mi fi . All the remaining components are Finally, substituting Proposition F.4 into Equation F.1, we obtain that there exists an instance F (αs,s) for which algorithm A's regret is lower bounded as follows</p><formula xml:id="formula_87">Regret T = Ω λ T .</formula><p>This completes the proof of Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Proof of Inequality (F.1)</head><p>For completeness, we provide a proof of inequality (F.1).</p><p>Lemma 12. KL(P 0 , P (a,i)</p><formula xml:id="formula_88">) ≤ 6β 2 i E P0 [T (a,i) ]</formula><p>Proof of Inequality (F.1). This proof is based on lemma B1 in <ref type="bibr" target="#b4">Auer et al. (1995)</ref>. We define a couple of notations for this proof. Let R t-1 indicate the filtration (of rewards and other observations) up to time t -1. and R t indicate the reward at time t for this proof.</p><p>KL(P 0 , P (a,i) ) = KL P P0 (R T , R T-1 , . . . , R 1 ) ∥ P P (a,i) (R T , R T-1 , . . . , R 1 )</p><p>We now state (without proof) a useful lemma for bounding the KL divergence between random variables over a number of observations.</p><p>Chain Rule for entropy (Theorem 2.5.1 in Cover &amp; Thomas (2006)): Let X 1 , . . . , X T be random variables drawn according to P 1 , . . . , P T . Then</p><formula xml:id="formula_89">H(X 1 , X 2 , . . . , X T ) = T i=1 H(X i | X i-1 , X i-2 , . . . , X 1 )</formula><p>where H(•) is the entropy associated with the random variables.</p><p>Using the chain rule for entropy KL(P 0 , P (a,i) ) = T t=1 KL P P0 (R t | R t-1 ) ∥ P P (a,i) (R t | R t-1 ) Let a t be the intervention chosen by the Algorithm A at time t. Then:</p><formula xml:id="formula_90">= T t=1 P P0 {a t ̸ = a | R t-1 } 1 2 ∥ 1 2 + P P0 {a t = a | R t-1 }KL 1 2 ∥ 1 2 + β i</formula><p>Since KL 1 2 ∥ 1 2 = 0, we get:</p><formula xml:id="formula_91">= T t=1 P P0 {a t = a | R t-1 }KL 1 2 ∥ 1 2 + β i = KL 1 2 ∥ 1 2 + β i T t=1 P P0 {a t = a | R t-1 } = KL 1 2 ∥ 1 2 + β i E P0 [T (a,i) ] Claim F.2. KL 1 2 ∥ 1 2 + β i = -1 2 log 2 (1 -4β 2 i ) ≤ 6β 2 i</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Illustrative Figure for Causal Graph at start state and at some intermediate context i ∈ [k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The transition to a particular context (chosen context in the figure on the left) is decided by the environment, whereas the interventions at the start state and an intermediate context (chosen interventions in the figure on the right) are chosen by the learner.</figDesc><graphic coords="4,236.69,78.08,61.46,163.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) start at context 0. (b) Choose an intervention a ∈ A 0 . (c) Transition to context i ∈ [k]. (d) Choose an intervention a ∈ A i . (e) Receive reward R i . At the end of these T steps, the goal of the learner is to compute a policy. Let the policy returned by the learner be π. Then the simple regret is defined as the expected value: E[µ(π * ) -µ( π]. Our algorithm seeks to minimize such a simple regret.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>+Theorem 1 .</head><label>1</label><figDesc>and thereby λ &lt; nk; See footnote 2 . The following theorem that upper bounds the regret of ConvExplore is the main result of the current work. The result requires the algorithm's time budget to be at least T 0 := O N max(m i )/p 3 Given number of rounds T ≥ T 0 and λ as in equation (3), that m 0 /T p + is independent of the number of contexts and interventions. Therefore λ dominates when number of interventions at an intermediate context is large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: We plot the Simple Regret under ConvExplore and UnifExplore. The figure on the left (3a) plots expected simple regret vs time, for the setup n = 25, k = 25, λ = 50, ε = 0.3 and m = 2 for all contexts. The figure on the right (3b) plots expected simple regret with λ. It was performed with the parameters: T = 25000, k = 25, m 0 = 2 and ε = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: We plot various baselines for two metrics of interest (1) Probability of the algorithm finding the best interventions and (2) Simple regret. These plots illustrate how these metrics vary with the exploration budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: We plot the variation of probability of finding the best intervention and simple regret with λ value. Notice that ConvExplore is the only algorithm that is causal-aware and hence varying with λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>; Maiti et al. (2022); Sen et al. (2017a); Lu et al. (2020); Nair et al. (2021); Sen et al. (2017b); Lu et al. (2021; 2022); Varici et al. (2022);<ref type="bibr" target="#b49">Xiong &amp; Chen (2023)</ref>. We detail these in the main Related Works Section 1.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>a</head><label></label><figDesc>o . Estimate P (a,i) = P[i | a] ∀a ∈ A c m 0 and i ∈ [k] 7: For intervention a ∈ Am o at context 0 8: For time t ← {1, . . . T ′ 2|Am 0 | } 9: Perform a ∈ Am o and transition to some i ∈ [k] 10: Count number of times context i is observed end end 11: Estimate P (a,i) = P[i | a] for each a ∈ Am 0 and contexts i ∈ [k] 12: return Estimated matrix P = P (a,i) i∈[k],a∈A 0 In the first half of time T ′ /2, we perform do() at State 0. b If A 0 := do() ∪ {X 0 j = 0, X 0 j = 1} j∈[n] , we can find m 0 ≤ |A 0 |/2 such that A 0 = Am 0 ∪ A c m 0 where the interventions in A c m 0 are observed with probability more than 1/m 0 and |Am 0 | = m 0 . c For the interventions a ∈ A c m 0 , we can estimate P (a,i) = P[i | a] ∀i ∈ [k] in the first half. d In the second half, we may intervene on the atomic interventions in Am 0 for time T /(2m 0 ) each. e Using observations of a ∈ Am 0 , we estimate P (a,i) = P[i | a] ∀a ∈ Am 0 and i ∈ [k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 3 For</head><label>3</label><figDesc>Estimate Causal Parameters 1: Input: Frequency vector f and time budget T ′ 2: Update f (a) time t ← {1, . . . T ′ • f (a)} 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1:For</head><label></label><figDesc>Input: Optimal frequency f * , min-max frequency f , and time budget T ′ 2: Set f (a) ← 1 3 f * (a) + f (a) + 1 time t ← {1, . . . f (a) • T ′ /2} 5: Perform a ∈ A0. Transition to some i ∈ [k]. Perform do() at context i ∈ [k].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>b∈A ia</head><label></label><figDesc>We perform the interventions in the ratio of f * which is the optimum given the m i values at the various contexts. b In the first half we estimate rewards for the interventions A c m i in the first half, and the interventions in Am i in the second half.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Corollary 1 .</head><label>1</label><figDesc>For any π computed by ConvExplore under good event E, ε( π) = O max{λ, m 0 /p + }/T log (N T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Proposition E. 1 .</head><label>1</label><figDesc>Let f = arg max fq. vectorf min contexts [k]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Regret T = E[ε( π)] = P P (αs ,s) {G(α s , s)} • E[Regret | G(α s , s)] + P P (αs ,s) {G c (α s , s)} • E[Regret | G c (α s , s)] β + P P (αs,s) {G c (α s , s)Note that we can construct the instances to ensure that m ℓ ≥ 8, for all contexts ℓ, and, hence, 1 2e -1 |Ji| = Ω(1) (see Proposition F.1). Therefore Equation F.1 gives us:Regret T = Ω(β) = Ω We now consider the case when β = 1 3 . In such a case, statements, there exists a context s such that ms 18T rs ≥ 1 3 . We now restate Inequality F.1 for such a context s ∈ [k]:P P (αs,s) {G(α s , s)} ≥Following the exact same procedure as in Case 1, we can derive that Regret T ≥ 1 2e -1 |Js| β. We saw in Case 1 that it is possible to construct instances such that 1 2e -1 |Js| = Ω(1). Therefore the Published as a conference paper at RLC F.1 imply that there exists a context s and an intervention α s such that, under instance F (αs,s) , algorithm A's regret satisfies Regret T proof of Theorem 2 by showing that in the current context λ = ℓ∈[k] m ℓ .Proposition F.4. For the chosen transition matrix λ := min fq. vectorf P M 1/2 P ⊤ f Recall that all the instances, F 0 and F (a,i) s, have the same (deterministic) transition matrix P . Also, parameter λ is computed via Equation 3.Consider any frequency vector f over the interventions {1, . . . , N }. From the chosen transition matrix, we have the following:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>m k f k +...+f N . Write ρ ℓ := f ℓ for all ℓ ∈ [k -1] and ρ k = N j=k f j . Since f is a frequency vector, (ρ 1 , . . . ρ k ) definition, λ = min (ρ1,...,ρ k )∈∆ max ℓ∈[k] m ℓ ρ ℓ . Now,using a complementary form of Claim F.1 we obtain λ = ℓ∈[k] m ℓ . The proposition stands proved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,92.34,48.42,424.03,169.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Summary of notations for our paper</figDesc><table><row><cell>Notation</cell><cell>Explanation</cell></row><row><cell>Context 0</cell><cell>Start state</cell></row><row><cell>Context [k]</cell><cell>Intermediate contexts {1, . . . , k}</cell></row><row><cell>X i j</cell><cell>Causal Variables:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 ConvExplore: Convex Exploration Algorithm 1: Input: Total rounds T 2: Estimate the transition probabilities P from the start state to the intermediate contexts for time</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>T /3,</cell></row><row><cell cols="3">by performing interventions at context 0 in a round robin manner.</cell></row><row><cell cols="4">3: Estimate the causal observational threshold matrix M for time T /3, by performing interventions at</cell></row><row><cell cols="2">context 0 as per frequency vector f where f ← arg max</cell><cell>min</cell><cell>P ⊤ f .</cell></row><row><cell></cell><cell>fq. vector f</cell><cell>contexts [k]</cell></row><row><cell cols="4">4: Estimate the reward matrix R for time T /3, by performing interventions a at context 0 as per frequency</cell></row><row><cell>vector f  *  where f  *  ← arg min</cell><cell>max</cell><cell></cell></row><row><cell>fq. vector f</cell><cell>interventions I 0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Plot for Probability of Best Intervention with Number of Intermediate Contexts</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Plot</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>0.12</cell><cell></cell></row><row><cell>Probability of not finding the Best Intervention</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell>Simple Regret</cell><cell>0.02 0.04 0.08 0.10 0.06</cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell>0.00</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20 Number of Intermediate Contexts 40 60 80</cell><cell>100</cell><cell>0</cell><cell>20 Number of Intermediate Contexts 40 60 80</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>for Simple Regret with Number of Intermediate Contexts</head><label></label><figDesc>We plot the variation of probability of finding the best intervention and simple regret with the number of contexts. Notice the outperformance of ConvExplore vs. the other baselines.</figDesc><table><row><cell>roundrobin_roundrobin</cell><cell>roundrobin_ucb</cell><cell>roundrobin_ts</cell><cell>ucb_over_intervention_pairs</cell><cell>ts_over_intervention_pairs</cell><cell></cell><cell>convex_explorer</cell></row><row><cell cols="3">30 Plot for Probability of Best Intervention 40 50 60 70 80 90 100 Lambda Value Figure 5: 20 0.0 0.2 0.4 0.6 0.8 1.0 Probability of not finding the Best Intervention with Lambda Value roundrobin_roundrobin roundrobin_ucb roundrobin_ts</cell><cell>20 ucb_over_intervention_pairs 30 0.01 0.02 0.03 0.04 0.05 Simple Regret</cell><cell>40 with Lambda Value 50 60 70 Lambda Value Plot for Simple Regret 80 ts_over_intervention_pairs</cell><cell>90</cell><cell>100 convex_explorer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>by definition, for each intervention b ∈ A mi \J i we have N (b,i) ≥ 2T ri mi . Hence, assuming |A mi \ J i | &gt; mi 2 would contradict inequality (F.1). This observation implies that |A mi \ J i | ≤ mi 2 and, hence, |J i | ≥ mi 2 . This completes the proof.Recall that T (a,i) denotes the number of times intervention a is observed at context i. The following proposition bounds E[T (a,i) ] for each intervention a ∈ J i .</figDesc><table><row><cell>Proposition F.2. For every intervention a ∈ J i</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>on KL-Divergence with number of observations (Adaptation of Equation 17 in Lemma B1 from Auer et al. (1995)):</head><label></label><figDesc>2 exp (-KL(P, P ′ )) . Let P 0 and P (a,i) be any two measures with differing expected rewards (for exactly the intervention a at context i) by an amount β. Then,</figDesc><table><row><cell cols="3">Bound KL(P 0 , P (a,i) ) ≤ 6β 2 E P0 [T (a,i) ]</cell></row><row><cell cols="3">Using this bound on KL divergence and Proposition F.2, we have, for all contexts i ∈ [k] and</cell></row><row><cell>interventions a ∈ J Tr i m i</cell><cell>= 18</cell><cell>Tr i β 2 m i</cell></row></table><note><p>i : KL(P 0 , P (a,i) ) ≤ 6β 2 • 3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Therefore, irrespective of how r i s are chosen, there always exists a context s ∈ [k] such that For such a context s ∈ [k] that satisfies Proposition F.3, we note that, ms 18T rs ≥ β 2 or 18T rsβ 2 Let us now restate Equation F.1 for such a context s. There exists a context s ∈ [k] and an intervention α s that satisfies P P (αs,s) {G(α s , s)} ≥</figDesc><table><row><cell cols="4">Published as a conference paper at RLC 2024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">An immediate consequence of Claim F.1 is that</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>min (r1,r2,...,r k )∈∆</cell><cell>max ℓ∈[k]</cell><cell cols="2">m ℓ 18T r ℓ</cell><cell>≥</cell><cell></cell><cell cols="2">ℓ∈[k] m ℓ 18T</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ms 18T rs ≥</cell></row><row><cell>ℓ∈[k] 18T</cell><cell>m ℓ</cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 2</cell><cell cols="2">exp -18</cell><cell cols="2">T r s β 2 m s</cell><cell>-</cell><cell>1 |J s |</cell><cell>≥</cell><cell>1 2e</cell><cell>-</cell><cell>1 |J s |</cell></row></table><note><p>However, this is a contradiction as ℓ∈[k] ρ ℓ = 1. ms ≤ 1.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The distribution of any node X i conditioned on it's parents in the causal graph is a Bernoulli random variable with a fixed parameter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The causal graph at each context is semi-Markovian. This is equivalent to making the following assumptions on the graph. No hidden variable in the graph has a parent. Further, every hidden variable has at most two children, both observable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We transform the causal graph for each context as follows: For every hidden variable with two children, we introduce bidirected edges between them. If no path of bidirected edges exists between an intervenable node and its child, the graph is identifiable -a necessary and sufficient condition for estimating the graph's associated distribution.<ref type="bibr" target="#b46">(Tian &amp; Pearl, 2002)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p><ref type="bibr" target="#b32">Maiti et al. (2022)</ref> extend the causal observational threshold from<ref type="bibr" target="#b24">Lattimore et al. (2016)</ref> to the general setting of causal graphs with unobserved confounders</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>λ is upperbounded by kn, but is typically significantly smaller (as m may be much smaller than n).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_5"><p>Recall that f * denotes the optimal frequency vector computed in Step</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6"><p>of ConvExplore. Also, ( P ⊤ f * ) i denotes the ith component of the vector P ⊤ f * .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_7"><p>Recall that, by definition, F = E c .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_8"><p>T 0 as defined in Lemma 10 in Section D.5 in the supplementary material.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_9"><p>Note the change in notation. We used the term F i,j instead of F (a,i) in the main paper. This has been amended in a later version of the main paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_10"><p>Note that a can be observed while performing the do-nothing intervention. Also, the expected value N (a,i) accounts for the number of times a is explicitly performed and not just observed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_11"><p>Here, we use the fact that the realization of a is independent of the visitation of context i.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p><rs type="person">Siddharth Barman</rs> gratefully acknowledges the support of the <rs type="funder">Walmart Center for Tech Excellence</rs> (<rs type="grantNumber">CSR WMGT-23-0001</rs>) and a <rs type="funder">SERB</rs> <rs type="grantName">Core research grant</rs> (<rs type="grantNumber">CRG/2021/006165</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4gAtgEg">
					<idno type="grant-number">CSR WMGT-23-0001</idno>
				</org>
				<org type="funding" xml:id="_zSzKHwP">
					<idno type="grant-number">CRG/2021/006165</idno>
					<orgName type="grant-name">Core research grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. We rewrite the above max fq. vectorf min i∈ <ref type="bibr">[k]</ref> (•) as a simpler program:</p><p>Where N = |A 0 |. This is equivalent to the standard form of a linear program, and hence is an LP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 11. min</head><p>fq. vectorf</p><p>Proof. First we write the min-max in terms of a single minimization. First let us use the shorthand A := P M 1 2 and {A 1 , . . . , A N } (where N := |A 0 |) denote the rows of the matrix</p><p>Proof. We observe that the second derivative is positive.</p><p>Proposition E.3. The constraint equations of OPT are convex in f</p><p>Proof. Consider the first constraint of the problem. We can simplify this to get i∈</p><p>Note that the ith term in the summand (i.e,</p><p>is convex as well. Similarly, all the other constraints are also convex.</p><p>Since the constraints are convex in f and the objective is linear, OPT is convex.</p><p>Proof.</p><p>where the last inequality is obtained from the Taylor series expansion of the log.</p><p>It follows that: KL(P 0 , P 1 ) ≤ 6β 2 i E P0 [T (a,i) ].</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and testing causal models with interventions</title>
		<author>
			<persName><forename type="first">Jayadev</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saravanan</forename><surname>Kandasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of thompson sampling for the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
	<note>Conference on learning theory</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards characterizing markov equivalence classes for directed acyclic graphs with latent variables</title>
		<author>
			<persName><forename type="first">Ayesha</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Best arm identification in multi-armed bandits</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gambling in a rigged casino: The adversarial multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Symposium on Foundations of Computer Science, FOCS &apos;95</title>
		<meeting>the 36th Annual Symposium on Foundations of Computer Science, FOCS &apos;95<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page">322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ucb revisited: Improved regret bounds for the stochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Periodica Mathematica Hungarica</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="55" to="65" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1013689704352</idno>
		<ptr target="https://doi.org/10.1023/A:1013689704352" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002-05">may 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<title level="m">Minimax regret bounds for reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey on applications of multi-armed and contextual bandits</title>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Congress on Evolutionary Computation (CEC)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pure exploration in multi-armed bandits problems</title>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Stoltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory: 20th International Conference</title>
		<title level="s">Proceedings</title>
		<meeting><address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009-10-03">2009. October 3-5, 2009. 2009</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regret analysis of stochastic and nonstochastic multiarmed bandit problems</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Testing bayesian networks</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Clément L Canonne</surname></persName>
		</author>
		<author>
			<persName><surname>Diakonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Daniel M Kane</surname></persName>
		</author>
		<author>
			<persName><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="370" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Elements of Information Theory</title>
		<title level="s">Wiley Series in Telecommunications and Signal Processing</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Testing ising models</title>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishanth</forename><surname>Dikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6829" to="6852" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The equivalence of weak, strong and complete convergence in l1 for kernel density estimates</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Devroye</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2240651" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<idno type="ISSN">00905364</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="896" to="904" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CVXPY: A Python-embedded modeling language for convex optimization</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">83</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="178" to="184" />
		</imprint>
	</monogr>
	<note>On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combinatorial causal bandits</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i6.25917</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/25917" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023-06">Jun. 2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7550" to="7558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two optimal strategies for active learning of causal models from interventional data</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="926" to="939" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Fateme</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jalal</forename><surname>Etesami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07578</idno>
		<title level="m">Confounded budgeted causal bandits</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inequality constraints in causal models with hidden variables</title>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06</title>
		<meeting>the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-optimal learning of causal graphs</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1875" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Experimental design for learning causal graphs with latent variables</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Algorithms for multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.6028</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal bandits: Learning good interventions via causal inference</title>
		<author>
			<persName><forename type="first">Finnian</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bandit algorithms</title>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bandit Algorithms</title>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108571401</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural causal bandits: Where to intervene</title>
		<author>
			<persName><forename type="first">Sanghack</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>271bc0ecb776a094786474322cb82-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural causal bandits with non-manipulable variables</title>
		<author>
			<persName><forename type="first">Sanghack</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33014164</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/4320" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4164" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regret analysis of bandit problems with causal background knowledge</title>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Meisami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal bandits with unknown graph structure</title>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Meisami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24817" to="24828" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient reinforcement learning with prior causal knowledge</title>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Meisami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="526" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A causal bandit approach to learning good atomic interventions in presence of unobserved confounders</title>
		<author>
			<persName><forename type="first">Aurghya</forename><surname>Maiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sinha</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Additive causal bandits with unknown graph</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23574" to="23589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The sample complexity of exploration in the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="623" to="648" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Budgeted and non-budgeted causal bandits</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakha</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sinha</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v130/nair21a.html" />
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">April 13-15, 2021. 2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond logarithmic bounds in online learning</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="823" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02732</idno>
		<title level="m">On lower bounds for regret in reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Logic and the Foundations of Mathematics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="789" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying best interventions through online importance sampling</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3057" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual bandits with latent confounders: An nmf approach</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="518" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning causal graphs with small interventions</title>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Introduction to multi-armed bandits</title>
		<author>
			<persName><forename type="first">Aleksandrs</forename><surname>Slivkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="286" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Clark N Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><surname>Heckerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Causal contextual bandits with targeted interventions</title>
		<author>
			<persName><forename type="first">Chandrasekar</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the testable implications of causal models with hidden variables</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eighteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Causal bandits for linear structural equation models</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12764</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Approximate allocation matching for structural causal bandits with unobserved confounders</title>
		<author>
			<persName><forename type="first">Lai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Qasim Elahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combinatorial pure exploration of causal bandits</title>
		<author>
			<persName><forename type="first">Nuoya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Causal bandits with propagating inference</title>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Yabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Hatano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naonori</forename><surname>Kakimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuro</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5512" to="5520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Causal bandits with general causal models and interventions</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Katz-Rogozhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.00233</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Designing optimal dynamic treatment regimes: A causal reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/zhang20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
