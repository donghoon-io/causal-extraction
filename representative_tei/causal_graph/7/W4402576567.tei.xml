<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Causal Discovery with Background Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-15">15 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingyuan</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LMAM</orgName>
								<orgName type="department" key="dep2">School of Mathematical Sciences</orgName>
								<orgName type="department" key="dep3">Center of Statistical Science</orgName>
								<orgName type="institution">LMEQF</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Applied Statistics and School of Statistics</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LMAM</orgName>
								<orgName type="department" key="dep2">School of Mathematical Sciences</orgName>
								<orgName type="department" key="dep3">Center of Statistical Science</orgName>
								<orgName type="institution">LMEQF</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Causal Discovery with Background Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-15">15 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.07890v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal relationship</term>
					<term>Background knowledge</term>
					<term>Causal DAG models</term>
					<term>Local learning method</term>
					<term>Maximally partially directed acyclic graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causality plays a pivotal role in various fields of study. Based on the framework of causal graphical models, previous works have proposed identifying whether a variable is a cause or non-cause of a target in every Markov equivalent graph solely by learning a local structure. However, the presence of prior knowledge, often represented as a partially known causal graph, is common in many causal modeling applications. Leveraging this prior knowledge allows for the further identification of causal relationships. In this paper, we first propose a method for learning the local structure using all types of causal background knowledge, including direct causal information, non-ancestral information and ancestral information. Then we introduce criteria for identifying causal relationships based solely on the local structure in the presence of prior knowledge. We also apply out method to fair machine learning, and experiments involving local structure learning, causal relationship identification, and fair machine learning demonstrate that our method is both effective and efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causality is crucial in artificial intelligence research. Recent work emphasizes that causal models should be applied in artificial intelligence systems to support explanation and understanding <ref type="bibr" target="#b26">(Lake et al., 2017)</ref>. Specifically, to achieve fairness in machine learning algorithms, some approaches propose excluding all variables influenced by the sensitive variable from the set of predictors <ref type="bibr" target="#b55">(Wu et al., 2019;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>. This highlights the importance of understanding the causal relationships between other variables and the sensitive variable.</p><p>Randomized experiments are the gold standard for identifying causal relationships <ref type="bibr" target="#b43">(Robins et al., 2000)</ref>. However, randomized trials may be impractical due to cost, ethical considerations, or infeasibility for non-manipulable variables. As a result, inferring causal relationships from observational data has gained increasing attention <ref type="bibr" target="#b10">(Cooper, 1997;</ref><ref type="bibr" target="#b12">Cox, 2018)</ref>. Causal relationships among multiple variables in observational data can be compactly represented using causal directed acyclic graphs (DAGs) <ref type="bibr" target="#b36">(Pearl, 1995;</ref><ref type="bibr" target="#b48">Spirtes et al., 2000;</ref><ref type="bibr" target="#b18">Geng et al., 2019)</ref>. In a causal DAG, a treatment variable is a cause of an outcome variable if and only if there exists a directed path from treatment variable to outcome variable <ref type="bibr" target="#b38">(Pearl, 2009)</ref>. Therefore, if the underlying DAG is known, causal relationships between any two variables can be judged directly. However, without additional distributional assumptions, we can only identify a Markov equivalence class (MEC) from observational data <ref type="bibr" target="#b6">(Chickering, 2002)</ref>. An MEC is a set of DAGs where each DAG encodes the same conditional independencies.</p><p>The DAGs in the same MEC may encode different causal relationships. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, G 2 and G 3 are two Markov equivalent DAGs. However, A is a cause of B in G 3 , whereas A is not a cause of B in G 2 . Typically, we define the causal relationship between two nodes X and Y within a set of DAGs, denoted as S, as follows <ref type="bibr">(Fang et al., 2022a;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>:</p><formula xml:id="formula_0">• X is a definite cause of Y in S if X is a cause of Y in every DAG G ∈ S; • X is a definite non-cause of Y in S if X is not a cause of Y in every DAG G ∈ S;</formula><p>• X is a possible cause of Y in S otherwise; that is, X is a cause of Y in some DAGs in S, and X is not a cause of Y in others.</p><p>Respectively, Y is called a definite descendant, definite non-descendant, or possible descendant of X.</p><p>We can see that if |S| = 1, X is either a definite cause or a definite non-cause of Y for every pair (X, Y ), and causal relationships are clearly identified. When |S| becomes larger, the causal relationships become more uncertain. Therefore, if we can restrict the underlying DAG to a small set, we can obtain a better understanding of the causal relationships.</p><p>In practice, there often exists some prior knowledge about the causal structure, which encodes causal relationship among observed variables <ref type="bibr" target="#b46">(Sinha and Ramsey, 2021)</ref>. For example, in clinical trials, prior knowledge is often available from previous research or expert opinions <ref type="bibr" target="#b21">(Hasan and Gani, 2022)</ref>. In algorithm fairness research, some works assume that the sensitive attribute set is closed under ancestral relationships <ref type="bibr" target="#b25">(Kusner et al., 2017;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>. By applying this prior knowledge, the MEC can be refined to a smaller set of DAGs, enabling more precise inference about causal relationships.</p><p>An MEC can be uniquely represented by a complete partially directed acyclic graph (CPDAG) <ref type="bibr" target="#b32">(Meek, 1995;</ref><ref type="bibr" target="#b1">Andersson et al., 1997;</ref><ref type="bibr" target="#b48">Spirtes et al., 2000;</ref><ref type="bibr" target="#b6">Chickering, 2002)</ref>. By applying some type of prior knowledge, such as pairwise direct causal relationships, tiered orderings, or non-ancestral knowledge, the refined set of MEC can be represented by a maximally partially directed acyclic graph (MPDAG) <ref type="bibr" target="#b32">(Meek, 1995;</ref><ref type="bibr" target="#b13">Fang and He, 2020;</ref><ref type="bibr" target="#b15">Fang et al., 2022b;</ref><ref type="bibr" target="#b3">Bang and Didelez, 2023)</ref> <ref type="foot" target="#foot_0">foot_0</ref> . In this way, prior knowledge can be seen as a partially known part of the underlying DAG. For example, Figure <ref type="figure" target="#fig_0">1</ref> shows three Markov equivalent DAGs, G 1 -G 3 , and their corresponding CPDAG, G * . The MEC is</p><formula xml:id="formula_1">[G * ] = {G 1 , G 2 , G 3 }. Since X is a cause of Y in all DAGs, X is a definite cause of Y in [G * ].</formula><p>On the other hand, A is a cause of B in G 3 but not in G 1 and G 2 . If A → X is known as prior knowledge, the MEC is restricted to {G 3 }, making A a definite cause of B.</p><p>In recent years, several studies have investigated the issue of identifying causal relationships <ref type="bibr" target="#b44">(Roumpelaki et al., 2016;</ref><ref type="bibr" target="#b33">Mooij and Claassen, 2020;</ref><ref type="bibr">Fang et al., 2022a;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>. Among them, <ref type="bibr">Fang et al. (2022a)</ref> proposed a method for determining causal relationships in CPDAG through a local learning approach. However, this method cannot be directly applied to equivalence classes constrained by background knowledge. <ref type="bibr" target="#b60">Zuo et al. (2022)</ref> proposed a method for determining causal relationships in MPDAGs. Nevertheless, the performance of this method is constrained by the estimation errors inherent in learning the entire MPDAG.</p><p>In this paper, we consider the identification of causal relationships in the refined set of MEC represented by an MPDAG. We assume no selection bias or presence of unmeasured confounders, consistent with other causal discovery methods <ref type="bibr" target="#b30">(Liu et al., 2020b;</ref><ref type="bibr">Fang et al., 2022a;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>. Unlike existing method <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>, we propose a local method for identifying causal relationships in an MPDAG. This approach requires only the local structure around treatment variable and some additional conditional independencies for identifying of causal relationships. To achieve this, we introduce an algorithm for learning the local structure in an MPDAG, based on the MB-by-MB algorithm <ref type="bibr" target="#b52">(Wang et al., 2014;</ref><ref type="bibr" target="#b56">Xie et al., 2024)</ref>.</p><p>The remainder of this paper is organized as follows. After reviewing existing methods in Section 2 and introducing basic concepts in Section 3, we propose a sound and complete algorithm in Section 4 for learning the local structure around the treatment variable in an MPDAG. In Section 5, we introduce criteria for judging causal relationships between the treatment and outcome given the local structure of the treatment variable. In Section 6, we test our method and compare it with existing methods by experiments involving local structure learning, causal relationship identification, and fair machine learning. Additionally, We apply our method to a real-world dataset in Section 7.</p><p>Our main contributions are summarized as follows:</p><p>• We propose a sound and complete algorithm for learning the local structure around any given variable in an MPDAG.</p><p>• We provide sufficient and necessary conditions for causal relationships in the equivalence class represented by an MPDAG. These conditions can be assessed solely based on the local structure of the treatment variable and some additional conditional independencies.</p><p>• Based on these conditions, we propose a sound and complete algorithm for identifying all definite descendants, definite non-descendants, and possible descendants of the treatment variable locally in an MPDAG.</p><p>• Our experiments demonstrate the efficiency and efficacy of our algorithm. It shows superior performance in experiments related to local structure learning, causal relationship identification, and fair machine learning. We also apply our method to a real-world dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works and Methods</head><p>Local structure learning. Local structure learning refers to finding the parent-children (PC) set of a target variable and distinguishing between the parent and child identities of nodes in the PC set. Multiple algorithms have been proposed for learning local structure in a CPDAG <ref type="bibr" target="#b57">(Yin et al., 2008;</ref><ref type="bibr" target="#b52">Wang et al., 2014;</ref><ref type="bibr" target="#b17">Gao and Ji, 2015;</ref><ref type="bibr" target="#b30">Liu et al., 2020b;</ref><ref type="bibr" target="#b20">Gupta et al., 2023)</ref>. Among them, the PCD-by-PCD <ref type="bibr" target="#b57">(Yin et al., 2008)</ref>, MB-by-MB <ref type="bibr" target="#b52">(Wang et al., 2014)</ref> and CMB algorithms <ref type="bibr" target="#b17">(Gao and Ji, 2015)</ref> sequentially find a local set, such as the PC set or the Markov blanket (MB) <ref type="bibr">(Tsamardinos et al., 2003b,a;</ref><ref type="bibr" target="#b0">Aliferis et al., 2003;</ref><ref type="bibr" target="#b39">Pena et al., 2007)</ref>, for nodes connected to the target node. They then find v-structures in this local set to orient edges in the local structure of the target node. This process continues until all edges around the target node are oriented or it is determined that further orientation is not possible. The LDECC algorithm <ref type="bibr" target="#b20">(Gupta et al., 2023)</ref> incorporates additional conditional independence tests into the PC algorithm, which may help in learning the local structure of the target variable more efficiently. Recently, <ref type="bibr" target="#b30">Liu et al. (2020b)</ref> extended the MB-by-MB algorithm to learn the chain component containing the target node and the directed edges around it in a CPDAG.</p><p>To obtain the local structure around the target node in an MPDAG, we can first use the aforementioned algorithms to learn the local structure in the corresponding CPDAG, then add background knowledge and use Meek's rules <ref type="bibr" target="#b32">(Meek, 1995)</ref> for edge orientation. However, since Meek's rules are applied on the entire graph, the completeness of such a method is not guaranteed. Additionally, the background knowledge is not exploited in this local structure learning process. In this paper, we propose a sound and complete algorithm based on the MBby-MB algorithm to learn the local structure in an MPDAG, which utilizes the background knowledge during the learning process.</p><p>Causal relationship identification. To judge causal relationship between a treatment variable X and an outcome variable Y in a set S of DAGs represented by an MPDAG G * , an intuitive method is to enumerate all DAGs in S <ref type="bibr" target="#b53">(Wienöbst et al., 2023)</ref>, and judge the causal relationship in each DAG. However, this is generally computationally inefficient since the size of S is large in most cases <ref type="bibr" target="#b23">(He et al., 2015;</ref><ref type="bibr" target="#b45">Sharma, 2023)</ref>.</p><p>Another way involves estimating the causal effect of X on Y. For almost every distribution faithful to the underlying DAG, X is a cause of Y if and only if the causal effect of X on Y is not zero. However, equivalent DAGs may encode different causal effects. <ref type="bibr" target="#b40">Perkovic (2020)</ref> proposed a sufficient and necessary graphical condition for checking whether the causal effect is identifiable in an MPDAG, i.e., all DAGs represented by an MPDAG encode the same causal effect. If the condition is met, we can estimate the causal effect using the causal identification formula given by <ref type="bibr" target="#b40">Perkovic (2020)</ref> and judge the causal relation.</p><p>If the causal effect is not identified, an alternative approach involves enumerating all possible causal effects. X is a definite cause (or definite non-cause) of Y if and only if all possible causal effects of X on Y are non-zero (or zero). This approach can be implemented through the intervention calculus when the DAG is absent (IDA) method without listing all equivalent DAGs <ref type="bibr" target="#b31">(Maathuis et al., 2009;</ref><ref type="bibr" target="#b41">Perković et al., 2017;</ref><ref type="bibr" target="#b35">Nandy et al., 2017;</ref><ref type="bibr" target="#b13">Fang and He, 2020;</ref><ref type="bibr">Liu et al., 2020a,b;</ref><ref type="bibr" target="#b54">Witte et al., 2020;</ref><ref type="bibr" target="#b19">Guo and Perkovic, 2021)</ref>. Intuitively, the IDA method enumerates all possible local structures around the treatment variable. Each local structure represents a set of DAGs, which encode the same causal effect. However, the computational complexity of IDA methods is exponential to the number of undirected edges incident to X on a possibly causal path from X to Y <ref type="bibr" target="#b19">(Guo and Perkovic, 2021)</ref>, making it time-consuming in the worse case. <ref type="bibr">Fang et al. (2022a)</ref> proposed a local method for identifying causal relations in a CPDAG. Their method focuses on learning a compact subgraph surrounding the target node and utilizes conditional independence tests to infer causal relations. However, directly applying their findings to MPDAGs is limited, as a CPDAG can be considered a special case of an MPDAG when the prior knowledge does not offer additional insights beyond observational data.</p><p>Recently, <ref type="bibr" target="#b60">Zuo et al. (2022)</ref> proposed a sufficient and necessary graphical condition for judging causal relations in an MPDAG. However, judging this condition requires enumerating paths that satisfy certain conditions in an MPDAG.</p><p>Therefore, their method relies on a fully learned MPDAG. In contrast, we propose a fully local method for identifying causal relations in an MPDAG.</p><p>B and applying <ref type="bibr">Meek's rules ((Meek, 1995)</ref>, also see A), we obtain a partially directed acyclic graph G * , called the maximally partially directed acyclic graph (MPDAG) of [C, B] <ref type="bibr" target="#b15">(Fang et al., 2022b)</ref>. Let [G * ] denote the set of DAGs in [C] which is consistent with B. It has been shown that an edge in G * is directed if and only if it has the same orientation in every DAG in [G * ] <ref type="bibr" target="#b32">(Meek, 1995)</ref></p><formula xml:id="formula_2">. A path p = ⟨V 1 , . . . , V k ⟩ is b-possibly causal in G * if no edge V i ← V j exists in G * for 1 ≤ i &lt; j ≤ k.</formula><p>A path p is chordless if any of its two non-consecutive nodes are not adjacent. For distinct nodes X and Y in G * , the critical set of X with respect to Y in G * consists of all adjacent vertices of X lying on at least one chordless b-possibly causal path from X to Y.</p><p>In a partially directed graph G = (V, E), which could be a DAG, CPDAG or MPDAG, we use ch(X,</p><formula xml:id="formula_3">G) = {Y ∈ V|X → Y in G}, pa(X, G) = {Y ∈ V|Y → X in G}, sib(X, G) = {Y ∈ V|X -Y in G} to denote the children, parents and siblings of a node X in G. Let adj(X, G) = pa(X, G)∪ch(X, G)∪sib(X, G</formula><p>) denote the set of nodes adjacent with X in G. For a set of nodes W ⊆ V, let G W denote the induced subgraph of G over W, containing all and only the edges in G that are between nodes in W. For a set of nodes If the true MPDAG G * can be learned from D and B, it has been shown that the causal relationship between any pair of nodes (X, Y ) can be judged by examining the critical set of X with respect to Y in G * .</p><formula xml:id="formula_4">Q ⊆ V, we say Q is a clique if every pair of nodes in Q are adjacent in G. A clique Q is maximal if for any clique Q ′ , Q ⊆ Q ′ implies Q = Q ′ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem formulation and existing result</head><p>Lemma 1. ( <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>, Theorem 4.5) Let X and Y be two distinct vertices in an MPDAG G * , and C be the critical set of</p><formula xml:id="formula_5">X with respect to Y in G * . Then Y is a definite descendant of X in [G * ] if and only if either C∩ch(X, G * ) ̸ = ∅, or C is non-empty and induces an incomplete subgraph of G * .</formula><p>However, learning the entire MPDAG is time-consuming and may introduce more errors <ref type="bibr" target="#b5">(Chickering, 1996;</ref><ref type="bibr" target="#b7">Chickering et al., 2004)</ref>. In this paper, we propose a method to judge the causal relationship between X and Y in [G * ] using only the local structure around X, specifically the tuple (pa(X, G * ), ch(X, G * ), sib(X, G * )) and the skeleton of G * sib(X,G * ) , thus avoiding the extra cost and potential errors associated with learning the entire MPDAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Local causal structure learning with background knowledge</head><p>Before presenting the theorems for identifying causal relations, we must address an important question: can we learn the local structure around X without learning the entire MPDAG? In this section, we provide an affirmative answer to this question. In Section 4.1, we first provide an algorithm to learn the local structure when background knowledge consists of only direct causal information. After that, in Section 4.2, we incorporate non-ancestral information and ancestral information, and propose algorithms learning the local structure when background knowledge consists of direct causal information and non-ancestral information, or when all three types of background knowledge are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Local structure learning with direct causal information</head><p>We first introduce the MB-by-MB in MPDAG algorithm to find the local structure when background knowledge consists of only direct causal information. This algorithm is an extension of the MB-by-MB algorithm, which discovers the local structure of X in the corresponding CPDAG <ref type="bibr" target="#b52">(Wang et al., 2014;</ref><ref type="bibr" target="#b30">Liu et al., 2020b;</ref><ref type="bibr" target="#b56">Xie et al., 2024)</ref>.</p><p>The main procedure of the algorithm in summarized in Algorithm 1. Let X be the target node in an MPDAG G * . We want to learn pa(X, G * ), ch(X, G * ), sib(X, G * ) and the skeleton of G * sib(X,G * ) from observational data D and background knowledge B consisting of direct causal information. In the learning procedure, we maintain a set of conditional independencies called IndSet. We use (a, b, S ab ) ∈ IndSet for two distinct nodes a, b and a node set S ab to denote that a, b are conditional independent given S ab .</p><p>For the beginning of the algorithm, we construct a partially directed graph G with edges only from background knowledge B. In the learning process, we gradually add undirected or directed edges into G, which are guaranteed to be true edges in G * . In Steps 4 to 16, we iteratively consider each node Z which is connected with X by an undirected path in G.</p><p>For each Z in the iteration, we find the Markov blanket of Z at Step 6. Let MB(Z) denote the Markov blanket of Z, which is the smallest set such that Z ⊥ V \ MB(Z) | MB(Z), and let MB + (Z) = MB(Z) ∪ {Z}. Multiple methods have been proposed for finding the Markov blanket of a given node, such as IAMB <ref type="bibr" target="#b50">(Tsamardinos et al., 2003b)</ref>, MMMB <ref type="bibr">(Tsamardinos et al., 2003a)</ref>, HITON-MB <ref type="bibr" target="#b0">(Aliferis et al., 2003)</ref>, PCMB and KIAMB <ref type="bibr" target="#b39">(Pena et al., 2007)</ref>. We choose one of such methods to find MB(Z) from observational data D.</p><p>After that, we find the marginal graph L Z over MB + (Z) at Step 7. The marginal graph L Z over MB + (Z) is defined as a DAG that the marginal distribution P MB + (Z) over MB + (Z) is Markovian and faithful to L Z . Namely, for any distinct subset X ′ , Y ′ , Z ′ of MB + (Z), X ′ and Y ′ are d-separated by Z ′ in L Z if and only if X ′ and Y ′ are conditional independent given Z ′ . Therefore, Algorithm 1: MB-by-MB in MPDAG: find local causal structure of a certain node in MPDAG G * .</p><p>Input : A target X, observational data D, background knowledge B. Output : A PDAG G containing the local structure of X.</p><p>1 Initialize DoneList = ∅ (list of nodes whose MBs have been found); 2 Initialize WaitList = {X} (list of nodes whose MBs will be found);  Put the edges connected to Z and the v-structures containing Z in LZ to G. we can learn the CPDAG corresponding to L Z by causal discovery algorithms such as the IC algorithm <ref type="bibr" target="#b37">(Pearl, 2000)</ref> or PC algorithm <ref type="bibr" target="#b48">(Spirtes et al., 2000)</ref>. On the other hand, if MB + (Z) ⊆ MB + (Z ′ ) for some Z ′ ∈ DoneList, we directly set L Z = (L Z ′ ) MB + (Z) ; if MB(Z) ⊆ DoneList, we set L Z be the subgraph of G over MB + (Z). While finding MB and learning the marginal graph, we also collect discovered conditional independencies in IndSet. Those conditional independencies can be used to check whether an edge does not exist in G * .</p><formula xml:id="formula_6">3 Initialize G = (V, B) (initial</formula><formula xml:id="formula_7">9 repeat 10 For (a → b -c) ∈ G, if (a, c, Sac) ∈ IndSet and b ∈ Sac, orient b → c. 11 For (a → b → c -a) ∈ G, orient a → c. 12 For a -b, a -c → b and a -d → b ∈ G, if (c, d, S cd ) ∈ IndSet and a ∈ S cd , orient a → b. 13 For a -b, a -c → b and a -d → c ∈ G, if (b,</formula><p>At Step 8, we put the edges connected to Z and the v-structures containing Z in L Z to G. These edges are guaranteed to be existing also in G * (Theorem 1,2 in <ref type="bibr" target="#b52">(Wang et al., 2014)</ref>). Then we update the orientation of edges in G by Meek's rules in Step 9 to 14. At Step 15, we update WaitList to be the nodes connected with X by an undirected path in G which is not considered previously. In this way, nodes are dynamically added into or removed from WaitList as new undirected edges are added into G or existing undirected edges in G are oriented.</p><p>The following theorem shows that we can obtain the local structure around X by running Algorithm 1: </p><formula xml:id="formula_8">Theorem 1. Let G *</formula><formula xml:id="formula_9">, G) = pa(Z, G * ), ch(Z, G) = ch(Z, G * ), sib(Z, G) = sib(Z, G * ).</formula><p>Theorem 1 shows that some critical properties for local structure of X are covered by the output of Algorithm 1. The parents, children and siblings of X and each sibling of X are identified. Therefore, local methods such as IDA <ref type="bibr" target="#b13">(Fang and He, 2020)</ref> can be applied to estimate the treatment effect of X on another vertice Y, which needs to check whether orienting the siblings of X forms a new v-structure or directed triangle. Our method in Section 5 also needs identification of pa(X, G * ), ch(X, G * ), sib(X, G * ) and adj(Z, G * ) for each Z ∈ sib(X, G * ). Actually, the output of Algorithm 1 covers the B-component containing X, which is defined in <ref type="bibr" target="#b15">(Fang et al., 2022b)</ref>, and all directed edges around the Bcomponent. That is similar to Corollary 4 in <ref type="bibr" target="#b30">(Liu et al., 2020b)</ref>.</p><p>As an alternative baseline method, we can first learn a part of the CPDAG C corresponding to G, and infer the orientation of other edges from background knowledge and Meek's rules. Let ChComp(X) be a partially directed graph consisting with the largest undirected subgraph of C containing X, which is also called the chain component containing X, and all directed edges connected with the chain component. We can learn ChComp(X) by Algorithm 3 in <ref type="bibr" target="#b30">(Liu et al., 2020b)</ref>. For each background knowledge A → B in B, we either orient A → B if A -B in ChComp(X) or otherwise directly add this edge to ChComp(X). Then we apply Meek's rules and get the local structure around X.</p><p>In the following theorem, we show that Algorithm 1 outperforms the baseline method in the worst case: Theorem 2. Suppose all conditional independencies are correctly checked. Let (V 1 , V 2 , . . . , V p ) be the nodes that are sequentially considered in Step 4-16 of Algorithm 1. If they are considered in the same order in the baseline method<ref type="foot" target="#foot_2">foot_2</ref> , the number of conditional independence test used by Algorithm 1 is not larger than the baseline method.</p><p>We then show how Algorithm 1 is performed by a running example. A variant of this example in C shows that in an extreme situation, we may need more conditional independence test comparing to the baseline method. See Section 6.1 for experimental comparison between these two methods.</p><p>Example 1. Consider the DAG G shown in Figure <ref type="figure" target="#fig_16">2a</ref>. We want to learn the local structure around X. Suppose first that we do not have any background knowledge about G. In this case, Algorithm 1 degenerates to the MB-by-MB algorithm. Firstly, we find the MB of X, which is {Y, Z}, and learn the marginal graph over {X, Y, Z}. Since there is no conditional independence over them, we remain edges Y -X -Z and add them into G, shown in Figure <ref type="figure" target="#fig_16">2b</ref>. Now both Y and Z are connected with X by an undirected path in G. We then find the MB of Y, which is {A, B, C, X, Z}. By performing conditional independence test among these nodes, we find that A ⊥ C|B, which implies that A → Y ← C is a v-structure. By Meek's rules, we orient Y → X in G. That gives Figure <ref type="figure" target="#fig_16">2c</ref>, and the only node connected with X by an undirected path is Z. After finding the MB of Z, we know that A and C are both adjacent to Z, so A ⊥ C|B implies that A → Z ← C is a v-structure, and we can orient Z → X by Meek's rules, giving Figure <ref type="figure" target="#fig_16">2d</ref>. Therefore, we conclude that the parents of X in G * is {Y, Z}, and it has no sibling or children. Now suppose that background knowledge B = {A → Y } is known in prior. After finding MB(X) = {Y, Z}, we know that A and X are not adjacent, so we can orient Y → X by Meek's rules, giving Figure <ref type="figure" target="#fig_16">2e</ref>. Then we find the MB of Z and learn the marginal graph over MB + (Z). We orient A → Z ← C after finding A ⊥ C|B and orient Z → X by Meek's rules, giving Figure <ref type="figure" target="#fig_16">2f</ref>.</p><formula xml:id="formula_10">(a) G (b) G (X) (c) G (X,Y ) (d) G (X,Y,Z) (e) G A→Y (X) (f) G A→Y (X,Z)</formula><p>The baseline method first follows the procedure of MB-by-MB algorithm, which also gives Figure <ref type="figure" target="#fig_16">2d</ref>. Then it applies background knowledge A → Y, which is already learned in previous steps, and gives Figure <ref type="figure" target="#fig_16">2d</ref> as the final output. We can see that, Algorithm 1 explores less nodes than the baseline method in this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local structure learning with non-ancestral or ancestral information</head><p>Non-ancestral information and ancestral information are characterized as orientation of edges between a set of nodes <ref type="bibr" target="#b15">(Fang et al., 2022b)</ref>.</p><formula xml:id="formula_11">For distinct nodes X, Y in an MPDAG G * with true underlying DAG G, let C denote the critical set of X respect to Y in G * . It has been shown that, X is not an ancestor of Y in G if and only if for all C ∈ C, C → X in G. On the other hand, X is an ancestor of Y if and only if there exists C ∈ C such that X → C in G.</formula><p>Therefore, non-ancestral information can be equivalently transformed into a set of direct edges in the MPDAG, while ancestral information may not be fully represented by an MPDAG, but an MPDAG with a set of direct causal clauses (DCC). For example, X or → C denotes the fact that there exists C ∈ C such that X → C. However, the definition of critical set is based on the entire MPDAG: in order to check whether a node adjacent to X is in the critical set of X with respect to Y in G * , we need to list all chordless b-possibly causal paths from X to Y in G * . Proposition 4.7 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref> proposed that we can list all b-possibly causal path of definite status from X to Y, but it still needs to learn the entire MPDAG. Instead, we want to find a local characterization of the critical set. The following lemma relates the IDA framework with the critical set.</p><formula xml:id="formula_12">Lemma 2. Let G * be an MPDAG and X, Y be two distinct nodes in G * . Suppose that X is not a definite cause of Y in G * . Let C be the critical set of X with respect to Y in G * . Let Q be the set of all Q ⊆ sib(X, G * ) such that orienting Q → X and X → sib(X, G * ) \ Q does not introduce any v-structure collided on X or any directed triangle containing X. Then we have ∩ {Q ∈ Q | X ⊥ Y | pa(X, G * ) ∪ Q} = an(C, G * ) ∩ sib(X, G * ).</formula><p>Lemma 2 shows that we can identify the ancestor of the critical set locally. If background knowledge shows that X is not a cause of Y, it is equivalent to C → X, and by acyclic constraint it implies an(C, G * ) ∩ sib(X, G * ) → X. This fact inspires Algorithm 2, which learns the local structure around X when background knowledge consists of direct causal information</p><formula xml:id="formula_13">B 1 = {(F i , T i )} k1 i=1</formula><p>where it is known that F i → T i exists in G for i = 1, 2, . . . , k 1 , and non-ancestral relationship B 2 = {(N j , T j )} k2 j=1 , where it is known that N j is not a cause of T j in G for j = 1, 2, . . . , k 2 .</p><p>In Algorithm 2, we first learn the local structure G under direct causal information B 1 using Algorithm 1. Let G * 1 denote the MPDAG under background knowledge B 1 . By Theorem 1, G recovers the B-component including X in G * 1 . For each (N j , T j ) ∈ B 2 , if N j and X are not connected by an undirected path in G, N j and X are not in the same B-component in G * 1 , so orienting edges around N j do not affect the local structure around X. On the other hand, if N j and X are connected by an undirected path in G, they are in the same B-component in G * 1 . Therefore, by Theorem 1, the local structure around the output of Algorithm 2 with input X, D, B. Then for each Z connected with X by an undirected path in G * , including X itself, we have pa</p><formula xml:id="formula_14">N j in G * 1 is identical to the local structure around N j in G. By Lemma 2, Step 4 to 9 in Algorithm 2 finds an(C, G * ) ∩ sib(X, G * ),</formula><formula xml:id="formula_15">(Z, G) = pa(Z, G * ), ch(Z, G) = ch(Z, G * ), sib(Z, G) = sib(Z, G * ).</formula><p>Then we consider ancestral information as background knowledge. Based on an learned MPDAG G * with true underlying DAG G ∈</p><formula xml:id="formula_16">[G * ], if there is extra information that F k is an ancestor of T k in G, equivalently we have T k or → C, where C is the critical set of T k with respect to F k in G * .</formula><p>Unfortunately, finding an(C, G * )∩sib(T k , G * ) is not enough in this case, since there is no extra information from the acyclic constraint. However, by Lemma 1 in <ref type="bibr" target="#b32">Meek (1995)</ref>, there is no partially directed cycle in CPDAGs. So when G * is a CPDAG, we have an(C, G * ) ∩ sib(X, G * ) = C. Therefore, in this case, we modify our algorithm to first learn a local structure of the CPDAG using MB-by-MB algorithm. Then we transform ancestral information into direct causal clauses by Lemma 2, and last consider direct causal information and non-ancestral information and update the local structure. The main procedure of the algorithm is given in Algorithm 5 in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Local characterization of causal relations in maximal PDAG</head><p>We now present the criteria for identifying all definite descendants, possible descendants, and definite non-descendants of the target node X in the MPDAG G * . These criteria rely only on the local structure learned by Algorithm 1 and some additional conditional independence tests. Before presenting the theorems, we classify definite causal relationships into explicit and implicit causes, similar to the CPDAG version in <ref type="bibr">(Fang et al., 2022a)</ref>.</p><formula xml:id="formula_17">Definition 1. Let X, Y be two distinct nodes in an MPDAG G * such that X is a definite cause of Y . We say X is an explicit cause of Y if there is a common causal path from X to Y in every DAG in [G * ] , otherwise X is an implicit cause of Y.</formula><p>In other words, X is an explicit cause of Y if and only if X ∈ an(Y, G * ). For more details on explicit and implicit causal relationships, refer to Section 3 of <ref type="bibr">(Fang et al., 2022a)</ref>.</p><p>The first theorem gives a sufficient and necessary condition for any node to be a definite non-descendant of X.</p><p>Theorem 4. For any two distinct nodes X and <ref type="bibr">ritzen et al., 1990;</ref><ref type="bibr" target="#b27">Lauritzen, 1996)</ref>. The key difference is that any node in sib(X, G * ) could also be a parent of X in G <ref type="bibr" target="#b32">(Meek, 1995)</ref>. Notably, the set pa(X, G * ) is always a subset of pa(X, G) for every G ∈ [G * ]. Intuitively, if a smaller set blocks all paths from X to Y, a stronger non-ancestral relation from X to Y is established. This intuition also applies to the next two theorems, which judge explicit cause and implicit cause relationships.</p><formula xml:id="formula_18">Y in an MPDAG G * , Y is a definite non-descendant of X if and only if X ⊥ Y | pa(X, G * ) holds. The condition in Theorem 4, X ⊥ Y | pa(X, G * ), is analogous to the Markov property X ⊥ Y | pa(X, G) for any non-descendant Y of X in a DAG G (Lau-</formula><p>Theorem 5. For any two distinct nodes X and Y in an MPDAG G * , X is an explicit cause of Y if and only if</p><formula xml:id="formula_19">X ̸ ⊥ Y | pa(X, G * ) ∪ sib(X, G * ) holds.</formula><p>The condition in Theorem 5, X ̸ ⊥ Y | pa(X, G * ) ∪ sib(X, G * ), is similar to the local Markov property in chain graphs <ref type="bibr" target="#b16">(Frydenberg, 1990)</ref>. However, an MPDAG is not generally a chain graph, as MPDAGs may contain partially directed cycles, which are not allowed in chain graphs. Therefore, Theorem 5 is not a straightforward extension of its CPDAG version. It can also serve as a technical lemma for deriving graphical properties in MPDAGs. Theorem 6. For any two distinct nodes X and Y in an MPDAG G * , let Q be the set of maximal cliques of the induced subgraph of G * over sib(X, G * ). Then,</p><formula xml:id="formula_20">X is an implicit cause of Y if and only if X ⊥ Y |pa(X, G * ) ∪ sib(X, G * ) and X ̸ ⊥ Y |pa(X, G * ) ∪ Q for every Q ∈ Q.</formula><p>In Theorem 6, we need to find all maximal cliques in the induced subgraph of G * over sib(X, G * ). By Theorem 1, Algorithm 1 can correctly discover the parents, children and sibling for each node in sib(X, G * ), thus fully recovering the induced subgraph of G * over sib(X, G * ).</p><p>The proof of Theorem 6 is based on Lemma 21 in B, which shows that for any maximal clique Q in Q, there exists a DAG G ∈ [G * ] such that pa(X, G) = Algorithm 3: LABITER: Local algorithm for finding all definite descendants, definite non-descendants and possible descendants of X in G * .</p><p>Input : A target X, observational data D, background knowledge B. Output : The set of definite descendants, definite non-descendants and possible descendants of pa(X, G * )∪Q. In other words, any maximal clique in Q can serve as an additional parent set of X. This lemma is fundamental in IDA theory. Let R be the set that includes all sets of nodes R such that there exists a DAG G ∈ [G * ] with pa(X, G) = pa(X, G * ) ∪ R. Lemma 21 divides R into two parts: Q, the set of maximal cliques of the induced subgraph of G * over sib(X, G * ), and the set of all cliques R that are not maximal but for which no R ∈ R and C ∈ sib(X, G * )\R exist such that C → R in G * <ref type="bibr" target="#b13">(Fang and He, 2020)</ref>.</p><formula xml:id="formula_21">X in the MPDAG G * . 1 Learn a local structure G by Algorithm 1. 2 Let Q be the set of maximal cliques of the induced subgraph of G over sib(X, G). 3 Initialize DefDes, DefNonDes, PosDes = ∅, ∅, ∅ 4 for Y ∈ V \ {X} do 5 if X ⊥ Y |pa(X, G) then 6 Add Y to DefNonDes; 7 else if X ̸ ⊥ Y |pa(X, G) ∪ sib(X, G) or X ̸ ⊥ Y |pa(X, G) ∪ Q for every Q ∈ Q.</formula><p>We can combine these two conditions for judging the definite cause relationship.</p><p>Corollary 1. For any two distinct nodes X and Y in an MPDAG G * , let Q be the set of maximal cliques of the induced subgraph of G * over sib(X, G</p><formula xml:id="formula_22">* ). Then X is an definite cause of Y if and only if X ̸ ⊥ Y |pa(X, G * ) ∪ sib(X, G * ) or X ̸ ⊥ Y |pa(X, G * ) ∪ Q for every Q ∈ Q.</formula><p>Based on these conditions, we can finally present an algorithm to identify all definite descendants, definite non-descendants and possible descendants of X, given data D and background knowledge B. This algorithm is shown in Algorithm 3. We call this Algorithm by LABITER (Local Algorithm with Background knowledge for Identifying the Type of causal Relations). The following theorem, directly derived from Theorem 4 and Corollary 1, verifies the correctness of LABITER.</p><p>Theorem 7. Suppose all conditional independencies are correctly checked, then Algorithm 3 correctly returns the definite descendants, definite non-descendants and possible descendants of X in [G * ] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Learning local structure in MPDAG</head><p>In this section, we conduct a simulation study to test Algorithm 1. The MBby-MB algorithm learns a chain component containing the target node and edges around it <ref type="bibr" target="#b30">(Liu et al., 2020b)</ref>, and it has been shown to outperforms other algorithms when learning local structure in sparse networks <ref type="bibr" target="#b52">(Wang et al., 2014)</ref>. Therefore, we compare the performance of Algorithm 1 with the MB-by-MB and PC algorithms when learning local structure in a single chain component.</p><p>In sparse DAGs, the maximal size of chain component is relatively small even in large graphs <ref type="bibr" target="#b22">(He et al., 2013)</ref>. A chain component with size 10 is larger than 95% of chain components in a DAG with 200 nodes and 300 edges <ref type="bibr" target="#b22">(He et al., 2013)</ref>. In our experiment, the size of chain components n is chosen from {5, 10, 15}. For each graph, we randomly select some edges as background knowledge and run three algorithms: MB-by-MB in MPDAG (Algorithm 1), the original MB-by-MB algorithm implemented on the CPDAG <ref type="bibr" target="#b52">(Wang et al., 2014)</ref>, and the PC algorithm <ref type="bibr" target="#b47">(Spirtes and Glymour, 1991)</ref>. The proportion of background knowledge in the undirected edges ranges from 0.1 to 0.9.</p><p>We first use an oracle conditional independence test, implemented via dseparation in the true DAG, and collect the number of conditional independence tests used by each algorithm. The number of conditional independence tests indicates the efficiency of each algorithm without considering the estimation error. The experiment is repeated 500 times, and the results are plotted in Figure <ref type="figure" target="#fig_6">3</ref>. Then, we draw N = 2000 samples from a linear Gaussian model faithful to the true DAG, where weights are randomly drawn from a uniform distribution between [0.6, 1.2]. We compare the structural Hamming distance (SHD) between the learned structure and the true local structure to evaluate the efficacy of each algorithm. The experiment is repeated for 5000 times, and the results are plotted in Figure <ref type="figure" target="#fig_7">4</ref>.</p><p>Figure <ref type="figure" target="#fig_6">3</ref> shows the ratio of the number of conditional independence test required by Algorithm 1 to that required by the comparison method. We can see that, as the amount of background knowledge increases, the number of conditional independence tests required by the algorithm indeed decreases. Since we focus on a single chain component instead of the whole DAG, the performance of the MB-by-MB algorithm is similar to the PC algorithm. When background knowledge is added, the number of nodes connected to the target node by an undirected path in the MPDAG decreases. Therefore, Algorithm 1 only needs to learn a smaller local structure, leading to fewer conditional independence tests.</p><p>Figure <ref type="figure" target="#fig_7">4</ref> shows the SHD between the learned local structure and the true local structure around the target node in the MPDAG for each algorithm. All algorithms perform better as the proportion of background knowledge increases,  since more edges around the target node are given as background knowledge.</p><p>Nevertheless, the MB-by-MB in MPDAG algorithm (Algorithm 1) outperforms other algorithms when more edges are known in prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Identifying causal relations</head><p>Next, we conduct an experiment to identify causal relations in an MPDAG.</p><p>We sample random DAGs G with n ∈ {50, 100} nodes and average degrees d ∈ {1.5, 2, 2.5, 3, 3.5, 4}. A sample of size N ∈ {100, 200, 500, 1000} is drawn from a linear Gaussian model faithful to G, with weights randomly drawn from a uniform distribution between [0.6, 1.2]. Thirty percent of the edges in G are chosen as background knowledge. We compare the performance of three algorithms for identifying causal relations: LABITER (Algorithm 3), local ITC (Algorithm 1 in <ref type="bibr">Fang et al. (2022a)</ref>) and Algorithm 2 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, which first learns the whole MPDAG and then identifies causal relations based on Lemma 1. In each experiment, three possible causal relations between two randomly sampled nodes, X and Y , are considered: X is a definite cause, a definite noncause, or a possible cause of Y. Let I ij (k) be the indicator that in the k-th experiment, the true causal relation is the i-th one and the output is the j-th one, where 1 ≤ i, j ≤ 3. The experiment is repeated for m = 5000 times. We use the Kappa coefficient <ref type="bibr" target="#b9">(Cohen, 1960)</ref> as an metric for evaluating the learning of causal relations, which is also used in <ref type="bibr">(Fang et al., 2022a)</ref>. Denote M ij = m k=1 I ij (k), and</p><formula xml:id="formula_23">p = 3 i=1 M ii 3 i=1 3 j=1 M ij , q = 3 i=1 3 j=1 M ij • 3 j=1 M ji 3 i=1 3 j=1 M ij 2 ,</formula><p>then the Kappa coefficient is given by κ = p -q 1 -q .</p><p>A higher value of κ implies better performance for classification. We plot the kappa coefficient for three algorithms in Figure <ref type="figure">5</ref> From the results shown in Figure <ref type="figure">5</ref>, it is evident that our algorithm (LABITER) outperforms the other algorithms in most cases. Local ITC does not utilize the additional information from background knowledge, which could further determine some possible cause relationships as definite cause or definite non-cause relationships. Although Algorithm 2 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref> incorporates background knowledge, its performance is constrained by the accuracy of CPDAG learning. In contrast, our method not only leverages background knowledge but also reduces errors through local graph learning, resulting in superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Counterfactual fairness</head><p>We also run an experiment on counterfactual fairness, similar to the synthetic data experiment in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>, comparing our method with Algorithm 2 in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>. We randomly sample DAGs with n ∈ {10, 20, 30, 40} nodes with 2n edges. Data are generated from a linear Gaussian model, with each variable uniformly discretized based on its sorted order. The sensitive variable A and the target variable Y are randomly selected. The range of all variables, except the binary sensitive variable A, are randomly sampled from the ranges of variables in the UCI Student Performance Dataset <ref type="bibr" target="#b11">(Cortez and Silva, 2008)</ref>. In generating counterfactual data, A is set to the other level while other variables are generated normally. Let Ŷ (a) and Ŷ (a ′ ) represent the estimates of Y in the observed and counterfactual data, respectively. Unfairness is measured as | Ŷ (a) -Ŷ (a ′ )|. According to <ref type="bibr" target="#b25">Kusner et al. (2017)</ref>, Ŷ is counterfactually fair if it depends only on the non-descendants of A. To reduce unfairness, we should therefore exclude the descendants of A from the prediction model.</p><p>We consider five baseline models as in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>. 1) Full uses all variables except Y to predict Y. 2) Unaware uses all variables except X, Y to predict Y. 3) Oracle uses all true non-descendants of X to predict Y. 4) FairRelax first learns the MPDAG, then use Lemma 1 to identify causal relations, and uses all definite non-descendants and possible descendants of X to predict Y. 5) Fair also identify causal relations as FairRelax does, and uses all definite non-descendants of X to predict Y. We propose two methods: LFairRelax uses LABITER to identify causal relations and uses all definite non-descendants and possible descendants of X to predict Y, and LFair uses LABITER to identify causal relations and uses all definite non-descendants of X to predict Y. We draw N = 1000 samples in each experiment and use all methods to obtain Ŷ from observational data. The experiment is repeated for m = 100 times. We collect the RMSE on the test set and unfairness for Ŷ for each method, and show them in Table <ref type="table" target="#tab_3">1</ref>. We can see that as RMSE is similar across different methods, out method can achieve a lower unfairness.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Application</head><p>We apply our method to the bank marketing dataset<ref type="foot" target="#foot_4">foot_4</ref>  <ref type="bibr" target="#b34">(Moro et al., 2014)</ref>. The Bank dataset pertains to direct marketing campaigns conducted by a Portuguese banking institution, primarily through phone calls. These campaigns often required multiple contacts with the same client to determine if they would subscribe to a bank term deposit. The dataset's classification goal is to predict whether a client will subscribe to a term deposit, indicated by the binary variable y. Existing studies have recognized marital status as a protected attribute <ref type="bibr" target="#b8">(Chierichetti et al., 2017;</ref><ref type="bibr" target="#b2">Backurs et al., 2019;</ref><ref type="bibr" target="#b4">Bera et al., 2019;</ref><ref type="bibr" target="#b24">Hu et al., 2020;</ref><ref type="bibr" target="#b59">Ziko et al., 2021)</ref>. Therefore, to predict y under the premise of counterfactual fairness, we should exclude descendant variables of marital status from the predictive variables. Due to the absence of counterfactual data to measure unfairness, we merely identify causal relationships between other variables and marital status and provide discussion accordingly.</p><p>Among the variables in the dataset, age is not influenced by other variables, which we consider as background knowledge. Under this premise, we use Algorithm 1 to learn the local structure around marital, with results shown in Figure <ref type="figure" target="#fig_10">6</ref>. After that, we apply two algorithms, LABITER (Algorithm 3) and Algorithm 2 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, to find definite descendants and definite non-descendants of marital status, with results shown in Table <ref type="table" target="#tab_5">2</ref>. Note that both algorithms do not identify any variable as a possible descendant of marital status.</p><p>We can see that, LABITER identifies job, month, campaign and pdays as definite descendants of marital status, while Algorithm 2 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref> identifies them as definite non-descendants. Specifically, it is plausible to argue that marital status can significantly influence one's occupation or job type. Therefore, our algorithm's recognition of job as a descendant of marital status is more in line with real-world causal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>In this paper, we propose a local learning approach for finding definite descendants, possible descendants and definite non-descendants of a certain variable in an MPDAG. We first propose an algorithm learning the local structure around any variable in the MPDAG. The algorithm find the adjacent nodes of the variable, distinguish them by parents, children, and siblings in the MPDAG, and provide similar results for its sibling nodes. Based on the local structure, we propose a criterion that identifies the definite descendants, definite non-descendants and possible descendants of the target variable. Different from similar results in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref> which needs to learn the whole MPDAG and check paths on it, our criterion only need the local structure and some conditional independent tests. Moreover, our criterion can further distinguish definite cause relationship into explicit cause and implicit cause relationship, defined as whether the causal path between two nodes remains the same for every DAG represented by the MPDAG.</p><p>Our work has made some progress in the theoretical understanding of MPDAG. Combining with previous studies <ref type="bibr" target="#b13">(Fang and He, 2020;</ref><ref type="bibr" target="#b60">Zuo et al., 2022)</ref>, it shows that even if MPDAG is not chain graph in general, most results for CPDAG can also be extended to MPDAG. MPDAG has a wider use than CPDAG in practice, especially for scenarios where most part of causal graph is known as prior from expert knowledge. Our results for judging causal relations can also be applied to fairness machine learning, which alleviates the issue of estimation errors for learning causal graphs by learning a local structure around the target variable.</p><p>A possible direction for future work is to consider unmeasured confounding variables and selection bias, which can be characterized using partial ancestral graphs <ref type="bibr" target="#b42">(Richardson and Spirtes, 2003;</ref><ref type="bibr" target="#b58">Zhang, 2008)</ref>. Recently, <ref type="bibr" target="#b56">Xie et al. (2024)</ref> proposed an algorithm to learn the local structure in a partial ancestral graph. Based on that algorithm, there may be similar criteria for identification of causal from Y to X, undirected edge between X and Y, directed path from X to Y , directed path from Y to X). They are adjacent if there is a directed or undirected edges between X and Y . We use the convention that each node is an ancestor and descendant of itself. The set of parents, children, siblings, ancestors, descendants and adjacent nodes of X are denoted as pa(X, G), ch(X, G), sib(X, G), an(X, G), de(X, G), adj(X, G). A collider is a triple (X, Y, Z) where X → Y ← Z, and it is also called a v-structure if X and Z are not adjacent. In a path Under a set of background knowledge B consisting of direct causal information with form V i → V j , the MEC is restricted to a smaller set which contains Markov equivalent DAGs that are consistent with background knowledge B, that is, all background knowledge V i → V j are correctly oriented in all DAGs in this set. Just like CPDAG, this restricted set can be represented by a partially directed graph G * , which is called the maximally partially directed acyclic graph (MPDAG) of G with background knowledge B.  Proof. Suppose such A, B, C in p exists, write p = p(L, A)⊕(A, B, C)⊕p(C, R), where L ∈ X and R ∈ Y. We claim that q = p(L, A) ⊕ (A, C) ⊕ p(C, R) is also d-connecting given S, which leads to a contradiction with the fact that p is the shortest. It is clear that other nodes except A, C on q have the same collider/non-collider status on q and p, so we just need to consider A, C on q. Since B is a collider on p and p is d-connecting given S, we have B ∈ an(S, G) and hence A ∈ an(S, G) by A → B in G. Therefore, if A is a collider on q, it cannot block q. Since A is a non-collider on p and p is d-connecting given S, we have A ̸ ∈ S, so if A is a non-collider on q, it also cannot block q. The same reasoning can be applied to conclude that C cannot block q. Therefore, q is d-connecting given S, which leads to a contradiction. Figure <ref type="figure" target="#fig_14">7</ref> shows how lemmas fit together to prove Theorem 1. Theorem 1 directly follows from Lemma 7,8,11. Then we proof the lemmas shown in this structure. The first two lemmas show the soundness of Algorithm 1.</p><formula xml:id="formula_24">p, V i is a collider if V i-1 → V i ← V i+1 , otherwise V i is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head><formula xml:id="formula_25">* . Let X, Y be two distinct nodes in G * , then X → Y ∈ G * if and only if X → Y ∈ G for any G ∈ [G * ], and X -Y ∈ G * if and only if there exists two DAGs G 1 , G 2 ∈ [G * ] such that X → Y ∈ G 1 and X ← Y ∈ G 2 . Lemma 4. (Lemma B.1 in (Perković et al., 2017)) Let p = ⟨V 1 , V 2 , . . . , V k ⟩ be a b-possibly causal definite path in a MPDAG M. If there is i ∈ {1, 2, . . . , n -1} such that V i → V i+1 in M, then p(V i , V k ) is causal in M. Lemma 5. Let X, Y be two distinct nodes in an MPDAG G * . If X ∈ an(Y, G * ) and X, Y are adjacent, then X → Y ∈ G * . Proof. Let G be any DAG in [G * ] By Lemma 3, X ∈ an(Y, G) and X, Y are adjacent in G. If X ← Y ∈ G, it</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof for Theorem 1</head><p>Lemma 7. Suppose the joint distribution is markovian and faithful with the underlying DAG , and all independence tests are correct, then the MB-by-MB in MPDAG algorithm can correctly discover the edges which are connected to each node in DoneList without their orientation.</p><p>Proof. According to Theorem 1 in <ref type="bibr" target="#b52">(Wang et al., 2014)</ref>, for each node X in DoneList, with a correct MB(X) such that X ⊥ (V \ MB + (X))|MB(X), for any Y ∈ V, Y and X are d-separated by a subset of V if and only if Y and X are d-separated by a subset of MB(X). So with conditional independence set learned by observed data of MB + (X), whether X and Y are connected could be correctly discovered.</p><p>Lemma 8. Suppose the joint distribution is markovian and faithful with the underlying DAG , and all independence tests are correct, then each directed edge in the output graph of MB-by-MB in MPDAG algorithm is correct, i.e. they are also directed in the true MPDAG with the same direction.</p><p>Proof. Denote M to be the true MPDAG and G the output PDAG of the algorithm. We show that each directed edge in output G is correct, in the sense that it is also a directed edge in M and has the same orientation in G and M. There are three sources of directed edges in G: background knowledge, learned by a v-structure or oriented by one of four Meek rules. Since G is initialized by background knowledge, each directed edge from background knowledge in G is correct. According to Theorem 2 in <ref type="bibr" target="#b52">(Wang et al., 2014)</ref>, each directed edge learned by a v-structure is correct.</p><p>Consider a directed edge in G oriented by one of four Meek rules. By induction, we can assume the directed edges needed by a Meek rule is correct since they finally comes from a v-structure or a background knowledge. ( <ref type="formula">1</ref> Then we define the directed edge generation sequence of an MPDAG, which represents the order and way for each directed edge in an MPDAG to be oriented. As in the following definition, each directed edge is marked as one of the following labels: "V", "B", "R1", "R2", "R3", "R4", which respectively implies that this edge is oriented in a v-structure, by background knowledge, or by Meek's rules R1-R4.</p><p>The motivation of this definition is that we need to prove that all directed edges around the target node are discovered by Algorithm 1. Actually, this fact can be explained by simple words: since all directed edges are oriented by v-structure, background knowledge, or one of four Meek's rules, and all nodes connect with the target node by an undirected path are explored by the Algorithm, continuing the Algorithm for exploring other nodes, which are blocked from the target node by directed edges, cannot help to orient the direction for undirected edges. If the reader believes that these words proves that all directed edges around the target node are discovered, there is no need to read the following proof carefully.</p><p>The following lemmas lies in the intuition that if an directed edge in the MPDAG is not discovered in Algorithm 1, then another directed edge, which participated in the orientation of this edge, is neither discovered. By iteration, at last an directed edge which is in a v-structure in which all nodes are explored, or is itself a background knowledge, is not discovered, which conflicts with previous lemmas. Therefore, we need to define the "orientation procedure" of directed edges in an MPDAG. </p><formula xml:id="formula_26">S = ⟨(d 1 , p 1 , r 1 ), . . . , (d k , p k , r k )⟩. For each i = 1, 2, . . . , k, d i ∈ E d M is a directed edge in M and E d M = {d 1 , d 2 , . . . , d k }. p i</formula><p>is a set of directed edges, and r i is a label which takes one of these values: {"V ", "B", "R1", "R2", "R3", "R4"}.</p><p>The consistency of DEGS S with respect to an MPDAG M and a set of background knowledge B is defined as whether M can be recovered from its skeleton with its directed edges sequentially oriented by v-structure, background knowledge, or one of the four Meek's rules. The sequential orientation procedure is described by S. Definition 3 gives a strict description of this nature.</p><formula xml:id="formula_27">Definition 3. (Consistency of DEGS) Let M = (V, E M ) be an MPDAG with background knowledge B ⊂ E M . A DEGS S = ⟨(d 1 , p 1 , r 1 ), . . . , (d k , p k , r k )⟩ is consistent with M, B, if Algorithm 4 returns TRUE for input M, S, B.</formula><p>Lemma 9. For any MPDAG M = (V, E M ) with background knowledge B, there exists at least one DEGS S that is consistent with M, B.</p><p>Proof. From Proposition 1 in <ref type="bibr" target="#b15">(Fang et al., 2022b)</ref>, there exists an CPDAG C such that [M ] is the restricted set of [C] under background knowledge B. As defined in <ref type="bibr" target="#b32">(Meek, 1995)</ref>, the pattern Π of C is the partially directed graph which shares the same skeleton with C and has an directed edge if and only if that edge is in a v-structure. By Theorem 4 in <ref type="bibr" target="#b32">(Meek, 1995)</ref>, M can be obtained by applying Meek's rules R1-R4 and orienting edges according to B in Π.</p><p>Let S be a sequence of two parts. The first part contains all (d, p, r) such that d is a directed edge in Π, p = ∅ and r ="V". The second part is a sequence of (d, p, r) along with the procedure of applying Meek's rules R1-R4 and orienting edges according to B in Π. In each step orienting an edge d i , if d i is oriented according to B, let (d, p, r) = (d i , ∅, "B"). If d i is oriented by R1, then there exists a, b, c ∈ V such that a → b -c before orienting d i , d i = b → c, and a, c are not adjacent in M, then let (d, p, r) = (d i , {a → b}, "R1"). If d i is oriented by R2, then there exists a, b, c ∈ V such that a → b → c and a -c before orienting</p><formula xml:id="formula_28">d i = a → c, then let (d, p, r) = (d i , {a → b, b → c}, "R2"). If d i is oriented by R3, then there exists a, b, c, d ∈ V such that b -a -c, b → d ← c, a -d before orienting d i = a → d, then let (d, p, r) = (d i , {b → d, c → d}, "R3"). If d i is oriented by R4, then there exists a, b, c, d ∈ V such that b-a-c, c → d → b, a-d before orienting d i = a → b, then let (d, p, r) = (d i , {c → d}, "R4").</formula><p>It can be verified that Algorithm 4 returns TRUE by inputting M, S, B.</p><p>Lemma 10. Let M = (V, E M ) be an MPDAG with background knowledge B ⊂ E M . Let S = {S 1 , . . . , S q } be the set of all DEGS that is consistent with M, B. Proof. Since a → b ∈ M, by the definition of consistent DEGS there exists (d, p, r) ∈ S i such that d = a → b. We firstly prove that r ̸ ∈ {"V ", "B"}, so p ̸ = ∅. If r = "V ", there exists a v-structure in M which contains d, i.e. there exists c ∈ V such that a → b ← c ∈ M and a, c are not adjacent in M. However, since a and T are connected by a undirected path in G, by Algorithm 1 a is in DoneList, so the v-structures contains a as a parent node is correctly learned in G, so a → b ∈ G, which contradicts with the assumption. If r = "B", we know d ∈ B so a → b ∈ G, which contradicts with the assumption.</p><formula xml:id="formula_29">Let G = (V, E G ) be the output PDAG of Algorithm 1 with target T . Let a, b ∈ V be two distinct nodes which are connected to T with an undirected path in G. If a → b ∈ M but a -b ∈ G, then for any i = 1, 2, . . . , q, let (d, p, r) ∈ S i such that d = a → b, there exists d ′ = a ′ → b ′ ∈ p such that (a ′ , b ′ ) ̸ = (a, b), d ′ ∈ M, d ′ ̸ ∈ G,</formula><p>Then we talk about each case of r. (3) Only one of them is directed. Without loss of generality suppose the edge between (a, c)  </p><formula xml:id="formula_30">is directed in G, then d is in DoneList, so c, d is not adjacent in G, (c, d, S cd ) ∈ IndSet and a ∈ S cd . So a → c ∈ G, otherwise c → a ∈ G implies a → b ∈ G by R1, which contradicts with the supposition that a -b ∈ G. Then a → c, c → b ∈ G implies a → b ∈ G by R2, which contradicts with a -b ∈ G. Therefore, one of c → b, d → b is not in G,</formula><formula xml:id="formula_31">∈ G since d → b ∈ M. If a → d ∈ G, then a → b ∈ G by R2, which contradicts with a -b ∈ G. So d → a ∈ G. If c → a ∈ G, then a → b ∈ G by R1, which contradicts with a -b ∈ G. So a → c ∈ G. Since all directed edges in G are correct, we have d → a → c ∈ M,</formula><formula xml:id="formula_32">(5) If d -a -c ∈ G, then we have a → b ∈ G by R4, which contradicts with a -b ∈ G. So c -d ∈ G.</formula><p>Now since one of (c, d) is connected with T by an undirected path, both of them are connected with T by an undirected path. So c → d is the directed edge that satisfies the conclusion of lemma.</p><p>Lemma 11. Suppose the joint distribution is markovian and faithful with the underlying DAG , and all independence tests are correct, then the MB-by-MB in MPDAG algorithm can correctly discover all directed edges around target T.</p><p>Proof. Denote M to be the true MPDAG, G be the output PDAG of Algorithm 1, and B be the background knowledge. Let S be a consistent DEGS of M, B, which exists by Lemma 9. Assume, for the sake of contradiction, there exists an directed edge d ∈ M which is around T and d ̸ ∈ G. Let i be the index such that (d i , p i , r i ) ∈ S and d i = d. Then by Lemma 10, there exists d ′ ∈ p i such that d ′ ∈ M , d ′ ̸ ∈ G, and the head and tail node of d ′ are connected with T by an undirected path in G. Let j be the index such that (d j , p j , r j ) ∈ S and d j = d ′ . By the consistency of DEGS, we know j &lt; i, and by Lemma 10 there exists d ′′ ∈ p j such that d ′′ ∈ M, d ′′ ̸ ∈ G,and the head and tail node of d ′′ are connected with T by an undirected path in G. Repeat that procedure, it will lead to a contradiction because by the consistency of DEGS, p 1 = ∅.</p><p>Restatement of Theorem 1. Suppose the joint distribution is markovian and faithful with the underlying DAG, and all independence tests are correct, then the MB-by-MB in MPDAG algorithm can correctly discover the edges connected to the target T. Further for each edge connected to T, if it is directed in MPDAG, then it is correctly oriented in the output of the algorithm, otherwise it remains undirected in the output of the algorithm. The same holds for all nodes connected with T by an undirected path in the MPDAG.</p><p>Proof. The conclusion for target node T holds directly from Lemma 7, Lemma 8 and Lemma 11. For another node T ′ which is connected with T by an undirected path in the MPDAG, note that Algorithm 1 gives the same output with T ′ as the target node. So the conclusion also holds for T ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof for Theorem 2</head><p>Proof. It suffices to prove that each independence test needed by Algorithm 1 is also needed in the baseline method.</p><p>Let (V 1 , V 2 , . . . , V p ) be the nodes that are sequentially considered in Algorithm 1. That is, node Z popped in Line 5 of Algorithm 1 is iteratively V 1 , V 2 , . . . , V p .</p><p>We first show that it is valid for the baseline method to explore these nodes in the same order. Let T denote the target node, so clearly V 1 = T. By induction, we only need to show that after exploring V i in both algorithms, T and V i+1 are connected by an undirected path. After exploring (V 1 , V 2 , . . . , V i ), each undirected edge in the local structure G is only added by Line 8 in Algorithm 1, which is connected with V j for some 1 ≤ j ≤ i. Therefore, this undirected edge is also added to G in the baseline method. Moreover, with background knowledge B and applying Meek's rules, we can only add directed edges or orient undirected edges into directed edges. Therefore, any undirected edge after exploring (V 1 , . . . , V i ) in Algorithm 1 also exists and is undirected in the baseline method. Since T and V i+1 are connected by an undirected path in Algorithm 1, they are also connected by an undirected path in the baseline method.</p><p>Then we consider conditional independence tests used by Algorithm 1. All conditional independence tests are used in learning the marginal graph over MB + (Z) for each Z ∈ (V 1 , V 2 , . . . , V p ), as shown in Line 7 in Algorithm 1. While learning the marginal graph, we need to check whether two nodes are dseparated by a subset of other nodes. This procedure is not affected by learned there exists G ∈ [G * ] such that the induced subgraph of G over V i is identical to G i for all i = 1, 2, . . . , k. For the sake of contradiction, we first suppose that there is a v-structure A → B ← C in G not included in G * . Then both edges between A, B and B, C are undirected in G * . Otherwise, without loss of generality, suppose A → B -C in G * , then by Meek's rule 1 we have</p><formula xml:id="formula_33">A → B → C in G * , leading to a contradiction. Since A -B -C in G * , A, B, C are in the same B-component G * i . Then we have A → B ← C is a v-structure in G i , which contradicts with the fact that G i ∈ [G * i ].</formula><p>Then we assume that there is a directed cycle p in G. If there is an directed edge in p which is not in any G i , then it is also directed in the chain skeleton of G * . So p is partially directed in the chain skeleton of G * , which contradicts with the fact that the chain skeleton of G * is a chain graph by Theorem 1 in <ref type="bibr" target="#b15">Fang et al. (2022b)</ref>. Since every edge between two B-components does not belong to any B-component, all nodes in p is in the same B-component G * i . Then p is a directed cycle in G i , which contradicts with the fact that</p><formula xml:id="formula_34">G i ∈ [G * i ].</formula><p>The converse of Theorem 8 is trivial.</p><p>Lemma 12. Let G * be an MPDAG. Let G * 1 , G * 2 , . . . , G * k be all B-components of</p><formula xml:id="formula_35">G * with vertices V 1 , V 2 , . . . , V k . For every G ∈ [G * ], i = 1, 2, . . . , k, let G i denote the induced subgraph of G over V i , then G i ∈ [G * i ]. Proof. Since G ∈ [G * ]</formula><p>, all directed edges in G * have the same direction in G, and G and G * have the same skeleton and v-structures. So all directed edges in G * i have the same direction in G i , and G i and G * i have the same skeleton and v-structures. Clearly G i has no directed cycle. So</p><formula xml:id="formula_36">G i ∈ [G * i ].</formula><p>Combining Theorem 8 and Lemma 12, we can construct an bijection from the MEC to the Cartesian product of MECs for each B-component.</p><formula xml:id="formula_37">Definition 4. Let G * be an MPDAG and G * 1 , . . . , G * k be all B-components of G * with vertices V 1 , . . . , V k . The decomposition map of G * is defined as a map ϕ G * from [G * ] to × k i=1 [G * i ] that for each G ∈ [G * ], ϕ G * (G) = (G 1 , G 2 , . . . , G k ), where G i is the induced subgraph of G over V i , i = 1, 2, . . . , k.</formula><p>Corollary 3. Every decomposition map is a bijection.</p><p>The next theorem shows that orientation of edges outside a B-component does not affect the orientation inside the B-component.</p><p>Finally we show (iii). Let A -B be an undirected edge in G * 2 . By the construction of B 21 , every undirected edge in B 21 is also undirected in B 11 . So every undirected edge in G * 2 is also undirected in</p><formula xml:id="formula_38">G * 1 . Therefore, A -B is undirected in G * 1 . Let G 1 , G 2 be two DAGs in [G * 1 ] such that A → B in G 1 and A ← B in G 2 . Denote (G 11 , G 12 , . . . , G 1k ) = ϕ G * 1 (G 1 ) and (G 21 , G 22 , . . . , G 2k ) = ϕ G * 1 (G 2 ). Since A -B is undirected in G * 1 , both A, B are in V p for some 1 ≤ p ≤ k. If p &gt; 1, let G 31 be a DAG in [B 21 ] over vertice set V 1 . Let G 3 = ϕ -1 G * 1 (G 31 , G 12 , G 13 , . . . , G 1k ) and G 4 = ϕ -1 G * 1 (G 31 , G 22 , G 23 , . . . , G 2k ). Then by Theorem 8, G 3 , G 4 ∈ [G * 1 ]. Since the induced subgraph of both G 3 and G 4 over V 1 are G 31 , which is in [B 21 ], we have X → Y in both G 3 and G 4 , so G 3 , G 4 ∈ S. Since p &gt; 1, by the construction of G 3 and G 4 we have A → B in G 3 and A ← B in G 4 . If p = 1, then A -B in B 21 . So there exists G 21 , G 31 in [B 21 ] over vertice set V 1 such that A → B in G 21 and A ← B in G 31 . Let G 1 be a DAG in G * 1 and denote (G 11 , G 12 , . . . , G 1k ) = ϕ G * 1 (G 1 ). Let G 2 = ϕ -1 G * 1 (G 21 , G 12 , G 13 , . . . , G 1k ) and G 3 = ϕ -1 G * 1 (G 31 , G 12 , G 13 , . . . , G 1k ). By Theorem 8, we have G 2 , G 3 ∈ [G * 1 ]. Moreover, since G 21 , G 31 ∈ [B 21 ], we have X → Y in both G 21 and G 31 , so X → Y in both G 2 and G 3 , so G 2 , G 3 ∈ S. Since A → B in G 21 and A ← B in G 31 , we have A → B in G 2 and A ← B in G 3 .</formula><p>Leveraging these theorems, we can give a proof for Theorem 3. Restatement of Theorem 3. Let G * be the MPDAG under background knowledge B consisting of direct causal information B 1 and non-ancestral information B 2 . Let G ∈ [G * ] be the true underlying DAG and D be i.i.d. observations generated from a distribution Markovian and faithful with respect to G. Let X be a target node in G. Suppose all conditional independencies are correctly checked, and let G be the output of Algorithm 2 with input X, D, B. Then for each Z connected with X by an undirected path in G * , including X itself, we have pa(Z, G) = pa(Z, G * ), ch(Z, G) = ch(Z, G * ), sib(Z, G) = sib(Z, G * ).</p><p>Proof. Let G * 0 be the MPDAG under observational data and background knowledge B 1 , and G 0 be the variable G after Step 1 in Algorithm 2, i.e., the output of Algorithm 1 under input X, D, B 1 . By Theorem 1, for each Z connected with X by an undirected path in G * 0 , including X itself, we have pa(Z, G</p><formula xml:id="formula_39">0 ) = pa(Z, G * 0 ), ch(Z, G 0 ) = ch(Z, G * 0 ), sib(Z, G 0 ) = sib(Z, G * 0 ). Write B 2 = {(N j , T j )} k</formula><p>j=1 , which implies that N j is not a cause of T j in the underlying DAG G for j = 1, 2, . . . , k. For each j = 1, 2, . . . , k, let G * j denote the MPDAG representing the restricted Markov equivalence class of [G * j-1 ] with a new background knowledge that N j is not a cause of T j . Let G j denote the PDAG G after the j-th iteration of Step 2 to Step 12 in Algorithm 2. By induction, it suffices to show that for each j = 1, 2, . . . , k, if for each Z connected with X by an undirected path in G</p><formula xml:id="formula_40">* j-1 , including X itself, we have pa(Z, G j-1 ) = pa(Z, G * j-1 ), ch(Z, G j-1 ) = ch(Z, G * j-1 ), sib(Z, G j-1 ) = sib(Z, G * j-1 ), then for each Z connected with X by an undirected path in G * j , including X itself, we have pa(Z, G j ) = pa(Z, G * j ), ch(Z, G j ) = ch(Z, G * j ), sib(Z, G j ) = sib(Z, G * j )</formula><p>. By symmetry, we only need to consider j = 1.</p><p>In the following lemmas, we prove the sufficiency of Theorem 5. Therefore, we assume that X is not an explicit cause of Y in G * , and we want to show that X ⊥ Y |pa(X, G * ) ∪ sib(X, G * ), which is equivalent to that any path π between X and Y are blocked by pa(X, G * ) ∪ sib(X, G * ). Let G be any DAG represented by G * . Lemma 14 considers the case that π is an edge between X and Y. Lemma 15 considers the case that π is causal in G with its second node in ch(X, G * ). Lemma 19 considers the case that π is non-causal in G with its second node in ch(X, G * ). Lemma 20 considers the case that the second node in π is in pa(X, G * ) ∪ sib(X, G * ). Combining all this lemmas, we can conclude that sufficiency of Theorem 5 holds.</p><p>Lemma 14. Let X, Y be two distinct nodes that are not adjacent in an MPDAG</p><formula xml:id="formula_41">G * . Suppose X is not an explicit cause of Y in G * . Let G by any DAG in [G * ].</formula><p>If X, Y are adjacent, then the path π as the edge between X and Y is blocked by pa</p><formula xml:id="formula_42">(X, G * ) ∪ sib(X, G * ). Proof. Since X is not an explicit cause of Y in G * , we have π = X ← Y or π = X -Y in G * , so Y ∈ pa(X, G * ) ∪ sib(X, G * ) and Y is a non-collider in π, so π is blocked by pa(X, G * ) ∪ sib(X, G * ). Lemma 15. Let X, Y be two distinct nodes that are not adjacent in an MPDAG G * . Suppose X is not an explicit cause of Y in G * . Let G by any DAG in [G * ]. If there exists a causal path π = ⟨X = v 0 , v 1 , . . . , v n , v n+1 = Y ⟩ in G from X to Y such that X → v 1 ∈ G * , then π is blocked by pa(X, G * ) ∪ sib(X, G * ). Proof. If there exists a node v k , 1 &lt; k ≤ n such that X, v k are adjacent, then by Corollary 2, X → v k ∈ G. So by Lemma 3, v k ̸ ∈ pa(X, G * ). If v k ∈ sib(X, G * ),</formula><p>π is blocked by pa(X, M ) ∪ sib(X, G * ) since v k is not a collider on π. Therefore, we only need to consider the case that for all such k that X and v k are adjacent, we have v k ∈ ch(X, G * ). Let 1 ≤ m ≤ n be the largest index that v m and X are adjacent, i.e. v m ∈ ch(X, G * ). Denote Y = v n+1 . For any i, j ∈ {m, m + 1, . . . , n, n + 1} and i + 1 &lt; j, if v i , v j are adjacent in G, by Corollary 2 we have</p><formula xml:id="formula_43">v i → v j ∈ G. Then the path π(v m , v i ) ⊕ ⟨v i , v j ⟩ ⊕ π(v j , Y</formula><p>) is also a causal path in G. Repeat this procedure until this causal path is unshielded, and denote it by p. Note that all nodes on p is not adjacent to X, so ⟨X, v m ⟩ ⊕ p is unshielded, hence of definite status in G * , and it is causal in G, So it is b-possibly causal in G * . So since v m ∈ ch(X, G * ) and by Lemma 4, it is causal in G * . That contradicts with that X is not an explicit cause of Y. Therefore, π is blocked by pa(X, G * ) ∪ sib(X, G * ).</p><p>Before proving the case that π is non-causal in G, we need some technical lemmas. The first lemma is trivial, which shows that in this case, π has at least one collider.</p><p>Lemma 16. Let X, Y be two distinct nodes that are not adjacent in an MPDAG</p><formula xml:id="formula_44">G * . Suppose X is not an explicit cause of Y in G * . Let G by any DAG in [G * ]. Suppose π = ⟨X = v 0 , v 1 , . . . , v n , v n+1 = Y ⟩ is a non-causal path in G from X to Y such that X → v 1 ∈ G * , then there exists 1 ≤ p ≤ n such that v p-1 → v p ← v p+1 is a collider on π.</formula><p>have the second node in ch(X, G * ). Since π ′ is blocked if it is causal in G, we only need to consider the case that it is non-causal in G. If the first collider in π ′ is shielded, we can repeat this procedure and get a path π ′′ shorter than π ′ , and we just need to prove π ′′ is blocked. At last, we just need to consider a path with its first collider unshielded.</p><p>The last technical lemma shows that we only need to consider the case that π(X, v p ) is causal in G * .</p><p>Lemma 18. Let X, Y be two distinct nodes that are not adjacent in an MPDAG</p><formula xml:id="formula_45">G * . Suppose X is not an explicit cause of Y in G * . Let G by any DAG in [G * ]. Suppose π = ⟨X = v 0 , v 1 , . . . , v n , v n+1 = Y ⟩ is a non-causal path in G from X to Y such that X → v 1 ∈ G * . Let v p-1 → v p ← v p+1</formula><p>be the collider on π which is closest to X. Suppose that v p-1 and v p+1 are not adjacent, and π is open given pa(X, G * ) ∪ sib(X, G * ). Let 1 ≤ k ≤ p be the largest index such that X and v k are adjacent, let s be a shortest subpath of π(v k , v p ) which is causal in G, and let t = ⟨X, v k ⟩ ⊕ s ⊕ π(v p , Y ). Then t is also a non-causal path in G with its second node in ch(X, G * ), and t is open given pa(X, G * ) ∪ sib(X, G * ).</p><formula xml:id="formula_46">Proof. Since v p-1 and v p+1 are not adjacent, v p-1 → v p ← v p+1 forms a v- structure, so v p-1 → v p ← v p+1 ∈ G * . If X and v p are adjacent, we have X → v p ∈ G by Corollary 2, so v p ̸ ∈ pa(X, G * ). If X -v p ∈ G * , since v p+1 → v p ∈ G * we know X and v p+1 are adjacent, otherwise the edge X -v p is oriented. If X → v p+1 ∈ G * , by Meek's rule R2 we have X → v p ∈ G * ,</formula><p>which leads to a contradiction. So v p+1 ∈ pa(X, G * ) ∪ sib(X, G * ), and then π is blocked by pa(X, G * ) ∪ sib(X, G * ) since v p+1 is a non-collider on π. It contradicts with the assumption. Therefore, if X and v p are adjacent, we can conclude that X → v p ∈ G * . For 1 &lt; i &lt; p, v i is a non-collider on π. Since π is open given pa(X, G * ) ∪ sib(X, G * ), we know v i ̸ ∈ pa(X, G * ) ∪ sib(X, G * ). Therefore, for any 1 &lt; i ≤ p, if X and v i are adjacent, we have X → v i ∈ G * .</p><p>Since X and v k are adjacent, we have X → v k ∈ G * . By construction of s and t, we know ⟨X, v k ⟩ ⊕ s is a chordless path in G * , and is causal in G, and v k ∈ ch(X, G * ). So by Lemma 4, ⟨X, v k ⟩ ⊕ s is causal in G * . Since π is open given pa(X, G * ) ∪ sib(X, G * ), and any node on t is a collider if and only if it is also a collider on π, t is open given pa(X, G * ) ∪ sib(X, G * ). Now we finally come to the lemma that shows sufficiency of Theorem 5 holds for the case that π is non-causal in G with its second node in ch(X, G * ).</p><p>Lemma 19. Let X, Y be two distinct nodes that are not adjacent in an MPDAG G * . Suppose X is not an explicit cause of Y in G * . Let G by any DAG in [G * ]. If there exists a non-causal path π = ⟨X = v 0 , v 1 , . . . , v n , v n+1 = Y ⟩ in G from X to Y such that X → v 1 ∈ G * , then π is blocked by pa(X, G * ) ∪ sib(X, G * ).</p><p>Proof. By Lemma 16, there exists 1 ≤ p ≤ n such that v p-1 → v p ← v p+1 is a collider on π. Let v p-1 → v p ← v p+1 be the collider on π which is closest to X. By Lemma 17, without loss of generality we can assume that v p-1 and v p+1 are not adjacent, so v p-1 → v p ← v p+1 forms a v-structure and we have Lemma 21. Suppose that G * is an MPDAG, X is a node in G * , and Q is a maximal clique in the induced subgraph of G * over sib(X, G * ). Then there exists a DAG G represented by G * , such that pa(X, G) = pa(X, G * ) ∪ Q and ch(X, G) = ch(X, G * ) ∪ sib(X, G * ) \ Q.</p><p>Proof. By Theorem 1 in <ref type="bibr" target="#b13">(Fang and He, 2020)</ref>, we just need to prove that there does not exist S ∈ Q and C ∈ adj(X, G * ) \ (Q ∪ pa(X, G * )) such that C → S. If such S, C exists, by Lemma B.1 in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>, Q∪{C} induces a complete subgraph of G * and C ∈ sib(X, G * )\Q. That means Q∪{C} induces a complete subgraph in the induced subgraph of G * over sib(X, G * ), which contradicts with that Q is maximal.</p><p>Then we prove Theorem 6:</p><p>Proof. Let C be the critical set of X with respect to Y in G * . Suppose that X is an implicit cause of Y, then by Theorem 5, X ⊥ Y |pa(X, G * ) ∪ sib(X, G * ). For any Q ∈ Q, from Theorem 4.5 in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref> we know that C \ Q ̸ = ∅, otherwise C induces a complete graph. So there is a chordless partially directed path in G * from X to Y, denoted by π = ⟨X, v 1 , v 2 , . . . , v n , Y ⟩, such that v 1 ̸ ∈ Q. Let G be a DAG represented by G * such that X → v 1 ∈ G. There is no collider on π in G, otherwise since π is chordless, that collider is a v-structure in G * , which contradicts with that π is partially directed in G * . Therefore, π is causal in G. Since π is chordless, v 2 , v 3 , . . . , v n , Y are not adjacent with X, so they are not in pa(X, G * ) ∪ Q. v 1 ̸ ∈ pa(X, G * ) since π is partially directed, and v 1 ̸ ∈ Q by the selection of π. So π is not blocked by pa(X, G * ) ∪ Q in G. Therefore, X ̸ ⊥ Y |pa(X, G * ) ∪ Q.</p><p>Conversely, suppose X ⊥ Y |pa(X, G * )∪sib(X, G * ) and X ̸ ⊥ Y |pa(X, G * )∪Q for any Q ∈ Q holds. Then by Theorem 5, X is not an explicit cause of Y. Assume, for the sake of contradiction, that X is not an implicit cause of Y. Then X is not a definite cause of Y. By Theorem 4.5 in <ref type="bibr" target="#b60">(Zuo et al., 2022)</ref>, C ∪ ch(X, G * ) = ∅, and either C = ∅ or C induces an complete subgraph of G. Both cases implies that there exists Q ∈ Q such that C ⊆ Q. We will show that X ⊥ Y |pa(X, G * ) ∪ Q, which leads to a contradiction. By Lemma 21, there exists a DAG G represented by G * such that pa(X, G) = pa(X, G * ) ∪ Q and ch(X, G) = ch(X, G * ) ∪ sib(X, G * ) \ Q. So C ⊂ pa(X, G). By Lemma 2 in <ref type="bibr" target="#b13">(Fang and He, 2020)</ref>, X ̸ ∈ an(Y, G). So by the Markov property, X ⊥ Y |pa(X, G * )∪Q. That contradicts with the assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Examples</head><p>In this section, we use a more complicated example to demonstrate that how Algorithm 1 is performed, and in which situation Algorithm 1 needs more conditional independence tests than the baseline method.</p><p>Suppose that the true DAG G is plotted by Figure <ref type="figure" target="#fig_22">9a</ref>. We want to learn the local structure around X. Firstly, assume that there is no prior knowledge. In Finally, we find the MB of Z, which contains all nodes except Z. By learning the marginal graph, we first find that all other nodes are adjacent with Z. As we found that A ⊥ C|B previously, we directly know that A → Z ← C is a v-structure in G. When finding MB(X) is previous steps, we have found that X ⊥ A|Y, Z. Therefore, we orient Z → X in Step 10 in Algorithm 1. After that, there is no undirected edge connected with X, so the algorithm returns and gives Figure <ref type="figure" target="#fig_22">9d</ref>. We find that the local structure around X is Y → X ← Z. Now suppose that A → Y is known as prior knowledge. In Algorithm 1, we first add this edge to an empty graph G. Then we find the MB of X, which is MB(X) = {Y, Z}. By learning the marginal graph over {X, Y, Z}, we find that X and Y, Z are adjacent. We also find that X ⊥ A|Y, Z. Therefore, we orient Y → X in Step 10 in Algorithm 1. Therefore, we have Figure <ref type="figure" target="#fig_22">9e</ref> as the current local structure.</p><p>Then we have WaitList = {Z}. We find that MB(Z) = V \ {Z}, so we need to learn the entire graph to find adj(Z, G) and v-structures including Z. The first step is to find adj(Z, G), which iterate over T ∈ V \ {Z} and tests whether Z ⊥ T | S for each S ⊆ V \ {Z, T }. This step is also done when there is no prior knowledge. However, we do not know A ⊥ C | B now, so we need to do the second step, that is, to test other conditional independencies over V, until we find that A ⊥ C | B, orient the v-structure A → Z ← C, and orient Z → X and Z → D i for 1 ≤ i ≤ k in Step 10 in Algorithm 1. Suppose that k is large, then we need more conditional independence tests to learn the local structure Y → X ← Z, comparing with the scenario without prior knowledge.</p><p>Although this example shows that in extreme situation we may use more conditional independence tests to learn the local structure with prior knowledge, experiments in Section 6.1 show that the presence of prior knowledge reduce the number of conditional independence test needed in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Algorithm for local structure learning with all types of background knowledge</head><p>By Theorem 3, background knowledge with direct causal information and nonancestral information can be utilized for learning local structure of any MPDAG by Algorithm 2. Ancestral information that F j is a cause of T j is equivalent to that there exists C ∈ C G * (F j , T j ) such that F j → C in the true underlying DAG G, where G * is an MPDAG representing a restricted Markov equivalence class [G * ] containing G, and C G * (F j , T j ) is the critical set of F j with respect to T j in G * . With theories of direct causal clause (DCC, <ref type="bibr" target="#b15">(Fang et al., 2022b</ref>)), we rewrite this fact as F j or → C G * (F j , T j ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experimental results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of three Markov equivalent DAGs G 1 -G 3 and their corresponding CPDAG G * . These Markov equivalent DAGs have the same edges despite their orientation and share the same v-structure A → Y ← B. The undirected edges A -X -B in G * indicate that these edges may have different orientations in the Markov equivalent DAGs.</figDesc><graphic coords="2,367.14,135.24,51.55,51.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Let</head><label></label><figDesc>D be a set of data over variables V, whose distribution is Markovian and faithful with respect to an underlying DAG G = {V, E}. Let B ⊆ E be a set of background knowledge. Based on D and B, there exists a unique MPDAG G * such that G ∈ [G * ] . Consider a target variable X ∈ V. As defined in Section 1, for any Y ∈ V, Y is a definite descendant (definite non-descendant, possible descendant) of X in [G * ] if Y is a descendant of X in all (none, some) DAGs within [G * ] . Our objective is to identify all definite descendants, definite nondescendants and possible descendants of X in [G * ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example for learning local structure. (a) The original DAG. (b)-(f) Learned structure G defined at Step 3 in Algorithm 1. G B D denote the learned graph after running Step 4-16 in Algorithm 1 for each node in D, with background knowledge B.</figDesc><graphic coords="11,137.09,135.32,51.55,61.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and we let them point to N j and update the local structure G in Step 10. The following theorem shows the correctness of Algorithm 2. Theorem 3. Let G * be the MPDAG under background knowledge B consisting of direct causal information B 1 and non-ancestral information B 2 . Let G ∈ [G * ] be the true underlying DAG and D be i.i.d. observations generated from a distribution Markovian and faithful with respect to G. Let X be a target node in G. Suppose all conditional independencies are correctly checked, and let G be Algorithm 2: Learning the local structure around X given background knowledge consisting of direct causal information B 1 and non-ancestral information B 2 . Input : Target node X, observational data D, background knowledge B = B1 ∪ B2. Output : A PDAG G containing the local structure of X. 1 Let G be the output of Algorithm 1 under input X, D, B1. 2 foreach (Nj, Tj) ∈ B2 do 3 if Nj and X are connected by an undirected path in G then 4 Let candC = sib(Nj, G). 5 foreach Q ⊆ sib(Nj, G) such that orienting Q → Nj does not introduce any v-structure collided on Nj or any directed triangle containing Nj do 6 if Nj ⊥ Tj | pa(Nj, G) ∪ Q then 7 Update candC := candC ∩ Q. candC → Nj and apply Meek's rules on G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>then 8 Add Y to DefDes; 9 else 10 Add Y to PosDes; 11 end 12 end 13 return DefDes, DefNonDes, PosDes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ratio of the number of conditional independence test required by MB-by-MB in MPDAG (Algorithm 1) to the comparison method (solid red line: MB-by-MB algorithm, dashed cyan line: PC algorithm) when learning local structure in a chain component.</figDesc><graphic coords="17,137.09,293.91,109.99,65.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average SHD between the learned local structure and true MPDAG around the target node. Solid red line: MB-by-MB in MPDAG (Algorithm 1); dashed green line: MB-by-MB algorithm; dotted blue line: PC algorithm.</figDesc><graphic coords="17,253.72,293.91,109.99,65.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 5: (a)-(d) The Kappa coefficients of different methods on random graphs with n nodes and N samples are drawn. (e)-(h) The average CPU time of different methods.</figDesc><graphic coords="18,140.94,225.35,75.62,56.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a)-(d) and the corresponding average CPU time in Figure 5(e)-(h). Results for n = 100 are shown in E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learned local structure from the Bank dataset. Marital status is directly affected by age, and it directly affects job, education, balance, personal loan, campaign (number of contacts performed during this campaign and for this client) and y.</figDesc><graphic coords="20,219.70,124.80,171.85,86.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>called a non-collider. A path p is open given node set S if none of its non-collider is in S and all its colliders have a descendant in S. Otherwise, p is blocked by S. Two nodes X, Y is d-separated by S if all paths between X and Y are blocked by S. Otherwise, they are d-connected given S. Let X ⊥ Y |S denote that X, Y are d-separated by S. Different DAGs may encode same d-separations. For example, X → Y and X ← Y both imply that X, Y are d-connected given the empty set. Two DAGs that encode the same d-separations are called to be Markov equivalent. Let G be a DAG. Let [G] denote the set of DAGs that are Markov equivalent with G, which is also called the Markov equivalence class (MEC) of G. The MEC [G] can be represented by a complete partially directed acyclic graph (CPDAG) C, which shares the same skeleton with G, and any edge in C is directed if and only if it has the same direction in all DAGs in [G].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>B. 1</head><label>1</label><figDesc>Existing results and lemmas Lemma 3. (Theorem 4 in Meek (1995)) Let G * be an MPDAG and [G * ] be the restricted Markov equivalence class represented by G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>will form a directed cycle, which contradicts with the definition of DAG. Therefore, X → Y ∈ G, and by Lemma 3 we haveX → Y ∈ G * . Corollary 2. Let X, Y be two distinct nodes in an DAG G. If X ∈ an(Y, G) and X, Y are adjacent, then X → Y ∈ G.Lemma 6. Let X, Y and S be disjoint node sets in a DAG G such that X ⊥ Y|S in G. Let p be the shortest d-connecting path between X and Y given S. Then there is no shielded collider on p. Namely, there does not exist A, B, C in p such that A → B ← C in G and A, C are adjacent in G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Proof structure of Theorem 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>) For rule R1, if a → b -c ∈ G, we know that a → b ∈ M , and b, c are adjacent in M. Since (a, c, S ac ) ∈ IndSet and b ∈ S ac , we know that a, c are not adjacent in M, and a → b ← c ̸ ∈ M. So b → c ∈ M by R1. (2) For rule R2, if a → b → c -a ∈ G, we know that a → b → c ∈ M , and a, c are adjacent in M. So a → c ∈ M by R2. (3) For rule R3, if a -b, a -c → b, a -d → b ∈ G, we know that c → b, d → b ∈ M, and (a, b), (a, c), (a, d) are adjacent pairs in M. Since (c, d, S cd ) ∈ IndSet and a ∈ S cd , we know that c, d are not adjacent in M, and c → a ← d ̸ ∈ M. Enumerate all structure about (c, a, d) in M : (3.1) a → c, then a → b by R2; (3.2) a → d, then a → b by R2; (3.3) c -a -d, then a → b by R3. So a → b ∈ M. (4) For rule R4, if a -b, a -c → b, a -d → c ∈ G, we know that d → c → b ∈ M, and (a, b), (a, c), (a, d) are adjacent pairs in M. Since (b, d, S bd ) ∈ IndSet and a ∈ S bd , we know that b, d are not adjacent in M and b → a ← d ̸ ∈ M. Enumerate all structure about (c, a, d) in M : (4.1) c → a, then d → a by R2 and a → b by R1; (4.2) a → c, then a → b by R2; (4.3) c -a -d, then a → b by R4; (4.4) c -a → d, which cannot exist since a → c by R2; (4.5) c -a ← d, then a → b by R1. So a → b ∈ M. Until now we verified that all directed edges oriented by one of four Meek rules in MB-by-MB algorithm is a correct directed edge in M. By induction we know all directed edges in G is correct in M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Definition 2 .</head><label>2</label><figDesc>(Directed edge generation sequence(DEGS) of an MPDAG) Let M = (V, E M ) be an MPDAG, and let E d M be its directed edges and k = |E d M |. A directed edge generation sequence of an MPDAG M is a sequence of tuples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>and a ′ , b ′ are connected to T with an undirected path in G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>If r = "R1", there exists c ∈ V such that c → a → b ∈ M, and c, b are not adjacent in M, and p = {c → a}. Since a, b is in DoneList, the adjacency of a, b in M is the same in G, so c, b are not adjacent in G, and (c, b, S cb ) ∈ IndSet, a ∈ S cb . Moreover, a, c are adjacent in G. If c → a ∈ G, then a → b ∈ G by R1, which contradicts with a -b ∈ G, so c -a ∈ G. So d ′ = c → a ∈ p satisfies the conclusion of lemma. If r = "R2", there exists c ∈ V such that a → c → b ∈ M, and p = {a → c, c → b}. Since a, b are in DoneList, their adjacency are correctly learned so (a, c), (c, b) are adjacent pairs in G. If a → c → b ∈ G, then a → b ∈ G by R2, which contradicts with a -b ∈ G, so one of them must be undirected in G. The undirected one satisfies the conclusion of lemma. If r = "R3", there exists c, d ∈ V such that c → b ← d ∈ M, (a, c), (a, d) are adjacent pairs in M, c, d are not adjacent in M, (c, a, d) is not a v-structure in M, and p = {c → b, d → b}. Since a, b are in DoneList, their adjacency are correctly learned so (a, c), (a, d), (b, c), (b, d) are adjacent pairs in G. If c → b ← d ∈ G, we enumerate all possible states of edges between (a, c) and (a, d) in G. (1) They are both undirected edges in G. Then c is in DoneList, so c, d is not adjacent in G, (c, d, S cd ) ∈ IndSet and a ∈ S cd , so a → b ∈ G by R3, which contradicts with a -b ∈ G. (2) They are both directed edges in G. Then a → c or a → d in G, otherwise they will form a v-structure, contradicts with that they do not form a v-structure in M. Without loss of generality suppose a → c ∈ G, then by c → b ∈ G and R2 we have a → b ∈ G, which contradicts with a -b ∈ G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>and that one satisfies the conclusion of lemma. If r = "R4", there exists c, d ∈ V such that c → d → b ∈ M, (a, c), (a, d) are adjacent pairs in M, p = {c → d}, and (b, a, c) is not a v-structure in M. i.e. (b, c, S bc ) ∈ IndSet and a ∈ S bc . Since a, b are in DoneList, their adjacency are correctly learned so (a, c), (a, d), (b, d) are adjacent pairs in G. We first claim that the three edges between (a, c), (a, d), (b, d) are not all directed in G. Assume, for the sake of contradiction, that they are all directed, then d → b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>which contradicts with c → d ∈ M since it will form a directed cycle. So at least one of the three edges between (a, c), (a, d), (b, c) are undirected in G. Since a, b are connected with T by an undirected path, at least one of c, d is also connected with T by an undirected path, so its adjacency is correctly learned and (c, d) is adjacent in G. Now for the sake of contradiction assume that c → d ∈ G. Since the adjacency of b is learned, (b, c, S bc ) ∈ IndSet and d ∈ S bc . So d → b ∈ G by R1. Enumerate all possible cases of edges between (a, d), (a, c). (1) If a → d ∈ G, then from d → b ∈ G we have a → b ∈ G by R2, which contradicts with a -b ∈ G. (2) If d → a ∈ G, then from c → d ∈ G we have c → a ∈ G by R2, and have a → b ∈ G by R1, which contradicts with a -b ∈ G. (3) If a → c ∈ G, then from c → d ∈ G we have a → d ∈ G and back to the first case. (4) If c → a ∈ G, then we have a → b ∈ G by R1, which contradicts with a -b ∈ G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Proof. By Theorem 1 in<ref type="bibr" target="#b15">Fang et al. (2022b)</ref>, G * 1 , G * 2 , . . . , G * k are MPDAGs, so [G * 1 ], [G * 2 ], . . . , [G * k ]are well-defined. By the definition of B-component, each undirected edge in G * lies in exactly one B-component of G * . Let G be a directed graph constructed by letting the induced subgraph of G * over V i be G i for all i = 1, 2, . . . , k. Then G and G * have the same skeleton, and all directed edges in G * has the same direction in G. It suffices to show that G ∈ [G * ], i.e. G has no directed cycle and no v-structure not included in G * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example for learning local structure. (a) The original DAG. (b)-(f) Learned structure G defined at Step 3 in Algorithm 1. G B D denote the learned graph after running Step 4-16 in Algorithm 1 for each node in D, with background knowledge B.</figDesc><graphic coords="45,170.09,243.06,85.93,83.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (a)-(d) The Kappa coefficients of different methods on random graphs with n nodes and N samples are drawn. (e)-(h) The average CPU time of different methods.</figDesc><graphic coords="47,140.94,386.32,75.62,56.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>d, S bd ) ∈ IndSet and a ∈ S bd , orient a → b.</figDesc><table><row><cell>14</cell><cell>until G is not changed ;</cell></row><row><cell>15</cell><cell>Update WaitList</cell></row><row><cell></cell><cell>= {U ∈ V|U and X are connected by an undirected path in G} \</cell></row><row><cell></cell><cell>DoneList;</cell></row><row><cell cols="2">16 until WaitList is empty;</cell></row><row><cell cols="2">17 return G.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Average RMSE and unfairness for each method in the counterfactual fairness experiment.</figDesc><table><row><cell></cell><cell>Node</cell><cell>Full</cell><cell>Unaware</cell><cell>Oracle</cell><cell>Method FairRelax</cell><cell>Fair</cell><cell>LFairRelax</cell><cell>LFair</cell></row><row><cell>Unfairness</cell><cell>10 20 30 40</cell><cell cols="7">0.251 ± 0.37 0.165 ± 0.267 0.085 ± 0.211 0.000 ± 0.000 0.079 ± 0.199 0.067 ± 0.152 0.118 ± 0.288 0.000 ± 0.000 0.069 ± 0.173 0.058 ± 0.162 0.012 ± 0.041 0.012 ± 0.041 0.035 ± 0.11 0.034 ± 0.109 0.099 ± 0.156 0.044 ± 0.103 0.000 ± 0.000 0.055 ± 0.281 0.053 ± 0.279 0.028 ± 0.157 0.029 ± 0.166 0.098 ± 0.18 0.054 ± 0.154 0.000 ± 0.000 0.054 ± 0.17 0.026 ± 0.077 0.025 ± 0.118 0.025 ± 0.122</cell></row><row><cell>RMSE</cell><cell>10 20 30</cell><cell cols="7">0.431 ± 0.189 0.441 ± 0.193 0.513 ± 0.268 0.491 ± 0.232 0.518 ± 0.251 0.591 ± 0.312 0.529 ± 0.434 0.537 ± 0.438 0.628 ± 0.643 0.623 ± 0.559 0.69 ± 0.646 0.794 ± 0.919 0.794 ± 0.919 0.59 ± 0.312 0.732 ± 0.745 0.731 ± 0.744 0.911 ± 1.118 0.816 ± 0.801 0.892 ± 0.95 0.891 ± 0.91 0.891 ± 0.91</cell></row><row><cell></cell><cell>40</cell><cell>0.703 ± 0.72</cell><cell cols="6">0.708 ± 0.724 0.849 ± 1.001 0.774 ± 0.819 0.843 ± 0.912 0.947 ± 1.092 0.947 ± 1.093</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>Identified causal relations for marital status by running LABITER (left two columns) and Algorithm 2 in Zuo et al. (2022) with learned MPDAG (right two columns). Differences are marked with bold.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that, with some type of prior knowledge such as ancestral relations, the refined set cannot be represented only by an MPDAG, but an MPDAG with a minimal residual set of direct causal clauses(Fang et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2022b). We discuss this type of prior knowledge in D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>That is, if for some i &lt; j, V i , V j are both in the WaitQueue in Algorithm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>in<ref type="bibr" target="#b30">(Liu et al., 2020b)</ref>, V j is never popped before V i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>Data download and data information are available at https://archive.ics.uci.edu/ml/ datasets/Bank+Marketing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>(a) n = 100, N = 100 (b) n = 100, N = 200 (c) n = 100, N = 500 (d) n = 100, N =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>(e) n = 100, N = 100 (f) n = 100, N = 200 (g) n = 100, N = 500 (h) n = 100, N =</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Graph terminologies and definitions</head><p>In this section, we review the graph terminologies and give the detailed definitions. Some definitions used in proofs but not in the main text are also listed.</p><p>A graph G = (V, E) is a tuple with node set V and edge set E ⊆ V × V, where × denotes the Cartesian product. For two distinct nodes X, Y ∈ V, we say that there is a directed edge from X to Y in G if (X, Y ) ∈ E and (Y, X) ̸ ∈ E, denoted by X → Y ∈ G. If (X, Y ) ∈ E and (Y, X) ∈ E, we say that there is a undirected edge between X and Y in G, denoted by X -Y ∈ G. Two nodes X and Y are adjacent in G if X → Y or Y → X or X -Y in G.</p><p>A path p = ⟨V 1 , . . . , V k ⟩ is a sequence of nodes where V i and V i+1 are adjacent in G for i = 1, 2, . . . , k -1. We say that p is causal (or directed) if V i → V i+1 ∈ G for each i. We say that p is partially directed if there is no i such that</p><p>If a cycle is causal, we call it a directed cycle. A graph G is directed (or undirected) if all of its edges are directed (or undirected). A directed acyclic graph (DAG) is a directed graph which does not contain any directed cycle. For two paths p = ⟨V 1 , . . . , V k ⟩ and q = ⟨W 1 , . . . , W l ⟩, let p ⊕ q = ⟨V 1 , . . . , V k , W 1 , . . . , W l ⟩ denote the connected path of p and q.</p><p>For two distinct nodes X, Y in a graph G, X is a parent (children, sibling, ancestor, descendant) of Y if there is a directed edge from X to Y (directed edge edges. Therefore, the total number of independence tests in Algorithm 1 is not larger than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof for Lemma 2</head><p>Proof. It suffices to show that: (i) For every Q ∈ Q such that X ⊥ Y | pa(X, G * )∪Q, we have an(C, G * )∩sib(X, G * ) ⊆ Q, (ii) an(C, G * )∩sib(X, G * ) ∈ Q, and (iii) X ⊥ Y | pa(X, G * ) ∪ (an(C, G * ) ∩ sib(X, G * )).</p><p>We first show (i). For the sake of contradiction, suppose that there exists <ref type="bibr" target="#b13">Fang and He (2020)</ref>, C is not in pa(X, G * ) ∪ Q. By the definition of critical set, there exists a chordless</p><p>We then show (ii). Since X is not a definite cause of Y in G * , by Theorem 4.5 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, the induced subgraph of <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, after each addition, Q remains to be complete. Moreover, after all additions, we have</p><p>Finally, we show (iii). By Theorem 1 in <ref type="bibr" target="#b13">Fang and He (2020)</ref>, there exists a <ref type="bibr" target="#b13">Fang and He (2020)</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Proof for Theorem 3</head><p>We first show that when applying a new background knowledge in an MPDAG, there are two properties: (i) Orienting an edge outside a B-component does not affect the orientation inside the B-component; (ii) Orientation within a Bcomponent can be done by applying Meek's rules within it. There has been a sufficient and necessary condition for a partially directed graph to be an MPDAG <ref type="bibr" target="#b15">(Fang et al., 2022b)</ref>. However, to the best of our knowledge, there is no formal theories for these two properties.</p><p>The first theorem shows that for an MPDAG, orienting each of its B-components independently induces an orientation of the entire MPDAG.</p><p>Theorem 9. Let G * 1 be an MPDAG, B 11 , B 12 be two different B-components of G * 1 with vertice sets V 1 , V 2 , respectively. Let X -Y be an undirected edge in B 12 . Let G * 2 be the MPDAG that represents the restricted MEC <ref type="bibr" target="#b32">Meek (1995)</ref>, there exists</p><p>The next theorem shows that when adding background knowledge in a Bcomponent, we can apply Meek's rules within the B-component to get the new MPDAG.</p><p>Theorem 10. Let G * 1 be an MPDAG and 2 , it has the same direction for every DAG in S; (iii) For every undirected edge</p><p>We first show (i). By the construction of B 21 , B 21 has the same skeleton with B 11 . So G * 2 has the same skeleton with G * 1 , and has the same skeleton with every DAG in [G * 1 ]. Therefore, G * 2 has the same skeleton with every DAG in S. Then we show (ii). In the following proof, let ϕ G * 1 be the decomposition map of</p><p>First suppose that N 1 is not connected with X by an undirected path in G 0 . Then N 1 is not connected with X by an undirected path in</p><p>By Lemma 2 in Fang and He (2020) and Lemma 4.3 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, the fact that N 1 is not a cause of T 1 is equivalent to that for every</p><p>Then by sequentially applying Theorem 10, G * 1 is obtained by replacing the induced subgraph of G * 0 over V 2 with B 02 . Therefore, the induced subgraph of</p><p>) such that orienting Q → X and X → sib(X, G * 0 ) \ Q does not introduce any v-structure collided on X or any directed triangle containing X. Then after Steps 4 to 9 in Algorithm 2, we have candC</p><p>So by Theorem 4.5 in <ref type="bibr" target="#b60">Zuo et al. (2022)</ref>, we have</p><p>Inductively applying this fact we also should orient an(C</p><p>In this case, let B 01 be the B-component of</p><p>By applying Theorem 10 inductively, we know that G * 1 can be constructed by replacing the induced subgraph of G * 0 over V 1 by B 11 . Let G 01 be the induced subgraph of G 0 over vertice set V 1 , then G 01 is identical to B 01 . By orienting C G * 0 (N 1 , T 1 ) → N 1 and applying Meek's rules in G 0 to get G 1 , the induced subgraph of G 1 over V 1 is identical to B 11 by Theorem 4 in <ref type="bibr" target="#b32">Meek (1995)</ref>. Since all directed edges in G 0 have the same direction in G 1 , we can conclude that for every Z connected with X in G * 1 , including X itself, we have pa(Z,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Proof for Theorem 4</head><p>Proof. If X is a definite non-cause of Y, then for any DAG G represented by M , X ̸ ∈ an(Y, G). By Theorem 1 in <ref type="bibr" target="#b13">(Fang and He, 2020)</ref>, there exists a DAG G represented by M such that pa(X, G) = pa(X, M ). By the Markov property we have X</p><p>Conversely, if X is not a definite non-cause of Y, then there exists a DAG G represented by M such that X ∈ an(Y, G). Let π be a causal path in G from X to Y, then each node on π is not in pa(X, M ), otherwise it is also in pa(X, G) and it will form a directed cycle in G. So X ̸ ⊥ Y |pa(X, M ). Figure <ref type="figure">8</ref> shows how lemmas fit together to prove Theorem 5. The first lemma shows the necessity of Theorem 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Proof for Theorem 5</head><p>Proof. Let π be a directed path in G * from X to Y, and G be any DAG in [G * ]. By Lemma 3, π is also directed in G. For any node Z ̸ = X on π, if Z and X are adjacent, by Lemma 5 we have</p><p>The next technical lemma shows that if we want to prove that π is blocked by pa(X, G * ) ∪ sib(X, G * ) in non-causal case, we only need to consider the case that the first collider on π in G is unshielded.</p><p>Lemma 17. Let X, Y be two distinct nodes that are not adjacent in an MPDAG <ref type="figure"></ref>and<ref type="figure">π</ref> ′ is also open given pa(X, G * ) ∪ sib(X, G * ).</p><p>Proof. We will discuss for two cases:</p><p>). Since all nodes in π ′ are non-collider and v p-1 → v p ∈ G, they are also non-collider on π. So π is also blocked by pa(X, G * ) ∪ sib(X, G * ), which conflicts with given conditions. Therefore, π ′ is non-causal in G.</p><p>Since π ′ is non-causal in G, we know that v p+1 ̸ = Y. We talk about the edge between v p+1 and v p+2 in G. If v p+1 → v p+2 ∈ G, then all non-colliders on π ′ are also non-colliders on π, and all colliders on π ′ are also colliders on π.</p><p>Until now, we have shown that if the first collider on π is shielded, there exists a shorter path π ′ such that π ′ is blocked implies π is blocked, and π ′ also v p-1 → v p ← v p+1 ∈ G * . By Lemma 18, without loss of generality we can assume that π(X, v p ) is causal in G * . Now consider the collider v p . We only need to consider the case that v p ∈ an(pa(X,  </p><p>If v 1 ∈ sib(X, G * ), we just need to consider the case that v 1 is a collider on π, otherwise π is blocked by pa(X, G * ) ∪ sib(X, G * ). Then X and v 2 are adjacent since (</p><p>block s whether it is a collider on s or it is a non-collider on s. For other nodes except v 2 on s, they are collider on s if and only if they are collider on π. Since s is a path from X to Y with v 2 ∈ ch(X, G * ), from (2) we know s is blocked by pa(X, G * ) ∪ sib(X, G * ), so π is also blocked by pa(X, G * ) ∪ sib(X, G * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Proof for Theorem 6</head><p>To prove Theorem 6, we need the following lemma: </p><p>and a, c are not adjacent in H. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hiton: a novel markov blanket algorithm for optimal variable selection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A characterization of markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable fair clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Onak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vakilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="405" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01638</idno>
		<title level="m">Do we become wiser with time? on causal equivalence with tiered background knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fair algorithms for clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negahbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning bayesian networks is np-complete. Learning from data: Artificial intelligence and statistics V</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of bayesian-network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fair clustering through fairlets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and psychological measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple constraint-based algorithm for efficiently mining observational databases for causal relationships</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using data mining to predict secondary school student performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M G</forename><surname>Silva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modernizing the bradford hill criteria for assessing causal relationships in observational data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<idno type="PMID">30433840</idno>
	</analytic>
	<monogr>
		<title level="j">Critical Reviews in Toxicology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="682" to="712" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ida with background knowledge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A local method for identifying causal relations under markov equivalence</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page">103669</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05067</idno>
		<title level="m">On the representation of causal background knowledge and its applications in causal inference</title>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The chain graph markov property</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frydenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian journal of statistics</title>
		<imprint>
			<biblScope unit="page" from="333" to="353" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local causal discovery of direct causes and effects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of causal effects and local structure learning of causal networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of statistics and its application</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimal enumeration of all possible total effects in a markov equivalence class</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perkovic</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2395" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local causal discovery for estimating causal effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Childers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="408" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kcrl: A prior knowledge based causal discovery framework with reinforcement learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Gani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="691" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reversible mcmc on markov equivalence classes of sparse directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1779" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counting and exploring sizes of markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2589" to="2609" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fairnn-conjoint learning of fair representations for fair decisions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discovery Science: 23rd International Conference, DS 2020, Thessaloniki</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-10-19">2020. October 19-21, 2020</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">253</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graphical models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Independence properties of directed markov fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Leimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="491" to="505" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collapsible ida: Collapsing parental sets for locally estimating possible causal effects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local causal network learning for finding pairs of total and direct effects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5915" to="5951" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating highdimensional intervention effects from observational data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3133" to="3164" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery using partial ancestral graphs in the presence of cycles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A data-driven approach to predict the success of bank telemarketing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating the effect of joint interventions from observational data in sparse high-dimensional settings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="647" to="674" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<imprint>
			<publisher>Cambridge Univ Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards scalable and data efficient learning of markov boundaries</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Björkegren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tegnér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="232" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying causal effects in maximally oriented partially directed acyclic graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perkovic</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Perković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02171</idno>
		<title level="m">Interpreting and using cpdags with background knowledge</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal inference via ancestral graph models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Statistical Science Series</title>
		<imprint>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Marginal structural models and causal inference in epidemiology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Marginal causal consistency in constraint-based causal learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roumpelaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borboudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causation: Foundation to Application Workshop, UAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Counting background knowledge consistent markov equivalent directed acyclic graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sharma</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1911" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using a general prior knowledge graph to improve data-driven causal network learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: combining machine learning with knowledge engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science computer review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Time and sample efficient discovery of markov blankets and direct causal relations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Algorithms for large scale markov blanket discovery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS conference</title>
		<meeting><address><addrLine>St. Augustine, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003b</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Sixth Annual Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discovering and orienting the edges connected to a target variable in a dag via a sequential local learning approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="252" to="266" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient enumeration of markov equivalent dags</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wienöbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luttermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liskiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="12313" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On efficient adjustment in causal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Henckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9956" to="10000" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Counterfactual fairness: Unidentification, bound and algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-eighth international joint conference on Artificial Intelligence</title>
		<meeting>the twenty-eighth international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Local causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Partial orientation and local structural learning of causal networks for prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Causation and Prediction Challenge</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="93" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Variational fair clustering</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11202" to="11209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Counterfactual fairness with partially known causal graph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
