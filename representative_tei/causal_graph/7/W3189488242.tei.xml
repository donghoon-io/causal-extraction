<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Provable Guarantees on the Robustness of Decision Rules to Causal Interventions *</title>
				<funder ref="#_q4nCf5F">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_hjGNeE6">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Benjie</forename><surname>Wang</surname></persName>
							<email>benjie.wang@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clare</forename><surname>Lyle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Provable Guarantees on the Robustness of Decision Rules to Causal Interventions *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robustness of decision rules to shifts in the datagenerating process is crucial to the successful deployment of decision-making systems. Such shifts can be viewed as interventions on a causal graph, which capture (possibly hypothetical) changes in the data-generating process, whether due to natural reasons or by the action of an adversary. We consider causal Bayesian networks and formally define the interventional robustness problem, a novel model-based notion of robustness for decision functions that measures worst-case performance with respect to a set of interventions that denote changes to parameters and/or causal influences. By relying on a tractable representation of Bayesian networks as arithmetic circuits, we provide efficient algorithms for computing guaranteed upper and lower bounds on the interventional robustness probabilities. Experimental results demonstrate that the methods yield useful and interpretable bounds for a range of practical networks, paving the way towards provably causally robust decision-making systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As algorithmic decision-making systems become widely deployed, there has been an increasing focus on their safety and robustness, particularly when they are applied to input points outside of the data distribution they were trained on. Much of the work in this area has focused on instance-based robustness properties of classifiers, which guarantee that the prediction does not change in some vicinity of a specific input point <ref type="bibr" target="#b11">[Shih et al., 2018;</ref><ref type="bibr">Narodytska et al., 2018]</ref>. However, there are many types of distribution shift that cannot be characterized by robustness against norm-bounded perturbations to individual inputs. Such distribution shifts are often instead characterized by causal interventions on the data-generating process <ref type="bibr" target="#b10">[Quionero-Candela et al., 2009;</ref><ref type="bibr">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Lipton et al., 2018]</ref>. These interventions give rise to a range of different environments (distributions), which can be the effect of natural shifts (e.g. different country) or actions of other agents (e.g. a hospital changing prescription policy).</p><p>To assess the impact of such interventions, we must leverage knowledge about the causal structure of the datagenerating distribution. This paper concerns itself with a simple question: given a decision-making system and a posited causal model, is the system robust to a set of plausible interventions to the causal model? Defining and verifying such model-based notions of robustness requires a formal representation of the decision-making system. For discrete input features and a discrete output class, regardless of how a classifier is learned, its role in decision-making can be unambiguously represented by its decision function, mapping features to an output class. This observation has spurred a recent trend of applying logic for meta-reasoning about classifier properties, such as monotonicity and instance-based robustness, by compiling the classifier into a tractable form <ref type="bibr" target="#b11">[Shih et al., 2018;</ref><ref type="bibr">Narodytska et al., 2018;</ref><ref type="bibr" target="#b0">Audemard et al., 2020]</ref>, for example an ordered decision diagram. We extend this approach to causal modelling by combining logical representations of the decision rule and causal model, and compiling this joint representation into an arithmetic circuit, a tractable representation of probability distributions.</p><p>Our main technical contributions are as follows. First, we motivate and formalize the robustness of a decision rule with respect to interventions on a causal model, which we call the interventional robustness problem, and characterize its complexity. Second, we develop a joint compilation technique which allows us to reason about a causal model and decision function simultaneously. Finally, we develop and evaluate algorithms for computing upper and lower bounds on the interventional robustness problem, enabling the verification of robustness of decision-making systems to causal interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>The problem of constructing classifiers which are robust to distribution shifts has received much attention from the machine learning perspective <ref type="bibr" target="#b10">[Quionero-Candela et al., 2009;</ref><ref type="bibr">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Lipton et al., 2018]</ref>. Particularly relevant to our work are proactive approaches to learning robust classifiers, which aim to produce classifiers that perform well across a range of environments (rather than a specific one) <ref type="bibr" target="#b11">[Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b12">Subbaswamy et al., 2019]</ref>.</p><p>A recent line of work analyses the behaviour of machine learning classifiers using symbolic and logical approaches by compiling these classifiers into suitable logical representations <ref type="bibr">[Narodytska et al., 2018;</ref><ref type="bibr" target="#b11">Shi et al., 2020]</ref>. Such representations can be used to answer a range of explanation and verification queries <ref type="bibr" target="#b0">[Audemard et al., 2020]</ref> about the classifier tractably, depending on the properties of the underlying propositional language . Our work uses this premise to tackle defining and verifying robustness to distribution shift, which involves not only the classifier but also a probablistic causal model such as a causal Bayesian network.</p><p>In the Bayesian network literature, sensitivity analysis [Chan and <ref type="bibr">Darwiche, 2004]</ref> is concerned with examining the effect of (typically small) local changes in parameter values on a target probability. We are concerned with providing worst-case guarantees against a set of possible causal interventions, which can involve changing parameters in multiple CPTs, and even altering the graphical structure of the network. This requires new methods that enable scalability to these large, potentially structural intervention sets. Our causal perspective generalizes and extends the work of <ref type="bibr" target="#b9">[Qin, 2015]</ref>, considering a richer class of interventions than previous work and using this perspective to prove robustness properties of a decision function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>In the rest of this paper, we use V = (X, Y, H) to denote the set of modelled variables, which includes observable features X, the prediction target Y , and hidden variables H. We use lower case (e.g. x) to denote instantiations of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Decision Functions</head><p>Consider the task of predicting Y given X. Though many machine learning (ML) techniques exist for this task, once learned, the input-output behaviour of any classifier can be characterized by means of a symbolic decision function F from X to Y . For many important classes of ML methods, including Bayesian network classifiers, binarized neural networks, and random forests, it is possible to encode the corresponding decision function as a Boolean circuit Σ [ <ref type="bibr" target="#b0">Audemard et al., 2020;</ref><ref type="bibr">Narodytska et al., 2018;</ref><ref type="bibr" target="#b12">Shih et al., 2019]</ref>. Such logical encodings can then be used to reason about the behaviour of the decision function, for instance providing explanations for decisions and verifying properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Bayesian Networks</head><p>In this paper, we are interested in robust performance of decision functions under distribution shift caused by changes in the data-generating process (DGP). In order to reason about this, we first need a causal model of the DGP which enables such changes to be represented. We first define Bayesian networks, which are a convenient way to specify a joint distribution over the set of variables V = {V 1 , V 2 , ..., V n }: Definition 1 (Bayesian Network). A (discrete) Bayesian network (BN) N over variables V is a pair (G, Θ). G = (V , E) is a directed acyclic graph (DAG) whose nodes correspond to the random variables V and whose edges indicate conditional dependence. Θ denotes the set of conditional probability tables (CPTs) θ Vi|Ui with parameters</p><formula xml:id="formula_0">θ vi|ui = P (V i = v i |U i = u i )</formula><p>which specify the distribution, where U i = pa G (V i ) are the parents of V i in G. We will denote by p N the distribution defined by the BN N .</p><p>Causal Bayesian networks (CBNs) are defined similarly to Bayesian networks, with the addition of causal, or interventional, semantics to the joint distribution. Intuitively, an edge (V, V ) in a causal Bayesian network indicates that V causes V , and the CPTs correspond to causal mechanisms. An intervention can be defined to be a change to some of these mechanisms, replacing Θ with Θ . A CBN can thus be characterized as representing a set of distributions, each of which is generated by a different intervention.</p><p>We now define a representation of a (causal) Bayesian network, called the network polynomial, based on the seminal work of <ref type="bibr" target="#b4">[Darwiche, 2003]</ref>. This is a multi-linear function of indicator variables, encoding the BN variables V , and parameter variables, encoding the BN parameters. Definition 2 (Network Polynomial). The network polynomial of causal BN N is defined to be:</p><formula xml:id="formula_1">l N [λ, Θ] = v1,...,vn n i=1 λ vi θ vi|ui (1)</formula><p>where λ vi denotes an indicator variable for each value v i in the support of each random variable V i , and θ vi|ui denotes each element of a CPT in Θ. Each component of the addition</p><formula xml:id="formula_2">l v [λ, Θ] := n i=1 λ vi θ vi|ui is called a term,</formula><p>and is associated with an instantiation V = v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Arithmetic Circuits</head><p>Arithmetic circuits (AC) are computational graphs used to encode probability distributions over a set of discrete variables V , which can tractably answer a broad range of probabilistic queries, depending on certain structural properties (called decomposability, smoothness and determinism). They were first introduced by <ref type="bibr" target="#b4">[Darwiche, 2003]</ref> as a means of compiling Bayesian networks for the purposes of efficient inference. Subsequently they have been considered as objects of study in their own right, with proposals for directly learning ACs from data <ref type="bibr" target="#b8">[Lowd and Domingos, 2008]</ref> and extensions relaxing determinism <ref type="bibr">[Poon and Domingos, 2011]</ref>. Definition 3 (Arithmetic Circuit). An arithmetic circuit AC over variables V and with parameters Φ is a rooted directed acyclic graph (DAG), whose internal nodes are labelled with with + or × and whose leaf nodes are labelled with indicator variables λ v , where v is the value of some variable V ∈ V , or non-negative parameters φ.</p><p>Crucially, evaluating an arithmetic circuit can be done in time linear in the size (number of edges) of the circuit. When an AC represents a probability distribution, this means that marginals can be computed efficiently.</p><p>Like Bayesian networks, arithmetic circuits can be represented as polynomials over indicator and parameter variables, based on subcircuits [Choi and <ref type="bibr" target="#b3">Darwiche, 2017]</ref>: Definition 4 (Complete Subcircuit). A complete subcircuit α of an AC is obtained by traversing the AC top-down, choosing one child of every visited +-node and all children of every visited ×-node. The term term(α) of α is the product of all leaf nodes visited (i.e. all indicator and parameter variables). The set of all complete subcircuits is denoted α α α AC . Definition 5 (AC Polynomial). The AC polynomial of arithmetic circuit AC is defined to be:</p><formula xml:id="formula_3">l AC [λ, Φ] = α∈α α α AC term(α)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Intervention Robustness Problem</head><p>Many distribution shifts faced by decision-making systems can be characterized by an intervention on the datagenerating process. For example, if an insurance company offers reduced premiums to drivers who take a driving class, the way 'risk aversion' affects 'class' in Figure <ref type="figure" target="#fig_0">1</ref> may change in response as more risk-seeking drivers take driving classes to benefit from reduced premiums. The company therefore seeks to determine whether this policy will be robust to changes in this relationship before it deploys the policy.</p><p>To model this, we formulate an intervention robustness problem, which considers the worst-case drop in performance of a classifier in response to changes to a subset of the causal mechanisms (CPTs) of the Bayesian network. This is inspired by the principle of independent causal mechanisms (ICM) <ref type="bibr" target="#b8">[Peters et al., 2017]</ref>, which states that causal mechanisms do not inform or influence each other; that is, even as some mechanisms are changed, other mechanisms tend to remain invariant. In the insurance example, this is reflected in that we would not necessarily expect the way 'risk aversion' or 'accident' is generated to change, for instance.</p><p>While many related notions of robustness exist in the literature, none accurately captures this notion of robustness to causal mechanism changes. Many popular definitions of robustness measure the size of a perturbation necessary to change an input's classification, without taking into account that such perturbations may change the value which the classifier tries to predict <ref type="bibr" target="#b11">[Shih et al., 2018]</ref>. <ref type="bibr" target="#b8">[Miller et al., 2020]</ref> highlight the connection between causal inference and robustness to distribution shifts caused by 'gaming' in the strategic classification <ref type="bibr" target="#b5">[Hardt et al., 2016]</ref> regime. However, <ref type="bibr" target="#b8">[Miller et al., 2020]</ref> does not assume access to a known causal model, and its focus is on identifying classifiers which are robust to gaming, whereas our objective is to verify robustness to a much richer collection of distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intervention Classes</head><p>To reason about the effects of changes to a causal model, we need a formal description of these interventions. We consider interventions as actions that modify the mechanisms of a causal Bayesian network N = (G, Θ), thereby changing its joint distribution. In particular, we consider two types of interventions: the first concerns changes to the parameters of the causal model, while the second concerns changes to the existence of cause-effect relationships themselves.</p><p>Typically, we might expect that only mechanisms for a subset of variables W ⊆ V will change. In what follows, given a subset of variables W ⊆ V , we will use θ </p><formula xml:id="formula_4">N [θ (G) W ] := (G, Θ )<label>(2)</label></formula><p>Parametric interventions encompass the do-interventions discussed by <ref type="bibr" target="#b9">[Qin, 2015]</ref>, but allow us to express more complex changes to causal mechanisms than fixing a variable to a set value. We can further consider changes not just to the parameters of the network, but also to its edge structure; such changes to a set of variables W can be described by a context function C W : W → P(V ), which replaces the parents of </p><formula xml:id="formula_5">W ∈ W in G with C W (W ),</formula><formula xml:id="formula_6">N [θ (G ) W , C W ] := (G , Θ )<label>(3)</label></formula><p>We will often be interested in considering all of the possible interventions of a given class on some subset W ⊆ V of the variables in the causal graph. Letting P G (W ) denote the set of valid parameter sets for W ⊆ V in graph G, we will write for parametric interventions:</p><formula xml:id="formula_7">I N [W ] := {N [θ (G) W ] | θ (G) W ∈ P G (W )} (4)</formula><p>and for structural interventions,</p><formula xml:id="formula_8">I N [W , C W ] := {N [θ (G ) W , C W ] | θ (G ) W ∈ P G (W )} (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition and Complexity</head><p>Accurately assessing the robustness of a decision function F requires an understanding of the causal data-generating process. We propose to model this DGP using a causal Bayesian network N on all variables V , thus enabling causal modelbased analysis of classifiers. In order to reason about the causal structure N and decision rule F simultaneously, we add an additional node to the CBN N .</p><p>Definition 8 (Augmented BN). For a CBN N over variables V and a classifier F : X → Y , we define the augmented BN N F based on N as follows:</p><formula xml:id="formula_9">V F = V ∪{ Ŷ } with pa( Ŷ ) = X and deterministic CPT θ ŷ|x = 1[ŷ = F (x)].</formula><p>This produces a well-defined joint distribution over the variables V and Ŷ = F (X), which allows us to specify performance metrics as probabilities of events e. For instance, a classifier's false positive probability can be expressed as the probability p N F (e) of event e = ( Ŷ = 1) ∧ (Y = 0). More importantly, we can consider how these metrics change as the joint distribution changes, due to hypothetical or observed interventions on the causal model. This provides a basis for model-based notions of robustness of decision rules.</p><p>We use intervention sets to represent all interventions which the modeller considers plausible. The interventional robustness problem then concerns the worst-case performance of the decision rule over interventions in that set. Definition 9 (IntRob). Given CBN N and decision rule F , let I N F be an intervention set for the augmented BN N F , e be an assignment of a subset of the variables in V , and &gt; 0.</p><p>The interventional robustness problem is that of computing:</p><formula xml:id="formula_10">IntRob(I N F , e) := max N ∈I N F p N (e) .</formula><p>We also have the corresponding decision problem:</p><formula xml:id="formula_11">IntRob(I N F , e, ) := max N ∈I N F p N (e) &gt; .</formula><p>We will be particularly interested in problem instances where I N is of the form I N [W ], in which case we can view the problem instance as IntRob((N , W ), e). Our next result shows that the causal semantics of IntRob do not increase the computational hardness of the problem beyond that of MAP inference. Theorem 1. Let N = (G, Θ) be a causal Bayesian network, with n nodes and maximal in-degree d. Then an instance of MAP can be reduced to an instance of IntRob on a BN N of size linear in |N |, and of treewidth w ≤ w+2. An instance of IntRob can be reduced to an instance of MAP on a BN N whose CPT Θ has size polynomial in the size of Θ, and with treewidth w ≤ 2w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Verification of Intervention Robustness</head><p>In this section, we present our approach to verifying interventional robustness. Due to the difficulty of the problem, we seek to approximate IntRob(I, e) by providing guaranteed upper and lower bounds that can be efficiently computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Joint Compilation</head><p>Our first goal is to compile N F into an equivalent arithmetic circuit AC. To do so, we make use of a standard CNF encoding ∆ N of the causal BN N , defined over the indicator and parameter variables λ V , Θ [Chavira and <ref type="bibr" target="#b3">Darwiche, 2005]</ref>, and additionally an encoding of the decision function F .</p><p>A naïve encoding of F is to explicitly enumerate all instantiations of features x and prediction ŷ, and encode these directly as CNF clauses. However, this approach is very inefficient for larger feature sets X. We instead assume access to an encoding of the classifier as a Boolean circuit Σ over input features X and prediction Ŷ . Such a circuit can be converted to CNF through the Tseitin transformation, introducing additional intermediate variables T , obtaining a CNF formula ∆ F over λ X , λ Ŷ , T . We then combine the encodings of F and N simply by conjoining the CNF formulae, to produce a new formula ∆ joint = ∆ N ∧ ∆ F , over λ V , λ Ŷ , Θ, T .</p><p>To construct an AC AC, we now compile this CNF encoding into d-DNNF (deterministic decomposable negation normal form), using the C2D compiler <ref type="bibr">[Darwiche, 2004]</ref>, and then replace ∨-nodes with +, ∧-nodes with ×, and set all negative literals and literals corresponding to T to 1. This produces an AC with polynomial l AC [λ, Θ], where λ := λ V ∪ λ Ŷ . Crucially, this AC is equivalent to the augmented BN, in the following sense:</p><formula xml:id="formula_12">Proposition 1. l AC [λ, Θ] is equivalent to l N F [λ, Θ].</formula><p>Further, AC can be used to faithfully evaluate marginal probabilities p N (e) under any parametric intervention N .</p><p>The time and space complexity of this procedure is O(nw2 w ), where n is the number of CNF variables and w the treewidth, a measure of the connectivity of the CNF. When jointly compiling a BN and a decision function, we can bound n, w in terms of the individual encodings ∆ N , ∆ F . Proposition 2. Suppose ∆ N has n variables and treewidth w, and ∆ F has n variables and treewidth w . Then ∆ joint has exactly n + n -|λ X | variables, and treewidth at most max(w, w , min(w, w ) + |λ X |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Orderings</head><p>For the correctness of our upper bounding algorithm, it is necessary to impose some structural constraints on the circuit.</p><p>Firstly, any circuit compiled using the procedure described above has the property that every +-node t has two children, and is associated with some CNF variable c, such that one child branch has c true, and the other has c false (information on the identity of this variable for each +-node is provided by the C2D compiler). We need to ensure that the arithmetic circuit only contains +-nodes associated with indicators λ, and not intermediate variables T . Provided this is the case, the branches of each +-node t will contain contradicting indicators for some unique variable V . We can thus say that t 'splits' variable V , as each of its child branches corresponds to different values of V , and we write split(t) to denote this splitting variable.</p><p>Secondly, provided the above holds, we require our circuit to satisfy some constraints of the following form.</p><p>Definition 10 (Ordering Constraint). An arithmetic circuit AC satisfies the ordering constraint (V j , V i ) if:</p><formula xml:id="formula_13">∀t, t , (split(t) = V i ∧ split(t ) = V j ) =⇒ t' is not a descendant of t in AC (6)</formula><p>Intuitively, our algorithm requires that for BN variables in the intervention set W , the relative position of splitting +-nodes in the AC agrees with the causal ordering in the BN. More formally, we say that AC satisfies the ordering constraints associated with intervention set V i ∈ W , and all V j such that</p><formula xml:id="formula_14">I N F , if</formula><formula xml:id="formula_15">V j ∈ pa G (V i ) (parametric inter- vention set) or V j ∈ pa G (V i ) (structural intervention set), AC satisfies the ordering constraint (V j , V i ).</formula><p>In practice, when computationally feasible, we compile ACs with topological and structural topological orderings, which satisfy these constraints for all V i , not just V i ∈ W ; such orderings have the advantage of being valid for any intervention sets W . We enforce these constraints by enforcing corresponding constraints on the elimination ordering π over the CNF variables, which is used to construct the dtree that is used in the compilation process, and affects the time and space taken by the compilation. Such an elimination ordering is usually chosen using a heuristic such as min-fill. We instead find a elimination ordering by using a constrained min-fill heuristic, which ensures that these constraints are satisfied, but may produce an AC which is much larger than can be achieved with an unconstrained heuristic in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Upper Bounds on Intervention Robustness</head><p>In order to compute upper bounds on the interventional robustness quantity, we propose Algorithm 1, which sets parameters in the AC for the CPTs of variables in W to 1, and applies maximization instead of addition at +-nodes splitting on variables in W , when evaluating the (appropriately ordered) AC. Algorithm 1 somewhat resembles the well-known MPE algorithm on ACs, introduced by [Chan and <ref type="bibr" target="#b6">Darwiche, 2006]</ref> and used as an upper bound on the MAP problem in <ref type="bibr" target="#b6">[Huang et al., 2006]</ref>. However, our algorithm maximizes over parameters rather than variables and makes use of specific AC structure ensured by our ordering constraints; the reason it produces correct upper bounds is thus also different.</p><p>Intuitively, the maximizations represent decision points, where choosing a child branch corresponds to intervening to set a particular parameter θ w|u W to 1 (and others to 0). For example, consider the augmented Bayesian network in Fig- , where all variables are binary, Ŷ = X ∨ W , and the context C(W ) is {X} (represented by the dashed line, which is not part of the original BN). Figure <ref type="figure">2</ref> also shows execution of Algorithm 1 for false positive probability, i.e. e = ( Ŷ = 1) ∧ (Y = 0). At the two +-nodes where maximizations occur, the value of X is already "decided", and the adversary can effectively choose to set θ w|x = 1 and θ w|x = 1. In this case, the result 0.4 turns out to be exactly equal to the interventional robustness quantity IntRob(I N F , e).</p><p>We might ask whether this intuition is correct in general. Our next result shows that, while the algorithm cannot always compute IntRob(I N F , e) exactly, it does produce guaranteed upper bounds: Theorem 2. Given a parametric/structural intervention set I N F , let AC be an arithmetic circuit with the same polynomial as N F , and satisfying the ordering constraints associated with the intervention set. Then, applying the UB algorithm U B(AC, e, W ) returns a quantity B U which is an upper bound on the interventional robustness quantity</p><formula xml:id="formula_16">IntRob(I N F , e).</formula><p>This result is quite surprising; it shows that it is possible, through a very simple and inexpensive procedure requiring just a single linear time pass through the AC, to upper bound the worst-case marginal probability over an exponentially sized set of interventions. That this also holds for structural intervention sets, which alter the structure of the Bayesian network which the AC was compiled from, is even more surprising. Further, a compiled AC can be used for any intervention set given that it satisfies the appropriate ordering constraints. For instance, an AC compiled using a topological ordering allows us to derive upper bounds for parametric intervention sets involving any subset (of any size) W ⊆ V , simply by setting the appropriate parameter nodes in the AC to 1 (Line 5). This allows us to amortize the cost of evaluating robustness against multiple intervention sets.</p><p>Algorithm 2: Lower Bounding Input: N = (G, Θ), a Bayesian network; evidence e whose probability will be maximized; intervenable variables</p><formula xml:id="formula_17">W ⊆ V . Result: Output probability p(e) ≤ max N ∈I N [W ] P N (e) 1 begin 2 v ← 0; 3 while p N [Θ W ] (e) &gt; v do 4 v ← p N [Θ W ] (e); 5 for CPT θ (G) W |u ∈ Θ W do 6 θ (G) W |u ← arg max θ W |u p N [θ W |u ] (e);</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Lower Bounds via Best-Response Dynamics</head><p>In addition to an upper bound on IntRob(I N F , e), we can also straightforwardly lower bound this quantity using any witness, in the case of parametric interventions</p><formula xml:id="formula_18">I N F [W ].</formula><p>That is, we search for an intervention in the set which approximately maximizes the probability of evidence e.</p><p>We obtain such an approach by formalizing the problem of finding an intervention which maximizes p N (e) as a multiplayer game, where each instantiation u W of parents pa G (W ) for each W ∈ W specifies a player, and where all players share a utility function given by p N [Θ] (e). Each player's strategy set consists of the set of deterministic conditional distributions θ W |u (we note w.l.o.g. that, by the multilinearity of the network polynomial, the optimal value of P N [Θ ] (e) is obtained by at least one deterministic interventional distribution). A Nash equilibrium in this game then corresponds to an interventional distribution for which no change in a single parameter can increase p N (e). Algorithm 2 follows best-response dynamics in this game. We provide an analysis of the time complexity and convergence of this approach in the proof of the following proposition. Proposition 3. Algorithm 2 converges to a locally optimal parametric intervention in finite time. Further, if the algorithm is stopped before termination, the current value v will be a lower bound on max N ∈I[W ] P N (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case Study: Insurance</head><p>In this case study, we look at an extended version of the car insurance example, using the Bayesian network model shown in Figure <ref type="figure" target="#fig_4">3</ref>  <ref type="bibr" target="#b1">[Binder et al., 1997]</ref>.</p><p>Suppose an insurance company wishes to predict MedCost (the medical cost of an insurance claim), given an insurant's Age, DrivHist, and MakeModel (categorical variables with 3-5 values). MedCost is either BelowThousand (0) or AboveThousand (1). They fit a Naïve Bayes classifier to historical data, obtaining a decision function F . This is then used as part of their decision-making policy determining what premiums to offer to customers.</p><p>The company is particularly concerned about false negatives, as the company could lose a lot of money in payouts. Based on the original Bayesian network model <ref type="bibr">(Figure 3)</ref>   time. However, insurants may attempt to game the classifier to predict BelowThousand (so that they get lower premiums), while actually being likely to have a high medical cost. In our framework, we model this using structural interventions, assuming that insurants can causally intervene on some of DrivHist (hide some accident history), MakeModel (choose a different type of car than they would normally choose), and Cushioning (upgrade/downgrade the degree of protection). We use structural intervention sets (with appropriately designed context function) because insurants will have access to some non-parent variables when adapting; for instance, they will know their Age when choosing the MakeModel of a new car. The company would like to understand how robust their classifier is to these adaptations.</p><p>We consider a number of structural intervention sets I N F , given by intervenable variables W , which may be any subset of {DrivHist, MakeModel, Cushioning}. We use structural sets because we assume the insurant has access to other variables when choosing how to adapt, such as Age or Mileage, which are not parents of these variables in the original BN. Under each of these intervention sets, we seek to obtain guaranteed upper bounds on these two quantities:</p><p>• FN: The probability of a false negative p(F = 0, MedCost = 1), i.e. predicted low medical cost, but high actual medical cost.</p><p>• P: The probability of a positive p(MedCost = 1), i.e. high actual medical cost.</p><p>The results are shown in Table <ref type="table" target="#tab_2">1</ref>. The insurance company can use these bounds to assess risk, and improve their classifier's robustness if they deem the false negative rate under intervention unacceptable.</p><p>The bounds can also provide further insight. We notice that whenever DrivHist is intervenable, the percentage of false negatives is the same as positives, i.e. the classifier always predicts wrong when MedCost is 1. This turns out to be because the Naïve Bayes classifier always predicts 0 whenever DrivHist is None, regardless of the other input variables. Thus, an insurant who can change their DrivHist can always fool the classifier to predict 0. In addition, the percentage of positives doesn't increase from the original BN: this can be seen from the causal graph, where DrivHist has no causal influence on MedCost.</p><p>On the other hand, Cushioning significantly increases the positive rate. Notice that, in the graph, intervening on Cushioning will not have any influence on the inputs to the classifier; thus, the increase in FN to 6.1% is not due to fooling the classifier, but rather making high medical expenses generally more likely, by downgrading the quality of cushioning. In this way, the intervention is "taking advantage" of the classifier not having full information about cushioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Compilation Performance</head><p>In Table <ref type="table" target="#tab_3">2</ref> we show the performance of our joint compilation approach on a number of benchmark Bayesian networks, where we jointly compile the network and a decision rule. We observe that the sizes of the compiled ACs are significantly smaller than the worst-case bounds would suggest (exponential in treewidth). Further, when we enforce a topological or structural topological ordering, the size of the compilation increases, but not by more than ∼ 100. Our results provide evidence that our methods can scale to fairly large networks and classifiers, including networks compiled with topological and structural topological orderings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Lower and Upper Bound Tightness</head><p>In Table <ref type="table" target="#tab_4">3</ref> we analyse the quality of our upper and lower bounds on interventional robustness. We compute bounds on false negative probability under different intervention sets. Overall, we find small or nonexistent gaps between the lower and upper bounds across all networks and intervention sets evaluated, suggesting that in many settings of interest it is possible to obtain tight guarantees using our algorithms. Further, both bounding algorithms are very fast to execute, taking no more than a few seconds for each run. This is remarkable given the sizes of the intervention sets. For instance, for the insurance network, the parametric intervention set P2 covers 6 variables (|W | = 6), 248 parameters, and ∼ 10 36 different interventions, making brute-force search clearly infeasible. For worst-case (interventional robustness) analysis, the sensitivity analysis method of [Chan and <ref type="bibr">Darwiche, 2004]</ref> requires ∼ 10 7 passes through the AC in this case. On the other hand, our upper bounding algorithm requires an ordered AC (which is ∼ 5 times larger in this case), but requires just a single pass through the AC, making it ∼ 10 6 faster. Further, our algorithm is uniquely able to provide guarantees for structural intervention sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we have motivated and formalized the interventional robustness problem, developed a compilation technique to produce efficient joint representations for classifiers and DGPs, and provided tractable upper and lower bounding algorithms which we have shown empirically to be tight on a range of networks and intervention sets. The techniques presented here provide ample opportunity for further work, such as extending the upper and lower bounding technique to networks where the modeller has uncertainty over the parameters, and developing learning algorithms for arithmetic circuits which permit reasoning about causal structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example causal model describing accident risk for a car insurance problem, and illustrating how strategic adaptation to a classifier can be characterized as a change to a causal model describing how the data was generated.</figDesc><graphic coords="3,54.00,54.00,233.28,95.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to denote the parameters associated with the CPTs for variables W ∈ W , where the parents of W are given by graph G. Definition 6 (Parametric Interventions). A parametric intervention on variables W substitutes a subset of parameters θ (G) W for new values θ (G) W obtaining a new parameter set Θ , which yields the BN:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>producing a new graph G . We refer to such interventions as structural interventions. In this work we restrict ourselves to context sets which preserve the acyclicity of the DAG. Definition 7 (Structural Interventions). A structural intervention on variables W modifies the edges E of the graph G = (V , E) according to a context function C W , obtaining a new graph G = (V , E ), and substitutes parameters θ (G) W for new values θ (G ) W , obtaining a new parameter set Θ , which yields the BN:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>U B(AC, e, W ) (Upper Bounding) Input: AC, the AC; evidence e; intervenable variables W ⊆ V ; Result: Output probability p 1 for node c ∈ AC (children before parents) do 2 switch type(c) do 3 case Indicator λ v do 4 p[c] := 0 if v not consistent with e else 1 5 case Parameter θ v|u do 6 p[c] := 1 if V ∈ W else θ v|u 7 case × do 8 p[c] := d p[d] where d are the children of c 9 case + do 10 if c splits on some W ∈ W then 11 p[c] := max d p[d] where d are the children of c 12 else 13 p[c] := d p[d] where d are the children of c 14 Return p[c root ], where c root is the root node of AC ure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: INSURANCE Bayesian network. Classifier features X are italicized, and (potential) interventions are shown in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and their new classifier, this should occur 2.5% of the</figDesc><table><row><cell>Age</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SocioEcon</cell></row><row><cell cols="3">GoodStudent RiskAversion OtherCar</cell></row><row><cell cols="2">SeniorTrain</cell><cell cols="2">HomeBase AntiTheft</cell></row><row><cell cols="2">DrivingSkill</cell><cell cols="2">MakeModel VehicleYear Mileage</cell></row><row><cell>DrivHist DrivQ</cell><cell></cell><cell>Airbag Antilock RuggedAuto</cell><cell>CarValue</cell></row><row><cell></cell><cell cols="2">Cushioning Accident</cell><cell>Theft</cell></row><row><cell>MedCost</cell><cell cols="2">ILiCost OtherCarCost ThisCarDam</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ThisCarCost</cell></row><row><cell></cell><cell></cell><cell>PropCost</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Guaranteed upper bounds on FN and P, under different structural intervention sets</figDesc><table><row><cell cols="3">Intervenable Variables W</cell><cell></cell><cell>FN</cell><cell>P</cell></row><row><cell>Empty Set</cell><cell></cell><cell></cell><cell></cell><cell>2.5%</cell><cell>7.2%</cell></row><row><cell>{DrivHist}</cell><cell></cell><cell></cell><cell></cell><cell>7.2%</cell><cell>7.2%</cell></row><row><cell cols="2">{MakeModel}</cell><cell></cell><cell></cell><cell>5.7%</cell><cell>10.0%</cell></row><row><cell cols="2">{Cushioning}</cell><cell></cell><cell></cell><cell>6.1%</cell><cell>12.9%</cell></row><row><cell cols="3">{DrivHist, MakeModel}</cell><cell></cell><cell cols="2">10.0% 10.0%</cell></row><row><cell cols="3">{DrivHist, Cushioning}</cell><cell></cell><cell cols="2">12.9% 12.9%</cell></row><row><cell cols="3">{MakeModel, Cushioning}</cell><cell></cell><cell cols="2">13.0% 13.9%</cell></row><row><cell cols="6">{DrivHist, MakeModel, Cushioning} 13.9% 13.9%</cell></row><row><cell>Net</cell><cell>CSize</cell><cell cols="2">Ord TW</cell><cell>AC size</cell><cell>Time (s)</cell></row><row><cell cols="2">insurance 3 (41)</cell><cell>N</cell><cell>24</cell><cell>167121</cell><cell>0.5</cell></row><row><cell></cell><cell>3 (41)</cell><cell>T</cell><cell>31</cell><cell>794267</cell><cell>4</cell></row><row><cell></cell><cell>3 (41)</cell><cell>S</cell><cell>33</cell><cell>1270075</cell><cell>8</cell></row><row><cell cols="3">win95pts 16 (799) N</cell><cell>51</cell><cell>1210072</cell><cell>3</cell></row><row><cell></cell><cell cols="2">16 (799) T</cell><cell>58</cell><cell>52266950</cell><cell>77</cell></row><row><cell>hepar2</cell><cell cols="2">12 (946) N</cell><cell>53</cell><cell>8096874</cell><cell>49</cell></row><row><cell></cell><cell cols="2">12 (946) T</cell><cell cols="2">51 123108407</cell><cell>73</cell></row><row><cell></cell><cell cols="2">12 (946) S</cell><cell cols="2">51 123164181</cell><cell>75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for the joint compilations used in the UB and LB algorithms. Shown are the number of input features d and the sizes of the Boolean circuits representing the classifier, ordering constraints (none, topological, or structural topological), treewidth of the combined CNF encoding, and AC size and compilation time.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Analysis of the tightness of bounds (on probability of false negatives) produced by Algorithms 1 and 2. For each network, we have different intervention sets (P/S indicates the intervention set is parametric/structural respectively). Lower and upper bounds, along with the difference, are shown for each intervention set.</figDesc><table><row><cell cols="5">Network IntSet LBound UBound ∆</cell></row><row><cell cols="2">insurance P1</cell><cell>0.1181</cell><cell>0.1276</cell><cell>0.0095</cell></row><row><cell></cell><cell>P2</cell><cell>0.3275</cell><cell>0.3433</cell><cell>0.0158</cell></row><row><cell></cell><cell>S1</cell><cell>0.1181</cell><cell>0.1297</cell><cell>0.0116</cell></row><row><cell cols="2">win95pts P1</cell><cell>0.2111</cell><cell>0.2111</cell><cell>0.0000</cell></row><row><cell></cell><cell>P2</cell><cell>0.2163</cell><cell>0.2191</cell><cell>0.0028</cell></row><row><cell>hepar2</cell><cell>P1</cell><cell cols="2">0.09445 0.09445</cell><cell>0.0000</cell></row><row><cell></cell><cell>P2</cell><cell cols="2">0.09585 0.09585</cell><cell>0.0000</cell></row><row><cell></cell><cell>S1</cell><cell>0.1029</cell><cell>0.1029</cell><cell>0.0000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project was funded by the <rs type="funder">ERC</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (<rs type="grantNumber">FUN2MODEL</rs>, grant agreement No. <rs type="grantNumber">834115</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q4nCf5F">
					<idno type="grant-number">FUN2MODEL</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_hjGNeE6">
					<idno type="grant-number">834115</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On Tractable XAI Queries based on Compiled Representations</title>
		<author>
			<persName><surname>Audemard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Principles of Knowledge Representation and Reasoning</title>
		<meeting>the 17th International Conference on Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="838" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive probabilistic networks with hidden variables</title>
		<author>
			<persName><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="213" to="244" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sensitivity analysis in bayesian networks: From single to multiple parameters</title>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Darwiche</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 20th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004-07">2004. July 2004</date>
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chavira and Darwiche, 2005] Mark Chavira and Adnan Darwiche. Compiling bayesian networks with local structure</title>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Darwiche</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 19th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Choi and Darwiche</publisher>
			<date type="published" when="2005">2006. July 2006. 2005. 2017. August 2017</date>
			<biblScope unit="page" from="825" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A differential approach to inference in bayesian networks</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="305" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Christos Papadimitriou, and Mary Wootters. Strategic classification</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science</title>
		<meeting>the 2016 ACM Conference on Innovations in Theoretical Computer Science<address><addrLine>Nimrod Megiddo</addrLine></address></meeting>
		<imprint>
			<publisher>Moritz Hardt</publisher>
			<date type="published" when="2004-08">2004. August 2004. 2016. January 2016</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
	<note>Proceedings of the 16th European Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving map exactly by searching on compiled arithmetic circuits</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006-07">2006. July 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1143" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">2018. July 2018</date>
			<biblScope unit="page" from="3128" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">Domingos</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Dominik</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Janzing</surname></persName>
		</editor>
		<editor>
			<persName><surname>Schlkopf</surname></persName>
		</editor>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Poon and Domingos</publisher>
			<date type="published" when="2008-07">2008. July 2008. 2020. July 2020. 2018. 2017. 2017. 2011. July 2011</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
	<note>Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differential semantics of intervention in bayesian networks</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015-07">2015. July 2015</date>
			<biblScope unit="page" from="710" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Quionero-Candela</surname></persName>
		</author>
		<title level="m">Dataset Shift in Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On tractable representations of binary neural networks</title>
		<author>
			<persName><surname>Rojas-Carulla</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Principles of Knowledge Representation and Reasoning</title>
		<meeting>the 17th International Conference on Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020. September 2020. September 2018</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
	<note>Proceedings of the Ninth International Conference on Probabilistic Graphical Models</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Preventing failures due to dataset shift: Learning predictive models that transport</title>
		<author>
			<persName><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research</title>
		<meeting>the 22nd International Conference on Artificial Intelligence and Statistics, Machine Learning Research</meeting>
		<imprint>
			<publisher>Kun Zhang</publisher>
			<date type="published" when="2015-01">2019. January 2019. 2019. 2019. 2015. January 2015</date>
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
		<respStmt>
			<orgName>Mingming Gong, and Bernhard Scholkopf. Multi-</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
