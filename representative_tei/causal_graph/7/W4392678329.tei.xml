<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Multimodal Sentiment Analysis Debiasing via Bias Purification</title>
				<funder ref="#_vH3SmE5">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_hhJ68P9">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-05">5 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
							<email>dkyang20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
							<email>mingchengli21@m.fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongling</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab 3 Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent Youtu Lab 3 Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering &amp; Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Engineering Research Center of AI and Robotics</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Multimodal Sentiment Analysis Debiasing via Bias Purification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-05">5 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.05023v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sentiment analysis ‚Ä¢ Multimodal learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal Sentiment Analysis (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly multimodal utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal graph to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual multimodal input, MCIS imagines two counterfactual scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biased observations by comparing factual and counterfactual outcomes. We conduct extensive experiments on several standard MSA benchmarks. Qualitative and quantitative results show the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an essential task in human intention understanding, Multimodal Sentiment Analysis (MSA) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref> attempts to empower machines with the senses of "hearing" <ref type="bibr" target="#b10">[11]</ref> and "seeing" <ref type="bibr" target="#b21">[22]</ref> to mimic human perception of emotions from diverse modalities. Following the traditional likelihood rule, most existing Fig. <ref type="figure">1</ref>: The distribution of (a) sentiment labels and (b) several context words from the training set on the MOSI dataset <ref type="bibr" target="#b58">[59]</ref>.</p><p>studies focus on improving MSA performance by exploiting various strategies, including disentangled representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>, attention-based cross-modal interactions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>, fusion mechanisms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref>, and well-designed auxiliary tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref>. Despite the impressive improvements achieved by numerous works, they all invariably captured harmful dataset biases <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> and suffered from unintended confounders <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>, which are multimodal utterance-level label bias and word-level context bias.</p><p>The harmful label bias usually occurs when the number of training samples for a specific category is more significant than for other categories. For instance, Fig. <ref type="figure">1</ref>(a) illustrates that the positive samples dominate MOSI dataset <ref type="bibr" target="#b58">[59]</ref> compared to the other samples. Worse still, a binary sentiment analysis dataset could have a label distribution of 95% : 5% <ref type="bibr" target="#b4">[5]</ref>. In this case, many previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref> have indicated that such unbalanced data distribution would lead to trained models relying heavily on label bias as statistical shortcuts to make inaccurate predictions. Different from unimodal tasks that potentially convey the adverse effects via specific modalities <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>, most MSA models are poisoned with side effects captured by multimodal representations due to multiple modalities in each sample sharing the same sentiment label <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Moreover, previous studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref> have demonstrated that language modality plays an important role in MSA compared to non-linguistic modalities, i.e., a suitable language model could achieve considerable performance <ref type="bibr" target="#b27">[28]</ref>. Nevertheless, linguistic information is not always beneficial due to the inherent context bias <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>. The fatal context bias generally emerges when trained models exhibit strong spurious correlations between specific categories and context words in language modality. In Fig. <ref type="figure">1</ref>(b), some emotionally ambiguous words appear with imbalanced frequency in negative and positive samples. Consequently, MSA models tend to predict samples containing those words to an incorrect category based on biased statistical information rather than intrinsic textual semantics <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>. For example, Fig. <ref type="figure" target="#fig_1">2(a)</ref> shows the predicted binary classification result from a state-of-the-art (SOTA) model <ref type="bibr" target="#b16">[17]</ref> on the MOSI. As the context words "good" and "very" appear more frequently in the positive than in the negative samples in the training set, the model predicts the testing sample as "positive" via an unreliable association. Therefore, to perform more reason-able sentiment inference, we need to suitably purify and eliminate the prejudicial effects caused by these biases in prior observations, as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>.</p><p>Unlike machines that make biased predictions directly from an inference process by considering prior observations, humans have a natural counterfactual intuition <ref type="bibr" target="#b23">[24]</ref>. Specifically, even though we are born and learn in a biased world, the counterfactual ability <ref type="bibr" target="#b34">[35]</ref> enables us to make unbiased decisions by removing exogenous interference (e.g., label bias under limited observations) and endogenous reason (e.g., language context bias). The underlying mechanism is causality-based: decisions are made by counterfactual inference to pursue a true causal effect rather than a statistical shortcut or spurious correlation. To this end, we depict the counterfactual scenario as follows: Counterfactual MSA: What will the prediction be, if the model does not see the multimodal input or only sees context words in the language modality?</p><p>Intuitively, the counterfactual MSA have two outcomes: (1) the trained model relies purely on the statistical shortcut for prediction under the no-treatment condition of the multimodal input. In this case, Fig. <ref type="figure" target="#fig_1">2(b)</ref> shows that the purified label bias results in a higher probability of "positive" than "negative". <ref type="bibr" target="#b1">(2)</ref> The trained model relies only on the spurious correlation for prediction under the intervention of preserving context words solely. The result contains the pure side effect obtained by distilling the context bias. Motivated by the above observations, we propose a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework to mitigate the deleterious impact of two types of dataset biases. Concretely, we first design a tailored causal graph for MSA to diagnose causalities among variables and identify the dataset biases as unintended confounders. The proposed framework is parameter-free and training-free, meaning that MCIS accommodates already-trained models following biased vanilla training via our generalized causal graph. During the inference phase, MCIS intervenes with confounding multimodal inputs via backdoor adjustment theory <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref> to mimic the two counterfactual outcomes described above. By subtracting the counterfactual outcomes of the pure dataset biases, MCIS consistently improves the performance of SOTA models with unbiased predictions.</p><p>The main contributions are summarized as follows:</p><p>-We are the first to identify and disentangle the label and context biases in the MSA task from a novel causal inference perspective. Based on innate human counterfactual intuition, we empower models to achieve unbiased predictions in biased observations.  sentiment-related representations, such as visual <ref type="bibr" target="#b57">[58]</ref> and acoustic signals <ref type="bibr" target="#b44">[45]</ref>.</p><p>Driven by learning-based techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, mainstream MSA studies follow two aspects: representation learning and multimodal fusion. Multimodal representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref> attends to mitigating modality gap or information redundancy to obtain refined modality semantics. For instance, Hazarika et al . <ref type="bibr" target="#b9">[10]</ref> advocated projecting each modality into modality-invariant and -specific spaces to learn complementary information. For multimodal fusion, previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b56">57]</ref> have explored sophisticated fusion strategies and mechanisms to obtain effective representations. As a typical example, Tsai et al . <ref type="bibr" target="#b36">[37]</ref> achieved potential adaption fusion from one modality to another based on multimodal transformers. Despite the impressive improvements achieved by previous studies following traditional likelihood estimation, they invariably ignored the adverse effects of the dataset biases, resulting in biased predictions. In comparison, we achieve unbiased decisions by exploiting causality-based counterfactual thinking. The proposed framework significantly improves the performance of existing models without any complex network designs and parameters.</p><p>Causal Inference. Causal inference is a tool that seeks actual effects in a specific phenomenon <ref type="bibr" target="#b24">[25]</ref>. Currently, the mainstream causal inference studies applied to deep learning consist of two aspects: intervention <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref> and counterfactuals <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52]</ref>. Intervention is an operation that alters original data distribution to discover causal effects <ref type="bibr" target="#b6">[7]</ref>. Counterfactuals depict imagined outcomes produced by factual variables under different treatments <ref type="bibr" target="#b25">[26]</ref>. Our study focuses on obtaining counterfactual outcomes via intervention. Causal inference can remove confounders in data and learn actual causal effects instead of spurious associations, so it has been widely used in many downstream tasks to improve the models' performance, including visual question answer <ref type="bibr" target="#b23">[24]</ref>, natural language understanding <ref type="bibr" target="#b35">[36]</ref>, and scene graph generation <ref type="bibr" target="#b34">[35]</ref>. A recent study <ref type="bibr" target="#b32">[33]</ref> focused on designing an additional model to capture the harmful effect of textual semantics. However, they ignored the label bias and failed to disentangle the main content and context at the word level, thus incapable of language bias ascription. Different from previous efforts <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>, this is the first work to identify both label bias and context bias in the MSA task from a causal perspective. Our framework effectively eliminates the side effects of dataset biases from multimodal inputs, which makes a step towards unbiased prediction in this field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>The proposed MCIS framework is illustrated in Fig. <ref type="figure" target="#fig_3">4(b)</ref>. Concretely, MCIS allows already-trained models to preserve harmful dataset biases via biased conventional learning. Given a factual multimodal input in the inference phase, MCIS imagines two types of multimodal counterfactual inputs to obtain two counterfactual outputs: purified label bias and context bias. Eventually, MCIS performs a bias elimination strategy in adaptive proportions to obtain unbiased counterfactual predictions by comparing factual and counterfactual outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural Causal Graph in MSA</head><p>Problem Formalization. Given multimodal utterance inputs from video segments, MSA aims to predict sentiment scores by learning multimodal models F(‚Ä¢) using language (l), audio (a), and visual (v) modalities. This conventional training procedure is represented as ≈∑i = F(l, a, v), where ≈∑i ‚àà R is a sentimental intensity variable. Aligned with previous mainstream works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56]</ref>, we regard MSA as a regression task to ensure a fair comparison. Cause-Effect Look at MSA. To diagnose the causal relationships among variables, we formulate a causal graph to summarize the MSA framework. Here, we represent a random variable as a capital letter (e.g., L), and denote its observed value as a lowercase letter (e.g., l). Theoretically, a causal graph G = {N , E} is considered a directed acyclic graph, which represents how a set of variables N convey causal effects through the causal links E. It provides an intuitive reference to causal correlations for counterfactual analysis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> and causal intervention <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>. In Fig. <ref type="figure" target="#fig_2">3(a)</ref>, there are six variables in MSA causal graph, including language modality L, audio modality A, visual modality V , multimodal representation M , harmful confounders Z, and prediction Y . From causal theories <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>, the adverse dataset biases as the confounders to "poison" models. All causal relationships among them are explained as follows: ‚ñ∂ Link (L, A, V ) ‚Üí M ‚Üí Y . Following biased learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>, the causal path (L, A, V ) ‚Üí M indicates that the multimodal inputs (L, A, V ) produce the final multimodal representation M through MSA models F(‚Ä¢) :</p><formula xml:id="formula_0">m = Œæ M (L = l, A = a, V = v),<label>(1)</label></formula><p>where Œæ M (‚Ä¢) is a fusion strategy that depends on different models (e.g., Transformer <ref type="bibr" target="#b9">[10]</ref> or concatenation <ref type="bibr" target="#b36">[37]</ref>). Subsequently, the link M ‚Üí Y reflects that MSA models estimate the desired prediction Y based on pure M . ‚ñ∂ Link M ‚Üê Z ‚Üí Y . According to <ref type="bibr" target="#b26">[27]</ref>, the confounders Z are the common cause of M and Y . The dataset biases follow the backdoor causal path M ‚Üê Z ‚Üí Y to establish spurious associations to prevent the models from pursuing true causal effects, which we should eliminate. Without loss of generality, the nodes (L, A, V ) are omitted for simplicity since they are not directly affected by Z. The new causal graph is illustrated in Fig. <ref type="figure" target="#fig_2">3(b</ref>). Existing models rely on the likelihood P (Y |M ) following the new graph. This process is formulated via the Bayes rule <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_1">F(m) = P (Y |M ) = z P (Y |M, z)P (z|M ),<label>(2)</label></formula><p>where z is any confounder caused by the label or context bias. In this case, MSA models would invariably focus on the statistical shortcut or spurious correlation to perform biased predictions, significantly limiting their performance. To remove the detrimental effect caused by z, our insight is to embrace backdoor adjustment <ref type="bibr" target="#b24">[25]</ref>, i.e., predicting an actively intervened outcome via the do-operator <ref type="bibr" target="#b6">[7]</ref>. As a typical causal intervention, do(‚Ä¢) prevents the effect of parent nodes that cause variables from the non-causal direction, i.e., Z ‚Üí M . As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(c), the intervention cuts the causal path from Z to m, i.e., m is no longer affected by Z.</p><p>In practice, we intervene m based on counterfactual embeddings under different scenarios to purify the pure label and context biases in Secs. 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label Bias Purification</head><p>As Fig. <ref type="figure" target="#fig_3">4</ref>(a) shows, the unbalanced label distribution (i.e., "positive" dominates the training data over "negative") misleads MSA models to establish non-causal associations between the input samples and the positive category. In this case, MSA models would give predictions based on statistical shortcuts even though the contents of the multimodal testing samples are not observed <ref type="bibr" target="#b7">[8]</ref>. To implement the theoretical do(‚Ä¢) intervention, we utilize m to denote the imagined counterfactual multimodal representation. The intervention-based counterfactual outcome is as follows:</p><formula xml:id="formula_2">P (Y |do(M )) = P (Y |M = m) = F( m), m = Œæ M (L = l, A = √¢, V = v).<label>(3)</label></formula><p>Here l, √¢, and v represent the no-treatment condition where l, a, and v are not given. As MSA models cannot "see" any multimodal inputs after the intervention, the counterfactual output F( m) actually reflects the purely adverse effect from the trained models, i.e., the label bias captured by Z. Considering that neural network models cannot deal with void inputs, we utilize average features over the entire training set as counterfactual embeddings for different modalities:</p><formula xml:id="formula_3">l = 1 N N i l i , √¢ = 1 N N i a i , v = 1 N N i v i , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where N is the number of training samples. Empirically, the average embedding usually produces a distribution similar to the ideal bias and forces the models to decouple the outcome of the harmful bias as humans do <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Context Bias Purification</head><p>Motivated by human decision-making that combines exogenous and endogenous reasons <ref type="bibr" target="#b38">[39]</ref>, language utterances can be summarized in the main content words and context words. The main content words provide valuable semantics clues (e.g., emotionally-beneficial semantics). Conversely, the context words (e.g., stop words or a part of adjectives) as the confounders trick the models into focusing on spurious correlations between semantically-unimportant contexts and specific categories (e.g., good ‚Üî positive mapping). To this end, we use m to achieve another counterfactual outcome with only context words:</p><formula xml:id="formula_5">P (Y |do(M )) = P (Y |M = m) = F( m), m = Œæ M (L = l, A = »É, V = v).<label>(5)</label></formula><p>Here l denotes counterfactual word embedding where the main content words are masked. The mask operation process is as follows:</p><formula xml:id="formula_6">‚àÄw j ‚àà l, w j ‚Üê-[MASK] if w j ‚àà l content , w j ‚Üê-w j if w j ‚àà l context ,<label>(6)</label></formula><p>where <ref type="bibr">[MASK]</ref> symbol is a special token to mask a single word w j . Meanwhile, »É and v denote unseen empty embeddings, a.k.a., zero feature embeddings. In this situation, MSA models could only rely on visible context words to make biasbased predictions. Essentially, the counterfactual outcome F( m) reflects the pure side effect from the trained vanilla models and word-level harmful context bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bias Elimination Strategy</head><p>Thanks to humans' innate counterfactual intuition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>, we can wisely reveal actual causal effects among variables in biased observations rather than superficial connections. The human inference process for unbiased decisions is essentially achieved by comparing factual and counterfactual outcomes <ref type="bibr" target="#b25">[26]</ref>. To block the transfer of biases from the training data to the inference process, we imitate such human intuition to introduce an operationally simple yet empirically powerful subtraction operation (i.e., bias elimination strategy). The debiased prediction via the strategy is as follows:</p><formula xml:id="formula_7">‚Ñµ(m) = F(m) -( Œª F( m) + Œª F( m)),<label>(7)</label></formula><p>where F(m) and ‚Ñµ(m) correspond to the traditional factual prediction and counterfactual prediction, respectively. F( m) and F( m) are the label bias and context bias purified from the poisoned models. Two adaptive trade-off parameters, Œª and Œª, are applied to measure the extent of label bias and context bias. Since different datasets suffer from varying extent of biases, the grid search strategy is utilized on the validation set to estimate the extent to which the two biases poison the models. We implement the search for Œª and Œª in a two-dimensional space of a specific interval:</p><formula xml:id="formula_8">Œª * , Œª * = arg max Œª, Œª‚àà[Œ±,Œ≤] Œ¶ D ‚Ñµ(m| Œª, Œª) ,<label>(8)</label></formula><p>where [Œ±, Œ≤] is the search interval. Œ¶(‚Ä¢) is a function used for calculating a specific metric that measures the model's performance on the validation set D.</p><p>The evaluation metric is the weighted F1-score, which is the balanced harmonic mean of precision and recall and can excellently reflect the extent of the dataset biases, especially for the imbalanced data. To reduce the invalid computational overhead during the bias elimination process, we employ a coarse-to-fine grid search strategy to perform a search by gradually narrowing the search interval and step size. As two dataset-level parameters, they are searched only once for each validation set and can be used in inference for all testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets. Here, we conduct experiments on two different scales of datasets that show significant label and context biases <ref type="bibr" target="#b32">[33]</ref>. MOSI <ref type="bibr" target="#b58">[59]</ref> is a realistic dataset comprising 2,199 opinion video clips collected from YouTube. There are 1,284, 229, and 686 video clips in train, valid, and test data, respectively. MOSEI <ref type="bibr" target="#b57">[58]</ref> benchmark contains 23,453 annotated video segments from over 1,000 speakers and 250 topics. There are a total of 16,326, 1,871, and 4,659 video segments in training, validation, and testing sets, respectively. Each sample has a label for both datasets from -3 (strongly negative) to +3 (strongly positive). Evaluation Metrics. Following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>, we leverage various metrics to evaluate the MCIS framework's performance, including seven-class classification accuracy (Acc-7) meaning the proportions of correct predicted scores in seven intervals from -3 to +3, binary classification accuracy (Acc-2), and the weighted F1 score computed for positive/negative classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Zoo</head><p>To fully evaluate the effectiveness of MCIS across different methods, we select five representative and reproducible state-of-the-art (SOTA) models. Concretely, MulT <ref type="bibr" target="#b36">[37]</ref> learns element correlations among modalities via paired crossmodal attention interactions. MISA <ref type="bibr" target="#b9">[10]</ref> projects each modality into two distinct subspaces to learn the discrepancy and consistency across modalities separately. CubeMLP <ref type="bibr" target="#b31">[32]</ref> utilizes three independent multi-layer perceptron units for feature-mixing on three axes. MMIM <ref type="bibr" target="#b8">[9]</ref> maximizes the mutual information during multimodal fusion to maintain task-related information. DMD <ref type="bibr" target="#b16">[17]</ref> introduces cross-modal distillations to facilitate the transfer of informative semantics from strong to weak modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Feature Extraction. Following the original protocols of the models above, the audio and visual features are provided by MOSI and MOSEI. The language embeddings are extracted by the pre-trained BERT <ref type="bibr" target="#b3">[4]</ref>, whether fine-tuning depends on the vanilla settings of different methods. Moreover, we employ the Python NLTK toolkit to tokenize sentences into word lists and then extract the main content words that may affect the semantics in the transcripts. The average mask ratio of the main content words is 68.96%. For the grid search strategy, the search step and search interval are 0.5 and [-2.0, 2.0] in the coarse search process. In the fine search process, the search step is 0.1, while the search interval depends on the results of the coarse search process. Experimental Setup. We re-implement these five SOTA models based on the public codebase and combine them with our MCIS framework. All models are reproduced on NVIDIA Tesla V100 GPUs. For impartiality, the training settings of these models (e.g., loss function, batch size, learning rate strategy, and other hyper-parameters) are consistent with the details reported in original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-art Methods</head><p>We compare the MCIS-based models with recent competitive methods, including TFN <ref type="bibr" target="#b56">[57]</ref>, LMF <ref type="bibr" target="#b20">[21]</ref>, MFM <ref type="bibr" target="#b37">[38]</ref>, ICCN <ref type="bibr" target="#b33">[34]</ref>, MAG-BERT <ref type="bibr" target="#b30">[31]</ref>, FDMER <ref type="bibr" target="#b43">[44]</ref>, and Self-MM <ref type="bibr" target="#b55">[56]</ref>. The results on MOSI and MOSEI are reported in Tables <ref type="table" target="#tab_5">1&amp;2</ref>.</p><p>The key observations are as follows. (i) The models with MCIS significantly and consistently outperform the vanilla versions by large margins on most evaluation metrics for both datasets. In particular, the MCIS-based MMIM <ref type="bibr" target="#b8">[9]</ref> achieves new SOTAs with the Acc-7/Acc-2/F1 scores of 47.9%/86.6%/86.5% on MOSI. Thanks to MCIS, the distillation-based DMD <ref type="bibr" target="#b16">[17]</ref> yields the best results on MOSEI with affluent improvements of 1.3%, 1.7%, and 1.6% on these three metrics. The performance gains across methods with different representation learning patterns <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> and fusion strategies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37]</ref> confirm the usefulness and generalizability of our framework.</p><p>(ii) Compared to existing models that obtain inadequate results (average about 0.54%‚àº1.26% gain across all metrics) via complex structures and numerous parameters <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, MCIS can easily achieve superior improvements (average about 0.94%‚àº 1.76% gain across all metrics) by removing harmful biases only at the inference phase in a parameter-free manner. In practice, our framework is cost-effective compared to training a new SOTA model from scratch since the time overhead is reduced by about 26 times on aver- age. The better results show that these biases are the ignored "culprits" and the importance of counterfactual debiasing.</p><p>(iii) Furthermore, we find that the MCIS-based models provide better improvements on MOSEI (average about 1.63% gain across models) than on MOSI (average about 1.16% gain across models). The phenomenon potentially derives from extensive data samples in the large-scale dataset beneficial to trained models preserving the two biases that obey the ideal distribution, thus facilitating MCIS to purify and mitigate the adverse effects more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We perform systematic ablation studies using the MCIS-based models on MO-SEI. Comprehensive experiments aim to evaluate the different designs and mechanisms in the proposed MCIS. Analysis of Different Dataset Biases. Table <ref type="table" target="#tab_6">3</ref> provides investigations of two types of bias eliminations and grid search strategy (GSS). (i) Firstly, the label and context bias eliminations are retained separately to verify the effect of the distinct biases. The gain drops for all metrics reveal that it is indispensable to simultaneously remove statistical shortcuts and spurious correlations. The core explanation is that the purified label bias provides a sample-agnostic global offset and the purified context bias provides utterance-specific local offsets to correct for the predicted space, allowing the trained models to sidestep the interference of harmful biases in the observed data. (ii) Another finding is that the impact of context bias is more severe than label bias, implying that misleading or unfair context words more easily mislead the trained models. This observation provides pertinent evidence for the dominance of language modality in MSA <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>. (iii) When our GSS is eliminated (i.e., Œª = Œª = 1), all gain degradation indicates that proper mitigation of varying degrees of biases is essential. Impact of MCE in Label Bias. Multimodal Counterfactual Embeddings (MCE) play an important role in obtaining the intervened outcomes based on the purified biases. (i) In practice, we investigate the necessity of Language, Audio, and Visual Counterfactual Embeddings (L/A/VCE) separately. From the decreased results in Table <ref type="table" target="#tab_7">4</ref>, the incomplete counterfactual embeddings (i.e., the absence of whichever of L/A/VCE) would impede the biased models from producing the multimodal representation that benefits from precise intervention, and then fail to imagine the bias-based outcome purely. According to Fig. <ref type="figure" target="#fig_2">3</ref>, the reason could be that M is confounded by the harmful effects of statistical shortcuts conveyed jointly by links to different modalities i.e., (L, A, V ) ‚Üí M . Therefore, it takes sufficient intervention with each modality to purify the effective label bias. (ii) Across all MCIS-based models, the worse deterioration is observed with the elimination of LCE. Meanwhile, the impact of A/VCE on gain depends on different models, e.g., removing ACE is less damaging to the performance of MulT and MMIM as well as VCE is slightly impairing MISA, CubeMLP, and DMD. (iii) Additionally, we empirically provide a candidate assumption that the average features from three modalities are replaced with the Random Counterfactual Embeddings (RCE), which are initialized by random distribution. The poor results are inevitable because random guesses potentially fail to produce a stable distribution similarly distributed with the ideal bias. Impact of MCE in Context Bias. Intuitively, the core of context bias elimination is masking the main content words and forcing the models to focus only on the spurious correlations provided by the context words. To explore this, (i) we perform the word non-masking (w/o Mask), all masking (w/ All Mask), and random masking (w/ Random Mask) separately before converting the transcripts And if that's the direction we're headed, then that's not the direction I want to go. into word embeddings via the pre-trained BERT in Table <ref type="table" target="#tab_8">5</ref>. The decreased results in F1 scores confirm three explanations: (1) Due to the language modality unavailability in all masking, the Context Bias Elimination (CBE) process does not impact linguistic effects. Despite the bias of vanilla models, the main content words contribute more valuable gains. (2) Instead, CBE purifies the effects of both good semantics and bad bias in non-masking, leading to worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Sample from MOSEI</head><note type="other">Language Audio Visual</note><p>The poor results for random masking than for all masking suggest that CBE probably over-eliminates meaningful clues as the main content words dominate.</p><p>(ii) Furthermore, the original features of different training samples are retained when ACE and VCE are removed separately. The most gain drops suggest that our zero feature embedding assumption guarantees a safe estimation for the purification of pure word-level context bias. (iii) As an alternative to L/A/VCE, the worst performance from all metrics with RCE verifies the rationality of the proposed embedding paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>Case Study of Counterfactual Learning. Fig. <ref type="figure" target="#fig_5">5</ref> shows a counterfactual example from each testing set on MOSI and MOSEI, respectively. Specifically, we provide the sentiment intensity scores of positive/negative evaluation results from vanilla DMD, two types of counterfactual outputs, and the counterfactual predictions. The corresponding label and context word distributions for the display samples intuitively show the presence of the dataset biases. Evidently, MCIS corrects the baseline predictions and gives reasonable sentiment polarities. Taking Case 1 (Fig. <ref type="figure" target="#fig_5">5</ref>(a)) as an example, the vanilla model obtains a falsely positive polarity, which is misled by the dataset biases. According to the two counterfactual outputs corresponding to the purified biases, the biased baseline results suffer from two deleterious effects: (1) the statistical shortcut caused by the large proportion of "positive" labels; (2) the spurious correlation between the context words (e.g., "also", "very") and "positive" category. Thanks to the proposed MCIS, we can empower the model to think twice and make unbiased predictions by comparing factual and counterfactual outcomes. Distribution Differences of Sentiment Scores. The distribution differences of sentiment scores on MOSI and MOSEI testing sets are displayed in Fig. <ref type="figure" target="#fig_6">6</ref>(a) and Fig. <ref type="figure" target="#fig_6">6</ref>(b), respectively. (i) Macroscopically, the predicted score distribution of the MCIS-based model is more compact with the ground truth distribution, indicating that MCIS can effectively correct prediction errors around ground truths. (ii) In practice, our framework mitigates the overall prediction gap caused by samples with outlier-predicted scores while maintaining correct predictions for most samples. For instance, the MCIS-based DMD successfully corrects about 90% and 93% of the predicted sentiment scores in samples with changes in sentiment polarities on MOSI and MOSEI. (iii) Microscopically, MCIS differs in its debiasing effect on different samples, depending on the misleading extent of the context words in the samples. In short, our method contributes to a meaningful step towards the unbiased estimation of existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigate and disentangle the dataset biases that have long poisoned MSA models from a causal inference perspective. As a model-agnostic causality-based framework, the proposed MCIS eliminates the detrimental effects caused by these biases via imitating human counterfactual intuition. Comprehensive experiments demonstrate that the MCIS-based models achieve better performance than their biased counterparts. Future Work. We plan to equip MCIS with modality reconstruction techniques to cope with potential modality missingness in realistic applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: An example of multimodal sentiment analysis. (a) Likelihood-based biased prediction from re-implemented model DMD [17]. (b) Unbiased prediction from the same model in the proposed framework. Binary classification results for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: (a) The tailored causal graph for MSA. (b) The simplified causal graph for MSA. (c) Comparison between factual MSA and counterfactual MSA. White nodes are at the value M = m while gray nodes are at the value M = m or M = m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (a) The biased learning of MSA models follows the factual training. (b) The architecture of our MCIS framework. MCIS compares factual and counterfactual outcomes for different multimodal input treatments. By subtracting the label and context biases, MCIS can achieve unbiased predictions from biased observations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Case study of counterfactual learning on MOSI and MOSEI. We report the binary evaluation results from the DMD [17] with our MCIS for the intuitive display. Label/Context Word Distribution: the imbalanced distribution of sentiment labels and context words in positive and negative categories comes from the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>SamplesFig. 6 :</head><label>6</label><figDesc>Fig.6: Distribution differences of sentiment scores for the testing set (sorted) on (a) MOSI and (b) MOSEI. The blue dots represent the predicted scores from the baseline DMD<ref type="bibr" target="#b16">[17]</ref>, while the red dots represent the predicted scores from the MCIS-based DMD. The more compact the distribution of predicted sentiment scores and ground truths, the better the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Believe nothing you hear, and only one half that you see." -Edgar Allan Poe, The System of Doctor Tarr and Professor Fether</figDesc><table><row><cell>1 Introduction</cell></row></table><note><p>"</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-</head><label></label><figDesc>Our causality-based MCIS is general and suitable for different MSA architectures and fusion mechanisms. -Comprehensive experiments on several MSA benchmarks demonstrate the effectiveness of our framework.</figDesc><table><row><cell>2 Related Work</cell></row></table><note><p><p><p>Multimodal Sentiment Analysis. Instead of modeling linguistic information alone</p><ref type="bibr" target="#b28">[29]</ref></p>, MSA aims to integrate additional non-linguistic modalities to</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>learn Language: Audio: Visual: Ground Truth: Negative (a) Prediction: Positive</head><label></label><figDesc></figDesc><table><row><cell>·àò ùúÜ*</cell><cell>·àö ùúÜ*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Factual Prediction Label Bias Context Bias Prediction: Negative (b) MSA Model</head><label></label><figDesc></figDesc><table><row><cell>I see the movie that's obviously</cell></row><row><cell>had a very big budget and it is</cell></row><row><cell>like not even good.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison results on the MOSI testing set. All models use the BERT-based word embedding. ‚Ä†: reproduced results from public code with hyper-parameters provided in original papers. The improved results are marked in bold.</figDesc><table><row><cell>Models</cell><cell cols="3">Acc-7 (%) Acc-2 (%) F1 (%)</cell></row><row><cell>TFN [57]</cell><cell>34.9</cell><cell>80.8</cell><cell>80.7</cell></row><row><cell>LMF [21]</cell><cell>33.2</cell><cell>82.5</cell><cell>82.4</cell></row><row><cell>MFM [38]</cell><cell>35.4</cell><cell>81.7</cell><cell>81.6</cell></row><row><cell>ICCN [34]</cell><cell>39.0</cell><cell>83.0</cell><cell>83.0</cell></row><row><cell>MAG-BERT [31]</cell><cell>43.6</cell><cell>84.4</cell><cell>84.6</cell></row><row><cell>FDMER [44]</cell><cell>44.1</cell><cell>84.6</cell><cell>84.7</cell></row><row><cell>Self-MM [56]</cell><cell>45.8</cell><cell>84.8</cell><cell>84.9</cell></row><row><cell>MulT  ‚Ä† (ACL'19) [37]</cell><cell>42.6</cell><cell>84.1</cell><cell>83.9</cell></row><row><cell>MulT + MCIS</cell><cell>43.5</cell><cell>85.5</cell><cell>85.2</cell></row><row><cell>MISA  ‚Ä† (ACM MM'20) [10]</cell><cell>42.1</cell><cell>82.3</cell><cell>82.6</cell></row><row><cell>MISA + MCIS</cell><cell>42.0</cell><cell>83.7</cell><cell>84.1</cell></row><row><cell>MMIM  ‚Ä† (EMNLP'21) [9]</cell><cell>46.4</cell><cell>85.5</cell><cell>85.4</cell></row><row><cell>MMIM + MCIS</cell><cell>47.9</cell><cell>86.6</cell><cell>86.5</cell></row><row><cell>CubeMLP  ‚Ä† (ACM MM'22) [32]</cell><cell>44.5</cell><cell>84.7</cell><cell>84.6</cell></row><row><cell>CubeMLP + MCIS</cell><cell>45.7</cell><cell>85.9</cell><cell>85.8</cell></row><row><cell>DMD  ‚Ä† (CVPR'23) [17]</cell><cell>45.3</cell><cell>85.1</cell><cell>85.1</cell></row><row><cell>DMD + MCIS</cell><cell>46.5</cell><cell>86.3</cell><cell>86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison results on the MOSEI testing set. All models use the BERT-based word embedding. ‚Ä†: reproduced results from public code with hyper-parameters provided in original papers. The improved results are marked in bold.</figDesc><table><row><cell>Models</cell><cell cols="3">Acc-7 (%) Acc-2 (%) F1 (%)</cell></row><row><cell>TFN [57]</cell><cell>50.2</cell><cell>82.5</cell><cell>82.1</cell></row><row><cell>LMF [21]</cell><cell>48.0</cell><cell>82.0</cell><cell>82.1</cell></row><row><cell>MFM [38]</cell><cell>51.3</cell><cell>84.4</cell><cell>84.3</cell></row><row><cell>ICCN [34]</cell><cell>51.6</cell><cell>84.2</cell><cell>84.2</cell></row><row><cell>MAG-BERT [31]</cell><cell>52.7</cell><cell>84.8</cell><cell>84.7</cell></row><row><cell>FDMER [44]</cell><cell>54.1</cell><cell>86.1</cell><cell>85.8</cell></row><row><cell>Self-MM [56]</cell><cell>53.5</cell><cell>85.0</cell><cell>84.9</cell></row><row><cell>MulT  ‚Ä† (ACL'19) [37]</cell><cell>52.3</cell><cell>82.7</cell><cell>82.5</cell></row><row><cell>MulT + MCIS</cell><cell>54.1</cell><cell>84.3</cell><cell>84.0</cell></row><row><cell>MISA  ‚Ä† (ACM MM'20) [10]</cell><cell>52.1</cell><cell>84.4</cell><cell>84.2</cell></row><row><cell>MISA + MCIS</cell><cell>53.6</cell><cell>85.8</cell><cell>85.7</cell></row><row><cell>MMIM  ‚Ä† (EMNLP'21) [9]</cell><cell>53.1</cell><cell>85.1</cell><cell>85.0</cell></row><row><cell>MMIM + MCIS</cell><cell>54.5</cell><cell>86.7</cell><cell>86.6</cell></row><row><cell>CubeMLP  ‚Ä† (ACM MM'22) [32]</cell><cell>52.7</cell><cell>84.2</cell><cell>83.7</cell></row><row><cell>CubeMLP + MCIS</cell><cell>54.2</cell><cell>86.2</cell><cell>85.9</cell></row><row><cell>DMD  ‚Ä† (CVPR'23) [17]</cell><cell>53.9</cell><cell>85.6</cell><cell>85.5</cell></row><row><cell>DMD + MCIS</cell><cell>55.2</cell><cell>87.3</cell><cell>87.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results of different dataset biases. We provide comprehensive results for five MCIS-based SOTA models on the MOSEI testing set. Similar trends are also observed on the MOSI. "w/o" is short for the without.</figDesc><table><row><cell>Designs/Mechanisms</cell><cell cols="3">MulT + MCIS MISA + MCIS MMIM + MCIS CubeMLP + MCIS DMD + MCIS Acc-7 Acc-2 F1 Acc-7 Acc-2 F1 Acc-7 Acc-2 F1 Acc-7 Acc-2 F1 Acc-7 Acc-2 F1</cell></row><row><cell>Full Framework</cell><cell cols="3">54.1 84.3 84.0 53.6 85.8 85.7 54.5 86.7 86.6 54.2 86.2 85.9 55.2 87.3 87.1</cell></row><row><cell cols="2">w/o Label Bias Elimination 53.8 83.7 83.5 53.2 85.3 85.2 54.2 86.0 85.9 54.0 85.7</cell><cell>85.5</cell><cell>54.8 86.7 86.6</cell></row><row><cell cols="2">w/o Context Bias Elimination 52.8 83.2 83.1 52.5 84.7 84.7 53.3 85.5 85.4 53.2 84.8</cell><cell>84.5</cell><cell>54.2 86.1 85.9</cell></row><row><cell>w/o Grid Search Strategy</cell><cell>52.6 83.0 82.8 51.5 83.8 83.6 52.3 84.4 84.2 52.8 84.4</cell><cell>84.0</cell><cell>53.7 86.0 85.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results of multimodal counterfactual embeddings in the label bias. "L/A/V/RCE" stands for language, audio, visual, and random counterfactual embeddings, respectively.</figDesc><table><row><cell>Models</cell><cell cols="5">Metrics Full w/o LCE w/o ACE w/o VCE w/ RCE</cell></row><row><cell>MulT [37] + MCIS</cell><cell>Acc-2 (%) 84.3 F1 (%) 84.0</cell><cell>83.9 83.7</cell><cell>84.2 83.9</cell><cell>84.1 83.8</cell><cell>83.6 83.3</cell></row><row><cell>MISA [10] + MCIS</cell><cell>Acc-2 (%) 85.8 F1 (%) 85.7</cell><cell>85.4 85.4</cell><cell>85.6 85.5</cell><cell>85.7 85.6</cell><cell>85.1 85.0</cell></row><row><cell>MMIM [9] + MCIS</cell><cell>Acc-2 (%) 86.7 F1 (%) 86.6</cell><cell>86.2 86.1</cell><cell>86.5 86.5</cell><cell>86.4 86.2</cell><cell>85.8 85.7</cell></row><row><cell>CubeMLP [32] + MCIS</cell><cell>Acc-2 (%) 86.2 F1 (%) 85.9</cell><cell>85.8 85.6</cell><cell>86.0 85.7</cell><cell>86.1 85.9</cell><cell>85.5 85.1</cell></row><row><cell>DMD [17] + MCIS</cell><cell>Acc-2 (%) 87.3 F1 (%) 87.1</cell><cell>86.8 86.7</cell><cell>87.0 86.8</cell><cell>87.1 87.0</cell><cell>86.5 86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study results of multimodal counterfactual embeddings in the context bias. "Mask" means the mask operation in Eq.<ref type="bibr" target="#b5">(6)</ref>. "w/" and "w/o" are short for the with and without, respectively. We only report F1 scores for visual clarity.</figDesc><table><row><cell>Designs</cell><cell>MulT [37] + MCIS</cell><cell>MISA [10] + MCIS</cell><cell>MMIM [9] + MCIS</cell><cell>CubeMLP [32] + MCIS</cell><cell>DMD [17] + MCIS</cell></row><row><cell>Full Framework</cell><cell>84.0</cell><cell>85.7</cell><cell>86.6</cell><cell>85.9</cell><cell>87.1</cell></row><row><cell>w/o Mask</cell><cell>83.2</cell><cell>84.9</cell><cell>85.6</cell><cell>84.7</cell><cell>86.3</cell></row><row><cell>w/ All Mask</cell><cell>83.6</cell><cell>85.2</cell><cell>86.1</cell><cell>85.4</cell><cell>86.7</cell></row><row><cell>w/ Random Mask</cell><cell>83.3</cell><cell>84.6</cell><cell>85.9</cell><cell>85.0</cell><cell>86.2</cell></row><row><cell>w/o ACE</cell><cell>83.7</cell><cell>85.6</cell><cell>86.5</cell><cell>85.8</cell><cell>86.9</cell></row><row><cell>w/o VCE</cell><cell>83.9</cell><cell>85.4</cell><cell>86.4</cell><cell>85.7</cell><cell>87.1</cell></row><row><cell>w/ RCE</cell><cell>82.8</cell><cell>84.4</cell><cell>85.4</cell><cell>84.3</cell><cell>85.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2021ZD0113503</rs> and in part by the <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> under Grant <rs type="grantNumber">2021SHZDZX0103</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hhJ68P9">
					<idno type="grant-number">2021ZD0113503</idno>
				</org>
				<org type="funding" xml:id="_vH3SmE5">
					<idno type="grant-number">2021SHZDZX0103</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Miss: A generative pretraining and finetuning approach for med-vqa</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05163</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Causal intervention for subjectdeconfounded facial action unit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07935</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring and mitigating unintended bias in text classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causalm: Causal model explanation through counterfactual language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Symposium: The causal theory of perception. Proceedings of the Aristotelian Society</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supplementary Volumes</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00412</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Misa: Modality-invariant and-specific representations for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2020">2020) 2, 4, 5, 6, 9, 10, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quality-aware bag of modulation spectrum features for robust speech emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards simultaneous segmentation of liver tumors and intrahepatic vessels via cross-attention mechanism</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Text-oriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13205</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified self-distillation framework for multimodal sentiment analysis with uncertain missing modalities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards robust multimodal sentiment analysis under uncertain signal missing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correlation-decoupled knowledge distillation for multimodal sentiment analysis with incomplete modalities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupled multimodal distilling for emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023) 2, 4, 9, 10, 11</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t just listen, use your imagination: Leveraging visual common sense for non-visual tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05087</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<title level="m">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain invariant feature learning for speaker-independent speech emotion recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A cause-effect look at language bias</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2021) 2, 3, 4, 5, 8</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2006">2009) 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Models, reasoning and inference</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CambridgeUni-versityPress</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>P√≥czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentic lda: Improving on lda with semantic similarity for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Counterfactual inference for text classification debiasing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5434" to="5445" />
		</imprint>
	</monogr>
	<note>2021) 2, 3, 4, 8</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cubemlp: An mlp-based model for multimodal sentiment analysis and depression estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning for out-of-distribution multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2020) 3, 4, 5, 7</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Debiasing nlu models via causal intervention and counterfactual reasoning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting (ACL)</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting (ACL)</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019">2019. 2019) 2, 4, 5, 6, 9, 10, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06176</idno>
		<title level="m">Learning factorized multimodal representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cognitive neuroscience of human counterfactual reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Van Hoeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Barbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hateful symbols or hateful people? predictive features for hate speech detection on twitter</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Student Research Workshop</title>
		<meeting>the NAACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A text-centered shared-private framework via cross-modal prediction for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context de-confounded emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contextual and cross-modal interaction for multi-modal speech emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emotion recognition for multiple context awareness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aide: A vision-driven multi-view, multi-modal, multi-tasking dataset for assistive driving perception</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning modality-specific and-agnostic representations for asynchronous multimodal language sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards asynchronous multimodal signal interaction and fusion via tailored transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>IEEE Signal Processing Letters</publisher>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Towards multimodal human intention understanding debiasing via subject-deconfounding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05025</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust emotion recognition in context debiasing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How2comm: Communication-efficient and collaboration-pragmatic multi-agent perception</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">What2comm: Towards communication-efficient collaborative perception via feature decoupling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th ACM International Conference on Multimedia</title>
		<meeting>the 31th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<title level="m">Tensor fusion network for multimodal sentiment analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
