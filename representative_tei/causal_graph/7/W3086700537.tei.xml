<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unfairness Discovery and Prevention For Few-Shot Regression</title>
				<funder ref="#_H5ks7P3 #_PyeASQW">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-09-23">23 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>chen.zhao@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
							<email>feng.chen@utdallas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unfairness Discovery and Prevention For Few-Shot Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-23">23 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.11406v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal Bayesian network</term>
					<term>statistic parity</term>
					<term>fewshot meta-learning</term>
					<term>fairness generalization</term>
					<term>bias discovery and prevention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study fairness in supervised few-shot metalearning models that are sensitive to discrimination (or bias) in historical data. A machine learning model trained based on biased data tends to make unfair predictions for users from minority groups. Although this problem has been studied before, existing methods mainly aim to detect and control the dependency effect of the protected variables (e.g. race, gender) on target prediction based on a large amount of training data. These approaches carry two major drawbacks that (1) lacking showing a global cause-effect visualization for all variables; (2) lacking generalization of both accuracy and fairness to unseen tasks. In this work, we first discover discrimination from data using a causal Bayesian knowledge graph which not only demonstrates the dependency of the protected variable on target but also indicates causal effects between all variables. Next, we develop a novel algorithm based on risk difference in order to quantify the discriminatory influence for each protected variable in the graph. Furthermore, to protect prediction from unfairness, a fast-adapted bias-control approach in meta-learning is proposed, which efficiently mitigates statistical disparity for each task and it thus ensures independence of protected attributes on predictions based on biased and few-shot data samples. Distinct from existing meta-learning models, group unfairness of tasks are efficiently reduced by leveraging the mean difference between (un)protected groups for regression problems. Through extensive experiments on both synthetic and real-world data sets, we demonstrate that our proposed unfairness discovery and prevention approaches efficiently detect discrimination and mitigate biases on model output as well as generalize both accuracy and fairness to unseen tasks with a limited amount of training samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Data-driven and big data technologies, nowadays, have advanced many complex domains such as healthcare, finance, social science, etc. With the development and increment of data, it is necessary to extract the potential and significant knowledge and unveil the messages hidden behind using data analysis. In data mining and machine learning, biased historical data are often learned and used to train a statistic predictive model. Depending on the application field, even though predictive models and computing process is fair, biased training data or data containing discrimination may lead to results with undesirability, inaccuracy, and even illegality. In recent years, there have been a number of news articles that discuss the concerns of bias and discrimination on crime forecasting. However, there is a lack of work that provides an extensive study on the potential bias and discrimination in public crime data and provides solutions to help produce predictive models that are free of discrimination towards the protected groups, such as African Americans. For example, 911 call data was used to predict crimes by the Seattle Police Department in 2016, but was late dropped due to potential racial bias in the provided data <ref type="bibr" target="#b0">[1]</ref>.</p><p>Non-discrimination can be defined as follows: (1) people that are similar in terms of non-sensitive characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-sensitive characteristics <ref type="bibr" target="#b1">[2]</ref>. The first condition is related to direct discrimination. For example, a hotel turns a customer away due to disability. The second condition ensures that there is no indirect discrimination, also referred to as redlining. For example, one is treated in the same way as everybody else, but it has a different and worse effect because of one's gender, race or other sensitive characters. The Equality Act <ref type="bibr" target="#b2">[3]</ref> calls these characters as protected characteristics. In the above-mentioned crime prediction example, even though race was not formally used as a forecasting criterion, it appeared that the geographic regions that have a much higher population of African American people have higher counts of 911 calls. Therefore, critics have voiced that human bias potentially has an influence on nowadays technology, which leads to make unfair decisions.</p><p>Machine learning models trained to give predicted outputs based on historical data will naturally inherit the past biases. With biased input, the main goal of training an unbiased model is to make the output fair. In other words, the predicted outcomes are statistically independent on protected variables (e.g. race and gender). These may be ameliorated by attempting to make the automated decision-maker blind to some attributes. This however, is difficult, as many attributes may be correlated with the protected one <ref type="bibr" target="#b3">[4]</ref>. Statistical parity, also known as group fairness, ensures that the overall proportion of members in a protected group receiving predictions are identical to the proportion of the population as a whole.</p><p>Fairness-aware in data mining is classified into unfairness discovery and unfairness prevention. How we address unfair-Fig. <ref type="figure">1</ref>: An overview of our proposed unfairness discovery and prevention approach in few-shot meta-learning. (Left) Discriminatory patterns in the collected data are visualized through a developed causal Bayesian network (CBN), where S and Y respectively denote protected attributes and target, and arrows represent causal effects between variables. (Right) A few-shot unfairness prevention approach is shown. In the meta-training stage, in each task, support loss is optimized under an unified mean-difference fairness constraint which performs a trade-off between accuracy and fairness (see the enlarged figure). The meta-parameter φ is thus iteratively optimized and then applied to calculate the final outcome, i.e. average loss and fairness, in the meta-testing stage. S and Q denote the support and query data, respectively. ness detection from biased data has significant practical consequences, which will further influence the following decisionmaking of a machine learning model. Traditional methods for discrimination discovery from data are under the assumption that the protected variables are predefined. This does not stand up to scrutiny because one or more variables may be correlated with protected variables and have strong dependency effect on predictions. We thus, in this paper, first reveal discriminatory patterns from data through developing a causal Bayesian network (CBN) which represents a flexible useful tool in this respect as it can be used to formalize, measure, and deal with different unfairness scenarios underlying a data set. The CBN contains information of causal effect of all variables (protected and explanatory variables).</p><p>In addition, to the best of our knowledge, unfortunately, the majority of existing unfairness prevention machine learning algorithms are under the assumption of giving abundant training examples. Learning quickly, however, is another significant hallmark of human intelligence. In meta-learning, also known as learning to learn, the goal of trained model is to quickly learn a new task from a small amount of new data (i.e. fewshot), and the model is trained by the meta-leaner to be able to learn on a large number of different tasks <ref type="bibr" target="#b4">[5]</ref>. In contrast to traditional machine learning algorithms, such as multitask learning <ref type="bibr" target="#b5">[6]</ref> and transfer learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, meta-learning framework has advantages: (1) it learns across tasks where each task takes one or few samples as input; (2) it therefore efficiently speeds up model adaptation (3) and generalizes accuracy to unseen tasks. The overall idea of existing methods of meta-learning is to train a model which is capability of generalizing accuracy, rather than fairness, to unseen data tasks. But techniques for unfairness prevention and bias control in the few-shot meta-learning study are challenging and rarely touched. To ensure prediction without biases, another contribution in this paper is that we feed each support set of a task with unified group fairness constraints and minimize meta-loss overall episodes. Specifically, we mitigate biases in each episode during meta-training by controlling mean difference <ref type="bibr" target="#b8">[9]</ref> to a small threshold. Our experimental results based on both synthetic and real-world data sets demonstrate our approach is capability of controlling bias and decreasing loss as well as generalize both to unseen tasks.</p><p>In summary, the main contributions of this paper are listed:</p><p>1) We first reveal unfairness from training data using a novel causal Bayesian network and quantify the discrimination effect of protected variables on target by developing a novel algorithm. 2) For the first time the problem of bias control in a fewshot meta-learning regression model is introduced. Our approach efficiently mitigates the dependency of predictions on the protected attribute using mean difference from statistics. 3) We validate the performance of our proposed approach of unfairness prevention on state-of-the-art metalearning techniques through extensive experiments based on both synthetic and real-world data sets. Our results demonstrate the proposed approach is capability of mitigating biases and generalizing both accuracy and fairness to unseen tasks, even with minimal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, researches involving processing biased data became increasingly significant. Fairness-aware in data mining is classified into unfairness discovery and prevention. Based on the taxonomy by tasks, it can be further categorized to classification <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b13">[14]</ref>, regression <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, clustering <ref type="bibr" target="#b17">[18]</ref>, recommendation <ref type="bibr" target="#b18">[19]</ref> and dimension reduction <ref type="bibr" target="#b19">[20]</ref>.</p><p>Unfairness discovery aims at finding discriminatory patterns in data using data mining methods. Data mining approach for discrimination discovery typically mines association and classification rules from the data, and then assesses those rules in terms of potential discrimination <ref type="bibr" target="#b1">[2]</ref>. A more traditional statistical approach to discrimination discovery typically fits a regression model to the data including the protected features, and then analyzes the magnitude and statistical significance of the regression coefficients at the protected attributes. If those coefficients appear to be significant, then discrimination is flagged. The existing techniques, however, only focused on the dependency of the protected attributes on target prediction, which blinded the causal effect of other explanatory variables on target and the relationship between variables. To address this flaw in unfairness detection, in this paper, we develop a causal Bayesian network containing all variables and it visually shows the causal effects among them.</p><p>Unfairness prevention develops machine learning algorithms that would produce predictive models, ensuring that those models are free from discrimination. Standard predictive models, induced by machine learning and data mining algorithms, may discriminate groups of people because (1) data bias comes from data being collected from different sources, or (2) dependence on a socially sensitive attribute was identified in the data mining community <ref type="bibr" target="#b8">[9]</ref>. Even though techniques for unfairness prevention on classification were well developed <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>, limited methods have been designed for regression models and the problem on regression is more challenging. Because (1) instead of evaluating the correlation between two categorical attributes, regression aims to assess the the correlation on the categorical protected attribute and continuous target variable; (2) in classification the goal of modification led to the change of one class label into another, however in a regression task, fairness learning allows the continuous character of targets for a continuous range of potential changes. <ref type="bibr" target="#b8">[9]</ref> first controlled bias in a regression model by restricting the mean difference in predictions on several data strata divided using the propensity scoring method from statistics. Furthermore, <ref type="bibr" target="#b20">[21]</ref> proposed a framework involving η-neutrality in which to use a maximum likelihood estimation for learning probabilistic models. Besides, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> recently came up with convex and non-convex optimization frameworks for fairness regression.</p><p>To the best of our knowledge, unfortunately, the majority of existing fairness-aware machine learning algorithms are under the assumption of giving abundant training examples. Learning quickly, however, is another significant hallmark of human intelligence. Several recent approaches have made significant progress in meta-learning. <ref type="bibr" target="#b21">[22]</ref> introduced Matching Networks which employed ideas from k-nearest neighbors algorithm and metric learning based on a bidirectional Long-Short Term Memory (LSTM) to encode in the context of the support set. Prototypical networks <ref type="bibr" target="#b22">[23]</ref> learn a metric space in which classification is able to be performed by computing Euclidean distances to prototype representations of each class. In addition, gradient descent based algorithms, such as <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref>, aim to learn good model initialization so that the meta-loss is minimum. The overall idea of these state-of-the-art is to train a meta-learning model which is capability of generalizing accuracy, but less attention on fairness generalization to unseen data tasks. In this paper, our proposed approach makes up for this regret of unfairness prevention using few-shot metalearning techniques in regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. UNFAIRNESS DISCOVERY</head><p>Intuitively, an attribute effects the target variable if one depends on the other. Strong dependency indicates strong effects. Currently, most fairness criteria used for evaluating and designing machine learning models focus on the relationships between the protected attribute and the system output. However, the training data can display different patterns of unfairness depending on how and why the protected attribute influences other variables. Using such criteria without fully accounting for this could be problematic. The development of technical solutions to fairness also requires considering the different, potentially intricate, ways in which unfairness can appear in the data. To this end, we construct a causal Bayesian network (CBN) using an open-source software TETRAD <ref type="bibr" target="#b27">[28]</ref>. A CBN is a graph formed by nodes representing random variables, connected by links denoting causal influence. By defining unfairness as the presence of a harmful influence from the protected attribute in the graph, CBNs provide us with a simple and intuitive visual tool for describing different possible unfairness scenarios underlying a data set. It effectively captures the existence of discrimination patterns and can provide quantitative evidence of discrimination in decision marking. Definition 1 (Causal Path). In a CBN, a path from node X to node Z is defined as a sequence of linked nodes starting at X and ending at Z. X is a cause of (has an influence on) Z if there exists a causal path from X to Z, namely a path whose links are pointing from the preceding nodes toward the following nodes in the sequence. For example, in Figure <ref type="figure" target="#fig_0">2</ref></p><formula xml:id="formula_0">, the path R → I → C is causal, but the path R → I → C ← A is non causal.</formula><p>We depict three possible scenarios in Figure <ref type="figure" target="#fig_0">2</ref>(a) to (c). In the first scenario, crime count is predicted according to the percentage of African American residents only. In the second scenario, crime counts are high in areas where African Americans percentage is high. This is because the percentage of African Americans determines the level of income and therefore R → I is consider unfair (red). As a consequence, the path I → C becomes partially unfair (green). In the third scenario, crime counts are high in the African American communities where the income level is low. In other words, for those communities with same percentage of African Americans but have high income, the crime counts may be low. This simplified example shows how CBNs can provide us with a visual framework for describing different possible unfairness scenarios. Understanding which scenario underlies a data set can be challenging or even impossible, and might require expert knowledge. It is nevertheless necessary to avoid pitfalls when evaluating or designing a decision system.</p><p>Next, to quantify the discrimination effect for each protected variable on target, we conducted the study from a data set D = {(x j i , y j i , s j i )} h i=1 , j = 1, ..., r, i = 1, ...h, where x j i ∈ R n denotes the i-th observation for the j-th task, y j i denotes the corresponding numeric output, s j i ∈ R m represents the continuous protected attributes, and h is the number of observations in each task. In order to reveal the causal effect from data, all variables, including target and the protected attributes, are discretized into three-bin categories (i.e. low, median, high) by frequency. We considered the unfairness measure called risk difference that is denoted as</p><formula xml:id="formula_1">∆P | s,j (y|s 1 , s 2 ) = |P (y|s 1 , j) -P (y|s 2 , j)|, ∀y|s, j<label>(1)</label></formula><p>where s 1 and s 2 refer to any two different sub-populations (e.g. s 1 ="low" and s 2 ="high") of a given protected variable s ∈ s j . The risk difference, ∆P , thus estimates the distribution shift for target Y under different protected sub-populations with a given task. Taking Crime data set as an example, each county is consider as a task and risk difference measures the distribution shift of crime rate given two sub-populations that the percentage of African American is high and low.</p><p>While the Supreme Court has resisted a "rigid mathematical formula" defining discrimination, we adopted a generalization of the 80 percent rule advocated by the US Equal Employment Opportunity Commission (EEOC) 1 . Formally, we define a fairness constraint of discrimination based on statistical parity:</p><formula xml:id="formula_2">P (P | j (y|s) ≤ ) ≤ 80%<label>(2)</label></formula><p>where is a given threshold to account for a degree of randomness in the decision making process and sampling. We 1 <ref type="url" target="https://www.eeoc.gov/">https://www.eeoc.gov/</ref> adopted the value = 0.05 as used in <ref type="bibr" target="#b28">[29]</ref>. Key steps of calculating risk difference are described in Algorithm 1.</p><p>Algorithm 1 Unfairness Discovery Using Risk Difference. Require: All variables are discretized into three-category bin. end for 12: end for 13: Calculate ratio: r = n/m. 14: return The ratio r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. UNFAIRNESS PREVENTION IN FEW-SHOT REGRESSION</head><p>Unfairness prevention develops machine learning algorithms that would produce predictive models, ensuring that those models are free from discrimination. In contrast to traditional regression settings, in this section, we introduce a novel few-shot discrimination prevention learning model based on the Model Agnostic Meta-Learning (MAML) framework <ref type="bibr" target="#b4">[5]</ref>, which is able to quickly learn a new task from a small amount of data (i.e. K-shot) and to generalize fairness onto unseen tasks. For simplicity, we select the most representative protected variable s ∈ s which takes the greatest risk difference and convert it into a binary variable {s + , s -}. However, our ideas can be easily extended to many protected attributes with multiple levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of Statistical Parity</head><p>A learning model is considered illegal discrimination if it has a disproportionately adverse effect on members of a protected group (e.g. race, gender). In other words, statistical parity ensures that the overall proportion of members in a protected group receiving prediction is identical to the proportion of the population as a whole.</p><p>To formulate, we split the data set for each task j into an episode {D S j , D Q j }, where D S j and D Q j are support and query set, respectively. A K-shot learning set D j = {x i , y i , s i } K i=1 , where s i ∈ {s + , s -} is the binary protected attribute. For regression problem, since y i ∈ R is a numeric scalar, it is not possible to use typical same-type measures of dependency like correlation coefficient and point-wise mutual information to quantify the statistical dependency between s and y. To quantify the effect of protected attribute s on its target y for D j , we apply Mean Difference (MD) for evaluating biases in regression problems.</p><p>Definition 2 (Mean Difference). The mean difference (MD) of numeric target variable y in data set D, partition into D + and D -by a binary protected variable s is given by:</p><formula xml:id="formula_3">M D(y, s; D) = | (x,y)∈D+ y N + - (x,y)∈D-y N - |</formula><p>where N + and N -are group sizes of D + and D -. The mean difference is a positive number with a value of zero indicating no dependency of target on the protected variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fair Model Agnostic Few-Shot Meta-Learning</head><p>Meta-learning is also known as learning to learn. In a general meta-learning setting, it consists of meta-train and meta-test stages where each contains a number of mini-batches of episodes split into support and query sets. We consider a distribution over tasks p(T ) that we want our model to be able to adapt to. In a K-shot learning setting, a task T j is sampled from p(T ), where the subscript j represents the j-th task of a mini-batch. In the supervised learning setting, supposing the meta-model is a parameterized function f φ with parameters φ. In a general meta-learning model, the goal is to learn an optimized meta-parameter φ so that the summation of query losses l Tj (f φ ) over all meta-training tasks is minimum.</p><formula xml:id="formula_4">φ * = arg min φ E T ∼p(T ) l T (f φ )<label>(3)</label></formula><p>The use of only K training examples for learning a new task is often referred to as K-shot learning. During meta-training, φ is updated iteratively. The trained meta-model is evaluated through a set of tasks that are not included in the meta-training procedure. To formulate the supervised regression problem in the context of the meta-learning model, the loss functions, mean squared error, is applied. It is represented by the error between the model's output for x and the corresponding target y. In order to control biases in prediction, it is required to restrict a statistical parity score g Tj (f φ ) for each task with a user-defined fairness threshold c &gt; 0. The objective function of a single task takes the form:</p><formula xml:id="formula_5">minimize φj l Tj (f φ ) = xi,yi∼Tj ||f (x i ; φ) -y i || 2 2 (4) subject to g Tj (f φ ) ≤ c</formula><p>where x i , y i are a pair of input feature vector and output target sampled from task T j , c is a small positive fairness relaxation, and g Tj (f φ ) is the mean difference of the continuous prediction bounded by c.</p><formula xml:id="formula_6">g Tj (f φ ) = (xi,yi∼Tj )∈D+ f (x i ; φ) N + - (<label>5</label></formula><formula xml:id="formula_7">) (xi,yi∼Tj )∈D-f (x i ; φ) N -</formula><p>To solve the optimization problem, we thus introduce an unified Lagrange multiplier λ ≥ 0 for all tasks and the Lagrange function L Tj (φ, λ) of a single task is defined by</p><formula xml:id="formula_8">L Tj (φ, λ) = l Tj (f φ ) + λ(g Tj (f φ ) -c)<label>(6)</label></formula><p>Therefore the original problem can be finally seen by minimizing L Tj (φ, λ) for each task and thus mitigates dependency of prediction on the protected attribute.</p><p>The goal of training a single task is to output a local parameter φ j given the meta-parameter φ such that it minimizes the task loss l Tj (f φ ) subject to the task constraint g Tj (f φ ) ≤ c. Next, to update the meta-parameter, we minimize the generalization error L meta using query sets across every task in the batch such that the query constraints are satisfied. Formally, the learning objective across all tasks is</p><formula xml:id="formula_9">min φ L meta ( T j=1 D Q j , φ) = T j=1 l Tj (f φj )(D Q j , φ j ) (7)</formula><p>where φ j = arg min φj ,g T j (f φ )≤c l Tj (f φ ) is the local optimum for each task. A step-by-step learning algorithm for the unfairness prevention approach in few-shot regression is proposed in Algorithm 2.</p><p>Algorithm 2 Unfairness Prevention in Few-Shot Regression. Require: p(T ): distribution over tasks. Require: α, β: step size hyperparameters. Require: q: inner gradient update steps. </p><formula xml:id="formula_10">for all T j = {D S j , D Q j } do 5:</formula><p>Sample K datapoints from D S j = {x j , y j , s j } 6:</p><formula xml:id="formula_11">φ j ← φ 7:</formula><p>for q = 1, 2, ... do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Evaluate ∇ φj L Tj (φ j , λ) using D S j 9:</p><p>φ j ← φ j -α∇ φj L Tj (φ j , λ)</p><p>end for 11:</p><p>Sample datapoints from D Q j = {x j , y j , s j } 12:</p><p>Evaluate query loss l Tj (f φj ) and query fairness g Tj (f φj ) using D Q Evaluate training fairness mean(g Tj (f φj )) 16: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In the section, we first demonstrate the individual utility of unfairness discovery approach that introduced in section III based on Crime data set. Then we conduct extensive experiments to validate the proposed few-shot unfairness prevention algorithm on both synthetic and real-world data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unfairness Discovery from Crime Data Set</head><p>Chicago Communities and Crime data set <ref type="bibr" target="#b14">[15]</ref> includes information relevant to crime (e.g. household, unemployment) as well as demographic information (such as race and gender) in different communities across the Chicago city in 2015. These information were separately collected from American FactFinder (AFF) 2 which is an online and selfservice database provided by the U.S. Census Bureau and then aggregated various sources to the final data prepared for experiments. More specifically, the economy related information such as points of interests (such as businesses and attractions) was extracted from location-based social networks (Foursquare check-in data). While the crime and geographical information in the data correspond to the specific crimes that occurred, our investigations clearly indicate that the local neighborhood information can provide strong indication about future crimes. In this data set, resident population of various ethnicities are considered as protected variables and crime count is the target attribute that we need to detect discrimination from. Fig. <ref type="figure">3</ref>: Causal Bayesian network conducted using the Crime data set. Red arrows are highlighted the causal effect between variables that target (crime count) is dependent on.</p><p>Experimental Results. First, we reveal unfairness from the Crime data set by conducting the causal Bayesian network (CBN) shown in Figure <ref type="figure">3</ref>, where "African American (%)" refers to the percentage of African American residents in each of census tract (geo-location unit) and "Other Ethnicity (%)" is the percentage of ethnicity groups other than white, African American, Asian, and American Indian groups. In the result of CBN, the biases and discrimination are modeled based on the causal paths from one variable to another. As shown in Figure <ref type="figure">3</ref>, highlighted paths demonstrate that crime count is dependent on four variables, "Total Population", "African American (%)", "Households Below Poverty (%)", and "Business Count". We thus need to consider a correct partition of the CBN network, in order to suppress all other influences rather than discrimination. Some of these are spurious and some although causal, can be explained by other attributes and hence are not regarded as discrimination.</p><p>To quantify dependency effect of protected variable on crime count, we block all causal paths from the protected   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unfairness Prevention with Few-Shot Learning</head><p>Synthetic Data. To evaluate, we start with a simple regression problem. We generated 12,000 synthetic data sets in total, 10,000 for training, 1000 for validation and 1000 for testing. Each data set can be considered as a single task. Specifically, for each data set, we generated 1000 data samples along with binary protected attributes uniformly. Each observation was uniformly assigned with a feature vector including seven explanatory attributes. Targets were generated from two Gaussian distributions with the same standard deviation of σ = 1 but shifted means. To make each task unfair to some extent, targets from the unprotected group generated with arbitrary mean in [0, 10], but targets means from protected group increased by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> randomly.</p><p>The same Chicago Communities and Crime (Crime) data <ref type="bibr" target="#b14">[15]</ref> that was applied in unfairness discovery experiments is continued to use for studying unfairness prevention in few-shot meta-learning model. Different from the previous setting, to simplify, we only keep one protected variable, "African American (%)", and treat the rest (i.e. "American Indian (%)", "Asian (%)", and "Other Ethnicity (%)") as explanatory attributes. The Crime data set is divided into 801 sub-tasks according to different communities in the Chicago city. All tasks were further split into 501 for training, 100 for validation and 200 for testing. Each task contains 52 crime records. Since the feature in the original data that described the percentage of African American population is numeric, in this experiment, we convert it into binary values based on the majority (&gt; 70%) population of Black and non-Black. Thus, each record represents a weekly information including 13 numeric explanatory variables and one binary protected variable.</p><p>All the attributes were standardized to zero mean and unit variance for all data sets and prepared for experiments. Our neural network trained follows the same architecture used by <ref type="bibr" target="#b4">[5]</ref>, which contains 2 hidden layers of size of 40 with ReLU activation functions. When training, we use only one step gradient update with 2K samples of query set with a fixed learning rate of 0.01 and use Adam as the meta-optimizer. Similarly, we set the learning rate of 0.001 used to update the meta-loss in the outer loop. Hyperparameters are selected by held-out validation data. All experiments are repeated 10 times with the same settings. Results shown with these two methods in this paper are mean of experimental outputs followed by the standard deviation.</p><p>To evaluate performance, we fine-tune a single meta-learned model on varying numbers of K ∈ {5, 10, 20} examples, and compare performance to two baselines: (a) the original MAML model <ref type="bibr" target="#b4">[5]</ref>, and (b) the baseline method which pre-trains and entails training the network to regress to random functions and fine-tuning at the meta-testing stage using an automatically tuned step size. Both MAML and the baseline (pre-trained) models share the same neural network architecture and parameter settings. In order to distinguish our approach from the two baselines, we add a prefix "Fair-" in Figure <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref>.</p><p>In addition, we introduce two off-the-shelf evaluation metrics to measure biases. These measurements come into play of quantifying the extent of bias and are designed for indicating indirect discrimination.</p><p>(1) The area under the ROC curve (AUC) <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_13">AU C = (si,yi)∈D+ (sj ,yj )∈D-I(y i &gt; y j ) |D + | × |D -|<label>(8)</label></formula><p>where I(•) is an indicator function which returns 1 if its argument is true, 0 otherwise. AU C = 0.5 represents random predictability, thus S is independent on Y .</p><p>(2) Impact Ratio (IR) <ref type="bibr" target="#b29">[30]</ref>.</p><formula xml:id="formula_14">IR = yi∈D+ y i |D + | yj ∈D-y j |D -|<label>(9)</label></formula><p>It is defined as the ratio of mean over the protected and unprotected group in data D. The decisions are deemed to be discriminatory if the ratio of positive outcomes for the protected attribute is below 80% <ref type="bibr" target="#b30">[31]</ref>. IR = 1 indicates that there is no bias of data D. Experimental Results. We evaluate the performance of our approach followed by <ref type="bibr" target="#b4">[5]</ref> by fine-tuning the (Fair-) Baseline and models learned by (Fair-) MAML on K = {5, 10, 20} data points. Results of validation loss and mean difference of each iteration are plotted in Figure <ref type="figure" target="#fig_4">4</ref>. In terms of losses (see Fig. <ref type="figure" target="#fig_4">4</ref> (a), (c) and (e)), MAML is outperformed than baseline methods and the gap between all methods is narrowing as the number of training data increases. Although our proposed approach (i.e. Fair-Baseline and Fair-MAML) returns a bit bigger validation loss, this is due to the trade-off between fairness and accuracy. In Figure <ref type="figure" target="#fig_4">4</ref> (b), (d) and (f), our approach demonstrates success in controlling bias and decreasing validation losses, even trained with few-shot samples.</p><p>More experimental results with synthetic (see Figure5(a)-(d)) and real-world data set ((e)-(h)), as well as those are examined with two fairness evaluation metrics (AUC and IR) are shown in Figure <ref type="figure" target="#fig_5">5</ref>. Results through the Crime data demonstrate that our approach of controlling disparate treatment significantly decreases AUC and MD, and hence increases IR above the boundary of bias level of 80% rules <ref type="bibr" target="#b30">[31]</ref> in contrast to methods without adding "Fair-" constraints. Besides, the larger K value (i.e. more samples are considered in the support set), the better generalization capability of loss and fairness based on a few novel instances performs. This demonstrates the working efficiency of the proposed model.</p><p>MAML became a famous meta-learning algorithm because of its fast adaptation and good generalization performance on losses. However, our results showed it fails to control biases nor performs success in fairness generalization in a few-shot meta-learning. Our approach nevertheless makes up for this deficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, for the first time we discover unfairness based on causal Bayesian network which reveals causal effect between all variables. In addition, we develop a novel algorithm based on risk difference in order to quantify the discriminatory influence for each protected variable in the graph. Besides, to prevent prediction from intervention of the protected variable, a fast-adapted bias-control approach by adding statistical parity constraints is proposed, which significantly mitigates dependence of prediction on the protected variable in each task and generalize both accuracy and fairness to unseen tasks. Due to the nature of MAML, which finds a task-specific model parameter for each task, one of the goal of future researches in few-shot learning is to design a fairness regulatory mechanism such that it automatically designs a task-specific fairness constraint through hyperparameter optimization techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: CBN representing a hypothetical crime forecasting process with three possible scenarios, where red and green paths are used to indicate unfair and partially-unfair links, respectively. Consider a hypothetical crime count prediction example in which crime are predicted based on times of being arrested (A), ethnicity or race (R), and income (I). The predicting process is represented by the CBN in Figure 2. Race has a direct influence on crime through the causal path R → C and an indirect influence through the causal path R → I → C. The direct influence captures the fact that individuals with the same arrested times who have the same income level might be treated differently based on their race (e.g. black and nonblack). The indirect influence captures differing crime counts between black and non-black individuals due to differing income levels.</figDesc><graphic coords="3,318.36,420.80,77.11,59.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 : 5 :</head><label>15</label><figDesc>Initialization n = 0, m = 0 2: for each task j do 3: for each configuration y of Y do 4: for each different configurations {s 1 , s 2 } of S do Calculate risk difference: ∆P | s,j (y|s 1 , s 2 ) = |P (y|s 1 , j) -P (y|s 2 , j)| 6: if ∆P | s,j (y|s 1 , s 2 ) ≤ then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1: Randomly initialize φ 2 : 3 :</head><label>23</label><figDesc>while not done do Sample batch of tasks T j 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Update φ ← φ -β∇ φ Tj ∼p(T ) l Tj (f φj ) 15:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Validation loss and mean difference stabilization over iterations.</figDesc><graphic coords="6,314.96,447.77,120.81,92.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Experiment results of controlling biases and fairness generalization to unseen data for meta-learning regression problem.</figDesc><graphic coords="7,307.72,150.83,125.94,88.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,48.96,50.54,514.06,163.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2 https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Analysis of statistical parity on protected groups for the Crime data set. to target y in the CBN and analyze each protected variables using statistical parity approach introduced in section III. The estimated statistical parity measures of different protected groups are shown in TableI. The result indicates that the Crime data contains potential bias based on the 80% rule. A lower statistical parity indicates a higher unfairness causal effect of crime count towards the protected group.</figDesc><table><row><cell cols="2">Protected Groups (%) Unfairness</cell></row><row><cell>African American</cell><cell>22.20%</cell></row><row><cell>American Indian</cell><cell>46.91%</cell></row><row><cell>Asian</cell><cell>40.70%</cell></row><row><cell>Other Ethnicity</cell><cell>40.70%</cell></row><row><cell>attribute s</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work is supported by the <rs type="funder">National Science Foundation (NSF)</rs> under Grant No #<rs type="grantNumber">1815696</rs> and #<rs type="grantNumber">1750911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_H5ks7P3">
					<idno type="grant-number">1815696</idno>
				</org>
				<org type="funding" xml:id="_PyeASQW">
					<idno type="grant-number">1750911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Seattle police drop plans for crime forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Herz</surname></persName>
		</author>
		<ptr target="https://www.thestranger.com/slog/2016/06/09/24190187/seattle-police-drop-plans-for-crime-forecasting" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on measuring indirect discrimination in machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zliobaite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00148</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The U.S. Civil Rights Act</title>
		<imprint>
			<date type="published" when="1964-07-02">July 2, 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaption in one-shot learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controlling attribute effect in linear regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fairness constraints: Mechanisms for fair classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A reductions approach to fair classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dudk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taking advantage of multitask learning for fair classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Doninini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/ACM Conference (AIES)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corepresentation learning framework for the open-set data classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="239" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rank-based multi-task learning for fair regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="916" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convex framework for fair regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT)</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency (FAT)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonconvex optimization for regression with fairness constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Komiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-redundant clustering with conditional ensembles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fairness of exposure in rankings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06520[cs.CL</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prediction with modelbased neutrality</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1503" to="1516" />
			<date type="published" when="2015-08">08 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31th Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic metalearning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The tetrad project causal models and statistical data</title>
		<ptr target="http://www.phil.cmu.edu/tetrad/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On discrimination discovery using causal networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="83" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring discrimination in socially-sensitive decision records</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2009 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adverse impact and test validation: A practitioner&apos;s guide to valid and defensible employment testing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Biddle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Gower</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
