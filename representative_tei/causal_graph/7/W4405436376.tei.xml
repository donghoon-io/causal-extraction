<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Human-like Perception: Learning Structural Causal Model in Heterogeneous Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-10">10 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianqianjin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Resources Management</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<postCode>311121</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaisong</forename><surname>Song</surname></persName>
							<email>kaisong.sks@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<postCode>311121</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoren</forename><surname>Jiang</surname></persName>
							<email>jiangzhuoren@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Resources Management</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyang</forename><surname>Kang</surname></persName>
							<email>yangyang.kangyy@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<postCode>311121</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weikang</forename><surname>Yuan</surname></persName>
							<email>yuanwk@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Resources Management</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xurui</forename><surname>Li</surname></persName>
							<email>xurui.lee@msn.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<postCode>311121</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changlong</forename><surname>Sun</surname></persName>
							<email>changlong.scl@taobao.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<postCode>311121</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cui</forename><surname>Huang</surname></persName>
							<email>huangcui@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Resources Management</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Worcester Polytechnic Institute</orgName>
								<address>
									<postCode>01609-2280</postCode>
									<settlement>Worcester Massachusetts</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Human-like Perception: Learning Structural Causal Model in Heterogeneous Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-10">10 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2312.05757v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>structural causal model heterogeneous graph node property prediction interpretability generalizability</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous graph neural networks have become popular in various domains. However, their generalizability and interpretability are limited due to the discrepancy between their inherent inference flows and human reasoning logic or underlying causal relationships for the learning problem. This study introduces a novel solution, HG-SCM (Heterogeneous Graph as Structural Causal Model). It can mimic the human perception and decision process through two key steps: constructing intelligible variables based on semantics derived from the graph schema and automatically learning task-level causal relationships among these variables by incorporating advanced causal discovery techniques. We compared HG-SCM to seven state-of-the-art baseline models on three real-world datasets, under three distinct and ubiquitous outof-distribution settings. HG-SCM achieved the highest average performance rank with minimal standard deviation, substantiating its effectiveness and superiority in terms of both predictive power and generalizability. Additionally, the visualization and analysis of the auto-learned causal diagrams for the three tasks aligned well with domain knowledge and human cognition, demonstrating prominent interpretability. HG-SCM's human-like nature and its enhanced generalizability and interpretability make it a promising solution for special scenarios where transparency and trustworthiness are paramount.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The surge and convergence of numerous readily available data sources have elicited exhilarating opportunities for investigation across various data-rich domains. Heterogeneous Graph Neural Network (HGNN), as a noteworthy approach in this realm, has garnered significant attention recently due to its effectiveness in modeling real-world complex systems with intricate relationships, such as academic networks, e-commerce networks, and social networks, in which different types of nodes and relations interplay following specific graph schema <ref type="bibr">(Han et al., 2022a;</ref><ref type="bibr" target="#b78">VDong et al., 2020;</ref><ref type="bibr" target="#b36">Jiang et al., 2020;</ref><ref type="bibr" target="#b9">Chen et al., 2021;</ref><ref type="bibr">Lu et al., 2022;</ref><ref type="bibr">Wang et al., 2022a;</ref><ref type="bibr" target="#b59">Mo et al., 2023;</ref><ref type="bibr" target="#b91">Xie et al., 2023)</ref>.</p><p>Despite the impressive results these models have achieved, three challenges have yet to be fully elucidated.</p><p>Generalizability: The vast majority of studies on HGNN are conducted under the premise of identically and independently distributed (i.i.d) data splits, which can potentially lead to a disregard for the issue of generalizability. In practical applications, however, the i.i.d assumption is not always valid as the presence of distribution shifts in unseen data, also known as out-of-distribution (o.o.d) problems <ref type="bibr" target="#b106">(Zhang et al., 2021;</ref><ref type="bibr">Yang et al., 2023a)</ref>. HGNNs that are susceptible to the presence of spurious correlations and connections in the training data may fail to generalize to unseen data <ref type="bibr" target="#b47">(Li et al., 2022;</ref><ref type="bibr" target="#b58">Miao et al., 2022;</ref><ref type="bibr" target="#b81">Wan et al., 2022;</ref><ref type="bibr" target="#b23">Fu et al., 2023)</ref>. As <ref type="bibr" target="#b40">Knyazev et al. (2019)</ref> claimed, the popular neighborhood aggregation mechanisms used in current HGNNs are vulnerable to the presence of spurious edges and correlations that mislead in how they attend to node neighbors, thereby resulting in inadequate generalizability of the models.</p><p>Interpretability: Building more accurate predictive models is not the only objective for graph models <ref type="bibr" target="#b95">(Ying et al., 2019)</ref>. It is imperative for researchers to discover the patterns from the input graph that induce certain predictions <ref type="bibr" target="#b11">(Cranmer et al., 2020)</ref>. While attention-based graph models <ref type="bibr" target="#b79">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b34">Hu et al., 2020;</ref><ref type="bibr" target="#b55">Lv et al., 2021)</ref> are capable to assign weights to edges in the input graph, research has found that these estimated weights can not provide any reliable interpretation for the learning tasks <ref type="bibr" target="#b97">(Yu et al., 2021;</ref><ref type="bibr" target="#b58">Miao et al., 2022)</ref>. These methods mostly estimate the input-output relationships at the sample level from an associational perspective <ref type="bibr">(Wang et al., 2022b)</ref>, which may overestimate the graph structure in a single input graph irrelevant to the outcome <ref type="bibr" target="#b60">(Moraffah et al., 2020)</ref>, rather than uncovering the plausible causation of the task itself .</p><p>Figure <ref type="figure" target="#fig_1">1</ref>: A toy academic heterogeneous graph is displayed in (c), where KDD and ACL are highly representative conferences in the fields of data mining and natural language processing, respectively. Its graph schema is shown in (a). For the task of predicting the author's research area, a hypothetical causal structure is depicted in <ref type="bibr">(b)</ref>, where the research area of an author is only determined by two factors: the author's papers and the venues of the author's papers. The two mainstream paradigms of current HGNNs are illustrated in <ref type="bibr">(d)</ref> and (e). It can be observed that the inherent associational inference flows of current HGNNs can not align well with human reasoning logic or underlying causal relationships. Therefore, they can face limitations in correct interpretation with respect to the learning tasks and are prone to get compromising generalizability due to inevitably introducing spurious correlations.</p><p>Learning level. Perhaps due to the success of many models that employ sample-level dynamic adaptation <ref type="bibr" target="#b79">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b72">Shi et al., 2021;</ref><ref type="bibr" target="#b90">Xian et al., 2022)</ref>, existing HGNNs primarily focus on sample-adaptive learning by leveraging complex attention modules in the neighbor aggregation and relations/meta-paths fusion <ref type="bibr" target="#b70">(Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b34">Hu et al., 2020;</ref><ref type="bibr" target="#b55">Lv et al., 2021;</ref><ref type="bibr" target="#b76">Vashishth et al., 2020)</ref>. However, beyond the sample-level adaptation, for a given learning problem on a graph, there is always a relatively consistent principle for processing all samples. For instance, the importance of different relationships/meta-paths should be stable in a specific task <ref type="bibr" target="#b37">(Jiang et al., 2018)</ref>. As a result, for prediction problems in heterogeneous graphs, it can be critical to prioritize learning at the task level, rather than solely focusing on fine-grained sample-level adaptation. The empirical evidence from <ref type="bibr">Yang et al. (2023b)</ref> has indicated that the sample-wise neighbor attention has limited impacts on the performance of the models in fact. Our supplementary experiment in Appendix A suggests that sample-wise semantic attention may also yield limited improvement in model performance.</p><p>We provide a toy example in Figure <ref type="figure" target="#fig_1">1</ref> to explain these three challenges from the perspective of the design of the inference flow. The inference flow of HGNN models naturally introduces spurious correlations to reduce the generalizability of the model. For instance, in the relation/meta-path fusion models (Figure <ref type="figure" target="#fig_1">1e</ref>), the co-author's information can always have a direct impact on the prediction. While information from coauthor B (whose research area is data mining) is useful for predicting the research area of author A (data mining), for predicting author C's research area (NLP), co-author B's information would introduce noise. Thus, if the model attaches great importance to co-author information, the generalizability of the model will decrease. Current HGNN models carry limitations in interpretability because their inference flows are fixed. For instance, in the layer-by-layer models (Figure <ref type="figure" target="#fig_1">1d</ref>), the venue's information can only indirectly impact the prediction of the research area through intermediate variables, i.e., paper and author. Additionally, the venue's information can become indistinguishable due to the blending with co-authors. As a result, it would be difficult for the model to provide a clear and explicit explanation: an author would be considered as a data mining scholar when this author publishes papers mainly at KDD.</p><p>Under the constraints of these two mainstream paradigms, current efforts tend to focus on designing sophisticated aggregation or fusion within the inference flow, rather than reflecting on whether the paradigm of the inference flow needs to be changed. Generally, ignoring achievable task-level causal relationships and strengthening the sample-level adaptive learning based on the inherent fixed associational inference logic may exacerbate the issues of interpretability and generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Research objectives and contributions</head><p>The three challenges mentioned above pose significant obstacles to the trustworthiness and reliability of current HGNNs in real-world settings. To shed light on the reasons behind these challenges, we present an instructive toy example in Figure <ref type="figure" target="#fig_1">1</ref> and corresponding theoretical analysis that illustrate the deficiencies inherent in the pre-defined and fixed inference flows in existing HGNNs.</p><p>As such, the focus of this work is to develop an HGNN architecture that aligns with human reasoning logic and causal mechanisms for the learning problem, achieving inherent and powerful interpretability and generalizability. Our motivation stems from a fundamental question: "What will humans do when facing a prediction task on a heterogeneous graph?" Generally, when reasoning about a task defined in a system, humans will cognitively perceive meaningful variables involved in the system, select potentially causallycorrelated factors from them, and estimate the causal effect of these factors on the task for task reasoning. From this viewpoint, we propose a novel solution for the node property prediction task, named HG-SCM (Heterogeneous Graph as Structural Causal Model). HG-SCM is designed to mimic the human perception and decision process and automatically learn task-level causal relationships by incorporating emerging causal techniques.</p><p>Specifically, the following research questions will be addressed in this work:</p><p>1. How to define, construct and represent human-understandable variables in the heterogeneous graph?</p><p>2. How to automatically discover the task-level causal relationships among the variables and identify the direct causes for the target variable?</p><p>3. How to make the prediction based on the learned task-level causal relationships?</p><p>4. How good is the task performance of the proposed solution?</p><p>5. How effective is the proposed solution for achieving better generalizability and interpretability?</p><p>Technically, HG-SCM constructs meaningful variables based on schema-level semantics in the heterogeneous graph. Specifically, the target node, the label of the target node, and neighbor sets of the target node based on different relations/meta-paths are considered available variables. HG-SCM embeds these variables via mutually-independent encoders without fitting the spurious correlations among them. By incorporating emerging causal structural learning techniques into the understanding of a heterogeneous graph task, HG-SCM further learns task-level causal relationships among these variables and makes predictions based only on variables that are likely to be causally correlated to the target variable. The learned causal structures can further provide clear interpretation with respect to the learning tasks. Own to such human-like reasoning logic, HG-SCM naturally equips enhanced interpretability and generalizability. Extensive experiments and in-depth analysis under both the i.i.d setting and the o.o.d setting effectively validate such hypotheses.</p><p>In summary, the contribution of this paper can be threefold:</p><p>1. We propose a novel heterogeneous graph algorithm HG-SCM. Unlike prior works, its inference flow aligns with human reasoning logic or underlying causal diagrams. To the best of our knowledge, this is a pioneer investigation to introduce the structural causal model into heterogeneous graph learning.</p><p>2. HG-SCM can consistently and significantly outperform various SOTA baselines in extensive experiments under the i.i.d setting and various types of the o.o.d settings, which verify the optimal efficiency and promising generalizability of HG-SCM.</p><p>3. HG-SCM can provide in-depth interpretations in accordance with the learning tasks by automatically discovering causal relationships among meaningful semantics hidden in a heterogeneous graph along with the graph schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Heterogeneous Graph Neural Networks</head><p>Recent studies have attempted to explore algorithms for modeling heterogeneous graphs since many real-world problems can hardly be represented by homogeneous graphs. Among these efforts, messagepassing-based heterogeneous graph neural networks (HGNNs), e.g., RGCN <ref type="bibr" target="#b70">(Schlichtkrull et al., 2018)</ref>, HAN <ref type="bibr">(Wang et al., 2019b)</ref>, GTN <ref type="bibr" target="#b102">(Yun et al., 2019)</ref>, CompGCN <ref type="bibr" target="#b76">(Vashishth et al., 2020)</ref>, HGT <ref type="bibr" target="#b34">(Hu et al., 2020)</ref>, PGRA <ref type="bibr" target="#b6">(Chairatanakul et al., 2021)</ref>, SimpleHGN <ref type="bibr" target="#b55">(Lv et al., 2021)</ref>, SeHGNN <ref type="bibr">(Yang et al., 2023b)</ref>, RHGCN <ref type="bibr" target="#b59">(Mo et al., 2023)</ref>, HetReGAT <ref type="bibr">(Li et al., 2023b)</ref>, and R-HGNN <ref type="bibr" target="#b98">(Yu et al., 2023)</ref>, have emerged as the dominant approach because these methods can leverage complex encoders along with deep neural networks and enable the natural modeling of both spatial proximity and node attributes <ref type="bibr" target="#b78">(VDong et al., 2020)</ref>. HGNNs generally learn node representations from neighbors in two approaches: meta-path-based fusion and layer-by-layer aggregation. The meta-path-based methods <ref type="bibr">(Wang et al., 2019b;</ref><ref type="bibr" target="#b22">Fu et al., 2020;</ref><ref type="bibr">Yang et al., 2023b)</ref> make in-depth use of the heterogeneous graph semantics. For example, two authors connected by a meta-path "Author-Publication-Author" suggest they have an academic partnership. Meanwhile, layerby-layer methods <ref type="bibr" target="#b70">(Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b76">Vashishth et al., 2020;</ref><ref type="bibr" target="#b34">Hu et al., 2020;</ref><ref type="bibr" target="#b55">Lv et al., 2021)</ref> learn a node's representation by simultaneously aggregating all the directly connected neighbors belonging to all edge types, and they update the node representation recursively. The rapid development of heterogeneous graph neural networks has also led to a series of transformations and applications in specific fields, such as fact verification <ref type="bibr" target="#b9">(Chen et al., 2021)</ref>, recommendation <ref type="bibr" target="#b68">(Qiao et al., 2020;</ref><ref type="bibr" target="#b14">Dai et al., 2023;</ref><ref type="bibr" target="#b7">Chang et al., 2023;</ref><ref type="bibr">Wang et al., 2023a)</ref>, sentiment analysis <ref type="bibr">(Lu et al., 2022;</ref><ref type="bibr" target="#b103">Zeng et al., 2023)</ref>, video question answering <ref type="bibr">(Wang et al., 2023b)</ref>, stock prediction <ref type="bibr" target="#b75">(Tan et al., 2022)</ref> and knowledge graph learning <ref type="bibr" target="#b92">(Xie et al., 2022)</ref>. <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b60">Moraffah et al., 2020;</ref><ref type="bibr" target="#b97">Yu et al., 2021;</ref><ref type="bibr" target="#b40">Knyazev et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfortunately, these approaches have limitations in terms of model generalizability and interpretability due to the deficiency of their inference flows</head><p>To address this issue, research on the generalizability and interpretability of graph neural networks has undergone notable advancements in recent years. In the realm of generalizability, prior efforts have not only expanded data augmentation <ref type="bibr" target="#b108">(Zhao et al., 2021;</ref><ref type="bibr" target="#b41">Kong et al., 2022;</ref><ref type="bibr" target="#b96">You et al., 2020)</ref> and training strategies <ref type="bibr" target="#b81">(Wan et al., 2022;</ref><ref type="bibr" target="#b49">Liu et al., 2022;</ref><ref type="bibr" target="#b21">Feng et al., 2021)</ref> intrinsic to the broader field of machine learning but have also formulated novel model structures and prediction pipelines based on disentanglement techniques <ref type="bibr" target="#b56">(Ma et al., 2019;</ref><ref type="bibr" target="#b50">Liu et al., 2020)</ref> and causality tools <ref type="bibr" target="#b48">(Lin et al., 2021;</ref><ref type="bibr" target="#b18">Fan et al., 2022)</ref>. Despite these advancements, the inherent black-box nature of these methods remains a fundamental concern. This opacity hinders human comprehension and collaboration, thereby constraining the applicability of the models, particularly in high-stakes domains <ref type="bibr" target="#b12">(Cui and Athey, 2022)</ref>. On the interpretability front, prevailing methods primarily adopt post-hoc approaches, evaluating the significance of nodes or edges in the input graph for a trained graph model. <ref type="bibr" target="#b101">Yuan et al. (2023)</ref> categorizes these methods into two branches: model-level and instance-level. While XGNN <ref type="bibr" target="#b100">(Yuan et al., 2020)</ref> stands as the sole model-level method, instance-level techniques encompass gradients-based <ref type="bibr" target="#b67">(Pope et al., 2019;</ref><ref type="bibr" target="#b3">Baldassarre and Azizpour, 2019)</ref>, perturbationbased <ref type="bibr" target="#b95">(Ying et al., 2019;</ref><ref type="bibr">Luo et al., 2020a)</ref>, decomposition <ref type="bibr" target="#b71">(Schnake et al., 2022)</ref>, and surrogate methods <ref type="bibr" target="#b35">(Huang et al., 2023)</ref>. Additionally, a few approaches enhance built-in interpretability through learnable prototypes <ref type="bibr" target="#b107">(Zhang et al., 2022;</ref><ref type="bibr" target="#b69">Ragno et al., 2022)</ref> and concept distillation <ref type="bibr" target="#b57">(Magister et al., 2023)</ref>. However, all of these approaches only aid in understanding the dependency path of predictions, falling short of aligning with human cognitive processes. Consequently, trust issues persist in real-world applications. Moreover, it is noteworthy that the preponderance of extant generalizability and interpretability methods is tailored for homogeneous graphs. Therefore, they struggle to effectively handle or leverage the intricate semantics inherent in heterogeneous graphs. This limitation further underscores the need for continued advancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Causal Structural Learning</head><p>Causal understanding is a fundamental problem of science <ref type="bibr" target="#b64">(Pearl, 2009;</ref><ref type="bibr" target="#b4">Bareinboim et al., 2022;</ref><ref type="bibr">Luo et al., 2020b)</ref> and is crucial for reasoning about the physical world <ref type="bibr" target="#b80">(Vowels et al., 2022;</ref><ref type="bibr" target="#b39">Kitson et al., 2023)</ref>. In order to discover causal relations and acquire causal understanding, randomized experiments (REs) with interventions and manipulations can be carried out <ref type="bibr" target="#b80">(Vowels et al., 2022;</ref><ref type="bibr" target="#b1">Alan, 2012)</ref>. However, in real applications, REs tend to be costly or even impractical due to ethical concerns, etc <ref type="bibr" target="#b80">(Vowels et al., 2022;</ref><ref type="bibr" target="#b24">Gamella and Heinze-Deml, 2020)</ref>. Therefore, researchers often discover causal structures from nonexperimental and observational data. One can achieve this goal by learning a Bayesian networks (BNs), which encodes the conditional independencies between variables using directed acyclic graphs (DAGs), but learning such networks from data can be computationally intractable due to the combinatorial explosion in the search space <ref type="bibr">(Luo et al., 2020b)</ref>. Recent work <ref type="bibr" target="#b109">(Zheng et al., 2018;</ref><ref type="bibr" target="#b99">Yu et al., 2019;</ref><ref type="bibr" target="#b43">Lachapelle et al., 2020;</ref><ref type="bibr" target="#b110">Zheng et al., 2020;</ref><ref type="bibr" target="#b13">Cundy et al., 2021;</ref><ref type="bibr">Zhu et al., 2020b;</ref><ref type="bibr" target="#b89">Wei et al., 2020;</ref><ref type="bibr" target="#b8">Charpentier et al., 2022;</ref><ref type="bibr" target="#b38">Kalainathan et al., 2022;</ref><ref type="bibr" target="#b19">Fang et al., 2022;</ref><ref type="bibr" target="#b73">Strobl, 2022;</ref><ref type="bibr" target="#b20">Fang et al., 2023;</ref><ref type="bibr">Li et al., 2023a)</ref> has made it possible to approximate this problem as a continuous optimization task <ref type="bibr">(Luo et al., 2020b)</ref> by minimizing an innovative smooth function that quantifies the "DAG-ness" in both linear and non-linear cases <ref type="bibr" target="#b42">(Kyono et al., 2020)</ref>. These techniques provide us with an unprecedented opportunity to develop neural models that could achieve both accuracy and interpretability simultaneously <ref type="bibr">(Luo et al., 2020b;</ref><ref type="bibr" target="#b42">Kyono et al., 2020;</ref><ref type="bibr" target="#b30">He et al., 2022)</ref>. For example, <ref type="bibr" target="#b42">Kyono et al. (2020)</ref> regard such techniques as a regularization method in regression models, <ref type="bibr" target="#b30">He et al. (2022)</ref> incorporate DAG into the recommendation domain, and <ref type="bibr" target="#b104">Zhai et al. (2023)</ref> utilize causality learning in the model in click-through rate prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>However, to the best of our knowledge, no prior works have integrated this technique into the framework design of heterogeneous graph algorithms.</head><p>Table <ref type="table">1</ref> Notations and Explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Explanation Notation Explanation an edge type a node feature vector a node type a label vector of a node a meta-path a hidden representation vector the number of used relations/meta-paths a matrix of node feature vectors the number of classes a matrix of node label vectors the number of samples in a batch a matrix of hidden representation vectors a neighbor set an adjacency matrix of a DAG</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Definitions and Notations</head><p>Definition 1. Heterogeneous Graph: A heterogeneous graph is defined as a directed graph  = (, ) with a node type mapping function ∶  →  and an edge type mapping function ∶  → , where each node ∈  belongs to a particular node type ∈  and each edge ∈  belongs to a particular edge type ∈  <ref type="bibr" target="#b74">(Sun et al., 2011)</ref>. Furthermore, in this work, we consider settings where nodes are associated with features. As a result, for each node ∈ , a feature vector ∈ ℝ is assigned. is the supposed feature dimension.</p><p>Definition 2. Node Property Prediction: Generally, a node property prediction task is defined on a specific node type in a graph  and is to predict properties of a single node belonging to the node type . In this work, we take the node classification task as an example. The task is to estimate a function  ∶  →  which can map each node ∈  to a categorical vector in the label space  ∈ ℝ based on a given labeled node set  * ⊆  , where  denotes the node set { | ∈  ∧ ( ) = } and is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. Meta-Path:</head><p>A meta-path <ref type="bibr" target="#b16">(Dong et al., 2017)</ref> is a path in the form of</p><formula xml:id="formula_0">1 1 ← ← ← ← ← ← ← ← → 2 2 ← ← ← ← ← ← ← ← → ⋯ ← ← ← ← ← ← ← → +1 , which defines a composite -hop relation = 1 • 2 • ⋯ • between the node type 1 and +1 .</formula><p>Definition 4. Ego-graph: Ego-graphs are local graphs with the focal node (known as the ego), while all other nodes connected to the ego are called alters <ref type="bibr" target="#b5">(Borgatti et al., 2009;</ref><ref type="bibr" target="#b15">Daly and Haahr, 2007)</ref>. An egograph can be defined as a -hop ego-graph when the maximum distance between the alters and the ego is and the alters contain all -hop neighbors of the ego.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5. Structural Causal Model: A Structural Causal Model (SCM) can describe the causal mechanisms of a system. Specifically, assuming no unobserved variable exists, a SCM of variables</head><formula xml:id="formula_1">{ | 1 ≤ ≤ } consists of a collection { | 1 ≤ ≤ } of structural assignments ∶= (pa )</formula><p>where pa is the set of direct causes of . "∶=" represents the assignment operation and it means the value of should be determined by its direct causes pa through the function . Usually, assignments are assumed acyclic and thus these assignments can be represented by a Directed Acyclic Graph (DAG) <ref type="bibr" target="#b10">(Christofides, 1975)</ref> with edges pointing from causes to effects <ref type="bibr" target="#b66">(Peters et al., 2017;</ref><ref type="bibr" target="#b63">Pawlowski et al., 2020)</ref>.</p><p>The notations are summarized in Table <ref type="table">1</ref> and vectors/matrices are indexed starting from zero in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we illustrate the proposed model, which can answer research questions 1, 2 and 3 in Section 4.1, Section 4.2 and Section 4.3, respectively. The overall HG-SCM architecture is depicted in Figure <ref type="figure" target="#fig_0">2</ref> and the general forward process of HG-SCM is described in Algorithm 1. In addition, the optimization objectives are explained in Section 4.4 and we provide the analysis of the computational complexity of the HG-SCM in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Variables Construction</head><p>As aforementioned, when given a task defined by a system, humans naturally tend to comprehend information and identify variables involved in the system. In the case of a node classification task,  ∶  → , which maps each node ∈  to a categorical vector in the label space  ∈ ℝ , two variables are naturally included, i.e., the node feature of a target node and the node label of the target node. In addition, we can construct meaningful variables based on various semantics that can be derived from the graph schema, i.e., relations and meta-paths. For example, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, given the graph schema and the target node type "author", we can obtain semantics such as "author's papers" and "co-authors" through the relation "Author-Paper" and the meta-path "Author-Paper-Author", respectively. Based on the real graph , the variables based on these semantics can be embodied by the set of neighboring nodes. For instance, in Figure <ref type="figure" target="#fig_0">2</ref>, the value of the variable based on semantic "author's papers" of the author "1" can be represented by the set of papers "1", "2", "3" and "4".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To maintain the context integrity of the target node, we consider all meta-paths starting from the target node type within a specified length limit. Thereby, in this work, the variables inherent to the task  consist of three parts: the node itself, the node's label, and the neighbor sets corresponding to all available relations/meta-paths.</head><p>Due to the heterogeneous nature, these variables typically have different forms and may exist in different feature spaces. For instance, nodes and labels are often vectors of different dimensional sizes, and the neighbor sets are collections of vectors. To address this, we need to embed all these variables into the same representation space.</p><p>For a node , its feature and its label are encoded by two linear transformations as follows:</p><formula xml:id="formula_2">= Linear ( ) (1) = Linear ( )<label>(2)</label></formula><p>Then, for the -th relation/meta-path based variable of the node , we encode the corresponding neighbor set to a fixed-size vector representation via a function NeighborSetEncoder , i.e.,</p><formula xml:id="formula_3">= NeighborSetEncoder ({ | ∈ }) = 1 | | ∑ ,<label>(3)</label></formula><p>where is the neighbor set of the target node based on the -th relation/meta-path given the graph  and | | is the number of nodes in . The set { | ∈ } represents the collection of node feature vectors of all nodes contained in . Practically, the NeighborSetEncoder can be implemented by any module that is capable of handling an unordered set of vectors, e.g., various pooling operators <ref type="bibr" target="#b26">(Hamilton et al., 2017)</ref>, Transformer <ref type="bibr" target="#b77">(Vaswani et al., 2017)</ref> and Set Transformer <ref type="bibr" target="#b44">(Lee et al., 2019)</ref>. For simplicity and motivated by <ref type="bibr">Yang et al. (2023b)</ref>, we use a simple average pooling here.</p><p>Note that these encoders, i.e., Linear , Linear and { NeighborSetEncoder }, need to be mutually independent to avoid fitting the spurious correlations among variables. For example, the widely used encoding methods in previous works <ref type="bibr" target="#b79">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b34">Hu et al., 2020;</ref><ref type="bibr" target="#b55">Lv et al., 2021)</ref>, which compute attention weights between neighboring nodes and target nodes, are not satisfactory since they introduce correlation between and { | ∈ } into the representation. Assuming there are relations/meta-paths, we can obtain a set of + 2 variables, i.e., { , } ∪ { | 1 ≤ ≤ }. For brevity, we ignore the superscript and use 0 and +1 to refer to and , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Causal Structure Modeling</head><p>Based on the assumption that a causal directed acyclic graph (DAG) can exist among the aboveconstructed variables, in this section, we illustrate how to leverage emerging causal discovery techniques to learn a structural causal model from the constructed variables. According to Definition 5, we need to learn a collection of { | 0 ≤ ≤ + 1 } of structural assignments function ∶= (pa ) where pa is the set of direct causes of , and its value should be determined by the causes.</p><p>We initialize a trainable matrix ∈ ℝ ( +2)×( +2) to represent the causal DAG, where , represent the probability that is one of the directed causes of . Note that the diagonal of the matrix is constrained to be zero since a variable cannot be its own cause, i.e., , = 0.</p><p>Based on the matrix , we define the structural assignment of a variable as:</p><formula xml:id="formula_4">(pa ) = VariableDecoder +1 ∑ =0 , ⋅ Ef fectEncoder ( ) ,<label>(4)</label></formula><p>where Ef fectEncoder is a function to computes the hidden state of the causal effect of on and VariableDecoder is a function to reconstruct the variable based on the received causal effects. We use multi-layer perceptrons (MLPs) to model and learn Ef fectEncoder and VariableDecoder without any assumption on the underlying functions of them, thanks to the universal approximation theorem <ref type="bibr" target="#b32">(Hornik et al., 1989)</ref>. In detail, we implement Equation 4 as following:</p><formula xml:id="formula_5">̂ = MLP +1 ∑ =0 , ⋅ Linear MLP , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where MLP is a three-layer MLP, MLP is a shared two-layer MLP when variable serves as a cause and imposes influence on other variables, and Linear between each pair of variables, i.e., and , are mutually independent because the function of the causal relationships between different variables can be different. Generally, Equation 4 can also be implemented based on many other powerful modules, as long as these modules have the ability to process the unordered set of weighted vectors. Our implementation is one of the simple modules, and we found that it can achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Label Prediction</head><p>Once the structural assignments collection, i.e., { | 0 ≤ ≤ + 1 } has been learned, we can have the reconstructed label representation ̂ , i.e., ̂ +1 , through +1 ∈ { | 0 ≤ ≤ + 1 }, i.e., Equation <ref type="formula" target="#formula_4">4</ref>and Equation 5 with = + 1. Thereby, we can achieve the label prediction under the causal constraints as</p><formula xml:id="formula_7">̂ = Linear -1 ( ̂ ),<label>(6)</label></formula><p>where Linear -1 is the inverse function of Linear mentioned in Equation <ref type="formula" target="#formula_2">2</ref>. However, Linear -1 may not be solvable mathematically. Therefore, we use a two-layer MLP with a shortcut to approximate this inverse linear transformation as follows:</p><formula xml:id="formula_8">̂ = Linear 2 ̂ + Linear 1 ( ̂ ) , .<label>(7)</label></formula><p>where Linear 2 and Linear 1 are the two linear transformation layers and is the activation function, such as Sigmoid( ) = 1 1+ -, ReLU( ) = max(0, ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Optimization Objectives</head><p>To ensure the structural assignment effectiveness, the least-squares loss is applied on the reconstructed variables, i.e.,</p><formula xml:id="formula_9"> = 1 ∑ =1 1 + 2 +1 ∑ =0 || -̂ || 2 , (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where is the number of training samples and || ⋅ || is the Frobenius norm. and ̂ are the raw value of the variable and the reconstructed value of the variable , respectively. Furthermore, to satisfy the directed acyclic constraint of the matrix , motivated by <ref type="bibr" target="#b109">Zheng et al. (2018)</ref>, a smooth optimizable objective can be minimized as follows:</p><formula xml:id="formula_11"> = 2 | | 2 +  , (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where and can be hyper-parameters (while we set them to 1 in this work for the sake of simplicity), and  is calculated by:</p><formula xml:id="formula_13"> = Tr( ⊙ ) --2 2 , (<label>10</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">Tr( ⊙ ) = +1 ∑ =0 ∞ ∑ =0 1 ! ( ⊙ ) , ,<label>(11)</label></formula><p>where Tr(⋅) is the trace of a square matrix, which is defined to be the sum of elements on the main diagonal (from the upper left to the lower right) of the square matrix. ⊙ is the matrix exponential of ⊙ . ( ⊙ ) , denotes the probability that can influence through steps and thus the term ( ⊙ ) , represents the probability of the existence of a cycle of length starting from node and returning to node , which indicates the probability that variable is its own cause. By allowing " " to take values from 0 to infinity, we can ensure that the result is equal to + 2 only if there are no cycles with any length in the causal DAG . This implies that there are no causal relationships between variables that can create loops. In other words, there is no variable that is its own cause or two variables are mutually causes and effects of each other.</p><p>In addition, a cross-entropy loss should be minimized to learn the inverse function of Linear in the Equation <ref type="formula" target="#formula_8">7</ref>, i.e.,</p><formula xml:id="formula_16"> = - 1 ∑ =1 -1 ∑ =0 ⋅ log( ̂ ),<label>(12)</label></formula><p>where is the number of classes. and ̂ are the probability of the class in the ground truth and the prediction, respectively. Finally, these objectives can be jointly optimized by:</p><formula xml:id="formula_17"> =  ⏟⏟⏟ Task +  +  ⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟ Causal Structure . (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>and can adjust the weight ratios of these three objectives. Generally, the larger values of and indicate that we have more confidence in the existence of an underlying causal DAG among the constructed variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computational Complexity</head><p>In the training stage, the computational cost mainly comes from the Equations 1, 2, 5 and 7. Suppose that the hidden dimension is , the complexity for HG-SCM is ( × × (2 × ) + ( 2 + 8 + 15) × × 2 ). In the evaluation stage, we only need to reconstruct the label variable, i.e., is set to + 1 in Equation <ref type="formula" target="#formula_5">5</ref>and the loop defined from line 9 to line 11 in Algorithm 1 is no longer needed. Therefore, the complexity for HG-SCM can be ( × × (2 × ) + (7 + 15) × × 2 ). Compared to other models, HG-SCM has comparable computational complexity during the training stage. For example, the computational complexities of SimpleHGN and SeHGNN are around ( × × 2 ) and ( ×( 2 +2 +1)× 2 ) <ref type="bibr">(Yang et al., 2023b)</ref>, respectively.</p><p>is the number of processed neighbors during the multi-layer aggregation and it generally exceeds 2 . However, HG-SCM can be faster during the evaluation stage due to the linear computational complexity regarding the number of relations/meta-paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To answer research questions 4 and 5, in this section, we conduct extensive experiments to validate HG-SCM task performance as well as promising generalizability and interoperability. In Section 5.1, we will describe the experimental setup, including datasets, baselines and reproducibility. In Section 5.2, we will report the experimental results. Based on comprehensive experiment outcomes, HG-SCM outperformed a series of strong baselines when applied to the independent and identical distribution (i.i.d) setting and also showed its superiority and stability under multiple out-of-distribution (o.o.d) settings. In-depth analyses of the learned DAGs in HG-SCM further demonstrated its potential in model interpretation with respect to the learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Dataset</head><p>Three open benchmark datasets including DBLP, ACM, and IMDB from the Heterogeneous Graph Benchmark (HGB) <ref type="bibr" target="#b55">(Lv et al., 2021)</ref> are employed in this work. These datasets were chosen for their ability to present a relatively complete system. Table <ref type="table" target="#tab_0">2</ref> summarizes the brief statistical information of the three  datasets, while Figure <ref type="figure" target="#fig_2">3</ref> displays the graph schema for each dataset. Below are specific introductions for the three datasets:</p><p>1. DBLP is a computer science bibliography website<ref type="foot" target="#foot_0">foot_0</ref> . The used dataset is a subset of it. Its graph schema is shown in Figure <ref type="figure" target="#fig_2">3a</ref>. The dataset comprises four types of nodes: author (N=4,057), paper (N=14,328), term (N=7,723), and venue (N=20). Additionally, there are three types of directed relations connecting two node types: an author writes a paper (N=19,645), a venue publishes a paper (N=14,328), and a paper uses a term (N=85,810). The papers' feature vectors are created based on the bag-of-words representation of their titles, while the authors' feature vectors are constructed based on the bag-of-words representation of their research keywords. The terms' feature vectors are represented by pre-trained GloVe vectors <ref type="bibr" target="#b65">(Pennington et al., 2014)</ref>, and the feature vectors of venues are represented by one-hot encoded vectors. The authors are manually labeled into four areas: Database, Data Mining, Machine Learning, and Information Retrieval. The task is to predict the author's area.</p><p>2. ACM is an international learned society for computing<ref type="foot" target="#foot_1">foot_1</ref> . The used dataset is a subset of the ACM Digital Library. Its graph schema is shown in Figure <ref type="figure" target="#fig_2">3b</ref>. The dataset contains three types of nodes: paper (N=3,025), author (N=5,959), and subject (N=56). Additionally, there are three types of directed relations connecting two node types: a paper cites a paper (N=5,343), an author writes a paper (N=9,949), and a paper belongs to a subject (N=3,025). Each paper or author or subject node is associated with a bag-of-words vector formed by 1,902 representative keywords. The papers are categorized into three classes, Database, Wireless Communication, and Data Mining. The task is to predict the paper's category.</p><p>3. IMDB is an online database of information related to films<ref type="foot" target="#foot_2">foot_2</ref> . The used dataset was a subset of it. Its graph schema is shown in Figure <ref type="figure" target="#fig_2">3c</ref>. The dataset contains four types of nodes: movie (N=4,932), director (N=2,393), actor (N=6,124), and keyword (N=7,971). In addition, there are three types of directed relations connecting two node types: an actor act in a movie (N=14,779), a director directs a movie (N=4,932), and a keyword describes a movie (N=23,610). The movie's feature vectors are bag-of-word representations of their plot keywords. The features of director and actor nodes are aggregated features from their associated movies. The movies are divided into five classes, namely Action, Comedy, Drama, Romance, and Thriller. The task is to predict the movie's category.</p><p>In the i.i.d setting, we follow the data splits used in HGB <ref type="bibr" target="#b55">(Lv et al., 2021;</ref><ref type="bibr">Yang et al., 2023b)</ref>, where node labels are split according to 24% for training, 6% for validation and 70% for test in each dataset. For the concerned o.o.d settings, we consider three types of bias that are ubiquitous in graph mining:</p><p>1. Homophily bias: Homophily is a principle of graphs whereby linked nodes often belong to the same class or have similar features. Many existing graph algorithms implicitly assume strong homophily, thus they can fail to generalize to graphs with heterophily (or low/medium level of homophily) <ref type="bibr">(Zhu et al., 2020a</ref><ref type="bibr" target="#b111">(Zhu et al., , 2021))</ref>. In the context of heterogeneous graphs, we can define the homophily level of a target node as the label consistencies between the target node and its neighbors based on various meta-paths, where the meta-paths start with and end with the same node type as the target node.</p><p>2. Degree bias: Degrees of nodes often obey a long-tailed or skewed distribution <ref type="bibr" target="#b61">(Newman and Park, 2003;</ref><ref type="bibr" target="#b51">Liu et al., 2021)</ref>. A model may have inferior performance on unseen nodes that have a different degree distribution compared to nodes in the training graph. In heterogeneous graphs, a node type can be associated with multiple relation types. Therefore, we define the degree size of a node as the degrees of the target nodes in terms of different relations.</p><p>3. Feature bias: Collected nodes in many real scenes are inherently imbalanced on features or classes <ref type="bibr" target="#b62">(Park et al., 2022;</ref><ref type="bibr" target="#b30">He et al., 2022)</ref>, hence HGNNs can be biased toward the dominant feature Red circles and arrows define the node property prediction tasks, starting from the target node types and ending with the predicted properties. Generally, relations can be represented by two consecutive node types without confusion. For example, "AP" in DBLP means</p><formula xml:id="formula_19">ℎ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← → .</formula><p>However, there exists one special case. Since</p><formula xml:id="formula_20">the relation ← ← ← ← ← ← ← ← ← ← ← ← ← ← →</formula><p>in ACM starts and ends with the same node type, "PP" can not clearly identify the relation. Therefore, we use "PcP" and "PrP" to represent</p><formula xml:id="formula_21">← ← ← ← ← ← ← ← ← ← ← ← ← ← → and ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← → , respectively.</formula><p>Based on this denotation convention, we can further represent a meta-path by a sequence of node types. For instance, "APA" can represent</p><formula xml:id="formula_22">ℎ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← → ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← → ℎ in DBLP and "PcPrP" can represent ← ← ← ← ← ← ← ← ← ← ← ← ← ← → ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← → in ACM.</formula><p>groups. The original node features in the above datasets are represented as bag-of-words, which are mostly sparse and high-dimensional. Therefore, we performed a Principal Component Analysis (PCA) on the node feature and we conducted the bias analysis based on the top 128 principal components.</p><p>To simulate the above potential bias, for each dataset, we cluster the labeled nodes into two clusters by K-MEANS <ref type="bibr" target="#b52">(Lloyd, 1982)</ref> based on their homophily level, degree size, and feature value. Then, the cluster with a larger sample size is randomly divided into the training and the validation sets in a six-to-four ratio, and the other cluster is regarded as the test set. In Figrue 4, we visualize the mean homophily levels, mean degree sizes, and mean feature values of all samples, training set samples, validation set samples, and test set samples under the o.o.d data splits for each dataset. For example, in Figure <ref type="figure" target="#fig_3">4a</ref>, on the DBLP dataset, based on the meta-path "Author-Paper-Author", the proportion of co-authors with the same label as the target authors is around 80% for all labeled authors. This proportion is nearly 100% for the authors in the training and validation sets, but only about 50% for the authors in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Baselines</head><p>We have validated the superiority of HG-SCM by comparing it with the following two categories of HGNNs: (1) relation/meta-path fusion methods including the Heterogeneous Graph Neural Network (HetGNN) <ref type="bibr" target="#b105">(Zhang et al., 2019)</ref>, Heterogeneous Graph Attention Network (HAN) <ref type="bibr">Wang et al. (2019b)</ref>, the Graph Transformer Network (GTN) <ref type="bibr" target="#b102">Yun et al. (2019)</ref>, and the Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN) <ref type="bibr">Yang et al. (2023b)</ref> and ( <ref type="formula" target="#formula_2">2</ref>) layer-by-layer methods including the Relational Graph Convolutional Network (RGCN) <ref type="bibr" target="#b70">Schlichtkrull et al. (2018)</ref>, the Composition-based Multi-Relational Graph Convolutional Networks (CompGCN) <ref type="bibr" target="#b76">Vashishth et al. (2020)</ref>, the Relation Structure-Aware Heterogeneous Graph Neural Network (RSHN) <ref type="bibr" target="#b114">(Zhu et al., 2019)</ref>, the Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding (MAGNN) <ref type="bibr" target="#b22">(Fu et al., 2020)</ref>, the Heterogeneous Graph Structural Attention Neural Network (HetSANN) <ref type="bibr" target="#b31">(Hong et al., 2020)</ref>, the Heterogeneous Graph Transformer (HGT) <ref type="bibr" target="#b34">Hu et al. (2020)</ref> and the Simple Heterogeneous Graph Neural Network (SimpleHGN) <ref type="bibr" target="#b55">Lv et al. (2021)</ref>. Note that HGT, SimpleHGN, and SeHGNN are the strongest state-of-the-art models recently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Reproducibility</head><p>All the experiments were run five times with random seeds from 0 to 4. In each experiment, we used the training set to fit the model, then used the validation set to tune the model's hyperparameters, and finally assessed and reported the performance of the model on the test set. For a fair comparison, for all models, the hidden dimension was set to 64, the number of graph layers was searched from 1 to 4, and the batch size was searched in 128, 256, 512, and 1024. All experiments are conducted on a Ubuntu (18.04) server with a Tesla V100 GPU. Baseline models except SeHGNN were implemented via the DGL <ref type="bibr">(Wang et al., 2019a)</ref> package with the PyTorch (1.10) backend based on OpenHGNN <ref type="bibr">(Han et al., 2022b)</ref>. SeHGNN was implemented according to its official code <ref type="bibr">(Yang et al., 2023b)</ref>. We set other hyperparameters of baselines, e.g., negative slope in the LeakyReLU activation and dropout ratio, following the <ref type="bibr" target="#b55">Lv et al. (2021)</ref> and <ref type="bibr">Yang et al. (2023b)</ref>. An optimizer AdamW Loshchilov and Hutter ( <ref type="formula">2018</ref>) with a learning rate of 0.001 Table <ref type="table">3</ref> Comparison on the three benchmark datasets under the official i.i.d data splits in HGB <ref type="bibr" target="#b55">(Lv et al., 2021)</ref>   <ref type="formula">2022</ref>), we will use Macro F1 and accuracy as evaluation metrics to present and discuss the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Comparison in i.i.d Setting</head><p>Table <ref type="table">3</ref> displays the comparison of HG-SCM with other baselines on the three benchmark datasets with their official i.i.d data splits, where the experimental results of all baselines except SeHGNN are referenced from <ref type="bibr" target="#b55">Lv et al. (2021)</ref>. The experimental results of SeHGNN are obtained using its official code with the hidden dimension set to 64 (for a fair comparison). It is observed that HG-SCM can achieve best performance on almost all metrics in three datasets. These experimental results prove that HG-SCM has a promising fitting ability to achieve competitive efficacy under conventional data distribution settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Comparison in o.o.d Setting</head><p>Tables 4 report the experimental results in the three o.o.d data splits (introduced in Section 5.1.1) on three benchmark datasets. HG-SCM can outperform current SOTA models in most settings. Especially in the setting of o.o.d data split by homophily, HG-SCM consistently achieves the optimal performance. Notably, It can be observed that, though the SOTA methods have already achieved a high performance, HG-SCM can still push the boundary forward to a higher level. For example, on ACM dataset (under the o.o.d data split by homophily), the best baseline's performance is 96.49% in Micro F1, HG-SCM can achieve 97.32%. Moreover, HG-SCM is the stablest model across these experiments, demonstrating its promising generalizability. In contrast, the baseline models usually show significant differences in performance under different o.o.d settings. For example, GTN achieved very competitive performance in the degree o.o.d setting while it became a relatively weak baseline in other settings. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, HG-SCM has kept its competitive performance in all settings. In detail, HG-SCM's ranking standard deviation is only 0.83, far smaller than the ranking standard deviation of other models. Additionally, the worst ranking HG-SCM reached is third, while all other models always have their worse ranking at a much lower level, e.g., sixth or even eighth place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Ablation Study</head><p>We carried out additional experiments to explore the impacts of the two objectives introduced in Section 4.4: the reconstruction loss of the structural assignments, denoted by  , and the directed acyclic constraint loss, denoted by  .</p><p>As shown in Figure <ref type="figure" target="#fig_5">6</ref>, without the optimization objective(s) of causal structure, the average of the task performances would decline. Furthermore, Figure <ref type="figure" target="#fig_4">5</ref> suggests that, overall, removing any of the two optimization objectives can decrease the average ranking and increase the standard deviation of the ranking. Specifically, removing  got an average ranking of 2.11 with a standard deviation of 1.15, and removing  got an average ranking of 2.11 with a standard deviation of 1.56. Moreover, removing both of the two objectives led to worse results, i.e., an average ranking of 2.22. These results demonstrate the importance of the objective of learning an underlying causal structure for the generalizability of the HG-SCM. An intriguing observation is that the average performance is higher when neither optimization objective is used compared to using only one of them. This highlights the interconnected nature of these two objectives. It is only when both objectives are simultaneously considered that the modeling of a Structural Causal Model (SCM) becomes possible. Therefore, when employing a single objective in isolation, it does not represent a weaker causal structure optimization objective and can even potentially have a negative impact, leading to inferior performance. We can infer the potential reasons for the decline in performance. Firstly, when solely considering the Directed Acyclic Graph (DAG) loss without incorporating the reconstruction loss, the model will tend to learn simple causal relationships that satisfy the DAG conditions but do not accurately reflect the true underlying causal relationships as deep learning models have a tendency to learn shortcuts <ref type="bibr" target="#b25">(Geirhos et al., 2020)</ref>. For instance, the model might learn that all variables are causes of the label variable. However, such a causal relationship deviates significantly from the true one and can detrimentally affect predictions on unseen data, particularly in out-of-distribution scenarios. Secondly, when solely considering the reconstruction loss without incorporating the DAG loss, the model essentially becomes an autoencoder trained on the training set. This increases the risk of overfitting the training data, resulting in diminished performance on the test set, especially in out-of-distribution scenarios. Therefore, using a single loss alone does not achieve the desired optimization process for the causal modeling process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Hyperparameters Sensitivity Analysis</head><p>Two hyper-parameters can be tuned in this work: the weight of the objective  , i.e., , and the weight of the objective  , i.e., . HG-SCM's sensitivity with respect to and is presented in Figure <ref type="figure" target="#fig_6">7</ref>. We only change the values of these two hyperparameters in this analysis, and other hyperparameters remain in the same setting as stated in Section 5.2.2. The average of the Micro F1 and Macro F1 under fixed hyperparameters were reported. We varied the values of from 1e-4 to 5e-1 and varied the values of from 1e0 to 5e3.</p><p>Although the degree of impact of hyperparameters on task performance varies across datasets, the improvement of model performance has a clear direction in all settings. For example, on the setting of DBLP with an o.o.d data split based on homophily, model performance is positively correlated with the value of and negatively correlated with the values of . Based on this observation, within the appropriate hyperparameter interval, HG-SCM can be quite robust to the adjustment of the hyperparameters.</p><p>We conducted a sensitivity analysis to examine the impact of the MLP configuration on our proposed model. In the experiments described in section 5.2.2 and Table <ref type="table" target="#tab_2">4</ref>, we kept the hidden size in hidden layers of the MLP fixed at 64 and the number of hidden layers fixed at 3. Here, we varied the hidden size from 64 to 256 and the number of hidden layers from 2 to 4 to explore different settings. The experimental results of all the different MLP configurations across all the datasets and o.o.d scenarios can be found in Figure <ref type="figure" target="#fig_7">8</ref>. We observed the following:</p><p>• Our proposed model demonstrates robustness to changes in MLP settings. In most cases, the model exhibits consistent performance across different MLP configurations. In addition, the original experiment in Table <ref type="table" target="#tab_2">4</ref> yielded an average performance of 81.39. In the additional experiments here, where the hidden size was 128 or 256 and the number of hidden layers was 2 or 4, the average performance was 81.32. These two values are very close, which indicates that the model maintains state-of-the-art or competitive performance.</p><p>• By tuning the MLP settings, we can further enhance the performance of our proposed model. In many cases shown in Figure <ref type="figure" target="#fig_7">8</ref>, the model with a new MLP configuration outperforms the reported results in Table <ref type="table" target="#tab_2">4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Variants Exploration</head><p>In this section, we analyze the impact of replacing simple modules with more complex ones on the performance of the original model. We introduce two variants:</p><p>• HG-SCM-TC: We change the encoding method of the neighbor set in Equation 3 from simple average pooling to a Transformer Convolution operator derived from <ref type="bibr" target="#b72">(Shi et al., 2021)</ref>. For each sample, this variant dynamically adjusts the weights of nodes in a neighbor set based on self-attention mechanisms <ref type="bibr" target="#b77">(Vaswani et al., 2017)</ref> among the nodes.</p><p>• HG-SCM-ST: We replace Equation 5 with an assignment function inspired by SetTransformer <ref type="bibr" target="#b44">(Lee et al., 2019)</ref>. SetTransformer is designed to model interactions among elements in an input set, utilizing attention mechanisms in both its encoder and decoder. This aligns well with the purpose of Equation <ref type="formula" target="#formula_4">4</ref>which encodes the causes of a variable and then decodes its value. For each sample, we conduct an element-wise product between the causal DAG and the self-attention matrix <ref type="bibr" target="#b77">(Vaswani et al., 2017)</ref> among the variables in the SetTransformer.</p><p>The results in Table <ref type="table" target="#tab_4">5</ref> indicate that HG-SCM-ST can outperform HG-SCM in terms of overall performance and stability across experimental settings. In addition, HG-SCM-TC achieved optimal performance under a few settings but showed a decrease in overall performance, and it also exhibited significant deterioration in several settings, particularly on the DBLP dataset. Hence, the following conclusions can be drawn:</p><p>• The use of more complex modules in neighbor set encoding may lead to reduced generalizability and stability of the model across different datasets or experimental conditions. This could be attributed to the fine-grained nature of changes in the neighbor set, with complex modules being prone to overfitting such information <ref type="bibr" target="#b25">Geirhos et al. (2020)</ref>, thereby diminishing generalizability. Furthermore, this overfitting tendency may amplify the model's preference for specific datasets or data distributions, leading to reduced stability across conditions.</p><p>• Employing more complex modules in the assignment function of SCM has the potential to enhance the overall generalizability and stability of the model. This is likely because complex assignment functions are better equipped to capture intricate causal mechanisms in the real world, thereby facilitating the overall learning process of the structural causal model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6.">Case Study</head><p>As aforementioned, HG-SCM can provide in-depth interpretations on the task level through the learned causal DAGs. Specifically, after obtaining the optimized matrix (mentioned in Section 4.2), we remove edges in in order of increasing absolute values of the weights until satisfies the directed acyclic property. This ensures that the remaining DAGs characterize relatively important and reliable causal structures. These trimmed DAGs are displayed in Figure <ref type="figure" target="#fig_8">9</ref>. We can find that they exhibit extremely high interpretability and can be partially aligned with our human knowledge/cognition.</p><p>For instance, in Figure <ref type="figure" target="#fig_8">9a</ref>, based on the learned DAG, an author's research area is considered to be labeled based on the venues in which this author has published papers, i.e., APV → Y (author's area). When authors publish their papers in specific venues, it usually signifies their active engagement within specific academic communities, as well as the recognition they garner within those communities <ref type="bibr" target="#b33">(Hsieh, 2017;</ref><ref type="bibr" target="#b17">Durmusoglu and Durmusoglu, 2021)</ref>. Therefore, using venues to predict an author's primary research area can be quite intuitive and widely accepted. This rule probably becomes more reasonable in the current landscape of academia, where interdisciplinary research is prevalent and papers across different venues often exhibit considerable semantic similarity <ref type="bibr" target="#b84">(Wang et al., 2021;</ref><ref type="bibr" target="#b0">Abramo et al., 2018)</ref>. Figuring out an author's primary research area through the venues they contribute can be advantageous in avoiding confusion arising from subtle differences in the meanings of their papers. Moreover, the author's research area, along with the target venue, can affect the content of the author's papers, i.e., APV → AP ← Y. As demonstrated by <ref type="bibr" target="#b2">Amon and Hornik (2022)</ref>, venues within the same field still exhibit significant differences in linguistic features. In practice, authors tailor their writing styles to align with the preferences of specific venues, thereby enhancing the likelihood of paper acceptance by the respective readership. Additionally, the author and the content of the papers may determine the academic partners of the author, i.e., A → APA ← AP. This is supported by research such as that conducted by <ref type="bibr" target="#b29">Hara et al. (2003)</ref>, which reveals that personal compatibility and work connections can impact collaboration. Lastly, the authors and the content of the papers may influence the terms used in writing, i.e., APA → APT ← AP, which is consistent with prior studies indicating that the choice of author keywords is not solely shaped by the paper content but is also substantially influenced by authors' prior knowledge and backgrounds <ref type="bibr" target="#b53">(Lu et al., 2020)</ref>. Notably, the causality embodied in this learned DAG is consistent with the logic of reality and human perception. Similarly, Figure <ref type="figure" target="#fig_8">9b</ref> and Figure <ref type="figure" target="#fig_8">9c</ref> provide explanations of the possible causal reasoning logic for predicting a paper's field and a movie's category, respectively. A paper's field could be determined by its subjects and authors. The plot of a movie and its actors usually determine the movie's category.</p><p>It is crucial to highlight that the automatically learned DAGs not only provide a high degree of interpretability but also present researchers with the opportunity to refine and improve the model through such feedback. Based on the DAGs, one can check whether the inference logic of the model is correct, thereby guaranteeing the model trustworthiness, which is of paramount importance for some real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Theoretical implications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From the theoretical perspective, this work analyzes and breaks through the limitations on the inference flow in current mainstream paradigms of heterogeneous graph neural networks and offers a new paradigm for heterogeneous graph learning that can dynamically learn causality-based inference flow for distinct tasks.</head><p>As our discussion with the instructive example and theoretical analysis in Section 1, under the current mainstream paradigm, the generalizability of the model is compromised because the inference flow naturally introduces spurious correlations and the interpretability of the model is limited because the inference flow is fixed. In contrast, the proposed methodology aligns well with human perception and decision-making processes and thus has the potential to achieve more satisfactory generalizability and interpretability than previous studies. Unlike previous studies that focus on the sophisticated design of sample-level aggregation and fusion modules, we highlight the importance of reflecting on and optimizing the heterogeneous graph learning paradigm. It is a more fundamental and critical issue to explore a paradigm that is causal and aligns with human knowledge or social theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Practical implications</head><p>This work has significant practical implications for real-world scenarios, including finance, policy, and healthcare, where transparency and trustworthiness are critical. Prior research often compromises task performance for better generalizability and their approaches to achieve generalizability can not enable human understanding. In contrast, HG-SCM achieves the best task performance and optimal generalizability according to the experimental results. The generalizability constrained by the causal mechanism is highly trustworthy because it aligns with human cognition and causal mechanisms are stable across environments. In addition, prior work can only provide explanations at the sample level, such as which subgraph of a sample influences the prediction. Such explanations are unreliable and difficult for humans to comprehend. In contrast, HG-SCM has the capacity to provide in-depth interpretations in accordance with the learning tasks by generating the learned causal relationships among expressive semantics involved in the heterogeneous graph. This type of interpretability aligns well with human cognitive habits, enabling researchers and developers to gain a deeper understanding of the phenomenon or task and reflect on the acceptability and reasonableness of the logic behind the phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>"What will humans do when facing a prediction task?" Facing this challenge, in this work, we proposed a novel heterogeneous graph model, HG-SCM (Heterogeneous Graph as Structural Causal Model). HG-SCM aligns with human cognition and learns reasoning logic at the task level by leveraging causal discovery and inference techniques. Through extensive experiments on multiple datasets with i.i.d and o.o.d settings, we found that HG-SCM earns task performance superiority while accomplishing inherent interpretability and Table <ref type="table">6</ref> The effect of using the average of estimated values of the self-attention matrices of training samples (globalmean), instead of dynamic sample-wise self-attention in the transformer-based semantic fusion module in SeHGNN <ref type="bibr">(Yang et al., 2023b)</ref>  enhanced generalizability. The DAGs learned by HG-SCM perceive strong interpretations of the learning tasks. In the future, we will further enhance the model's generalizability by incorporating advanced causal techniques. Furthermore, to improve the comprehensiveness and persuasiveness of the assessments regarding model interpretability, we will undertake additional quantitative evaluations with human assistance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture of HG-SCM.</figDesc><graphic coords="7,38.88,53.88,466.52,171.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>The training forward process of HG-SCM.Input: Graph  = (,); Features of all nodes ∈ ℝ ||× ; Target node type ; training nodes  * and their labels ∈ ℝ × ; valid relations/meta-paths. Output: Task Prediction ̂ ∈ ℝ × . // define model parameters 1: Ego transformation Linear ; Label transformation Linear ; Causal DAG matrix ; neighbor set encoder { NeighborSetEncoder | 1 ≤ ≤ } for each relation/meta-path; + 2 variable-wise causal effect encoders { Ef fectEncoder | 0 ≤ ≤ + 1 }; ( +2)×( +1) pair-wise causal effect transformation { Linear | 0 ≤ , ≤ + 1 ∧ ≠ }; + 2 variable decoders { VariableDecoder | 0 ≤ ≤ + 1 }; Inverse label encoder Linear . // define function 2: function STRUCTURALASSIGNMENT( ) // reconstruct a variable based on a DAG 3:̂ ∈ ℝ × ← Equation 5, given , , { Ef fectEncoder | ≠ }, { Linear | ≠ } and VariableDecoder × ← Equation 1, given  * and 7: +1 ∈ ℝ × ← Equation 2, given 8: { ∈ ℝ × | 1 ≤ ≤ } ← Equation 3,given  and // This loop is implemented by parallel matrix multiplication 9: for 0 ≤ ≤ + 1 do 10: ̂ ∈ ℝ × ←STRUCTURALASSIGNMENT( ) 11: end for 12: ̂ ∈ ℝ × ← Equation 6, given ̂ +1 and Linear // optimization objectives 13:  ← Equation 8, given { ( , ̂ ) | 0 ≤ ≤ + 1 } 14:  ← Equation 9, given 15:  ← Equation 12, given and ̂</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graph Schemas. Black circles represent node types in the datasets. Black arrows describe directional relations between node types. The shown relations and their inverse relations together form all the edge types.Red circles and arrows define the node property prediction tasks, starting from the target node types and ending with the predicted properties. Generally, relations can be represented by two consecutive node types without</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of data distribution difference in o.o.d settings. The greater the difference in height between the pink bar and the two corresponding light and deep blue bars, the greater the difference in the data distribution between the test set and the training/validation set. (a) homophily level (y-axis) in terms of specific meta-path (x-axis). (b) log-transformed degree size (y-axis) based on specific relation (x-axis). (c) component values (y-axis) of the top five principal components (denoted by C1 to C5) of the PCA-decomposed features of the target nodes (x-axis). Please refer to Figure 3 for interpretation of the abbreviated relations and meta-paths.</figDesc><graphic coords="14,62.16,53.83,419.90,377.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Box plot of the statistics of the performance rankings. and represent the mean and the standard deviation of rankings in all the o.o.d experiments. • represents fliers and "worst" represents the lowest ranking. Dashed boxes are variants in the ablation study and their rankings are what they would get if they replaced the normal HG-SCM in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation Study: the averaged performance of variants of HG-SCM under all o.o.d settings.</figDesc><graphic coords="17,85.44,331.37,373.22,151.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sensitivity Analysis of and .</figDesc><graphic coords="18,38.88,119.86,466.51,445.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sensitivity Analysis of the hidden size of the hidden layer and the number of hidden layers (#Layer) in MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Learned DAGs for different tasks: these DAGs can provide explanations of the possible causal relationships in the task. Please refer to Figure 3 for interpretation of the abbreviated relations and meta-paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Node Types</cell><cell>#Edges</cell><cell>#Edge Types</cell><cell>Target</cell><cell>#Classes</cell></row><row><cell>DBLP</cell><cell>26,128</cell><cell>4</cell><cell>239,566</cell><cell>6</cell><cell>author</cell><cell>4</cell></row><row><cell>ACM</cell><cell>9,040</cell><cell>3</cell><cell>36,634</cell><cell>6</cell><cell>paper</cell><cell>3</cell></row><row><cell>IMDB</cell><cell>21,420</cell><cell>4</cell><cell>86,642</cell><cell>6</cell><cell>movie</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Bold and underline indicate the best and the top 3 performance, respectively.</figDesc><table><row><cell>Methods</cell><cell cols="2">DBLP</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell></row><row><cell></cell><cell>Macro F1</cell><cell>Accuracy</cell><cell>Macro F1</cell><cell>Accuracy</cell><cell>Macro F1</cell><cell>Accuracy</cell></row><row><cell>RGCN</cell><cell>91.52±0.50</cell><cell>92.07±0.50</cell><cell>91.55±0.74</cell><cell>91.41±0.75</cell><cell>58.85±0.26</cell><cell>62.05±0.15</cell></row><row><cell>HAN</cell><cell>91.67±0.49</cell><cell>92.05±0.62</cell><cell>90.89±0.43</cell><cell>90.79±0.43</cell><cell>57.74±0.96</cell><cell>64.63±0.58</cell></row><row><cell>GTN</cell><cell>93.52±0.55</cell><cell>93.97±0.54</cell><cell>91.31±0.70</cell><cell>91.20±0.71</cell><cell>60.47±0.98</cell><cell>65.14±0.45</cell></row><row><cell>RSHN</cell><cell>93.34±0.58</cell><cell>93.81±0.55</cell><cell>90.50±1.51</cell><cell>90.32±1.54</cell><cell>59.85±3.21</cell><cell>64.22±1.03</cell></row><row><cell>HetGNN</cell><cell>91.76±0.43</cell><cell>92.33±0.41</cell><cell>85.91±0.25</cell><cell>85.05±0.25</cell><cell>48.25±0.67</cell><cell>51.16±0.65</cell></row><row><cell>MAGNN</cell><cell>93.28±0.51</cell><cell>93.76±0.45</cell><cell>90.88±0.64</cell><cell>90.77±0.65</cell><cell>56.49±3.20</cell><cell>64.67±1.67</cell></row><row><cell>HetSANN</cell><cell>78.55±2.42</cell><cell>80.56±1.50</cell><cell>90.02±0.35</cell><cell>89.91±0.37</cell><cell>49.47±1.21</cell><cell>57.68±0.44</cell></row><row><cell>HGT</cell><cell>93.01±0.23</cell><cell>93.49±0.25</cell><cell>91.12±0.76</cell><cell>91.00±0.76</cell><cell>63.00±1.19</cell><cell>67.20±0.57</cell></row><row><cell cols="2">SimpleHGN 94.01±0.24</cell><cell>94.46±0.22</cell><cell>93.42±0.44</cell><cell>93.35±0.45</cell><cell cols="2">63.53±1.36 67.36±0.57</cell></row><row><cell>SeHGNN</cell><cell>94.49±0.20</cell><cell>94.89±0.18</cell><cell>93.60±0.44</cell><cell>93.51±0.45</cell><cell>64.67±0.29</cell><cell>65.98±0.12</cell></row><row><cell>HG-SCM</cell><cell cols="6">94.51±0.15 94.90±0.15 93.64±0.31 93.56±0.32 65.34±0.33 66.90±0.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparison on the three benchmark datasets under the three o.o.d data splits. Bold and underline indicate the best and the top 3 performance, respectively.</figDesc><table><row><cell>DBLP</cell><cell>By Homophily Macro F1 Accuracy</cell><cell>By Degree Macro F1 Accuracy</cell><cell>By Feature Macro F1 Accuracy</cell></row><row><cell>RGCN</cell><cell cols="3">88.23±0.83 88.55±0.73 88.87±0.37 89.76±0.27 87.00±3.47 90.20±3.05</cell></row><row><cell cols="4">CompGCN 85.27±1.86 85.82±1.61 86.65±0.89 87.59±0.95 85.65±3.89 89.35±2.67</cell></row><row><cell>HGT</cell><cell cols="3">88.48±1.10 88.98±1.02 89.37±0.80 90.27±0.77 85.00±4.77 88.65±3.64</cell></row><row><cell cols="4">SimpleHGN 89.06±1.17 89.40±1.08 89.42±1.29 90.32±1.24 85.27±5.22 89.11±4.31</cell></row><row><cell>HAN</cell><cell cols="3">82.42±1.33 82.81±1.22 85.41±1.39 86.42±1.38 83.72±4.33 87.79±3.43</cell></row><row><cell>GTN</cell><cell cols="3">88.38±1.09 88.70±1.07 90.26±0.80 90.98±0.74 84.29±2.71 88.00±3.13</cell></row><row><cell cols="4">SeHGNN 88.85±0.30 89.14±0.29 90.88±0.32 91.56±0.31 86.05±5.12 89.31±4.37</cell></row><row><cell cols="4">HG-SCM 89.35±0.75 89.63±0.75 90.11±0.70 90.85±0.60 87.69±5.39 90.48±3.98</cell></row><row><cell>ACM</cell><cell>By Homophily Macro F1 Accuracy</cell><cell>By Degree Macro F1 Accuracy</cell><cell>By Feature Macro F1 Accuracy</cell></row><row><cell>RGCN</cell><cell cols="3">92.65±1.16 93.13±1.10 92.93±0.93 93.72±0.80 92.38±0.93 92.25±0.96</cell></row><row><cell cols="4">CompGCN 94.42±0.58 94.84±0.55 92.86±0.65 93.65±0.55 91.89±0.22 91.79±0.26</cell></row><row><cell>HGT</cell><cell cols="3">91.87±0.74 92.28±0.73 92.07±0.75 92.76±0.73 90.72±1.37 90.63±1.38</cell></row><row><cell cols="4">SimpleHGN 93.43±1.43 93.86±1.36 92.54±1.01 93.33±0.83 91.65±1.12 91.57±1.00</cell></row><row><cell>HAN</cell><cell cols="3">91.72±1.64 92.18±1.66 89.69±1.47 90.80±1.51 91.11±1.26 90.97±1.29</cell></row><row><cell>GTN</cell><cell cols="3">93.94±1.96 94.36±1.87 93.56±0.50 94.29±0.43 92.54±0.48 92.43±0.56</cell></row><row><cell cols="4">SeHGNN 96.21±0.66 96.49±0.62 94.26±0.50 94.90±0.44 93.77±0.94 93.66±0.93</cell></row><row><cell cols="4">HG-SCM 97.10±1.12 97.32±1.06 94.24±0.30 94.94±0.27 94.18±0.70 94.11±0.67</cell></row><row><cell>IMDB</cell><cell>By Homophily Macro F1 Accuracy</cell><cell>By Degree Macro F1 Accuracy</cell><cell>By Feature Macro F1 Accuracy</cell></row><row><cell>RGCN</cell><cell cols="3">50.49±2.49 55.77±1.43 34.55±3.89 51.85±2.10 55.79±9.63 64.52±6.07</cell></row><row><cell cols="4">CompGCN 41.67±0.78 49.74±0.95 30.63±3.82 49.76±4.07 56.93±6.21 62.79±5.49</cell></row><row><cell>HGT</cell><cell cols="3">49.01±0.96 54.85±0.70 30.04±1.91 49.75±2.42 56.77±6.05 65.59±4.11</cell></row><row><cell cols="4">SimpleHGN 49.47±2.39 55.24±1.46 43.49±4.68 54.70±3.62 56.56±5.91 63.99±5.91</cell></row><row><cell>HAN</cell><cell cols="3">48.92±2.04 54.07±1.40 44.33±5.06 51.58±3.91 55.71±4.08 62.92±4.50</cell></row><row><cell>GTN</cell><cell cols="3">47.90±1.35 54.23±1.12 40.70±3.80 54.13±2.49 55.54±5.25 63.97±5.63</cell></row><row><cell cols="4">SeHGNN 65.20±0.56 65.86±0.58 43.87±1.42 48.04±1.29 62.88±4.00 67.31±3.43</cell></row><row><cell cols="4">HG-SCM 65.26±0.67 66.17±0.66 43.51±2.61 52.37±1.63 61.03±5.14 66.64±2.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. For example, in the IMDB dataset with a homophily o.o.d setting, increasing the hidden size from 64 to 128 improves the model's Macro F1 score from 65.26 to 66.18. Similarly, reducing the number of hidden layers from 3 to 2 increases the model's Macro F1 score from 65.26 to 66.68.</figDesc><table><row><cell></cell><cell cols="4">DBLP -By Homophily</cell><cell></cell><cell cols="4">DBLP -By Degree</cell><cell></cell><cell cols="4">DBLP -By Feature</cell><cell></cell><cell cols="3">DBLP -By Homophily</cell><cell></cell><cell cols="3">DBLP -By Degree</cell><cell></cell><cell>DBLP -By Feature</cell></row><row><cell>92</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>92</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>95</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>92</cell><cell cols="2">Macro F1</cell><cell>Accuracy</cell><cell>92</cell><cell cols="2">Macro F1</cell><cell>Accuracy</cell><cell>95</cell><cell>Macro F1</cell><cell>Accuracy</cell></row><row><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell>89</cell></row><row><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell>86</cell><cell></cell><cell></cell><cell></cell><cell>83</cell></row><row><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell>77</cell></row><row><cell>80</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>80</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>71</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>80</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>80</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>71</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell></row><row><cell></cell><cell cols="4">ACM -By Homophily</cell><cell></cell><cell cols="4">ACM -By Degree</cell><cell></cell><cell cols="4">ACM -By Feature</cell><cell></cell><cell cols="3">ACM -By Homophily</cell><cell></cell><cell cols="3">ACM -By Degree</cell><cell></cell><cell>ACM -By Feature</cell></row><row><cell>99</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>96</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>97</cell><cell cols="2">Macro F1</cell><cell cols="2">Accuracy</cell><cell>99</cell><cell cols="2">Macro F1</cell><cell>Accuracy</cell><cell>96</cell><cell cols="2">Macro F1</cell><cell>Accuracy</cell><cell>97</cell><cell>Macro F1</cell><cell>Accuracy</cell></row><row><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell></cell><cell>94</cell></row><row><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91</cell></row><row><cell>93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell>88</cell></row><row><cell>91</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>90</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>85</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>91</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>90</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>85</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell></row><row><cell>71</cell><cell cols="4">IMDB -By Homophily Macro F1 Accuracy</cell><cell>62</cell><cell cols="4">IMDB -By Degree Macro F1 Accuracy</cell><cell>72</cell><cell cols="4">IMDB -By Feature Macro F1 Accuracy</cell><cell>71</cell><cell cols="3">IMDB -By Homophily Macro F1 Accuracy</cell><cell>62</cell><cell cols="3">IMDB -By Degree Macro F1 Accuracy</cell><cell>72</cell><cell>IMDB -By Feature Macro F1 Accuracy</cell></row><row><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell>54</cell><cell></cell><cell></cell><cell></cell><cell>64</cell></row><row><cell>63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63</cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell></cell><cell></cell><cell></cell><cell>56</cell></row><row><cell>59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59</cell><cell></cell><cell></cell><cell></cell><cell>38</cell><cell></cell><cell></cell><cell></cell><cell>48</cell></row><row><cell>55</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>30</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>40</cell><cell>64</cell><cell cols="2">128 Hidden Size</cell><cell>256</cell><cell>55</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>30</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell><cell>40</cell><cell>2</cell><cell>3 #Layer</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Variants comparison on the three benchmark datasets under the three o.o.d data splits. Bold and underline indicate the best Macro F1 and Accuracy among the three variants in each row.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">HG-SCM</cell><cell cols="2">HG-SCM-TC</cell><cell cols="2">HG-SCM-ST</cell></row><row><cell>Dataset</cell><cell>OOD Type</cell><cell>Macro F1</cell><cell>Accuracy</cell><cell>Macro F1</cell><cell>Accuracy</cell><cell>Macro F1</cell><cell>Accuracy</cell></row><row><cell>ACM</cell><cell cols="2">By Homophily 97.10±1.12</cell><cell cols="4">97.32±1.06 97.19±0.33 97.44±0.31 96.97±0.80</cell><cell>97.22±0.76</cell></row><row><cell>ACM</cell><cell>By Degree</cell><cell cols="3">94.24±0.30 94.94±0.27 94.06±0.10</cell><cell>94.78±0.09</cell><cell>94.10±0.44</cell><cell>94.81±0.37</cell></row><row><cell>ACM</cell><cell>By Feature</cell><cell cols="3">94.18±0.70 94.11±0.67 92.73±0.33</cell><cell>92.69±0.28</cell><cell>94.15±1.02</cell><cell>94.09±1.01</cell></row><row><cell>DBLP</cell><cell cols="2">By Homophily 89.35±0.75</cell><cell>89.63±0.75</cell><cell>85.18±0.40</cell><cell cols="3">85.70±0.42 89.92±0.72 90.13±0.68</cell></row><row><cell>DBLP</cell><cell>By Degree</cell><cell cols="3">90.11±0.70 90.85±0.60 84.26±1.42</cell><cell>85.58±1.35</cell><cell>89.11±0.98</cell><cell>90.03±0.89</cell></row><row><cell>DBLP</cell><cell>By Feature</cell><cell>87.69±5.39</cell><cell>90.48±3.98</cell><cell>82.24±5.25</cell><cell cols="3">86.04±3.85 88.44±3.10 90.95±2.62</cell></row><row><cell>IMDB</cell><cell cols="2">By Homophily 65.26±0.67</cell><cell>66.17±0.66</cell><cell>64.95±0.55</cell><cell cols="3">65.67±0.56 65.31±0.73 66.45±0.58</cell></row><row><cell>IMDB</cell><cell>By Degree</cell><cell>43.51±2.61</cell><cell cols="3">52.37±1.63 43.70±2.74 51.50±2.60</cell><cell cols="2">41.82±5.53 54.09±5.90</cell></row><row><cell>IMDB</cell><cell>By Feature</cell><cell>61.03±5.14</cell><cell cols="3">66.64±2.58 63.25±1.99 66.86±4.28</cell><cell cols="2">63.19±3.71 67.46±3.60</cell></row><row><cell></cell><cell>Average</cell><cell>80.27</cell><cell>82.50</cell><cell>78.62</cell><cell>80.70</cell><cell>80.33</cell><cell>82.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>on test sets. 97±0.33 94.62±0.36 93.81±0.41 93.86±0.40 66.06±0.12 64.68±0.24 Global-mean 94.97±0.43 94.62±0.45 93.82±0.42 93.89±0.41 66.05±0.12 64.67±0.24</figDesc><table><row><cell></cell><cell cols="2">DBLP</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell></row><row><cell>Metric</cell><cell>Micro F1</cell><cell>Macro F1</cell><cell>Micro F1</cell><cell>Macro F1</cell><cell>Micro F1</cell><cell>Macro F1</cell></row><row><cell cols="2">Sample-wise 94.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://dblp.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.acm.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.imdb.com/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Learning Level Experiment</head><p>As shown in Table <ref type="table">6</ref>, there is no significant performance difference on test sets when using the average of estimated values of the self-attention matrices of training samples, instead of dynamic sample-wise self-attention in the transformer-based semantics fusion module in SeHGNN <ref type="bibr">(Yang et al., 2023b)</ref>. These empirical results suggest that, compared with conventional sample-wise learning modules, an advanced task-level semantic fusion module may have more potential. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRediT authorship contribution statement</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison of two approaches for measuring interdisciplinary research output: The disciplinary diversity of authors vs the disciplinary diversity of the reference list</title>
		<author>
			<persName><forename type="first">G</forename><surname>Abramo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>D'angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2018.09.001</idno>
		<ptr target="https://doi.org/10.1016/j.joi.2018.09.001" />
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1182" to="1193" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Field experiments: Design, analysis, and interpretation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>W.W. Norton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is it all bafflegab? -linguistic and meta characteristics of research articles in prestigious economics journals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2022.101284</idno>
		<ptr target="https://doi.org/10.1016/j.joi.2022.101284" />
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2022">2022. 101284</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explainability techniques for graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshops, 2019 Workshop on Learning and Reasoning with Graph-Structured Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On Pearl&apos;s Hierarchy and the Foundations of Causal Inference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ibeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501714.3501743</idno>
		<ptr target="https://doi.org/10.1145/3501714.3501743" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="507" to="556" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 ed.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network analysis in the social sciences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Borgatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labianca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="892" to="895" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pgra: Projected graph relation-feature attention network for heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chairatanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murata</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2021.04.070</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION SCIENCES</title>
		<imprint>
			<biblScope unit="volume">570</biblScope>
			<biblScope unit="page" from="769" to="794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-relation assisted knowledgeaware coupled graph neural network for recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2023.103353</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2023.103353" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">103353</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differentiable DAG sampling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9wOQOgNe-w" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hhgn: A hierarchical reasoning-based heterogeneous graph neural network for fact verification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2021.102659</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2021.102659" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">102659</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph theory: An algorithmic approach (Computer science and applied mathematics)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Christofides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Academic Press, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering symbolic models from deep learning with inductive biases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17429" to="17442" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stable learning establishes some common ground between causal inference and machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<idno type="DOI">10.1038/S42256-022-00445-Z</idno>
		<idno>doi:10.1038/S42256-022-00445-Z</idno>
		<ptr target="https://doi.org/10.1038/s42256-022-00445-z" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="110" to="115" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bcd nets: Scalable variational approaches for bayesian causal discovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/39799c" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021. 18791e8d7eb29704fc5bc04ac8-Paper.pdf</date>
			<biblScope unit="page" from="7095" to="7110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous deep graph convolutional network with citation relational bert for covid-19 inline citation recommendation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2022.118841</idno>
	</analytic>
	<monogr>
		<title level="j">EXPERT SYSTEMS WITH APPLICATIONS</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social network analysis for routing in disconnected delay-tolerant manets</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haahr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM international symposium on Mobile ad hoc networking and computing</title>
		<meeting>the 8th ACM international symposium on Mobile ad hoc networking and computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A TOPSIS model for understanding the authors choice of journal selection</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D U</forename><surname>Durmusoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durmusoglu</surname></persName>
		</author>
		<idno type="DOI">10.1007/S11192-020-03770-5</idno>
		<idno>doi:10.1007/S11192-020-03770-5</idno>
		<ptr target="https://doi.org/10.1007/s11192-020-03770-5" />
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="521" to="543" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Debiased graph neural networks with agnostic label selection bias</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2022.3141260</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A local method for identifying causal relations under markov</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2022.103669</idno>
	</analytic>
	<monogr>
		<title level="j">ARTIFICIAL INTELLIGENCE</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On low-rank directed acyclic graphs and causal structure learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2023.3273353</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2019.2957786</idno>
		<idno>doi:10.1109/TKDE.2019.2957786</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2019.2957786" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2493" to="2504" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MAGNN: metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380297</idno>
		<idno>doi:10.1145/3366423.3380297</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380297" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust representation learning for heterogeneous attributed networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2023.01.038</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION SCIENCES</title>
		<imprint>
			<biblScope unit="volume">628</biblScope>
			<biblScope unit="page" from="22" to="49" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active invariant causal prediction: Experiment selection through stability</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gamella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/b" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 197ffdef2ddc3308584dce7afa3661b-Paper.pdf</date>
			<biblScope unit="page" from="15464" to="15475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Openhgnn: An open source toolkit for heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511808.3557664</idno>
		<idno>doi:10.1145/3511808.3557664</idno>
		<ptr target="https://doi.org/10.1145/3511808.3557664" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3993" to="3997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Openhgnn: An open source toolkit for heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An emerging view of scientific collaboration: Scientists&apos; perspectives on collaboration and factors that impact collaboration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Sonnenwald</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.10291</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.10291" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="952" to="965" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causpref: Causal preference learning for out-of-distribution recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3511969</idno>
		<idno>doi:10.1145/3485447.3511969</idno>
		<ptr target="https://doi.org/10.1145/3485447.3511969" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An attention-based graph neural network for heterogeneous structural learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.5833</idno>
		<idno>doi:10.1609/aaai.v34i04.5833</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i04.5833" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4132" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90020-8</idno>
		<idno>doi:10.1016/0893-6080(89)90020-8</idno>
		<ptr target="https://doi.org/10.1016/0893-6080(89)90020-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Author publication preferences and journal competition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23657</idno>
		<ptr target="https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23657" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphlime: Local interpretable model explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2022.3187455</idno>
		<idno>doi:10.1109/TKDE.2022.3187455</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2022.3187455" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6968" to="6972" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Task-oriented genetic activation for large-scale complex heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1581" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-language citation recommendation via hierarchical representation learning on heterogeneous graph</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structural agnostic modeling: Adversarial learning of causal graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF MACHINE LEARNING RESEARCH</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of bayesian network structure learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Kitson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C C</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chobtham</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-022-10351-w</idno>
	</analytic>
	<monogr>
		<title level="j">ARTIFICIAL INTELLIGENCE REVIEW</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="8721" to="8814" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust optimization as data augmentation for large-scale graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.00016</idno>
		<idno>doi:10.1109/CVPR52688.2022.00016</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.00016" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18">2022. June 18-24, 2022</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
	<note>CVPR 2022</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Castle: Regularization via auxiliary causal graph discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gradient-based neural dag learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/lee19d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">2023a. Nonlinear causal discovery with confounders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2023.2179490</idno>
		<idno>doi:10.1080/01621459.2023.2179490</idno>
		<ptr target="https://doi.org/10.1080/01621459.2023.2179490" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hetregat-fc: Heterogeneous residual graph attention network via feature completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2023.03.034</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION SCIENCES</title>
		<imprint>
			<biblScope unit="volume">632</biblScope>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/lin21d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
		<respStmt>
			<orgName>PMLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Confidence may cheat: Self-training on graph neural networks under distribution shift</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3512172</idno>
		<idno>doi:10.1145/3485447.3512172</idno>
		<ptr target="https://doi.org/10.1145/3485447.3512172" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;22: The ACM Web Conference 2022, Virtual Event</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Laforest</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Troncy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Simperl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Herman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Médini</surname></persName>
		</editor>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-25">2022. April 25 -29, 2022</date>
			<biblScope unit="page" from="1248" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Independence promoted graph disentangled networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V34I04.5929</idno>
		<idno>doi:10.1609/AAAI.V34I04.5929</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i04.5929" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4916" to="4923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tail-gnn: Tail-node graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467276</idno>
		<idno>doi:10.1145/3447548.3467276</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467276" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="1109" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aspect sentiment analysis with heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.102953</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.102953" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="1982">1982. 2018. 2022. 102953</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
	<note>Information Processing &amp; Management</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How do authors select keywords? a preliminary study of author keyword selection behavior</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2020.101066</idno>
		<ptr target="https://doi.org/10.1016/j.joi.2020.101066" />
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020">2020. 101066</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020. 2020/hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html. 2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="426" to="427" />
		</imprint>
	</monogr>
	<note>When causal inference meets deep learning</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467350</idno>
		<idno>doi:10.1145/3447548.3467350</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467350" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery; Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/ma19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA, PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 2019, 9-15 June 2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Concept distillation in graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Magister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Siciliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciravegna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-44070-0_12</idno>
		<idno>doi:10.1007/978-3-031-44070-0\_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-44070-0_12" />
	</analytic>
	<monogr>
		<title level="m">Explainable Artificial Intelligence -First World Conference, xAI 2023, Lisbon, Portugal</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023-07-26">2023. July 26-28, 2023</date>
			<biblScope unit="page" from="233" to="255" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable graph learning via stochastic attention mechanism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15524" to="15543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A relation-aware heterogeneous graph convolutional network for relationship prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2022.12.059</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION SCIENCES</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="page" from="311" to="323" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Causal interpretability for machine learning-problems, methods and evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moraffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Why social networks are different from other types of networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">36122</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graphens: Neighbor-aware ego network synthesis for class-imbalanced node classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MXEl7i-iru" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep structural causal models for tractable counterfactual inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/0987" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="857" to="869" />
		</imprint>
	</monogr>
	<note>b8b338d6c90bbedd8631bc499221-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://aclanthology.org/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Explainability methods for graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
			<affiliation>
				<orgName type="collaboration">Computer Vision Foundation / IEEE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
			<affiliation>
				<orgName type="collaboration">Computer Vision Foundation / IEEE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
			<affiliation>
				<orgName type="collaboration">Computer Vision Foundation / IEEE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
			<affiliation>
				<orgName type="collaboration">Computer Vision Foundation / IEEE</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Computer Vision Foundation / IEEE</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01103</idno>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Network" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. 2019. June 16-20, 2019</date>
			<biblScope unit="page" from="10772" to="10781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Heterogeneous graph-based joint representation learning for users and pois in location-based social network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.102151</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2019.102151" />
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management 57, 102151</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Prototype-based interpretable graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Capobianco</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAI.2022.3222618</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Higher-order explanations of graph neural networks via relevant walks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schnake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lederer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3115452</idno>
		<idno>doi:10.1109/TPAMI.2021.3115452</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3115452" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7581" to="7596" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Masked label prediction: Unified message passing model for semisupervised classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/214</idno>
		<idno>doi:10.24963/ijcai.2021/214</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/214" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08">2021. 19-27 August 2021</date>
			<biblScope unit="page" from="1548" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Causal discovery with a mixture of dags</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10994-022-06159-y</idno>
	</analytic>
	<monogr>
		<title level="j">MACHINE LEARNING</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Finhgnn: A conditional heterogeneous graph learning to address relational attributes for stock predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2022.11.013</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION SCIENCES</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="page" from="317" to="335" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylA_C4tPr" />
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Heterogeneous network representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Vdong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4861" to="4867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings, OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">D&apos;ya like dags? a survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<idno type="DOI">10.1145/3527154</idno>
		<idno>doi:10.1145/3527154</idno>
		<ptr target="https://doi.org/10.1145/3527154" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Robust graph learning with graph convolutional network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.102916</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.102916" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="2022">2022. 102916</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">2023a. Meta-learning adaptation network for few-shot link prediction in heterogeneous social networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2023.103418</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2023.103418" />
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management 60, 103418</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Understanding interdisciplinary knowledge integration through citance analysis: A case study on ehealth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.joi.2021.101214</idno>
		<ptr target="https://doi.org/10.1016/j.joi.2021.101214" />
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2021">2021. 101214</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Hgnn: Hyperedge-based graph neural network for mooc course recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.102938</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.102938" />
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management 59, 102938</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Reinforced causal explainer for graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Regr: Relation-aware graph reasoning framework for video question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2023.103375</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION PROCESSING &amp; MANAGEMENT</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Dags with no fears: A closer look at continuous optimization for learning bayesian networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 28a7602724ba16600d5ccc644c19bf18-Paper.pdf</date>
			<biblScope unit="page" from="3895" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Adaptive path selection for dynamic image captioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2022.3155795</idno>
		<idno>doi:10.1109/TCSVT.2022.3155795</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2022.3155795" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5762" to="5775" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Unifying knowledge iterative dissemination and relational reconstruction network for image-text matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.103154</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.103154" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">103154</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">An efficiency relation-specific graph transformation network for knowledge graph representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.103076</idno>
	</analytic>
	<monogr>
		<title level="j">INFORMATION PROCESSING &amp; MANAGEMENT</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning causal representations for robust domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3119185</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2750" to="2764" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Simple and efficient heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i9.26283</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/26283" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10816" to="10824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020. 3fe230348e9a12c13120749e3f9fa4cd-Abstract.html</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Graph information bottleneck for subgraph recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Heterogeneous graph representation learning with relation awareness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2022.3160208</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5935" to="5947" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/yu19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">XGNN: towards model-level explanations of graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403085</idno>
		<idno>doi:10.1145/3394486.3403085</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403085" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</editor>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3204236</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5782" to="5799" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11983" to="11993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Heterogeneous graph convolution based on in-domain self-supervision for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2022.119240</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2022.119240" />
	</analytic>
	<monogr>
		<title level="m">Expert Systems with Applications 213, 119240</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Causality-based ctr prediction using graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2022.103137</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2022.103137" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2023">2023. 103137</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Deep stable learning for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5372" to="5382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Protgnn: Towards self-explaining graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V36I8.20898</idno>
		<idno>doi:10.1609/AAAI.V36I8.20898</idno>
		<ptr target="https://doi.org/10.1609/aaai.v36i8.20898" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V35I12.17315</idno>
		<idno>doi:10.1609/AAAI.V35I12.17315</idno>
		<ptr target="https://doi.org/10.1609/aaai.v35i12.17315" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="11015" to="11023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Dags with NO TEARS: continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018. 51419ffb23ca3fd5050202f9c3d-Abstract.html</date>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric dags</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v108/zheng20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, PMLR</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics, PMLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/17332" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="11168" to="11176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/58" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
	<note>ae23d878a47004366189884c2f8440-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Relation structure-aware heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2019.00203</idno>
		<idno>doi:10.1109/ICDM.2019.00203</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2019.00203" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining, ICDM 2019</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-08">2019. November 8-11, 2019</date>
			<biblScope unit="page" from="1534" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
