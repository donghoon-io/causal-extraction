<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonlinear causal discovery with confounders *</title>
				<funder ref="#_KwbsD2t">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_Kv8dtQz #_P8XeyQs #_b4EJube #_JrnZdRR #_Y3fuesw #_pWrtr3j">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-04-30">30 Apr 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunlin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Statistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaotong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Statistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Division of Biostatistics</orgName>
								<orgName type="institution">Uni- versity of Minnesota</orgName>
								<address>
									<postCode>55455</postCode>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonlinear causal discovery with confounders *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-30">30 Apr 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.03178v3[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Directed acyclic graph</term>
					<term>Deconfounding</term>
					<term>Neural networks</term>
					<term>Variable selection</term>
					<term>Gene regulatory networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article introduces a causal discovery method to learn nonlinear relationships in a directed acyclic graph with correlated Gaussian errors due to confounding. First, we derive model identifiability under the sublinear growth assumption. Then, we propose a novel method, named the Deconfounded Functional Structure Estimation (DeFuSE), consisting of a deconfounding adjustment to remove the confounding effects and a sequential procedure to estimate the causal order of variables. We implement DeFuSE via feedforward neural networks for scalable computation. Moreover, we establish the consistency of DeFuSE under an assumption called the strong causal minimality. In simulations, DeFuSE compares favorably against state-of-the-art competitors that ignore confounding or nonlinearity. Finally, we demonstrate the utility and effectiveness of the proposed approach with an application to gene regulatory network analysis. The Python implementation is available at <ref type="url" target="https://github.com/chunlinli/defuse">https://github.com/chunlinli/defuse</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal relationships are fundamental to understanding the mechanisms of complex systems and the consequences of actions in natural and social sciences. Causal discovery, namely to learn a directed acyclic graph (DAG) representing causal relationships, arises in many applications. In gene network analysis, scientists explore gene-to-gene regulatory relationships to unravel the genetic underpinnings of a disease <ref type="bibr" target="#b35">(Sachs et al., 2005)</ref>. In such a situation, latent confounders such as environmental or lifestyle factors could introduce spurious associations or mask causal relationships in observed gene expression levels, making causal discovery more challenging. Currently, causal discovery from observational data is an important research topic as randomized experiments are often unethical, expensive, or infeasible. In this paper, we concentrate on the discovery of causal relationships in the presence of latent confounders.</p><p>Linear causal discovery without confounders has been extensively studied <ref type="bibr" target="#b41">(Spirtes et al., 2000;</ref><ref type="bibr" target="#b7">Chickering, 2002;</ref><ref type="bibr" target="#b43">Tsamardinos et al., 2006;</ref><ref type="bibr" target="#b40">Shimizu et al., 2006;</ref><ref type="bibr" target="#b10">de Campos, 2006;</ref><ref type="bibr" target="#b17">Jaakkola et al., 2010;</ref><ref type="bibr" target="#b9">de Campos and Ji, 2011;</ref><ref type="bibr" target="#b14">Gu et al., 2019;</ref><ref type="bibr" target="#b51">Zheng et al., 2018;</ref><ref type="bibr" target="#b49">Yuan et al., 2019;</ref><ref type="bibr" target="#b23">Li et al., 2020)</ref>. However, in practice, many causal relations are nonlinear, raising concerns about using a linear model <ref type="bibr" target="#b46">(Voorman et al., 2014)</ref>. For nonlinear causal models without confounders, three major approaches include (1) nonlinear independent component analysis <ref type="bibr" target="#b26">(Monti et al., 2020;</ref><ref type="bibr" target="#b50">Zhang and Hyvärinen, 2009)</ref>, (2) combinatorial search for the causal order <ref type="bibr" target="#b27">(Mooij et al., 2009;</ref><ref type="bibr" target="#b4">Bühlmann et al., 2014)</ref>, and (3) continuous constrained optimization for causal structure learning <ref type="bibr" target="#b52">(Zheng et al., 2020)</ref>. The first estimates the functional relations through the mutual independence of errors. The second determines the causal order based on a certain criterion. For example, the causal additive model (CAM) <ref type="bibr" target="#b4">(Bühlmann et al., 2014)</ref> assumes the nonlinear functions are of additive form and estimates the causal order that maximizes the likelihood. The third approach directly optimizes an objective function subject to a smooth constraint characterizing acyclicity. The most representative example is NOTEARS <ref type="bibr" target="#b52">(Zheng et al., 2020)</ref>. The reader may consult <ref type="bibr" target="#b32">Peters et al. (2017)</ref> and <ref type="bibr" target="#b13">Glymour et al. (2019)</ref> for excellent surveys of nonlinear causal discovery.</p><p>In the presence of latent confounders, several methods are available for linear causal discovery. As extensions of the PC algorithm, FCI <ref type="bibr" target="#b41">(Spirtes et al., 2000)</ref> and its variant RFCI <ref type="bibr" target="#b8">(Colombo et al., 2012)</ref> address latent confounders by producing a partial ancestral graph (PAG) instead of a completed partially DAG (CPDAG). Another approach <ref type="bibr" target="#b12">(Frot et al., 2019;</ref><ref type="bibr" target="#b37">Shah et al., 2020)</ref> assumes the confounding is pervasive <ref type="bibr" target="#b5">(Chandrasekaran et al., 2012;</ref><ref type="bibr" target="#b47">Wang and Blei, 2019)</ref> and recovers the CPDAG in two steps. For example, LRpS-GES <ref type="bibr" target="#b12">(Frot et al., 2019)</ref> uses the low-rank plus sparse estimator <ref type="bibr" target="#b5">(Chandrasekaran et al., 2012)</ref> to remove confounding, followed by the GES algorithm <ref type="bibr" target="#b7">(Chickering, 2002)</ref> to perform causal structure estimation. Besides, the instrumental variable estimation is a well-known approach but requires the availability of valid instruments <ref type="bibr" target="#b6">(Chen et al., 2018;</ref><ref type="bibr" target="#b24">Li et al., 2021)</ref>.</p><p>Despite the foregoing progress, nonlinear causal discovery with confounders remains largely unexplored. In a bivariate case, the work of <ref type="bibr" target="#b18">Janzing et al. (2009)</ref> estimates the confounding effect by minimizing the L 2 -distance between data points and a curve evaluated at the estimated values of the confounder. For a multivariate case, it remains unclear whether nonlinearity can help causal discovery with confounding, although third-order differentiability suffices for the identifiability of nonlinear causal discovery without confounders <ref type="bibr">(Peters et al., 2014)</ref>. Moreover, major computational and theoretical challenges arise when we confront the curse of dimensionality in learning a nonparametric DAG. During the review process, a preprint by <ref type="bibr" target="#b0">Agrawal et al. (2021)</ref> proposes a two-step procedure for nonlinear causal discovery in the presence of pervasive confounders. However, for consistent estimation, their method requires that the sample size grows slower than the quadratic graph size, n ≪ p 2 , which may be restrictive, especially for nonparametric estimation. This paper contributes to the following areas. First, we derive a new condition, called the sublinear growth assumption, for model identifiability in the presence of latent confounders.</p><p>Second, we propose a novel approach for causal discovery, called the Deconfounded Functional Structure Estimation (DeFuSE), comprising a deconfounding adjustment and an iterative procedure to reconstruct the topological order of the variables. Third, we implement DeFuSE through feedforward neural networks without assuming additive functional relationships while allowing efficient computation for a reasonable graph size p, say p = 100. This is in contrast to traditional nonparametric methods that suffer from inefficiency in high dimensions, such as tensor-product B-splines <ref type="bibr" target="#b15">(Hastie et al., 2009)</ref>. Fourth, we develop a novel theory for DeFuSE, establishing its consistency for discovering the underlying DAG structure. DeFuSE requires an assumption for consistent causal discovery, called the strong causal minimality, which is an analogy of the strong faithfulness <ref type="bibr" target="#b44">(Uhler et al., 2013)</ref> and the beta-min condition <ref type="bibr" target="#b25">(Meinshausen and Bühlmann, 2006)</ref>. A central message of this paper is that nonlinearity plays an important role in causal discovery, permitting the separation of the nonlinear causal effects from linear confounding effects.</p><p>The rest of the article is structured as follows. Section 2 introduces the DAG model with hidden confounders and the proposed method DeFuSE. Section 3 implements DeFuSE based on feedforward neural networks for scalable computation. Section 4 provides a theoretical guarantee of DeFuSE for consistent discovery. Section 5 presents some numerical examples and compares DeFuSE with CAM, NOTEARS, RFCI, and LRpS-GES, followed by a discussion in Section 6. The Appendix contains additional theoretical results and implementation details, and the Supplementary Materials contain the technical proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Directed acyclic graph with confounders</head><p>Consider a random vector Y = (Y 1 , . . . , Y p ) generated from a nonlinear structural equation model with additive confounders and noises,</p><formula xml:id="formula_0">Y j = f j Y pa(j) + η j + e j , j ∈ V = {1, . . . , p},<label>(1)</label></formula><p>where f j maps the subvector Y pa(j) = (Y k ) k∈pa(j) to a real number, pa(j) ⊆ V \{j} is an index subset, η = (η 1 , . . . , η p ) ∼ N p (0, Σ η ) is a vector of hidden confounders and is independent of random errors e = (e 1 , . . . , e p ) ∼ N p (0, Diag(σ 2 1 , . . . , σ 2 p )), Σ η is an unknown covariance matrix, and Diag(σ 2 1 , . . . , σ 2 p ) is an unknown diagonal matrix. Then (1) is associated with a directed graph G = (V, E) such that E = {k → j : k ∈ pa(j), j ∈ V }. In this situation, pa(j) denotes the set of parents of j. Throughout this article, we assume that G is a directed acyclic graph (DAG) in that no directed path j → • • • → j exists in G. As a result, (1) generalizes the nonlinear DAG without unmeasured confounders <ref type="bibr" target="#b16">(Hoyer et al., 2008;</ref><ref type="bibr">Peters et al., 2014)</ref> and the linear DAG <ref type="bibr" target="#b31">(Peters and Bühlmann, 2014)</ref>.</p><p>In (1), we assume the causal minimality to ensure that the effect of each parent is nonvanishing. In other words, we require pa(j) = arg(f j ); j = 1, . . . , p, where arg(f j ) denotes the minimal argument set B ⊆ pa(j) such that the value of f j only depends on</p><formula xml:id="formula_1">Y B = (Y k ) k∈B .</formula><p>In particular, if f j is a constant function, we have pa(j) = arg(f j ) = ∅. When η ≡ 0 (no confounder), this definition agrees with the usual causal minimality condition <ref type="bibr" target="#b30">(Pearl, 2009)</ref>, requiring that the probability distribution of Y is not Markov to any proper subgraph of G.</p><p>The causal minimality, as a form of causal faithfulness <ref type="bibr" target="#b41">(Spirtes et al., 2000)</ref>, ensures that the problem of nonlinear causal discovery is well-defined.</p><p>Equivalently, we rewrite (1) by letting ε j = η j + e j ,</p><formula xml:id="formula_2">Y j = f j Y pa(j) + ε j , j ∈ V = {1, . . . , p},<label>(2)</label></formula><p>where ε = (ε 1 , . . . , ε p ) ∼ N (0, Σ) and Σ = Σ η + Diag(σ 2 1 , . . . , σ 2 p ). Whereas (1) has a clear causal interpretation, (2) is simpler for the subsequent discussion. Our goal is to discover the causal relations between variables Y 1 , . . . , Y p by identifying {f j } 1≤j≤p and {pa(j)} 1≤j≤p .</p><p>One major challenge is that the error ε j may be correlated with Y pa(j) due to unmeasured confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model identifiability</head><p>This subsection establishes the identifiability conditions for (2). First, we introduce the concept of topological depth for a DAG G = (V, E) with nodes V = {1, . . . , p} and directed edges E ⊆ V × V . A node j is a root if it has no parent, i.e., pa(j) = ∅. If there exists a directed path k → • • • → j, then node k is an ancestor of j and j is a descendant of k.</p><p>The topological depth d j of node j ∈ V is the maximal length of a directed path from a root to j. Clearly, a root node has depth zero, and we have 0 ≤ d j ≤ d max ≤ p -1 for j ∈ V , where d max is the length of the longest directed path in G. Let V (d) = {j : d j &lt; d} be the set of nodes with topological depth less than d, where 1</p><formula xml:id="formula_3">≤ d ≤ d max + 1. Then ∅ ≡ V (0) ⊆ V (1) ⊆ • • • ⊆ V (d max + 1) = V and V (d j</formula><p>) contains all the ancestors (and hence all the parents) of Y j but contains no descendant of Y j . See Figure <ref type="figure" target="#fig_0">1</ref> for an illustration.</p><p>Next, we present a new condition for {f j } 1≤j≤p and {pa(j)} 1≤j≤p in (2) to be identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For continuous function</head><formula xml:id="formula_4">f : R m → R, f is of sublinear growth if lim ∥x∥→∞ f (x)/∥x∥ = 0,</formula><p>where ∥ • ∥ is the Euclidean norm.</p><p>Condition 1. Assume that {f j } 1≤j≤p are of sublinear growth.</p><p>For example, Condition 1 is satisfied if {f j } 1≤j≤p are continuous and bounded. This sublinear growth assumption imposes restrictions on the nonlinearity of {f j } 1≤j≤p , in contrast Here V (1) = {1, 3}, V (2) = {1, 2, 3}, and</p><formula xml:id="formula_5">V (3) = V = {1, 2, 3, 4}.</formula><p>to the third-order differentiability condition for DAGs without confounders <ref type="bibr" target="#b16">(Hoyer et al., 2008;</ref><ref type="bibr">Peters et al., 2014)</ref>.</p><formula xml:id="formula_6">Theorem 1 (Identifiability). Assume Condition 1 is satisfied. (A) The sets V (1) ⊆ • • • ⊆ V (d max )</formula><p>are uniquely identifiable for almost every positive definite Σ with respect to the Lebesgue measure, where the set of such Σ is denoted as Ψ.</p><formula xml:id="formula_7">Moreover, for Σ ∈ Ψ, if d j = d, then Y j -E Y j | Y V (d) is normally distributed with mean zero and constant variance Var Y j | Y V (d) ; if d j &gt; d, then Y j -E Y j | Y V (d) is not normally distributed; j = 1, . . . , p. (B) Given V (1) ⊆ • • • ⊆ V (d max )</formula><p>, we have {f j } 1≤j≤p and {pa(j)} 1≤j≤p are well-defined and identifiable from the distribution of Y .</p><p>By Theorem 1, model (2) is generically identifiable under Condition 1. Different from <ref type="bibr" target="#b12">Frot et al. (2019)</ref>, Theorem 1 does not require pervasive confounding. The sublinear growth assumption (Condition 1) allows us to separate the linear confounding effect from nonlinear causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DeFuSE</head><p>This subsection proposes the causal discovery method Deconfounded Functional Structure Estimation (DeFuSE). We commence with least squares regressions of</p><formula xml:id="formula_8">{Y j } j / ∈V (d) on Y V (d) , Y j = E(Y j | Y V (d) ) (i) + Y j -E(Y j | Y V (d) )<label>(ii)</label></formula><p>, where (i) is the regression function and (ii) is the residual of the regression.  <ref type="bibr" target="#b2">(Anderson and Darling, 1952)</ref>) for</p><formula xml:id="formula_9">{Y j -E(Y j | Y V (d) )} j / ∈V (d) can be utilized to identify V (d + 1). Further, if d j = d, then (i) becomes E(Y j | Y V (d) ) = f j (Y pa(j) ) + E(ε j | Y V (d) ), where E(ε j | Y V (d)</formula><p>) is the bias arising from hidden confounding. Theorem 2 allows us to estimate {f j } j∈V (d+1) and {pa(j)} j∈V (d+1) by regressions with deconfounding adjustment.</p><formula xml:id="formula_10">Theorem 2. In (2), if d j = d, then E(Y j | Y V (d) ) = f j (Y pa(j) ) + ξ V (d) , β j , (<label>3</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">where ξ V (d) ≡ (Y k -E(Y k | Y V (d k )</formula><p>)) k∈V (d) , β j is a parameter vector, ⟨•, •⟩ is the Euclidean inner product, and we define ξ V (d) , β j ≡ 0 whenever V (d) = ∅. Now, we develop an algorithm that iteratively estimates V (d + 1), ξ V (d+1) , {f j } j∈V (d+1) , and {pa(j)} j∈V (d+1) , given V (d) and ξ V (d) as input. To proceed, suppose an independent sample {(Y</p><formula xml:id="formula_13">(i) 1 , . . . , Y (i) p )} 1≤i≤n from model (2) is given. Let ξ (i) V (d) = (Y (i) k -Y (i)</formula><p>k ) k∈V (d) be the estimated residual vector for the i-th observation, where</p><formula xml:id="formula_14">Y (i) k = f k Y (i) V (d k ) + ξ (i) V (d k ) , β j for k ∈ V (d). Based on (3), we regress each variable in {Y j } j / ∈V (d) on Y V (d) , ξ V (d) , ( f j , β j ) = arg min {(f j ,β j ):f j ∈F j } n i=1 Y (i) j -f j Y (i) V (d) -ξ (i) V (d) , β j 2 s.t. |arg(f j )| ≤ κ j ,<label>(4)</label></formula><p>where |arg(f j )| is the effective input dimension of f j , κ j ≥ 0 is an integer-valued hyperparameter and is estimated via a standalone validation set (see Section A.3), and F j is a function space consisting of sublinear growth continuous functions. Then we perform normality tests for {( ξ</p><formula xml:id="formula_15">(1) j , . . . , ξ (n) j )} j / ∈V (d)</formula><p>, and estimate V (d + 1) by including V (d) and all the indices failing to reject the tests. Finally, we estimate { pa(j)} j∈V (d+1) by pa(j) = arg( f j ).</p><p>We summarize the procedure in Algorithm 1, where a bold-face letter denotes a data vector/matrix of sample size n.</p><formula xml:id="formula_16">Algorithm 1: DeFuSE Input: An n × p data matrix Y = (Y 1 , . . . , Y p );</formula><p>Parameters: significance level α for normality test; hyperparameters {κ j } 1≤j≤p ;</p><p>1 Let V (0) ← ∅ and d ← 0;</p><formula xml:id="formula_17">2 while V (d) ̸ = V do 3 Regress {Y j } j / ∈V (d) on (Y V (d) , ξ V (d) ) based on (4); 4 Update { ξ j ← Y j -Y j } j / ∈V (d) ; 5 Let V (d + 1) ← V (d) ∪ {j / ∈ V (d) : ξ j fails to reject the normality test}; 6 Let { pa(j) ← arg( f j )} j∈V (d+1) and d ← d + 1; 7 end Output: { f j } 1≤j≤p and { pa(j)} 1≤j≤p ;</formula><p>Remark 1 (Normality test and the choice of α). For implementation, we use the Anderson-Darling test <ref type="bibr" target="#b2">(Anderson and Darling, 1952)</ref> to examine the null hypotheses</p><formula xml:id="formula_18">H (j,d) 0 : Y j -E(Y j | Y V (d) ) is normal; j / ∈ V (d), 0 ≤ d ≤ d max .</formula><p>Other tests or metrics, such as the Wasserstein distance, can also be used. Moreover, the normality test can be combined with a goodness-of-fit measure to further improve performance.</p><p>The significance level 0 &lt; α &lt; 1 is a hyperparameter similar to that in the PC algorithm <ref type="bibr" target="#b19">(Kalisch and Bühlman, 2007)</ref>. To choose α, denoting by T the set of true null hypotheses, then P some H Finally, Example 1 illustrates the importance of deconfounding for causal discovery.</p><formula xml:id="formula_19">(j,d) 0 ∈ T is rejected ≤ H (j,d) 0 ∈T P H (j,d) 0 is rejected ≈ |T |α. For 1 ≤ d ≤ d max +1, identifying V (d)</formula><p>Example 1. Consider a special case of (1) with three variables,</p><formula xml:id="formula_20">Y 1 = e 1 + η, Y 2 = e 2 + η, Y 3 = cos(Y 1 ) + e 3 + η,<label>(5)</label></formula><p>where e 1 , e 2 , e 3 , η ∼ N (0, 1) independently; see Figure <ref type="figure" target="#fig_3">2</ref>. As a special case of (3), we have  </p><formula xml:id="formula_21">E(Y 3 | Y 1 , Y 2 ) = cos(Y 1 ) + E(η | Y 1 , Y 2 ) = cos(Y 1 ) + Y 1 /3 + Y 2 /3, where d 3 = 1, V (1) = {1, 2}, ξ V (1) = (ξ 1 , ξ 2 ) = (e 1 + η, e 2 + η), and ξ V (2) = ξ 3 = e 3 + (η -e 1 -e 2 )/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeFuSE via neural networks</head><p>Solving ( <ref type="formula" target="#formula_14">4</ref>) is challenging for a large-scale problem due to fitting nonparametric functions.</p><p>Existing nonparametric methods such as tensor-product splines and kernels are not scalable in a growing sample size and dimension. For example, tensor-product B-splines least squares regression suffers from exponential growth of time and space complexity with increasing dimensions. To overcome this difficulty, we solve (4) via a feedforward neural network (FNN) together with stochastic gradient descent for scalable computation.</p><p>Specifically, for</p><formula xml:id="formula_22">d j ≥ d, we approximate f j Y V (d) + ξ V (d) , β j by an FNN, g j Y V (d) , ξ V (d) = f L j •• • ••f 1 j Y V (d) + ξ V (d) , β j , f l j (•) = σ l W l (•) + b l ; l = 1, . . . , L,<label>(6)</label></formula><p>where W l ∈ R h l ×h l-1 is the weight matrix of links from the (l-1)-th to the l-th layer, b l ∈ R h l is the bias vector in the l-th layer, h l is the number of neurons in the l-th layer with h l = h; l = 1, . . . , L -1, and h L = 1, L is the number of layers, and σ l (•) is an activation function.</p><p>For l = 1, . . . , L -1, we use the Rectifier Linear Unit (ReLU) activation σ l (z) = max(0, z).</p><p>To solve (4), consider a FNN parameter vector</p><formula xml:id="formula_23">θ j = ((W l j , b l j ) 1≤l≤L , β j ) which belongs to a parameter space Θ d . We impose constraints k∈V (d) min(∥W 1 k ∥/τ, 1) ≤ κ j on the k-th column W 1 k of the weight matrix W 1 at the first layer to enforce the constraint |arg(f j )| ≤ κ j in (4), where min(| • |/τ, 1) is to approximate I(• ̸ = 0) as τ → 0 + (Shen et al., 2012). As such, if W 1 k = 0 then g j Y V (d) , ξ V (d)</formula><p>does not depend on Y k . Finally, we regularize the FNN by an L 2 -norm constraint ∥θ j ∥ ≤ s on the model parameters θ j for numerical consideration. This leads to the following regression for estimating (f j , β j ), min </p><formula xml:id="formula_24">{θ j :∥θ j ∥≤s} n i=1 Y (i) j -f j Y (i) V (d) -ξ (i) V (d) , β j 2 , s.t. k∈V (d) min(∥W 1 k ∥/τ, 1) ≤ κ j , k∈V (d) min(|β j,k |/τ, 1) ≤ ς j ,<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning theory</head><p>This section develops a novel theory to quantify the finite-sample error of DeFuSE. In what follows, c 1 -c 6 are positive constants and • decorates the truth. Let G j be the function space of regression functions g j (•, ⋆) = f j (•) + ⋆, β • j , and denote the true regression function by g</p><formula xml:id="formula_25">• j (•, ⋆) = f • j (•) + ⋆, β • j . By definition, pa • (j) = arg(f • j ).</formula><p>Condition 2. There exists an approximating function g</p><formula xml:id="formula_26">* j (•, ⋆) = f * j (•) + ⋆, β • j ∈ G j such that ∥g * j -g • j ∥ L 2 = ∥f * j -f • j ∥ L 2 ≤ c 3 ϵ n ; j = 1, . . . , p, where ∥ • ∥ L 2 is the L 2 -norm with respect to measure P . Moreover, assume {f • j } 1≤j≤p are continuous and ∥f • j ∥ ∞ ≤ c 1 , where ∥ • ∥ ∞ is the sup-norm.</formula><p>To measure the signal strength, we define the degree of nonlinear separation as</p><formula xml:id="formula_27">D min = min 1≤j≤p inf    ∥g j -g • j ∥ 2 L 2 |pa • (j) \ arg(f j )| : g j ∈ G j , arg(f j ) ̸ = pa • (j),</formula><formula xml:id="formula_28">∥β j ∥ 0 ≤ ς • , |arg(f j )| ≤ |pa • (j)|    . Condition 3 (Strong causal minimality). Assume D min ≥ c 4 max 4ϵ 2 n , n -1 log n, n -1 log p , where c 4 ≥ 1.</formula><p>The strong causal minimality (Condition 3) requires that the signal strengths of parent variables are sufficiently strong so that the corresponding causal function is distinguishable from those supported on non-parent variables. It is a strong version of the causal minimality for nonlinear causal discovery from a finite sample, similar to the strong faithfulness <ref type="bibr" target="#b44">(Uhler et al., 2013)</ref> for linear causal discovery and the beta-min condition <ref type="bibr" target="#b25">(Meinshausen and Bühlmann, 2006)</ref> for high-dimensional variable selection.</p><p>Theorem 3 (Error bounds for DeFuSE). Assume Conditions 1-3, Conditions 5-6 in Section A.2 are met and Σ ∈ Ψ.</p><p>(A) The DAG recovery error is</p><formula xml:id="formula_29">P ( G ̸ = G • ) ≤ c 6 exp(-c 5 nϵ 2 n -log n) + π α (G • ), when the hyperparameters κ j = |pa • (j)| and ∥β • j ∥ 0 ≤ ς j ≤ ς • ; 1 ≤ j ≤ p, where π α (G • ) is the normality test error given the true model. Consequently, P ( G ̸ = G • ) → 0 provided that π α (G • ) → 0, as n → ∞. (B) The regression estimation error is max 1≤j≤p ∥ g j -g • j ∥ L 2 = O p (ϵ n ). Suppose f • j satisfies ∥f • j ∥ ∞ ≤ C and has bounded support; 1 ≤ j ≤ p. Then the causal function estimation error is max 1≤j≤p ∥ f j -f • j ∥ L 2 = O p (ϵ n ) provided that ∥ f j ∥ ∞ ≤ C ′ for C ′ ≥ C.</formula><p>Typically, we have π α (G • ) → 0 when α = o(1/p) and the dimension p does not grow too fast. Moreover, Theorem 3 indicates that hyperparameter κ j is critical to consistent discovery, while ς j is less important provided that ς j ≥ ∥β • j ∥ 0 and is not too large; see also Section A.3.</p><p>Next, we apply Theorem 3 to the implementation via FNNs in (7). Before proceeding, we define C r j , the space of functions with r-continuous derivatives over the domain R |pa • (j)| . For any function f j ∈ C r j , the C r j -norm of f j is defined as</p><formula xml:id="formula_30">∥f j ∥ C r j = α:|α|&lt;r ∥∂ α f j ∥ ∞ + α:|α|=⌊r⌋ sup x 1 ̸ =x 2 |∂ α f (x 1 ) -∂ α f (x 2 )| ∥x 1 -x 2 ∥ r-⌊r⌋ ∞ ,</formula><p>where</p><formula xml:id="formula_31">∂ α = ∂ α 1 • • • ∂ α |pa(j)| with α ∈ N |pa(j)| and |α| = |pa(j)| k=1 α k ; j = 1, . . . , p.</formula><p>In what follows, C 1 -C 3 are positive constants that may depend on (κ • , r).</p><formula xml:id="formula_32">Condition 4. Assume f • j ∈ f j ∈ C r j : ∥f j ∥ C r j ≤ C 1 , where r does not depend on (p, n).</formula><p>Theorem 4 (Consistency of FNN-DeFuSE). Under Conditions 3-4, and 6 in Section A.2, DeFuSE implemented by FNNs in (7) consistently recovers all causal relations defined in</p><formula xml:id="formula_33">(2) with ϵ 2 n = C 3 (n -r/(r+κ • +ς • ) (log n) 3 + n -1 (κ • + ς • ) log p) in Theorem 3, provided that the width of the FNN h = C 2 ϵ -κ • /r n and its depth L = C 2 log(1/ϵ n ), the hyperparameters s = C 2 ϵ -(κ • +ς • )/r n log(1/ϵ n ), κ j = |pa • (j)|, ∥β • j ∥ 0 ≤ ς j ≤ ς • ; j = 1, . . . , p.</formula><p>Here, the FNN function space G j = {g j = g j (•; θ) : θ ∈ Θ j } is associated with the FNN parameter space</p><formula xml:id="formula_34">Θ j = θ = ((W l , b l ) 1≤l≤L , β j ) : max 1≤l≤L h l ≤ h, ∥θ∥ ≤ s ; j = 1, . . . , p. It is worth noting that the rate ϵ 2 n ≍ n -r/(r+κ • +ς • ) (log n) 3 + n -1 (κ • + ς •</formula><p>) log p for FNN relies on the approximation result of Schmidt-Hieber (2019) as well as the choice of L, h, and s. This rate agrees with <ref type="bibr" target="#b11">Farrell et al. (2021)</ref> up to logarithm terms; however, it is slower than n -r/(r+(κ • +ς • )/2) in view of <ref type="bibr" target="#b42">Stone (1982)</ref> for nonparametric regression over [0, 1] κ • +ς • , suggesting that it may be suboptimal. This may be due to the approximation, namely the use of non-differentiable ReLU FNNs to approximate smooth functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simulations</head><p>This subsection examines the operating characteristics of DeFuSE and compares DeFuSE with CAM <ref type="bibr" target="#b4">(Bühlmann et al., 2014)</ref>, NOTEARS (FNN version) <ref type="bibr" target="#b52">(Zheng et al., 2020)</ref>, LRpS-GES <ref type="bibr" target="#b12">(Frot et al., 2019)</ref>, and RFCI <ref type="bibr" target="#b8">(Colombo et al., 2012)</ref>. We implement DeFuSE in Python. For competitors, we use R packages for CAM (CAM), RFCI (pcalg), and LRpS-GES (lrpsadmm and pcalg), and use a Python program for NOTEARS (notears).</p><p>In simulations, we consider two types of DAGs with hidden confounders. Define an adjacency matrix U = (U jk ) p×p of a DAG as U jk = 1 if j ∈ pa(k) and 0 otherwise.</p><p>Random DAG. Consider a sparse graph where the edges are added independently with equal probability. In particular, an adjacency matrix U ∈ {0, 1} p×p is randomly generated: P (U jk = 1) = s if j &lt; k and P (U jk = 1) = 0 otherwise, where s controls the degree of sparseness of the DAG. In our simulation, we choose s = 1/p. Hub DAG. Consider a sparse graph with a hub node. Let U ∈ {0, 1} p×p , where U 1k = 1 and U jk = 0 otherwise. In this case, node 1 has a dense neighborhood, but the whole DAG remains sparse.</p><p>Simulated data. Given U , we generate a random sample of size n from</p><formula xml:id="formula_35">Y j = α 0 Y k 1 Y k 2 + k∈pa(j) α j,k f j,k (Y k + ω j,k ) + ε j ; j = 1, . . . , p,<label>(8)</label></formula><p>where the function f j,k is randomly sampled from {x → x 2 , x → cos(x)}, the coefficients</p><formula xml:id="formula_36">α j,k ∼ Uniform([-3, -2] ∪ [2, 3]), ω j,k ∼ Uniform([-1, 1]),<label>and</label></formula><formula xml:id="formula_37">     α 0 = 0, |pa(j)| = 1, α 0 = 1, k 1 , k 2 are randomly sampled from pa(j), |pa(j)| &gt; 1.</formula><p>For error terms, let ε ∼ N (0, Σ) with Σ jj = 2 for 1 ≤ j ≤ p, Σ 2k-1,2k = Σ 2k,2k-1 = 1 for 1 ≤ k ≤ ⌊p/2⌋, and Σ jj ′ = 0 otherwise. Of note, (8) violates Condition 1 as the functions (y 1 , y 2 ) → α 0 y 1 y 2 and f j,k may not be of sublinear growth. As suggested in Table <ref type="table" target="#tab_1">1</ref>, DeFuSE performs the best across all the situations in terms of FPR, FDR, TPR, and SHD. As expected, CAM and NOTEARS cannot treat unobserved confounders, whereas RFCI and LRpS-GES cannot deal with nonlinear causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>It is worth noting that DeFuSE* takes standardized data as input and achieves comparable performance to DeFuSE, indicating that DeFuSE is insensitive to the degree of varsortability <ref type="bibr" target="#b34">(Reisach et al., 2021)</ref>. Moreover, DeFuSE seems robust in the absence of Condition 1; see also Theorem 5 in Appendix and discussions there. Overall, nonlinearity helps identify causal relations, allowing for a separation of nonlinear causal effects from linear confounding effects.</p><p>Sensitivity to normality test significance level α. In the above experiments, we use the Anderson-Darling test <ref type="bibr" target="#b2">(Anderson and Darling, 1952)</ref> with α = 0.025 as the default choice. Now, we assess the algorithmic sensitivity to different choices of α ∈ {0.1, 0.05, 0.025, 0.01}.</p><p>As suggested in Table <ref type="table" target="#tab_2">2</ref>, the overall performance of DeFuSE seems insensitive to the choice of α, although the default choice α = 0.025 may be sub-optimal. Based on our limited numerical experience, we suggest α = o(1/p) as an empirical rule to reduce the tuning cost of α; see also Remark 1.  <ref type="bibr" target="#b21">Shen, 2017;</ref><ref type="bibr" target="#b29">Palmqvist et al., 2020)</ref>, so we focus on the metabolic pathways of these proteins.</p><p>Specifically, we extract the reference pathways in <ref type="url" target="https://genome.jp/pathway/map05010">https://genome.jp/pathway/map05010</ref> from the KEGG database <ref type="bibr" target="#b20">(Kanehisa and Goto, 2000)</ref>, including p = 20 genes in the data.</p><p>For data analysis, we first regress the gene expression levels on five covariates, Gender, Handedness, Education level, Age, and Intracranial volume, then use the residuals as gene expressions in the subsequent analysis. We normalize all gene expression levels and use the same FNN structure for fitting as in the simulation study. The normality test is conducted at a significance level α = 0.05.</p><p>As displayed in Figure <ref type="figure" target="#fig_6">3</ref>, the reconstructed DAGs exhibit some common and distinctive characteristics for the AD-MCI and CN groups. In the AD-MCI group, (1) directed edges GRIN1 → MAPT and PSEN1 → GSK3B agree with the reference pathways of the tau protein;</p><p>(2) genes {APH1A, PSENEN, NCSTN, PPP3R1, APBB1, APP} have more directed connections, corresponding to the amyloid precursor protein. So do genes {PSEN1, GSK3B}</p><p>for the presenilin proteins. By comparison, the genes participating in the amyloid precursor protein and tau protein metabolism have fewer connections in the CN group (O'brien and <ref type="bibr" target="#b28">Wong, 2011;</ref><ref type="bibr" target="#b29">Palmqvist et al., 2020)</ref>. This observation seems consistent with previous studies that both genes may be involved in AD. Moreover, there are six and two non-root genes, respectively for the AD-MCI and CN groups.</p><p>For model diagnostics, we check the nonlinearity assumption on the gene expression levels. To this end, we compare a linear and a quadratic regression model for each non-root gene in the AD-MCI and CN groups in terms of their AIC values <ref type="bibr" target="#b1">(Akaike, 1992)</ref>. These A P B B 1 a non-root variable Y j , the AIC value is defined as</p><formula xml:id="formula_38">A P H 1 A PS EN EN NCSTN G RI N1 C A C N A 1 C P P P 3 R 1 M AP T GSK3B PS EN 1 P S E N 2 A T P 2 A 2 RY R3 CALM2 CO X7 C A T P 5 C 1 N D U F S 4 SD HA (a) AD-MCI APP NA E1 A P B B 1 A P H 1 A PS EN EN NCSTN G RI N1 C A C N A 1 C P P P 3 R 1 M AP T GSK3B PS EN 1 P S E N 2 A T P 2 A 2 RY R3 CALM2 CO X7 C A T P 5 C 1 N D U F S 4 SD HA (b) CN</formula><formula xml:id="formula_39">AIC( m) = (n σ 2 FNN ) -1 n i=1 (Y (i) j -Y (i) j ) 2 + 2n -1 dim( m),<label>(9)</label></formula><p>where m and σ 2 FNN are the fitted model and the error variance estimated by FNN, Y  <ref type="table" target="#tab_3">3</ref>, the quadratic model generally fits better than the corresponding linear model, as measured by AIC, suggesting that the nonlinearity assumption is approximately satisfied. Finally, the correlation plots of <ref type="figure" target="#fig_7">4</ref> exhibit the presence of (linear) hidden confounding as evident from the fact that many genes have multiple connections to other genes, indicating nonzero off-diagonals of Σ. This observation seems plausible due to the absence of some genes in the analysis. </p><formula xml:id="formula_40">Y (i) j -f j (Y (i) pa(j) ) j∈V ; i = 1, . . . , n in Figure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This article proposes a novel method for learning functional causal relations with additive confounders. For modeling, we establish identifiability under a sublinear growth condition on the functional relationships. On this basis, we propose a novel method called DeFuSE and implement it with feedforward neural networks for scalability. Theoretically, we show that the proposed method consistently reconstructs all nonlinear causal relations.</p><p>One central message is that nonlinearity permits the separation of the nonlinear causal relationships from the confounding effects in model ( <ref type="formula" target="#formula_0">1</ref>) with observational data only. As nonlinear causal discovery with hidden confounding remains understudied, we hope the work could inspire further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Additional results on identifiability</p><p>If Σ ∈ Ψ, the sublinear growth condition (Condition 1) is sufficient for identifying both {f j } 1≤j≤p and {pa(j)} 1≤j≤p in (1). When this condition is not satisfied, it is still possible to establish identifiability under an alternative assumption. Now, we consider model (2) with additive functions,</p><formula xml:id="formula_41">Y j = k∈pa(j) f j,k (Y k ) + ε j , j ∈ V = {1, . . . , p},<label>(10)</label></formula><p>where {f j,k } are nonlinear and ε ∼ N (0, Σ). Theorem 5 establishes the identifiability of {pa(j)} 1≤j≤p in (10), without the sublinear growth condition.</p><p>Theorem 5. In (10), assume that</p><formula xml:id="formula_42">Y j -E Y j | Y V (d) is not normally distributed for d j &gt; d; 0 ≤ d ≤ d max .</formula><p>For any univariate function f , we define its equivalence class</p><formula xml:id="formula_43">[f ] = { f : f (z) = f (z) + γz, γ ∈ R}. If [f j,k ] ̸ = j ′ ∈V (d j ) γ j ′ [f j ′ ,k ] for all γ j ′ ∈ R; j ′ ∈ V (d j ), j ∈ V = {1, . . . , p},</formula><p>then {pa(j)} 1≤j≤p are uniquely identifiable.</p><p>The assumption that </p><formula xml:id="formula_44">Y j -E Y j | Y V (d) is not normal for d j &gt; d imposes</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Regularity conditions</head><p>We impose the following regularity conditions to establish the consistency of DeFuSE.</p><p>Metric entropy. We define the bracketing L 2 -metric entropy as a complexity measure of function spaces G j = {g j : g j (•, ⋆) = f j (•) + ⟨⋆, β j ⟩}; j = 1, . . . , p, where • and ⋆ represent a |V (d j )|-dimensional vector, respectively. The bracketing L 2 -metric entropy of G j is the logarithm of the smallest u-bracket cardinality, H(u, G j ) = log(min{m : S(u, m)}), where</p><formula xml:id="formula_45">a u-bracket S(u, m) = {g - 1 , g + 1 , . . . , g - m , g + m } ⊆ L 2 (P ) is a set of functions such that (i) max 1≤k≤m ∥g - k -g + k ∥ L 2 ≤ u and (ii) for any g ∈ G j there exists g - k ≤ g ≤ g + k almost surely.</formula><p>Condition 5. For some positive</p><formula xml:id="formula_46">ϵ n &lt; 1/2, max 1≤j≤p max {A:|A|≤|pa • (j)|} √ 2ϵn ϵ 2 n /256 H 1/2 (u/c 1 , G j (A))du ≤ c 2 √ nϵ 2 n ,</formula><p>where</p><formula xml:id="formula_47">G j (A) = g j ∈ F j : A = arg(f j ), ∥g j -g • j ∥ 2 ≤ 2ϵ n is the 2ϵ n -neighborhood of g • j on the index set of effective arguments A.</formula><p>In view of Condition 5, the error rate ϵ n is determined by solving the integral equation in ϵ n . Such a condition has been used to quantify the convergence rate of sieve estimates <ref type="bibr" target="#b48">(Wong and Shen, 1995;</ref><ref type="bibr" target="#b45">van de Geer, 2000)</ref>. The entropy results are available for many function classes, such as the FNN in Theorem 4.</p><p>Sparsity and confounding. Next, we impose a regularity condition on sparsity and confounding structures, requiring the true support of g • j , the maximum depth d max , and the error variance not to increase with the sample and graph sizes (n, p). <ref type="bibr">(p, n)</ref>, where λ min (Σ) and λ max (Σ) are the smallest and largest eigenvalues of Σ ∈ Ψ.</p><formula xml:id="formula_48">Condition 6. Assume κ • = max 1≤j≤p |pa • (j)|, ς • = max 1≤j≤p ∥β • j ∥ 0 , d max = max 1≤j≤p d j , and c -≤ λ min (Σ) ≤ λ max (Σ) ≤ c + are independent of</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation details</head><p>The code is open-sourced at <ref type="url" target="https://github.com/chunlinli/defuse">https://github.com/chunlinli/defuse</ref>. To tune hyperparameters κ j , ς j in (7), we adopt a greedy strategy combined with an asynchronous-synchronous training technique since it is unnecessary to identify the exact value of ς j , c.f., Theorem 3. We first optimize (7) in β j with θ j = 0, subject to the sparsity constraint k∈V (d) min(|β j,k |/τ, 1) ≤ ς j , followed by selecting ς j ∈ {0, 1, . . . , |V (d)|} that minimizes the mean squared error on the validation set. Throughout, we fix τ = 0.05 as a signal-noise threshold. This stage intends to perform a sparsity-constrained linear regression, so it is very efficient in computing. Next, given the selected variable set B = {k : <ref type="formula" target="#formula_24">7</ref>), we estimate (θ j , β j,B ) with β j,B c = 0 by minimizing min</p><formula xml:id="formula_49">|β jk | ≥ τ } in (</formula><formula xml:id="formula_50">θ j n i=1 Y (i) j -f j Y (i) V (d) -ξ (i) V (d) , β j,B 2 , s.t. k∈V (d) min(∥W 1 k ∥/τ, 1) ≤ κ j .</formula><p>To leverage the automatic differentiation in modern deep learning libraries, we consider its regularized version with κ j replaced by a hyperparameter λ j &gt; 0: min</p><formula xml:id="formula_51">θ j n i=1 Y (i) j -f j Y (i) V (d) -ξ (i) V (d) , β j,B 2 + λ j k∈V (d) min(∥W 1 k ∥/τ, 1).</formula><p>where λ j &gt; 0 controls the degree of regularization. Then, after the regularized optimization is completed, we tune κ j ∈ {0, 1, . . . , |V (d)|} using the top κ j variables (sorted by weight ∥W 1 k ∥) among all variables and masking the rest. To speed up the computation, we also implement a nonparametric screening procedure <ref type="bibr" target="#b3">(Azadkia and Chatterjee, 2021)</ref> for variable selection.</p><p>In our experiments, we use an adaptive regularization approach for λ j &gt; 0 during training, similar to adaptive learning rate scheduling. Specifically, we consider three candidate values λ j ∈ {0.0001, 0.001, 0.05}. The training process starts with λ j = 0.0001 and gradually increases λ to achieve better validation performance by inducing more sparsity. Based on our limited experience, this adaptive regularization strategy is effective and can be combined with other deep learning techniques such as early stopping.</p><p>For network structure, we use an FNN with one hidden layer and 50 hidden neurons. For optimization, we use the Adam optimizer <ref type="bibr" target="#b22">(Kingma and Ba, 2014)</ref>  Supplementary Materials for "Nonlinear causal discovery with confounders"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical proofs</head><p>In what follows, c j 's and C denote generic constants.</p><p>Lemma 1. If W ∼ N (0, 1) and t &gt; 0, then</p><formula xml:id="formula_52">2 π t 1 + t 2 e -t 2 /2 ≤ P (|W | ≥ t) ≤ 2 π 1 t e -t 2 /2 . By Lemma 1, if W ∼ N (0, σ 2 W ), σ 2 W = inf c : lim t→∞ P (|W | &gt; t) exp(t 2 /2c) = 0 . Lemma 2. Assume X = (X 1 , . . . , X q ) ∼ N (0, Σ X ) and σ 2 γ = γ ⊤ Σ X γ. If Z = f (X) + ⟨X, γ⟩, then under Condition 1, σ 2 γ = inf c : lim t→∞ P (|Z| &gt; t) exp(t 2 /2c) = 0 .</formula><p>Proof. Note that P (|Z| &gt; t) = P (|f (X) + γ ⊤ X| &gt; t) = P (|γ ⊤ X| &gt; t/|1 + f (X)/γ ⊤ X|).</p><p>On event {|γ ⊤ X| &gt; t/|1 + f (X)/γ ⊤ X|}, when t → ∞, we have |γ ⊤ X| → ∞. Hence, by Condition 1, for any small ε &gt; 0, when t is large enough,</p><formula xml:id="formula_53">P (|γ ⊤ X| &gt; t/(1 -ε)) ≤ P (|Z| &gt; t) ≤ P (|γ ⊤ X| &gt; t/(1 + ε)).</formula><p>Let W = γ ⊤ X/σ γ . By Lemma 1,</p><formula xml:id="formula_54">P (|W | &gt; t/σ γ (1 + ε)) ≤ 2 π σ γ (1 + ε) t e -t 2 /2σ 2 γ (1+ε) 2 , P (|W | &gt; t/σ γ (1 -ε)) ≥ 2 π tσ γ (1 -ε) σ 2 γ (1 -ε) 2 + t 2 e -t 2 /2σ 2 γ (1-ε) 2 .</formula><p>As a result, inf c : lim t→∞ P (|Z| &gt; t) exp(t 2 /2c) = 0 = σ 2 γ .</p><p>For identifiability, we first prove Theorem 2, followed by Theorem 1.</p><p>The solution(s) γ to the above algebraic equations forms a closed, measure zero, and nowhere dense set in R |V (d j )\V (d)| . For d j &gt; d, the corresponding β j depends on Var(ξ V (d ′ )\V (d ′ -1) ) and</p><formula xml:id="formula_55">β j ′ for j ′ ∈ V (d ′ ); d ′ ≤ d.</formula><p>Thus, Ψ c is a finite union of closed and measure zero sets, and as a result, it is nowhere dense. This completes the proof.</p><p>Proof of Theorem 1. Based on Lemma 3, we identify</p><formula xml:id="formula_56">V (1) ⊆ • • • ⊆ V (d max + 1) = V of the true graph. It remains to show {f k } 1≤k≤p are identifiable. Now, suppose ξ V (d) are given. By Theorem 2, E(Y j | Y V (d) ) = f j (Y pa(j) ) + ξ V (d) , β j . If there exist ( fj , βj , pa(j)) such that E(Y j | Y V (d) ) = fj (Y pa(j) ) + ⟨ξ V (d) , βj ⟩. Then, f j (Y pa(j) ) -fj (Y pa(j) ) = ⟨ξ V (d) , βj -β j ⟩.<label>(11)</label></formula><p>To prove that f j (Y pa(j) )-fj (Y pa(j) ) = 0 almost surely, we first show that f</p><formula xml:id="formula_57">j (Y pa(j) )-fj (Y pa(j) ) is constant. Otherwise, F (Y A ) = f j (Y pa(j) ) -fj (Y pa(j)</formula><p>) functionally depends on Y A for a nonempty subset A ⊆ pa(j) ∪ pa(j), which we assume, without loss of generality, that </p><formula xml:id="formula_58">A = arg(F ) is minimal in that F (Y A ) depends on all variables indexed by A. Consider any k ∈ A with d k = max l∈A d l . Since ξ k = Y k -E(Y k | Y V (d k ) ),</formula><formula xml:id="formula_59">, F (Y A ) | Y V (d k ) = (f (Y pa(j) ) -f (Y pa(j) )) | Y V (d k ) is not Gaussian under Condition 1 by</formula><p>Lemma 1, which leads to a contradiction. So A = ∅ and F (Y A ) is constant. Note that the right-hand side of (11) has mean zero, which completes the proof.</p><p>Lemma 4. Assume that Conditions 2-3, 5-6 are met. Let g j be an δ 2 n -minimizer of a least squares regression criterion such that</p><formula xml:id="formula_60">Y j -g j Y V (d j ) , ξ V (d j ) 2 ≤ min g j ∈F j Y j -g j Y V (d j ) , ξ V (d j ) 2 + c 8 nδ 2 n , with δ 2 n ≥ ϵ 2 n . Then P pa(j) ̸ = pa • (j) ≤ c 7 exp(-c 5 nδ 2 n -log n); j = 1, . . . , p.</formula><p>Proof of Lemma 4. By Condition 3, any f j ∈ F j with a wrong support set arg(f</p><formula xml:id="formula_61">j ) ̸ = pa • (j) satisfies ∥g j -g • j ∥ 2 2 ≥ 4c 3 ϵ 2 n . However, by Condition 2, ∥g * j -g • j ∥ L 2 ≤ c 3 ϵ 2 n &lt; 4c 3 ϵ 2 n , implying that f * j has the same support of f • j or arg(f * j ) = pa • (j). Let d = d j . Step 1. Partitioning. Given a class {A : A ̸ = pa • (j), |A| ≤ |pa • (j)|} of candidate augmented sets of arg(g j ), we partition A as A = (A \ pa • (j)) ∪ (A ∩ pa • (j)); j = 1, . . . , p. Now consider a partition of F j . Let E(ν 1 , ν 2 ) =    g j ∈ F j : arg(f j ) ̸ = pa • (j), |A ∩ pa • (j)| = ν 1 , |A \ pa • (j)| = ν 2 , (|pa • (j)| -ν 1 )D min ≤ ∥g -g * j ∥ 2 L 2    be a subclass of functions of F j ; ν 1 = 0, . . . , |pa • (j)| -1 and ν 2 = 1, . . . , |pa • (j)| -ν 1 . Then functions in E(ν 1 , ν 2 ) have at most |pa • (j)| ν 1 p-|pa • (j)| ν 2 different supports. By definition, g j ∈ F j , A = arg(f j ) : A ̸ = pa • (j), |A| ≤ |pa • (j)| ⊆ |pa • (j)|-1 ν 1 =0 |pa • (j)| ν 2 =1 E(ν 1 , ν 2 ).</formula><p>Denote by the log-likelihood L j (g</p><formula xml:id="formula_62">j ) = -∥Y j -g j Y V (d j ) , ξ V (d j ) ∥ 2 /2σ 2 j = 1.</formula><p>Here, without loss of generality, we assume that σ 2 j = 1 and |pa • (j)| ≥ 1. Using the previously established fact that arg(g * j ) = pa • (j), we have</p><formula xml:id="formula_63">P ( pa(j) ̸ = pa • (j)) = P * sup {g j ∈F j :arg(f j )̸ =pa • (j),|arg(f j )|≤|pa • (j)|} (L j (g j ) -L j (g * j )) ≥ -c 8 nϵ 2 n ≤ |pa • (j)|-1 ν 1 =0 |pa • (j)|-ν 1 ν 2 =1 P * sup g∈E(ν 1 ,ν 2 ) (L j (g j ) -L j (g * j )) ≥ -c 8 nϵ 2 n ,</formula><p>where P * denotes the outer probability.</p><p>Step 2. Large-deviation bounds.</p><formula xml:id="formula_64">Let ∆ n = max 1≤j≤p E(p g • j /p g * j ) -1 be the Kullback- Leibler divergence, where p g j = p g j (Y j , Y V (d) , ξ V (d) ) is the joint probability density function for (Y j , Y V (d) , ξ V (d) ). By (2), p g j (Y j , Y V (d) , ξ V (d) ) = exp(-(Y j -g j (Y V (d) , ξ V (d) )) 2 /2). By Condition 2, E(p g • j /p g * j ) equals to E exp ξ j (g * j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )) + (g * j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )) 2 2 = E exp |g * j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )| 2 = E exp(|f * j (Y V (d) ) -f • j (Y V (d) )| 2 ) ≤ c∥f * j -f • j ∥ 2 L 2 + 1 = c∥g * j -g • j ∥ 2 L 2 + 1. Note that, for some constant c &gt; 0, c∥g j -g * j ∥ 2 L 2 ≤ h 2 (g j , g * j ) ≡ 1 -exp(-∥g j -g * j ∥ 2 2 /8) when ∥g j -g * j ∥ 2 L 2 ≤ 1</formula><p>, where h is the Hellinger-distance. By Theorem 3 of Wong and Shen (1995) with δ n (1) = ∆ n there, under Conditions 3 and 5, there exists a constant c 5 &gt; 0 such that</p><formula xml:id="formula_65">P * sup g j ∈E(ν 1 ,ν 2 ) L j (g j ) -L j (g * j ) ≥ -c 8 nϵ 2 n ≤5 p -|pa • (j)| ν 1 |pa • (j)| ν 2 exp(-c 5 n(|pa • (j)| -ν 1 )D min + n(∆ n -1)).</formula><p>Thus, P ( pa(j) ̸ = pa • (j)) is upper bounded by This completes the proof.</p><formula xml:id="formula_66">|pa • (j)|-1 ν 1 =0 |pa • (j)|-ν 1 ν 2 =1 5 |pa • (j)| ν 1 p -|pa • (j)| ν 2 exp(-c</formula><p>Lemma 5. Under the assumptions of Theorem 3,</p><formula xml:id="formula_67">P ∥ g j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )∥ 2 ≥ c 8 nϵ 2 n ≤ 3 exp(-(1 -c 6 )nϵ 2 n ) + c 7 exp(-c 5 nϵ 2 n -log n),</formula><p>provided that P ( pa(j) ̸ = pa • (j)) ≤ c 7 exp(-c 5 nϵ 2 n -log n); j = 1, . . . , p.</p><p>Step 1. Bounds for error-in-variable ξ V (d) . For j ∈ V (d) with d = d j ≥ 1, let g j be the estimated function via (7) based on error-in-variables (Y V (d) , ξ V (d) ) ∈ R n×2|V (d)| , where</p><formula xml:id="formula_68">ξ V (d) = ( ξ k ) k∈V (d) and ξ k = Y k -g k (Y V (d) , ξ V (d) ). Let ξ k = Y k -g • k (Y V (d) , ξ V (d)</formula><p>) be the oracle residual vector. We bound ∥ξ j -ξ j ∥ 2 for j ∈ V (d + 1) inductively.</p><p>For d ≥ 1, consider an induction hypothesis for V (d) P ( pa(k) ̸ = pa • (k)) ≤ c 7 e -c 5 nD min +log p+n(∆n-1) , ∥ g k -g</p><formula xml:id="formula_69">• k ∥ L 2 = O p (ϵ n ), P ∥ ξ k -ξ k ∥ 2 ≥ c d-1 nϵ 2 n ≤ c 7 e -c d-1 nϵ 2 n , ∀k ∈ V (d),<label>(13)</label></formula><p>where c d-1 &gt; 0 is a constant.</p><p>For k ∈ V (1), ξ j = ξ j and g k = g • k = 0, so the induction hypothesis ( <ref type="formula" target="#formula_69">13</ref>) is satisfied. For d j = d, we will prove that ( <ref type="formula" target="#formula_69">13</ref>) is met given that it is satisfied by k ∈ V (d). Let</p><formula xml:id="formula_70">δ 2 n ≥ n -1 ∥ g j (Y V (d) , ξ V (d) ) -g • (Y V (d) , ξ V (d) )∥ 2 . By (4), n -1 Y j -g j (Y V (d) , ξ V (d) ) 2 ≤ n -1 Y j -g * j (Y V (d) , ξ V (d) ) 2 . If δ 2 n ≥ ϵ 2 n , then n -1 ∥Y j -g j (Y V (d) , ξ V (d) )∥ 2 ≤ n -1 ∥Y j -g * j (Y V (d) , ξ V (d) )∥ 2 -2n -1 (Y j -g j (Y V (d) , ξ V (d) )) ⊤ (ξ V (d) -ξ V (d) ) β j + 2n -1 (Y j -g * j (Y V (d) , ξ V (d) )) ⊤ (ξ V (d) -ξ V (d) )β • j + n -1 ∥(ξ V (d) -ξ V (d) )β • j ∥ 2 ≤ n -1 ∥Y j -g * j (Y V (d) , ξ V (d) )∥ 2 + 3c(κ • ) 2 δ 2 n ,</formula><p>where the second inequality follows from the Cauchy-Schwarz inequality. By Lemma 4, P ( pa(j) ̸ = pa • (j)) ≤ c 7 e -c 5 nD min +log p+n(∆n-1) and ∥ g j -g • j ∥ 2 2 ≤ δ 2 n . By the triangular inequality,</p><formula xml:id="formula_71">∥ ξ j -ξ j ∥ = ∥ g j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )∥ ≤ ∥ g j (Y V (d) , ξ V (d) ) -g j (Y V (d) , ξ V (d) )∥ + ∥ g j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )∥ = ∥( ξ V (d) -ξ V (d) ) β j ∥ + ∥ g j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )∥.</formula><p>Note that ∥( ξ V (d) -ξ V (d) ) β j ∥ 2 ≤ cnκ • ϵ 2 n by the induction hypothesis. Also, by Lemma 5, for δ 2 n ≥ ϵ 2 n , we have</p><formula xml:id="formula_72">P (∥ g j (Y V (d) , ξ V (d) ) -g • j (Y V (d) , ξ V (d) )∥ 2 ≥ cnδ 2 n ) ≤ ce -nδ 2 n .</formula><p>Finally, let δ 2 n = Cϵ 2 n for a sufficiently large constant C &gt; 0. Then we have P ( pa(j) ̸ = pa • (j)) ≤ c 7 e -c 5 nD min +log p+n(∆n-1) ,</p><formula xml:id="formula_73">∥ g k -g • k ∥ 2 2 = O p (ϵ 2 n ), P (∥ ξ j -ξ j ∥ 2 ≥ c d nϵ 2 n ) ≤ e -nc d ϵ 2 n ,</formula><p>for d j = d. This proves the induction hypothesis.</p><p>Finally, note that</p><formula xml:id="formula_74">P ( G ̸ = G • ) ≤ p j=1</formula><p>P ( pa(j) ̸ = pa • (j)).</p><p>Then max 1≤j≤p ∥ g j -g • j ∥ L 2 = O p (ϵ n ).</p><p>Step 2. Bounds for max 1≤j≤p ∥ f j -f • j ∥ 2 . Suppose that f j is supported on a uniformly bounded set {∥Y V (d) ∥ ∞ ≤ ρ 1 } for some constant ρ 1 &gt; 0. Then, there exists ρ 2 such that</p><formula xml:id="formula_75">E = {∥ξ V (d) ∥ ∞ &lt; ρ 2 } ⊇ {∥Y V (d) ∥ ∞ &lt; ρ 1 }. Let S = {k : β • jk ̸ = 0} ⊆ V (d). Note that ∥ g j -g • j ∥ 2 L 2 ≥ S c | g j -g • j | 2 dP = ( β j,S -β • j,S ) ⊤ E(I E c ξ S ξ ⊤ S )( β j,S -β • j,S ),</formula><p>where I E c (•) denotes the indicator. Since c -≤ λ min (Σ) ≤ λ max (Σ) ≤ c + , this implies ξ S is not degenerated and E(I E c ξ S ξ ⊤ S ) ≥ c for some constant c &gt; 0. Hence, we have that ∥ β j -β • j ∥ 2 = ∥ β j,S -β • j,S ∥ 2 = O p (ϵ 2 n ). If follows that E |⟨ξ V (d) , β j -β • j ⟩| 2 = O p (ϵ 2 n ). By the triangular inequality, ∥ f j -f</p><formula xml:id="formula_76">• j ∥ L 2 ≤ ∥ g j -g • j ∥ L 2 + E |⟨ξ V (d) , β j -β • j ⟩| 2 1/2</formula><p>= O p (ϵ n ), which completes the proof.</p><p>Proof of Theorem 4. The proof consists of three steps.</p><p>Step 1. Truncation. We truncate Y (i) j</p><p>: i = 1, . . . , n, j = 1, . . . , p to treat the unbounded issue. From (2),</p><formula xml:id="formula_77">Y (i) j | Y (i) pa(j) ∼ N f j Y (i) pa(j) , σ 2 j + σ 2 η,j ,</formula><p>where σ 2 η,j is the j-th diagonal of Σ η . By the uniform boundedness of f j , max </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Topological depth: d 1 = d 3 = 0 (nodes 1 and 3 are root nodes), d 2 = 1, d 4 = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>requires p-|V (d-1)| tests, among which |V (d)|-|V (d-1)| null hypotheses are true and p-|V (d)| are not. Thus, |T | = dmax+1 d=1 (|V (d)|-|V (d-1)|) = p, suggesting an empirical rule α = o(1/p) so that |T |α → 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3. The presence of Y 2 /3 is due to the confounder η. If we have regressed Y 3 on Y 1 and Y 2 to identify the parent variables of Y 3 , then the regression would yield a true discovery Y 1 → Y 3 and a false discovery Y 2 → Y 3 . Consequently, direct regression of Y j on Y V (d j ) without any adjustment renders false discovery of functional causal relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Display of the directed acyclic graph in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where τ &gt; 0, 0 ≤ κ j ≤ |V (d)|, 0 ≤ ς j ≤ |V (d)|, and s ≥ 0 are hyperparameters. See Section A.3 for more details on network training and hyperparameter tuning. Remark 2. Algorithm 1 requires O(d max (p -1)) normality tests and regressions (4). Each regression (4), solved by (7) with stochastic gradient descent, requires O(N epoch n dim(θ)) operations, where N epoch is the number of epochs in training and one epoch means that each sample in training has an opportunity to update model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc>For evaluation, we consider four graph metrics: the false discovery rate (FDR), the false positive rate (FPR), the true positive rate (TPR), and the structural Hamming distance (SHD). To compute the metrics, let TP, RE, and FP be the numbers of identified edges with correct directions, those with wrong directions, and estimated edges not in the skeleton of the true graph. Moreover, denote by PE the total number of estimated edges, TN the number of correctly identified non-edges, and FN the number of missing edges compared to the true skeleton. Then FDR = (RE + FP)/PE, FPR = (RE + FP)/(FP + TN), TPR = TP/(TP + FN), SHD = FP + FN + RE. Note that LRpS-GES outputs a completed partially DAG (CPDAG) and RFCI outputs a partial ancestral graph (PAG). Both PAG and CPDAG may contain undirected edges, in which case they are evaluated favorably by assuming the correct directions for undirected edges whenever possible, similar to Zheng et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reconstructed directed acyclic graphs for (a) AD-MCI and (b) CN groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Undirected graph displaying the estimated residual correlations of ε = Y jf j (Y pa(j) ) j∈V , where a connection between two genes indicates the absolute value of residual correlation exceeds 0.15. Edge connections from one gene to other multiple genes suggest the presence of confounders or nonzero off-diagonal elements of the covariance matrix Σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>dim( m) denotes the number of parameters in model m. As suggested in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>constraints on the compositions of nonlinear functions, which is automatically satisfied by sublinear growth functions when Σ ∈ Ψ (Theorem 1). As suggested by the simulations in Section 5, DeFuSE continues to perform well in recovering the DAG even when Condition 1 and the additive function model (10) are both violated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Training and hyperparameter tuning for DeFuSE. Training and tuning a neural network requires intensive computation. Following the conventional practice of deep learning, we split the original sample into training and validation sets with a partition ratio 9:1, and use on-the-fly evaluation over the validation set for tuning during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>we must have that k ∈ B = {l ∈ V (d) : β jl ̸ = βjl } and d k = max l∈B d l . Moreover, the only term involving Y k in the right-hand side of (11) is a term ( βjk -β jk )Y k , because by definition Y k does not appear in any ξ l for any l ∈ B such that l ̸ = k. If βjk -β jk ̸ = 0, then the right-hand side of (11) becomes l∈B ( βjl -β jl )ξ l | Y V (d k ) , which is Gaussian. However, on the left-hand side</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>5 n(|pa • (j)| -ν 1 )D min + n(∆ n -1)) ≤ |pa • (j)|-1 ν 1 =0 5 |pa • (j)| ν 1 exp(-(|pa • (j)| -ν 1 )(c 5 nD min -log p) + n(∆ n -1))≤c 7 exp(-c 5 nD min + log p + n(∆ n -1)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>c log(np) almost surely for some constant c &gt; 0. Let Y j |, B be the truncated Y (i) j ; i = 1, . . . , n, j = 1, . . . , p, where B = c log(np) is a truncation constant. Then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Averaged false positive rate (FPR), false discovery rate (FDR), true positive rate (TPR), structural Hamming distance (SHD), and their standard deviations in parenthesis, for five methods based on 50 replications. A smaller value of FPR, FDR, and SHD indicates higher accuracy, whereas a larger value of TPR means higher accuracy. For DeFuSE*, the data are standardized. For hub DAG, when p = 100 and n = 500, LRpS-GES fails to deliver the computational results after 96 hours.</figDesc><table><row><cell>Hub</cell><cell>FPR FDR TPR SHD</cell><cell>.00 (.00) .06 (.06) .87 (.10) 5.3 ( 4.6)</cell><cell>.00 (.00) .07 (.10) .91 (.16) 4.2 ( 5.6)</cell><cell>.09 (1.0) .69 (.05) .53 (.07) 48.2 ( 6.9)</cell><cell>.19 (.02) .84 (.05) .52 (.17) 94.3 (12.8)</cell><cell>.22 (.02) .95 (.01) .04 (.01) 74.4 ( 3.7)</cell><cell>.08 (.01) .92 (.01) .06 (.01) 44.5 ( 1.4)</cell><cell>.00 (.00) .05 (.03) .72 (.24) 31.4 (23.7)</cell><cell>.00 (.00) .10 (.18) .71 (.27) 32.9 (26.2)</cell><cell>.05 (.01) .94 (.01) .16 (.03) 306.3 (13.0)</cell><cell>.18 (.02) .96 (.01) .03 (.05) 992.6 (65.4)</cell><cell>.07 (.01) .99 (.01) .01 (.00) 268.6 ( 6.7)</cell><cell>----</cell></row><row><cell>Graph Random</cell><cell>(p, n) Method FPR FDR TPR SHD</cell><cell>(30,500) DeFuSE .00 (.00) .12 (.06) .93 (.04) 2.6 ( 1.2)</cell><cell>DeFuSE* .00 (.00) .13 (.11) .93 (.07) 1.7 ( 1.4)</cell><cell>CAM .03 (.00) .52 (.02) 1.0 (.02) 14.2 ( 1.0)</cell><cell>NOTEARS .28 (.07) .91 (.02) .80 (.13) 120.2 (31.6)</cell><cell>RFCI .07 (.01) .89 (.03) .29 (.11) 26.8 ( 1.2)</cell><cell>LRpS-GES .07 (.01) .91 (.03) .21 (.07) 31.9 ( 1.7)</cell><cell>(100,500) DeFuSE .00 (.00) .03 (.03) .92 (.03) 4.0 ( 1.7)</cell><cell>DeFuSE* .00 (.00) .16 (.06) .85 (.06) 10.6 ( 3.0)</cell><cell>CAM .01 (.00) .61 (.01) 1.0 (.01) 57.4 ( 2.5)</cell><cell>NOTEARS .04 (.02) .93 (.04) .18 (.15) 130.6 (24.8)</cell><cell>RFCI .02 (.00) .95 (.02) .15 (.06) 83.5 ( 1.1)</cell><cell>LRpS-GES .02 (.00) .96 (.01) .10 (.04) 83.3 ( 2.0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Sensitivity analysis: Averaged false positive rate (FPR), false discovery rate (FDR), true positive rate (TPR), structural Hamming distance (SHD), and their standard deviations in parenthesis, for different choices of α based on 50 replications. A smaller value of FPR, FDR, and SHD indicates higher accuracy, whereas a larger value of TPR means higher accuracy. Here, p = 30 and n = 500.</figDesc><table><row><cell>Graph</cell><cell>α</cell><cell>FPR</cell><cell>FDR</cell><cell>TPR</cell><cell>SHD</cell></row><row><cell cols="6">Random .100 .00 (.00) .12 (.08) .95 (.05) 2.4 (1.7)</cell></row><row><cell></cell><cell cols="5">.050 .00 (.00) .13 (.07) .96 (.04) 2.4 (1.5)</cell></row><row><cell></cell><cell cols="5">.025 .00 (.00) .12 (.06) .93 (.04) 2.6 (1.2)</cell></row><row><cell></cell><cell cols="5">.010 .00 (.00) .13 (.07) .92 (.07) 3.0 (1.6)</cell></row><row><cell>Hub</cell><cell cols="5">.100 .00 (.00) .08 (.04) .91 (.04) 5.0 (2.5)</cell></row><row><cell></cell><cell cols="5">.050 .00 (.00) .05 (.04) .95 (.03) 3.0 (2.0)</cell></row><row><cell></cell><cell cols="5">.025 .00 (.00) .06 (.06) .87 (.10) 5.3 (4.6)</cell></row><row><cell></cell><cell cols="5">.010 .00 (.00) .03 (.02) .97 (.02) 1.8 (1.5)</cell></row><row><cell cols="2">5.2 Real data analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">This subsection applies DeFuSE to reconstruct gene regulatory networks for the Alzheimer's</cell></row><row><cell cols="6">Disease Neuroimaging Initiative (ADNI) data. In particular, we construct two gene networks</cell></row><row><cell cols="6">respectively for Alzheimer's Disease (AD) and healthy subjects to highlight some gene-gene</cell></row><row><cell cols="6">interactions differentiating patients with AD/cognitive impairments and healthy individuals.</cell></row><row><cell cols="6">The ADNI dataset (http://adni.loni.usc.edu/) includes gene expressions, whole-</cell></row><row><cell cols="6">genome sequencing, and phenotypic data. After cleaning and merging, we obtain a sample</cell></row><row><cell cols="6">of 712 subjects in four groups, Alzheimer's Disease (AD), Early Mild Cognitive Impairment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The AIC values for quadratic and linear models fitted for each non-root gene, as defined in (9). A smaller AIC value indicates better model fitting.</figDesc><table><row><cell>Group</cell><cell></cell><cell></cell><cell cols="2">AD-MCI</cell><cell></cell><cell></cell><cell>CN</cell><cell></cell></row><row><cell cols="7">Gene name APH1A PPP3R1 MAPT GSK3B COX7C NDUFS4</cell><cell cols="2">ATP2A2 COX7C</cell></row><row><cell>Quadratic</cell><cell>.717</cell><cell>.656</cell><cell>.528</cell><cell>.620</cell><cell>.356</cell><cell>.606</cell><cell>.572</cell><cell>.304</cell></row><row><cell>Linear</cell><cell>.701</cell><cell>.732</cell><cell>.567</cell><cell>.695</cell><cell>.395</cell><cell>.657</cell><cell>.656</cell><cell>.349</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>with a learning rate 0.1 and various numbers of epochs {250, 500, . . . , 4000} in our experiments. Then we choose the best-performing model.Other methods. The R packages CAM, pcalg, and lrpsadmm are available at https: //github.com/cran/CAM, https://github.com/cran/pcalg, and https://github.com/ benjaminfrot/lrpsadmm, respectively. The Python program notears is available at https: //github.com/xunzheng/notears. We use their default settings for CAM, NPTEARS, LRpS-GES, and RFCI.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>The research is supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS-1952539</rs>, <rs type="funder">NIH</rs> grants <rs type="grantNumber">R01GM113250</rs>, <rs type="grantNumber">R01GM126002</rs>, <rs type="grantNumber">R01AG065636</rs>, <rs type="grantNumber">R01AG074858</rs>, <rs type="grantNumber">R01AG069895</rs>, <rs type="grantNumber">U01AG073079</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KwbsD2t">
					<idno type="grant-number">DMS-1952539</idno>
				</org>
				<org type="funding" xml:id="_Kv8dtQz">
					<idno type="grant-number">R01GM113250</idno>
				</org>
				<org type="funding" xml:id="_P8XeyQs">
					<idno type="grant-number">R01GM126002</idno>
				</org>
				<org type="funding" xml:id="_b4EJube">
					<idno type="grant-number">R01AG065636</idno>
				</org>
				<org type="funding" xml:id="_JrnZdRR">
					<idno type="grant-number">R01AG074858</idno>
				</org>
				<org type="funding" xml:id="_Y3fuesw">
					<idno type="grant-number">R01AG069895</idno>
				</org>
				<org type="funding" xml:id="_pWrtr3j">
					<idno type="grant-number">U01AG073079</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof of Theorem 2. Note that E(ε j | Y V (d) ) = E(ε j | ξ V (d) ) for any d j = d. By (2), we have</p><p>Transforming ε = (ε V (1) , ε V (2)\V (1) , . . . , ε V \V (dmax) ) to ξ = (ξ V (1) , ξ V (2)\V (1) . . . , ξ V \V (dmax) ) can be regarded as a block Gram-Schmidt process, where ξ V (1) , ξ V (2)\V (1) . . . , ξ V \V (dmax) are uncorrelated. Thus, (ε j , ξ V (d) ) follows a joint Gaussian distribution. The desired result follows from the fact that E(ε j | ξ V (d) ) = ⟨ξ V (d) , β j ⟩. This completes the proof.</p><p>Lemma 3. Under Condition 1, the set Ψ c is closed and nowhere dense in {Σ : Σ ≻ 0}.</p><p>Moreover, Ψ c has zero Lebesgue measure.</p><p>Proof. Note that Σ can be reparameterized by</p><p>) and constant variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that the distribution</head><p>By Lemma 2, Z has to be normal with constant variance σ 2 = Var(⟨ξ</p><p>For each x, this implies an infinite set of moment conditions for Z,</p><p>where ξ</p><p>has mean zero and is independent of (Y</p><p>). Next, we apply Theorem 3 of <ref type="bibr" target="#b39">Shen and Wong (1994)</ref> to bound the empirical process</p><p>To verify the conditions there, we assume, without loss of generality, that Var(ξ j ) = 1 subsequently. It suffices to consider { pa(j) = pa(j)}. Define the function space</p><p>Moreover, ϵ n ≤ T and by Condition 5,</p><p>By Theorem 3 of <ref type="bibr" target="#b39">Shen and Wong (1994)</ref>, we have</p><p>The desired result follows immediately.</p><p>Proof of Theorem 3. We prove Theorem 3 by induction for V (1), . . . , V (d max ). First, note that no estimation is needed for V (1). For j ∈ V \ V (1), we bound P ( pa(j) ̸ = pa • (j)) as well as ∥ g j -g • j ∥ L 2 . The proof proceeds in two steps.</p><p>1≤i≤n are independent and identically distributed. Let P and P denote the probability for Y</p><p>where C &gt; 0 is a generic constant and C 1 &gt; 0 is defined in Condition 4. Note that P is supported on [-B, B] p , so it suffices to consider the convergence rate of</p><p>, where f j is based on truncated data Y (i)  1≤i≤n on a bounded domain [-B, B] p .</p><p>Step 2. Approximation Error. Note that F j is uniformly bounded. By Theorem 1 of Schmidt-Hieber ( <ref type="formula">2019</ref>), for any 0 &lt; ϵ n &lt; 1/2, there exists an FNN f * ∈ F n j with depth</p><p>Then Condition 2 is satisfied.</p><p>Step</p><p>, where H ∞ (u, •) denotes the entropy under the sup-norm. Then,</p><p>Thus, the entropy integral in Condition 5 becomes max 1≤j≤p max</p><p>This implies Condition 5.</p><p>Finally, an application of Theorem 3 yields the desired result when</p><p>, κ j = |pa • (j)|, and ∥β • j ∥ 0 ≤ ς j ≤ ς • ; j = 1, . . . , p, which completes the proof.</p><p>Proof of Theorem 5. First, when Y j given Y V (d) is non-normal for d j &gt; d, V (1), . . . , V (d max ) are uniquely identifiable by the same argument in the proof of Theorem 1.</p><p>by mathematical induction on d j -d k .</p><p>We begin with</p><p>Consider d j -d k = l &gt; 1. Suppose that ( <ref type="formula">14</ref>) holds for j ′ , k ′ with d j ′ -d k ′ &lt; l. Then, for the terms containing Y k , we have f j,k -f j,k = j ′ ∈V (d) γ j ′ f j ′ k -γ j ′ f j ′ k . For f j ′ ,k ̸ = 0 on the right-hand side, d j ′ -d k &lt; l, so</p><p>for some ψ j ′ ; j ′ ∈ V (d). Hence, [ f j,k ] = [f j,k ]j ′ ∈V (d) ψ j ′ [f j ′ ,k ]. This leads to (14).</p><p>In ( <ref type="formula">14</ref>), [ f j,k ] cannot be [0] if the condition in Theorem 5 holds, so pa(j) ⊆ pa(j). By symmetry, pa(j) ⊆ pa(j), which completes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The DeCAMFounder: Non-linear causal discovery in the presence of hidden variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07921</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="610" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asymptotic theory of certain &quot;goodness of fit&quot; criteria based on stochastic processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Darling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="212" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple measure of conditional dependence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azadkia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3070" to="3102" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, highdimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1935" to="1967" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A two-stage penalized least squares method for constructing large systems of structural equations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="73" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient structure learning of Bayesian networks using constraints</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="663" to="689" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A scoring function for learning Bayesian networks based on mutual information and conditional independence tests</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2149" to="2187" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks for estimation and inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust causal structure learning with some hidden variables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="459" to="487" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Penalized estimation of directed acyclic graphs from discrete data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="176" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Neural Information Processing Systems</title>
		<meeting>the 21st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Bayesian network structure using LP relaxations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying confounders using additive noise models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PC-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">KEGG: Kyoto encyclopedia of genes and genomes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kanehisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="30" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Presenilin-1 mutations and Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="631" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Likelihood ratio tests for a large directed acyclic graph</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">531</biblScope>
			<biblScope unit="page" from="1304" to="1319" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Inference for a large directed acyclic graph with unspecified interventions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03805</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the Lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal discovery with general non-linear relationships using non-linear ICA</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regression by dependence minimization and its application to causal inference in additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amyloid precursor protein processing and Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative accuracy of plasma phosphotau217 for Alzheimer disease vs other neurodegenerative disorders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janelidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Quiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zetterberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stomrud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leuzy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="772" to="781" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identifiability of Gaussian structural equation models with equal error variances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2009" to="2053" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beware of the simulated DAG! Causal discovery benchmarks may be easy to game</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reisach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weichwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27772" to="27784" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal proteinsignaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt-Hieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00695</idno>
		<title level="m">Deep ReLU network approximation of functions on a manifold</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Right singular vector projection graphs: fast high dimensional covariance matrix estimation under latent confounding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Thanei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Likelihood-based selection and sharp parameter estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">497</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convergence rate of sieve estimates</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="580" to="615" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimal global rates of convergence for nonparametric regression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1040" to="1053" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Geometry of the faithfulness assumption in causal inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="436" to="463" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Van De Geer</surname></persName>
		</author>
		<title level="m">Empirical Processes in M-Estimation</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph estimation with joint additive models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Voorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="101" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">528</biblScope>
			<biblScope unit="page" from="1574" to="1596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probability inequalities for likelihood ratios and convergence rates of sieve MLES</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="362" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Constrained likelihood for reconstructing a directed acyclic Gaussian graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric DAGs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
