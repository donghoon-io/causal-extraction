<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning</title>
				<funder ref="#_utPrYPh #_E4j5SKM">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_NAyQ6rU #_aByJBqy #_fHBCYJw">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_hPXYfdM">
					<orgName type="full">Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wangzhen</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinkang</forename><surname>Gong</surname></persName>
							<email>gongqk@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
							<email>raoyangh@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred to as disconnected reasoning problem. To alleviate this issue, we propose a novel counterfactual multihop QA, a causaleffect approach that enables to reduce the disconnected reasoning. It builds upon explicitly modeling of causality: 1) the direct causal effects of disconnected reasoning and 2) the causal effect of true multi-hop reasoning from the total causal effect. With the causal graph, a counterfactual inference is proposed to disentangle the disconnected reasoning from the total causal effect, which provides us a new perspective and technology to learn a QA model that exploits the true multi-hop reasoning instead of shortcuts. Extensive experiments have been conducted on the benchmark HotpotQA dataset, which demonstrate that the proposed method can achieve notable improvement on reducing disconnected reasoning. For example, our method achieves 5.8% higher points of its Supp s score on HotpotQA through true multihop reasoning. The code is available at <ref type="url" target="https://github.com/guowzh/CFMQA">https://github.com/guowzh/CFMQA</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-hop question answering (QA) <ref type="bibr" target="#b6">(Groeneveld et al., 2020;</ref><ref type="bibr" target="#b4">Ding et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2019;</ref><ref type="bibr" target="#b22">Shao et al., 2020)</ref> requires the model to reason over multiple supporting facts to correctly answer a complex question. It is a more challenging task than the single-hop QA since not only the correct answer but the explicit reasoning across multiple evidences should be provided.</p><p>Hence, recent work <ref type="bibr" target="#b6">(Groeneveld et al., 2020;</ref><ref type="bibr" target="#b5">Fang et al., 2019)</ref> has shown that the multi-hop QA is always formulated as two sub-tasks: question * *Corresponding Author answering and support identification. For example, <ref type="bibr" target="#b6">Groeneveld et al. (2020)</ref> solved the multi-hop QA via 1) question answering which uses a BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> span prediction model to answer the questions, and 2) support identification which aims to identify the supporting sentences. <ref type="bibr">Yavuz et al. (2022)</ref> proposed PATHFID based on fusionin-decoder (FID) <ref type="bibr" target="#b8">(Izacard and Grave, 2020)</ref> for question answering and support identification.</p><p>However, dividing the multi-hop QA into question answering and support identification doesn't mean that QA models answer the questions according to what the multi-hop QA wants. These QA models are all based on the large pre-training language models, e.g., BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref>, these black-box language models are rather opaque models in terms of the reasoning processes. It may results in one main problem of multihop QA models: disconnected reasoning <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>, which allows the QA models to exploit the reasoning shortcuts <ref type="bibr" target="#b9">(Jiang and Bansal, 2019;</ref><ref type="bibr" target="#b11">Lee et al., 2021)</ref> instead of multihop reasoning to cheat and obtain the right answer. Taking Fig. <ref type="figure" target="#fig_1">1</ref> as an example, to answer the question "until when in the U.S. Senate", multi-hop QA requires to answer the question with a true reasoning path (e.g., the second paragraph ‚Üí the third paragraph in Fig. <ref type="figure" target="#fig_1">1</ref>). However, the black-box QA models can also infer the correct answer by just utilizing the types of problems, e.g., we can find the corresponding fact "from 2005 to 2008" in the contexts to answer this type of question "until when" without reasoning.</p><p>To address the above problem, two issues should be considered: 1) how to measure the disconnected reasoning? To the best of our knowledge, <ref type="bibr" target="#b23">Trivedi et al. (2020)</ref> firstly defined an evaluation measure, DiRe in short, to measure how much the QA model can cheat via disconnected reasoning. A probing dataset should be constructed to measure the DiRe. And 2) how to reduce the disconnected reasoning? One possible solution is to strengthen the training dataset via extra annotations or adversarial examples, which makes it cannot find the correct answers by only one supporting fact. For example, <ref type="bibr" target="#b9">Jiang and Bansal (2019)</ref> constructed the adversarial examples to generate better distractor facts. Besides, counterfactual intervention <ref type="bibr" target="#b11">(Lee et al., 2021;</ref><ref type="bibr" target="#b29">Ye et al., 2021)</ref> had also been explored to change the distribution of the training dataset. However, when these existing approaches decrease the disconnected reasoning, the original performance also drops significantly. It is still challenging to reduce disconnected reasoning while maintaining the same accuracy on the original test set.</p><p>Motivated by causal inference <ref type="bibr" target="#b19">(Pearl and Mackenzie, 2018;</ref><ref type="bibr" target="#b18">Pearl, 2022;</ref><ref type="bibr" target="#b17">Niu et al., 2021)</ref>, we utilize the counterfactual reasoning to reduce the disconnected reasoning in multi-hop QA and also obtain the robust performance on the original dataset. We formalize a causal graph to reflect the causal relationships between question (Q), contexts, and answer (Y ). To evaluate the disconnected reasoning, contexts are further divided into two subsets: S is a supporting fact and C are the remaining supporting facts. Hence, we can formulate the disconnected reasoning as two natural direct causal effects of (Q, S) and (Q, C) on Y as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. With the proposed causal graph, we can relieve the disconnected reasoning by disentangling the two natural direct effects and the true multihop reasoning from the total causal effect. A novel counterfactual multihop QA is proposed to disentangle them from the total causal effect. We utilize the generated probing dataset proposed by <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref> and DiRe to measure how much the proposed multi-hop QA model can reduce the disconnected reasoning. Experiment results show that our approach can substantially decrease the disconnected reasoning while guaranteeing the strong performance on the original test set. The results indicate that the proposed approach can improve the true multi-hop reasoning capability.</p><p>The main contribution of this paper is threefold. Firstly, our counterfactual multi-hop QA model formulates disconnected reasoning as two direct causal effects on the answer, which is a new perspective and technology to learn true multi-hop reasoning. Secondly, our approach achieves notable improvement on reducing disconnected reasoning compared to various baselines. Thirdly, our causaleffect approach is model-agnostic and can be used for reducing disconnected reasoning in many multihop QA architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-hop question answering (QA) requires the model to retrieve the supporting facts to predict the answer. Many approaches and datasets have been proposed to train QA systems. For example, HotpotQA <ref type="bibr" target="#b28">(Yang et al., 2018)</ref> dataset is a widely used dataset for multi-hop QA, which consists of fullwiki setting <ref type="bibr" target="#b2">(Das et al., 2019;</ref><ref type="bibr" target="#b15">Nie et al., 2019;</ref><ref type="bibr" target="#b19">Qi et al., 2019;</ref><ref type="bibr" target="#b1">Chen et al., 2019;</ref><ref type="bibr" target="#b12">Li et al., 2021;</ref><ref type="bibr" target="#b25">Xiong et al., 2020)</ref> and distractor setting <ref type="bibr">(Min et al., 2019b;</ref><ref type="bibr" target="#b16">Nishida et al., 2019;</ref><ref type="bibr" target="#b20">Qiu et al., 2019;</ref><ref type="bibr" target="#b9">Jiang and Bansal, 2019;</ref><ref type="bibr" target="#b23">Trivedi et al., 2020)</ref>.</p><p>In fullwiki setting, it first finds relevant facts from all Wikipedia articles and then answers the multi-hop QA with the found facts. The retrieval model is important in this setting. For instance, SMRS <ref type="bibr" target="#b15">(Nie et al., 2019)</ref> and DPR <ref type="bibr" target="#b10">(Karpukhin et al., 2020)</ref> found the implicit importance of retrieving relevant information in the semantic space. Entitycentric <ref type="bibr" target="#b2">(Das et al., 2019)</ref>, CogQA <ref type="bibr" target="#b4">(Ding et al., 2019)</ref> and Golden Retriever <ref type="bibr" target="#b19">(Qi et al., 2019)</ref> explicitly used the entity that is mentioned or reformed in query key words to retrieve the next hop document. Furthermore, PathRetriever <ref type="bibr" target="#b0">(Asai et al., 2019)</ref> and HopRetriever <ref type="bibr" target="#b12">(Li et al., 2021)</ref> can iteratively select the documents to form a paragraph-level reason path using RNN. MDPR <ref type="bibr" target="#b25">(Xiong et al., 2020)</ref> retrieved passages only using dense query vectors many times. These methods hardly discuss the QA model's disconnected reasoning problem.</p><p>In the distractor setting, 10 paragraphs, two gold paragraphs and eight distractors, are given. Many methods have been proposed to strengthen the model's capability of multi-hop reasoning, using graph neural network <ref type="bibr" target="#b20">(Qiu et al., 2019;</ref><ref type="bibr" target="#b5">Fang et al., 2019;</ref><ref type="bibr" target="#b22">Shao et al., 2020)</ref> or adversarial examples or counterfactual examples <ref type="bibr" target="#b9">(Jiang and Bansal, 2019;</ref><ref type="bibr" target="#b11">Lee et al., 2021)</ref> or the sufficiency of the supporting evidences <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref> or make use of the pre-trained language models <ref type="bibr" target="#b31">(Zhao et al., 2020;</ref><ref type="bibr" target="#b30">Zaheer et al., 2020)</ref>.</p><p>However, <ref type="bibr">Min et al. (2019a)</ref> demonstrated that many compositional questions in HotpotQA can be answered with a single hop. It means that QA models can take shortcuts instead of multi-hop reasoning to produce the corrected answer. To relieve the issue, <ref type="bibr" target="#b9">Jiang and Bansal (2019)</ref> added adversarial examples as hard distractors during training. Re-cently, <ref type="bibr" target="#b23">Trivedi et al. (2020)</ref> proposed an approach, DiRe, to measure the model's disconnected reasoning behavior and used the supporting sufficiency label to reduce the disconnected reasoning. <ref type="bibr" target="#b11">Lee et al. (2021)</ref> selected the supporting evidence according to the sentence causality to the predicted answer, which guarantees the explainability of the behavior of the model. While the original performance also drops when reducing the disconnected reasoning.</p><p>Causal Inference. Recently, causal inference <ref type="bibr" target="#b19">(Pearl and Mackenzie, 2018;</ref><ref type="bibr" target="#b18">Pearl, 2022)</ref> has been applied to many tasks of natural language processing and computer vision, and it shows promising results and provides strong interpretability and generalizability. The representative works include counterfactual intervention for visual attention <ref type="bibr" target="#b21">(Rao et al., 2021)</ref>, causal effect disentanglement for VQA <ref type="bibr" target="#b17">(Niu et al., 2021)</ref>, the back-door and frontdoor adjustments <ref type="bibr">(Zhu et al., 2021;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref><ref type="bibr" target="#b26">Yang et al., 2021)</ref>. Our method can be viewed as a complement of the recent approaches that utilize the counterfactual inference <ref type="bibr" target="#b11">(Lee et al., 2021;</ref><ref type="bibr" target="#b29">Ye et al., 2021)</ref> to identify the supporting facts and predict the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this section, we use the theory of causal inference <ref type="bibr" target="#b19">(Pearl and Mackenzie, 2018;</ref><ref type="bibr" target="#b18">Pearl, 2022)</ref> to formalize our multi-hop reasoning method. Suppose that we have a multi-hop dataset D and each instance has the form of (Q, P ; Y ), where Q is a question and P = {s 1 , s 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , s n } is a context consisting of a set of n paragraphs. And Y is the ground-truth label. Given a question Q with multiple paragraphs as a context P , the multi-hop QA models are required to identify which paragraphs are the supporting facts and predict an answer using the supporting facts.</p><p>Causal graph. In multi-hop QA, multiple supporting facts are required to predict the answer. While QA models may use only one fact to give the answer, which is referred to as disconnected reasoning. For example, given a question q and only a paragraph s, the disconnected reasoning model can predict the correct answer or correctly determine whether the paragraph s is the supporting fact. Hence, to define the causal graph of disconnected reasoning, the context P is further divided into a paragraph S and the remaining paragraphs C.</p><formula xml:id="formula_0">Now the (Q, P ; Y ) becomes (Q, S, C; Y ). That is each instance (Q, P ; Y ) is converted into n examples, i.e., (q, s 1 , C = {s 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , s n }; Y (s 1 )), ‚Ä¢ ‚Ä¢ ‚Ä¢ , (q, s n , C = {s 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , s n-1 }; Y (s n ))</formula><p>, where Y (s i ) = {F s i ; A} includes the supporting fact and answer, where A is the answer and F s i = 1 means the paragraph s is the supporting fact otherwise it is not. For each example, we consider the disconnected reasoning with the fact S (not C).</p><p>The causal graph for multi-hop QA is shown in Figure <ref type="figure" target="#fig_1">1 (a)</ref>, where nodes denote the variables and directed edges represent the causal-and-effect relationships between variables. The paths in Figure <ref type="figure" target="#fig_1">1</ref> (a) are as follows.</p><p>(Q, S) ‚Üí K 1 ‚Üí Y : (Q, S) ‚Üí K 1 denotes that the feature/knowledge K 1 is extracted from the question (Q) and the paragraph (S) via the QA model backbone, e.g., BERT. K 1 ‚Üí Y represents the process that the label Y is predicted by only using the K 1 .</p><p>(Q, C) ‚Üí K 2 ‚Üí Y : Similarly, the feature/knowledge K 2 extracted from the question (Q) and the remaining paragraphs</p><formula xml:id="formula_1">(C) is used to predict the label Y . (Q, S, C) ‚Üí (K 1 , K 2 ) ‚Üí K ‚Üí Y : This path indicates that the QA model predicts the label Y based on both the K 1 and K 2 .</formula><p>Based on the above, the effect of Q, S, C on Y can be divided into: 1) shortcut impacts, e.g.,</p><formula xml:id="formula_2">(Q, S) ‚Üí K 1 ‚Üí Y and (Q, C) ‚Üí K 2 ‚Üí Y , and 2) reasoning impact, e.g., (Q, S, C) ‚Üí (K 1 , K 2 ) ‚Üí K ‚Üí Y . The shortcut impacts cap- ture the direct effect of (Q, S) or (Q, C) on Y via K 1 ‚Üí Y or K 2 ‚Üí Y . The reasoning impact captures the indirect effect of (Q, S, C) on Y via K ‚Üí Y .</formula><p>Hence, to reduce the multi-hop QA model's disconnected reasoning proposed in <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>, we should exclude shortcut impacts (K 1 ‚Üí Y and K 2 ‚Üí Y ) from the total effect.</p><p>Counterfactual definitions. Figure <ref type="figure" target="#fig_1">1</ref> (a) shows the causal graph. From causal graph to formula, we denote the value of Y , i.e., the answer A (e.g., 2008) or the supporting paragraph F s (e.g., is the paragraph s supporting fact?), would be obtained when question Q is set to q, the paragraph S is set to s and the remaining paragraphs c = Ps are used as the context C, which is defined as</p><formula xml:id="formula_3">Y q,s,c (A) = Y (A | Q = q, S = s, C = c), Y q,s,c (s) = Y (s | Q = q, S = s, C = c).</formula><p>For simplicity, we omit A, s and unify both equations as Y q,s,c = Y q,s,c (A) or Y q,s,c =  Y q,s,c (s). Since the causal effect of q, s, c on Y via K 1 , K 2 , K on Y , we have</p><formula xml:id="formula_4">Y k 1 ,k 2 ,k = Y q,s,c<label>(1)</label></formula><p>in the following discussion.</p><p>To disentangle the shortcut impacts from the total causal effect, we use the counterfactual causal inference to block other effects. To model the K 1 ‚Üí Y , the counterfactual formulation</p><formula xml:id="formula_5">K 1 ‚Üí Y : Y k 1 ,k * 2 ,k * ,<label>(2)</label></formula><p>which describes the situation where K 1 is set to the original value k 1 and K and K 2 are blocked. The k * and k * 2 are the counterfactual notations. The k 1 and k * , k * 2 represent the the two situations where the k 1 is under treatment in the factual scenario and k * , k * 2 are not under treatment <ref type="bibr" target="#b19">(Pearl and Mackenzie, 2018)</ref> in the counterfactual scenario. The same definitions for other two effects as</p><formula xml:id="formula_6">K 2 ‚Üí Y : Y k * 1 ,k 2 ,k * ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_7">K ‚Üí Y : Y k * 1 ,k * 2 ,k .<label>(4)</label></formula><p>Causal effects. According to the counterfactual definitions, the total effect of q, s, c on Y can be decomposed into the natural direct effects of K 1 , K 2 on Y and the effect of K on Y as discussed before. The two natural direct effects cause the disconnected reasoning problem. The effect of K ‚Üí Y is the desired multi-hop reasoning.</p><p>As shown in Figure <ref type="figure" target="#fig_1">1</ref> (b), the effect of K 1 on Y with K 2 , K blocked and the effect of K 2 on Y with K 1 , K blocked can be easily obtained by setting the S or C to counterfactual values (please refer to Section 4 for more details). While the effect of K on Y can not be obtained by changing the values S/C to S * /C * . We follow <ref type="bibr" target="#b17">(Niu et al., 2021)</ref> and total indirect effect (TIE) is used to express the effect of K on Y , which is formulated as</p><formula xml:id="formula_8">Y k * 1 ,k * 2 ,k = Y k 1 ,k 2 ,k -Y k 1 ,k 2 ,k * .</formula><p>(5)</p><p>4 Counterfactual Multihop QA</p><p>Following the former formulations, we propose to construct the counterfactual examples to estimate the natural direct effect of K 1 and K 2 , as well as using parameters to estimate the total indirect effect of K. And our calculation of Y in Eq. ( <ref type="formula" target="#formula_5">2</ref>), ( <ref type="formula" target="#formula_6">3</ref>) and ( <ref type="formula">5</ref>) is parametrized by a neural multi-hop QA model F. Please note that F can be any multi-hop QA model and our method is model-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disentanglement of causal effect</head><p>K 1 ‚Üí Y . Specifically, in Eq. ( <ref type="formula" target="#formula_5">2</ref>), the Y k 1 ,k * 2 ,k * describes the situation where K 1 is set to the factual value with Q = q, S = s as inputs, and K 2 /K are set to the counterfactual values. Taking Figure <ref type="figure" target="#fig_1">1</ref> as an example, the QA model only considers the interaction between question q and a given paragraph s.</p><p>The remaining paragraphs c are not given. It is the disconnected reasoning <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>. To obtain the counterfactual values of K 2 and K, we can set the context C as its counterfactual sample, and we have</p><formula xml:id="formula_9">Y k 1 ,k * 2 ,k * = Y q,s,c * = F(q, s, c * ).<label>(6)</label></formula><p>In this paper, we randomly sample the remaining contexts from the training set to construct the counterfactual c * . It represents the no-treatment or the prior knowledge of the remaining context. In the implementation, we randomly sample the remaining contexts in a mini-batch to replace the c in the original triple example (q, s, c) and obtain the corresponding counterfactual triple example (q, s, c * ).</p><p>With that, we can feed it into the QA model to get Y q,s,c * . K 2 ‚Üí Y . Similarly, in Eq. ( <ref type="formula" target="#formula_6">3</ref>), the</p><formula xml:id="formula_10">Y k * 1 ,k 2 ,k *</formula><p>describes the situation where K 2 is set to the factual value with the inputs Q = q and C = c. The K 1 and K are set to the counterfactual values with the counterfactual sample S = s * as input, which is defined as</p><formula xml:id="formula_11">Y k * 1 ,k 2 ,k * = Y q,s * ,c = F(q, s * , c).<label>(7)</label></formula><p>One may argue that how to predict the label when S is set to the counterfactual values? As the example shown in Figure <ref type="figure" target="#fig_1">1</ref>, even without the paragraph s as input, the QA model still can infer the paragraph s is supporting fact via wrong reasoning: since all paragraphs in C do not include the words about time range, the rest paragraph s should be the supporting fact to answer the "until when" question. It is the exclusive method. The wrong reasoning is caused by an incorrect interaction between the paragraph s and the remaining context c. Hence, the S is set to the counterfactual values that can correct such incorrect interactions.</p><p>Hence, in the implementation, we use the adversarial examples as suggested in <ref type="bibr" target="#b9">(Jiang and Bansal, 2019)</ref> to construct the counterfactual s * , which aims to remove the incorrect interaction and perform multi-hop reasoning. Specifically, we randomly perturb the 15% tokens of the paragraph s, 80% of which will be replaced by other random tokens, 10% of which will be replaced by the mask token (e.g. [M ASK]) of the tokenizer, and 10% of which will keep unchanged. After that, we obtain another counterfactual triple example (q, s * , c), we can feed it into QA model to get Y q,s * ,c . K ‚Üí Y . In Eq. ( <ref type="formula">5</ref>), Y q,s,c indicates that the question q, paragraph s and remaining context c are visible to the QA model F:</p><formula xml:id="formula_12">Y k 1 ,k 2 ,k = Y q,s,c = F(q, s, c). (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>The main problem is that Y k 1 ,k 2 ,k * is unknown. It is also hard to use the counterfactual samples of Q, S, C to obtain its value since the q, s and c should be the factual values for k 1 , k 2 . In this paper, we follow the work <ref type="bibr" target="#b17">(Niu et al., 2021)</ref> and also assume the model will guess the output probability under the no-treatment condition of k * , which is represented as</p><formula xml:id="formula_14">Y k 1 ,k 2 ,k * = C, (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where C is the output and a learnable parameter. Similar to Counterfactual VQA <ref type="bibr" target="#b17">(Niu et al., 2021)</ref>, we guarantee a safe estimation of Y k 1 ,k 2 ,k * in expectation.</p><p>Training objective. From the above discussion, we can estimate the total causal effect:</p><formula xml:id="formula_16">Y ‚Üê Y q,s,c * + Y q,s * ,c + Y q,s,c -C.<label>(10)</label></formula><p>Note that Y can be any ground-truth labels of answer span prediction, supporting facts identification or answer type prediction. See Appendix A.1 for further implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Inference</head><p>Training. The model F is expected to disentangle the two natural direct effects and the true multi-hop effect from the total causal effect. To achieve this goal, we apply the Eq. (10) to train the QA model F. Our training strategy follows the HGN <ref type="bibr" target="#b5">(Fang et al., 2019)</ref>:  where Œª is a hyper-parameter and each term of L is cross-entropy loss function. Specifically, for answer prediction, we utilize the Eq. ( <ref type="formula" target="#formula_16">10</ref>) to obtain the predicted logits of the start and end position of the answer span, and respectively calculate the cross-entropy loss L start and L end with corresponding ground truth labels. As for supporting facts prediction, similarly, we use the Eq. ( <ref type="formula" target="#formula_16">10</ref>) to calculate the predicted logits in sentence level and paragraph level, and then calculate L sent and L para . We also apply our counterfactual reasoning method to identify the answer type <ref type="bibr" target="#b20">(Qiu et al., 2019;</ref><ref type="bibr" target="#b5">Fang et al., 2019)</ref>, which consists of yes, no, span and entity. We use the [CLS] token as the global representation to predict the answer type under the Eq. ( <ref type="formula" target="#formula_16">10</ref>) and calculate L type with the ground truth label. Entity prediction (L entity ) <ref type="bibr" target="#b5">(Fang et al., 2019)</ref> is only a regularization term and the Eq. ( <ref type="formula" target="#formula_16">10</ref>) is not applied to this term.</p><formula xml:id="formula_17">L = L start + L end + ŒªL sent + L para + L type + L entity ,<label>(11)</label></formula><p>Inference. As illustrated in Section 3, our goal is to exclude the natural direct effect (K 1 ‚Üí Y, K 2 ‚Üí Y ) and use the true multi-hop effect (K ‚Üí Y ) to reduce the multi-hop QA model's disconnected reasoning, so we use Eq. ( <ref type="formula">5</ref>) for inference:</p><formula xml:id="formula_18">F(q, s, c) -C. (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>The time consumption of our approach is equal to the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We extensively conduct the experiments on the Hot-potQA <ref type="bibr" target="#b28">(Yang et al., 2018)</ref> dataset in the distractor setting. See Appendix A.2 for more experimental results on the other multihop benchmark 2Wiki-MultihopQA <ref type="bibr" target="#b7">(Ho et al., 2020)</ref>.</p><p>Metrics and Dataset: To measure multi-hop QA models, we report two results: 1) original denotes the model's original performance (larger is better) and 2) dire denotes how much the QA model is cheated (smaller is better). Please note original-dire denotes the model's true multi-hop reasoning.</p><p>A probing dataset should be constructed to measure DiRe. To the best of our knowledge, only <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref> provides the probing dataset of the development set of HotpotQA. Thus we use the development set of HotpotQA to evaluate the original and dire. Specifically, the probing dataset for HotpotQA in the distractor setting divides each example of the original dataset into two instances, both of which only contain one of two ground truth supporting paragraphs respectively. If the multihop QA model can arrive at the correct test output on two instances, it means that the model performs disconnected reasoning on the original example. Please refer to <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref> for more details.</p><p>Following the Dire <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>, we report the metrics for HotpotQA: answer span (Ans), supporting paragraphs (Supp p ), supporting sentences (Supp s ), joint metrics (Ans +Supp p , Ans+ Supp s ). We show both EM scores and F1 scores to compare the performance between baselines and our counterfactual multi-hop reasoning method.</p><p>Baselines: First, we simply use the BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> to predict the answer, supporting sentences and supporting paragraphs as the baseline. BERT + ours denotes that we apply our counterfactual multi-hop reasoning method based on BERT as the backbone. The proposed approach is model-agnostic and we also implement it on several multi-hop QA architectures, including DFGN <ref type="bibr" target="#b20">(Qiu et al., 2019)</ref>, HGN <ref type="bibr" target="#b5">(Fang et al., 2019)</ref> and XLNet in Dire <ref type="bibr" target="#b23">(Trivedi et al., 2020;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref>. Our proposed algorithm also can be implemented on other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Results</head><p>For fairness, we conduct the experiments under the same preprocessing of the dataset following HGN <ref type="bibr" target="#b5">(Fang et al., 2019)</ref>, which select top K relevant paragraphs corresponding to each question example. And the experimental results are shown in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>. The main observation can be made as follows:</p><p>Our method can significantly reduce disconnected reasoning. Compared to BERT baseline, our proposed counterfactual multi-hop reasoning method can reduce the disconnected reasoning of answer prediction and supporting facts identification in both the paragraph level and sentence level. In particular, we can see big drops of 9.5 F1 points on Supp s (from 64.1 to 54.6) and 13.6 EM points on Supp p (from 17.3 to 3.7) in disconnected reasoning (dire). Our method is better at reducing disconnected reasoning on the Exact Match (EM) evaluation metric. This is because EM is a stricter evaluation metric. For example, EM requires both of the supporting facts should be predicted correctly, while it has F1 scores even when only one supporting fact is predicted correctly. For dire evaluation where only one supporting fact is provided, our approach punishes this situation and achieves lower scores on EM metric of disconnected reasoning. It demonstrates that our method effectively reduces disconnected reasoning when the supporting facts are insufficient.</p><p>Our method still guarantees comparable performance on the original dev set. As seen from Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, the proposed method also maintains the same accuracy on the original set. It even shows a better performance on the supporting facts prediction in the paragraph level.</p><p>Our method is model-agnostic and it demonstrates effectiveness in several multi-hop QA models.</p><p>Based on our causal-effect insight, our proposed approach can easily be applied to other multi-hop QA architectures including XLNET, DFGN, HGN <ref type="bibr" target="#b23">(Trivedi et al., 2020;</ref><ref type="bibr" target="#b20">Qiu et al., 2019;</ref><ref type="bibr" target="#b5">Fang et al., 2019)</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>, our proposed counterfactual reasoning method achieves better performance. Our method can reduce disconnected reasoning by introducing the proposed counterfactual approach in the training procedure. The dire scores of HGN <ref type="bibr" target="#b5">(Fang et al., 2019)</ref> and XLNET <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref> in Ans, Supp p , Supp s all drop to some extent. Besides, the performances on the original dev set are comparable simultaneously.</p><p>In summary, reducing the disconnected reasoning and guaranteeing the strong performance on the original development set indicate that the most progress of the model is attributed to the multi-top reasoning (K ‚Üí Y ) capability. For intuitiveness, we also show the real multi-hop reasoning promoted by our proposed counterfactual reasoning approach, as shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>As illustrated in Section 4, our goal is to exclude the shortcut impacts (K 1 ‚Üí Y, K 2 ‚Üí Y ) to reduce the disconnected reasoning. Hence, we study the ablation experiments by excluding one of the shortcut impacts. We explore removing K 1 ‚Üí Y or K 2 ‚Üí Y that reduces the disconnected reasoning, as shown in nected reasoning to some extent on supporting facts identification except the answer span prediction. However, relieving the both impacts of K 1 ‚Üí Y and K 2 ‚Üí Y can achieve better performance on decreasing disconnected reasoning. Because the model can always exploit another shortcut if only one of the shortcuts is blocked.</p><p>We further conduct ablation studies to validate the distribution assumption for the counterfactual output of the parameter C. Similar to CF-VQA <ref type="bibr" target="#b17">(Niu et al., 2021)</ref>, we empirically validate the two distribution assumptions, as shown in Table <ref type="table" target="#tab_4">4</ref>. The "random" denotes that C are learned without constraint and it means that C Ans ‚àà R n , C supp ‚àà R 2 , C type ‚àà R 4 respectively, and n represents the length of the context. The "uniform" denotes that C should satisfy uniform distribution and it means that C Ans , C supp and C type are scalar. As shown in Table <ref type="table" target="#tab_4">4</ref>, the random distribution assumption performs better than the uniform distribution assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed a novel counterfactual reasoning approach to reduce disconnected reasoning in multi-hop QA. We used the causal graph to explain the existing multi-hop QA approaches' behaviors, which consist of the shortcut impacts and reasoning impacts. The shortcut impacts capture the disconnected reasoning and they are formulated as the natural direct causal effects. Then we constructed the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification. The reasoning impact represents the multi-hop reasoning and is estimated by introducing learnable parameters.</p><p>During the test phase, we exclude the natural direct effect and utilize the true multi-hop effect to decrease the disconnected reasoning. Experimental results demonstrate that our proposed counterfactual reasoning method can significantly drop the disconnected reasoning on the probing dataset and guarantee the strong performance on the original dataset, which indicates the most progress of the multi-hop QA model is attributed to the true multi-hop reasoning. Besides, our approach is modelagnostic, and can be applied to other multi-hop QA architectures to avoid exploiting the shortcuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our proposed method needs to construct counterfactual examples to estimate the natural direct effect of disconnected reasoning during the training phase, thus we need a little more GPU resources and computational time. However, the need of resource occupancy and time consumption of our approach does not increase during inference. Another limitation is that we use the learnable parameters to approximate the Y k 1 ,k 2 ,k * . In our future work, we will explore a better approach to model it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Implementation Details Specifically, given the question Q = q, and context P = {s 1 , s 2 , ..., s m } where m is the number of paragraphs, we denote the remaining context c i = P -{s i }, 1 ‚â§ i ‚â§ m. To distinguish whether s i is supporting fact and get the answer distribution on s i , we construct the s * i and c * i as illustrated in the subsection 4.1. We respectively encode (q, s i , c i ),(q, s * i , c i ) and (q, s i , c * i ) to get the contextualized representation O ‚àà R n√ód , M i ‚àà n√ód , G i ‚àà R n√ód , where n is the length of the question and context.</p><p>For supporting facts identification, similar to Dire <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>, we use the start token s start i of the paragraph as its representation and obtain their predicted logits under factual and counterfactual scenario:</p><formula xml:id="formula_20">Y q,s i ,c i (s i ) = g(O[s start i ]) Y q,s * i ,c i (s i ) = g(M i [s start i ]) Y q,s i ,c * i (s i ) = g(G i [s start i ]), (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where g is a classifier instantiated as M LP layer in practice. And s start i and s end i are denoted as the start and end position of the paragraph s i respectively. In sentence level, we use the their start positions in paragraph s i and operate in the same way.</p><p>As for answer span prediction, we concatenate the representation of s i in M i or G i (1 ‚â§ i ‚â§ m)  We also apply our counterfactual reasoning method to identify the answer type, consisting of yes, no, span and entity: We use the [CLS] token as the global representation to predict the answer type, following previous work <ref type="bibr" target="#b20">(Qiu et al., 2019;</ref><ref type="bibr" target="#b5">Fang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Extensive experiments</head><p>In this section, we extensively evaluate our method on 2WikiMultihopQA dataset <ref type="bibr" target="#b7">(Ho et al., 2020)</ref>. And following Dire <ref type="bibr" target="#b23">(Trivedi et al., 2020)</ref>, we construct the probing dataset of the corresonding development set to evaluate the metric of dire. The experimental results are shown in Table <ref type="table" target="#tab_5">5</ref>.</p><p>Our proposed counterfactual reasoning approach method can be generalized to other multihopQA benchmarks. Our approach can still significantly reduce the disconnected reasoning, and guanrantee the comparable preformance on the original dev set. Besides, our method is model-agnostic and it demonstrates effectiveness in several multi-hop QA models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>as Director of Research for a 2008 presidential candidate, who served until when in the U.S. Senate ? Barack Obama previously served in the U.S. Senate representing Illinois from 2005 to 2008‚Ä¶the first African American to have served as president‚Ä¶ Adler‚Ä¶served as Director of Research for the Barack Obama campaign for the 2008 Democratic presidential nomination. distractors facts‚Ä¶ Answer: 2008 Supporting facts: Obama previously served in the U.S. School representing Illinois from 2005 to 2008‚Ä¶ the first African American to have served as president‚Ä¶ (, ùëê * : ùëû, ùë† * , ùëê:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of disconnected reasoning in multi-hop QA, where red node denotes question Q, blue node is a supporting fact S, and orange and green nodes denote the remaining facts C. Deep gray nodes mean their variables are reference values instead of the given values, (e.g., S = s * instead of S = s). (a): Causal graph of multi-hop QA model; (b): is a possible scenario of disconnected reasoning, which uses only one fact s to answer the question. (c): is another possibility of disconnected reasoning, e.g., the exclusive method to find whether s is a supporting fact by a process of elimination of other facts c. (d): is the true multi-hop reasoning. All facts s and c are taken into considered to produce the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Y q,s,c (start) = g(O) Y q,s * ,c (start) = g( M ) Y q,s,c * (start) = g( ·∏†),(14) where [; ] denotes the operation of concatenation. And we can predict the end position of the answer in the same way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Y</head><label></label><figDesc>q,s,c (type) = g(O[0]) Y q,s * ,c (type) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>F1 scores. The "original" denotes the model's performance on the development set of HotpotQA in the distractor setting, and the "dire" indicates that the model scores on the corresponding probing set, which measures how much disconnected reasoning the model can achieve. The smaller score of "dire" is better. We can see that the proposed method can reduce disconnected reasoning while maintaining the same accuracy on the original dataset.</figDesc><table><row><cell></cell><cell cols="10">Ans original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì Supp p Supp s Ans + Supp p Ans + Supp s</cell></row><row><cell>BERT</cell><cell>74.2</cell><cell>47.5</cell><cell>94.1</cell><cell>73.9</cell><cell>83.8</cell><cell>64.1</cell><cell>71.0</cell><cell>36.5</cell><cell>64.2</cell><cell>32.5</cell></row><row><cell>+ours</cell><cell>74.0</cell><cell>45.5</cell><cell>95.4</cell><cell>67.5</cell><cell>82.8</cell><cell>54.6</cell><cell>71.6</cell><cell>31.8</cell><cell>63.9</cell><cell>26.7</cell></row><row><cell>XLNET</cell><cell>76.2</cell><cell>50.3</cell><cell>96.5</cell><cell>75.0</cell><cell>86.6</cell><cell>64.8</cell><cell>74.4</cell><cell>39.1</cell><cell>68.0</cell><cell>34.6</cell></row><row><cell>+ours</cell><cell>75.9</cell><cell>49.8</cell><cell>96.6</cell><cell>74.1</cell><cell>86.6</cell><cell>63.6</cell><cell>74.1</cell><cell>38.0</cell><cell>67.9</cell><cell>33.6</cell></row><row><cell>DFGN</cell><cell>71.7</cell><cell>44.5</cell><cell>94.4</cell><cell>73.8</cell><cell>83.8</cell><cell>64.0</cell><cell>68.7</cell><cell>34.2</cell><cell>62.1</cell><cell>30.4</cell></row><row><cell>+ours</cell><cell>73.3</cell><cell>48.3</cell><cell>96.1</cell><cell>72.1</cell><cell>85.4</cell><cell>61.6</cell><cell>71.5</cell><cell>36.0</cell><cell>65.1</cell><cell>31.7</cell></row><row><cell>HGN</cell><cell>73.3</cell><cell>47.0</cell><cell>91.1</cell><cell>67.4</cell><cell>81.4</cell><cell>59.0</cell><cell>68.3</cell><cell>33.6</cell><cell>62.0</cell><cell>30.2</cell></row><row><cell>+ours</cell><cell>70.9</cell><cell>41.1</cell><cell>93.4</cell><cell>67.6</cell><cell>83.5</cell><cell>58.1</cell><cell>67.6</cell><cell>28.9</cell><cell>61.8</cell><cell>25.5</cell></row><row><cell></cell><cell cols="10">Ans original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì Supp p Supp s Ans + Supp p Ans + Supp s</cell></row><row><cell>BERT</cell><cell>59.7</cell><cell>35.3</cell><cell>86.1</cell><cell>17.3</cell><cell>54.8</cell><cell>10.1</cell><cell>53.6</cell><cell>7.0</cell><cell>35.7</cell><cell>4.4</cell></row><row><cell>+ours</cell><cell>60.2</cell><cell>33.7</cell><cell>87.9</cell><cell>3.7</cell><cell>53.5</cell><cell>2.0</cell><cell>55.2</cell><cell>1.4</cell><cell>36.6</cell><cell>0.8</cell></row><row><cell>XLNET</cell><cell>62.0</cell><cell>38.0</cell><cell>91.8</cell><cell>14.5</cell><cell>58.9</cell><cell>8.5</cell><cell>58.6</cell><cell>6.5</cell><cell>40.1</cell><cell>4.2</cell></row><row><cell>+ours</cell><cell>61.7</cell><cell>37.6</cell><cell>92.1</cell><cell>6.9</cell><cell>59.3</cell><cell>3.9</cell><cell>58.5</cell><cell>2.6</cell><cell>40.9</cell><cell>1.6</cell></row><row><cell>DFGN</cell><cell>57.4</cell><cell>44.5</cell><cell>85.9</cell><cell>19.5</cell><cell>53.4</cell><cell>11.7</cell><cell>51.3</cell><cell>6.9</cell><cell>33.6</cell><cell>4.6</cell></row><row><cell>+ours</cell><cell>59.6</cell><cell>35.9</cell><cell>90.9</cell><cell>6.2</cell><cell>57.7</cell><cell>3.3</cell><cell>55.9</cell><cell>2.3</cell><cell>38.4</cell><cell>1.4</cell></row><row><cell>HGN</cell><cell>58.9</cell><cell>35.2</cell><cell>79.5</cell><cell>16.9</cell><cell>52.3</cell><cell>10.4</cell><cell>49.5</cell><cell>6.8</cell><cell>34.2</cell><cell>4.4</cell></row><row><cell>+ours</cell><cell>57.1</cell><cell>30.1</cell><cell>85.1</cell><cell>6.4</cell><cell>56.1</cell><cell>3.3</cell><cell>51.3</cell><cell>2.2</cell><cell>36.0</cell><cell>1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>EM scores on the development set and probing set of HotpotQA in the distractor setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>We can see that excluding one of them can decrease the amount of discon-</figDesc><table><row><cell></cell><cell cols="10">Ans original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì Supp p Supp s Ans + Supp p Ans + Supp s</cell></row><row><cell>BERT</cell><cell>74.2</cell><cell>47.5</cell><cell>94.1</cell><cell>73.9</cell><cell>83.8</cell><cell>64.1</cell><cell>71.0</cell><cell>36.5</cell><cell>64.2</cell><cell>32.5</cell></row><row><cell>+K 1 ‚Üí Y +K 2 ‚Üí Y +ours(full)</cell><cell>74.2 74.6 74.0</cell><cell>50.2 48.8 45.5</cell><cell>96.6 96.4 95.4</cell><cell>73.5 68.2 67.5</cell><cell>85.7 85.6 82.8</cell><cell>61.9 57.7 54.6</cell><cell>72.5 72.8 71.6</cell><cell>37.8 34.5 31.8</cell><cell>65.7 66.0 63.9</cell><cell>32.8 30.1 26.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F1 scores of ablation study on the development set and probing set of HotpotQA in the Distractor setting. The "+ K 1 ‚Üí Y " denotes that we only utilize the counterfactual examples (q, s, c * ) to estimate the shortcut impact of K 1 ‚Üí Y , and similarly the "+ K 2 ‚Üí Y " represents that only the counterfactual examples (q, s * , c) are used to estimate the shortcut impact of K 2 ‚Üí Y .</figDesc><table><row><cell></cell><cell cols="10">Ans original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì original dire‚Üì Supp p Supp s Ans + Supp p Ans + Supp s</cell></row><row><cell>random uniform</cell><cell>74.0 73.8</cell><cell>45.5 45.9</cell><cell>95.4 92.1</cell><cell>67.5 67.4</cell><cell>82.8 76.6</cell><cell>54.6 46.3</cell><cell>71.6 69.2</cell><cell>31.8 31.8</cell><cell>63.9 59.6</cell><cell>26.7 23.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>F1 scores of ablation study of assumptions for counterfactual outputs C on the development set and probing set of HotpotQA in the distractor setting.</figDesc><table><row><cell>35 40</cell><cell>BERT DFGN HGN BERT+ours</cell><cell></cell><cell></cell><cell>34.5 34.5 34.7 39.8</cell><cell>37.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31.7 31.7 31.8</cell></row><row><cell>30</cell><cell>26.7 27.2 26.3 28.5</cell><cell>27.9</cell><cell>28.2</cell><cell></cell><cell></cell></row><row><cell>25</cell><cell></cell><cell>23.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>22.4</cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell>20.2 20.6</cell><cell>19.7 19.8</cell><cell></cell><cell></cell></row><row><cell>15</cell><cell>Ans</cell><cell>Suppp</cell><cell>Supps</cell><cell>Ans + Suppp</cell><cell>Ans + Supps</cell></row><row><cell cols="6">Figure 2: F1 scores of real multi-hop reasoning, which is denoted as the original scores minus the dire scores. We compare BERT, DFGN, HGN, and our method except XLNET, as they utilize the same pre-trained language model (i.e. bert-base-uncased).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>F1 scores on the development set of 2WikiMultihopQA dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">U1911203</rs>, <rs type="grantNumber">U2001211</rs>, <rs type="grantNumber">U22B2060</rs>), <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2019B1515130001</rs>, <rs type="grantNumber">2021A1515012172</rs>), <rs type="funder">Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101100001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NAyQ6rU">
					<idno type="grant-number">U1911203</idno>
				</org>
				<org type="funding" xml:id="_aByJBqy">
					<idno type="grant-number">U2001211</idno>
				</org>
				<org type="funding" xml:id="_fHBCYJw">
					<idno type="grant-number">U22B2060</idno>
				</org>
				<org type="funding" xml:id="_utPrYPh">
					<idno type="grant-number">2019B1515130001</idno>
				</org>
				<org type="funding" xml:id="_E4j5SKM">
					<idno type="grant-number">2021A1515012172</idno>
				</org>
				<org type="funding" xml:id="_hPXYfdM">
					<idno type="grant-number">2020B0101100001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Left blank.</p><p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Left blank.</p><p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Left blank.</p><p>B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Left blank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Did you run computational experiments?</head><p>Left blank.</p><p>C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Left blank.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10470</idno>
		<title level="m">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02610</idno>
		<title level="m">Multi-hop question answering via reasoning chains</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-step entity-centric information retrieval for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Kavarthapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05460</idno>
		<title level="m">Cognitive graph for multihop reading comprehension at scale</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03631</idno>
		<title level="m">Hierarchical graph network for multi-hop question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06753</idno>
		<title level="m">A simple yet strong pipeline for hotpotqa</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Xanh</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh-Khoa</forename><surname>Duong Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01060</idno>
		<title level="m">Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Eun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dohyeon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03242</idno>
		<title level="m">Robustifying multi-hop qa through pseudo-evidentiality training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hopretriever: Retrieve hops over wikipedia to answer complex questions</title>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13279" to="13287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02900</idno>
		<title level="m">Compositional questions do not necessitate multi-hop reasoning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02916</idno>
		<title level="m">Multi-hop reading comprehension through question decomposition and rescoring</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08041</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction</title>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08511</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A cause-effect look at language bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Direct and indirect effects</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Causal Inference: The Works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="373" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie ; Peng Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07000</idno>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The book of why: the new science of cause and effect. Basic books</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6140" to="6150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for finegrained visual categorization and re-identification</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1025" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03096</idno>
		<title level="m">Is graph structure necessary for multi-hop question answering? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00789</idno>
		<title level="m">Is multihop qa in dire condition? measuring and reducing disconnected reasoning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal attention for unbiased visual recognition</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3091" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12756</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal attention for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9847" to="9857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<idno>arXiv:2205.09226</idno>
		<title level="m">Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, and Caiming Xiong. 2022. Modeling multi-hop question answering as single sequence prediction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hotpotqa: A dataset for diverse, explainable multi-hop question answering</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04515</idno>
		<title level="m">Connecting attributions and qa model behavior on realistic counterfactuals</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Xian-Sheng Hua, and Hanwang Zhang. 2021. Cross-domain empirical risk minimization for unbiased long-tailed classification</title>
		<author>
			<persName><forename type="first">Beier</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14380</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values</title>
		<imprint/>
	</monogr>
	<note>Left blank</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean</title>
	</analytic>
	<monogr>
		<title level="m">Did you report descriptive statistics about your results</title>
		<imprint/>
	</monogr>
	<note>Left blank</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m">did you report the implementation, model, and parameter settings used</title>
		<meeting><address><addrLine>NLTK, Spacy, ROUGE</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>If you used existing packages. Left blank</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">crowdworkers) or research with human participants</title>
	</analytic>
	<monogr>
		<title level="m">D Did you use human annotators</title>
		<imprint/>
	</monogr>
	<note>Left blank</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators</title>
		<author>
			<persName><surname>D1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Left blank</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants&apos; demographic</title>
		<author>
			<persName><surname>D2</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Left blank</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Did you discuss whether and how consent was obtained from people whose data you&apos;re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Left blank. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?</title>
		<imprint/>
	</monogr>
	<note>Left blank. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Left blank</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
