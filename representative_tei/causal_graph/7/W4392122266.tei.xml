<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models</title>
				<funder ref="#_QbRRQvk">
					<orgName type="full">MIT Simons MMLS Foundation</orgName>
				</funder>
				<funder ref="#_cFKazdN">
					<orgName type="full">NCCIH/NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">AstraZeneca</orgName>
				</funder>
				<funder ref="#_qytxk8F">
					<orgName type="full">Eric and Wendy Schmidt Center at the Broad Institute of MIT</orgName>
				</funder>
				<funder ref="#_AkQ4NCa">
					<orgName type="full">Eric and Wendy Schmidt Center at the Broad Institute</orgName>
				</funder>
				<funder ref="#_6qaXuKE">
					<orgName type="full">Office of Advanced Scientific Computing Research</orgName>
				</funder>
				<funder ref="#_58WNyhh">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_vFw8Gx3">
					<orgName type="full">United States Department of Energy (DOE)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-22">22 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">√Ålvaro</forename><surname>Ribot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep3">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep4">Centre de Formaci√≥ Interdisciplin√†ria Superior (CFIS)</orgName>
								<orgName type="laboratory" key="lab1">Laboratory for Information and Decision Systems</orgName>
								<orgName type="laboratory" key="lab2">Laboratory for Information and Decision Systems</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Universitat Polit√®cnica de Catalunya (UPC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
							<email>csquires@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep3">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep4">Centre de Formaci√≥ Interdisciplin√†ria Superior (CFIS)</orgName>
								<orgName type="laboratory" key="lab1">Laboratory for Information and Decision Systems</orgName>
								<orgName type="laboratory" key="lab2">Laboratory for Information and Decision Systems</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Universitat Polit√®cnica de Catalunya (UPC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
							<email>cuhler@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Engineering and Applied Sciences</orgName>
								<orgName type="department" key="dep2">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep3">MIT, and Broad Institute of MIT and Harvard</orgName>
								<orgName type="department" key="dep4">Centre de Formaci√≥ Interdisciplin√†ria Superior (CFIS)</orgName>
								<orgName type="laboratory" key="lab1">Laboratory for Information and Decision Systems</orgName>
								<orgName type="laboratory" key="lab2">Laboratory for Information and Decision Systems</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Universitat Polit√®cnica de Catalunya (UPC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent Factor Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-22">22 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.14777v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal imputation</term>
					<term>latent factor models</term>
					<term>synthetic interventions</term>
					<term>matrix completion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of causal imputation, where we aim to predict the outcomes of some set of actions across a wide range of possible contexts. As a running example, we consider predicting how different drugs affect cells from different cell types. We study the index-only setting, where the actions and contexts are categorical variables with a finite number of possible values. Even in this simple setting, a practical challenge arises, since often only a small subset of possible action-context pairs have been studied. Thus, models must extrapolate to novel action-context pairs, which can be framed as a form of matrix completion with rows indexed by actions, columns indexed by contexts, and matrix entries corresponding to outcomes. We introduce a novel SCM-based model class, where the outcome is expressed as a counterfactual, actions are expressed as interventions on an instrumental variable, and contexts are defined based on the initial state of the system. We show that, under a linearity assumption, this setup induces a latent factor model over the matrix of outcomes, with an additional fixed effect term. To perform causal prediction based on this model class, we introduce simple extension to the Synthetic Interventions estimator <ref type="bibr" target="#b2">(Agarwal et al., 2020)</ref>. We evaluate several matrix completion approaches on the PRISM drug repurposing dataset, showing that our method outperforms all other considered matrix completion approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A core goal in scientific modeling -often left implicit -is to construct models that accurately predict a system's behavior across a wide range of conditions. More precisely, we consider the following general causal prediction problem:</p><p>We know some context C, i.e., some partial information about the system's state. We are considering whether to perform an action A that will affect the system. We wish to predict some outcome Y , i.e. some feature(s) of the system after performing A.</p><p>In general, causal prediction may involve high-dimensional contexts, actions, and/or outcomes, such as images or text <ref type="bibr" target="#b11">(Chalupka et al., 2015;</ref><ref type="bibr" target="#b10">Castro et al., 2020;</ref><ref type="bibr" target="#b18">Feder et al., 2022)</ref>. In this work, we study causal prediction in the index-only setting 1 , where actions take values in [m] := {1, . . . , m}</p><formula xml:id="formula_0">Y = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ Y 11 Y 12 ? ‚Ä¢ ‚Ä¢ ‚Ä¢ Y 1n ? ? Y 23 ‚Ä¢ ‚Ä¢ ‚Ä¢ Y 2n Y 31 ? Y 33 ‚Ä¢ ‚Ä¢ ‚Ä¢ ? . . . . . . . . . . . . . . . Y m1 ? ? ‚Ä¢ ‚Ä¢ ‚Ä¢ ? Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ contexts j ‚àà [n] Ô£º Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£Ω Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£æ actions i ‚àà [m]</formula><p>Figure <ref type="figure">1</ref>: The causal matrix completion problem. Each row of Y corresponds to an action, and each column corresponds to a context. Y ij denotes the outcome after performing action i in context j. We represent missing entries with "?".</p><p>and contexts take values in [n] := {1, . . . , n}, with the values i ‚àà [m] and j ‚àà [n] carrying no semantic meaning. In particular, the only information available about an action/context is an index which distinguishes it from other actions/contexts; there is no prior notion of similarity between actions/contexts. To emphasize our setting, we will write I A instead of A and I C instead of C. By restricting our focus to the index-only setting, we aim to maximize the clarity of this work, while building a solid foundation for future works.</p><p>As a running example of causal prediction, we consider the problem of viability prediction in biology, an essential step in tasks such as drug repurposing <ref type="bibr" target="#b38">(Wu et al., 2022)</ref>. In this problem, we are given a cell type j ‚àà [n] (e.g., skin or lung) and a drug label i ‚àà [m] (e.g. tyloxapol or gepefrine), and we aim to predict what proportion Y of cells will survive if we administer drug i to a large group of type-j cells. We note that existing databases <ref type="bibr">(Southan et al., 2013)</ref> offer extensive additional information about drugs, which can be used when extending beyond the index-only setting considered here. Thus, the prediction accuracy in our setting should be seen as a lower bound on the accuracy that could be attained by leveraging this additional information.</p><p>To generate causal predictions, we need some model specifying how an action I A interacts with the context I C to produce the outcome Y . In statistics and machine learning, a model is obtained via data-driven approaches, which consider a fixed model class Œò and use data to select a model Œ∏ from Œò (called learning or model fitting). In this work, we consider supervised learning, where the available data consists of samples of (I A , I C , Y ). For large m and n, the available data often contains samples from only a small subset ‚Ñ¶ of all possible m ‚Ä¢ n possible action-context pairs. For example, given n ‚âà 100 cell types and m ‚âà 10, 000 drugs, we have m ‚Ä¢ n ‚âà 1 million, but our dataset might cover only 5-10% of these pairs. By arranging the available data into a matrix with rows indexed by actions and columns indexed by contexts, we obtain a partially-observed matrix Y, as in Fig. <ref type="figure">1</ref>. In these situations, causal prediction requires extrapolating from ‚Ñ¶ to the entire space [m] √ó [n], a problem known as causal imputation <ref type="bibr">(Squires et al., 2022)</ref>.</p><p>For extrapolation to be feasible, the model class Œò must encode some inductive biases governing the interplay between actions, contexts, and outcomes. The model class must also be reasonably well-specified, i.e., there must exist some Œ∏ * ‚àà Œò which accurately describes the relationship between Y , I A , and I C , at least to a good approximation. Given the importance of the model class Œò, this LFMs assume that Y ij = ‚ü®u i , v i ‚ü© + Œµ ij for u i , v i ‚àà R r . This generative process can be viewed as an SCM over 3 observed (shaded) and 2 latent (unshaded) variables. Here, I A and I C represent action and context indices, respectively (note that they are independent).</p><formula xml:id="formula_1">I A = i u i Y ij v j I C = j</formula><p>work aims to advance our understanding of the relationship between two model classes that are commonly used for causal prediction. In particular, we study the class of latent factors models (LFMs) and the class of structural causal models (SCMs). LFMs, also known as interactive fixed effects models, are widely used in econometrics and recommendation systems <ref type="bibr" target="#b4">(Athey et al., 2021;</ref><ref type="bibr" target="#b22">Koren et al., 2009)</ref>. In an LFM, each action i ‚àà [m] is associated with an unknown vector u i ‚àà R r , and each context j ‚àà [n] is associated with an unknown vector v j ‚àà R r . LFMs assume Y ij = ‚ü®u i , v j ‚ü© + Œµ ij for some mean-zero Œµ ij , and can be seen as a simple form of SCM, see Fig. <ref type="figure" target="#fig_0">2</ref>. We can also extend these models to include terms for a fixed action effect and/or a fixed context effect by letting</p><formula xml:id="formula_2">Y ij = ‚ü®v i , u j ‚ü© + Œ± i + Œ≤ j + Œµ ij for some Œ± i , Œ≤ j ‚àà R. LFMs are related to low-rank factorizations: letting L ‚àà R m√ón with L ij = E[Y ij ],</formula><p>the definition of the LFM (with no fixed effects) implies that rank(L) ‚â§ r.</p><p>LFMs are often assumed as a model class without any "deeper" justification. One intuitive way to motivate LFMs is to show that they arise from other (potentially more transparent) modeling assumptions, as in <ref type="bibr" target="#b37">Udell and Townsend (2017)</ref>. Relating LFMs to other model classes is important for several reasons: such connections offer insights that could legitimize trust in the model's prediction, and can serve as a starting point from which to develop more general model classes. Thus, a primary aim of this work is to offer a new justification for LFMs, starting from the assumption that the system's state obeys a structural causal model (SCM), with Y ij defined as a counterfactual outcome under action i when the system is in context j. That is, in this paper, we work with quantities that are defined in terms of a counterfactual distribution, and which cannot be defined only using interventional distributions. Such quantities are commonly seen, for example, in the literature on mediation analysis <ref type="bibr" target="#b24">(Malinsky et al., 2019)</ref>.</p><p>Finally, note that there is a substantial difference between the roles that columns (contexts) and rows (actions) are playing. Inspired by this distinction, let us call a matrix completion approach symmetric when, performed on Y ‚ä§ , we obtain the same predictions. We want to use a non-symmetric approach that is compatible with our causal viewpoint.</p><p>Organization of the paper and contributions. After an overview of related work in Section 2, we review SCMs and formally define our model class in Section 3. We specialize to linear models in Section 4, where we establish the following results:</p><p>‚Ä¢ We generalize the model class from <ref type="bibr">Squires et al. (2022)</ref> to allow for the context I C to be defined in terms of the system's state, which was not previously allowed. In Theorem 1, we show that our new SCM-based model class implies an LFM with fixed action effects. ‚Ä¢ We propose a slight modification to the Synthetic Interventions estimator of <ref type="bibr" target="#b3">Agarwal et al. (2021)</ref> that accounts for fixed action effects.</p><p>‚Ä¢ We give additional conditions on contexts and actions, which imply further structure on L; see Prop. 2 and Corollary 3. We show how this additional structure can be leveraged to improve estimation and hypothesis testing. Finally, in Section 5, we compare the performance of several causal prediction approaches on the PRISM drug repurposing dataset, showing that our method outperforms alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since this paper bridges between latent factor models and structural causal models, the potential scope for related work is vast. We limit our discussion to works that consider the index-only setting. These works are complementary to recent works which give extrapolation guarantees for more general/structured action spaces, such as <ref type="bibr" target="#b31">Saengkyongam et al. (2023)</ref> and <ref type="bibr" target="#b1">Agarwal et al. (2023)</ref>. A summary of the algorithms presented can be found in Table <ref type="table" target="#tab_0">1</ref>. We provide the formulae for these algorithms in Appendix A. Finally, recall that L ‚àà R m√ón with L ij = E[Y ij ] and that ‚Ñ¶ denotes the set of observed action-context pairs.</p><p>Local Approaches to Matrix Completion. We call a causal prediction approach local if it predicts individual entries L ij , rather than predicting the entire matrix L in one go. Many local approaches predict L ij via some weighted average (k,‚Ñì)‚àà‚Ñ¶ w k‚Ñì Y k‚Ñì , with different choices for w k‚Ñì giving rise to different methods. For example, in Collaborative Filtering (CF) <ref type="bibr" target="#b32">(Schafer et al., 2007;</ref><ref type="bibr" target="#b23">Linden et al., 2003)</ref> and nearest-neighbors approaches <ref type="bibr" target="#b15">(Dwivedi et al., 2022)</ref>, the weight w k‚Ñì depends on some measure of data-dependent similarity (e.g. cosine similarity) between actions i, k and contexts j, ‚Ñì.</p><p>Very simple examples of local, weighting-based estimators include the mean-over-contexts estimator (which has w i ‚Ä≤ j ‚Ä≤ ‚àù 1 i ‚Ä≤ =i with the weights summing to one), and the similarly-defined mean-over-actions estimator. The estimators are appropriate for models with only fixed action effects and fixed context effects, respectively. To handle a two-way fixed effect model (one with both fixed action and context effects), the mean-over-actions and mean-over-contexts estimators can be combined into the Fixed Effects (FE) estimator. More precisely, this estimator corresponds to taking an average of the fixed action effect estimators presented in <ref type="bibr">Squires et al. (2022)</ref>.</p><p>Connections between causal prediction and matrix completion are explored in a number of works. In particular, the widely-used Synthetic Controls method predicts outcomes for treated units under the counterfactual setting where they received no treatment <ref type="bibr" target="#b0">(Abadie and Gardeazabal, 2003;</ref><ref type="bibr" target="#b14">Doudchenko and Imbens, 2016;</ref><ref type="bibr" target="#b5">Bai and Ng, 2019)</ref>. <ref type="bibr" target="#b2">Agarwal et al. (2020)</ref> generalized this idea to predict counterfactuals under treatment with the Synthetic Interventions (SI) estimator, which has also been connected to causal matrix/tensor completion <ref type="bibr" target="#b3">(Agarwal et al., 2021;</ref><ref type="bibr">Squires et al., 2022)</ref>.</p><p>Global Approaches to Matrix Completion. Other approaches, such as nuclear norm minimization (NNM) <ref type="bibr">(Candes and Plan, 2009)</ref>, predict the entire matrix L in one go, often by casting matrix completion as an optimization problem and developing fast optimizers <ref type="bibr" target="#b6">(Cai et al., 2008;</ref><ref type="bibr" target="#b8">Candes and Recht, 2008;</ref><ref type="bibr" target="#b25">Mazumder et al., 2010)</ref>. In fact, there are also global approaches that take advantage of latent factor models <ref type="bibr" target="#b19">(Hastie et al., 2014;</ref><ref type="bibr" target="#b21">Jain et al., 2012)</ref>. There is a vast literature studying the optimality properties of NNM and its variations <ref type="bibr" target="#b9">(Candes and Tao, 2009;</ref><ref type="bibr" target="#b29">Recht et al., 2010;</ref><ref type="bibr" target="#b39">Zhang and Aeron, 2015)</ref>. However, these results rely on the assumption of uniformly at random observations, which is usually not satisfied in real datasets, especially for biological applications (see Section 5). Thus, we might prefer to use local approaches when the missingness pattern is far from uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Model *SI is symmetric for matrices but it is non-symmetric for third-order tensors.</p><p>Among global approaches, the one most related to the present work is an extension of NNM by <ref type="bibr" target="#b4">Athey et al. (2021)</ref>. This approach excludes fixed effects from regularization, hence we call it NNM-FE. Roughly speaking, if we let ≈∂FE denote the fixed effect estimator for Y, NNM-FE is similar to using NNM on the matrix Y -≈∂FE . In Section 4, we use the same idea to improve the SI estimator when the model class includes fixed effects.</p><p>Causal Prediction in Biological Applications. We use the DepMap PRISM dataset <ref type="bibr" target="#b12">(Corsello et al., 2020)</ref>, which measures the viability score of drug and cell line combinations. <ref type="bibr" target="#b28">Radhakrishnan et al. (2022)</ref> used additional information from the Connectivity Map (CMAP) dataset <ref type="bibr" target="#b12">(Corsello et al., 2020)</ref> to predict the viability scores from DepMap, going beyond the index-only setting considered here. In the index-only setting, <ref type="bibr">Squires et al. (2022)</ref> used Synthetic Interventions for the CMAP dataset while <ref type="bibr" target="#b20">(Hodos et al., 2018)</ref> used a nearest-neighbor approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background and Setup</head><p>We begin by reviewing relevant background on structural causal models <ref type="bibr" target="#b27">(Peters et al., 2017)</ref>, and then use the concepts to define the model class that we consider for causal prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Definition 1 (Structural Causal Model (SCM)) A structural causal model (SCM) is defined by a tuple S = (S, P E ), where S = (S 1 , . . . , S q ) is an indexed set of q causal mechanisms</p><formula xml:id="formula_3">S k : Z k = f k (pa(Z k ), Œµ k ), k = 1, . . . , q.</formula><p>Here, pa(Z k ) ‚äÜ {Z 1 , . . . , Z q } \ {Z k } are called the parents of Z k , and P E = P Œµ 1 ,...,Œµq is the joint distribution of the exogenous noise variables Œµ 1 , . . . , Œµ q . The causal graph G of an SCM has a directed edge Z l ‚Üí Z k for all k and for all Z l ‚àà pa(Z k ). We assume that G is acyclic, i.e., it is a DAG.</p><p>Unless otherwise noted, we assume that P E is a product distribution, i.e., that the noise variables Œµ 1 , . . . , Œµ q are jointly independent. An SCM is called</p><formula xml:id="formula_4">Gaussian if Œµ k ‚àº N (0, œÉ 2 k ) with œÉ k &gt; 0 ‚àÄk. It is called linear if all causal mechanisms f k are linear.</formula><p>In particular, in a linear SCM, the causal mechanisms are defined by some parameters B l,k ‚àà R and they can be written as</p><formula xml:id="formula_5">Z k = Z l ‚ààpa(Z k ) B l,k Z l + Œµ k for all k = 1, . . . , q<label>(1)</label></formula><p>We now define interventions and counterfactuals, mostly following the notation of <ref type="bibr" target="#b27">Peters et al. (2017)</ref>. Fix an SCM S = (S, P E ) over nodes Z. A (soft) intervention<ref type="foot" target="#foot_0">foot_0</ref> I is defined by a set T (I) ‚äÜ Z of intervention targets, and an indexed set {f I k } k‚ààT (I) of interventional causal mechanisms, where f k is generally allowed to be a function of pa(Z k ) and Œµ k . Given an intervention I, we define the interventional SCM as S I = (S I , P E ), where</p><formula xml:id="formula_6">(S I ) k = S k if k ‚àà T (I) and (S I ) k = S k otherwise.</formula><p>On the other hand, letting C ‚äÜ Z and conditioning on C = c, we define the counterfactual SCM as S C=c = (S, P E|C=c ). To combine counterfactuals and interventions (in that order), we define S [C=c,I] = (S I , P E|C=c ). This SCM entails a new joint distribution over Z, which we denote by</p><formula xml:id="formula_7">P(Z | [C = c, I]), or in the special case of a do-intervention setting A ‚äÇ Z to a, P(Z | [C = c, do(A = a)]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">An SCM-based Model Class for Causal Prediction</head><p>We give general results for the setting where we observe a vector-valued outcome Y ij ‚àà R p for each action-context pair (i, j). This gives rise to a partially observed 3-order tensor Y ‚àà R m√ón√óp where the rows (first index) correspond to actions and the columns (second index) correspond to contexts. Let ‚Ñ¶ denote the set of indices (i, j) for which we have observed Y ij ‚àà R p . Our ultimate goal is to impute the missing entries of E[Y]. In the special case p = 1, Y reduces to an m √ó n matrix, and tensor completion reduces to matrix completion.</p><p>We assume that there is an underlying SCM over some Z = (Z 1 , . . . , Z q ) that defines our system (each possible context). Let I A and I C be index variables that define the action and context, respectively. We add them to our SCM as follows. First, we observe I C before applying any action. As I C defines a context, it is defined as a function of Z, so it is downstream from every node. Then, conditioning on I C = j corresponds to conditioning on some Z C j where C j ‚äÇ [q] := {1, . . . , q}.</p><p>On the other hand, the index I A can be thought of as an instrumental variable (Newey and Powell, 2003) or a regime indicator <ref type="bibr" target="#b13">(Dawid, 2021)</ref>, which encodes the (unknown) intervention that each action applies. In particular, using action i corresponds to setting I A = i, which induces an intervention on some set of variables</p><formula xml:id="formula_8">Z A i , A i ‚äÇ [q]</formula><p>. Together, the intervention on I A and conditioning on I C gives rise a new SCM, and defines a new set of variables Z(i) that represent the counterfactual state of the system.<ref type="foot" target="#foot_1">foot_1</ref> Finally, we only observe a subset of Z(i), which we denote with some indices X ‚äÇ [q], |X | = p, and the outcome Y ij is drawn from the distribution The context may potentially depend on any subset of Z and the action may potentially affect any subset of Z. The exogenous noise terms Œµ 1 , Œµ 2 , . . . , Œµ q are shared between the pre-interventional and post-interventional SCMs.</p><formula xml:id="formula_9">ùëç 1 ùëç 2 ùëç 3 ùëç ùëû ùëç 4 ‚ãÆ ùúÄ 1 ùúÄ 2 ùúÄ 3 ùúÄ 4 ùúÄ ùëû ùëç 1 (ùëñ) ùëç 2 (ùëñ) ùëç 3 (ùëñ) ùëç ùëû (ùëñ) ùëç 4 (ùëñ) ‚ãÆ ‚ãÆ ùêº ùê∂ = ùëó ùêº ùê¥ = ùëñ Pre-intervention Post-intervention Context Action</formula><p>Here, the observed outcome is</p><formula xml:id="formula_10">Y ij ‚àº P(Z X (i) | I C = j) for X = {2, 4}. P(Z X (i) | I C = j).</formula><p>This definition of our model class is summarized in Fig. <ref type="figure" target="#fig_1">3</ref>. Altogether, the expected value of the (i, j)-th entry of L is</p><formula xml:id="formula_11">E(Y ij ) = E (Z X (i) | I C = j)<label>(2)</label></formula><p>We clarify our setup with a simple example which uses only do-interventions.</p><p>Example 1 Let Z 1 ‚Üí Z 2 ‚Üí Z 3 follow an SCM S = (S, P E ). Assume that context is defined solely based on the value of the marker gene Z 3 , e.g.,</p><formula xml:id="formula_12">I C = 1 if and only if Z 3 = c 1 and I C = 2 if and only if Z 3 = c 2 .</formula><p>This gives us a new distribution on the noise, P E|Z 3 =c j for each possible c j . Assume also that each action corresponds to a do-intervention on the gene Z 1 , e.g. if I A = 1, then we have performed the intervention do(Z 1 = a 1 ), if I A = 2, then we have performed the intervention do(Z 1 = a 2 ). So for I A = i we have a new set of structural equations S do(Z 1 =a i ) . Therefore, the SCM after conditioning and intervening is</p><formula xml:id="formula_13">S [Z 3 =c j ,do(Z 1 =a i )] = (S do(Z 1 =a i ) ), P E|Z 3 =c j ).</formula><p>Finally, assume that we use an assay which measures only the gene Z 2 . Then our matrix satisfies</p><formula xml:id="formula_14">E(Y ij ) = E(Z 2 | [Z 3 = c j , do(Z 1 = a i )])</formula><p>where we use the notation [Z 3 = c j , do(Z 1 = a i )] to emphasize that the order of the terms is important: the intervention is considered in the model obtained after conditioning. Using our notation, shown in Equation ( <ref type="formula" target="#formula_11">2</ref>), we would write</p><formula xml:id="formula_15">E(Y ij ) = E(Z 2 (a i ) | Z 3 = c j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Synthetic Interventions</head><p>The Synthetic Interventions (SI) estimator <ref type="bibr" target="#b2">(Agarwal et al., 2020</ref>) is a local causal prediction approach for latent factor models. To predict Y ij for (i, j) Ã∏ ‚àà ‚Ñ¶, SI follows the following procedure: (1) take the set of columns C(i) for which we have observed row i, (2) take all rows for which we have observed columns C(i) ‚à™ {j}, (3) fit a linear regression on the available data defined by these subsets, and (4) use the linear regression coefficient to predict Y ij as a linear combination of {Y i‚Ñì } ‚Ñì‚ààC(i) .</p><p>Note that, analogously, we could use SI regressing along columns. See Appendix A for more details. One of the main advantages of using SI is that, in contrast of NNM, it does not require significant assumptions on the missingness structure of the data. Instead, to prove theoretical guarantees, e.g. finite-sample consistency <ref type="bibr" target="#b2">(Agarwal et al., 2020)</ref>, the data should be generated from a latent factor model, and also satisfy a linear span inclusion assumption, namely that v j ‚àà span(v k : k ‚àà C(i)). Intuitively, if C(i) is sufficiently large in relation to the rank of our factor model, SI provides a consistent estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Results</head><p>In this section, we establish expressions for the expected value of the entries of our tensor/matrix Y. To connect these expressions to specific tensor completion approaches, we will call an estimator consistent for an expression if it returns E[Y ij ] when given as input the expected values for each observed entry, i.e. the values E[Y uv ] for (u, v) ‚àà ‚Ñ¶. This corresponds to taking a limit, when each observed entry is the average of K independent samples of Y uv ‚àº P I C =v;do(I A =u) , and we let K ‚Üí ‚àû. To simplify notation, for any v ‚àà R d , we define aug(v) = (1, v 1 , . . . , v d ) ‚ä§ ‚àà R d+1 as the vector given by prepending a 1 to v. The proofs of the following results can be found in Appendix B.</p><p>Assumption 1 The entries of Y come from a linear SCM over a set of variables Z = (Z 1 , . . . , Z q ) as in Equation (1). That is, the (i, j)-th entry corresponds to the counterfactual when conditioning on I C = j and then intervening to set I A = i, as in Equation (2).</p><p>Theorem 1 Under Assumption 1, we have</p><formula xml:id="formula_16">E(Y ij ) = E (Z X (i) | I C = j) = U i v i + U ‚Ä≤ i w j . for some U i , U ‚Ä≤ i ‚àà R |X |√ó|Z| , v i ‚àà R |Z|</formula><p>depending on the action index i, and some w j ‚àà R |Z| depending on the context index j.</p><p>Proof [Sketch] For a linear SCM, we have Z = (I -B) -1 E with B ‚àà R q√óq and E = (Œµ 1 , . . . , Œµ q ). Conditioning on I C = j gives a posterior P E|I C =j over the exogenous noise. Intervening gives a new matrix B i and new exogenous noise ·∫ºA i , where A i are the intervention targets when</p><formula xml:id="formula_17">I A = j. Defining E ‚Ä≤ = ·∫ºA i ‚à™E A i , then Z(i) equals (I -B i ) -1 E ‚Ä≤ in distribution. Thus, E(Z(i) | I C = j) = U i v i +U ‚Ä≤ i w j for U i = (I -B i ) -1 and U ‚Ä≤ i a masked version of U i , with v i and w j coming from E[E ‚Ä≤ ].</formula><p>Theorem 1 shows that E[Y] follows a factor model with a fixed action effect. The fixed effect can be folded into the factor model as follows:</p><formula xml:id="formula_18">U i v i + U ‚Ä≤ i w j = U i v i U ‚Ä≤ i R |X |√ó(1+|Z|) aug(w j )<label>(3)</label></formula><p>Thus, Synthetic Interventions will be consistent for completing the tensor. However, we propose an alternative to deal with the fixed effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Theoretically-motivated extensions of Synthetic Interventions</head><p>The factor model in Equation (3) has more structure than a general rank |Z| + 1 factor model. We have a fixed effect along rows plus a factor model of rank |Z|. The best approximation of a one-way fixed effect is given by the mean-over-contexts estimator (see Appendix C). Therefore, following the same idea from the NNM-FE <ref type="bibr" target="#b4">Athey et al. (2021)</ref>, we propose the following modification to SI:</p><p>(1) subtract the fixed effect, i.e., let D = Y -≈∂mean-over-contexts , (2) run SI on this matrix and obtain DSI , (3) return the estimate ≈∂mean-over-contexts + DSI . Intuitively, by removing the exact fixed effect, our factor model would have rank |Z| instead of |Z| + 1, so it would be easier to satisfy the linear span inclusion assumption.</p><p>Tensor case. The non-symmetry in the factor model from Equation ( <ref type="formula" target="#formula_18">3</ref>) is even more relevant in the tensor case, i.e. |X | &gt; 1 (we have a matrix that depends on i and a vector that depends on j). Therefore, it is more reasonable to regress along contexts than along actions. To show how SI is non-symmetric for tensors, in Appendix D, we provide a detailed example for a two-node case where we observe both Z 1 and Z 2 . Following our setup, we provide a 4 √ó 4 √ó 2 tensor that can only be completed if we use SI within columns (contexts). In particular, for using SI within rows we would need more data to satisfy the linear span inclusion assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploiting Additional Structure in the Model Class</head><p>One drawback about the factor model from Equation ( <ref type="formula" target="#formula_18">3</ref>) is that it has rank |Z| + 1, which might be considerably large. We now consider two additional assumptions which impose additional structure on how I A and I C interact with the SCM over Z. First, we consider models where the all contexts are defined by some common set of system variables.</p><p>Assumption 2 There is some C ‚äÇ [q] such that C j = C for all j ‚àà [n].</p><p>In our setting, where we observe only a very coarse-grained context index, such as cell type, it is reasonable to suppose that (at least approximately) the index can be determined from only a small set of context markers C. For example, there may exist some small set of marker genes that define cell states. Assumption 2 is important to consider, since it can have a substantial effect on identifiability and estimation by reducing the rank of our factor model to |C|.</p><p>Proposition 2 Let Assumptions 1 and 2 hold, and assume that the underlying SCM is Gaussian.</p><p>Then the latent factor model from Theorem 1 is</p><formula xml:id="formula_19">U ‚Ä≤ i w j = U ‚Ä≤ i W c j</formula><p>where W is a constant matrix and c j is the value that context markers take to define context j. Therefore, by defining</p><formula xml:id="formula_20">U ‚Ä≤‚Ä≤ i = U ‚Ä≤ i W ‚àà R |X |√ó|C|</formula><p>we obtain a latent factor model of rank |C|.</p><p>Assumption 3 There is some</p><formula xml:id="formula_21">A ‚äÇ [q] such that A i = A for all i ‚àà [m].</formula><p>Assumption 3 indicates that all actions change the SCM in a similar way. Although this may not be realistic in most settings, we might have this setup for a particular sub-matrix of our whole matrix Y.</p><p>For example, Assumption 3 may hold for drugs which use the same mechanism of actions, or if the different actions correspond to the same drug over different dosage concentrations. In this case, the latent factor model reduces to a two-way fixed effects model.</p><p>Corollary 3 Under Assumptions 1 and 3 we have</p><formula xml:id="formula_22">E (Z X (i) | I C = j) = U v i + U ‚Ä≤ w j .</formula><p>so Fixed Effect (FE) is a consistent estimator for completing Y.</p><p>In our previous results, we have considered only the case of a linear SCM. This can be extended in the simplified case where (1) all interventions occur on the same set of nodes and (2) all contexts use the same context-defining nodes. While linearity may appear to be a strong assumption, it is closely related to the low-rank assumption, which is essential in matrix completion. In Appendix E, we study some examples of non-linear SCMs. In particular, when considering polynomials, we observe how the rank of the LFM increases as the degree of the polynomials increases. These results provide evidence that some form of linearity or low-degree polynomial assumption may be required to justify a connection between factor models and causal models. Finally, in real applications, we need to test the validity of our assumptions, i.e. we need to test whether our tensor comes from a certain graphical model. So far, we have only exploited the structure that the SCM induces on expectations of Y. However, we can also use the structure to obtain fine-grained implications regarding the variance of our observations. This structure on the variance is helpful for inference and for hypothesis testing. We demonstrate this idea in Appendix F, where we propose a hypothesis test for the fixed effects model implied by Corollary 3. In particular, we test Corollary 3 implies homoscedastic errors within a row. Thus, we develop a hypothesis test for homoscedasticity, and then a particular test for fixed effects which reduces to a Welch's test. Similarly, for the latent factor model (LFM) case from Prop. 2, we can test homoscedasticity within columns, then use the test for LFMs proposed by <ref type="bibr" target="#b2">Agarwal et al. (2020)</ref>. In Appendix F, we demonstrate the performance of these tests on synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical results</head><p>We work with the PRISM Repurposing dataset <ref type="bibr" target="#b12">(Corsello et al., 2020)</ref>. The entries of Y measure a viability score, which indicates how lethal a drug is for a given cell line (i.e., negative viability means that a large proportion of cells die). Since we consider the index-only setting, we use only the observed entries of this matrix to make our predictions. To relate the PRISM data into the model class introduced in Section 4, we think of the initial cell states in terms of Fig. <ref type="figure" target="#fig_2">4</ref> (left). In particular, we assume that there is some latent space (defined by the context variables) where we can perfectly distinguish different cell lines, e.g. the expression levels of some marker genes.</p><p>Arranging drugs in rows and cell lines in columns, as in Fig. <ref type="figure">1</ref> from Section 1, we obtain a 4, 686 √ó 568 matrix. To check the symmetry of an algorithm, we want the same number of rows as columns, so that completing along one direction (e.g. columns) is not better simply because there is more data to make predictions. Thus, we run the experiments on a square submatrix to avoid of this bias. For the same reason, we use symmetric missing data patterns, as described in Section 5.1. We compare the performance of the algorithms in terms of the R 2 score. The baseline used in the denominator of the R 2 is the average over the missing data. As an example, for the matrix shown in Fig. <ref type="figure" target="#fig_2">4</ref> (right) and a missing pattern from Fig. <ref type="figure" target="#fig_3">5</ref>, the baseline MSE is approximately 0.87. Getting high R 2 is a difficult task because the matrix is relatively noisy (see Appendix G.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">About the missing data pattern</head><p>In the PRISM dataset, we have observed all the matrix. As a consequence, we can decide the missing data pattern to test the performance of the matrix completion approaches. But which missing data pattern is more appropriate for biological data? As we have seen in Section 2, Nuclear Norm Minimization algorithms are near-optimal when we have uniformly at random missing entries. However, in biology applications this is usually not the case. Instead, it is more common to have a small portion of cell lines that has been tested against many of the drugs, but for the vast majority of cell lines we have only run a few experiments. For example, in Appendix G.2 we can see the missing data pattern for the CMAP dataset.</p><p>For this reason, we test the algorithms on missing patterns as the ones from Fig. <ref type="figure" target="#fig_3">5</ref>. Here, for each missing entry (i, j), the number of non-missing entries (black squares) in the i-th row is equal to the number of non-missing entries in the j-th column. We run the experiments using different data patterns to see the effect of having less/more available data.</p><p>In Appendix G.3 we consider an alternative missing data pattern where we do not have a constant number of observations for each missing entry. We can observe similar results also in this setting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance of matrix completion algorithms</head><p>In this section we show the performance of the matrix completion algorithms presented before. In particular, we are interested in analyzing how this performance depends on the amount of observed data. An extensive analysis of this dependence can be bound in Appendix G.4. In Fig. <ref type="figure" target="#fig_4">6</ref> we see that mean-over-contexts is a strong baseline and that Synthetic Interventions (and its variants) outperform all the other approaches. NNM performs competitively when we observe a sufficiently large amount of entries (right-hand side plot) but it performs quite poorly otherwise.</p><p>These results reinforce our theoretical findings. In Section 4 we see how, assuming an underlying linear SCM, the counterfactual quantity of our interest leads to a particular factor model, implying consistency of the Synthetic Interventions estimator. For further analysis, in Appendix G.5 we report results for other variations of these approaches. In Appendix G.6 we run similar experiments on a particular submatrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we generalize the SCM-based model class introduced by <ref type="bibr">Squires et al. (2022)</ref> to allow for situations where the measured outcome is defined as a counterfactual. We showed that latent factor models (LFMs) naturally arise out of this model class, though with an additional term for fixed action effects. This demonstrates the fundamentally distinctive nature of causal matrix completion, where there is a difference between conditioning (completing within columns) and intervening (completing within rows). As a practical consequence, this led us to propose a simple extension of the Synthetic Interventions (SI) estimator which includes a fixed effect term. These model-inspired causal matrix completion approaches work considerably well, especially in the low-data regime.</p><p>Limitations and Future Work. In this paper, we largely focused on linear models, and only sought to develop consistent estimators for causal predictions. An important next step is to analyze the noisy case, focusing on statistical aspects such as sample complexity and efficient inference.</p><p>A key limitation of our work is that we only considered index-only causal prediction problems. In many applications, additional data is available for actions and/or contexts, e.g., the molecular structure of a drug would be highly relevant to predicting its effect. We expect that the current work will be a useful conceptual foundation for developing model-based approaches to these causal prediction problems, especially when developing further connections between SCMs and latent factor models. It would be especially interesting to explore connections with recent works for intervention extrapolation, such as <ref type="bibr" target="#b1">Agarwal et al. (2023)</ref> and <ref type="bibr" target="#b31">Saengkyongam et al. (2023)</ref>.</p><p>Finally, while our work was mainly motivated by a biological problem, the causal prediction problem is very general, and it would be interesting to apply our framework in other settings. where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents of Appendix</head><formula xml:id="formula_23">A i = [q] \ A i . Hence, E(Z(i) | I C = j) = (I -B i ) -1 E( ·∫º) + (I -B i ) -1 I A i E(E | I C = j)</formula><p>Recall that we are interested only in Z X (the variables we observe). Let</p><formula xml:id="formula_24">U i = (I -B i ) -1 X ‚àà R |X |√óq U ‚Ä≤ i = U i I A i ‚àà R |X |√óq Therefore, we have E(Z X (i) | I C = j) = U i v i + U ‚Ä≤ i w j</formula><p>Proposition 2 Let Assumptions 1 and 2 hold, and assume that the underlying SCM is Gaussian.</p><p>Then the the latent factor model from Theorem 1 is</p><formula xml:id="formula_25">U ‚Ä≤ i w j = U ‚Ä≤ i W c j</formula><p>where W is a constant matrix and c j is the value that context markers take to define context j. Therefore, by defining</p><formula xml:id="formula_26">U ‚Ä≤‚Ä≤ i = U ‚Ä≤ i W ‚àà R |X |√ó|C| we obtain a latent factor model of rank |C|. Proof We have E = (Œµ 1 , . . . , Œµ q ) ‚àº N (¬µ Œµ = 0, Œ£ Œµ = diag(œÉ 2 i )), so Œ£ := Cov(Z) = (I -B) -1 Œ£ Œµ (I -B ‚ä§ ) -1</formula><p>Let Z C be the context markers variables and C = [q] \ C. Using the Schur Complement we obtain</p><formula xml:id="formula_27">E(Z C | Z C = c j ) = Œ£ CC Œ£ -1 CC c j Of course, we also have E(Z C | Z C = c j ) = c j , so E(Z | Z C = c j ) = M</formula><p>c j for some matrix M . One way to be more specific about this M is the following: Let P be the permutation matrix such that P Z = (Z C , Z C ) ‚ä§ . Then we have</p><formula xml:id="formula_28">E(P Z | Z C = c j ) = Œ£ CC Œ£ -1 CC I |C| c j =‚áí M = P -1 Œ£ CC Œ£ -1 CC I |C|</formula><p>Hence, after conditioning, the expected value of the noise is</p><formula xml:id="formula_29">E(E | Z C = c j ) = (I -B)E(Z | Z C = c j ) = (I -B)M c j</formula><p>Therefore, we can define</p><formula xml:id="formula_30">W = (I -B)M = (I -B)P -1 Œ£ CC Œ£ -1 CC I |C| ‚àà R q√ó|C|</formula><p>Note that W depends on the nodes we are conditioning on, but not on their values. The rest of the proof is completely analogous to the previous one.</p><p>Remark 4 Although it is common to work with ¬µ Œµ = 0, there is no need to assume that. In general, again using the Schur complement, we would have (let ¬µ = ¬µ Œµ )</p><p>Appendix D. SI is non-symmetric in the tensor case</p><p>In Section 4, we argued that SI is non-symmetric in the tensor case. The following example shows that this is true. In particular, completing within columns recovers the desired outcome while completing within rows does not. The key idea is that we have to flatten the tensor in one direction or another, and depending on the direction, linear span inclusion does or does not hold. Consider the following 3-order tensor</p><formula xml:id="formula_31">Y = Ô£Æ Ô£Ø Ô£Ø Ô£∞ Y 111 Y 121 Y 131 Y 141 Y 112 Y 122 Y 132 Y 142 Y 211 Y 221 Y 231 Y 241 Y 212 Y 222 Y 232 Y 242 Y 311 Y 321 Y 331 Y 341 Y 312 Y 322 Y 332 Y 342 Y 411 Y 421 Y 431 Y 441 Y 412 Y 422 Y 432 Y 442 Ô£π Ô£∫ Ô£∫ Ô£ª = Ô£Æ Ô£Ø Ô£Ø Ô£∞ 1 1 1 1 0 2 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 Ô£π Ô£∫ Ô£∫ Ô£ª</formula><p>For the (i.j)-th entry, we use the notation</p><formula xml:id="formula_32">Y ij = (Y ij1 , Y ij2 ) ‚àà R 2 .</formula><p>Suppose we have observed all the matrix except for the (4, 4)-th entry. By regressing along rows, we are predicting the (flattened) first three rows of the 4th column (1, 0, 1, 1, 1, 1) from the (flattened) first three rows of the 1st, 2nd and 3rd columns (1, 1, 1, 0, 1, 1), etc. Thus, we obtain the following system of equations:</p><formula xml:id="formula_33">Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 1 1 1 1 0 1 1 1 1 0 2 1 1 1 1 1 1 1 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ Œ≤ = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 1 0 1 1 1 1 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏</formula><p>This linear system is independent and we obtain Œ≤ = (1, 1, -1). Thus, our prediction for the (4, 4)-th entry is</p><formula xml:id="formula_34">≈∂44 = 0 0 0 1 1 1 Œ≤ = 0 1 = Y 44</formula><p>Thus, regressing along rows works successfully. However, by regressing along columns we would obtain the following:</p><formula xml:id="formula_35">Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 1 1 1 1 0 1 1 1 1 0 1 1 2 1 1 1 1 1 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ Œ± = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ 0 0 0 1 1 1 Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏</formula><p>which is an inconsistent system. The least-squares solution is Œ± = (0, 0.6, 0), so the prediction would be</p><formula xml:id="formula_36">≈∂44 = 1 0 1 1 1 1 Œ± = 0 0.6 Ã∏ = Y 44</formula><p>So this shows how SI works only in one direction. Moreover, this example can be related to our causal setup. Let Z 1 ‚Üí Z 2 follow the SCM</p><formula xml:id="formula_37">Z 1 = Œµ 1 Z 2 = Z 1 + Œµ 2</formula><p>where Œµ 1 , Œµ 2 ‚àº N (0, 1) are independent noise variables. Suppose that we are observing both variables. Each column of Y corresponds to conditioning on Z = (c 1 , c 2 ). In particular, we have</p><formula xml:id="formula_38">E [(Œµ 1 , Œµ 2 ) | (Z 1 , Z 2 ) = (c 1 , c 2 )] = (c 1 , c 2 -c 1 ). The rows of Y correspond to an intervention do(Z 1 = a 1 , Z 2 = a 2 ).</formula><p>The values of (c 1 , c 2 ) and (a 1 , a 2 ) are defined in Table <ref type="table">2</ref>, where "-" denotes</p><formula xml:id="formula_39">column (I C ) c 1 c 2 j = 1 1 0 j = 2 0 1 j = 3 1 1 j = 4 0 0 row (I A ) a 1 a 2 i = 1 1 - i = 2 - 1 i = 3 1 1 i = 4 0 1</formula><p>Table 2: Values used for defining i-th row and j-th column of Y.</p><p>that we are not intervening on that variable. It is easy to check that E(Z(i)</p><formula xml:id="formula_40">| I C = j) = Y ij .</formula><p>Therefore, our matrix comes from a linear causal model. To see why SI only worked on one direction, recall that Equation (3) tells us that we can write</p><formula xml:id="formula_41">Y ij = P i q j</formula><p>for some P i ‚àà R 2√ó3 and q j = R 3 . Following the Proof of Theorem 1, we can construct these factors:</p><formula xml:id="formula_42">P 1 = 1 0 0 1 0 1 P 2 = 0 1 0 1 0 0 P 3 = 1 0 0 1 0 0 P 4 = 0 0 0 1 0 0 q 1 = 1 1 -1 q 2 = 1 0 1 q 3 = 1 1 0 q 4 = 1 0 0</formula><p>Note that q 4 ‚àà span(q 1 , q 2 , q 3 ). In fact, q 4 = Œ≤1 q 1 + Œ≤2 q 2 + Œ≤3 q 3 , so the linear span inclusion is satisfied along columns. Nevertheless, P 4 / ‚àà span(P 1 , P 2 , P 3 ), so the linear span inclusion is not satisfied along rows.</p><p>Hence,</p><formula xml:id="formula_43">E(X(a)|C = c) = exp a a + c 2 + a 2 4 = exp 5 4 a 2 exp ac 2</formula><p>But this expression cannot be expressed as a low-rank factor model.</p><p>Example 3 suggests that it might be difficult to obtain low-rank factor models for SCMs that involve functions "more complicated" than polynomials. Moreover, Example 2 shows that the rank of the factor model increases as the degree of the polynomial increases. Note that the expression obtained is the exponential of a polynomial in a, c. If we applied an entry-wise logarithm to our matrix, we could apply the SI estimator and then take the exponential of the outcome observed. With this idea in mind, we could think about developing SI theory for feature spaces. However, this goes beyond the scope of this work and we leave it for future discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F. Hypothesis tests</head><p>In Section 4, we discussed the necessity to test the validity of our assumptions. Here, we show how our causal modeling may also be useful for constructing hypothesis tests. For simplicity, assume that we have observed n s independent samples for each observed entry, i.e. for each</p><formula xml:id="formula_44">(i, j) ‚àà ‚Ñ¶. Assume that Y ij ‚àº N (¬µ ij , œÉ 2 ij ) and let Y ij ‚àº N (¬µ i,j , œÉ 2 ij ns</formula><p>) denote the average of these samples. We can estimate the sample variance as follows</p><formula xml:id="formula_45">S 2 ij = 1 ns-1 ns k=1 (Y (k) ij -Y ij ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Homoscedasticity within rows/columns</head><p>Before discussing the different decompositions obtained in Section 4, is it worth noting that the SCMs we studied give rise to certain structure in the variance of our matrix. In particular, Prop. 2 leads to homoscedasticity within the entries of the same row (œÉ i,j = œÉ i,j ‚Ä≤ ). Indeed, looking at the Schur Complement, the variance of the noise after conditioning on some variables depends on the variables we condition on, but not on the actual values they take. On the other hand, in Corollary 3 we have the same SCMs after the interventions for different rows, which implies homoscedasticity within columns (œÉ i,j = œÉ i ‚Ä≤ ,j ). This lead to a classical test for comparing the variances of two independent samples. For example, for testing homoscedasticity within column j we consider</p><formula xml:id="formula_46">Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ H 0 : œÉ 2 i,j œÉ 2 i-1,j = 1 ‚àÄ i ‚â• 2, j H 1 : œÉ 2 i,j œÉ 2 i-1,j Ã∏ = 1, ‚àÉ i, j</formula><p>So we can use the following estimator</p><formula xml:id="formula_47">F = S 2 i,j S 2 i-1,j</formula><p>and we have F | H 0 ‚àº F ns-1,ns-1 . A test for homoscedasticity within row i would be analogous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Test for fixed effects</head><p>Suppose that we are in the case of Corollary 3. How can we test if FE is appropriate for our matrix? For simplicity in notation, suppose that we have observed all the entries of Y ‚àà R m√ón . For a partially observed entries, we would use these tests for the observed entries. With this notation, all the matrices with a two-way fixed effect are defined by the following equations.</p><formula xml:id="formula_48">Y ij + Y i ‚Ä≤ j ‚Ä≤ -Y i ‚Ä≤ j -Y ij ‚Ä≤ = 0 ‚àÄ i Ã∏ = i ‚Ä≤ , j Ã∏ = j ‚Ä≤<label>(5)</label></formula><p>Many of this equations are linearly dependent. In fact, all of them can be expressed as a linear combination of equations</p><formula xml:id="formula_49">E ij : Y 11 + Y ij -Y i1 -Y 1j = 0, i = 2, . . . , m, j = 2, . . . n Indeed, by computing E ij + E i ‚Ä≤ j ‚Ä≤ -E i ‚Ä≤ j -E ij ‚Ä≤</formula><p>we get Equation ( <ref type="formula" target="#formula_48">5</ref>). So we only need to check (m -1)(n -1) equations instead of m(m -1)n(n -1). However, one drawback about using Equation (F.2) for constructing hypothesis tests is that they heavily rely on the (1,1)-th entry. So we can consider a more robust set of equations:</p><formula xml:id="formula_50">·∫ºij : Y i-1,j-1 + Y i,j -Y i,j-1 -Y i-1,j = 0, i = 2, . . . , m, j = 2, . . . , n Let ¬µ i,j = E(Y i,j</formula><p>) , the hypothesis test for FE is the following.</p><formula xml:id="formula_51">H 0 : ¬µ i-1,j-1 + ¬µ i,j -¬µ i,j-1 -¬µ i-1,j = 0, ‚àÄ i, j H 1 : ¬µ i-1,j-1 + ¬µ i,j -¬µ i,j-1 -¬µ i-1,j Ã∏ = 0, ‚àÉ i, j</formula><p>Therefore, we can consider the following statistic:</p><formula xml:id="formula_52">Tij = Y i-1,i-1 + Y i,j -Y i,j-1 -Y i-1,j 1 ns S 2 i-1,j-1 + S 2 i,j + S 2 i,j-1 + S 2 i-1,j</formula><p>The key idea is that, in Corollary 3, we have homoscedasticity within rows. This makes our test similar to a Welch's test, and we can approximate the distribution of Ti,j | H 0 as t ŒΩ where ŒΩ = (n s -1)</p><formula xml:id="formula_53">(S 2 i,j + S 2 i-1,j ) + (S 2 i,j-1 + S 2 i-1,j-1 ) 2 (S 2 i,j + S 2 i-1,j ) 2 + (S 2 i,j-1 + S 2 i-1,j-1</formula><p>) 2 Multiple comparisons problem. We have N = (n -1)(m -1) different null hypothesis, corresponding to each (i, j) pair for i, j ‚â• 2. If we want to achieve a familywise error rate (FWER) of Œ±, we can reject a null hypothesis if we get a p-value lower than Œ± N (Bonferroni correction), or 1 -(1 -Œ±) N (≈†id√°k correction).</p><p>Empirical Results. In Fig. <ref type="figure" target="#fig_6">7</ref> we show the how this estimator is useful for testing a fixed-effect behaviour. In particular, for each plot, we simulate 200 matrices satisfying the assumptions from Corollary 3 and 200 matrices satisfying the assumptions from Prop. 2. More specifically, we use the causaldag Python library to generate a DAG with random weights (we use the default parameters). We fix the node we observe (Z X ) and the context variables Z C . For each simulated matrix, we condition on a different set of values (z C ), sampled from a uniform distribution over <ref type="bibr">[0,</ref><ref type="bibr">10]</ref>. The nodes where we intervene on change for each matrix. We perform a do-intervention on each node with probability 0.2 and the values we used are also sampled from a uniform distribution over [0, 10]. For the matrices where the fixed effect behavior is not satisfied, we do not intervene on the same nodes for all the matrix, so we follow the procedure mentioned before for each row.</p><p>We observe how, as the size of the matrix and the number of observed samples increase, the statistic we propose can fully distinguish the two types of matrices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Test for mean-over-contexts</head><p>To be precise, before testing whether or not we our matrix follows a two-way fixed effect model, we should test whether it has only a one-way fixed effect. That is, test if we can use mean over actions or contexts. For example, to test whether we only have a fixed effect along rows, instead of Equation <ref type="formula" target="#formula_48">5</ref>we would have The singular values of Y present a relatively heavy tail. This makes it difficult to obtain good metrics in our predictions. The truncated SVD of rank r is the best rank r approximation of a matrix. Therefore, the R 2 obtained by the truncated SVD may be considered as an upper bound for our completion algorithms. We should not expect to achieve an R 2 of 0.90 as this would involve finding the best approximation of rank 80.</p><formula xml:id="formula_54">Y ij -Y ij ‚Ä≤ = 0 ‚àÄ i, j Ã∏ = j ‚Ä≤</formula><p>Figure <ref type="figure">9</ref>: One-way fixed effect behavior. The truncated SVD of rank 1 (SVD1 = S 1 ) has approximately constant rows. This suggests that there is a fixed effect for actions so mean-overcontexts should be a relatively good estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Missing pattern in CMAP dataset</head><p>As we mentioned in Section 5.1, in biology applications we usually do not have a missing at random pattern. In Fig. <ref type="figure" target="#fig_8">10</ref> we show the entries observed for the CMAP dataset. We use this example as a starting point for the missing data pattern considered in Section 5.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. An alternative missing data pattern</head><p>The missing data patterns from Section 5.1 may seem very artificial, as we have a constant number of observed rows and columns for each missing entry. In Fig. <ref type="figure">11</ref> we propose an alternative pattern, still following the motivation from the CMAP dataset (see Appendix G.2), and we show the performance of different algorithms for this setting. The results are not significantly different to the ones from Fig. <ref type="figure" target="#fig_4">6</ref>.</p><p>Figure <ref type="figure">11</ref>: Non-square missing data pattern. Here, we do not have a constant value of rows and columns for each missing entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Performance of the algorithms depending on the amount of data observed</head><p>In Section 5.2, we showed the performance of different matrix completion approaches for a specific number of observed entries. The experiment we show here consists on varying the number of rows and columns observed per missing entry. This parameter will range from 5 until 275. For each case, the square corresponding to the missing values has constant size 284 √ó 284. The reason for having constant size square (same number of missing entries) is that the MSE is computed on the same number of samples.</p><p>We have used the following bootstrap technique. For each case we shuffle the rows and columns of the full matrix and take a sub-matrix of our desired size (e.g. 289 √ó 289). Then, we take the corresponding missing data pattern and compute the R 2 for each algorithm. We repeat the same process for 20 different shuffles. See Fig. <ref type="figure" target="#fig_9">12</ref>. As a takeaway, it is worth noting that Synthetic Interventions benefits from subtracting the mean-over-contexts, especially in the low data regime.</p><p>Note that we can think about Fig. <ref type="figure" target="#fig_4">6</ref> as taking slices from Fig. <ref type="figure" target="#fig_9">12</ref>. In particular, Fig. <ref type="figure" target="#fig_4">6</ref> shows that Collaborative Filtering approach does not improve the baseline algorithms and Kernel Linear Regression is not significantly better than the standard Synthetic Interventions approach. For that reason, we do note include them in Fig. <ref type="figure" target="#fig_9">12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Performance of all the matrix completion algorithms considered</head><p>In Section 5.2, we showed the performance of some matrix completion approaches. Here, in Fig. <ref type="figure" target="#fig_10">13</ref>, we include more approaches and some variations of the previous ones.</p><p>In particular, we see that the nearest neighbors approaches are not significantly better than the baselines. Doubly Robust NN is the method proposed in <ref type="bibr" target="#b15">Dwivedi et al. (2022)</ref>. CF10 stands for a 10-nearest neighbor Collaborative Filtering approach. Finally, it is worth noting that SI and its variations outperform the other approaches. G.6. Removal of "killer drugs"</p><p>In this section, we run the same experiments on certain portions of the whole matrix Y. Looking back at our original matrix in Fig. <ref type="figure" target="#fig_2">4</ref>, we observe that there are some blue (negative) rows that seem to be constant along all the columns. This would mean that this particular drug is killing all the cell lines.</p><p>In light of this effect, we call a drug a "killer drug" if it kills more than a certain percentage of cell lines. It is not clear which threshold we should use. In Fig. <ref type="figure" target="#fig_6">17</ref>, we show the number of "killer drugs" depending on the threshold we define.</p><p>For the following results, we used a threshold of 80%. The resulting matrix can be seen in Fig. <ref type="figure" target="#fig_2">14</ref>. Note that we have removed almost all the blue lines that we mentioned before. Now the fixed effect along rows does not seem that clear. In particular, the MSE of the baseline decreases significantly compared to the previous setup. This means that the task of completing the task is easier for the baseline model. In consequence, getting high R 2 values is going to be much harder.</p><p>In Fig. <ref type="figure" target="#fig_3">15</ref> we repeat the same experiment from Fig. <ref type="figure" target="#fig_7">8</ref> for this new data. As expected, now the R 2 value we obtain are much lower. Finally, we show the performance of the different matrix completion approaches on this matrix in Fig. <ref type="figure" target="#fig_4">16</ref>.</p><p>Figure <ref type="figure" target="#fig_2">14</ref>: Matrix obtained when we remove "killer drugs". Here, a "killer drug" is a drug that kills more than 80% of cell lines.</p><p>Figure <ref type="figure" target="#fig_3">15</ref>: Getting high R2 values is even more difficult than before (compare it with Fig. <ref type="figure" target="#fig_7">8</ref>).</p><p>After removing the "killer drugs", MSE(average) decreases from 0.87 to 0.28. This means that the completion task is easier for this baseline, so it is more difficult to obtain high R 2 values.</p><p>Figure <ref type="figure" target="#fig_4">16</ref>: Performance after removing "killer drugs". The gap between mean-over-actions and SI is similar from the one in Fig. <ref type="figure" target="#fig_4">6</ref>, despite having more room for improvement.</p><p>Figure <ref type="figure" target="#fig_6">17</ref>: Definition of a "killer drug". Here we show the number of drugs that kill more than threshold % of cell lines. We consider that a cell line is dead if the viability score is negative. In our experiments, we used a threshold of 80%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The latent factor model (LFM) written as a simple structural causal model (SCM).LFMs assume thatY ij = ‚ü®u i , v i ‚ü© + Œµ ij for u i , v i ‚àà R r .This generative process can be viewed as an SCM over 3 observed (shaded) and 2 latent (unshaded) variables. Here, I A and I C represent action and context indices, respectively (note that they are independent).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DAG defining our model class. Shaded nodes are observed, while unshaded nodes are unobserved. Data is generated by conditioning on the context index I C = j (indicated by the double-edge for the node I C ) followed by intervening to set the action index I A = i (indicated by the square node I A ).The context may potentially depend on any subset of Z and the action may potentially affect any subset of Z. The exogenous noise terms Œµ 1 , Œµ 2 , . . . , Œµ q are shared between the pre-interventional and post-interventional SCMs.Here, the observed outcome is Y ij ‚àº P(Z X (i) | I C = j) for X = {2, 4}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (Left) Translating the PRISM data to our model class. Letting C = {k, l} indicates that cell state is defined solely in terms of Z k and Z l . (Right) Matrix of viability scores. Each entry represents the viability score for a drug-cell line pair. Negative viability indicates cell death. Viability scores are normalized for visualization purposes.</figDesc><graphic coords="11,312.82,90.86,194.40,166.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Missing data patterns used in our experiments. Observed entries are denoted with black. For each missing entry we have the same number of observations along rows and columns.</figDesc><graphic coords="11,100.80,623.53,194.40,70.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Variants of Synthetic Interventions have the best performance on matrix completion for the PRISM dataset. Using the missing data patterns are ones shown in Fig. 5, we test the performance of each algorithm on 20 different shuffles of rows and columns. The results are shown using boxplots. The numbers at the top denote the median (red lines).</figDesc><graphic coords="12,90.00,90.86,432.00,204.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>beyond polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F Hypothesis tests 24 F.1 Homoscedasticity within rows/columns . . . . . . . . . . . . . . . . . . . . . . . 24 F.2 Test for fixed effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Test for mean-over-contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 F.4 Test for Synthetic Interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 G Further analysis on PRISM Repurposing dataset 28 G.1 SVD analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 G.2 Missing pattern in CMAP dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 An alternative missing data pattern . . . . . . . . . . . . . . . . . . . . . . . . . . 30 G.4 Performance of the algorithms depending on the amount of data observed . . . . . 31 G.5 Performance of all the matrix completion algorithms considered . . . . . . . . . . 33 G.6 Removal of "killer drugs" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROC curves. m denotes the size of the matrix, i.e. Y ‚àà R m√óm . s denotes the number of samples observed per each entry.</figDesc><graphic coords="26,154.80,199.47,302.41,325.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Difficulty in getting high R 2 values. The singular values of Y present a relatively heavytail. This makes it difficult to obtain good metrics in our predictions. The truncated SVD of rank r is the best rank r approximation of a matrix. Therefore, the R 2 obtained by the truncated SVD may be considered as an upper bound for our completion algorithms. We should not expect to achieve an R 2 of 0.90 as this would involve finding the best approximation of rank 80.</figDesc><graphic coords="29,126.38,91.03,172.80,137.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Missing pattern NOT at random. Availability matrix for CMAP dataset. A black rectangle means that we have observed that entry. Columns have been sorted from left to right according to the number of available entries (for each column). In a similar way, rows have been sorted from bottom to top. Figure obtained from Squires et al. (2022).</figDesc><graphic coords="30,219.60,90.86,172.79,149.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure12: Dependence on number of observations. "obs" denotes the number of rows and columns observed for each missing entry. Figures from the right-hand side are focused on the low-data regime. We can see some spikes in the curves corresponding to NNM, these are produced by a change in the regularization parameter (e.g. from 10 -3 to 10 -4 ).</figDesc><graphic coords="32,118.66,509.39,172.80,144.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: All the matrix completion approaches considered. Here we show the performance of all the algorithms considered, using the missing pattern from Figure 5.</figDesc><graphic coords="33,90.00,206.94,431.99,224.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="35,90.00,115.20,432.00,204.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the algorithms. Model Class indicates that the algorithm is consistent when the true data-generating process belongs to that class, e.g., Synthetic Interventions is consistent under a factor model. Dependence on |‚Ñ¶| (the number of observed entries) is a qualitative judgement based on the performance results shown in Section 5.</figDesc><table><row><cell>Class</cell><cell>Symmetric Dependence on |‚Ñ¶|</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The terminology for different classes of interventions is fairly inconsistent. Soft interventions have many other names, e.g., parametric interventions<ref type="bibr" target="#b16">(Eberhardt and Scheines, 2007)</ref> or mechanism shifts<ref type="bibr" target="#b36">(Tian and Pearl, 2001)</ref>. Our definition for soft interventions contains perfect (hard) interventions and do-interventions as special cases, see(Squires  and Uhler, 2022)  for additional terms for these classes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This notation is inspired by Single World Intervention Graphs (SWIGs)<ref type="bibr" target="#b30">(Richardson and Robins, 2013)</ref>. Indeed, we would obtain a SWIG after performing do-interventions. However, we also consider soft interventions. We could write IA = ‚àÖ for defining the control state, i.e. no action. In that case Z(‚àÖ) = Z.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>In this case, we could defined this polynomial more explicitly as a function of E(Œµ k c | C = cj), but this is not necessary for expressing the factor model.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by funding from the <rs type="funder">Eric and Wendy Schmidt Center at the Broad Institute of MIT</rs> and <rs type="person">Harvard. √Å. Ribot</rs> acknowledges support by the <rs type="programName">mobility grants program</rs> of <rs type="institution">Centre de Formaci√≥ Interdisciplin√†ria Superior (CFIS) -Universitat Polit√®cnica de Catalunya (UPC)</rs>, the <rs type="funder">MIT Simons MMLS Foundation</rs> research grant (<rs type="grantNumber">6941629</rs>), and a <rs type="grantName">MOBINT-MIF grant</rs>. <rs type="person">C. Squires</rs> and <rs type="person">C. Uhler</rs> acknowledge support by the <rs type="programName">NSF TRIPODS program</rs> (<rs type="grantNumber">DMS-2022448</rs>), <rs type="funder">NCCIH/NIH</rs> (<rs type="grantNumber">1DP2AT012345</rs>), <rs type="funder">ONR</rs> (<rs type="grantNumber">N00014-22-1-2116</rs>), the <rs type="funder">United States Department of Energy (DOE)</rs>, <rs type="funder">Office of Advanced Scientific Computing Research</rs> (ASCR), via the <rs type="institution" subtype="infrastructure">M2dt MMICC center</rs> (<rs type="grantNumber">DE-SC0023187</rs>), <rs type="funder">AstraZeneca</rs>, the <rs type="funder">Eric and Wendy Schmidt Center at the Broad Institute</rs>, and a <rs type="grantName">Simons Investigator Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qytxk8F">
					<orgName type="program" subtype="full">mobility grants program</orgName>
				</org>
				<org type="funding" xml:id="_QbRRQvk">
					<idno type="grant-number">6941629</idno>
					<orgName type="grant-name">MOBINT-MIF grant</orgName>
					<orgName type="program" subtype="full">NSF TRIPODS program</orgName>
				</org>
				<org type="funding" xml:id="_cFKazdN">
					<idno type="grant-number">DMS-2022448</idno>
				</org>
				<org type="funding" xml:id="_58WNyhh">
					<idno type="grant-number">1DP2AT012345</idno>
				</org>
				<org type="funding" xml:id="_vFw8Gx3">
					<idno type="grant-number">N00014-22-1-2116</idno>
				</org>
				<org type="funding" xml:id="_6qaXuKE">
					<idno type="grant-number">DE-SC0023187</idno>
				</org>
				<org type="funding" xml:id="_AkQ4NCa">
					<orgName type="grant-name">Simons Investigator Award</orgName>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">M2dt MMICC center</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental material</head><p>All code for running the experiments presented in this paper can be found at <ref type="url" target="https://github.com/alvaro-ribot/causal-matrix-completion-PRISM">https://github.com/alvaro-ribot/causal-matrix-completion-PRISM</ref> Appendix A. Summary of matrix completion approaches</p><p>In Section 2, we presented the different matrix completion approaches considered in this paper. For completeness, in this section, we provide formulae for such approaches.</p><p>We follow the notation used in <ref type="bibr">Squires et al. (2022)</ref>. Let Y = (Y ij ) ‚àà R m√ón be our matrix of interest. Let ‚Ñ¶ ‚äÇ [m] √ó [n] be the set of observed entries and M = [m] √ó [n] \ ‚Ñ¶ be the set of missing entries. For a given row i, let C(i) = {j : (i, j) ‚àà ‚Ñ¶} be the set of column indices j for which we have observed Y ij . Similarly, for a column j define R(j) = {i : (i, j) ‚àà ‚Ñ¶}. This notation is extended to sets of rows (R) and sets of columns (C):</p><p>The following example clarifies our notation.</p><p>Then solve linear regression:</p><p>, where Œ≤ = Y ‚Ä† train y output and ‚Ä† denotes the pseudoinverse.</p><p>‚Ä¢ SI-FE:</p><p>‚Ä¢ Nuclear Norm Minimization (NNM):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proofs</head><p>Notation. Recall that [q] := {1, . . . , q}. Given a tuple of indices K = (k 1 , . . . , k p ) where k r ‚àà [q] for all r = 1, . . . , p, we define the matrix I K as follows:</p><p>Furthermore, given a vector Z ‚àà R q , we define</p><p>as the vector given by prepending a 1 to v.</p><p>Theorem 1 Under Assumption 1, we have</p><p>for some</p><p>depending on the action index i, and some w j ‚àà R |Z| depending on the context index j.</p><p>Proof Let Z = (Z 1 , . . . , Z q ) follow a linear SEM, i.e.</p><p>0 whenever i ‚â§ j and E = (Œµ 1 , . . . , Œµ q ). Let I be the identity q √ó q matrix. Since B is lower triangular, (I -B) is invertible and we have that</p><p>When we condition on the value of the context index, i.e. on I C = j, we get a new distribution for the noise, P E|I C =j . In particular,</p><p>be the target set of indices for the intervention with index i. For each k ‚àà A i , the k-th row of B is changed (modifying the dependency from its parents). Let B i be the weight matrix after the intervention 4 .</p><p>Moreover, for each k ‚àà A i , we have a new noise variable Œµk , independent of E. So, the expected value of these new variables depend on the intervention index i but not on the context index j. Let ·∫º be a q-dimensional random vector such that its k-th entry is Œµk if k ‚àà A i and 0 otherwise. Define</p><p>Therefore, the variables Z(i) after the intervention satisfy the following SEM</p><p>In the particular case of do-interventions, we would remove these dependencies, so all the entries in the k-th row of Bi would be 0.</p><p>Appendix C. Best approach for estimating a one-way fixed effect</p><p>In Section 4, we claimed that the best approximation of a one-way fixed effect is given by the mean-over-contexts estimator. Indeed, consider that following problem</p><p>where Y ‚àà R m√ón , 1 = (1, . . . , 1) ‚àà R n , and ‚à• ‚Ä¢ ‚à• F denotes the Frobenius norm of a matrix. By the linearity of trace we have</p><p>Using the cyclic property of the trace and taking the gradient of f we get</p><p>, which corresponds to the mean-over-contexts estimator. Since f is strictly convex, this is the solution to Problem 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Polynomial case</head><p>In Section 4, we discussed the possibility of extending our results to non-linear SCMs. Here, we show an example where the latent variables follow a linear SCM and there is a polynomial link function mapping the latent variables to the observed ones. For simiplicity in notation we use X = Z X , A = Z A , and C = Z C . Note that A and C are the same for all rows and columns, respectively.</p><p>Example 2 Consider the following SCM:</p><p>Since there is linear relation between A and C, we can still use the Schur Complement. For a gaussian variable Z ‚àº N (¬µ, œÉ 2 ), we have</p><p>Therefore, E(X(a i )|C = c j ) is a polynomial in a i and c j . In particular, we can write it as</p><p>for some polynomials q k . 5 Therefore, for a polynomial of degree d we have a factor model of degree d + 1, so we could also use Synthetic Interventions in this case.</p><p>If we had multiple fixed action nodes A 1 , . . . , A r and a polynomial of degree d from the latent nodes to X, we could use the same idea and express E(X(a i ) | C = c j ) as a polynomial on a 1 i , . . . , a r i . The number of monomials in r variables of degree no greater than d is r+d d , so that would be the rank of our factor model. However, it is not clear how our decomposition would look like if allowed intervening on different nodes, because the q k 's would depend on these indices. We leave this question open as a future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Going beyond polynomials</head><p>Example 3 Consider now the following SCM:</p><p>After conditioning and intervening, we get that</p><p>And we can repeat all the reasoning from the previous section to obtain an appropriate estimator. The case of a fixed effect along columns would be completely analogous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Test for Synthetic Interventions</head><p>Agarwal <ref type="bibr">et al. (2020)</ref> propose a hypothesis test for the SI estimator, so we could use it for our particular case to check the linear span inclusion assumption. Moreover, as mentioned before, assuming Gaussianity as in Prop. 2 we can also test homoscedasticity within rows. Therefore, constructing such tests is an interesting future direction.</p><p>Appendix G. Further analysis on PRISM Repurposing dataset G.1. SVD analysis</p><p>In Section 5, we argued that getting high R 2 was difficult because the PRISM data is relatively noisy.</p><p>Here we analyze the Singular Value Decomposition (SVD) of our matrix Y shown Figure <ref type="figure">4</ref>. It is important to mention that here we are showing the SVD computed on all the matrix. That is, we are also using the entries that were supposed to be missing and that we want to impute as a matrix completion task. Therefore, the results shown in this section are merely an analysis of our data, but we are not going to use them for the imputation task.</p><p>In Figure <ref type="figure">8</ref>, we show the Singular Values of Y. We can see that they are relatively diffuse, suggesting that there is a considerably amount of noise in the data. For example, the explained variance by the top 10 SVD vectors</p><p>The Eckart-Young theorem <ref type="bibr" target="#b17">(Eckart and Young, 1936)</ref> states that the best rank r approximation to a matrix is given by truncating its SVD to the top r singular values. We can use this result to establish upper bounds in performance that we should expect from matrix completion approaches. For instance, mean-over-contexts produces a rank 1 approximation of our matrix, so this approximation must be worse (i.e. smaller R 2 ) than the rank 1 truncated SVD.</p><p>In Figure <ref type="figure">8</ref>, we also show the R 2 values for the truncated SVD using the missing data pattern like the one from Figure <ref type="figure">5</ref>, with 25% of missing entries. It is important to remark that in this case we are computing the SVD using all the matrix Y (including missing entries), so this is not a valid completion approach. It should be understood as the optimal performance we can expect.</p><p>Generally, it is helpful to see how the first singular values pairs look like. Let S 1 be rank 1 truncated SVD of Y. In Fig. <ref type="figure">9</ref> we observe that S 1 has almost constant rows, so it seems to be capturing the fixed effect in that direction. This indicates that the fixed effect along rows is much more important than the fixed effect along columns. Therefore, we may expect that the mean-overcontexts is going to be a considerably good baseline. We also see how in Y -S 1 we lose a big part of this fixed effect along rows, and our matrix seems to have less variability. This suggests that subtracting the mean-over-contexts may lead us to better predictions (see Section 4). Finally, SVD2 also seems to capture some of this fixed effect behavior, but in a smaller order of magnitude.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The economic costs of conflict: A case study of the basque country</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Gardeazabal</surname></persName>
		</author>
		<idno type="DOI">https://www.aeaweb.org/articles?id=10.1257/000282803321455188</idno>
		<ptr target="https://www.aeaweb.org/articles?id=10.1257/000282803321455188" />
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="132" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthetic combinations: A causal inference framework for combinatorial interventions</title>
		<author>
			<persName><forename type="first">Abhineet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Vijaykumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Synthetic a/b testing using synthetic interventions</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.07691" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Anish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munther</forename><surname>Dahleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causal matrix completion</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matrix completion methods for causal panel data models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Doudchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khashayar</forename><surname>Khosravi</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2021.1891924</idno>
		<ptr target="https://doi.org/10.1080%2F01621459.2021.1891924" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">536</biblScope>
			<biblScope unit="page" from="1716" to="1730" />
			<date type="published" when="2021-05">may 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Matrix completion, counterfactuals, and factor analysis of missing data</title>
		<author>
			<persName><forename type="first">Jushan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.06677" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Matrix completion with noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Candes and Yaniv Plan</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/0805.4471" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The power of convex relaxation: Near-optimal matrix completion</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/0903.1476" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causality matters in medical imaging</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Daniel C Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3673</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual causal feature learning</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering the anticancer potential of non-oncology drugs by systematic viability profiling</title>
		<author>
			<persName><surname>Steven M Corsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">D</forename><surname>Nagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Spangler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Rossen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">G</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranad</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Humeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature cancer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decision-theoretic foundations for statistical causality</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="77" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Balancing, regression, difference-in-differences and synthetic control methods: A synthesis</title>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Doudchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Doubly robust nearest neighbors in factor models</title>
		<author>
			<persName><forename type="first">Raaz</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabina</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Predrag</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interventions and causal inference</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gale</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936-09">September 1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal inference in natural language processing: Estimation, prediction, interpretation and beyond</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Wood-Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1138" to="1158" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Matrix completion and low-rank svd via fast alternating least squares</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cell-specific prediction and application of drug-induced gene expression profiles</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Hodos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Chih</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaonan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Neil R Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ma'ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianying</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACIFIC SYMPOSIUM ON BIOCOMPUTING 2018: Proceedings of the Pacific Symposium</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Low-rank matrix completion using alternating minimization</title>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2009.263</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Amazon.com recommendations: item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIC.2003.1167344</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A potential outcomes calculus for identifying conditional path-specific effects</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v11/mazumder10a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">80</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instrumental variable estimation of nonparametric models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1565" to="1578" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ruiz Luyten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<title level="m">Transfer learning with kernel methods</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<idno type="DOI">10.1137/070697835</idno>
		<ptr target="https://doi.org/10.1137%2F070697835" />
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010-01">jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single world intervention graphs (swigs): A unification of the counterfactual and graphical approaches to causality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Center for the Statistics and the Social Sciences</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page">2013</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Identifying representations for intervention extrapolation</title>
		<author>
			<persName><forename type="first">Sorawit</forename><surname>Saengkyongam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web: methods and strategies of web personalization</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="291" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparing the chemical structure and protein content of chembl, drugbank, human metabolome database and the therapeutic target database</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Southan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorel</forename><surname>Muresan</surname></persName>
		</author>
		<idno type="DOI">10.1002/minf.201300103</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular informatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal structure learning: A combinatorial perspective</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal imputation via synthetic interventions</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="688" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causal discovery from changes</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Why are big data matrices approximately low rank?</title>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Townsend</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning prediction of chemical-induced dosedependent and context-specific multiplex phenotype responses and its application to personalized alzheimer&apos;s disease drug repurposing</title>
		<author>
			<persName><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1010367</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Exact tensor completion using t-svd</title>
		<author>
			<persName><forename type="first">Zemin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchin</forename><surname>Aeron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
