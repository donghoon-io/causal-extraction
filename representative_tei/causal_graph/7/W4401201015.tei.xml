<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-30">30 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuni</forename><surname>Susanti</surname></persName>
							<email>yuni.susanti@fujitsu.com</email>
							<idno type="ORCID">0009-0001-1314-0286</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">Artificial Intelligence Lab</orgName>
								<orgName type="institution">Fujitsu Ltd</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Färber</surname></persName>
							<email>michael.faerber@tu-dresden.de</email>
							<idno type="ORCID">0000-0001-5458-8645</idno>
							<affiliation key="aff1">
								<orgName type="institution">ScaDS.AI &amp; TU</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">KIT and ScaDS.AI/TU</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-30">30 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2407.18752v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal relation</term>
					<term>language model</term>
					<term>knowledge graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present "KG Structure as Prompt", a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the fundamental tasks in various scientific disciplines is to find underlying causal relationships and eventually utilize them <ref type="bibr" target="#b9">[10]</ref>. Causal discovery is a branch of causality study which estimates causal structures from observational data and generates a causal graph as a result. A causal graph, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, is a directed graph modeling the causal relationships between observed variables; a node represents a variable and an edge represents a causal relationship. Conventionally, causal discovery involves learning causal relations from observational data by measuring how changes in one variable are associated with changes in another variable, an approach referred to as covariancebased causal discovery <ref type="bibr" target="#b20">[21]</ref>. Driven by the recent advancements in LLMs, recent work has explored the causal capabilities of LLMs using metadata (e.g., variable names) rather than their actual data values. In other words, the causal relation is queried in natural language directly to the LLMs. This paper focuses on the latter, and to differentiate with covariance-based causal discovery, we refer to this approach as knowledge-based causal discovery, following the definition of <ref type="bibr" target="#b20">[21]</ref>.</p><p>Typically, such metadata-based causal reasoning is performed by Subject Matter Experts (SMEs) as they construct a causal graph, drawing from their expertise in domain-specific subjects and common sense <ref type="bibr" target="#b20">[21]</ref>, or based on literature surveys on subjects related to the variables. The advancement in LLMs has simplified this formerly challenging process, as LLMs are now capable of providing the knowledge that previously can only be provided by SMEs. Recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b43">45</ref>] also show promising results, notably, <ref type="bibr" target="#b20">[21]</ref> explores causal capabilities of LLMs by experimenting on cause-effect pairs. Their finding suggests that LLM-based methods achieved state-of-the-art performance on several causal benchmarks. Similarly, <ref type="bibr" target="#b43">[45]</ref> investigated the causal capability of LLMs by analyzing their behavior given a certain causal question. However, in contrast to <ref type="bibr" target="#b20">[21]</ref>, their result suggests that LLMs currently lack the capability to offer satisfactory answers for discovering new knowledge. Meanwhile, a work by <ref type="bibr" target="#b17">[18]</ref> focused on investigating the LLMs' capability for causal association among events expressed in natural language. Thus, their study is more oriented towards extracting a causal diagram (e.g., a chain of events) from unstructured text instead of discovering new causal relations.</p><p>In this paper, we investigate the capabilities of language models for knowledgebased causal discovery between variable pairs given a textual context from text sources. Specifically, given a pair of variables e 1 and e 2 , the task is to predict if a causal relation can be inferred between the variables. Therefore, similar to <ref type="bibr" target="#b17">[18]</ref>, our focus also lies in inferring causal relations from text rather than discovering new causal relations. In particular, we present "KG Structure as Prompt," a novel approach for integrating structural information from a Knowledge Graph (KG) into prompt-based learning with Small Language Models (SLMs). Promptbased learning adapts LMs for specific tasks by incorporating prompts-taskspecific instruction combined with the text input-to guide the models' output for the downstream tasks. Our approach enhances this method by incorporating additional information from KGs, leveraging the strengths of KGs in providing context and background knowledge. We opted for SLMs because a smaller model that can outperform larger models is more cost-effective and therefore preferable. We conduct experiments on three types of biomedical and an open-domain datasets, and further evaluate the performance of the proposed approach under three different architectures of language models.</p><p>To summarize, our main contributions are as follows:</p><p>1. We present "KG Structure as Prompt", a novel approach for injecting structural information from KGs into prompt-based learning. In experiments under few-shot settings, we demonstrate that our approach outperforms most of the no-KG baselines and achieves performance comparable to the conventional fine-tuning using a full dataset, even with limited samples. 2. We show that our approach is effective with different types of language model architectures and knowledge graphs, showcasing its flexibility and adaptability across various language models and knowledge graphs. 3. We demonstrate the robust capabilities of SLMs: fused with prompt-based learning and an access to a knowledge graph, SLMs are able to surpass an LLM with much larger number of parameters. <ref type="foot" target="#foot_0">2</ref>2 Background and Related Work</p><p>Small Language Models. Small Language Models (SLMs) refer to language models with fewer parameters, resulting in a reduced capacity to process text compared to larger-parameter LLMs. However, SLMs typically require less computation resources, making them faster to train and deploy, and maintaining them is generally more cost-effective. On the contrary, LLMs are trained on vast amounts of diverse data, thus have significantly more parameters and are capable of handling more complex language tasks than SLMs. Nevertheless, LLMs are expensive and difficult to train and deploy as they typically require more computational resource. For instance, GPT-3 <ref type="bibr" target="#b3">[4]</ref>, which consists of 175 billion parameters, is impractical to run on hardware with limited resources.</p><p>In this work, we define SLMs as LMs with less than 1 billion parameters. We explore the causal capability of SLMs with different architectures: (1) Masked Language Model (MLM) especially the encoder-only model, (2) Causal Language Model (CLM) or decoder-only language model, and (3) Sequence-to-Sequence Language Model (Seq2SeqLM) or encoder-decoder model. We provide an overview of each type of architecture below.</p><p>MLMs, especially encoder-only models such as BERT <ref type="bibr" target="#b7">[8]</ref>, are a type of LM that utilizes encoder blocks within the transformer architecture and are trained to predict masked tokens based on the context provided by surrounding words. They excel in natural language understanding (NLU) tasks, e.g., text classification, as they are able to capture relationships between words in a text sequence. CLMs, such as GPT-3 <ref type="bibr" target="#b3">[4]</ref>, use the decoder blocks within the transformer architecture and are trained to generate text one token at a time, by conditioning each token on the preceding tokens in the sequence. Meanwhile, Seq2SeqLMs, such as T5 <ref type="bibr" target="#b30">[32]</ref>, consist of both encoder and decoder blocks. The encoder transforms the input sequence into vector representation, while the decoder produces the output based on the encoded vector. CLMs and Seq2SeqLMs generally work well for natural language generation (NLG) and NLU tasks such as translation and summarization, as they can produce coherent and grammatically accurate sentences. We list our choice of language models in §5.</p><p>Prompt-based Learning &amp; Knowledge Injection. Research on classifying causal relations from text has predominantly occurred within supervised settings, utilizing classical machine learning (ML) approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> or fine-tuning pre-trained language models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b36">38]</ref>. Classical ML techniques often require extensive feature engineering and have shown inferior performance compared to fine-tuning language models such as BERT <ref type="bibr" target="#b7">[8]</ref>. Therefore, we evaluate our method against fine-tuning methods as baselines.</p><p>Meanwhile, prompt-based learning, also known as prompt-tuning, has recently emerged as a promising alternative to the conventional fine-tuning approach for a variety of Natural Language Processing (NLP) tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35]</ref>. Typically, a prompt is composed of discrete text (hard prompt); however, recent work has introduced soft prompt, a continuous vector that can be optimized through backpropagation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. In the relation classification task, promptbased learning often involves inserting a prompt template containing masked tokens into the input, essentially converting the task into masked language modeling or text generation problems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. This approach is particularly wellsuited for few-shot or zero-shot scenarios, where only limited labeled data is available <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">35]</ref>. This motivates us to investigate such prompt-based learning under few-shot settings, given the scarcity of datasets for our causal relation classification task.</p><p>Other works explore knowledge injection for the prompt construction, for instance, KnowPrompt <ref type="bibr" target="#b6">[7]</ref> injects latent knowledge contained in relation labels into prompt construction with learnable virtual words. KAPING <ref type="bibr" target="#b1">[2]</ref> retrieves top-K similar triples of the target entities from Wikidata and further augments them as a prompt. KiPT <ref type="bibr" target="#b23">[24]</ref> uses WordNet to calculate semantic correlation between the input and manually constructed core concepts to construct the prompts. Our work differs from them since we focus on leveraging structural information of knowledge graphs to construct the prompt (see §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Formulation</head><p>In this work, we focus on pairwise knowledge-based causal discovery: given a pair of entities e 1 and e 2 , i.e., variable or node pairs such as FGF6 and prostate cancer, the task is to predict if a causal relation can be inferred between the pair. We formulate the task as a binary classification task, classifying the relation as causal or non-causal. We evaluate our approach on a dataset D = {X , Y}, where X is a set of training instances and Y = {causal, non-causal} is a set of relation labels. Each instance x ∈ X consists of a token sequence x = {w 1 , w 2 , ...w |n| } and the spans of a marked variable pair, and is annotated with a label y x ∈ Y. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We illustrate our proposed approach in Fig. <ref type="figure" target="#fig_1">2</ref>. First, we generate a graph context, which is derived from the structural information of a knowledge graph with our KG Structure as Prompt method. Next, we feed the generated graph context and the inputs, i.e., the variable pair and its textual context, into the SLMs to train a prompt-based learning model. We elaborate our proposed approach in the following subsections. We start with preliminaries ( §4.1), followed by the design of the KG structure as Prompt for generating the graph context ( §4.2), and the incorporation of the generated graph context into the SLMs architecture with prompt-based learning ( §4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>Formally, we define a directed graph G = (V, E) where V is a set of vertices or nodes, and E ⊆ V × V is a set of directed edges. A knowledge graph is a specific type of directed graph representing a network of entities and the relationships between them. Formally, we define a knowledge graph as a directed labeled graph KG = (N, E, R, F) where N is a set of nodes (entities), E ⊆ N × N is a set of edges (relations), R is a set of relation labels, and F : E → R, is a function assigning edges to relation labels. For instance, assignment label r to an edge e = (x, y) can be viewed as a triple (x, r, y), e.g., (Tokyo, IsCapitalOf, Japan).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Graph Structure as Prompt</head><p>In the field of Graph Neural Networks (GNNs), <ref type="bibr" target="#b42">[44]</ref> explores whether LLMs can replace GNNs as the foundation model for graphs by using natural language to describe the geometric structure of the graph. Their results on several graph datasets surpass traditional GNN-based methods, showing the potential of LLMs as a new foundational model for graphs. Inspired by their success, we similarly leverage the structural information of a specific type of graph, i.e., a knowledge graph, to infer causal relationships between variable pairs. We select knowledge graphs due to their rich structured information and their capability to express interconnected relationships. We call our approach "Knowledge Graph Structure as Prompt".</p><p>For instance, we may infer a causal relationship by looking at multi-hop relations between a node pair in a knowledge graph, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. In Fig. <ref type="figure" target="#fig_2">3</ref>, the node FGF6 is indirectly connected to the node prostate cancer within one hop via the node FGFR4. As verified by a human expert, there is indeed a causal relation between the nodes FGF6 and prostate cancer. We argue that such graph structural information, in this example a path, adds background knowledge on top of internal knowledge of LMs, effectively assisting LMs in inferring causal relation between the variable pair. Specifically, we aim to use a natural language description of the structural information from the knowledge graph to be used as a prompt for prompt-based learning. We refer to such a description of knowledge graph structure as a graph context.</p><p>In this work, we specifically examine three kinds of vital structural information of a KG to be used as the graph context, namely (1) neighbor nodes, (2) common neighbor nodes, (3) metapath, described in detail as follows.</p><p>(1) Neighbor Nodes (N N ). The essence of GNNs lies in applying different aggregate functions to the graph structure, i.e., passing node features to neighboring nodes, where each node aggregates the feature vectors of its neighbors to further update its feature vector. Thus, it is evident that the neighbor nodes are the most crucial feature within a graph. Inspired by that, we examine the neighboring nodes of the target node pairs to infer their causal relationship.</p><p>Formally, a node x is a neighbor of a node y in a knowledge graph KG = (V, E) if there is an edge {x, y} ∈ E. We provide an example of neighbor nodes from Wikidata <ref type="bibr" target="#b38">[40]</ref> in Fig. <ref type="figure" target="#fig_3">4</ref>. According to the provided example, the node prostate cancer has urology as one of its neighbor nodes, while FGF6 has urinary bladder as one of its neigbors. Thus, it is likely that a connection exists between the node pair (FGF6, prostate cancer) due to their respective neighboring nodes: urinary bladder ↔ urology. For utilizing the neighbor nodes structure in the prompt, we describe it in natural language to form a graph context C, which we formally denote as:</p><formula xml:id="formula_0">C(x, V, E) = {x} "is connected to" {[x 2 ] x2∈V x 2 }<label>(1)</label></formula><p>We also create a variation of C where we include the edge description/relation labels E, formally denoted as follows:</p><formula xml:id="formula_1">C(x, V, E) = {x} "has" {E x,x2 } "relation with" {[x 2 ] x2∈V x 2 }<label>(2)</label></formula><p>where V x k represents the list of node x's k-hop neighbor nodes and E x,x2 represents the relation or edge description between the node x and its neighbor nodes. The additional template words such as "is connected to" and "has relation with" are optional, and can be replaced by any other fitting words. Then, with Eq. 1, the generated graph context C for the node prostate cancer in Fig. <ref type="figure" target="#fig_3">4</ref> is shown in the following Example 1.</p><p>Example 1. prostate cancer is connected to nilutamide, cabazitaxel, urology, FSHR, F6F10</p><p>When including the relation labels (Eq. 2), the graph context would be:</p><p>Example 2. prostate cancer has drug or therapy used for treatment relation with nilutamide and cabazitaxel, has genetic association with FSHR and F6F10</p><p>(2) Common Neighbor Nodes (CN N ). Unlike neighbor nodes, common neighbor nodes capture the idea that the more neighbors a pair of nodes (x, y) shares, the more likely it is for the pair to be connected, i.e., there exists an edge e = x, y between them. We argue that common neighbors between two nodes help infer their relationship, so we examine the common neighbors information between the node pair as graph context, as well. Fig. <ref type="figure" target="#fig_4">5</ref> shows an example of common neighbor nodes for the pair (breast cancer, ERBB2), taken from Hetionet knowledge graph <ref type="bibr" target="#b16">[17]</ref>. According to the provided example, the pair has in total 95 common neighbors, confirming a close relationship between them.</p><p>Formally, common neighbors between the nodes x and y can be defined as: where N (x) is the set of nodes adjacent to node x (the neighbors of x), and N (y) is the set of nodes adjacent to node y (the neighbors of y). Subsequently, the graph context C for describing the common neighbors between the pairs x and y can be formed as follows:</p><formula xml:id="formula_2">CN {x, y} = N (x) ∩ N (y)<label>(3)</label></formula><p>C(x, CN , y) = "Common neighbor nodes of {x} and {y} are" :</p><formula xml:id="formula_3">{[n] n∈CN } (4)</formula><p>where CN represents the list of common neighbor nodes of the pair x and y as defined in Eq. 3. Again, the additional template words "Common neighbor nodes of..." are optional and can be replaced by other words. Then, we can generate the graph context C including the common neighbor nodes information for the pair in Fig. <ref type="figure" target="#fig_4">5</ref>, as follows:</p><p>Example 3. Common neighbor nodes of breast cancer and ERBB2 are: ADH5, mammary gland, exemestane, TGFBR2, DPYSL2</p><p>(3) Metapath (MP). Metapaths, or meta-paths are sequences of node types which define a walk from an origin node to a destination node <ref type="bibr" target="#b35">[37]</ref>. The term "metapath" in this work is borrowed from the biomedical domain, referring to specific node type combinations thought to be informative <ref type="bibr" target="#b29">[30]</ref>. Due to its importance in biomedical network analysis <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b41">43]</ref>, we investigate the metapaths of two nodes for inferring their causal relationship. Moreover, causal relationships are frequently observed in the biomedical domain. Fig. <ref type="figure">6</ref> shows examples of metapaths of the pair prostate cancer and FGF6.</p><p>Formally, a metapath MP can be defined as a path As explained by the example provided in Fig. <ref type="figure" target="#fig_2">3</ref>, we argue that an indirect path between two nodes can be useful for inferring a causal relation between two pairs, even when the edge itself does not describe a causal relation. Thus, an indirect metapath, or combination of meaningful node types could be latent evidence of causality between a pair of variables. We describe the metapath structure from KG in natural language to form a graph context C, as follows:</p><formula xml:id="formula_4">Z 1 R1 --→ Z 2 R2 --→ ... Rn --→ Z n+1 describing</formula><formula xml:id="formula_5">C(x, y, MP{V, E}) = {x} "is connected to" {y} "via the following path: "{v} {E v,v2 } {v 2 }<label>(5)</label></formula><p>where MP{V, E} is a metapath MP with length n containing a set of nodes V (v 1 , v 2 , ...v |n| ) and a set of edges E(v 1 , v 1+n ). Additional tokens "is connected to..." are optional and can be replaced by other tokens. Then, the graph context C with metapath information for the example in Fig. <ref type="figure">6</ref> would be:</p><p>Example 4. FGF6 is connected to prostate cancer via the following paths: FGF6 expressed in tendon, tendon expresses SQRDL, FGFR2 regulates SQRDL, FGFR2 associates with prostate cancer To prevent bias in prediction by the LMs, we avoid the direct path. For instance, for the pair (x, y), we avoid the path MP = (x, y) and MP = (y, x), when such path exists in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prompt-based learning with graph context</head><p>As illustrated in the model architecture in Fig. <ref type="figure" target="#fig_1">2</ref>, we feed the textual context into SLMs together with the graph context generated with KG Structure as Prompt. We further design a prompt-based learning approach utilizing both contexts elaborated in this section. To get a clear distinction between conventional fine-tuning and our proposed prompt-based learning approach, we first provide a short overview of the conventional fine-tuning approach, as follows.</p><p>Given a pre-trained LM L to fine-tune on a dataset D, the conventional fine-tuning method encodes the training sequence x = {w 1 , w 2 , ...w |n| } into the corresponding output hidden vectors of the LMs h = {h 1 , h 2 , ...h |n| }. For MLMs such as BERT <ref type="bibr" target="#b7">[8]</ref>, the special token "[CLS]" is inserted at the beginning of the sequence, and this special token is used as the final sequence representation h ′ , since it is supposed to contain information from the whole sequence. A fully-connected layer and a softmax layer are further applied on top of this representation to calculate the probability distribution over the class set Y, as follows.</p><formula xml:id="formula_6">p = sof tmax(W f h ′ + b f )<label>(6)</label></formula><p>Prompt-based learning, on the other hand, adapts the pre-trained LMs for the downstream task via priming on natural language prompts-pieces of text that are combined with the input and fed to the LMs to produce an output for downstream tasks <ref type="bibr" target="#b0">[1]</ref>. Concretely, we first convert each input sequence x with a template T to form a prompt x ′ : T : x → x ′ . In addition, a mapping function M is used to map the downstream task class set Y to a set of label words V constituting all vocabularies of the LM L, i.e., M : Y → V. As in the pretraining of LMs, we further insert the special token "[MASK]" into x ′ for L to fill with the label words V. We provide an example of the prompt formulation below. Given x ="Smoking causes cancer in adult male.", we set a template T , e.g.,</p><formula xml:id="formula_7">T = [x] "It shows [MASK] relation."</formula><p>Then, the prompt x ′ would be:</p><p>x ′ = "Smoking causes cancer in adult male. It shows [MASK] relation."</p><p>We further feed the prompt x ′ into L to obtain the hidden vector h Here, the mapping function can also be set manually e.g., M(true) = "causal" and M(f alse) = "non-causal". Note that depending on the task, dataset, and the prompt design, the class labels themselves can be used directly without any mapping function M.</p><p>In this study, our prompt-based learning combines the input sequence x with the graph context C into the prompt x ′ , as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, we formulate the prompt x ′ to include the following elements:</p><p>(1) textual context: input sequence x containing the pair, (2) graph context C: context generated from KG structures as described in §4.2, (3) target pair: pair e 1 and e 2 as the target, e.g., (FGF6, prostate cancer), (4) [MASK] token, (5) (optional ) template tokens. Subsequently, our final prompt x ′ as the input to the LM for the pair e 1 and e 2 can be formally defined as:</p><formula xml:id="formula_8">x ′ = [x] [C] The pair [e 1 ] and [e 1 ] shows a [MASK] relation. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>In this study, we select three SLMs, one for each of the three architectures: MLM, CLM, Seq2SeqLM. Since each type of SLMs is trained differently, we design the prompt x ′ differently across each type of SLMs. For instance, the prompt x ′ in Eq. 7, which is a cloze-style task prompt, suits the MLM architecture, since this model is trained to be able to see the preceding and succeeding words in texts. As for CLM and Seq2SeqLM, we cast the task as a generation-type, with prompt x ′ such as:</p><formula xml:id="formula_10">x ′ = [x] [C] The pair [e 1 ]</formula><p>and [e 2 ] shows a causal relation:</p><formula xml:id="formula_11">[MASK].<label>(8)</label></formula><p>As mentioned earlier, the design of the mapping function M to map the output into the downstream task labels varies depending on the task, dataset, and the prompt design. For instance, with the prompt x ′ as in Eq. 7, we can directly use the class labels set Y = {causal, non-causal} without any mapping function. Meanwhile, for prompt x ′ in Eq. 8, we manually define a mapping function, e.g., M(causal) = "true" and M(non-causal) = "f alse". Note that the template "The pair shows..." is optional and can be replaced with other text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Experiment Settings. We evaluate the proposed approach under few-shot settings, using k = 16 training samples across all experiments. Precision (P), Recall (R), and F1-score (F1) metrics are employed to evaluate the performance. Since fine-tuning on low resources often suffers from instability and results may change given a different split of data <ref type="bibr" target="#b33">[35]</ref>, we apply 5-fold cross-validation and the metric scores are averaged. We restrict the number of contents from the KG structures to be included in the prompt since the length of the prompt for SLMs is limited, and we restrict the number of hops when querying the KG, as well. We experimented with different settings and reported the best performing models. Additional technical details are provided online as supplementary materials.</p><p>Datasets. The evaluation datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>. Causality is often observed in the biomedical domain, thus we primarily evaluate our approach within this field, supplemented by an open-domain dataset. Each instance in the dataset comprises textual context where a variable pair co-occurs in a text (see <ref type="bibr">Example 5 &amp; 6)</ref>, and is annotated by human experts to determine if there is a causal relation between the variables.</p><p>Example 5. FGF6 contributes to the growth of prostate cancer by activating... Example 6. The deadly train crash was caused by a terrorist attack.</p><p>Choice of SLMs. In this work, we define SLMs as LMs with less than 1 billion of total parameters. We experimented with SLMs with three different architectures, as follows.</p><p>(a) MLM: roberta <ref type="bibr" target="#b26">[27]</ref> model adapted to the biomedical domain, with 125 million parameters (biomed-roberta-base-125m <ref type="bibr" target="#b11">[12]</ref>), (b) CLM: bloomz-560m <ref type="bibr" target="#b28">[29]</ref> with 560 million parameters, (c) Seq2SeqLM: T5-base-220m <ref type="bibr" target="#b30">[32]</ref> model with 220 million parameters  <ref type="bibr" target="#b15">[16]</ref> biomedical 33,508 drug-drug causality COMAGC <ref type="bibr" target="#b21">[22]</ref> biomedical 820 gene-disease causality SEMEVAL-2010 Task 8 <ref type="bibr" target="#b14">[15]</ref> open-domain 10,717 general domain causality</p><p>Choice of KGs. We selected the following two KGs for the experiments:</p><p>(a) Wikidata <ref type="bibr" target="#b38">[40]</ref>, as a representation of the general-domain KG, (b) Hetionet <ref type="bibr" target="#b16">[17]</ref>, a domain-specific KG assembled from 29 different databases, covering genes, compounds, and diseases.</p><p>We selected Wikidata for its broad coverage of numerous subjects and topics. As a comparison, we selected biomedical-KG Hetionet since we primarily evaluate our approach on the datasets from this particular domain.</p><p>Model Comparison. We compare the following models: Models (1) to (4) represent the models trained without the graph context, i.e., the baselines, while models marked with "PBL" (model 5 to 7) are our proposed prompt-based learning method injected with graph context information from KGs.</p><p>(1) ICL: In-Context Learning refers to a prompting method where few demonstrations of the task are provided to the LLMs as part of the prompt <ref type="bibr" target="#b3">[4]</ref>. For this method, we selected GPT-3.5-turbo-instruct model by OpenAI, and provided k = 16 samples as demonstrations to query the model. (2) FT full : Conventional Fine-Tuning models trained using the full datasets.</p><p>(3) FT few-shot : Conventional Fine-Tuning models under few-shot k = 16 setting. (4) PT few-shot : Original Prompt Tuning <ref type="bibr" target="#b22">[23]</ref>  Some variants of the proposed approach, such as PBL MP-Wiki-few-shot , were omitted due to the computational expense of multi-hop querying in Wikidata, which often results in no usable metapaths for many pairs. In addition, to focus the discussion on the more effective models, we have omitted the results of less effective models, such as PBL N N -Het-few-shot . We provide the more complete results, including zero-shot prompting and classical ML experiment results, online as supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>Table <ref type="table" target="#tab_2">2</ref> &amp; 3 summarize the results. We report the averaged Precision (P), Recall (R), and F1 scores, including the standard deviation values of the F1 scores over the 5-folds cross-validation. We provide a summary of the primary findings ( §6.1), followed by analysis and discussion of the results ( §6.2).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Primary Findings</head><p>We listed the summary of the main results from Table <ref type="table" target="#tab_2">2</ref> below. To train a robust model that is able to generalize well given any KG structure, we opted to not optimize the content selection of the KG structures in the current experiments. For instance, when there are more than m metapaths for a pair, we randomly select m of them, m being a hyperparameter of the number of metapaths to be included as prompt.</p><p>In spite of that, our proposed approach achieved a relatively satisfactory performance, suggesting that rather than the content of the structure, the type of the structural information, i.e., N N vs. CN N vs. MP, is arguably more important based on the experiments.</p><p>MLM vs. CLM vs. Seq2SeqLM. For classification tasks, language models trained with MLM architecture are often preferred. This preference comes from the fact that MLMs are trained to consider both preceding and succeeding words, a crucial aspect for accurately predicting the correct class in a classification model. In line with this, the top-performing models trained on both full and fewshot datasets are based on the MLM architecture. The second best-performing models using full dataset are based on the Seq2SeqLM architecture, followed by those based on the CLM architecture. This is most likely because, similar to MLMs, Seq2SeqLMs also include encoder blocks and are trained to recognize the surrounding words <ref type="bibr" target="#b30">[32]</ref>. However, this trend slightly differs in experiments under few-shot settings, as the models based on CLM architecture outperformed those based on Seq2SeqLM architecture. Thus, selecting an appropriate architecture, specifically how the LMs are trained, is crucial when adapting the LMs for downstream tasks. As demonstrated by the outcomes of our experiments, LMs trained with the MLM architecture are generally more suitable for classification tasks than those with Seq2SeqLM and CLM architectures.</p><p>Wikidata vs. Hetionet. In the biomedical domain, the proposed approach injected with structural information from Hetionet demonstrates better performance in most experiments. This is expected considering the domain-specific nature of the dataset. Nevertheless, both Wikidata and Hetionet performed relatively well; the top-performing models for COMAGC and GENEC datasets are attained with Hetionet, while for DDI dataset are achieved with Wikidata. We also achieved 6.8 points of F1 score improvement on SEMEVAL dataset with Wikidata. This suggests that the proposed approach is rather flexible regarding the choice of KGs.</p><p>SLMs vs. LLMs. We selected OpenAI's GPT-3.5-turbo-instruct [31] as a representative of larger parameter-LLMs. However, OpenAI does not provide technical details such as the numbers of parameters; except the context windows which is 4,096 tokens in size for this model <ref type="bibr">[31]</ref>. This is much larger than our choice of SLMs with a maximum token length ranging from 128 to 512. To summary, the results demonstrate that the SLMs outperformed this model across all datasets in most experiment. This further shows the potential of SLMs: combined with prompt-based learning and access to KGs, the proposed approach outperforms LLMs with considerably larger size and parameters, with minimal training effort (few-shot). Note that we also provided k = 16 training samples as task demonstration to query the GPT model for a more fair comparison with the experiments under few-shot settings.</p><p>Typically, SLMs are trained on significantly less data compared to LLMs, which leads to reduced capacity and inferior performance in downstream tasks. Therefore, the graph context derived from the structural information of KG by our proposed KG Structure as Prompt approach effectively serves as an additional evidence of causality; in other words, it assists the SLMs to rely not only on their constrained internal knowledge, but also by enhancing their capacity through denser information sourced from the KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented "KG Structure as Prompt", a novel approach for integrating structural information from KGs into prompt-based learning, to further enhance the capability of Small Language Models (SLMs). We evaluated our approach on knowledge-based causal discovery tasks. Extensive experiments under few-shot settings on biomedical and open-domain datasets highlight the effectiveness of the proposed approach, as it outperformed most of the no-KG baselines, including the conventional fine-tuning method with a full dataset. We also demonstrated the robust capabilities of SLMs: in combination with promptbased learning and KGs, SLMs are able to surpass a language model with larger parameters. Our proposed approach has proven to be effective with different types of LMs architectures and KGs, as well, showing its flexibility and adaptability across various LMs and KGs.</p><p>Our work has been centered on discovering causal relationships between pairs of variables. In future work, we aim to tackle more complex scenarios by developing methods to analyze causal graphs with multiple interconnected variables, which will offer a deeper understanding of causalities.</p><p>Supplemental Material Statement: Datasets, source code, and other details are available online at <ref type="url" target="https://github.com/littleflow3r/kg-structure-as-prompt">https://github.com/littleflow3r/kg-structure-as-prompt</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall framework of our KG Structure as Prompt with prompt-based learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FGFR4Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of inferring a causal relationship in KG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of neighbor nodes for FGF6 (left) and prostate cancer (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of common neighbor nodes for the pair (ERBB2, breast cancer). Different colors of the nodes represent different node types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[MASK] of [MASK]. Next, with the mapping function M connecting the class set Y and the label words, we formalize the probability distribution over Y at the masked position, i.e., p(y|x) = p([MASK] = M(y)|x ′ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>as a baseline, which is essentially prompt-based learning without any graph context. (5) PBL N N -Wiki-few-shot : Our proposed Prompt-based Learning + KG Structure as Prompt using the neighbor nodes N N structure from Wikidata. (6) PBL CN N -Het-few-shot : Our proposed Prompt-based Learning + KG Structure as Prompt using the common neighbor nodes CN N structure from Hetionet. (7) PBL MP-Het-few-shot : Our proposed Prompt-based Learning + KG Structure as Prompt using the metapaths MP structure from Hetionet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(.03) (ours) PBL N N -Wiki-few-shot 82.0 85.0 83.5 (.02) 63.0 63.0 63.0 (.03) 70.2 85.0 76.6 (.03) (ours) PBL CN N -Het-few-shot 77.4 89.5 82.7 (.05) 54.5 54.4 54.4 (.02) 65.1 89.9 74.6 (.05) (ours) PBL MP-Het-few-shot 80.0 89.3 83.9 (.03) 60.5 60.5 60.5 (.02) 67.0 87.6 75.4 (.02)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>64.5 91.0 75.0 (.05) 63.0 83.0 69.3 (.06) 63.3 82.0 70.3 (.04) (ours) PBL N N -Wiki-few-shot 65.3 90.0 74.7 (.01) 66.5 76.0 70.3 (.02) 57.5 95.0 70.9 (.03) (ours) PBL CN N -Het-few-shot 65.4 86.3 73.4 (.02) 56.0 95.8 70.6 (.01) 56.2 96.0 70.6 (.02) (ours) PBL MP-Het-few-shot 73.3 82.0 75.6 (.03) 56.5 91.0 69.7 (.02) 60.6 92.9 72.9 (.03)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a relation R between node types Z and Z n+1 . The following examples illustrate metapaths of different path length n from Fig. 6.</figDesc><table><row><cell>FGF6</cell><cell cols="2">interact</cell><cell>FGFR4</cell><cell>associate</cell><cell>prostate cancer</cell></row><row><cell>express</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>associate</cell></row><row><cell>tendon</cell><cell>express</cell><cell>SDRDL</cell><cell cols="2">regulate</cell><cell>FGFR2</cell></row><row><cell cols="6">Fig. 6. Example of metapaths for the pair (FGF6, prostate cancer). Different colors</cell></row><row><cell cols="3">of the nodes represent different node types.</cell><cell></cell><cell></cell></row><row><cell cols="6">MP = (FGF6, FGFR4, prostate cancer), composed of node types {gene,</cell></row><row><cell cols="2">gene, disease}, with n = 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">MP = (FGF6, tendon, SDRDL, FGFR2, prostate cancer), composed</cell></row><row><cell cols="6">of node types {gene, anatomy, gene, gene, disease}, with n = 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset sizes and types.</figDesc><table><row><cell>dataset</cell><cell>domain</cell><cell>total instances</cell><cell>description</cell></row><row><cell>GENEC (ours)</cell><cell>biomedical</cell><cell>789</cell><cell>gene-gene causality</cell></row><row><cell>DDI</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on biomedical-domain datasets. Values in parenthesis are the standard deviations of F1 scores over 5-cv test folds. N N , CN N , MP indicate the KG structures used as graph context: neighbors nodes, common neighbors nodes, and metapath, respectively. bold: highest F1 scores per LMs architecture and per dataset, underline: F1 scores of the highest-performed models per dataset. (.02) 61.0 61.0 61.5 (.03) 84.0 98.9 90.7 (.04) (baseline) FT few-shot 87.0 71.0 76.8 (.02) 52.0 52.0 52.0 (.02) 64.9 87.0 73.9 (.07) (baseline) PT few-shot 87.0 77.2 80.6 (.03) 58.0 58.0 58.0 (.02) 69.6 83.0 75.4</figDesc><table><row><cell></cell><cell cols="2">COMAGC</cell><cell cols="2">GENEC</cell><cell>DDI</cell></row><row><cell></cell><cell>P R</cell><cell>F1</cell><cell>P R</cell><cell>F1</cell><cell>P R</cell><cell>F1</cell></row><row><cell cols="7">(baseline) ICL (GPT-3.5-turbo) 64.1 67.7 65.5 (.18) 55.4 77.2 62.6 (.16) 53.2 99.0 68.9 (.08)</cell></row><row><cell></cell><cell cols="5">*MLM architecture (biomed-roberta-base-125m)*</cell></row><row><cell>FT full</cell><cell cols="2">90.0 86.8 88.2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PBL N N -Wiki-few-shot 71.7 90.9 79.0 (.02) 54.1 90.0 67.4 (.01) 63.3 88.0 72.6 (.02) (ours) PBL CN N -Het-few-shot 65.7 94.7 77.2 (.05) 52.8 100 69.1 (.00) 60.4 93.9 72.8 (.05) (ours) PBL MP-Het-few-shot 76.9 89.0 81.8 (.04) 52.8 91.0 68.5 (.03) 65.5 90.8 74.8 (.05)</figDesc><table><row><cell></cell><cell>*CLM architecture (bloomz-560m)*</cell></row><row><cell>FT full</cell><cell>58.0 91.8 71.3 (.07) 53.8 73.0 60.7 (.07) 83.0 91.4 86.7 (.06)</cell></row><row><cell>(baseline) FT few-shot</cell><cell>73.4 85.0 77.6 (.04) 48.2 86.0 61.7 (.01) 72.0 61.5 66.1 (.04)</cell></row><row><cell>(baseline) PT few-shot</cell><cell>64.1 95.0 76.3 (.01) 51.3 97.0 67.6 (.01) 60.2 92.0 72.7 (.02)</cell></row><row><cell cols="2">(ours) *Seq2SeqLM architecture (T5-base-220m)*</cell></row><row><cell>FT full</cell><cell>88.5 78.0 82.6 (.07) 60.6 43.0 50.0 (.11) 96.3 83.0 88.8 (.04)</cell></row><row><cell>(baseline) FT few-shot</cell><cell>68.8 81.0 72.7 (.06) 48.3 67.0 55.5 (.01) 55.6 94.0 69.6 (.04)</cell></row><row><cell>(baseline) PT few-shot</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results for open-domain dataset: SEMEVAL-2010 Task 8 (.00) 51.7 98.0 67.6 (.02) 52.5 93.0 66.7 (.02) (ours) PBL N N -Wiki-few-shot 55.7 93.0 69.7 (.01) 53.4 91.2 67.4 (.00) 53.7 93.0 67.9 (.01) (a) Our proposed approach outperforms the no-graph context baseline models, in most of the experiments across different dataset domains: up to 15.1 points of improvement of F1 scores on biomedical datasets (55.5 -→ 70.6, GENEC dataset) and 6.8 points of improvement on open-domain dataset (61.1 -→ 67.9, SEMEVAL dataset). (b) Under few-shot settings with k = 16 training samples, our proposed approach generally achieves the second-best performance compared to FT full model, which is the conventional fine-tuning models trained with full datasets. Few even surpassed them, such as PBL N N -Wiki-few-shot model with MLM architecture (63.0 vs. 61.5) and PBL CN N -Het-few-shot model with CLM architecture, on GENEC dataset (69.1 vs. 60.7). (c) Our models based on SLMs with less that 1 billion parameters surpassed the ICL prompting method on much larger model across all datasets in most experiments, underlining the importance of KGs to support the constrained internal knowledge of smaller LMs. CN N vs. MP In our experiments, the KG structure metapath MP contributed the most to the top-performing models, while the neighbors nodes N N and common neighbors nodes CN N roughly exhibited comparable performance across models and datasets. The effectiveness of MP likely depends on the hop count between entity pairs in the dataset, i.e., the hop count is relatively high (2.8) for the COMACG dataset, where MP gave the best performance. Conversely, for the GENEC dataset, where the average MP hop is low (1.8), CN N and N N outperformed MP.</figDesc><table><row><cell></cell><cell>P R</cell><cell>F1</cell><cell>P R</cell><cell>F1</cell><cell>P R</cell><cell>F1</cell></row><row><cell cols="3">(baseline) ICL GPT-3.5-turbo 52.2 97.2 67.5 (.07)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MLM</cell><cell></cell><cell>CLM</cell><cell></cell><cell cols="2">Seq2SeqLM</cell></row><row><cell>FT full</cell><cell cols="6">94.8 88.7 91.6 (.02) 73.0 74.0 71.3 (.07) 82.8 77.0 79.4 (.02)</cell></row><row><cell>(baseline) FT few-shot</cell><cell cols="6">57.2 83.0 67.2 (.04) 50.2 99.0 66.6 (.01) 52.1 90.9 61.1 (.04)</cell></row><row><cell>(baseline) PT few-shot</cell><cell cols="2">51.9 94.0 66.9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p> with ICL<ref type="bibr" target="#b3">[4]</ref> prompting method</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large language models are few-shot clinical information extractors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.130" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12">Dec 2022</date>
			<biblScope unit="page" from="1998" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-augmented language model prompting for zero-shot knowledge graph question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.nlrse-1.7" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Dalvi Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Neves Ribeiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</editor>
		<meeting>the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-06">Jun 2023</date>
			<biblScope unit="page" from="78" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal relation extraction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Castell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2008/summaries/87.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008</title>
		<meeting>the International Conference on Language Resources and Evaluation, LREC 2008<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2008-06-01">26 May -1 June 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/1457" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. c0d6bfcb4967418bfb8ac142f64a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting causal relations on hiv drug resistance from literature</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">C</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Ó</forename><surname>Nualláin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Sloot</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-11-101</idno>
		<ptr target="https://doi.org/10.1186/1471-2105-11-101" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101</biblScope>
			<date type="published" when="2010-02">Feb 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental cue phrase learning and bootstrapping method for causality extraction using cue phrase and word pair probabilities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0306457305000580" />
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="662" to="678" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3511998</idno>
		<ptr target="https://doi.org/10.1145/3485447.3511998" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>WWW &apos;22, Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2778" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.acl-long.295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Review of Causal Discovery Methods Based on Graphical Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.3389/fgene.2019.00524</idno>
		<ptr target="https://doi.org/10.3389/fgene.2019.00524" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Chemical-induced disease relation extraction with various linguistic features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw042</idno>
		<ptr target="https://doi.org/10.1093/database/baw042" />
		<imprint>
			<date type="published" when="2016-04">2016 (04 2016</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.740" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative prompt tuning for relation classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-emnlp.231" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12">Dec 2022</date>
			<biblScope unit="page" from="3170" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ptr: Prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2666651022000183" />
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S10-1006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07">Jul 2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1532046413001123" />
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Systematic integration of biomedical knowledge prioritizes drugs for repurposing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lizee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brueggeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khankhanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Baranzini</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.26726</idno>
		<ptr target="https://doi.org/10.7554/eLife.26726" />
		<imprint>
			<date type="published" when="2017-09">sep 2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26726</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MIM-ICause: Representation and automatic extraction of causal relation types from clinical notes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartusiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fano</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-acl.63" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting causal knowledge from a medical database using graphical patterns</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S G</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075218.1075261</idno>
		<ptr target="https://doi.org/10.3115/1075218.1075261" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
	<note>ACL &apos;00</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic Extraction of Cause-Effect Information from Newspaper Text Without Knowledgebased Inferencing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S G</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kornfilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Oddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Myaeng</surname></persName>
		</author>
		<idno type="DOI">10.1093/llc/13.4.177</idno>
		<ptr target="https://doi.org/10.1093/llc/13.4.177" />
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="1998">12 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causal reasoning and large language models: Opening a new frontier for causality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.00050</idno>
		<idno>CoRR abs/2305.00050</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2305.00050" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comagc: a corpus with multi-faceted annotations of gene-cancer relations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-14-323</idno>
		<ptr target="https://doi.org/10.1186/1471-2105-14-323" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">323</biblScope>
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021-11">Nov 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">KiPT: Knowledgeinjected prompt tuning for event detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.169" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-10">Oct 2022</date>
			<biblScope unit="page" from="1943" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.acl-long.353" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
		<ptr target="https://doi.org/10.1145/3560815" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2023-01">jan 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of causal relations in biomedical scientific discourse</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mihăilă</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.1186/1475-925X-13-S2-S1</idno>
		<ptr target="https://doi.org/10.1186/1475-925X-13-S2-S1" />
	</analytic>
	<monogr>
		<title level="j">BioMedical Engineering OnLine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014-12">Dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.891</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.acl-long.891" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023. 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15991" to="16111" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metapaths: similarity search in heterogeneous knowledge graphs via meta-paths</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btad297</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btad297" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
	<note>btad297 (05 2023</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified textto-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Medicause: Causal relation modelling and extraction from medical publications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Reklos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meroño-Peñuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>D'souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bozzato</surname></persName>
		</author>
		<author>
			<persName><surname>Carriero</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3184/TEXT2KG_Paper_1.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text and the 1st International Workshop on Modular Knowledge co-located with 19th Extended Semantic Conference (ESWC 2022)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Hahmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename></persName>
		</editor>
		<meeting>the 1st International Workshop on Knowledge Graph Generation From Text and the 1st International Workshop on Modular Knowledge co-located with 19th Extended Semantic Conference (ESWC 2022)<address><addrLine>Hersonissos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05-30">May 30th, 2022. 2022</date>
			<biblScope unit="volume">3184</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.eacl-main.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Merlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Tsarfaty</surname></persName>
		</editor>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">Apr 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.naacl-main.185" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">Jun 2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Investigation of improving the pre-training and finetuning of bert model for biomedical relation extraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-022-04642-w</idno>
		<ptr target="https://doi.org/10.1186/s12859-022-04642-w" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2022-04">Apr 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pathsim: meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.14778/3402707.3402736</idno>
		<ptr target="https://doi.org/10.14778/3402707.3402736" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2011-08">aug 2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causal-evidence graph for causal relation classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Susanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uchino</surname></persName>
		</author>
		<idno type="DOI">10.1145/3605098.3635894</idno>
		<ptr target="https://doi.org/10.1145/3605098.3635894" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing, SAC 2024</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</editor>
		<meeting>the 39th ACM/SIGAPP Symposium on Applied Computing, SAC 2024<address><addrLine>Avila, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2024">April 8-12, 2024. 2024</date>
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.13819</idno>
		<idno>CoRR abs/2301.13819</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.13819" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
		<ptr target="https://doi.org/10.1145/2629489" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014-09">sep 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">deepdga: Biomedical heterogeneous network-based deep learning framework for disease-gene association predictions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIBM55620.2022.9995651</idno>
		<ptr target="https://doi.org/10.1109/BIBM55620.2022.9995651" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mondal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">December 6-8, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="601" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Can foundation models talk causality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Willig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zecevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Dhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.10591</idno>
		<idno>CoRR abs/2206.10591</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.10591" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MPGNN-DSA: A meta-path-based graph neural network for drug-side effect association prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIBM55620.2022.9995486</idno>
		<ptr target="https://doi.org/10.1109/BIBM55620.2022.9995486" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mondal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">December 6-8, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="627" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language is all a graph needs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.findings-eacl.132" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2024</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Graham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Purver</surname></persName>
		</editor>
		<meeting><address><addrLine>St. Julian&apos;s, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-03">Mar 2024</date>
			<biblScope unit="page" from="1955" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding causality with large language models: Feasibility and opportunities</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilmkil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.05524</idno>
		<idno>CoRR abs/2304.05524</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2304.05524" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
