<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uplift Modeling based on Graph Neural Network Combined with Causal Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-14">14 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haowen</forename><surname>Wang</surname></persName>
							<email>wanghaowen@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Xinyan</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yikang</forename><surname>Wang</surname></persName>
							<email>yikang.wang.21@ucl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
							<email>yangze.zhou@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Longhan</forename><surname>Zhang</surname></persName>
							<email>longhanz@zhejianglab.com</email>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jiangj@zhejianglab.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>Alipay</settlement>
									<country>AntGroup China UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Zhejiang Lab China</orgName>
								<orgName type="institution" key="instit1">Zhejiang University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<orgName type="institution" key="instit3">Haowen Wang</orgName>
								<address>
									<addrLine>Xinyan Ye Yikang Wang Yangze Zhou Zhiyi Zhang</addrLine>
									<settlement>Long-</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uplift Modeling based on Graph Neural Network Combined with Causal Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-14">14 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2311.08434v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>â€¢ Information systems â†’ Information systems applications</term>
					<term>â€¢ Computing methodologies â†’ Artificial intelligence</term>
					<term>Machine learning Uplift Modeling, Graph Neural Network, Causal Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Uplift modeling is a fundamental component of marketing effect modeling, which is commonly employed to evaluate the effects of treatments on outcomes. Through uplift modeling, we can identify the treatment with the greatest benefit. On the other side, we can identify clients who are likely to make favorable decisions in response to a certain treatment. In the past, uplift modeling approaches relied heavily on the difference-in-difference (DID) architecture, paired with a machine learning model as the estimation learner, while neglecting the link and confidential information between features. We proposed a framework based on graph neural networks that combine causal knowledge with an estimate of uplift value. Firstly, we presented a causal representation technique based on CATE (conditional average treatment effect) estimation and adjacency matrix structure learning. Secondly, we suggested a more scalable uplift modeling framework based on graph convolution networks for combining causal knowledge. Our findings demonstrate that this method works effectively for predicting uplift values, with small errors in typical simulated data, and its effectiveness has been verified in actual industry marketing data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Uplift modeling <ref type="bibr" target="#b7">[8]</ref> has traditionally relied on randomized experiments, such as randomized controlled trials (RCTs) <ref type="bibr" target="#b18">[19]</ref>, in which customers are randomly allocated to either receive or not receive the intervention. In such instances, obtaining an accurate and interpretable estimate from observational data becomes critical. However, carrying out such an experiment in a business context frequently results in several challenges, including high costs in terms of time and money, uneven intervention distribution, and selection bias in the specific population.</p><p>Response modeling or outcome prediction uses supervised learning models to model the relation between features and target variables to predict response variation. Although response modeling is typically preferable to random targets, distinguishing between treatment-induced be-behavioral changes is often challenging. The population that should be targeted is the one most likely to respond positively to the intervention. As a result, a thorough knowledge of the behavioral changes that occur after the intervention is essential. Uplift modeling simulates the causal effect between the intervention and the outcomes based on response modeling. Causal inference frameworks and machine learning models are corporated to provide accurate forecasts and optimized performance on intuitive metrics.</p><p>The counterfactual nature of intervention data is central to causal inference in Rubin's Potential Outcome Framework (POF) <ref type="bibr" target="#b14">[15]</ref>. This characteristic pertains to a person's inability to both receive and refuse intervention. This means that the effects of many therapies cannot be seen in the same person. Two frameworks that have been extensively examined for causal impact estimations based on this counterfactual characteristic are the meta-learner framework <ref type="bibr" target="#b16">[17]</ref> and the customised machine learning model-based framework <ref type="bibr" target="#b3">[4]</ref>. The ultimate goal is to increase the accuracy of causal impact estimation through the use of feature engineering and validation approaches such as PS matching <ref type="bibr" target="#b2">[3]</ref>, weighting <ref type="bibr" target="#b10">[11]</ref>, feature representation <ref type="bibr" target="#b12">[13]</ref>, and so on.</p><p>In the past, researchers in uplift modeling were largely concerned with how to employ unbiased data and models in the estimation framework. We increased the amount of data information by defining causal knowledge and implementing structured representation, then used a graph convolution neural network <ref type="bibr" target="#b20">[21]</ref> to efficiently and directionally integrate feature neighborhood information, achieving excellent performance in uplift modeling and prediction tasks. The following is a description of our paper's contribution to methodological and empirical evaluation perspectives:</p><p>â€¢ First, we propose to use conditional average treatment effect (CATE) as the attribute representing the causal information of the feature and as part of uplift modeling and propose a causal network model framework to effectively calculate it based on knowledge distilling and double machine learning.</p><p>â€¢ Second, we propose to learn the causal diagram structure of the data before uplift modeling and reconstruct the data according to the learned adjacency matrix.</p><p>â€¢ Third, we propose an uplift modeling estimator based on graph convolution neural networks, which can integrate and characterize neighborhood feature attributes according to the cause and effect diagram structure and improve the performance of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The estimation of the uplift value in uplift modeling is often based on the Potential Outcome Framework(POF) <ref type="bibr" target="#b14">[15]</ref>. The individual treatment effect(ITE) can be expressed as:</p><formula xml:id="formula_0">ğ¼ğ‘‡ ğ¸ : ğœ (ğ‘–) = ğ‘Œ ğ‘– (1) -ğ‘Œ ğ‘– (0)<label>(1)</label></formula><p>Shere ğ‘Œ ğ‘– (1) and ğ‘Œ ğ‘– (0) represents the result of the outcome variable under the treatment condition and control condition, respectively, for individual ğ‘–, ğœ (ğ‘–) is the ITE value.</p><p>Considering that the individual effect of treatment will vary from individual and the high cost of marketing experiments in the industry, the conditional average treatment effect(CATE) is proposed as the effect of treatment on subgroups evaluated by the conditional average treatment effect (CATE), which is calculated by:</p><formula xml:id="formula_1">ğ¶ğ´ğ‘‡ ğ¸ : ğœ ğ‘– = ğ¸ [ğ‘Œ ğ‘– (1) | ğ‘‹ ğ‘– ] -ğ¸ [ğ‘Œ ğ‘– (0) | ğ‘‹ ğ‘– ]<label>(2)</label></formula><p>where ğ‘‹ ğ‘– is the feature vector for individual ğ‘–.</p><p>For the estimation of CATE and ITE, the most direct method is to make an unbiased adjustment to the regression model. Series of meta learners represented by s-learner are designed based on the concept, that is, train one or more models with y as the output training target, input T and X, and get the change of Y by changing the value of T to estimate ITE and CATE.</p><formula xml:id="formula_2">ğœ (ğ‘¥) = ğ¸ [ğ‘Œ ğ‘– (1) -ğ‘Œ ğ‘– (0)|ğ‘‹ ] = ğ¸ [ğœ ğ‘– |ğ‘‹ ]<label>(3)</label></formula><p>Another series of methods for uplift modeling prediction is the probability score matching (PSM) <ref type="bibr" target="#b2">[3]</ref> method based on randomized controlled trials (RCT) <ref type="bibr" target="#b18">[19]</ref>. By calculating the probability score ğ‘ƒ (ğ‘¡ | ğ‘¥), each sample is given a different treatment object according to its similarity, so for sample ğ‘–, we find sample ğ‘—:</p><formula xml:id="formula_3">argmin ğ‘— dist(ğ‘–, ğ‘—) = ğ‘ƒ (ğ‘¡ | ğ‘¥ ğ‘– ) -ğ‘ƒ ğ‘¡ | ğ‘¥ ğ‘—<label>(4)</label></formula><p>Then CATE could be calculated by:</p><formula xml:id="formula_4">Ï„ = 1 ğ‘› âˆ‘ï¸ ğ‘–:ğ‘¡ ğ‘– =1 ğ‘¦ ğ‘– -ğ‘¦ ğ‘— + âˆ‘ï¸ ğ‘–:ğ‘¡ ğ‘– =0 ğ‘¦ ğ‘— -ğ‘¦ ğ‘–<label>(5)</label></formula><p>In addition, the industry's research on lift modeling also includes methods based on the Covariate Balancing Method and Modeling Unobserved Confounder. Typical methods of the first category include Inverse Probability of Treatment Weighting (IPTW) <ref type="bibr" target="#b6">[7]</ref>, Entropy Balancing (EB) <ref type="bibr" target="#b9">[10]</ref>, and Approximate Residual Balancing (ARB) <ref type="bibr" target="#b0">[1]</ref>, in which core is how to re-assign weights to samples. The core of the second type of method is to model the confounder. One way is to model the instrumental variable, which is represented by the two-stage least square (2SLS) method <ref type="bibr" target="#b1">[2]</ref>. The first stage is to fit the impact of the change of I on T, and the second stage is to fit the impact of the change of T on y caused by the change of I. The other way is to use deep learning to represent the confounder, such as SITE <ref type="bibr" target="#b19">[20]</ref>, Dragonnet <ref type="bibr" target="#b17">[18]</ref>, and CEVAE <ref type="bibr" target="#b11">[12]</ref>.</p><p>The past research mainly focused on adjusting and optimizing the uplift value estimation model in a structured or unstructured way. Estimation methods based on the foundation model have shown us the importance of embedding causal structure knowledge into the estimation process. This paper will try to conduct data mining on features. On the one hand, it expands the amount of information by defining and applying causal information; on the other hand, it reconstructs structured origin data through causal diagram structural information and uses GCN to learn neighborhood information from unstructured reconstructed data to improve the performance of uplift modeling using the framework of meta learner <ref type="bibr" target="#b16">[17]</ref>.</p><p>The structure of the paper is as follows. Section II reviews the critical concepts of uplift modeling and frameworks of the learning approach. In Section III, we introduce the methodology of our causal knowledge framework. Section IV evaluates these methods with both synthetic and real-world data. Finally, Section V summarizes the findings and recommends future research for uplift modeling applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>This section will introduce the calculation method and architecture of graph neural networks embedded with causal knowledge. We propose an interpretable causal graph network representation learning framework with features as nodes. It can expand the representation of features by node embedding, mapping the originally scalar features into a high-dimension space, and then integrating the causal information and structural information into the graph features through graph convolution to achieve a more accurate estimation of uplift value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Knowledge Representation</head><p>We propose a framework for computing causal knowledge representation. We transfer knowledge through the concept of the soft target in knowledge distillation as the estimation target of the causal estimator. We estimate each feature's causal average treatment effect(CATE) and take it as the weight of the feature based on the causal effect. This work has been proven to obtain more information. Figure <ref type="figure" target="#fig_0">1</ref> shows the architecture of the causal average treatment effect(CATE) calculation. Firstly, in the module of knowledge distillation and representation, We will build a knowledge distillation task for label Y(0/1), using the teacher model(XGBoost <ref type="bibr" target="#b4">[5]</ref>, etc. as base regressor) to get the probability Å¶ as the soft label to replace Y as the target label. Secondly, in the multi-head causal weight calculation module, we establish a causal graph for each feature as shown in Figure <ref type="figure">2</ref>, use the soft label Å¶ got in Module 1 as a knowledge label, and estimate CATE in the framework of double machine learning (DML) <ref type="bibr" target="#b5">[6]</ref>. Since the CATE estimation of each feature is independent, we designed a multi-head mechanism to make the calculation more efficient. Double machine learning is a classic estimator to estimate (heterogeneous) treatment effects when treatment is classified and all potential confounders/controls. DML makes the following structural equation assumptions for the data generation process:</p><formula xml:id="formula_5">ğ‘Œ = ğœƒ (ğ‘‹ ) â€¢ ğ‘‡ + ğ‘”(ğ‘‹,ğ‘Š ) + ğœ– E[ğœ– | ğ‘‹,ğ‘Š ] = 0 (6) ğ‘‡ = ğ‘“ (ğ‘‹,ğ‘Š ) + ğœ‚ E[ğœ‚ | ğ‘‹,ğ‘Š ] = 0 (7) E[ğœ‚ â€¢ ğœ– | ğ‘‹,ğ‘Š ] = 0<label>(8)</label></formula><p>After modeling Y and T, respectively, the estimated CATE value ğœƒ (ğ‘‹ ) satisfies the equation: </p><formula xml:id="formula_6">á»¸ = ğœƒ (ğ‘‹ ) â€¢ T + ğœ– (<label>9</label></formula><p>D is the given data, M is the number of training samples, g is the given structure, Dim[g] is the number of independent parameters of model g, Î¸ is the maximum likelihood estimate of the parameter given the structure g and the data D.</p><p>After determining the scoring function, here we use the hillclimbing <ref type="bibr" target="#b15">[16]</ref> algorithm as the optimization algorithm for the structural learning problem. The Hill-climbing algorithm is a classical algorithm for local search based on a greedy algorithm, starting with a candidate solution and continuing to search in its neighborhood until there is no better solution. The steps of the local search algorithm are described as follows: Firstly, initialize a feasible solution X. Secondly, select a moved solution s (x) in the neighborhood of the current solution so that f (s (x)) &lt; f (x), s (x) in S (x). If there is no such solution, X is the optimal solution, and the algorithm stops. Thirdly, make x = s (x) and repeat the second step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNN based uplift modeling</head><p>After Causal Knowledge Representation and Causal Graph Structure Learning, we obtained more information about the dataset and a specific relationship between features. Considering the excellent representation ability of graph neural networks, we propose a graph neural network representation framework based on causal graph representation, which can integrate this information more efficiently.</p><p>GCN <ref type="bibr" target="#b20">[21]</ref> is a multi-layer neural network that can operate directly on the graph and induce nodes to obtain information on neighborhood vectors based on the neighborhood attributes of nodes. Consider a graph G = (V, e), where V (| V | = n) and E are the sets of nodes and edges, respectively. It is assumed that each node is connected to itself, that is, (v, v) âˆˆ ğ¸ for any v. Let x âˆˆ R ğ‘›Ã—ğ‘š be a matrix containing all N nodes and their vector features, where m is the dimension of the vector, and each row ğ‘¥ ğ‘£ âˆˆ R ğ‘š are the vectors of V. We introduce the adjacency matrix A and its degree matrix D of G, where ğ‘‘ ğ‘–ğ‘– = ğ‘— ğ´ ğ‘– ğ‘— . Due to the characteristics of the self-circulation hypothesis, the diagonal element of a is set to 1. In general, GCN can only capture information about its neighbors through one layer of convolution. We can integrate information about a wider range of neighbors by stacking multiple GCN layers:</p><formula xml:id="formula_8">ğ» (ğ‘™+1) = ğœ D -1 2 Ã‚ D -1 2 ğ» (ğ‘™ ) ğ‘Š (ğ‘™ )<label>(14)</label></formula><p>Here ğ» (ğ‘™+1) and ğ» (ğ‘™ ) are the output and input matrices. Ã‚ = ğ´+ğ¼ , where A is the adjacency matrix, and I is the identity matrix. D is the degree matrix of Ã‚, D -1 2 Ã‚ D -1 2 is the normalized symmetric adjacency matrix, and ğ‘Š (ğ‘™ ) âˆˆ R ğ‘šÃ—ğ‘˜ is a weight matrix. ğœ is an activation function, e.g., a LeakyReLU.</p><p>Here we use GCN to extract and integrate features based on the causal neighborhood structure we learned in the previous step. We take advantage of the feature that GCN can efficiently fuse features according to the neighborhood structure to get graph embedding of each sample and then perform prediction tasks based on it. Figure <ref type="figure" target="#fig_2">3</ref> shows that we have expanded the information on each feature. In addition to the value of each feature itself, we have also expanded the information of structural features and causal weights. For the estimation of uplift value, we refer to the design method of S-learner in meta learner and use our GNN-based model as the base learner.</p><formula xml:id="formula_9">ğœ‡ 0 (ğ‘¥) = E[ğ‘Œ (ğ‘‡ = 0) | ğ‘‹ = ğ‘¥]<label>(15)</label></formula><formula xml:id="formula_10">ğœ‡ 1 (ğ‘¥) = E[ğ‘Œ (ğ‘‡ = 1) | ğ‘‹ = ğ‘¥]<label>(16)</label></formula><p>After ğœ‡ 0 and ğœ‡ 1 are calculated, respectively, the uplift value for each sample can be calculated:</p><formula xml:id="formula_11">D1 ğ‘– := ğ‘Œ 1 ğ‘– -Î¼0 ğ‘‹ 1 ğ‘–<label>(17)</label></formula><formula xml:id="formula_12">D0 ğ‘– := Î¼1 ğ‘‹ 0 ğ‘– -ğ‘Œ 0 ğ‘–<label>(18)</label></formula><p>Here D1 ğ‘– and D0 ğ‘– are the uplift values for samples in the intervention group and control group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Dataset</head><p>4.1.1 Synthetic dataset. We used a method to simulate the generation of a dataset containing individual treatment effects, which is available in causalml. In Chen et al. <ref type="bibr" target="#b3">[4]</ref> research, it is used as a method to provide simulated data, which is available in Causalml. This synthetic method in the study provides the test groundings for estimating individual treatment effects and facilitating validation. The following is the generating mechanism: for different choices of X-distribution ğ‘ƒ ğ‘‘ , there is dimension ğ‘‘, noise level ğœ, propensity function ğ‘’ * (â€¢), baseline primary effect ğ‘ * (â€¢), and treatment effect function ğœ * (â€¢). The distributions and relations are mathematically expressed in terms:</p><formula xml:id="formula_13">ğ‘‹ ğ‘– âˆ¼ ğ‘ƒ ğ‘‘<label>(19)</label></formula><formula xml:id="formula_14">ğœ€ ğ‘– | ğ‘‹ ğ‘– âˆ¼ ğ‘ (0, 1)<label>(20)</label></formula><formula xml:id="formula_15">ğ‘Š ğ‘– | ğ‘‹ ğ‘– âˆ¼ Bernoulli ğ‘’ * (ğ‘‹ ğ‘– )<label>(21)</label></formula><formula xml:id="formula_16">ğ‘Œ ğ‘– = ğ‘ * (ğ‘‹ ğ‘– ) + (ğ‘Š ğ‘– -0.5) ğœ * (ğ‘‹ ğ‘– ) + ğœğœ€ ğ‘–<label>(22)</label></formula><p>The generation mechanism is featured by nuisance components and an easy treatment effect function. The initial distribution is constructed from ğ‘‹ ğ‘–1 âˆ¼ ğ‘ˆ ğ‘›ğ‘– ğ‘“ (0, 1) ğ‘‘ is constructed, followed by ğ‘’ * (ğ‘‹ ğ‘– = ğ‘¡ğ‘Ÿğ‘–ğ‘š 0.1 {ğ‘ ğ‘–ğ‘›(ğœ‹ğ‘‹ ğ‘–1 ğ‘‹ ğ‘–2 )} and ğœ * (ğ‘‹ ğ‘– ) = (ğ‘‹ ğ‘–1 + ğ‘‹ ğ‘–2 )/2 to compute propensity scores and treatment effects, respectively. The treatment ğ‘Š is generated as a binary distribution. Eventually, interval trimming of the distribution is performed via ğ‘¡ğ‘Ÿğ‘–ğ‘š(ğ‘¥) = ğ‘šğ‘ğ‘¥ {ğœ‚, ğ‘šğ‘–ğ‘›(ğ‘¥, 1-ğœ‚)}. This simulation method is adpoted as a scaled version of the Friedman <ref type="bibr" target="#b8">[9]</ref> function, where a baseline main effect is computed through ğ‘ * (ğ‘‹ ğ‘– ) = ğ‘ ğ‘–ğ‘›(ğœ‹ğ‘‹ ğ‘–1 ğ‘‹ ğ‘–2 ) + 2(ğ‘‹ ğ‘–1 -0.5) 2 + ğ‘‹ ğ‘–4 + 0.5ğ‘‹ ğ‘–5 .</p><p>4.1.2 Real-world dataset. We use the criteo uplift dataset <ref type="bibr" target="#b7">[8]</ref> as the evaluation of the real-world dataset, which is constructed by collecting data from the incremental test. It randomly divides the people into two categories, whether it is advertised or not. The criteo uplift dataset has 25 million rows, each representing a user with 11 characteristics, a treatment indicator, and two tags (click and conversion). Here we use conversion as the tag we focus on in uplift estimation. Figure <ref type="figure" target="#fig_3">4</ref> shows the results after learning the Bayesian network structure of the dataset. In this case, the actual causal effect of features can be calculated easily because the datasets are produced with a certain mechanism The absolute loss (Abs) is adopted to measure the deviation between the actual causal effect and the estimated causal effect. As for the prediction accuracy, the mean squared error (MSE) is adopted. The proposed method has been compared to traditional models like linear regression (LR), SVR, and XGBoost.    the GNN-based model performs much better when combined with the causal weighting. As for uplift modeling estimation, as shown in Figure <ref type="figure" target="#fig_5">6</ref>, causal weighting combined architecture has a much more apparent effect. Before combining causal weighting information, GNN based model is slightly better than LR and SVR in the estimation of uplift value but worse than xgboost while achieving a very accurate result when adopting the causal weighting combined architecture. Another result is that GNN based model can have a much more stable performance when the number of confounders increases, which means that it can have a much more robust performance when facing more complex situations. 4.2.2 Real-world dataset. In a real-world dataset, the actual causal effect of treatment remains unknown, leading to the abovementioned indicators being inapplicable. As a result, Area Under Uplift Curve (AUUC) is adopted to measure the performance of an uplifting model on the real-world dataset. AUUC can be calculated as follows:</p><formula xml:id="formula_17">ğ´ğ‘ˆğ‘ˆğ¶ (ğ‘“ ) = âˆ« 1 0 ğ‘‰ (ğ‘“ , ğ‘¥)ğ‘‘ğ‘¥ â‰ˆ ğ‘› âˆ‘ï¸ ğ‘˜=1 ğ‘‰ (ğ‘“ , ğ‘˜) (23) ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘‰ (ğ‘“ , ğ‘˜) = 1 |ğ‘‡ | âˆ‘ï¸ ğ‘– âˆˆ ğ‘“ ( D,ğ‘˜ ) ğ‘¦ 1 ğ‘– [ğ‘¡ ğ‘– =1] - 1 |ğ¶ | âˆ‘ï¸ ğ‘— âˆˆ ğ‘“ ( D,ğ‘˜ ) ğ‘¦ 1 ğ‘— [ğ‘¡ğ‘— =0]</formula><p>(24) Here ğ‘“ (D, ğ‘˜) can be the k first samples of the dataset when ordered by the prediction of the model ğ‘“ , |ğ‘‡ | is the number of samples in the treatment group(t=1), and |ğ¶ | is the number of samples in the control group(t=0).</p><p>For certain causal relationships, the higher the AUUC is, the better the uplifting model performs. The AUUC of the baseline models and ours are listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows that when estimating the uplift value in the realworld dataset, although origin GNN has a similar performance with LR and SVR, it has a better performance than XGBoost when with the causal weighting combined architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this work, we investigated how to describe causal information in uplift modeling (add conditional average treatment effect (CATE) and build an adjacency matrix using Bayesian network structure learning). In addition, we addressed how to incorporate this causal information into uplift estimations by proposing a framework for uplift modeling that is based on graph neural networks.</p><p>Experiments on simulated and real-world datasets reveal that while the origin graph convolutional neural network performs comparably to conventional approaches when directly predicting uplift values, when paired with causal neighbourhood features and causal representation information, it demonstrates exceptional performance in both the prediction job and the uplift estimation task of the target, owing to the GCN's excellent neighbourhood learning features.</p><p>It is worthwhile to investigate more methods of characterising causal knowledge in the future. Weighted adjacent matrices might be seen as a means of guiding graph convolutional neural networks to provide accurate data. Alternatively, it is equally intriguing to investigate the size of the receptive domain of neighbourhood features. A wider receptive domain denotes more information, which might aid us in enhancing the performance of this job in downstream prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Causal Weighting Calculation Framework</figDesc><graphic coords="3,96.90,332.96,155.90,85.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) Here á»¸ is the residual of Y, T is the residual of T. Considering E[ğœ– â€¢ ğœ‚ | ğ‘‹ ] = 0, the problem of estimating ğœƒ (ğ‘‹ ) can be transformed into the following regression problem. Î¸ = arg min ğœƒ âˆˆÎ˜ E ğ‘› ( á»¸ -ğœƒ (ğ‘‹ ) â€¢ T ) 2 (10)3.2 Causal Graph Structure LearningThe graph network structure contains the dataset's prior information. The connection relationship indicates the direction and distance of information transmission and determines the direction and degree of information sharing and transmission of nodes in the subsequent graph network characterization operation.Here, we use the classical Bayesian network structure as the structure of the causal feature representation graph. Scoring search is a standard method to solve the problem of Bayesian network structure to evaluate the degree of fit between the Bayesian network and training data and then find the optimal Bayesian network based on the scoring function. The goal is now to solve the following task: argmax ğº âˆˆğº score(ğº, D). (11) The scoring function introduces the inductive preference of what kind of Bayesian network you want to obtain. Here we use the Bayesian Information Criterion(BIC) [14] as the score function, which approximates the Bayes Dirichlet equivalent uniform(BDeu), sharing the critical property of decomposability. score(ğº, D) = âˆ‘ï¸ ğ‘‹ ğ‘– score (ğ‘‹ ğ‘– , Î  ğ‘– , D) (12) Score ğ‘ğ‘–ğ‘ (ğ‘” : ğ·) = ğ‘™ (( Î¸, ğ‘”) : ğ·) -log ğ‘€ 2 Dim[ğ‘”]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GNN-based uplift modeling architecture.</figDesc><graphic coords="4,53.80,451.41,240.24,148.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Correlation network between confounders, treatment, and outcome from the real-world dataset, CRITEO</figDesc><graphic coords="5,53.80,85.60,240.24,125.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mean squared error for y-prediction accuracy of base methods and ours with the numbers of confounders are 5, 9, and 20, respectively.</figDesc><graphic coords="5,71.87,371.02,204.09,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Absolute error of ITE of base methods and ours with the numbers of confounders are 5, 9 and 20, respectively.</figDesc><graphic coords="5,71.87,539.04,204.09,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>Figure5shows that the origin GNN-based model performs similarly to traditional models in the traditional regression task, while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AUUC for uplifting evaluation and MSE for yprediction of baseline methods and proposed method</figDesc><table><row><cell>Model</cell><cell>AUUC MSE</cell></row><row><cell>LR</cell><cell>0.4980 0.0026</cell></row><row><cell>SVR</cell><cell>0.5475 0.0037</cell></row><row><cell>XGBoost</cell><cell>0.8756 0.0025</cell></row><row><cell>GCN</cell><cell>0.5443 0.0028</cell></row><row><cell cols="2">GCN (Causal Weighting) 0.8807 5e-06</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Code Availability</head><p>The code that supports the findings of this study will be available on GitHub.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate residual balancing</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="597" to="623" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An alternative two stage least squares (2SLS) estimator for latent variable equations</title>
		<author>
			<persName><surname>Kenneth A Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="109" to="121" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Some practical guidance for the implementation of propensity score matching</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Caliendo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Kopeinig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic surveys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Causalml: Python package for causal machine learning</title>
		<author>
			<persName><forename type="first">Huigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Totte</forename><surname>Harinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong-Yoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11631</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Khotilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R package version 0</title>
		<imprint>
			<date type="published" when="1921-04">2015. 4-2 1, 4 (2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Double/debiased/neyman machine learning of treatment effects</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitney</forename><surname>Newey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="261" to="265" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to inverse probability of treatment weighting in observational research</title>
		<author>
			<persName><surname>Nicholas C Chesnaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vianda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Stel</surname></persName>
		</author>
		<author>
			<persName><surname>Tripepi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><forename type="middle">L</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmine</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kitty</forename><forename type="middle">J</forename><surname>Zoccali</surname></persName>
		</author>
		<author>
			<persName><surname>Jager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Kidney Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="14" to="20" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A large scale benchmark for uplift modeling</title>
		<author>
			<persName><forename type="first">Eustache</forename><surname>Diemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Betlei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Renaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multidimensional additive spline approximation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jerome H Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="291" to="301" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hainmueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Political analysis</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="25" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Balancing covariates via propensity score weighting</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kari</forename><forename type="middle">Lock</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="390" to="400" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Bayesian information criterion: background, derivation, and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Neath</surname></persName>
		</author>
		<author>
			<persName><surname>Cavanaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="199" to="203" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes: Design, modeling, decisions</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hill-climbing search</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of cognitive science</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta learning for causal direction</title>
		<author>
			<persName><forename type="first">Jean-FranÃ§ois</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9897" to="9905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DragonNet: a robust mobile internet service system for long-distance trains</title>
		<author>
			<persName><forename type="first">Po</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhuo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on mobile computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2206" to="2218" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Treatment of obsessions: A randomized controlled trial</title>
		<author>
			<persName><forename type="first">Maureen</forename><forename type="middle">L</forename><surname>Whittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><forename type="middle">R</forename><surname>Woody</surname></persName>
		</author>
		<author>
			<persName><surname>Peter D Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melisa</forename><surname>Sj Rachman</surname></persName>
		</author>
		<author>
			<persName><surname>Robichaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour research and therapy</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: a comprehensive review</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Social Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
