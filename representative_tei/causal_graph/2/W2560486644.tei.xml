<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Experts&apos; Knowledge for Structure Learning of Bayesian Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hossein</forename><surname>Amirkhani</surname></persName>
							<email>amirkhani@aut.ac.ir</email>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Rahmati</surname></persName>
							<email>rahmati@aut.ac.ir.</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J F</forename><surname>Lucas</surname></persName>
							<email>plucas@liacs.nl.</email>
						</author>
						<author>
							<persName><forename type="first">Arjen</forename><surname>Hommersom</surname></persName>
							<email>arjenh@cs.ru.nl.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering and Information Technology Department</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Iran. Presently</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">he is with the Technology and Engineering Department</orgName>
								<orgName type="institution">University of Qom</orgName>
								<address>
									<settlement>Qom</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Engineering and Information Tech- nology Department</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute for Computing and Information Sciences</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute for Computing and Information Sciences</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Science, Management &amp; Technology</orgName>
								<orgName type="institution">Open University</orgName>
								<address>
									<settlement>Heerlen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Experts&apos; Knowledge for Structure Learning of Bayesian Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2016.2636828</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2636828, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian networks</term>
					<term>structure learning</term>
					<term>experts&apos; knowledge</term>
					<term>experts&apos; accuracy</term>
					<term>marginalization-based score</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning Bayesian network structures from data is known to be hard, mainly because the number of candidate graphs is super-exponential in the number of variables. Furthermore, using observational data alone, the true causal graph is not discernible from other graphs that model the same set of conditional independencies. In this paper, it is investigated whether Bayesian network structure learning can be improved by exploiting the opinions of multiple domain experts regarding cause-effect relationships. In practice, experts have different individual probabilities of correctly labeling the inclusion or exclusion of edges in the structure. The accuracy of each expert is modeled by three parameters. Two new scoring functions are introduced that score each candidate graph based on the data and experts' opinions, taking into account their accuracy parameters. In the first scoring function, the experts' accuracies are estimated using an expectation-maximization-based algorithm and the estimated accuracies are explicitly used in the scoring process. The second function marginalizes out the accuracy parameters to obtain more robust scores when it is not possible to obtain a good estimate of experts' accuracies. The experimental results on simulated and real world datasets show that exploiting experts' knowledge can improve the structure learning if we take the experts' accuracies into account.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>B AYESIAN networks are a popular class of probabilistic graphical models that are applicable to many problems that are characterized by uncertainty concerning multiple variables and their relationships. At a qualitative level, the structure of a Bayesian network describes the relationships between random variables in the form of conditional independence relations. At a quantitative level, (local) relationships between random variables are described by (conditional) probability distributions, also called Bayesian network parameters. To apply Bayesian networks to a particular domain, it is first necessary to learn the Bayesian network structure and its parameters for that particular problem domain. Alternatively, one may design a Bayesian network structure based on experts' knowledge alone and then use either subjective estimates or statistical parameter estimation to obtain the Bayesian network. This paper focuses on the issue of how structure learning can benefit from available experts' knowledge.</p><p>During the last two decades, many Bayesian network structure learning algorithms have been proposed (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>). One of the most widely used class of structure learning algorithms are the score-based methods.</p><p>They attempt to identify the model that best fits the data by searching through the space of candidate models and selecting the one that obtains the highest score. The search is guided by various heuristics, very often hill-climbing <ref type="bibr" target="#b1">[2]</ref>, but genetic algorithms <ref type="bibr" target="#b7">[8]</ref> and particle swarm optimization <ref type="bibr" target="#b8">[9]</ref> have also been used. Typical scoring functions are the Akaike information criteria (AIC) <ref type="bibr" target="#b9">[10]</ref>, the Bayesian information criteria (BIC) <ref type="bibr" target="#b10">[11]</ref>, and the Bayesian Dirichlet equivalence uniform (BDeu) <ref type="bibr" target="#b1">[2]</ref> scores. The other common approaches, often referred to as the constraint-based methods <ref type="bibr" target="#b11">[12]</ref>, estimate from the data whether certain conditional independencies between the variables hold. Networks that are consistent with these independencies are selected.</p><p>There are several challenges a Bayesian network structure learning algorithm encounters when trying to discover a good model. First, the number of candidate graphs is super-exponential in the number of variables. More precisely, the number of DAGs with n variables is greater than 2 ( n 2 ) (i.e., the number of undirected graphs with n variables); the exact number of DAGs can be computed using Robinson's formula <ref type="bibr" target="#b12">[13]</ref>. Because of the huge search space, the learning problem is hard <ref type="bibr" target="#b13">[14]</ref>. This implies that for more than six variables, heuristic search is needed, and thus the globally optimal Bayesian network may not be found. This is complicated by the fact that in many practical learning settings, there is little data or the data are noisy, so that the score that is being used is not accurate. Furthermore, for most structures there are many different Markov equivalent graphs that encode the same independence relations, i.e., these structures cannot be distinguished based on data alone. These limitations generally lead to learned models that substantially differ from the true causal network of the underlying problem.</p><p>Given these limitations of Bayesian network structure learning, some researchers have proposed the use of experts' knowledge to bias the search procedure and reduce the complexity of the search space <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. A shortcoming in the majority of such methods is that they assume that there exists a completely reliable expert, and the expert's opinions about the structure are considered to be consistent with the true structure. It is obvious that in a real world setting, each expert may produce some errors in the provided opinions. In fact, it is more realistic to assume that we have to deal with multiple experts with varying levels of expertise rather than an omniscient expert.</p><p>In this paper, we propose two novel scoring functions to combine the available data with the knowledge from multiple, possibly unreliable, experts. The main advantages are that (i) it is not necessary to have a completely reliable expert, (ii) experts only have to label some of the edges (included in the graph, or not), and (iii) these scores can deal with conflicts between experts. In the first approach, we propose an expectation-maximization-based method for estimating the accuracy of each expert, then this information is explicitly used to score each structure based on both data and experts' opinions. In the second approach, we propose a Bayesian alternative by taking into account the uncertainty in the accuracy of each expert.</p><p>The first scoring function which is proposed in this paper, which we refer to as the explicit-accuracy-based score, builds upon the method originally proposed by <ref type="bibr" target="#b15">[16]</ref>. The main advantage of our approach is that we assume that experts are heterogeneous, i.e., different experts have different levels of accuracy. In addition, with our second score, referred to as the marginalization-based score, we are able to handle the problem that the estimated experts' accuracies may not be so reliable, and we obtain a more robust score by marginalizing out the experts' accuracy parameters. Experimental results reveal that exploiting experts' knowledge can improve the structure learning if we take the experts' accuracies into account. Specifically, if the experts' accuracies can be confidently estimated, it is suggested to explicitly use the estimated accuracies in the scoring process, otherwise, marginalizing out the accuracy parameters yields more robust scores.</p><p>The rest of this paper is organized as follows. In Section 2, we introduce the notations and preliminaries that will be used in subsequent sections. Specially, we present a three-parameter-based model of experts' accuracies in this section. Then, in Section 3, we clarify our problem setting based on some graphical models. Sections 4 and 5 present our scoring functions, i.e. explicit-accuracy-based score and marginalization-based score, respectively. Section 6 details our experimental procedures and presents the results. Finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we first introduce Bayesian networks and some Markov independence properties. Subsequently, we briefly review score-based Bayesian network structure learning. Finally, we present our three-parameter-based model of experts' accuracies, along with some further notations that will be used throughout the remainder of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian Networks</head><p>Formally, a Bayesian network, or BN for short, is a tuple B = (G, X , P ), with G = (V, E) a directed acyclic graph (DAG) with set of nodes V and directed edges or arcs E ⊆ V × V , X = {X 1 , . . . , X n } is a set of random variables with a 1-1 correspondence to V , and P is a joint probability distribution over X . An arc is denoted by (X i → X j ) ∈ E or (X j ← X i ) ∈ E. In the following we assume that the random variables X i are all discrete. According to the chain rule for Bayesian networks, P can be written as the product of the probabilities of the random variables, conditioned on their parents:</p><formula xml:id="formula_0">P (X 1 , . . . , X n ) = n i=1 P (X i | π(X i )),</formula><p>where π(X i ) is the set of parents of X i , i.e., the set {X j | (X j → X i ) ∈ E}. The number of values that these parents can take is denoted by q i = X j ∈π(X i ) r j , where r j is the number of values that X j can take.</p><p>The graph structure of G encodes a set of independence assumptions about P which is formalized by d-separation (directed separation): If a set of nodes X d-separates another set of nodes Y given a set of nodes Z, then X is independent of Y given Z, written as (X ⊥ Y | Z). D-separation is defined as follows. Let ρ be a trail in G, i.e., a path without considering the directions of the arcs. A trail ρ is said to be blocked by a set of nodes Z if and only if (at least) one of the following holds:</p><formula xml:id="formula_1">• ρ contains a chain U → Z i → W , such that Z i is in Z, • ρ contains a fork U ← Z i → W , such that Z i is in Z, • ρ contains a collider U → Z i ← W , such that neither Z i nor any descendant of Z i is in Z.</formula><p>Then, X and Y are said to be d-separated by Z if any trail between any node in X and any node in Y is blocked by Z. One of the special features of a Bayesian network is that, through the notion of collider, variables that are (conditionally) independent could become dependent by conditioning on the collider or one of its descendants. An undirected network, i.e., Markov random field, does not have this property <ref type="bibr" target="#b19">[20]</ref>.</p><p>If two graphs encode the same set of independencies, then we say that these graphs are Markov equivalent. To represent equivalence classes of DAGs, partially directed acyclic graphs (PDAGs) are employed, which are acyclic graphs with both directed and undirected edges. The completed PDAG (CPDAG) <ref type="bibr" target="#b20">[21]</ref> -also called the essential graph <ref type="bibr" target="#b21">[22]</ref> -of a DAG G = (V, E) is a PDAG G such that (i) it contains the same nodes as G, and (ii) for each edge</p><formula xml:id="formula_2">(X → Y ) ∈ E, if each graph in the equivalence class of G has the edge X → Y , then X → Y is in G ; otherwise X -Y is in G .</formula><p>The consequence of the definition of Markov equivalence is that some arcs have a strict orientation and meaning, whereas in others the orientation can also be reversed without changing the meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Score-Based Structure Learning</head><p>The structure learning task is basically to find the graph, or the structure, that fits the data best. In this paper, we employ a score-based approach that attempts to identify the appropriate model by a hill-climbing search through the space of candidate models and selecting the one with the highest score <ref type="bibr" target="#b1">[2]</ref>. The hill-climbing search selects at each step the best transformation among all feasible edge removals, edge reversals, and edge additions. Obviously, it ignores the edge reversals and edge additions that create directed cycles in the graph. When the score cannot be strictly improved anymore, the search stops.</p><p>Bayesian network scores are usually based on a maximum likelihood principle that picks the model that best 'fits' the observed data. To prevent overfitting, a Bayesian Occam's razor <ref type="bibr" target="#b22">[23]</ref> can be used to select the model with the highest marginal likelihood P (D | G), i.e., where the parameters are integrated out:</p><formula xml:id="formula_3">P (D | G) = P (D | G, Θ)f (Θ | G)dΘ,</formula><p>with f a probability density, such that Θ are the possible parameters for DAG G. Assuming that the conditional distributions defined in a Bayesian network are independent, <ref type="bibr" target="#b1">[2]</ref> showed that this implies that the prior of these conditional distributions must be a Dirichlet, i.e., θ ij ∼ Dir(α ij ), where θ ij represents P (X i | π(X i ) = j) such that j is one of the configuration of the parents of X i , 1 ≤ j ≤ q i , and α ij is a vector of length r i . Let N ijk be the counts of X i = k and its parents having the value j in the data D, and</p><formula xml:id="formula_4">N ij = r i</formula><p>k=1 N ijk , it can then be shown that:</p><formula xml:id="formula_5">P (D | G) = |V | i=1 q i j=1 Γ(α ij ) Γ(α ij + N ij ) r i k=1 Γ(α ijk + N ijk ) Γ(α ijk ) ,</formula><p>where Γ(x) = ∞ 0 t x-1 e -t dt is the Gamma function. This score is called the BD (Bayesian Dirichlet) score. <ref type="bibr" target="#b1">[2]</ref> also proved that for complete graphs, the only prior that assigns the same marginal likelihood to Markov equivalent graphs is the prior where:</p><formula xml:id="formula_6">α ijk = αP 0 (X i = k, π(X i ) = j)</formula><p>with α &gt; 0, where P 0 is a prior distribution. Finally, taking a uniform prior for P 0 , i.e., P 0 (X i = k, π(X i ) = j) = 1 q i r i , we obtain a very popular score called the BDeu (Bayesian Dirichlet equivalent uniform) score. In this score, the only parameter which we need to choose is α, which is also referred to as the equivalent sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Edge Types, Experts' Opinions and Accuracies</head><p>If the number of nodes in the structure is n, i.e., n = |V |, then there are N = n(n-1)/2 different node pairs. Throughout this paper we assume that there is a fixed ordering over node pairs, and a fixed ordering over the nodes in each pair. If the ith pair is (X, Y ), the status of the edge between X and Y is indicated by g i , where</p><formula xml:id="formula_7">• g i =→ if (X → Y ) ∈ E, • g i =← if (X ← Y ) ∈ E, • g i = if neither (X → Y ) nor (X ← Y ) is in E.</formula><p>According to the above notations, there are three edge types in the structure: {→, ←, }. Note that the edge types → and ← do not essentially differ, but depend on the ordering over the nodes in the pairs. As an example, consider the Bayesian network structure depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Since there are 4 nodes in this graph, the number of node pairs is N = 6. If these pairs are ordered as (X,Y), (X,Z), (X,W), (Y,Z), (Y,W), (Z,W), we have g 1 =→, g 2 =→, g 3 =←, g 4 = , g 5 = , g 6 = .</p><p>We denote the prior distribution over edge types as p = {p → , p ← , p }. For example, when p = {p → = 0.1, p ← = 0.2, p = 0.7} it means that prior to having any data or experts' knowledge, we believe that 10%, 20%, and 70% of g i s are respectively equal to →, ←, .</p><p>The number of experts is indicated by R. The opinion of the ith expert regarding the jth pair is denoted by O i j ∈ {∅, →, ←, }, where O i j = ∅ meaning that the ith expert has not provided any opinion about the jth pair. We use O i to indicate all opinions provided by the ith expert, O j to mention the opinions provided by all experts about the jth pair, and O to denote all provided opinions.</p><p>We model the accuracy of an expert by three parameters:</p><p>• γ 1 : The probability of detecting the existing edges with correct directions, • γ 2 : The probability of detecting the existing edges with reverse directions, • γ 3 : The probability of correctly detecting the absent edges.</p><p>We add a superscript such as γ i 1 , γ i 2 , γ i 3 to denote the accuracy parameters of the ith expert. In addition, γ i indicates the set containing all three accuracy parameters of the ith expert. Finally, the accuracy parameters of all experts are collectively denoted by boldface γ.</p><p>As an example assume that the accuracy parameters of the ith expert are γ i 1 = 0.6, γ i 2 = 0.1, γ i 3 = 0.8. We can conclude that if this expert gives an opinion about the jth pair, the following confusion matrix shows the probabilities of providing different opinions by this expert:</p><formula xml:id="formula_8">  → ← → 0.6 0.1 0.3 ← 0.1 0.6 0.3 0.1 0.1 0.8   ,</formula><p>where each row shows a possible value for g j and each column indicates a possible opinion O i j . Obviously, each row must sum to one. About the last row note that when the expert is wrong about the absent edge g j = , he/she selects one of the possible edges → or ←. We consider these two possibilities equally likely because we do not have any evidence to favor one over the other. experts' opinions; p: prior distribution over edge types; K: information that determines G; γ: accuracy of experts; : noise). See text for further explanation. and experts' opinions O. In this model, K denotes the set of information determining the structure. Note that the prior distribution p can be seen as a part of K, but we separate it because it plays a distinguished role in the next section. According to this model, data is directly affected by the structure and a noise factor . Experts' opinions are also directly affected by the graph structure and experts' accuracy parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM SETTING</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows graphical models of the experts' opinions. The right model represents a more detailed version of the left one. According to this figure, the opinions of each expert are determined by the graph structure and the individual's accuracy parameters. More precisely, according to the detailed model, the opinion of one expert regarding one particular node pair is influenced by the edge status of that pair and the experts' accuracy parameters.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> indicates the roles of different accuracy parameters of an expert in determining the personal opinions. In this figure <ref type="figure">,</ref><ref type="figure">O i</ref> E and O i A are the opinions of the ith expert about the existing and absent edges in G, respectively. Based on this model, the opinions regarding existing edges are influenced by γ 1 and γ 2 parameters, and the opinions regarding absent edges are influenced by the γ 3 parameter.</p><p>Using the d-separation rules in Bayesian networks, introduced in Section 2, we can derive a set of conditional independence statements from these models. Some of these statements are presented in Table <ref type="table" target="#tab_0">1</ref>. Only those used in the remainder of the paper are listed here, as clearly, more statements can be read off from the graphical models. In each of the statements in Table <ref type="table" target="#tab_0">1</ref></p><formula xml:id="formula_9">, assume that 1 ≤ i, j ≤ R, 1 ≤ x, y ≤ N , i = j, and x = y.</formula><p>There is one point that must be noted about the independence statements in Table <ref type="table" target="#tab_0">1</ref>. Consider statement 3 as an example. According to this statement, D and γ are independent given G. Note that based on Fig. <ref type="figure">2</ref>, if O is not given, D and γ are independent, regardless of whether G is given or not. Anyway, having G does not violate this independence, and because we need D ⊥ γ | G in the subsequent sections, we introduce this statement instead of D ⊥ γ. This also holds for some other statements in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPLICIT-ACCURACY-BASED SCORE</head><p>In our first scoring function, we explicitly use the estimated accuracy parameters of experts to quantify the quality of   </p><formula xml:id="formula_10">γ i O i G O j γ j p K (a) Abstract model O i 1 O i 2 ... O i N p K O j 1 O j 2 ... O j N γ i γ j g1 g2 ... g N (b) Detailed model</formula><formula xml:id="formula_11">γ i 1 , γ i 2 O i E G O i A γ i 3</formula><formula xml:id="formula_12">G ⊥ γ Fig. 2 2 G ⊥ γ | p Fig. 2 3 D ⊥ γ | G Fig. 2 4 O ⊥ D | G Fig. 2 5 O ⊥ D | G, γ Fig. 2 6 O i ⊥ O j | G Fig. 3a 7 O i ⊥ O j | G, γ Fig. 3a 8 O j ⊥ γ i | G, γ j Fig. 3a 9 O j x ⊥ O j y | G, γ j Fig. 3b 10 O j x ⊥ gy | gx, γ j Fig. 3b 11 O i x ⊥ O j x | gx, p, γ Fig. 3b 12 O j x ⊥ γ i | gx, p, γ j Fig. 3b 13 O j x ⊥ p | gx, γ j Fig. 3b 14 (γ i 1 , γ i 2 ) ⊥ γ i 3 Fig. 4</formula><p>candidate structures. In subsection 4.1, we introduce this scoring function assuming that there is an estimate of experts' accuracies. Then, in subsection 4.2, an expectationmaximization-based method is described to estimate these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Score Derivation</head><p>The goal of the explicit-accuracy-based scoring function is to score a candidate structure G using the data D, experts' opinions O, and estimated experts' accuracies γ. A Bayesian measure of the goodness of fit of G is its posterior probability given D, O, and γ:</p><formula xml:id="formula_13">P (G | D, O, γ) ∝ P (G, D, O, γ) = P (γ) P (G | γ) P (D | G, γ) P (O | G, D, γ).</formula><p>Since P (γ) does not depend on the graph structure, we omit it from the score.</p><formula xml:id="formula_14">P (G | γ) is simplified to P (G) based on statement 1 in Table 1. In addition, P (D | G, γ) is simplified to P (D | G) according to statement 3. Fi- nally, P (O | G, D, γ) is simplified to P (O | G, γ) using statement 5</formula><p>. Therefore, the explicit-accuracy-based score is introduced as the log of P (G | D, O, γ) and given as:</p><formula xml:id="formula_15">Score explicit (G; D, O, γ) = log P (G) + log P (D | G) + log P (O | G, γ). (1)</formula><p>For the first two parts of this score, there are different choices mentioned in the literature. For the prior P (G), the simplest and most common choice, which we also use in our experiments, is the uniform prior. It means that all structures are equally likely a priori, and therefore we can omit it from the score. Other choices are to provide greater penalty to dense networks <ref type="bibr" target="#b23">[24]</ref> and to consider the number of options in determining the parents of each node <ref type="bibr" target="#b24">[25]</ref>. For the second part log P (D | G), we use the BDeu score introduced in Section 2.</p><p>For the last part of the explicit-accuracy-based score, we use statements 7, 8, 9, 10 from Table <ref type="table" target="#tab_0">1</ref> and obtain</p><formula xml:id="formula_16">log P (O | G, γ) = R j=1 N i=1 log P (O j i | g i , γ j ).<label>(2)</label></formula><p>The term P (O j i | g i , γ j ) in equation ( <ref type="formula" target="#formula_16">2</ref>) is computed using the decision tree depicted in Fig. <ref type="figure" target="#fig_4">5</ref>. Note that we only use the provided opinions (i.e., O j i = ∅) to score G. The reason for dividing 1 -γ j 3 by 2 in this figure is that when the expert is wrong about an absent edge in G like X Y , he/she selects one of the possible edges X → Y or X ← Y . We consider these two possibilities equally likely because there is no evidence to favor one over the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expectation-Maximization-Based Accuracy Estimation</head><p>One way to estimate the experts' accuracies is to use the expectation-maximization (EM) algorithm <ref type="bibr" target="#b25">[26]</ref>. This approach has been used in the crowdsourcing literatures such as <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In the structure learning problem, we also previously used this algorithm to estimate the experts' confusion matrices <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Here, we follow the same framework but derive the formulas for the three-parameter-based model of experts' accuracies proposed in Section 2.</p><p>If we consider the prior distribution p and the experts' accuracies γ as the parameters, the maximum-likelihood estimate of these parameters is</p><formula xml:id="formula_17">(p, γ) = arg max p,γ {log P (O | p, γ)}.</formula><p>Note that we use only the experts' opinions O in the likelihood function. Obviously, the data D can also help in this estimation problem, but we ignore it for simplicity's sake.</p><p>To solve this optimization problem, we consider the true structure G as a hidden variable and use the EM algorithm. We assume that (O i , g i ) is independent of (O j , g j ), for each j = i, given (p, γ). This assumption is not true in general, but to make the computations tractable, we ought to consider it. With this assumption, the log-likelihood of complete data (O, G) is</p><formula xml:id="formula_18">log P (O, G | p, γ) = N i=1 log P (O i , g i | p, γ).</formula><p>Since g i is a member of {→, ←, }, we can write</p><formula xml:id="formula_19">log P (O i , g i | p, γ) = k∈{→,←, } I(g i = k) log P (O i , g i = k | p, γ),</formula><p>where I(c) is the indicator function, which is one if the condition c is satisfied and zero otherwise. We simply expand the inner term in the above expression as</p><formula xml:id="formula_20">P (O i , g i = k | p, γ) = P (g i = k | p, γ)P (O i | g i = k, p, γ).</formula><p>Using statement 2 in Table <ref type="table" target="#tab_0">1</ref>, we have</p><formula xml:id="formula_21">P (g i = k | p, γ) = P (g i = k | p) = p k .<label>(3)</label></formula><p>Also according to statement 11 in Table <ref type="table" target="#tab_0">1</ref>, we have</p><formula xml:id="formula_22">P (O i | g i = k, p, γ) = R j=1 P (O j i | g i = k, p, γ).<label>(4)</label></formula><p>Finally, based on statements 12 and 13 in Table <ref type="table" target="#tab_0">1</ref>, the term in the above equation is simplified to</p><formula xml:id="formula_23">P (O j i | g i = k, p, γ) = P (O j i | g i = k, γ j ),</formula><p>which is simply computed using the decision tree in Fig. <ref type="figure" target="#fig_4">5</ref>. Again, note that we consider only the provided opinions (i.e., O j i = ∅) in the computations. Putting all the above together, the log-likelihood of complete data is</p><formula xml:id="formula_24">log P (O, G | p, γ) = N i=1 k∈{→,←, } I(g i = k) × log p k + R j=1 log P (O j i | g i = k, γ j ) . (<label>5</label></formula><formula xml:id="formula_25">)</formula><p>The EM algorithm iterates between two steps: an Expectation step (E-step) and a Maximization step (M-step). In the E-step, the expectation of the complete log-likelihood is computed using the current estimate of parameters. In the M-step, this expectation is maximized to obtain the next estimate of the parameters. </p><formula xml:id="formula_26">gi = O j i = gi O j i = O j i ∈ {→, ←} 1 -γ j 1 -γ j 2 γ j 2 γ j 1 1-γ j</formula><formula xml:id="formula_27">(O j i | g i , γ j ) for O j i = ∅.</formula><p>The conditional expectation of the complete loglikelihood <ref type="bibr" target="#b4">(5)</ref> given opinions O and the current estimate of the parameters</p><formula xml:id="formula_28">p (t) , γ (t) is E η [log P (O, G | p, γ)] = N i=1 k∈{→,←, } E η [I(g i = k)] × log p k + R j=1 log P (O j i | g i = k, γ j ) ,<label>(6)</label></formula><p>where we denote E G|O,p (t) ,γ (t) by E η for the ease of reading. The expectation of the indicator function is the probability of the associated event. Therefore,</p><formula xml:id="formula_29">E η [I(g i = k)] = P (g i = k | O, p (t) , γ (t) ).</formula><p>To make the computation of the above probability tractable, we assume that</p><formula xml:id="formula_30">P (g i = k | O, p (t) , γ (t) ) = P (g i = k | O i , p (t) , γ (t) ).</formula><p>This informally means that the status of the edge between two particular nodes is independent of the opinions regarding other node pairs, given the opinions about that node pair. This assumption means that that experts offer opinions about individual edges without taking opinions about other edges into account. It is based on assuming limited understanding of the semantics of Bayesian networks, quite common in domain experts of real-life problems.</p><p>Based on the above assumption and using Bayes' rule, we have</p><formula xml:id="formula_31">E η [I(g i = k)] = P (g i = k | p (t) , γ (t) ) × P (O i | g i = k, p (t) , γ (t) ) P (O i | p (t) , γ (t) ) .<label>(7)</label></formula><p>The numerator terms can be computed using equations ( <ref type="formula" target="#formula_21">3</ref>) and ( <ref type="formula" target="#formula_22">4</ref>), respectively, and the denominator is simply a normalization factor. The next estimates of parameters are obtained by maximizing the expectation <ref type="bibr" target="#b5">(6)</ref>. By setting the partial derivatives of ( <ref type="formula" target="#formula_28">6</ref>) with respect to each parameter equal to zero, we obtain the following estimates for the parameters:</p><formula xml:id="formula_32">p (t+1) k = 1 N N i=1 E η [I(g i = k)], (γ j 1 ) (t+1) = N i=1 k∈{→,←} E η [I(g i = k)] × I(O j i = k) N i=1 k∈{→,←} E η [I(g i = k)] × I(O j i = ∅) , (γ j 2 ) (t+1) = N i=1 k∈{→,←} E η [I(g i =k)]×I(O j i =k,O j i ∈{→,←}) N i=1 k∈{→,←} E η [I(g i =k)]×I(O j i =∅)</formula><p>,</p><formula xml:id="formula_33">(γ j 3 ) (t+1) = N i=1 E η [I(g i = )] × I(O j i = ) N i=1 E η [I(g i = )] × I(O j i = ∅) ,<label>(8)</label></formula><p>for k ∈ {→, ←, } and 1 ≤ j ≤ R.</p><p>In summary, the EM-based accuracy estimation algorithm works as follows:</p><p>(i) Take initial estimates of the parameters p, γ. (ii) Use equation <ref type="bibr" target="#b6">(7)</ref> and the current estimates of the parameters to calculate estimates of the expectation of the hidden variables g i . (iii) Use equations <ref type="bibr" target="#b7">(8)</ref> to obtain new estimates of the parameters. (iv) Repeat steps (ii) and (iii) until the results converge.</p><p>The EM algorithm yields only local optima, but the considerable experience with the algorithm indicates that the results are usually satisfactory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MARGINALIZATION-BASED SCORE</head><p>The scoring approach proposed in the previous section consists of two steps. In the first step, the experts' accuracies are estimated, then in the second step, the estimated accuracies are used to score the structures. Obviously, the reliability of this score depends on the reliability of the estimated accuracies in the first step. When we are not confident about the estimated accuracies, this approach is not appropriate. In this section, we introduce an alternative approach that is based on marginalizing out the accuracy parameters instead of explicitly estimating them.</p><p>Since the estimated experts' accuracies are not explicitly used in the marginalization-based score, only the data D and experts' opinions O are used for scoring a candidate structure G. The posterior probability of G given D and O is a reasonable measure for this purpose:</p><formula xml:id="formula_34">P (G | D, O) ∝ P (G, D, O) = P (G) P (D | G) P (O | G, D).</formula><p>Based on the statement 4 in Table <ref type="table" target="#tab_0">1,</ref><ref type="table"></ref>  Comparing the above scoring function with the explicitaccuracy-based score (1), the only difference is the last term. In the rest of this section, we explain how to compute log P (O | G) to complete the computation of the marginalization-based score.</p><p>According to statement 6 in Table <ref type="table" target="#tab_0">1</ref>,</p><formula xml:id="formula_35">log P (O | G) = R i=1 log P (O i | G).<label>(9)</label></formula><p>To compute P (O i | G), we marginalize out the accuracy parameters:</p><formula xml:id="formula_36">P (O i | G) = 1 0 1-γ i 1 0 1 0 P (γ 1 , γ i 2 , γ i 3 | G) × P (O i | γ i 1 , γ i 2 , γ i 3 , G) dγ i 3 dγ i 2 dγ i 1 .<label>(10)</label></formula><p>Note that the domain of integration for</p><formula xml:id="formula_37">γ i 2 is not [0, 1] but is [0, 1 -γ i 1 ]</formula><p>, because γ i 1 + γ i 2 must be lower than or equal to 1. According to statements 1 and 14 in Table <ref type="table" target="#tab_0">1</ref>,</p><formula xml:id="formula_38">P (γ i 1 , γ i 2 , γ i 3 | G) = P (γ i 1 , γ i 2 ) P (γ i 3 ).<label>(11)</label></formula><p>In addition, based on statement 9 we have</p><formula xml:id="formula_39">P (O i | γ i 1 , γ i 2 , γ i 3 , G) = N j=1 P (O i j | γ i 1 , γ i 2 , γ i 3 , G),</formula><p>which can be written as</p><formula xml:id="formula_40">P (O i | γ i 1 , γ i 2 , γ i 3 , G) = (γ i 1 ) n i1 (γ i 2 ) n i2 (1 -γ i 1 -γ i 2 ) n i3 (γ i 3 ) m i1 1 -γ i 3 2 m i2 ,<label>(12)</label></formula><p>where</p><p>• n i1 is the number of existing edges in G that are detected by expert i with correct directions, • n i2 is the number of existing edges in G that are detected by expert i with reverse directions, • n i3 is the number of existing edges in G that are mentioned as absent edges by expert i, • m i1 is the number of absent edges in G that are correctly detected by expert i, • m i2 is the number of absent edges in G that are mentioned as existing edges by expert i.</p><p>Plugging equations ( <ref type="formula" target="#formula_38">11</ref>) and ( <ref type="formula" target="#formula_40">12</ref>) into equation <ref type="bibr" target="#b9">(10)</ref>, we get</p><formula xml:id="formula_41">P (O i | G) = 1 0 P (γ i 3 ) (γ i 3 ) m i1 1 -γ i 3 2 m i2 dγ i 3 × 1 0 1-γ i 1 0 P (γ i 1 , γ i 2 ) (γ i 1 ) n i1 (γ i 2 ) n i2 (1 -γ i 1 -γ i 2 ) n i3 dγ i 2 dγ i 1 . (<label>13</label></formula><formula xml:id="formula_42">)</formula><p>We denote the components of the above equation by I i 1 and I i 2 , respectively:</p><formula xml:id="formula_43">I i 1 = 1 0 P (γ i 3 ) (γ i 3 ) m i1 1 -γ i 3 2 m i2 dγ i 3 , I i 2 = 1 0 1-γ i 1 0 P (γ i 1 , γ i 2 ) (γ i 1 ) n i1 (γ i 2 ) n i2 (1 -γ i 1 -γ i 2 ) n i3 dγ i 2 dγ i 1 .</formula><p>An appropriate distribution for P (γ i 3 ) in I i 1 is the Beta distribution. If the shape parameters of this distribution are denoted by β i1 and β i2 , we have</p><formula xml:id="formula_44">P (γ i 3 ) = 1 B(β i1 , β i2 ) (γ i 3 ) β i1 -1 (1 -γ i 3 ) β i2 -1 ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_45">B(β i1 , β i2 ) = 1 0 t β i1 -1 (1 -t) β i2 -1 dt<label>(15)</label></formula><p>is the Beta function. Note that the shape parameters can be different for different experts. Nevertheless, we use the same parameters for all experts in our experiments.</p><p>Plugging equation ( <ref type="formula" target="#formula_44">14</ref>) into the definition of I i 1 , we have</p><formula xml:id="formula_46">I i 1 = 1 0 (γ i 3 ) β i1 +m i1 -1 (1 -γ i 3 ) β i2 +m i2 -1 dγ i 3 2 m i2 × B(β i1 , β i2 ) ,</formula><p>which can be written as</p><formula xml:id="formula_47">I i 1 = B(β i1 + m i1 , β i2 + m i2 ) 2 m i2 × B(β i1 , β i2 ) .<label>(16)</label></formula><p>After deriving a closed-form formula for I i 1 , we now turn our attention to I i 2 . We use a Dirichlet distribution Dir(α i1 , α i2 , α i3 ) for P (γ i 1 , γ i 2 ):</p><formula xml:id="formula_48">P (γ i 1 , γ i 2 ) = (γ i 1 ) α i1 -1 (γ i 2 ) α i2 -1 (1 -γ i 1 -γ i 2 ) α i3 -1 B(α i1 , α i2 , α i3 )</formula><p>,</p><p>where B(α i1 , α i2 , α i3 ) is the multivariate Beta function.</p><p>Using this prior in the definition of I i 2 we have</p><formula xml:id="formula_49">I i 2 = 1 B(α i1 , α i2 , α i3 ) 1 0 (γ i 1 ) α i1 +n i1 -1 × 1-γ i 1 0 (γ i 2 ) α i2 +n i2 -1 (1 -γ i 1 -γ i 2 ) α i3 +n i3 -1 dγ i 2 dγ i 1 .</formula><p>Changing the variable t in integral <ref type="bibr" target="#b14">(15)</ref> to γ i 2 by substituting t =</p><formula xml:id="formula_50">γ i 2 1-γ i 1</formula><p>, the inner integral in the above equation is and therefore,</p><formula xml:id="formula_51">1-γ i 1 0 (γ i 2 ) α i2 +n i2 -1 (1 -γ i 1 -γ i 2 ) α i3 +n i3 -1 dγ i 2 = B(α i2 + n i2 , α i3 + n i3 ) × (1 -γ i 1 ) α i2 +n i2 +α i3 +n i3 -1 ,</formula><formula xml:id="formula_52">I i 2 = B(α i2 + n i2 , α i3 + n i3 ) B(α i1 , α i2 , α i3 ) × 1 0 (γ i 1 ) α i1 +n i1 -1 (1 -γ i 1 ) α i2 +n i2 +α i3 +n i3 -1 dγ i 1 ,</formula><p>which can be written as</p><formula xml:id="formula_53">I i 2 = 1 B(α i1 , α i2 , α i3 ) × B(α i2 + n i2 , α i3 + n i3 ) × B(α i1 + n i1 , α i2 + n i2 + α i3 + n i3 ). (<label>17</label></formula><formula xml:id="formula_54">)</formula><p>Based on equation ( <ref type="formula" target="#formula_41">13</ref>), we can compute P (O i | G) by multiplying equations ( <ref type="formula" target="#formula_47">16</ref>), <ref type="bibr" target="#b16">(17)</ref>. So, </p><formula xml:id="formula_55">log P (O i | G) = log B(β i1 + m i1 , β i2 + m i2 ) -m i2 log 2 -log B(β i1 , β i2 ) + log B(α i1 + n i1 , α i2 + n i2 + α i3 + n i3 ) + log B(α i2 + n i2 , α i3 + n i3 ) -log B(α i1 , α i2 , α i3 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>The developed scores are evaluated in this section using simulated experts (subsection 6.1) and real experts (subsection 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Simulation Experiments</head><p>To evaluate the developed scores, some of the experiments are performed on simulated experts. The merit of simulation is that we can change the values of different parameters, such as the experts' accuracies or the amount of available knowledge, and evaluate the scores under different conditions. In this part of the paper, we present the setup of our simulation experiments and discuss the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Experimental Setup</head><p>We use four Bayesian networks which have been widely used in the structure learning experiments: Asia <ref type="bibr" target="#b30">[31]</ref>, Insurance <ref type="bibr" target="#b31">[32]</ref>, Alarm <ref type="bibr" target="#b32">[33]</ref>, and Hailfinder <ref type="bibr" target="#b33">[34]</ref>, briefly described in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Our experiments are implemented in a MATLAB environment using the Bayes net toolbox <ref type="bibr" target="#b34">[35]</ref> and the BNT structure learning package <ref type="bibr" target="#b35">[36]</ref>. For each network, we generate the data samples and experts' opinions and learn the structure using different scoring functions. Comparing the learned network with the gold-standard structure reveals the effectiveness of the corresponding scoring function. In addition to comparing the DAGs, we also compare the CPDAGs representing the equivalence classes of the learned structure and the gold-standard network <ref type="bibr" target="#b20">[21]</ref>. The reason for comparing the CPDAGs is that we do not penalize for structural differences that cannot be distinguished only based on data <ref type="bibr" target="#b36">[37]</ref>.</p><p>There may be three types of errors in the learned DAG (CPDAG):</p><p>• Wrong Connection where an absent edge in the original graph is available in the learned network, • Missed Edge where an available edge in the original graph is missed in the learned structure, • Wrong Orientation where one edge has different orientations in the original graph and the learned structure. Note that, when comparing two CPDAGs such as G 1 and G 2 , if one edge is undirected in G 1 and directed in G 2 , this is also considered as wrong orientation error, since there is at least one graph in the equivalence class of G 1 where its corresponding edge has wrong orientation than that of G 2 .</p><p>The total number of these errors is called the structural Hamming distance (SHD) <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>In our simulations, we consider three different populations each with R = 10 experts. According to the experts' accuracies, these populations are labeled as "weak", "mediocre" and "good". Table <ref type="table" target="#tab_2">3</ref> lists the details of the experts' accuracies in these populations. Three experts are equally accurate in all populations. Three other experts are equally accurate in the "weak" and "mediocre" populations, but more accurate in the "good" population. The next three experts are equally accurate in the "mediocre" and "good" populations, but less accurate in the "weak" population. Note that since higher γ 1 and γ 3 means more accurate experts, these parameters have higher values in the "good" population. On the other hand, since higher γ 2 means less accurate experts, this parameter has higher values in the "weak" population. <ref type="foot" target="#foot_0">1</ref>In our experiments, we compare six different functions:</p><p>1) Data: This function neglects the experts' opinions and only uses the data D to score the structures. We use the marginal likelihood part of the BDeu score <ref type="bibr" target="#b1">[2]</ref>, introduced in Section 2, for this purpose. 2) Expert: This function neglects the data D and only uses the experts' opinions O to decide about the Bayesian network structure. It uses a majority voting approach, in which, for each pair (X, Y ), the status that the majority of experts agree on is considered as the status of the edge between X and Y . In the case of a tie, a random decision is made. 3) PE: Stands for perfect experts, this scoring function assumes that all experts are completely accurate. For this, the explicit-accuracy-based score (1) is used, where γ 1 and γ 3 parameters of all experts are set to one, and γ 2 is zero. 4) Mean: This function also exploits both data and experts' opinions using the explicit-accuracy-based score <ref type="bibr" target="#b0">(1)</ref>. It considers the same accuracies for all experts to resemble the method proposed in <ref type="bibr" target="#b15">[16]</ref>. For the accuracy parameters, it uses the mean of true accuracy parameters of all experts in each population. More precisely, if the opinions are generated from the "weak" population, γ 1 , γ 2 , γ 3 are set to 0.4, 0.35, 0.5, respectively. For the "mediocre" population, these parameters are set to 0.5, 0.25, 0.65, respectively. Finally, for the "good" population, 0.6, 0.2, 0.8 are used as the accuracy parameters for all experts. This scoring function is included in the experiments to compare the best achievable results from a method such as <ref type="bibr" target="#b15">[16]</ref> that considers the same accuracy levels for all experts with the methods such as the EM-based method proposed in Section 4 which try to estimate the accuracies of all experts. 5) EM: In this function, we first estimate the experts' accuracies using the EM-based algorithm introduced in subsection 4.2, and then, score the structures using the explicit-accuracy-based score <ref type="bibr" target="#b0">(1)</ref>. As the initial prior distribution over edge types, we use p = {p → = 0.1, p ← = 0.1, p = 0.8}, since we know that most real world Bayesian network structures are sparse. The true distributions for the used Bayesian networks are presented in Table <ref type="table" target="#tab_3">4</ref>. For the initial accuracy parameters γ, we assume that we have the mean of true accuracy parameters of all experts a priori, and therefore, we initialize γ with the values used for the Mean scoring function. We use these initial values because we want to provide the same prior information for both Mean and EM scores, and can fairly compare their results.</p><p>The EM algorithm stops when the absolute change in the estimated accuracies is smaller than 0.001. 6) Marg: This function is the marginalization-based score introduced in Section 5. For the parameters β i1 , β i2 , α i1 , α i2 , α i3 , we use the same values for all experts, again using the mean of true accuracy param-eters. If the mean of true accuracy parameters of all experts in a particular population is γ1 , γ2 , γ3 , we have</p><formula xml:id="formula_56">β i1 = cγ 3 , βi2 = c(1 -γ3 ), α i1 = cγ 1 , α i2 = cγ 2 , α i3 = c(1 -γ1 -γ2 ),<label>(18)</label></formula><p>for i = 1, . . . , R, where c is a constant coefficient. In the following, wherever the value of c is not clearly mentioned, its value is 10. At the end of this section, we evaluate the influence of this coefficient and show that its value does not have such a considerable impact on the obtained results. In all of the above scoring functions, for the loglikelihood log P (D | G), the corresponding component from the BDeu score is used, and the parameter representing the equivalent sample size is set to 1. For the prior distribution over structures P (G), the uniform prior is used. Finally, as the search procedure, we use the same hill-climbing search as <ref type="bibr" target="#b1">[2]</ref> introduced in Section 2, starting from an empty network.</p><p>The number of opinions is controlled by a parameter ν ∈ [0, 1]. If there are N pairs of variables and R experts, the total number of opinions provided by all experts is equal to round(ν × R × N ), where the function round(x) outputs the closest integer to x. In our experiments, the value of the parameter ν is selected from {0.3, 0.4, 0.5, 0.6}.</p><p>For the training dataset D, two different sizes are considered: 1000 and 5000. To reduce the effect of randomness in the reported results, we repeat each experiment 10 times and report the average results over these iterations. In other words, for each Bayesian network, we generate 10 different datasets with 1000 samples and 10 different datasets with 5000 samples; and for each triple {Bayesian network, population, ν}, we simulate 10 different opinion sets. Then, in each experiment, we use one dataset and one opinion set to learn the Bayesian network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results and Discussion</head><p>Tables <ref type="table" target="#tab_7">5 to 8</ref> show the obtained structural Hamming distances for the Asia, Insurance, Alarm, and Hailfinder networks, respectively. Each table includes three subtables, one for each population. In each row, the best obtained results are indicated by boldface values.</p><p>According to these tables, we observe that:</p><p>• The results obtained for the Expert scenario show that using only the experts' opinions gives rise to very lowaccurate networks, especially for large networks and weak populations. The main reason is that, in our experiments in accordance with the real world, the experts are not forced to present their opinions regarding all parts of the network, and we do not have enough information to accurately decide about the parts of the network that the majority of the experts are silent about. • For the weak populations, the second worst results (after the Expert scenario) are related to the PE scenario. The reason is that when there are considerable errors in the provided experts' opinions, the raw usage of these opinions (as in the PE scenario) reduces the accuracy of the learned network. • The third worst results for the weak populations are related to the EM-based score. It is obvious that for the scores which try to explicitly use the estimated experts' accuracies, if the estimation process is not successful, it is not so useful to use the experts' knowledge based on the explicit-accuracy-based score. This is the case for the EM-based scenario for the weak populations. In fact, it is well known that the EM algorithm depends on the starting point and may converge to a local optimum. On the other hand, for the weak populations, the starting points are really questionable (which is why we call them weak). Therefore, the EM-based method fails to obtain robust results for the weak populations. • In the majority of cases for the mediocre and good populations, the EM-based method obtains either the best or the second best results. This is because, in these situations, the EM algorithm starts with good enough starting points. The conclusion is that, when we are faced with accurate enough experts' opinions, the EM-based method obtains reasonable results. It may be asked, what if we do not know anything about the accuracy level of the opinions at hand? The EM-based method may get stuck in a local optimum far from true experts' accuracies. This was the main stimulus for proposing an alternative approach, the marginalizationbased scoring function, in Section 5. It is clear from the tables that the marginalization-based score obtains more robust results. • In the majority of cases for the weak populations, the marginalization-based score yields better results than the other scores. On the other hand, in situations where the marginalization-based score is not the winner, it obtains acceptable results compared to the best achieved results. Therefore, if we have no idea about the accuracy levels of our experts, it is better to use the marginalization-based score. • In the majority of cases for the mediocre and good populations, the results obtained by the EM-based score are superior to those obtained by the Mean score. This shows that if we can successfully estimate the experts' accuracies, using these estimated values for different experts is better than considering a constant accuracy level for all experts. • One may think that with larger training datasets (e.g., |D| = 5000 in comparison with |D| = 1000), the structural Hamming distances will become smaller. However, this is not always the case. It can be explained by noting that learning Bayesian networks aims at two goals: (1) to obtain the structure of the domain, and (2) to obtain the probability distribution underlying the domain. These two goals are sometimes in conflict: adding an extra edge to a Bayesian network may increase the structural Hamming distance, yet resulting in a more accurate probability distribution. This is sometimes the case with larger datasets, where complex networks with more wrong connections are preferred over the true network, yielding a more accurate probability distribution.</p><p>There is some literature that supports this claim <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In these situations, the experts' opinions can be really useful to obtain more accurate structures of the domain, such as for the Hailfinder network in the Table <ref type="table" target="#tab_7">8</ref>.  As our final simulation experiment, we investigate the impact of coefficient c in equation ( <ref type="formula" target="#formula_56">18</ref>) on the accuracy of the marginalization-based score. We report the obtained   <ref type="table" target="#tab_4">5</ref> and<ref type="table" target="#tab_6">7</ref>, the value of the coefficient c for the marginalization-based score is set to 10. Here, we repeat the same experiment for three completely different values from the set {0.1, 1, 100}. Table <ref type="table" target="#tab_8">9</ref> shows the obtained structural Hamming distances between the learned structures using the marginalization-based score and the original DAGs. Clearly, the accuracy of the marginalization-based score does not vary substantially when changing the value of c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Real World Experiments</head><p>To assess whether our methods of expert-opinion-guided structure learning actually work in practice, we have carried out an experiment with real experts. This experiment grew out of our work in computer-aided diagnosis of breast cancer using Bayesian networks <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The field of concern was the interpretation of X-ray images by clinical radiologists, in particular X-ray images of breasts, referred to as mammograms. Our previous research has shown that this task can be done by means of a Bayesian network. We expected that the radiologists would be able to draft the structure of a Bayesian network, reflecting their knowledge of mammogram interpretation, depending on their experience with the task. The radiologists had varying amounts of experience in this task: from more than 20 years of specialized experience to no specialized experience at all. Of course, all radiologists have some knowledge of mammogram interpretation. Eight radiologists were asked to fill in the adjacency matrix shown in Table <ref type="table" target="#tab_0">10</ref> and seven of them responded to the request. <ref type="foot" target="#foot_1">2</ref> Three of the radiologists were experienced breast  radiologists and also had ample experience as screening radiologists; two were starting breast and screening radiologists, and two were screening radiologists but no breast radiologists. The accuracy parameters and the number and ratio of provided opinions by these experts are presented in Table <ref type="table" target="#tab_10">11</ref>.</p><p>In our previous research, we have designed several Bayesian network structures in collaboration with experienced radiologists. These radiologists were different from the radiologists we asked for our current experiment. None of the radiologists had ever seen one of the Bayesian networks we had designed previously. One of those networks is shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The Bayesian network model combines clinical features with radiological examination by X-rays. In addition, the Bayesian network integrates the results of microcalcification analysis, which is a separate image analysis procedure.</p><p>Training data was generated from the Bayesian network shown in Fig. <ref type="figure" target="#fig_7">6</ref>, which is considered as the gold-standard structure. <ref type="foot" target="#foot_2">3</ref> Table <ref type="table" target="#tab_11">12</ref> shows the obtained structural Hamming distances between the gold-standard structure and the structures learned using the scoring functions mentioned in subsection 6.1.1. In this table, |D| means the number of training data which is selected from the set {100, 400, 700, 1000}. In order to reduce the effect of randomness in the reported results, each experiment is repeated 10 times (for 10 different training datasets) and the average results over these repetitions are reported. Finally, the coefficient c in equation ( <ref type="formula" target="#formula_56">18</ref>) for the marginalization-based score is set to 100.</p><p>As it is clear from Table <ref type="table" target="#tab_11">12</ref>, the Mean and Marg scoring functions obtained the best results. The success of Mean scoring function is due to the low variance in the experts' accuracies. More precisely, according to Table <ref type="table" target="#tab_10">11</ref>, the accuracies of different experts are not so far from the average values and therefore, using these average values instead of individual accuracies yielded good results.</p><p>Although Mean and Marg scores obtained similar results in Table <ref type="table" target="#tab_11">12</ref>, we now show that our marginalization-based scoring function is more reliable. Note that both functions need an estimation of the average experts' accuracies as input. These average values are used as the individual experts' accuracies in the Mean function, and for calculating the parameters β i1 , β i2 , α i1 , α i2 , α i3 using equation <ref type="bibr" target="#b17">(18)</ref> in the Marg function. Obviously, in real world applications, there might be some errors in the estimated average accuracies. To compare Mean and Marg scoring functions, we studied the behaviors for different levels of errors in this input Fig. <ref type="figure" target="#fig_8">7</ref> shows the mean and one standard deviation error bars for the structural Hamming distances obtained from Mean and Marg scores. The horizontal axis is the available error in the input average accuracy vector, which is equal to the sum of the absolute errors in the input vector related to the true vector ([0.55, 0.13, 0.67]). According to this figure, in general, the marginalization-based score obtains lower structural Hamming distances with lower standard deviations, which shows the robustness of this function compared to the Mean scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper focused on exploiting the opinions of multiple domain experts regarding the cause-effect relationships between random variables for structure learning of Bayesian networks. The proposed approach enables structure learning to exploit experts' opinions to learn more accurate network structures than from data alone. Well-known limitations of structure learning algorithms, such as the huge, super-exponential size of the search space and the impossibility to distinguish between Markov-equivalent structures using data alone, motivated this research.</p><p>The proposed approach only takes into account realistic assumptions of experts' opinions. For example, experts' opinions need not be error free, and neither have each expert to give a complete judgment of the presence or absence of edges, nor is it necessary that the opinions are conflict free.  To exploit the provided opinions, we introduced two new scoring functions to be used in the score-based Bayesian network structure learning. The main novelty of the proposed scores is that we take into account the natural point of view that different experts have different individual probabilities of correctly labeling the inclusion or exclusion of edges in the structure. The accuracy of each expert was modeled by three parameters. In the first scoring function, the experts' accuracies are first estimated using an expectation-maximization-based algorithm. Then, the estimated values are explicitly used in the scoring process. When we are confident about the estimated accuracies, this scoring function results in robust decisions. On the other hand, when it is not possible to find a confident estimate of experts' accuracies, our second score, the marginalizationbased score, which marginalizes out the accuracy parameters results in more robust scores. Some of the future research directions are (i) to work on relaxing the assumptions made in the development of the EM-based accuracy estimation algorithm described in Section 4.2, (ii) to develop algorithms that use data along with experts' opinions to obtain improved estimates of experts' accuracies, (iii) to use the recently published agreement/disagreement algorithm <ref type="bibr" target="#b41">[42]</ref> for estimating the experts' accuracies in the structure learning problem, and (iv) to exploit the experts' opinions for constraint-based structure learning. For example, the provided opinions can help to obtain more accurate conditional independencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Simple Bayesian network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figs. 2 Fig. 2 .</head><label>22</label><figDesc>Figs. 2 to 4 depict the various problem component models. Fig.2shows the factors affecting the structure G, data D,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Graphical models of the experts' opinions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Graphical model of the roles of different accuracy parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Decision tree for computing P (O j i | g i , γ j ) for O j i = ∅.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>P (O | G, D) is simplified as P (O | G). Therefore, we define our marginalization-based score as: Score marg (G; D, O) = log P (G)+log P (D | G)+log P (O | G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Finally, based on</head><label></label><figDesc>equation (9) we get log P (O | G) by summing the above expression for i = 1, . . . , R, which completes the computation of the marginalization-based score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The structure of the Bayesian network for breast cancer diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. The mean and one standard deviation error bars for the structural Hamming distances obtained from Mean and Marg scores as functions of the available error in the input average accuracy vector for the breast cancer network with real experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Some Independence Statements Derived from Models of</cell></row><row><cell></cell><cell>Figs. 2 to 4</cell><cell></cell></row><row><cell>Number</cell><cell>Statement</cell><cell>Model</cell></row><row><cell>1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Description of the Networks Used in the Simulation Experiments</figDesc><table><row><cell>Name</cell><cell>Description</cell><cell cols="2">Nodes Edges</cell></row><row><cell>Asia</cell><cell>Diagnosing some respiratory diseases</cell><cell>8</cell><cell>8</cell></row><row><cell>Insurance</cell><cell>Evaluating car insurance risks</cell><cell>27</cell><cell>52</cell></row><row><cell>Alarm</cell><cell>Monitoring patients in intensive care</cell><cell>37</cell><cell>46</cell></row><row><cell cols="2">Hailfinder Predicting summer hails in northern Colorado</cell><cell>56</cell><cell>66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 The</head><label>3</label><figDesc>Accuracy Parameters Assigned to Experts in Simulated Populations</figDesc><table><row><cell></cell><cell></cell><cell>Weak</cell><cell></cell><cell></cell><cell>Mediocre</cell><cell></cell><cell></cell><cell>Good</cell></row><row><cell></cell><cell>γ1</cell><cell>γ2</cell><cell>γ3</cell><cell>γ1</cell><cell>γ2</cell><cell>γ3</cell><cell>γ1</cell><cell>γ2</cell><cell>γ3</cell></row><row><cell>1</cell><cell cols="3">0.15 0.80 0.85</cell><cell cols="3">0.15 0.80 0.85</cell><cell cols="3">0.15 0.80 0.85</cell></row><row><cell>2</cell><cell cols="3">0.30 0.30 0.30</cell><cell cols="3">0.30 0.30 0.30</cell><cell cols="3">0.30 0.30 0.30</cell></row><row><cell>3</cell><cell cols="3">0.75 0.10 0.90</cell><cell cols="3">0.75 0.10 0.90</cell><cell cols="3">0.75 0.10 0.90</cell></row><row><cell>4</cell><cell cols="3">0.40 0.25 0.50</cell><cell cols="3">0.40 0.25 0.50</cell><cell cols="3">0.85 0.05 0.85</cell></row><row><cell>5</cell><cell cols="3">0.45 0.35 0.45</cell><cell cols="3">0.45 0.35 0.45</cell><cell cols="3">0.70 0.15 0.80</cell></row><row><cell>6</cell><cell cols="3">0.55 0.20 0.60</cell><cell cols="3">0.55 0.20 0.60</cell><cell cols="3">0.75 0.15 0.70</cell></row><row><cell>7</cell><cell cols="3">0.20 0.15 0.50</cell><cell cols="3">0.20 0.15 0.95</cell><cell cols="3">0.20 0.15 0.95</cell></row><row><cell>8</cell><cell cols="3">0.33 0.33 0.33</cell><cell cols="3">0.90 0.05 0.80</cell><cell cols="3">0.90 0.05 0.80</cell></row><row><cell>9</cell><cell cols="3">0.50 0.30 0.40</cell><cell cols="3">0.70 0.20 0.70</cell><cell cols="3">0.70 0.20 0.70</cell></row><row><cell>10</cell><cell cols="3">0.30 0.50 0.30</cell><cell cols="3">0.60 0.30 0.65</cell><cell cols="3">0.80 0.10 0.90</cell></row><row><cell>Mean</cell><cell cols="3">0.39 0.33 0.51</cell><cell cols="3">0.50 0.27 0.67</cell><cell cols="3">0.61 0.21 0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 The</head><label>4</label><figDesc>True Probability Distributions Over Edge Types for the Bayesian Networks Used in the Simulation Experiments</figDesc><table><row><cell>BN</cell><cell>p→</cell><cell>p←</cell><cell>p</cell></row><row><cell>Asia</cell><cell cols="3">0.11 0.18 0.71</cell></row><row><cell>Insurance</cell><cell cols="3">0.07 0.08 0.85</cell></row><row><cell>Alarm</cell><cell cols="3">0.04 0.03 0.93</cell></row><row><cell cols="4">Hailfinder 0.02 0.02 0.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="14">The Obtained Structural Hamming Distances for the Asia</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Weak population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>7.2</cell><cell>13.6</cell><cell>12.0</cell><cell>6.8</cell><cell>12.7</cell><cell>6.7</cell><cell>7.0</cell><cell>14.4</cell><cell>12.3</cell><cell>6.2</cell><cell>13.5</cell><cell>6.5</cell></row><row><cell></cell><cell>0.4</cell><cell>7.2</cell><cell>11.2</cell><cell>11.0</cell><cell>6.0</cell><cell>8.7</cell><cell>5.7</cell><cell>7.0</cell><cell>12.3</cell><cell>12.1</cell><cell>5.9</cell><cell>9.0</cell><cell>5.3</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>7.2</cell><cell>11.1</cell><cell>9.4</cell><cell>5.0</cell><cell>9.2</cell><cell>3.8</cell><cell>7.0</cell><cell>12.2</cell><cell>10.2</cell><cell>5.2</cell><cell>10.3</cell><cell>4.1</cell></row><row><cell></cell><cell>0.6</cell><cell>7.2</cell><cell>12.0</cell><cell>8.8</cell><cell>4.3</cell><cell>7.3</cell><cell>3.9</cell><cell>7.0</cell><cell>13.7</cell><cell>10.0</cell><cell>4.4</cell><cell>8.4</cell><cell>4.5</cell></row><row><cell></cell><cell>Avg</cell><cell>7.2</cell><cell>12.0</cell><cell>10.3</cell><cell>5.5</cell><cell>9.5</cell><cell>5.0</cell><cell>7.0</cell><cell>13.2</cell><cell>11.2</cell><cell>5.4</cell><cell>10.3</cell><cell>5.1</cell></row><row><cell></cell><cell>0.3</cell><cell>9.3</cell><cell>13.6</cell><cell>10.5</cell><cell>6.9</cell><cell>10.3</cell><cell>7.4</cell><cell>9.5</cell><cell>14.4</cell><cell>10.9</cell><cell>7.0</cell><cell>11.0</cell><cell>7.7</cell></row><row><cell></cell><cell>0.4</cell><cell>9.3</cell><cell>11.2</cell><cell>9.2</cell><cell>5.4</cell><cell>7.6</cell><cell>4.9</cell><cell>9.5</cell><cell>12.3</cell><cell>9.9</cell><cell>4.6</cell><cell>7.6</cell><cell>4.5</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>9.3</cell><cell>11.1</cell><cell>9.3</cell><cell>5.2</cell><cell>9.2</cell><cell>3.8</cell><cell>9.5</cell><cell>12.2</cell><cell>10.2</cell><cell>5.1</cell><cell>9.9</cell><cell>3.9</cell></row><row><cell></cell><cell>0.6</cell><cell>9.3</cell><cell>12.0</cell><cell>8.8</cell><cell>4.5</cell><cell>7.1</cell><cell>4.1</cell><cell>9.5</cell><cell>13.7</cell><cell>10.0</cell><cell>4.1</cell><cell>7.6</cell><cell>3.4</cell></row><row><cell></cell><cell>Avg</cell><cell>9.3</cell><cell>12.0</cell><cell>9.5</cell><cell>5.5</cell><cell>8.5</cell><cell>5.1</cell><cell>9.5</cell><cell>13.2</cell><cell>10.3</cell><cell>5.2</cell><cell>9.0</cell><cell>4.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Mediocre population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell cols="3">Mean EM Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell cols="3">Mean EM Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>7.2</cell><cell>8.6</cell><cell>4.7</cell><cell>3.9</cell><cell>5.0</cell><cell>2.8</cell><cell>7.0</cell><cell>9.9</cell><cell>5.3</cell><cell>3.8</cell><cell>5.0</cell><cell>2.6</cell></row><row><cell></cell><cell>0.4</cell><cell>7.2</cell><cell>7.6</cell><cell>5.2</cell><cell>5.1</cell><cell>5.5</cell><cell>4.3</cell><cell>7.0</cell><cell>8.6</cell><cell>5.6</cell><cell>4.7</cell><cell>5.5</cell><cell>4.4</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>7.2</cell><cell>5.1</cell><cell>3.0</cell><cell>2.8</cell><cell>3.3</cell><cell>1.7</cell><cell>7.0</cell><cell>6.4</cell><cell>2.9</cell><cell>2.1</cell><cell>3.2</cell><cell>1.1</cell></row><row><cell></cell><cell>0.6</cell><cell>7.2</cell><cell>4.4</cell><cell>2.7</cell><cell>2.1</cell><cell>2.8</cell><cell>1.3</cell><cell>7.0</cell><cell>5.8</cell><cell>3.7</cell><cell>1.9</cell><cell>3.1</cell><cell>1.2</cell></row><row><cell></cell><cell>Avg</cell><cell>7.2</cell><cell>6.4</cell><cell>3.9</cell><cell>3.5</cell><cell>4.2</cell><cell>2.5</cell><cell>7.0</cell><cell>7.7</cell><cell>4.4</cell><cell>3.1</cell><cell>4.2</cell><cell>2.3</cell></row><row><cell></cell><cell>0.3</cell><cell>9.3</cell><cell>8.6</cell><cell>4.6</cell><cell>3.4</cell><cell>5.1</cell><cell>2.6</cell><cell>9.5</cell><cell>9.9</cell><cell>4.7</cell><cell>2.8</cell><cell>4.8</cell><cell>2.0</cell></row><row><cell></cell><cell>0.4</cell><cell>9.3</cell><cell>7.6</cell><cell>5.4</cell><cell>4.7</cell><cell>4.4</cell><cell>4.7</cell><cell>9.5</cell><cell>8.6</cell><cell>5.6</cell><cell>4.0</cell><cell>4.3</cell><cell>4.2</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>9.3</cell><cell>5.1</cell><cell>2.9</cell><cell>2.5</cell><cell>3.6</cell><cell>1.8</cell><cell>9.5</cell><cell>6.4</cell><cell>2.6</cell><cell>2.7</cell><cell>3.6</cell><cell>1.9</cell></row><row><cell></cell><cell>0.6</cell><cell>9.3</cell><cell>4.4</cell><cell>2.7</cell><cell>3.1</cell><cell>2.2</cell><cell>2.1</cell><cell>9.5</cell><cell>5.8</cell><cell>3.1</cell><cell>2.6</cell><cell>2.3</cell><cell>2.0</cell></row><row><cell></cell><cell>Avg</cell><cell>9.3</cell><cell>6.4</cell><cell>3.9</cell><cell>3.4</cell><cell>3.8</cell><cell>2.8</cell><cell>9.5</cell><cell>7.7</cell><cell>4.0</cell><cell>3.0</cell><cell>3.8</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Good population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell cols="3">Mean EM Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell cols="3">Mean EM Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>7.2</cell><cell>5.7</cell><cell>4.2</cell><cell>4.4</cell><cell>3.9</cell><cell>3.2</cell><cell>7.0</cell><cell>7.6</cell><cell>4.9</cell><cell>4.3</cell><cell>4.4</cell><cell>2.6</cell></row><row><cell></cell><cell>0.4</cell><cell>7.2</cell><cell>4.6</cell><cell>2.0</cell><cell>3.3</cell><cell>2.6</cell><cell>2.0</cell><cell>7.0</cell><cell>5.4</cell><cell>2.3</cell><cell>3.1</cell><cell>2.7</cell><cell>1.8</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>7.2</cell><cell>3.7</cell><cell>1.2</cell><cell>2.2</cell><cell>0.9</cell><cell>1.1</cell><cell>7.0</cell><cell>5.1</cell><cell>1.4</cell><cell>2.2</cell><cell>1.2</cell><cell>1.1</cell></row><row><cell></cell><cell>0.6</cell><cell>7.2</cell><cell>2.4</cell><cell>1.1</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell><cell>7.0</cell><cell>4.0</cell><cell>1.1</cell><cell>0.4</cell><cell>0.7</cell><cell>0.7</cell></row><row><cell></cell><cell>Avg</cell><cell>7.2</cell><cell>4.1</cell><cell>2.1</cell><cell>2.7</cell><cell>2.1</cell><cell>1.8</cell><cell>7.0</cell><cell>5.5</cell><cell>2.4</cell><cell>2.5</cell><cell>2.3</cell><cell>1.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="13">The Obtained Structural Hamming Distances for the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Insurance Network</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Weak population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>29.2</cell><cell>126.9</cell><cell>53.9</cell><cell>29.6</cell><cell>36.9</cell><cell>28.0</cell><cell>36.6</cell><cell>132.0</cell><cell>61.6</cell><cell>36.8</cell><cell>43.1</cell><cell>36.0</cell></row><row><cell></cell><cell>0.4</cell><cell>29.2</cell><cell>107.0</cell><cell>49.9</cell><cell>28.0</cell><cell>29.1</cell><cell>23.3</cell><cell>36.6</cell><cell>114.5</cell><cell>58.1</cell><cell>36.1</cell><cell>34.7</cell><cell>28.9</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>29.2</cell><cell>108.2</cell><cell>49.5</cell><cell>31.1</cell><cell>28.6</cell><cell>29.0</cell><cell>36.6</cell><cell>115.0</cell><cell>56.6</cell><cell>39.4</cell><cell>36.6</cell><cell>34.7</cell></row><row><cell></cell><cell>0.6</cell><cell>29.2</cell><cell>104.0</cell><cell>50.3</cell><cell>30.0</cell><cell>30.6</cell><cell>25.7</cell><cell>36.6</cell><cell>110.2</cell><cell>57.2</cell><cell>36.6</cell><cell>39.1</cell><cell>34.8</cell></row><row><cell></cell><cell>Avg</cell><cell>29.2</cell><cell>111.5</cell><cell>50.9</cell><cell>29.7</cell><cell>31.3</cell><cell>26.5</cell><cell>36.6</cell><cell>117.9</cell><cell>58.4</cell><cell>37.2</cell><cell>38.4</cell><cell>33.6</cell></row><row><cell></cell><cell>0.3</cell><cell>24.8</cell><cell>126.9</cell><cell>41.8</cell><cell>27.4</cell><cell>33.9</cell><cell>28.6</cell><cell>22.8</cell><cell>132.0</cell><cell>51.7</cell><cell>30.2</cell><cell>41.6</cell><cell>30.4</cell></row><row><cell></cell><cell>0.4</cell><cell>24.8</cell><cell>107.0</cell><cell>37.7</cell><cell>26.0</cell><cell>27.1</cell><cell>27.3</cell><cell>22.8</cell><cell>114.5</cell><cell>47.0</cell><cell>29.9</cell><cell>34.2</cell><cell>31.2</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>24.8</cell><cell>108.2</cell><cell>39.8</cell><cell>28.7</cell><cell>27.6</cell><cell>30.0</cell><cell>22.8</cell><cell>115.0</cell><cell>46.7</cell><cell>34.5</cell><cell>32.7</cell><cell>35.3</cell></row><row><cell></cell><cell>0.6</cell><cell>24.8</cell><cell>104.0</cell><cell>39.9</cell><cell>28.0</cell><cell>27.1</cell><cell>26.5</cell><cell>22.8</cell><cell>110.2</cell><cell>48.2</cell><cell>29.4</cell><cell>31.5</cell><cell>31.9</cell></row><row><cell></cell><cell>Avg</cell><cell>24.8</cell><cell>111.5</cell><cell>39.8</cell><cell>27.5</cell><cell>28.9</cell><cell>28.1</cell><cell>22.8</cell><cell>117.9</cell><cell>48.4</cell><cell>31.0</cell><cell>35.0</cell><cell>32.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Mediocre population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>29.2</cell><cell>74.0</cell><cell>27.2</cell><cell>21.0</cell><cell>21.5</cell><cell>22.8</cell><cell>36.6</cell><cell>82.8</cell><cell>36.1</cell><cell>26.1</cell><cell>28.1</cell><cell>32.0</cell></row><row><cell></cell><cell>0.4</cell><cell>29.2</cell><cell>63.3</cell><cell>23.6</cell><cell>21.9</cell><cell>20.6</cell><cell>18.8</cell><cell>36.6</cell><cell>72.2</cell><cell>31.5</cell><cell>32.1</cell><cell>28.4</cell><cell>26.5</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>29.2</cell><cell>56.0</cell><cell>20.9</cell><cell>24.6</cell><cell>22.8</cell><cell>22.7</cell><cell>36.6</cell><cell>65.1</cell><cell>24.8</cell><cell>31.5</cell><cell>31.3</cell><cell>28.7</cell></row><row><cell></cell><cell>0.6</cell><cell>29.2</cell><cell>49.5</cell><cell>21.8</cell><cell>26.2</cell><cell>16.7</cell><cell>23.2</cell><cell>36.6</cell><cell>58.4</cell><cell>30.3</cell><cell>32.7</cell><cell>24.5</cell><cell>30.0</cell></row><row><cell></cell><cell>Avg</cell><cell>29.2</cell><cell>60.7</cell><cell>23.4</cell><cell>23.4</cell><cell>20.4</cell><cell>21.9</cell><cell>36.6</cell><cell>69.6</cell><cell>30.7</cell><cell>30.6</cell><cell>28.1</cell><cell>29.3</cell></row><row><cell></cell><cell>0.3</cell><cell>24.8</cell><cell>74.0</cell><cell>21.0</cell><cell>19.1</cell><cell>21.5</cell><cell>19.6</cell><cell>22.8</cell><cell>82.8</cell><cell>28.5</cell><cell>21.1</cell><cell>25.0</cell><cell>22.4</cell></row><row><cell></cell><cell>0.4</cell><cell>24.8</cell><cell>63.3</cell><cell>22.3</cell><cell>24.1</cell><cell>24.5</cell><cell>20.9</cell><cell>22.8</cell><cell>72.2</cell><cell>29.2</cell><cell>28.7</cell><cell>26.4</cell><cell>24.9</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>24.8</cell><cell>56.0</cell><cell>20.6</cell><cell>26.7</cell><cell>21.6</cell><cell>20.7</cell><cell>22.8</cell><cell>65.1</cell><cell>26.2</cell><cell>30.2</cell><cell>26.1</cell><cell>24.6</cell></row><row><cell></cell><cell>0.6</cell><cell>24.8</cell><cell>49.5</cell><cell>21.0</cell><cell>23.7</cell><cell>16.1</cell><cell>19.7</cell><cell>22.8</cell><cell>58.4</cell><cell>27.7</cell><cell>27.0</cell><cell>19.2</cell><cell>24.8</cell></row><row><cell></cell><cell>Avg</cell><cell>24.8</cell><cell>60.7</cell><cell>21.2</cell><cell>23.4</cell><cell>20.9</cell><cell>20.2</cell><cell>22.8</cell><cell>69.6</cell><cell>27.9</cell><cell>26.8</cell><cell>24.2</cell><cell>24.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Good population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>29.2</cell><cell>57.7</cell><cell>18.3</cell><cell>21.9</cell><cell>18.7</cell><cell>19.4</cell><cell>36.6</cell><cell>67.9</cell><cell>25.6</cell><cell>30.7</cell><cell>24.5</cell><cell>26.1</cell></row><row><cell></cell><cell>0.4</cell><cell>29.2</cell><cell>43.5</cell><cell>13.6</cell><cell>19.2</cell><cell>14.9</cell><cell>17.3</cell><cell>36.6</cell><cell>54.3</cell><cell>21.2</cell><cell>27.3</cell><cell>21.9</cell><cell>26.3</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>29.2</cell><cell>28.7</cell><cell>7.9</cell><cell>21.1</cell><cell>14.3</cell><cell>19.0</cell><cell>36.6</cell><cell>41.1</cell><cell>11.9</cell><cell>27.7</cell><cell>21.3</cell><cell>25.4</cell></row><row><cell></cell><cell>0.6</cell><cell>29.2</cell><cell>20.4</cell><cell>5.5</cell><cell>16.2</cell><cell>12.6</cell><cell>13.9</cell><cell>36.6</cell><cell>29.9</cell><cell>7.9</cell><cell>23.8</cell><cell>19.0</cell><cell>19.6</cell></row><row><cell></cell><cell>Avg</cell><cell>29.2</cell><cell>37.6</cell><cell>11.3</cell><cell>19.6</cell><cell>15.1</cell><cell>17.4</cell><cell>36.6</cell><cell>48.3</cell><cell>16.6</cell><cell>27.4</cell><cell>21.7</cell><cell>24.4</cell></row><row><cell></cell><cell>0.3</cell><cell>24.8</cell><cell>57.7</cell><cell>19.7</cell><cell>22.4</cell><cell>16.9</cell><cell>19.4</cell><cell>22.8</cell><cell>67.9</cell><cell>27.1</cell><cell>27.0</cell><cell>20.9</cell><cell>22.3</cell></row><row><cell></cell><cell>0.4</cell><cell>24.8</cell><cell>43.5</cell><cell>14.0</cell><cell>18.5</cell><cell>12.8</cell><cell>18.6</cell><cell>22.8</cell><cell>54.3</cell><cell>18.8</cell><cell>18.8</cell><cell>14.9</cell><cell>20.2</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>24.8</cell><cell>28.7</cell><cell>12.7</cell><cell>19.1</cell><cell>9.7</cell><cell>17.4</cell><cell>22.8</cell><cell>41.1</cell><cell>18.0</cell><cell>24.4</cell><cell>12.9</cell><cell>21.5</cell></row><row><cell></cell><cell>0.6</cell><cell>24.8</cell><cell>20.4</cell><cell>8.2</cell><cell>16.3</cell><cell>9.4</cell><cell>13.8</cell><cell>22.8</cell><cell>29.9</cell><cell>11.5</cell><cell>21.3</cell><cell>10.0</cell><cell>15.6</cell></row><row><cell></cell><cell>Avg</cell><cell>24.8</cell><cell>37.6</cell><cell>13.7</cell><cell>19.1</cell><cell>12.2</cell><cell>17.3</cell><cell>22.8</cell><cell>48.3</cell><cell>18.9</cell><cell>22.9</cell><cell>14.7</cell><cell>19.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 The</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell cols="13">Obtained Structural Hamming Distances for the Alarm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(a) Weak population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>30.2</cell><cell>212.7</cell><cell>108.6</cell><cell>31.2</cell><cell>44.7</cell><cell>28.9</cell><cell>31.9</cell><cell>213.7</cell><cell>110.5</cell><cell>34.8</cell><cell>47.5</cell><cell>31.3</cell></row><row><cell></cell><cell>0.4</cell><cell>30.2</cell><cell>186.6</cell><cell>98.7</cell><cell>34.3</cell><cell>34.0</cell><cell>28.2</cell><cell>31.9</cell><cell>188.2</cell><cell>100.3</cell><cell>37.5</cell><cell>38.9</cell><cell>32.1</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>30.2</cell><cell>173.2</cell><cell>93.1</cell><cell>31.2</cell><cell>33.0</cell><cell>28.1</cell><cell>31.9</cell><cell>174.3</cell><cell>95.1</cell><cell>32.2</cell><cell>36.2</cell><cell>31.1</cell></row><row><cell></cell><cell>0.6</cell><cell>30.2</cell><cell>166.5</cell><cell>94.0</cell><cell>29.4</cell><cell>28.9</cell><cell>26.0</cell><cell>31.9</cell><cell>167.8</cell><cell>96.0</cell><cell>31.1</cell><cell>33.5</cell><cell>31.2</cell></row><row><cell></cell><cell>Avg</cell><cell>30.2</cell><cell>184.8</cell><cell>98.6</cell><cell>31.5</cell><cell>35.1</cell><cell>27.8</cell><cell>31.9</cell><cell>186.0</cell><cell>100.5</cell><cell>33.9</cell><cell>39.0</cell><cell>31.4</cell></row><row><cell></cell><cell>0.3</cell><cell>29.5</cell><cell>212.7</cell><cell>83.8</cell><cell>29.9</cell><cell>38.4</cell><cell>28.4</cell><cell>30.5</cell><cell>213.7</cell><cell>87.0</cell><cell>33.2</cell><cell>41.3</cell><cell>30.5</cell></row><row><cell></cell><cell>0.4</cell><cell>29.5</cell><cell>186.6</cell><cell>79.9</cell><cell>31.1</cell><cell>31.5</cell><cell>27.8</cell><cell>30.5</cell><cell>188.2</cell><cell>83.0</cell><cell>34.0</cell><cell>34.9</cell><cell>30.5</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>29.5</cell><cell>173.2</cell><cell>77.0</cell><cell>30.2</cell><cell>32.6</cell><cell>24.8</cell><cell>30.5</cell><cell>174.3</cell><cell>78.9</cell><cell>30.9</cell><cell>35.0</cell><cell>28.3</cell></row><row><cell></cell><cell>0.6</cell><cell>29.5</cell><cell>166.5</cell><cell>76.8</cell><cell>27.0</cell><cell>26.4</cell><cell>24.8</cell><cell>30.5</cell><cell>167.8</cell><cell>78.8</cell><cell>29.6</cell><cell>29.0</cell><cell>28.0</cell></row><row><cell></cell><cell>Avg</cell><cell>29.5</cell><cell>184.8</cell><cell>79.4</cell><cell>29.6</cell><cell>32.2</cell><cell>26.5</cell><cell>30.5</cell><cell>186.0</cell><cell>81.9</cell><cell>31.9</cell><cell>35.0</cell><cell>29.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Mediocre population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>30.2</cell><cell>123.0</cell><cell>61.8</cell><cell>29.1</cell><cell>30.5</cell><cell>25.3</cell><cell>31.9</cell><cell>125.5</cell><cell>65.7</cell><cell>32.7</cell><cell>34.8</cell><cell>29.4</cell></row><row><cell></cell><cell>0.4</cell><cell>30.2</cell><cell>95.9</cell><cell>48.6</cell><cell>28.0</cell><cell>21.7</cell><cell>23.4</cell><cell>31.9</cell><cell>98.7</cell><cell>51.8</cell><cell>30.6</cell><cell>25.3</cell><cell>26.0</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>30.2</cell><cell>87.0</cell><cell>43.7</cell><cell>29.9</cell><cell>21.9</cell><cell>28.1</cell><cell>31.9</cell><cell>89.3</cell><cell>47.0</cell><cell>33.0</cell><cell>24.3</cell><cell>32.2</cell></row><row><cell></cell><cell>0.6</cell><cell>30.2</cell><cell>78.5</cell><cell>38.1</cell><cell>22.5</cell><cell>15.6</cell><cell>17.6</cell><cell>31.9</cell><cell>81.1</cell><cell>40.1</cell><cell>26.8</cell><cell>17.8</cell><cell>20.3</cell></row><row><cell></cell><cell>Avg</cell><cell>30.2</cell><cell>96.1</cell><cell>48.1</cell><cell>27.4</cell><cell>22.4</cell><cell>23.6</cell><cell>31.9</cell><cell>98.7</cell><cell>51.1</cell><cell>30.8</cell><cell>25.5</cell><cell>27.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(c) Good population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>30.2</cell><cell>87.9</cell><cell>38.3</cell><cell>21.7</cell><cell>17.6</cell><cell>17.6</cell><cell>31.9</cell><cell>90.6</cell><cell>42.4</cell><cell>25.8</cell><cell>21.3</cell><cell>21.5</cell></row><row><cell></cell><cell>0.4</cell><cell>30.2</cell><cell>62.4</cell><cell>26.5</cell><cell>20.8</cell><cell>16.1</cell><cell>18.0</cell><cell>31.9</cell><cell>65.4</cell><cell>28.7</cell><cell>25.1</cell><cell>19.6</cell><cell>21.1</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>30.2</cell><cell>37.7</cell><cell>13.3</cell><cell>16.3</cell><cell>8.2</cell><cell>10.6</cell><cell>31.9</cell><cell>40.8</cell><cell>15.0</cell><cell>19.6</cell><cell>10.0</cell><cell>12.1</cell></row><row><cell></cell><cell>0.6</cell><cell>30.2</cell><cell>36.1</cell><cell>15.1</cell><cell>15.7</cell><cell>8.8</cell><cell>9.0</cell><cell>31.9</cell><cell>39.8</cell><cell>17.3</cell><cell>18.8</cell><cell>9.6</cell><cell>11.2</cell></row><row><cell></cell><cell>Avg</cell><cell>30.2</cell><cell>56.0</cell><cell>23.3</cell><cell>18.6</cell><cell>12.7</cell><cell>13.8</cell><cell>31.9</cell><cell>59.2</cell><cell>25.8</cell><cell>22.3</cell><cell>15.1</cell><cell>16.5</cell></row><row><cell></cell><cell>0.3</cell><cell>29.5</cell><cell>87.9</cell><cell>33.4</cell><cell>20.2</cell><cell>15.5</cell><cell>14.6</cell><cell>30.5</cell><cell>90.6</cell><cell>37.5</cell><cell>24.0</cell><cell>18.3</cell><cell>18.3</cell></row><row><cell></cell><cell>0.4</cell><cell>29.5</cell><cell>62.4</cell><cell>26.2</cell><cell>19.8</cell><cell>16.8</cell><cell>17.2</cell><cell>30.5</cell><cell>65.4</cell><cell>29.2</cell><cell>23.3</cell><cell>20.7</cell><cell>21.1</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>29.5</cell><cell>37.7</cell><cell>13.3</cell><cell>11.5</cell><cell>8.1</cell><cell>9.1</cell><cell>30.5</cell><cell>40.8</cell><cell>16.6</cell><cell>14.2</cell><cell>9.7</cell><cell>10.8</cell></row><row><cell></cell><cell>0.6</cell><cell>29.5</cell><cell>36.1</cell><cell>15.6</cell><cell>15.5</cell><cell>7.4</cell><cell>10.4</cell><cell>30.5</cell><cell>39.8</cell><cell>18.6</cell><cell>18.4</cell><cell>8.9</cell><cell>12.7</cell></row><row><cell></cell><cell>Avg</cell><cell>29.5</cell><cell>56.0</cell><cell>22.1</cell><cell>16.8</cell><cell>11.9</cell><cell>12.8</cell><cell>30.5</cell><cell>59.2</cell><cell>25.5</cell><cell>20.0</cell><cell>14.4</cell><cell>15.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="13">The Obtained Structural Hamming Distances for the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Hailfinder Network</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Weak population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell></cell><cell>0.3</cell><cell>67.0</cell><cell>409.5</cell><cell>79.1</cell><cell>40.6</cell><cell>48.0</cell><cell>35.0</cell><cell>70.1</cell><cell>415.9</cell><cell></cell><cell>45.5</cell><cell>52.5</cell><cell>40.5</cell></row><row><cell></cell><cell>0.4</cell><cell>67.0</cell><cell>399.9</cell><cell>86.4</cell><cell>44.2</cell><cell></cell><cell>36.3</cell><cell>70.1</cell><cell>405.5</cell><cell>92.3</cell><cell>48.9</cell><cell>49.7</cell><cell>41.5</cell></row><row><cell>1000</cell><cell>0.5</cell><cell>67.0</cell><cell>375.3</cell><cell>85.5</cell><cell>45.0</cell><cell>45.0</cell><cell>44.3</cell><cell>70.1</cell><cell>380.9</cell><cell>92.2</cell><cell>48.9</cell><cell>49.2</cell><cell>48.7</cell></row><row><cell></cell><cell>0.6</cell><cell>67.0</cell><cell>367.6</cell><cell>90.3</cell><cell>44.2</cell><cell>46.1</cell><cell>36.1</cell><cell>70.1</cell><cell>374.1</cell><cell>96.7</cell><cell>49.4</cell><cell>51.3</cell><cell>41.5</cell></row><row><cell></cell><cell>Avg</cell><cell>67.0</cell><cell>388.1</cell><cell>85.3</cell><cell>43.5</cell><cell>46.0</cell><cell>37.9</cell><cell>70.1</cell><cell>394.1</cell><cell>91.4</cell><cell>48.2</cell><cell>50.7</cell><cell>43.1</cell></row><row><cell></cell><cell>0.3</cell><cell>69.4</cell><cell>409.5</cell><cell>65.2</cell><cell>36.3</cell><cell>45.6</cell><cell>28.9</cell><cell>71.2</cell><cell>415.9</cell><cell>70.5</cell><cell>37.9</cell><cell>48.5</cell><cell>30.3</cell></row><row><cell></cell><cell>0.4</cell><cell>69.4</cell><cell>399.9</cell><cell>71.1</cell><cell>40.4</cell><cell>42.8</cell><cell>30.4</cell><cell>71.2</cell><cell>405.5</cell><cell>74.8</cell><cell>42.3</cell><cell>44.2</cell><cell>31.5</cell></row><row><cell>5000</cell><cell>0.5</cell><cell>69.4</cell><cell>375.3</cell><cell>72.9</cell><cell>42.5</cell><cell>40.9</cell><cell>39.3</cell><cell>71.2</cell><cell>380.9</cell><cell>76.1</cell><cell>44.3</cell><cell>41.8</cell><cell>41.5</cell></row><row><cell></cell><cell>0.6</cell><cell>69.4</cell><cell>367.6</cell><cell>71.5</cell><cell>41.2</cell><cell>38.7</cell><cell>30.9</cell><cell>71.2</cell><cell>374.1</cell><cell>77.6</cell><cell>44.8</cell><cell>40.0</cell><cell>32.8</cell></row><row><cell></cell><cell>Avg</cell><cell>69.4</cell><cell>388.1</cell><cell>70.2</cell><cell>40.1</cell><cell>42.0</cell><cell>32.4</cell><cell>71.2</cell><cell>394.1</cell><cell>74.8</cell><cell>42.3</cell><cell>43.6</cell><cell>34.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">(b) Mediocre population</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DAG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPDAG</cell><cell></cell><cell></cell></row><row><cell>|D|</cell><cell>ν</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">results for Asia and Alarm networks as two representative</cell></row><row><cell cols="5">examples. In Tables</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="9">The Obtained Structural Hamming Distances Using the</cell></row><row><cell cols="9">Marginalization-Based Score with Different Values of</cell></row><row><cell cols="9">Coefficient c for Asia and Alarm Bayesian Networks</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Weak population</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Asia BN</cell><cell></cell><cell></cell><cell cols="2">Alarm BN</cell><cell></cell></row><row><cell>ν/c</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell>0.3</cell><cell cols="3">7.7 7.5 6.7</cell><cell>6.5</cell><cell cols="4">29.4 29.4 28.9 29.0</cell></row><row><cell>0.4</cell><cell cols="3">5.5 5.3 5.7</cell><cell>6.0</cell><cell cols="4">28.7 29.3 28.2 32.4</cell></row><row><cell>0.5</cell><cell cols="3">4.2 4.0 3.8</cell><cell>5.0</cell><cell cols="4">29.5 29.3 28.1 29.3</cell></row><row><cell>0.6</cell><cell cols="3">4.6 4.3 3.9</cell><cell>4.4</cell><cell cols="4">28.9 26.8 26.0 25.8</cell></row><row><cell>Avg</cell><cell cols="3">5.5 5.3 5.0</cell><cell>5.5</cell><cell cols="4">29.1 28.7 27.8 29.1</cell></row><row><cell></cell><cell></cell><cell cols="5">(b) Mediocre population</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Asia BN</cell><cell></cell><cell></cell><cell cols="2">Alarm BN</cell><cell></cell></row><row><cell>ν/c</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell>0.3</cell><cell cols="3">3.7 3.1 2.8</cell><cell>3.5</cell><cell cols="4">24.3 24.9 25.3 27.3</cell></row><row><cell>0.4</cell><cell cols="3">4.1 4.3 4.3</cell><cell>4.9</cell><cell cols="4">22.2 21.3 23.4 24.4</cell></row><row><cell>0.5</cell><cell cols="3">1.8 1.4 1.7</cell><cell>2.3</cell><cell cols="4">28.6 27.6 28.1 27.9</cell></row><row><cell>0.6</cell><cell cols="3">3.1 2.5 1.3</cell><cell>1.9</cell><cell cols="4">22.0 19.0 17.6 21.0</cell></row><row><cell>Avg</cell><cell cols="3">3.2 2.8 2.5</cell><cell>3.2</cell><cell cols="4">24.3 23.2 23.6 25.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(c) Good population</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Asia BN</cell><cell></cell><cell></cell><cell cols="2">Alarm BN</cell><cell></cell></row><row><cell>ν/c</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell>0.3</cell><cell cols="3">3.9 3.2 3.2</cell><cell>3.2</cell><cell cols="4">20.2 18.4 17.6 18.6</cell></row><row><cell>0.4</cell><cell cols="3">3.4 2.0 2.0</cell><cell>2.3</cell><cell cols="4">15.7 16.7 18.0 17.5</cell></row><row><cell>0.5</cell><cell cols="3">1.4 0.5 1.1</cell><cell>1.4</cell><cell cols="4">10.6 10.6 10.6 12.5</cell></row><row><cell>0.6</cell><cell cols="3">0.9 1.1 0.9</cell><cell>0.6</cell><cell>12.3</cell><cell>8.1</cell><cell>9.0</cell><cell>12.6</cell></row><row><cell>Avg</cell><cell cols="3">2.4 1.7 1.8</cell><cell>1.9</cell><cell cols="4">14.7 13.5 13.8 15.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10 Table that the</head><label>10that</label><figDesc>Radiologists had to Fill in as Part of the Experiment. Entries in the Upper Triangular Part of the Table had to be Filled in by →, ←, , or Remain Empty if the Radiologists had no Idea what to Fill in</figDesc><table><row><cell>Microcalcifications</cell><cell>Spiculation</cell><cell>Location</cell><cell>Age</cell><cell>Lymph Nodes</cell><cell>Skin Retraction</cell><cell>Shape</cell><cell>Size</cell><cell>Breast cancer</cell><cell>Fibrous Tissue Develop</cell><cell>Breast Density</cell><cell>Margin</cell><cell>Nipple Discharge</cell><cell>Distort Architectural</cell><cell>Metastasis</cell><cell>Mass</cell></row><row><cell>Microcalcifications</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spiculation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Location</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Age</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lymph Nodes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Skin Retraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Breast cancer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fibrous Tissue Develop</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Breast Density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Margin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nipple Discharge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Architectural Distort</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metastasis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mass</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 11 The</head><label>11</label><figDesc>Accuracy Parameters and the Volume of Opinions Provided by Different Experts in the Breast Cancer Experiment</figDesc><table><row><cell></cell><cell cols="3">Accuracy Parameters</cell><cell cols="2">Volume of Opinions</cell></row><row><cell></cell><cell>γ1</cell><cell>γ2</cell><cell>γ3</cell><cell>number</cell><cell>ratio</cell></row><row><cell>1</cell><cell cols="2">0.17 0.06</cell><cell>0.93</cell><cell>120</cell><cell>1</cell></row><row><cell>2</cell><cell cols="2">0.44 0.11</cell><cell>0.72</cell><cell>120</cell><cell>1</cell></row><row><cell>3</cell><cell cols="2">0.50 0.08</cell><cell>0.50</cell><cell>34</cell><cell>0.28</cell></row><row><cell>4</cell><cell cols="2">0.50 0.17</cell><cell>0.73</cell><cell>120</cell><cell>1</cell></row><row><cell>5</cell><cell cols="2">0.59 0.24</cell><cell>0.61</cell><cell>104</cell><cell>0.87</cell></row><row><cell>6</cell><cell cols="2">0.75 0.17</cell><cell>0.63</cell><cell>60</cell><cell>0.5</cell></row><row><cell>7</cell><cell cols="2">0.91 0.09</cell><cell>0.56</cell><cell>43</cell><cell>0.36</cell></row><row><cell>Avg</cell><cell cols="2">0.55 0.13</cell><cell>0.67</cell><cell>85.86</cell><cell>0.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 12 The</head><label>12</label><figDesc>Obtained Structural Hamming Distances for the Breast Cancer Network with Real Experts</figDesc><table><row><cell>|D|</cell><cell cols="2">Data Expert</cell><cell>PE</cell><cell>Mean</cell><cell>EM</cell><cell>Marg</cell></row><row><cell>100</cell><cell>13.2</cell><cell>32.0</cell><cell>26.4</cell><cell>9.7</cell><cell>25.3</cell><cell>10.2</cell></row><row><cell>400</cell><cell>8.5</cell><cell>32.0</cell><cell>23.4</cell><cell>7.2</cell><cell>19.6</cell><cell>7.2</cell></row><row><cell>700</cell><cell>6.8</cell><cell>32.0</cell><cell>22.0</cell><cell>5.8</cell><cell>18.2</cell><cell>5.8</cell></row><row><cell>1000</cell><cell>6.8</cell><cell>32.0</cell><cell>21.0</cell><cell>5.8</cell><cell>17.9</cell><cell>5.8</cell></row><row><cell>Avg</cell><cell>8.8</cell><cell>32.0</cell><cell>23.2</cell><cell>7.1</cell><cell>20.3</cell><cell>7.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The detailed description of the process of simulating the experts' opinions based on their accuracy parameters is available at http://ceit. aut.ac.ir/ ∼ amirkhani.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The experts' instruction, including the description of the variables, is available at http://ceit.aut.ac.ir/ ∼ amirkhani.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This Bayesian network is available at http://www.cs.ru.nl/ ∼ peterl/teaching/CI/networks/bc.net.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We would like to thank <rs type="person">dr. Mechli Imhof-Tas</rs> from <rs type="affiliation">Rad-boudUMC, Nijmegen</rs> for her valuable help in gathering the opinions from the radiologists regarding the breast cancer network.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mohammad Rahmati received the MSc in electrical engineering from the University of New Orleans, USA in 1987 and the PhD degree in electrical and computer engineering from University of Kentucky, Lexington, KY USA in 1994. He is currently an associate professor at the Computer Engineering Department, Amirkabir University of Technology (Tehran Polytechnic). His research interests are in the fields of pattern recognition, image processing, bioinformatics, video processing, and data mining. He is the chair of the department and he is also a member of IEEE Signal Processing Society.</p><p>Peter J.F. Lucas is a principal investigator with the Institute of Computing and Information Sciences at Radboud University, Nijmegen, and professor at the Leiden Institute of Advanced Computer Science, Leiden University, The Netherlands. His research interests include probabilistic logics, probabilistic graphical models, decision-support systems, and mHealth solutions. Lucas received a PhD in mathematics and computer science from Free University, Amsterdam.</p><p>Arjen Hommersom received the PhD degree from the University of Nijmegen for his work on quality assurance of clinical practice guidelines. Currently, he is an assistant professor at the Open University, the Netherlands and a researcher at the Institute of Computing and Information Sciences at Radboud University Nijmegen, the Netherlands. His research interests include knowledge representation and reasoning, in particular probabilistic graphical models and probabilistic logic programming.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A scoring function for learning Bayesian networks based on mutual information and conditional independence tests</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2149" to="2187" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient structure learning of Bayesian networks using constraints</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="663" to="689" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A sparse structure learning algorithm for Gaussian Bayesian network identification from high-dimensional data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fleisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1328" to="1342" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Bayesian network structure: Towards the essential graph by integer linear programming tools</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haws</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1043" to="1071" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure learning of Bayesian networks by genetic algorithms: A performance analysis of control parameters</title>
		<author>
			<persName><forename type="first">P</forename><surname>Larra Ñaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yurramendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Murga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M H</forename><surname>Kuijpers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="912" to="926" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Research on learning Bayesian networks by particle swarm optimization</title>
		<author>
			<persName><forename type="first">X.-C</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="540" to="545" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note>Automatic Control</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counting unlabeled acyclic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LNM 622</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-sample learning of Bayesian networks is NP-hard</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Priors on network structures. Biasing the search for Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning with knowledge from multiple experts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian network learning algorithms using structural restrictions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Castellano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method for integrating expert knowledge when learning Bayesian networks from data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Masegosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1382" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Not just data: A method for improving prediction with knowledge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Marsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<meeting><address><addrLine>Palo Alto</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of Bayesian network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A characterization of Markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistic</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probable networks and plausible predictions-A review of practical Bayesian methods for supervised neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="469" to="505" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Theory refinement on Bayesian networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Seventh conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Being Bayesian about network structure. a Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Bayesian network structure learning using heterogeneous experts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Amirkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hommersom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multi-target prediction</title>
		<meeting><address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expectation maximization based ordering aggregation for improving the k2 structure learning algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Amirkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local computations with probabilities on graphical structures and their application to expert systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="page" from="157" to="224" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive probabilistic networks with hidden variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="213" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Alarm monitoring system: A case study with two probabilistic inference techniques for belief networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Beinlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Suermondt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Intelligence in Medicine</title>
		<meeting>the European Conference on Artificial Intelligence in Medicine</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hailfinder: A Bayesian system for forecasting severe weather</title>
		<author>
			<persName><forename type="first">B</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Bayes net toolbox for MATLAB</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing science and statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">BNT structure learning package: Documentation and experiments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Leray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Francois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The max-min hillclimbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An interactive approach for Bayesian network learning using domain/expert knowledge</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Masegosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1168" to="1181" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comparison of structural distance measures for causal Bayesian network models</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Jongh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Intelligent Information Systems, Challenging Problems of Science</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A probabilistic framework for image information fusion with an application to mammographic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Velikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="865" to="875" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the interplay of machine learning and background knowledge in image interpretation by Bayesian networks</title>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="86" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iran in 2015 for his work on expert-based structure learning of Bayesian networks. He is currently an assistant professor at the Technology and Engineering Department</title>
		<author>
			<persName><forename type="first">H</forename><surname>Amirkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include machine learning, pattern recognition, and data mining</title>
		<meeting><address><addrLine>Iran</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="212" to="222" />
		</imprint>
		<respStmt>
			<orgName>Computer Engineering Amirkabir University of Technology (Tehran Polytechnic ; University of Qom</orgName>
		</respStmt>
	</monogr>
	<note>Hossein Amirkhani received the PhD degree in artificial intelligence from the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
