<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GGNN@Causal News Corpus 2022: Gated Graph Neural Networks for Event Causality Identification from Social-Political News Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paul</forename><surname>Trust</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College Cork Cork</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rosane</forename><surname>Minghim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College Cork Cork</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kadusabe</forename><surname>Provia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Worldquant University Louisiana</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evangelos</forename><surname>Millos</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Dalhousie University Halifax</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GGNN@Causal News Corpus 2022: Gated Graph Neural Networks for Event Causality Identification from Social-Political News Articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The discovery of causality mentions from text is a core cognitive concept and appears in many natural language processing (NLP) applications. In this paper, we study the task of Event Causality Identification (ECI) from social-political news. The aim of the task is to detect causal relationships between event mention pairs in text. Although deep learning models have recently achieved a state-of-theart performance on many tasks and applications in NLP, most of them still fail to capture rich semantic and syntactic structures within sentences which is key for causality classification. We present a solution for causal event detection from social-political news that captures semantic and syntactic information based on gated graph neural networks (GGNN) and contextualized language embeddings. Experimental results show that our proposed method outperforms the baseline model (BERT (Bidirectional Embeddings from Transformers) in terms of f 1-score and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causality is a core cognitive concept and appears in many natural language processing (NLP) tasks. We can define causality in generic terms as a semantic relationship between two arguments known as cause and effect. The occurrence of one argument (cause argument) causes the occurrence of the other (effect argument) <ref type="bibr" target="#b5">(Feder et al., 2021;</ref><ref type="bibr">Tan et al., 2022b)</ref>.</p><p>Event Causality Identification (ECI) is a task that identifies causal relationships between events from a given text <ref type="bibr" target="#b25">(Zuo et al., 2021)</ref>. To understand how documents containing causal relationships are identified, we present a sample of 5 sentences highlighting causes, effects and causal-markers leading to the rationale for classifying different documents in Figure <ref type="figure" target="#fig_0">1</ref> . Let us take an example of two sentences; Sentence 1: "The protests spread to 15 other towns and resulted in two death and the destruc-tion of property" and sentence 5: "The properties including houses, banks were destroyed" as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Sentence 1 is causal and sentence 5 is non-causal. The first sentence is regraded as causal because it has the cause (in blue color) and effect (in green color) linked by a causal-marker (in red color) unlike the 5-th sentence which only has the effect. In general, an expression is regarded as noncausal if any of the following conditions are satisfied; (1) the reader is unable to construct a "why" question regarding the effect, (2) the cause does not precede the effect in time, (3) the effect is equally likely to occur or not without the cause and (4) the cause and effect can be swapped without change in meaning <ref type="bibr">(Tan et al., 2022b)</ref>.</p><p>Event Causality Identification has been actively studied in information retrieval with deep learning as the dominant approach delivering state-of-theart performance <ref type="bibr" target="#b1">(Chen et al., 2015;</ref><ref type="bibr" target="#b11">Lai et al., 2020;</ref><ref type="bibr" target="#b25">Zuo et al., 2021)</ref>. BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> has been utilized for automatic event causality detection on the Causal News Corpus (a dataset used in this study) <ref type="bibr">(Tan et al., 2022b,a)</ref>. The challenge with deep learning models is that they represent documents as a sequence of tokens either using the traditional count based methods or embedding based methods yet the task of causality detection requires understanding rich structures and reasoning within a sentence. The main contribution of this work is the use of the gated graph neural networks (GGNN) initialized with contextualized language representations on the task of causal event detection from social-political news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Background</head><p>In this section, we highlight some of the related work and background information relevant to our proposed methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Representations</head><p>The nature in which words are represented directly influences the performance of models trained using them on downstream tasks. Traditionally, documents were represented using bag of word approaches that base on co-occurrence statistics of terms within documents <ref type="bibr" target="#b18">(Salton et al., 1975)</ref>. The key challenge with this approach is that it does not easily capture semantic relationships among words. An alternative approach to bag of words is word embeddings <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>. Word embeddings represent words as real-valued vectors rather than counts capturing semantic and syntactic information. Word embeddings are classified into static word embeddings and contextualized word embeddings.</p><p>Static word embeddings obtain stand-alone representations of words without considering the context in which these words are used . Popular models in this category are Word2Vec models (Skip-gram and CBOW (Continuous bag of Words) ) <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>. Skip-gram uses center words to predict contextual words while CBOW uses contextual words to predict central words. GloVe (Global Vectors for Word Representation) <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> is a log bi-linear regression model which leverages co-occurrence statistics of the corpus to represent documents. Contextual embeddings such ELMO <ref type="bibr" target="#b17">(Peters et al., 2018)</ref> (Embeddings from Language Models) and BERT move beyond global representations like Word2Vec and assign each word a representation basing on its context hence achieving a better performance compared to static word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Deep learning models especially those based on the recent transformer architecture have become dominant strategies for NLP tasks because of their impressive performance. One of the most popular transformer models is BERT <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b22">Vaswani et al., 2017)</ref>. BERT is a language representation model that pre-trains deep bi-directional representations from unlabeled text by jointly conditioning on both left and right contexts in all layers. It is pre-trained with two objectives: masked language modeling and next sentence prediction using the bookcorpus (800 million words) and English wikepedia (2,500 million words).</p><p>Despite the impressive performance, transformer models represent documents as a sequence of tokens which is a limitation for some NLP problems that can be naturally expressed with a graph structure. There is now a growing interest to perform deep learning on graphs using graph neural networks. Graph neural networks exploit the global features in text representations learning by aggregating information from neighbors through edges. Convolutional neural networks were first extended to handle graphs for text classification <ref type="bibr" target="#b3">(Defferrard et al., 2016)</ref>. Graph Neural Networks have since been extended to other architectures like Recurrent Neural Networks and Gated Recurrent Unit <ref type="bibr" target="#b24">(Wu et al., 2021)</ref>. In our work, we apply models graph neural networks in an application context for event causality classification from social-political news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Event Causality Identification</head><p>The task of event causality detection from text is a semantically challenging task since it involves understanding the complex structure, relationships and dependencies within text. Traditional methods have used lexical and syntactical patterns <ref type="bibr" target="#b7">(Hashimoto, 2019;</ref><ref type="bibr" target="#b6">Gao et al., 2019)</ref>, co-occurrence statistics of events <ref type="bibr" target="#b10">(Hu et al., 2017)</ref>, causality markers like "due" and "because" <ref type="bibr" target="#b8">(Hidey and McKeown, 2016)</ref> and temporal semantics of events <ref type="bibr" target="#b15">(Ning et al., 2018)</ref>. Our proposed model uses GGNN to automatically extract and induce more abstract representations.</p><p>Advanced deep learning methods based on the transformer architecture <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> like BERT (Bidirectional Embeddings from Transformers) <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> have also been applied for this task <ref type="bibr" target="#b0">(Al-Garadi et al., 2022;</ref><ref type="bibr" target="#b14">Nan et al., 2020)</ref>. Even-though these models have achieved good per- A gated graph neural encoder (GGNN) and recurrent neural network decoder were used for graph neural network encoding. Finally, a fully connected neural networks was used for Event Causality Identification binary classification task formance on event causality detection, they represent text as sequences which may not be sufficient to capture the long dependencies that are required for this event causality detection task.</p><p>Graph neural networks which extract rich structures and represent text as graph have also been explored. Graph convolutional Network (GCN) have been proposed for document level event causality detection that captures inter-sentence event mention pairs <ref type="bibr" target="#b21">(Tran Phu and Nguyen, 2021)</ref>.</p><p>Our model is different from such related work in that we use a gated graph neural network on a novel dataset; Causal News Corpus where such models have not yet as of writing the paper not explored <ref type="bibr">(Tan et al., 2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our proposed methodology for the task of Event Causality Identification from social-political news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Representation</head><p>Formally, let us denote a corpus of N documents we would like to classify as D = {x i , y i } N where x i is the i-th document with a co-responding label y i ∈ Y for Y ∈ {1, ..., K}. Each document x i ∈ D is represented by a sequence of words {w 1 , ..., w nt }(w i ∈ v) where nt is the number of words in document x i and v is the vocabulary size.</p><p>We encode words w i ∈ x i into a continu- ] for an input into pre-trained BERT. We concatenate vectors of the top layers of the pre-trained BERT to obtain continuous vector representations of each word denoted as E = {e i , ...e n }. The embedding vectors in E are fed into a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) (Long Short Term Memory) to produce a sequence of hidden vectors h 0 = {h 0 n , ..., h 0 n } that will be used as initialization to the graph encoder <ref type="bibr" target="#b24">(Wu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated Graph Neural Encoder</head><p>After representing each word in the corpus C with a corresponding word embedding, we build a graph representation of all documents in the corpus and their associated dependencies. To apply our encoder, we represent our documents as G = (V, E), where V indicates a set consisting of different word embeddings for each word in the vocabulary and E indicates a set of edges (relationships) formed between documents.</p><p>We use a Gated Graph Neural network (GGNN) which is a modification of the vanilla Graph Neural Network by adding Gated Recurrent Unit filters <ref type="bibr" target="#b2">(Chung et al., 2014)</ref>. Our GGNN encoder consists of L stacked GGNN layers operating over a sequence of hidden vectors at the i-th layer h (i) . The hidden vector h l i at the l-th layer is computed by averaging the hidden vectors of neighboring nodes x i at the (l -1)-th layer: Gated Recurrent Unit (GRU) is used to update node embeddings by incorporating the aggregated information taking into consideration of edge type and edge direction:</p><formula xml:id="formula_0">h (0) i = [x T i , 0] T a (l) i = A T i: [h (l-1) i , ..., h (l-1) n ] T h (l) i = GRU (a (l) i , h (l-1) l )<label>(1)</label></formula><p>where A ∈ R is a matrix determining how nodes in the graph are communicating with each other, x i are the initial node features, a</p><p>i is the aggregation of information from different nodes and h (l) i is the i-th hidden state at the l-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recurrent Neural Network Decoder</head><p>The graph-level embeddings C obtained by the Graph Encoder are fed into a sequence decoder as heuristic information. In the decoding stage, an embedding layer is used to embed all the previous sequences. We used graph embedding C and sequence embedding e t at time step t using a recurrent neural network:</p><formula xml:id="formula_2">h t =RN N (Concat(e (t) , C), h (t-1) ) y t =F C(e (t) , h (t) , C)</formula><p>where h (t) represents hidden state at time step t, F C(.) represents fully connected layer and we initialize the hidden state with global graph representation C i.e h (0) = C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>The dataset used for experiments in this paper was provided by the organizers of the shared task on Causal Event Classification organized at 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE) at EMNLP 2022. The training data consists of 2925 news articles, validation set contained 323 news articles and test data consisted of 311 news articles <ref type="bibr">(Tan et al., 2022b,a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We conduct experiments with pre-trained BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and gated graph neural networks . Experiments are done with 50 epochs, max length of 512, batch size of 50 and the learning rate was set at 0.0005. The final submissions are evaluated using f 1-score. Transformers are implemented using hugging-face transformer library <ref type="bibr" target="#b23">(Wolf et al., 2020)</ref> and graph neural networks were implemented using graph4nlp library <ref type="bibr" target="#b24">(Wu et al., 2021)</ref>. Our code implementation can be found on the this link (<ref type="url" target="https://github.com/TrustPaul/ggnn.git">https://github.com/TrustPaul/ ggnn.git</ref>). Experimental results demonstrate that the performance of our proposed method (GGNN-B) compared to the baseline method that uses BERT <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr">Tan et al., 2022b)</ref> proposed by <ref type="bibr">Tan et al.,(2022)</ref>   Experimental results on the test set demonstrate that our proposed method GGNN-B achieves an accuracy of 80.06% compared to an accuracy of 77.81% achieved by the baseline model (BERT). GGNN-B (our model) achieves a better f1-score compared to the baseline (82.58% against 81.12%) but BERT outperforms the same graph neural network architecture initialized with Word2Vec embeddings <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>We hypothesize that the performance difference observed between our model which is based on graph neural networks and the baseline model based on only BERT is due to the superiority of graphs in representing complex structures required for understanding causal relationship against BERT that represents text as sequences. The fact that BERT outperforms Graph Neural networks when initialized with Word2Vec reinforces the role played by graph initialization of graph neural networks on performance and also demonstrates the advantages of contextualized embeddings extracted by BERT to downstream tasks over static embeddings extracted by Word2Vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel deep learning approach for event causality detection from socialpolitical news articles. Our proposed approach use gated graph neural networks and contextualized language representations which represent text documents as a graph and model complex semantic relationships ideal for causality detection. Experimental results reveal that our proposed model improves performance over the baseline comparison model (BERT) in terms of accuracy (80.06% versus 77.81%) and f 1-score (82.58% versus 81.12%).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of different text statements indicating whether they contain causal relationships or not. The causal markers are in red color, causes are in blue color and effects are in green color</figDesc><graphic coords="1,306.14,330.82,223.94,125.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We first obtain contextualized embeddings of the news articles which we use to build a graph representation. A gated graph neural encoder (GGNN) and recurrent neural network decoder were used for graph neural network encoding. Finally, a fully connected neural networks was used for Event Causality Identification binary classification task</figDesc><graphic coords="3,113.38,70.85,368.51,207.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ous vector representation using contextualized language representations produced by BERT (Devlin et al., 2019). Each document x i in the corpus is represented in one token sequence which may contain a single sentence or a pair of sentences. The first token of every sequence is always a special classification token ([CLS]) and different sentences are separated by a special token ([SEP]). Documents are represented as follows [[CLS],w 1 , ...w n ,[SEP],w t ,[SEP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>f 1-score and accuracy on the development set of the baseline model (BERT (Bidirectional Embeddings from Transformers) and our proposed model (GGNN(Gated Graph Neural Network<ref type="bibr" target="#b12">(Li et al., 2016;</ref><ref type="bibr" target="#b4">Devlin et al., 2019;</ref> Tan et al., 2022b))    </figDesc><table><row><cell>Model</cell><cell>f1</cell><cell>Accuracy</cell></row><row><cell cols="2">BERT (Baseline) 80.06</cell><cell>81.11</cell></row><row><cell>GGNN-W2V</cell><cell>81.01</cell><cell>75.23</cell></row><row><cell cols="2">GGNN-B(Ours) 84.78</cell><cell>84.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>as shown in Table1. Our method improves over the baseline in terms of precision (84.78% versus 80.06%), f1 (86.19 versus 83.47%) and accuracy (84.52% versus 81.11).</figDesc><table><row><cell cols="3">However fine-tuned BERT outperforms GGNN-</cell></row><row><cell cols="3">W2V (83.47% against 76.19%) in terms of f1-</cell></row><row><cell cols="3">score, a gated neural network of the same archi-</cell></row><row><cell cols="3">tecture as GGNN-B but with the graph constructed</cell></row><row><cell cols="2">with Word2Vec embeddings.</cell><cell></cell></row><row><cell>Model</cell><cell>f1</cell><cell>Accuracy</cell></row><row><cell cols="2">BERT (Baseline) 78.01</cell><cell>77.81</cell></row><row><cell>GGNN-W2V</cell><cell>75.72</cell><cell>72.03</cell></row><row><cell cols="2">GGCN-B(Ours) 81.67</cell><cell>80.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: f 1-score and accuracy on the test set of the baseline model (BERT (Bidirectional Embeddings from Transformers) and our proposed model (GGNN(Gated Graph Neural Network (Li et al., 2016; Devlin et al., 2019; Tan et al., 2022b))</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We thank <rs type="institution">Science Foundation Ireland (SFI) Center for Research Training in Advanced Networks and Future communications at University College Cork</rs> for funding this research and <rs type="institution">Irish Centre for High-End Computing (ICHEC)</rs> for providing access to computing power for running some of our experiments.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural language model for automatic identification of intimate partner violence reports from twitter. Array</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al-Garadi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sangmi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elise</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Chi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahithi</forename><surname>Lakamana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abeed</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.array.2022.100217</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">100217</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Causal inference in natural language processing</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Wood-Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Estimation. Prediction, Interpretation and Beyond</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling document-level causal structures for event causal relation identification</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kumar Choubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised multilingual causality extraction from Wikipedia</title>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2988" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying causal relations using parallel Wikipedia articles</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Germany. Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1424" to="1433" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inference of fine-grained event causality from blogs and films</title>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2708</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Events and Stories in the News Workshop</title>
		<meeting>the Events and Stories in the News Workshop<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="52" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Event detection: Gate diversity and syntactic importance scores for graph convolution neural networks</title>
		<author>
			<persName><forename type="first">Dac</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Ngo Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5405" to="5411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Workshop Track Proceedings</publisher>
			<date type="published" when="2013-05-02">2013. May 2-4, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint reasoning for temporal and causal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1212</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2288" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Shu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Onur Uca, Farhana Ferdousi Liza, and Nelleke Oostdijk. 2022a. Event causality identification with causal news corpusshared task 3, CASE 2022</title>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Anting Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2022)</title>
		<meeting>the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2022)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Onur Uca, Farhana Ferdousi Liza, and Tiancheng Hu. 2022b. The causal news corpus: Annotating causal relations in event sentences from news</title>
		<author>
			<persName><forename type="first">Fiona Anting</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iqra</forename><surname>Ameer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<meeting>the Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2298" to="2310" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for event causality identification with rich document-level structures</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3480" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning on graphs for natural language processing</title>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-tutorials.3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving event causality identification via selfsupervised representation learning on external causal statement</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.190</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2162" to="2172" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
