<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introducing Diminutive Causal Structure into Graph Representation Learning</title>
				<funder ref="#_rF6ydaW #_VjmM8bT #_RdBHMGS">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-14">June 14, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fengge</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangmeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changwen</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science &amp; Technology on Integrated Information System Laboratory</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Introducing Diminutive Causal Structure into Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-14">June 14, 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.08709v1[cs.LG]</idno>
					<note type="submission">Preprint submitted to Arxiv</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph representation learning</term>
					<term>Graph Neural Network</term>
					<term>Causal learning</term>
					<term>Causal Structure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships. A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model. However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset. Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible. Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process. Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models. Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance. Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process. Theoretical analysis serves to corroborate the efficacy of our proposed method. Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph representation learning is a specialized field that presents unique challenges compared to other representation learning tasks such as images, videos, or text. The studied graph data finds extensive practical applications in research, such as knowledge graphs [1, 2, 3], social networks <ref type="bibr" target="#b4">[4]</ref>, and molecular analysis <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>. Graphs inherently encapsulate richer semantics as they encode intricate relationships and connections between entities <ref type="bibr" target="#b8">[8]</ref>. Such property heightened semantic complexity often leads to increased intricacy in the learning process <ref type="bibr" target="#b9">[9]</ref>. Additionally, due to the nature of graph data, it is subject to more stringent structural constraints <ref type="bibr" target="#b10">[10]</ref>. However, when employing GNNs for graph representation learning, conventional GNNs model the existing associations in the data through an end-to-end approach, without delving into the exploration of the inherent causal relationships present in the data <ref type="bibr" target="#b11">[11]</ref>. Therefore, when conducting end-to-end graph representation learning with GNNs, two primary challenges arise: effectively modeling the intricate relationships and rules within the complex graph data to capture genuine causal relationships, while simultaneously preventing the model from being perturbed by potential confounding factors <ref type="bibr" target="#b12">[12]</ref>.</p><p>Recently, methods including DIR <ref type="bibr" target="#b13">[13]</ref> and RCGRL <ref type="bibr" target="#b14">[14]</ref> have introduced causal learning mechanisms into GNN model training, such as interventions and instrumental variable, to enable Email address: jiangmeng2019@iscas.ac.cn ( Jiangmeng <ref type="bibr">Li)</ref> the model to capture causal relationships better and eliminate confounding factors. Additionally, approaches such as RCL-OG <ref type="bibr" target="#b15">[15]</ref> and ACD <ref type="bibr" target="#b16">[16]</ref> directly utilize causal discovery methods to construct causal graphs from data. There is also some methods that focus on utilizing causal learning to enhance the interpretability of GNN models <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. These methods have demonstrated significant effectiveness and contributed to the optimization of graph learning tasks. However, it is crucial to note that none of these methods can guarantee that the model accurately learns causal relationships. The incorporation of causal learning methods does not ensure that the relationships modeled by the model are genuinely causal. Simultaneously, methods for constructing causal graphs or boosting model interpretability struggle to effectively leverage discovered causal knowledge to guide GNN models, thus limiting their ability to enhance GNN models' capability to model causal relationships. Therefore, we cannot help but wonder: If we possess prior knowledge about specific causal relationships, can we inject this knowledge into GNN models and ensure that the model can reliably identify and model these relationships? Furthermore, can we ensure that such injection of prior knowledge is applicable and helpful to a wide range of scenarios?</p><p>One approach to addressing this challenge is to directly inject the rules or relationships that correspond to the graph data into the model. For typical machine learning tasks, such as image recognition and text analysis, we possess well-founded insights about causal structures that we can express symbolically. These insights range from common-sense intuitions about how the world operates to advanced scientific knowledge. Recent research shows that introducing such causal structures into neural network models can significantly enhance their performance <ref type="bibr" target="#b20">[20]</ref>. However, in the context of graph representation learning, achieving this directly is challenging. Due to the complexity of graph data, it is difficult to identify a comprehensive causal structure that can describe the general rules or relationships governing graph data as a whole. Instead, we often find ourselves dealing with what we term 'diminutive causal structures', which can only be used to describe the rules that apply to specific, limited subsets of graph data (Please refer to Definition 1 for a precise definition of 'diminutive causal structure'). Therefore, there are several fundamental questions that need to be answered here: What is the relationship between graph representation learning models and diminutive causal structures? Does the introduction of such diminutive causal structures still contribute to improving the effectiveness of graph representation learning?</p><p>To acquire the answers, we intuitively introduce the experiments to investigate the relationship between GNN models and diminutive causal structures. As demonstrated in Figure <ref type="figure">1(a)</ref>, GNNs tend to converge towards models capable of representing diminutive causal structures for specific graph data as training progresses. Based on this observation, we propose that as training progresses and GNN acquires more information about the training dataset, certain internal structures within the GNN model gradually approach diminutive causal structures. This phenomenon suggests that such causal structures benefit the task, leading the GNN to gradually approximate similar structures during end-to-end training. However, it is evident that the GNN fails to achieve complete consistency with diminutive causal structures. Hence, we hypothesize that actively introducing such causal structures can further optimize the GNN and yield improved performance. To facilitate further exploration, we conduct experiments on various domains in Figure <ref type="figure">1(b)</ref>. Based on the empirical results, it can be observed that there is a general boost of model performances of GNNs introducing diminutive causal structures to a certain extent across different tasks. The observation leads to an empirical conclusion: introducing diminutive causal structure into graph representation learning practically improves the model performance.</p><p>In light of such empirical finding, we propose the Diminutive Causal Structure guided Graph Representation Learning, dubbed DCSGL, to improve the model to learn the diminutive causal structure that is related to the graph representation learning tasks. DCSGL achieves the aforementioned objectives by enabling the GNN model to learn knowledge from higherlevel causal models constructed with diminutive causal structures. We also conduct a theoretical analysis of DCSGL based on the Structural Causal Model (SCM) <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>, demonstrating its effectiveness. Moreover, we further incorporate the concept of interchange intervention <ref type="bibr" target="#b20">[20]</ref>, enabling DCSGL to facilitate more comprehensive learning. The sufficient comparisons on various datasets, including the crafted and real-world datasets, support the consistent effectiveness of DCSGL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>• We discover an intriguing observation, which leads to an empirical conclusion: introducing diminutive causal structure into GNNs improves the model performance. Additionally, we undertake a thorough investigation, employing comprehensive theoretical analyses and rigorous proofs, to substantiate and elucidate this conclusion.</p><p>• We introduce a novel DCSGL method, which enables the GNN model to learn diminutive causal structure that is associated with the graph representation learning task, thereby refining its performance.</p><p>• Extensive empirical evaluations on various datasets, including crafted and benchmark datasets, demonstrate the effectiveness of the proposed DCSGL.</p><p>The outline of the article includes a review of related work in Section 2, covering research on causal learning and GNNs, as well as the application of causal learning methods to GNNs. Section 3 outlines the methodology, introducing foundational concepts of causal structures and Diminutive Causal Structures, along with interventional exchange methods. Theoretical analysis in Section 4 includes the introduction of a structural causal model and a detailed examination of the proposed approach. Finally, Section 5 validates the approach through empirical studies, leveraging topological and semantic domain knowledge for comparison with other methods and further experimental analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Causal Learning</head><p>Causal learning employs statistical causal inference techniques <ref type="bibr" target="#b25">[25]</ref> to uncover the causal relationships among observable variables. Integrating causal learning into deep learning algorithms is a growing area of interest. Researchers are exploring how causal models can enhance the performance, interpretability, and fairness of deep learning models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. This includes developing methods to incorporate causal assumptions <ref type="bibr" target="#b20">[20]</ref>, leveraging causal structures in deep learning architectures <ref type="bibr" target="#b32">[32]</ref>, and incorporating causal explanations into model predictions <ref type="bibr" target="#b33">[33]</ref>.</p><p>However, traditional causal inference, often reliant on direct interventions in the studied system <ref type="bibr" target="#b34">[34]</ref>, encounters challenges when employed in deep learning models. The causal learning methods used in deep learning models often struggle to implement changes in experimental scenarios corresponding to the dataset. Consequently, while these methods have demonstrated promising results, they cannot guarantee the definitive learning of causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Neural Networks</head><p>GNNs have introduced the mechanism of neural networks into the realm of graph representation learning. Since the inception of the first GNN model, multiple variants have been proposed, including GCN <ref type="bibr" target="#b35">[35]</ref>, GAT <ref type="bibr" target="#b36">[36]</ref>, GIN <ref type="bibr" target="#b37">[37]</ref>  <ref type="bibr" target="#b21">[21]</ref> and ARMA <ref type="bibr" target="#b22">[22]</ref>) and the proposed method on various datasets. The left plot demonstrates the KL divergence between the features derived by the model capable of representing diminutive causal structures and the candidate GNN model. The right plot collects the classification performance of the compared methods. The empirical observations jointly support that our proposed method can better learn the diminutive causal structures and consistently outperform the baseline.</p><p>Figure <ref type="figure">1</ref>: The motivating explorations.</p><p>Extremum GNN <ref type="bibr" target="#b21">[21]</ref>, GSAT <ref type="bibr" target="#b38">[38]</ref> among others. These models perform graph representation learning by propagating information between adjacent nodes. In addition, to improve performance and adapt to a broader range of downstream tasks, paradigms commonly employed by other types of deep learning architectures have been applied to GNNs, such as unsupervised learning <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref>, semi-supervised learning <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref>, meta learning <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref>, and reinforcement learning <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>The aforementioned GNNs, akin to other types of neural networks, are trained using an end-to-end approach, fundamentally modeling the probabilistic relationships between the data. However, this manner of relationship modeling hinders the accurate representation of causal relationships, particularly when dealing with intricate and semantically rich graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Application of Causal Learning within GNNs</head><p>Despite the achievements of GNNs, these approaches encounter difficulties in explicitly uncovering causal relationships. To overcome this limitation, researchers have integrated causal inference techniques into GNN-based graph representation models <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref>. Some focused on utilizing causal learning to enhance the interpretability of GNN models <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>, e.g., GLGExplainer <ref type="bibr" target="#b53">[53]</ref> tries to represent the causal structure within the model more understandably. Some methods attempt to directly mine causal structures from the data <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Other methods facilitate the incorporation of causality through data intervention <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b14">14]</ref>. Furthermore, some works have successfully combined GNNs with causal learning for time-series data, achieving promising results <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57]</ref>. As a representative approach, DIR <ref type="bibr" target="#b13">[13]</ref> proposes to employ adaptive graph neural networks to identify distinct data within a graph, and then guide causal interventions to introduce causality into the model to boot the performance of models within Out-Of-Distribution (OOD) tasks.</p><p>However, the aforementioned methods, while achieving satisfactory results, still exhibit certain limitations. Approaches aiming to enhance model interpretability and conduct causal model mining have not effectively reinforced GNN models using the acquired causal relationships. Similarly, methods that leverage causal learning for GNN model improvement encounter challenges in accurately learning causal relationships due to the intricate nature of graph representation learning. Theoretically, the prerequisites of causal reasoning methods are challenging to meet in the context of GNNs, given the complex nature of data processing and the inherent nature of end-to-end training. Therefore, our approach endeavors to take a different route by directly injecting easily obtainable diminutive causal structures into the model to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we introduce our methodology, DCSGL, providing a detailed exposition on our proposed concept of 'diminutive causal structures' and explaining how we integrate them into our model. We will also outline the enhancements made to the entire process to achieve optimal results. For clarification, we summarizes the main notations within Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>We begin by reviewing some concepts related to causal learning, as they will be employed in the subsequent narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Causal Structure</head><p>Causal structure refers to the underlying arrangement or configuration of causal relationships among variables or events in a system <ref type="bibr" target="#b58">[58]</ref>. It represents the way in which different elements influence one another and the directionality of these influences. Understanding the causal structure of a system is crucial in causal inference and learning, as it helps uncover the relationships between variables and enables predictions or interventions based on a deeper comprehension of the cause-andeffect mechanisms at play. We will leverage the concept of causal structure, coupled with graph representation learning, to analyze which types of causal Structures are more readily attainable and expressible. Additionally, we will elucidate how to incorporate them into our GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Interchange Intervention</head><p>For a model M that is used to process two different input samples, X a and X b , the interchange intervention <ref type="bibr" target="#b20">[20]</ref> conducted on a model M can be viewed as providing the output  In order to convey the information more clearly, we provide the precise definition of diminutive causal structure. For graph G, we denote the graph data possesses causal relationships with the ground truth labels as causal part X and the data unrelated to the labels as confounder C. In graph representation learning, accurately modeling the causal relationship between X and Y proves challenging, involving intricate task-specific knowledge that is complex and difficult to analyze and acquire. However, more generalized foundational knowledge about X can be acquired. Specifically, based on domain-specific knowledge, we can analyze the factors S that influence the composition of X. The causal structure between S and X is then referred to as the diminutive causal structure. We provide its formal definition below:</p><formula xml:id="formula_0">Definition 1. (Diminutive Causal Structure).</formula><p>Diminutive causal structure denotes a causal structure that represents the causal relationship between a subset of data in X and the factors S . S be the causes of X. Furthermore, there is no direct causal effect between S and Y.</p><p>We will illustrate the diminutive causal structure based on a specific example. In Figure <ref type="figure">3</ref>, we present a graph sample from the Graph-Twitter dataset <ref type="bibr" target="#b59">[59]</ref>. The Graph-Twitter dataset consists of a series of syntactic dependency tree graphs extracted from a collection of movie comments. The task of the dataset is to determine, based on these syntactic dependency tree graphs, whether the corresponding comments contain positive or negative evaluations.</p><p>Specifically regarding the sample illustrated in Figure <ref type="figure">3</ref>, we have provided its corresponding raw textual data, namely, 'You The confounder within the graph sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>Factors influencing X that can be explicitly delineated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M(•)</head><p>Concrete model that represent certain diminutive causal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ϕ(•)</head><p>The node selection procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>The output prediction of M(•). q(•) q(•) represents the probability density function that T follows. t</p><p>The value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{(•)</head><p>GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>The prediction of T given based on {(•). p(•) p(•) represents the probability density function that T follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>The value space of T and T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KL(•)</head><p>The KL-divergence.</p><formula xml:id="formula_1">Mγ (•) M(•) after γ-th interchange intervention φγ (•) ϕ(•) after γ-th interchange intervention D KL</formula><p>Function that calculates the KL divergence via a discrete manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G i</head><p>The i-th graph sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>Total number of the graph samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>Total number of interchange intervention.</p><formula xml:id="formula_2">f m (•)</formula><p>The output of the m-th layer of f (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POOL(•)</head><p>Average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP(•)</head><p>Multi-layer perception.</p><formula xml:id="formula_3">I(; ) Mutural information. S *</formula><p>Factors influencing X that can't be explicitly delineated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><p>The model's representation of G.</p><p>were correct on Harry Potter but wrong with this one!' In this sentence, the conjunction 'but' is present, and based on linguistic knowledge, we can determine that there is a semantic contrast between the content before and after 'but.' In the graph below, we use white nodes to represent the content before 'but' and deep blue to represent the content after 'but.' At this point, the presence of 'but' in the sentence can be considered as cause factor S , and such S leads to a semantic shift between the content represented by the white nodes and that represented by the deep blue nodes in the graph. Furthermore, the knowledge related to 'but' is associated with the causal information X in the graph data G. As the Graph-Twitter dataset is a semantic sentiment analysis dataset, domain-specific knowledge, such as 'but' indicating a semantic contrast, is relevant to the task. We can consider that conjunction 'but' leads to a semantic contrast as a diminutive causal structure because it reveals a causal relationship between X and S . Meanwhile, 'but' is not directly related to the labels; the presence or absence of 'but' cannot be used to determine whether the entire graph corresponds to a positive or negative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Guided Learning</head><p>Next, we incorporate the diminutive causal structure into our model to enhance its performance. To achieve this, we represent the diminutive causal structure as a concrete model, denoted as M(•). M(•) output the prediction T about S based on graph data G. For instance, using the example above, we can define M(•)'s output T as whether the conjunction 'but' exists in the graph. In Section 5.1.2 and 5.2.2, we provide more detailed descriptions of the specific designs of M(•) for different diminutive causal structures.</p><p>With M(•), our objective is to enable our GNN model, denoted as f (•), to possess an internal causal structure that realizes M(•), so as to learn the diminutive causal structure. In simple terms, we believe incorporating the diminutive causal structure within M(•) into our model is advantageous for training. This is akin to introducing domain-specific knowledge to the model. We also leverage Theorem 3 to theoretically substantiate the effectiveness of this approach.</p><p>To achieve the objective of introducing M(•), we enforce f (•) to output a prediction of S . We denote such prediction as T , T ∼ q(t). t is the value. q(t) is the corresponding probability density function of T . T and T share the same value space T . Furthermore, we assume T ∼ p(t). p(t) is the corresponding probability density function of T . Intuitively, if given S , the output distributions of f (•) and M(•) are the same, i.e., the following equation holds:</p><formula xml:id="formula_4">p(t| s) = q(t| s), t ∈ T , s ∈ S,<label>(1)</label></formula><p>then f (•) has learned the knowledge that M(•) possesses. Theorem 4 gives a theoretical justification for such intuition. We utilize the KL divergence to assess the distance between p(t| s) and q(t| s), and formulate our training objective as follows:</p><formula xml:id="formula_5">L o = KL p(t| s)||q(t| s) .<label>(2)</label></formula><p>In practical, we could let M(•) and f (•) directly predict the probability values of p(t| s) and q(t| s). For training set {G i } N i=1 , we propose the following loss function:</p><formula xml:id="formula_6">L c = N i=1 D KL M(G i ), f (G i ) ,<label>(3)</label></formula><p>where D KL (•) calculates the KL divergence of two input distributions via a discrete manner. We have provided a justification for the rationality of this loss function through Corollary 5.</p><p>With Equation <ref type="formula" target="#formula_6">3</ref>, we are able to align the output of function f (•) with that of M(•). By doing so, we effectively induce convergence in the internal logic of f (•) towards that of M(•). However, for downstream tasks, our objective is not to make f (•) identical to M(•). What we aspire to achieve is the incorporation of the causal structure represented by M(•) within f (•), while allowing f (•) to acquire necessary knowledge from generic dataset-based training. To accomplish this, we constrain the scope of the impact of the loss function L c . Specifically, we opt to select specific node features from the m-th layer of f (•), with m as a hyperparameter. In essence, we choose to extract node features from the m-th layer of f (•) as a means to ensure that f (•) captures the causal structure represented by M(•), while retaining the ability to learn essential knowledge through general dataset-based training. Furthermore, we only select the graph data that may be related to S . For instance, for the data corresponding to Figure <ref type="figure">3</ref>, we only choose the features of the lexical nodes corresponding to the occurrence of 'but' in the sentence. We denote the selection procedure as ϕ(•). As described in the example above, the framework of ϕ(•) is closely tied to the chosen diminutive causal structure. Therefore, we provide a detailed explanation of the concrete framework of ϕ(•) in the specific exposition of M(•) in sections 5.1.2 and 5.2.2. Next, we conduct average pooling <ref type="bibr" target="#b35">[35]</ref> on the selected node features, the project them into probability predictions using a Multi-layer Perception (MLP) <ref type="bibr" target="#b60">[60]</ref>. Therefore, we can formulated our altered loss fuction as follows:</p><formula xml:id="formula_7">L c = N i=1 D KL M(G i ), MLP • POOL • ϕ • f (m) (G i ) , (4)</formula><p>where MLP(•) denotes the Multi-layer Perception, POOL(•) denotes the average pooling operation, • represents the composition of functions, f (m) (G i ) denote the output of the m-th layer of f (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Enhancement with Interchange Interventions</head><p>The aforementioned design delineates a viable approach to facilitate the GNN model in acquiring knowledge regarding the diminutive causal structure, enabling f (•) to assimilate precise causal structural information encapsulated in M(•). However, it is noteworthy that the dataset employed may not comprehensively cover graph data related to S ; in fact, only a minute fraction may be directly associated with S . For example, as previously mentioned, the incidence rate of the word 'but' in the sample extracted from the Graph-Twitter dataset is merely 5%. Such limited data may prove inadequate for robust model training. To ensure that the model f (•) can effectively capture the desired knowledge, we leverage interchange intervention <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b61">61]</ref>, a method rooted in causal theory , to facilitate and augment the learning process. A comprehensive introduction to interchange intervention is provided in Section 3.1.2.</p><p>We conduct interchange intervention on the GNN model by modifying ϕ(•), which selects the node features for training. By changing ϕ(•), we effectively altered the original input values of the subsequent neural network, akin to what an interchange intervention accomplishes. Subsequently, given our explicit understanding of the intervention we performed, we can instruct model M(•) to provide accurate altered predictions for T given certain interchange intervention, thereby augmenting the quantity of data available for training. We continue to elucidate using the example in Figure <ref type="figure">3</ref>. In Figure <ref type="figure">3</ref>, the nodes within the graph are input into the model for training. We have the capability to selectively remove specific nodes from such sentences. For instance, we can eliminate 'but' and the subsequent content, thereby altering the semantic meaning of the sentence. Clearly, in this case, the word 'but' no longer exists in the sentence, along with its associated semantic shift. We can instruct model M(•) to predict the probability of the existence of 'but' as being zero under these circumstances.</p><p>Our desideratum is that the output of the f (•) and M(•) are the same under identical interchange interventions, so as to align the two models. We denote the ϕ(•) and M(•) after interchange </p><formula xml:id="formula_8">L d = N i=1 K γ=1 D KL Mγ (G i ), MLP • POOL • φγ • f (m) (G i ) ,<label>(5)</label></formula><p>where K denotes the total number of interchange interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Procedure</head><p>The overall framework is illustrated in Figure <ref type="figure">2</ref>. For learning diminutive causal structure, we define our training objective with both L c and L d :</p><formula xml:id="formula_9">L a = L c + λL d ,<label>(6)</label></formula><p>L a denotes the total loss for diminutive causal structure learning, λ is a hyperparameter that balances the influence of learning under interchange intervention. Besides diminutive causal structure, our GNN model is also trained with conventional labeled data. The training object of which can be formulated as:</p><formula xml:id="formula_10">L g = N i=1 H f (G i ), Y i ,<label>(7)</label></formula><p>where H(•) calculates the cross entropy loss, and f H (•) denotes the projection head for label prediction. We update the model alternatively with L g and L a . Then f (•) will be utilized for the performance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Causal Modeling with Structural Causal Model</head><p>To embark on our investigation, it is imperative to establish a sound modeling of our problem scenario. We employ Structural Causal Models (SCM) to fulfill this task. SCM <ref type="bibr" target="#b23">[23]</ref> is a framework utilized to describe the manner in which nature assigns values to variables of interest. Formally, an SCM consists of variables and the relationships between them. Each SCM can be represented as a graphical model consisting of a set of nodes representing the variables, and a set of edges between the nodes representing the relationships.</p><p>We formalize the candidate problem by using an SCM illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. Among the SCM, S , as defined before, refers to those factors that influence the structure of graph data, and their causal relationships with graph data can be explicitly delineated. S * denotes the rest factors. S and S * together decide the formulation of X, which represents the data within the graph G that holds causal relationship with the ground truth labels. C represents any confounding factors present within the data, G represents the graph data itself, and R represents the output representation of f (•). T and T are the outputs of the high-level causal model and the GNN. The links in Figure <ref type="figure" target="#fig_3">4</ref> are as follows:</p><p>• S * → X, S → X. The form of X is desicde with S * and S .</p><p>• X → G ← C. The graph data G consists of two parts: X and confounder C.</p><p>• G → R. f (•) encodes G into representation R that consists of the node representations of different layers and the graph representation.</p><p>• S * ← -→ S . The bidirectional arrow with a dashed line indicates that the causal relationship between the two cannot be confirmed.</p><p>• G → T , R → T. T and T are variables that calculated based on G and R.</p><p>Furthermore, we propose the following proposition:</p><p>Proposition 2. The SCM illustrated in Figure <ref type="figure" target="#fig_3">4</ref> can represent scenarios encountered when applying our model to general graph learning tasks.</p><p>Proof. To establish the validity of the proposed SCM in Figure <ref type="figure" target="#fig_3">4</ref>, we employ the IC algorithm <ref type="bibr" target="#b23">[23]</ref> to construct the SCM from scratch and provide the detailed construction process. IC algorithm is a method for identifying causal relationships from the observed data. Please refer to Chapter 2 of <ref type="bibr" target="#b23">[23]</ref> for the details of the IC algorithm. The input of the IC algorithm is a set of variables and their distributions. The output is a pattern that represents the underlying causal relationships, which can be a structural causal model. The IC algorithm can be divided into three steps:</p><p>Step 1. For each pair of variables a and b in V, the IC algorithm searches for a set S ab s.t. the conditional independence relationship (a ⊥ b|S ab ) holds. In other words, a and b should be independent given S ab . The algorithm constructs an undirected graph G with vertices corresponding to variables in V. A pair of vertices a and b are connected with an undirected edge in G if and only if no set S ab can be found that satisfies the conditional independence relationship (a ⊥ b|S ab ).</p><p>Step 2. For each pair of non-adjacent variables a and b that share a common neighbor c, check the existence of c ∈ S ab . If such a c exists, proceed to the next pair; if not, add directed edges from a to c and from b to c.</p><p>Step 3. In the partially directed graph results, orient as many of the undirected edges as possible subject to the following two conditions: (i) any alternative orientation of an undirected edge would result in a new y-structure, and (ii) any alternative orientation of an undirected edge would result in a directed cycle.</p><p>Accordingly, we employ IC algorithm to provide a step-bystep procedure for constructing our structural causal model.</p><p>As for step 1, we first represent all variables as nodes in Figure <ref type="figure" target="#fig_4">5(a)</ref>. For each node, we traverse all other nodes to determine whether to establish a connection. For general graph learning scenarios, R represents the model's representation of G. Since any other variable can only affect R through G, R is conditionally independent of all other variables given G. Therefore, R is only connected to G. G represents the graph data, which is composed of X and C. There is no other variable that can block the relationship between G and X, or G and C. Therefore, G is connected to both X and C. As X blocks the path from S * and S to G, the corresponding connections do not exist. We define C as a confounder in the graph data, which has no causal relationship with other variables. Therefore, an empty set can make C independent of other variables, except for G.</p><p>As S and S * are the factors that decide the form of X, each of them is correlated with X. Their connections cannot be blocked by any set of variables, and thus we connect all these nodes with X. The relationship between S * and S can not be figured out, therefore we adopt a dashed line to link them. The result is demonstrated in Figure <ref type="figure" target="#fig_4">5(b)</ref>.</p><p>We then move on to step 2. Similar to step 1, we process with the traversal analysis starting from R. For R, since G ∈ S RG and G ∈ S RX , no edges related to R can be directed. Then, as S XC = ∅, X S XC , we direct edge X to G and C to G. As S * and S is the cause of X by definition, therefore we direct edge S * to X and S to X.</p><p>In Step 3, we adopted the rule 1 from <ref type="bibr" target="#b62">[62]</ref> for systematizing this step, which states that if a → b and a and c are not adjacent, then b → c should be set as the orientation for bc. Based on this, we established the following orientations: G → T , G → R. Then, according to the same rule, R → T should also be set. As for the remaining edge ( S , S * ), since we cannot determine its direction, we represent this edge with a bidirectional dashed line, indicating its directionality is uncertain. The final result is illustrated in Figure <ref type="figure" target="#fig_4">5(d)</ref>, which is identical to the SCM in Figure <ref type="figure" target="#fig_3">4</ref>. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Causal Model Based Analysis</head><p>Based on SCM in Figure <ref type="figure" target="#fig_3">4</ref>, we provide theoretical proof to support the intuition that incorporating diminutive causal structure is beneficial for enhancing a model's performance. To achieve such a goal, We propose the following theorem.</p><p>Theorem 3. For a graph representation learning process with a causal structure represented by the SCM in Figure <ref type="figure" target="#fig_3">4</ref>, increasing the mutual information I(R; S ) between R and S can decrease the upper bound of the mutual information I(R; C) between R and C. Formally: Proof of Theorem 3.</p><formula xml:id="formula_11">I(R; C) ≤ 1 -I(R; S ).<label>(8)</label></formula><p>To prove the theorem, we follow <ref type="bibr" target="#b63">[63]</ref> and suppose the proposed SCM possesses Markov property. Therefore, according to the SCM in Figure <ref type="figure" target="#fig_3">4</ref>, C and R are conditionally independent given G, as G blocks any path between C and R. Therefore, we could apply the Data Processing Inequality <ref type="bibr" target="#b64">[64]</ref> on the path C → G → R. Formally, we have:</p><formula xml:id="formula_12">I(G; C) ≥ I(R; C).<label>(9)</label></formula><p>According to the Chain Rule for Information <ref type="bibr" target="#b64">[64]</ref>, we also have:</p><formula xml:id="formula_13">I(X, C; G) = I(X; G|C) + I(C; G).<label>(10)</label></formula><p>As X and C are independent, we have:</p><formula xml:id="formula_14">I(X, C; G) = I(X; G) + I(C; G).<label>(11)</label></formula><p>Therefore:</p><formula xml:id="formula_15">I(C; G) = I(X, C; G) -I(X; G). (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>As G is the graph data that consist of X and C, we have:</p><formula xml:id="formula_17">I(X, C; G) = 1.<label>(13)</label></formula><p>Then:</p><formula xml:id="formula_18">I(C; G) = 1 -I(X; G).<label>(14)</label></formula><p>Therefore:</p><formula xml:id="formula_19">1 -I(X; G) ≥ I(R; C).<label>(15)</label></formula><p>According to the SCM in Figure <ref type="figure" target="#fig_3">4</ref>, S and R are conditionally independent given G, we have:</p><formula xml:id="formula_20">I( S ; G) ≥ I( S ; R).<label>(16)</label></formula><p>Also:</p><formula xml:id="formula_21">I( S , X; G) = I( S ; G) + I(X; G| S )<label>(17)</label></formula><p>and</p><formula xml:id="formula_22">I( S , X; G) = I(X; G) + I( S ; G|X)<label>(18)</label></formula><p>holds. S and G is independent given X, therefore:</p><formula xml:id="formula_23">I( S ; G|X) = 0. (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>Thus:</p><formula xml:id="formula_25">I( S , X; G) = I( S ; G) + I(X; G| S ) = I(X; G).<label>(20)</label></formula><p>As I(X; G| S ) ≥ 0:</p><formula xml:id="formula_26">I( S ; G) ≤ I(X; G).<label>(21)</label></formula><p>Based on Inequality 16 and 21, we have:</p><formula xml:id="formula_27">I( S ; R) ≤ I(X; G).<label>(22)</label></formula><p>Substituting Inequality 22 into Inequality 15, we can obtain:</p><formula xml:id="formula_28">I(R; C) ≤ 1 -I( S ; R). (<label>23</label></formula><formula xml:id="formula_29">)</formula><p>Theorem 3 is proved. Theorem 3 establishes a direct relationship between a model's alignment with diminutive causal structure and the reduction of confounding influence. As the model gains an increased alignment, the confounding influence diminishes. Next, we prove the validity of the training objective proposed in Equation <ref type="formula" target="#formula_5">2</ref>.</p><p>Theorem 4. For a certain graph learning process represented by the SCM in Figure <ref type="figure" target="#fig_3">4</ref>, T ∼ p(t), T ∼ q(t), t ∈ T , if the highlevel causal models r(•) is effective enough such that I( T , S ) = I(G, S ), then I(R, S ) is maximized if p(t| s) = q(t| s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 4.</head><p>We begin with calculating the boundaries of I(R, S ). According to the SCM in Figure <ref type="figure" target="#fig_3">4</ref>, S and G are conditionally independent given X, as X block any path between S and G. Therefore, we could apply the Data Processing Inequality <ref type="bibr" target="#b64">[64]</ref> on the path S → X → G. Formally, we have:</p><formula xml:id="formula_30">I( S ; X) ≥ I( S ; G).<label>(24)</label></formula><p>Likewise, we have:</p><formula xml:id="formula_31">I( S ; G) ≥ I( S ; R),<label>(25)</label></formula><p>and:</p><formula xml:id="formula_32">I( S ; R) ≥ I( S ; T ). (<label>26</label></formula><formula xml:id="formula_33">)</formula><p>According to the assumptions:</p><formula xml:id="formula_34">I( T ; S ) = I(G; S ),<label>(27)</label></formula><p>we have:</p><formula xml:id="formula_35">I( S ; T ) ≥ I( S ; R) ≥ I( S ; T ). (<label>28</label></formula><formula xml:id="formula_36">)</formula><p>So far, we can acquire the upper and lower bounds of I( S ; R). Moreover, since the data set and M are determined in advance in the learning task, the only thing that can be changed for this system is the parameters of the neural network model. Therefore, the upper bound I( S ; T ) holds a fixed value. If we can make the lower bound I( S ; T ) equal to the upper bound I( S ; T ), then we have I( S ; R) reached the maximum. Next, we will proof if p(t| s) = q(t| s), then I( S ; T ) = I( S ; T ). When p(t| s) = q(t| s), then:</p><formula xml:id="formula_37">H(T | S ) = - s∈ S p S ( s) t∈T p(t| s)log p(t| s) = - s∈ S p S ( s) t∈T q(t| s)log q(t| s) = H( T | S ),<label>(29)</label></formula><p>where S ∼ p S ( s), s ∈ S. We also have: = -t∈T s∈ S p S ( s)q(t| s)log s∈ S p S ( s)q(t| s)</p><formula xml:id="formula_38">H(T ) = -</formula><formula xml:id="formula_39">= - t∈T q(t)log q(t) = H( T )<label>(30)</label></formula><p>According to the definition of mutual information <ref type="bibr" target="#b64">[64]</ref>, we have:</p><formula xml:id="formula_40">I( S ; T ) = H( T ) -H( T | S ), I(T ; S ) = H(T ) -H(T | S ). (<label>31</label></formula><formula xml:id="formula_41">)</formula><p>Based on equation 29, 30 and 31, we have:</p><formula xml:id="formula_42">I( S ; T ) = I( S ; T ). (<label>32</label></formula><formula xml:id="formula_43">)</formula><p>Based on the discussions above, I( S ; R) reached the maximum. Theorem 4 is proved.</p><p>According to Theorem 4, if Equation 2 holds, I(R, S ) will be maximized, which indicates that the model has learned the maximum amount of knowledge about S . With Theorem 4, we present a corollary to substantiate the validity of L c . Corollary 5. For a specific graph dataset G, if the training samples in G sufficiently cover all embodiments corresponding to S , and the L c which is defined by Equation 3 reaches zero, then the maximized value of I(R, S ) stated in Theorem 4 can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Corollary 5.</head><p>As in Equation 2, for each graph G i , S holds a fixed value s i . Therefore, we have:</p><formula xml:id="formula_44">p i (t) = p i (t| s i ) = p i (t| s),<label>(33)</label></formula><p>and</p><formula xml:id="formula_45">qi (t) = qi (t| s i ) = qi (t| s).<label>(34)</label></formula><p>Therefore, we have:</p><formula xml:id="formula_46">L c = N i KL p i (t| s)|| qi (t| s) = N i KL p i (t)|| qi (t)<label>(35)</label></formula><p>As r P (r E (G i )) can predict p i (t). f P ( f E (G i )) output a estimation qi (t) of q i (t). Therefore:</p><formula xml:id="formula_47">L c = N i KL p i (t)|| qi (t) = N i=1 D KL r P (r E (G i )), f P ( f E (G i ))<label>(36)</label></formula><p>Then, if:</p><formula xml:id="formula_48">L c = N i=1 D KL r P (r E (G i )), f P ( f E (G i )) = 0,<label>(37)</label></formula><p>we have:</p><formula xml:id="formula_49">N i KL p i (t)|| qi (t) = N i KL p i (t| s)|| qi (t| s) = 0. (<label>38</label></formula><formula xml:id="formula_50">)</formula><p>Further more, the training samples in G sufficiently cover all embodiments corresponding to S , we have:</p><formula xml:id="formula_51">KL p(t| s)||q(t| s) = 0. (<label>39</label></formula><formula xml:id="formula_52">)</formula><p>Then p(t| s) equals q(t| s), according to Theorem 4, I(R; S ) reaches maximization. Corollary 5 is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct a comprehensive analysis of the performance of our approach through a series of experiments. We extract distinct diminutive causal structures in two prevalent domains of graph representation learning: graph topology analysis and text graph analysis. Validation of our method is carried out across multiple datasets within each domain. Additionally, we perform a series of further analyses to delve deeper into the intrinsic mechanisms of DCSGL. For the study of graph representation learning, the topological structure of the graph is particularly crucial, encompassing a wealth of essential information <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b71">71]</ref>. In this experiment, we undertake diminutive causal structure discovery based on knowledge pertaining to graph topological structures and construct M(•) and M γ (•). Subsequently, a series of experiments were conducted on artificially synthesized datasets for validation. In the graph, the white nodes and blue nodes respectively represent two distinct motifs, while the junction of these two motifs is indicated in orange.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Dataset</head><p>The datasets employed in this experiment include the following: 1) Spurious-Motif, a synthetic Out-Of-Distribution (OOD) dataset <ref type="bibr" target="#b13">[13]</ref> generated using the methodology from <ref type="bibr" target="#b72">[72]</ref>; 2) Motif-Variant, another synthetic OOD dataset created by ourselves, employing the data generation method outlined in <ref type="bibr" target="#b72">[72]</ref>, but featuring distinct topological structures compared to the previous dataset; 3) The In-Distribution (ID) counterparts of these datasets. 4) Spurious-Motif-N and Motif-Variant-N, The node classification versions for Spurious-Motif and Motif-Variant. Nodes within different motifs are assigned distinct labels to evaluate the effectiveness of node classification. Detailed information about the datasets is provided in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Model M(•) Construction</head><p>For the diminutive causal structure, our focus lies in the shape of junctions between distinct motifs. Graphs rich in topological information are often composed of numerous motifs with specific shapes. The junctions between these motifs typically exhibit shapes distinct from the motifs themselves. Identifying these junctions can contribute to a clearer analysis of the topological structure of the graph. However, recognizing junctions alone is insufficient for providing direct assistance in predictions. Thus, we consider the principles governing the composition of junctions as a diminutive causal structure.</p><p>To provide a clearer explanation, practical examples of motif junctions are presented in Figure <ref type="figure" target="#fig_6">6</ref>. Subsequently, we proceed with the construction of M(•). We employ a graph search algorithm to identify the junctions between the motifs. These discovered junctions are then labeled. Since the data in the training dataset remains constant throughout the training process, labeling needs to be performed only once, and subsequent repetitions of this process are unnecessary.</p><p>We built M(•) to judge whether a specific node is a junction. Clearly, labeled nodes are junctions, while other nodes are not. Therefore, our ϕ(•) is designed to output the features of nodes labeled as junctions. Thus, when the input corresponds to a labeled node, M(•) outputs a probability of 1.</p><p>For M γ (•) and ϕ γ (•), we randomly replace 50% of the features of the nodes output by ϕ(•) with the features of other nodes. In this scenario, the corresponding output probability of M γ (•) is 0.5. We repeat the random replacement for different ϕ γ (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Settings</head><p>We compare our method with the backbone network named Local Extremum GNN and various causality-enhanced methods that can be divided into two groups: 1) the interpretable baselines, including GAT, Top-k Pool and GSN, 2) the robust learning baselines, including Group DRO, IRM, V-REx and DIR. Furthermore, hierarchical pooling methods such as MT-Pool <ref type="bibr" target="#b75">[75]</ref>, may also positively impact causal information extraction. However, we illustrate this with the example of Top-k Pool and do not delve into a detailed one-to-one comparison. These approaches differ significantly from our method in that they place greater emphasis on internal modifications within the model, aiming to achieve causal modeling capabilities through architectural innovations. In contrast, our method prioritizes the introduction of external causal information. For a fair comparison, we follow the experimental principles of <ref type="bibr" target="#b13">[13]</ref> and adopt the same training setting for all models. For each task, we report the mean performance ± standard deviation over five runs.</p><p>Moreover, to substantiate that the diminutive causal structure we have adopted does not inherently contribute directly to predictions, we have introduced additional models named DCS-Only-D and DCS-Only-L. In DCS-Only-D, junction marks (assigned as 1 for junction nodes and 0 for other nodes) are treated as extra node classification task labels. Meanwhile, DCS-Only-L incorporates the marking of junctions as extra graph classification training labels, providing information on the number of junctions within the graph.</p><p>To further justify the structure of our proposed method, we have introduced the DCSGL-T model. The DCSGL-T model accepts input from the last layer instead of the m-th layer. Additionally, we developed the DCSGL-A model to conduct further ablation studies, wherein the enhancement with interchange interventions is deliberately replaced with negative samples from random select node features. For hyperparameters, we specify m = 2, λ = 1, and K = 3. The learning rate is set to 0.001. Our model undergoes training for 200 epochs, and early stopping is implemented if there is no observed improvement in performance on the validation set for five consecutive epochs. The model exhibiting the best validation performance is selected as the final result. The maximum allowable number of training epochs is established at 400 for all datasets. A training batch size of 32 is employed. The backbone of our GNN architecture is the Local Extremum GNN <ref type="bibr" target="#b21">[21]</ref>, featuring 4 layers and a hidden dimension of 32. Global mean pooling is adopted as the pooling method.</p><p>All our experiments are performed on a workstation equipped with two Quadro RTX 5000 GPUs (16 GB), an Intel Xeon E5-1650 CPU, 128GB RAM and Ubuntu 20.04 operating system. We employ the Adam optimizer for optimization. We set the maximum training epochs to 400 for all tasks. For backpropagation, we optimize Graph-SST2 and Graph-Twitter using stochastic gradient descent (SGD), while utilizing gradient descent (GD) on Spurious-Motif and Motif-Variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Results</head><p>The results are reported in Table <ref type="table" target="#tab_2">3</ref> and<ref type="table">4</ref>. From the results, we observe that our proposed DCSGL method consistently outperforms various baseline models across multiple datasets, demonstrating the efficacy of our approach. Additionally, it is noteworthy that the performance of DCS-Only-D and DCS-Only-L, two baseline models incorporating the diminutive causal structure, does not exhibit significant improvement compared to the fundamental backbone GNN. This suggests that the adopted diminutive causal structure is not directly task-relevant. Furthermore, the performance of DCSGL significantly surpasses the average performance of DCS-Only-D and DCS-Only-L, highlighting the utility of our introduced diminutive causal structure. This empirical evidence substantiates the effectiveness of our approach. Additionally, the ablative models, DCSGL-T and DCSGL-A, exhibit weaker perfor-mance compared to DCSGL, emphasizing the necessity of the proposed modules. This underscores the importance and effectiveness of our introduced methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments with Diminutive Causal Structure from Linguist Domain</head><p>In this section, we anchor our validation efforts on a selection of real-world datasets. The semantic graphs derived from textual information are chosen for their origin in textual knowledge, facilitating a more straightforward analysis of their internal logical structures. Additionally, datasets within the field of textual graph analysis are widely employed for the validation of graph representation learning methods <ref type="bibr" target="#b8">[8]</ref>. Consequently, we employ these datasets, along with diminutive causal structures extracted from the field of linguistics, for the experiments conducted in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Dataset</head><p>The datasets employed in this experiment include multiple real-world datasets, including Graph-SST2, Graph-SST5, and Graph-Twitter <ref type="bibr" target="#b59">[59]</ref>, along with their OOD versions. The OOD versions were generated following <ref type="bibr" target="#b13">[13]</ref>. The details of the datasets are demonstrated within Table <ref type="table" target="#tab_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Model M(•) Construction</head><p>In this section, we construct diminutive causal structures based on relevant knowledge about the conjunction 'but.' A brief description of the corresponding model construction for these causal structures has been provided in the preceding text. Essentially, we employ the semantic contrastive relationship represented by the conjunction 'but' to facilitate training. An illustrative example of the role of the conjunction 'but' in the textual graph is presented in Figure <ref type="figure">3</ref>.</p><p>For the model M(•), we generate predictions regarding the existence of the conjunction 'but' and its associated contrastive  To address this issue, we employ the interchange intervention method mentioned in Section 3.4. Specifically, we apply ϕ γ (•) to delete the conjunction 'but' and its subsequent content, thereby removing the contrastive relationship. Simultaneously, we have the corresponding model M γ (•) output a judgment result of 0, indicating the absence of the conjunction 'but.' Through this approach, we supplement the negative samples. We delete different amounts of contents for different ϕ γ (•), and adjust the output of M γ (•) according to the percentage of content removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Settings</head><p>We maintain consistency with the baselines and most settings from the previous experiment, with the exception of the backbone GNN and hyperparameters. Specifically, we adopt ARMA <ref type="bibr" target="#b22">[22]</ref> as the GNN backbone, with three layers and a hidden dimension of 128. We set m = 2, K = 3, λ = 1.5, and the learning rate to 0.0002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Results</head><p>The results are demonstrated in Table <ref type="table" target="#tab_5">7</ref>. From the results, it is evident that DCSGL achieved optimal performance across all datasets. Additionally, when compared to the average values of DCS-Only-D and DCS-Only-L, DCSGL demonstrated a significant improvement in performance. These experimental observations substantiate the efficacy of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">In-Depth Study</head><p>To gain a more profound understanding of the characteristics of our approach, we conducted a series of analytical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Generalizability Analysis</head><p>To analyze the generalization capability of our method, we conducted additional experiments on the Motif-Variant dataset   by replacing the backbone network. Compared to the original backbone network, we introduced two new backbones: FAGCN and EGC. We separately tested the performance of the models using only the backbone network and the models incorporating the DCSGL method.</p><p>The experimental results are presented in Table <ref type="table" target="#tab_7">9</ref>. It can be observed that DCSGL achieved certain improvements on the two newly introduced baselines. On datasets with both confounding factors and those without confounding factors, DC-SGL consistently enhanced the algorithm's performance. These experimental results strongly validate the generalization capability of our proposed method. We attribute this generalization to the structural design of DCSGL, which does not rely on specific network architectures but can inject subtle causal structures into different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Computational Complexity Analysis</head><p>For the purpose of computational complexity analysis, we will undertake additional investigation and experimentation. The computational complexity of DCSGL is intricately linked to the volume of graph data involved in the computation and the number of GNN layers engaged. During the computation of logic learning loss, a subset of graph data was selected, and a portion of network layers was extracted to perform forward propagation on the GNN. Therefore the computational complexity associated with this part is similar to that of graph neural networks, scaling linearly with the number of edges in the selected graph data, i.e., O(|E| × F), where E denotes the selected edge set, F denotes the computational complexity induced by dimensionality. Based on Equation <ref type="formula">4</ref>, the subsequent addition of KL divergence calculation, pooling and MLP operations maintains the computational complexity at O(|E| × F). Consequently, the additional computational complexity of DC-SGL is less than the computational complexity of the backbone GNN, and proportional to |E|. We have conducted further analysis through experimental validation.</p><p>We separately recorded the computation time and GPU memory consumption for a single epoch of the standalone backbone network, DIR, and our proposed method on the Spurious-Motif dataset and Graph SST2 dataset, as depicted in Table <ref type="table" target="#tab_6">8</ref>. It is observed that, although our method incurs an additional computational time compared to the standalone backbone network, this overhead is less than that of DIR. Furthermore, our method achieves superior performance compared to DIR, indicating that such computational costs are deemed acceptable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Training Procedure Evaluation</head><p>To gain a comprehensive understanding of the dynamic evolution of our model during the training process, we conducted meticulous monitoring, encompassing both accuracy and L a loss, across multiple Motif-Variant datasets characterized by varying bias coefficients. Additionally, we performed a similar L a loss calculation for the Local Extremum GNN, acting as the backbone GNN, akin to the DCSGL method, albeit without utilizing backpropagation for model updates.</p><p>The results presented in Figure <ref type="figure" target="#fig_9">7</ref> distinctly illustrate a rapid and substantial reduction in the L a loss for DCSGL over the entire course of 200 epochs, maintaining a consistently low level. Simultaneously, the model's performance continues to improve even after multiple epochs. These findings unequivocally indicate that DCSGL not only possesses the capability to effectively diminish causal structure loss during the learning process but also sustains a continuous enhancement in its performance over multiple epochs.</p><p>Notably, we observed that datasets with different bias coefficients have similar values of L a loss, suggesting that the encoded diminutive causal structure by DCSGL is not intricately intertwined with the complexity of downstream tasks. Consequently, DCSGL demonstrates robust learning efficiency in the face of dataset variations and varying task contexts.</p><p>We observe distinct behavior in the baseline model, Local Extremum GNN. Despite a gradual decrease in the L a loss during training, it does not exhibit as pronounced a reduction as the DCSGL method. This phenomenon aligns with the experimental findings presented in Figure <ref type="figure">1</ref>(a), indicating that, throughout training, a general GNN model can to some extent acquire knowledge about the diminutive causal structure. However, the model falls short of fully capturing such a causal structure. Ad-ditionally, the knowledge of the diminutive causal structure acquired by Local Extremum GNN may be prone to forgetting as evidenced by the L a loss resurging in later stages, as illustrated in the graph.</p><p>Furthermore, we could also see a decline in the performance of Local Extremum GNN on certain datasets during the later stages of training. Consequently, we can conclude that, due to the absence of explicit support for a well-defined diminutive causal structure akin to DCSGL, Local Extremum GNN struggles to sustain a more continuous and stable learning process. This limitation becomes particularly evident in datasets containing confounding factors.</p><p>Our observations lead us to conclude that DCSGL, through the effective integration of diminutive causal structures, enables the model to consistently maintain a correct causal framework at specific locations. This stability aids the model in preserving a consistent understanding of the desired causal relationships throughout the learning process, thereby facilitating continuous and effective learning. The adaptive learning strategy allows DCSGL to better accommodate dynamic changes in tasks and diverse biases in datasets, showcasing its substantial potential in complex and diverse real-world applications. In practical scenarios, leveraging the advantages of incorporating diminutive causal structures lies in the ability to make optimal use of readily available domain knowledge, ensuring outstanding performance across various tasks within the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Visualized Evaluation</head><p>Figure <ref type="figure" target="#fig_11">8</ref> illustrates the visualized output features of our model, presenting two modes of data visualization. Specifically, Figure <ref type="figure" target="#fig_11">8</ref>    It is evident from the figure that DCSGL can indeed learn more refined and accurate features. This outstanding performance is attributed to the injection of minute causal relationships into our model, enabling the model to continuously optimize its feature representation based on accurately determined knowledge, thereby ensuring the model's accuracy. The introduction of these subtle causal relationships enhances the model's flexibility in adapting to and understanding complex data structures, providing a more reliable foundation for its application in various task scenarios. In conclusion, these results thoroughly validate the superiority of our model and its robust capability in learning and representing causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5.">Hyperparamter Analysis</head><p>In Figure <ref type="figure" target="#fig_12">9</ref>, the experimental results regarding m and K further confirm that extracting representations in the intermediate layers for causal knowledge learning, along with the simultaneous application of multiple exchange interventions, significantly enhances the model's performance in causal relationship learning. It underscores the effectiveness of our approach in strengthening the model's understanding of causal relationships. It is noteworthy that by adjusting the λ parameter, we can effectively balance the relationship between causal learning and dataset training, providing better control and adjustment for the overall performance of the model. These experimental results not only emphasize the necessity of the proposed structure but also deepen our understanding of the model's performance. Specifically, our structural design, considering the joint utilization of intermediate layer representations and multiple exchange interventions, plays a crucial role in causal knowledge learning. Through sensitivity analysis of the λ parameter, we further confirm its importance in balancing causal learning and dataset training.</p><p>Furthermore, we observe a relatively consistent performance variation of the model across different datasets within the same domain. This suggests that despite dataset differences, the consistent introduction of diminutive causal structures results in a uniform trend in model performance. This further highlights the generality and robustness of our proposed method, enabling it to consistently outperform on diverse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Drawing on empirical observations from motivating experiments, it becomes clear that integrating diminutive causal structures into graph representation learning significantly enhances model performance. In response, we present DCSGL, a methodology explicitly crafted to guide the model in learning these specific causal structures. DCSGL accomplishes this objective by leveraging insights gained from models constructed based on diminutive causal structures. The learning process is further refined through interactive interventions. The validity and effectiveness of our approach are substantiated through rigorous theoretical analyses. Additionally, empirical comparisons underscore the substantial performance enhancements achieved by DCSGL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Visualization of a conventional GNN approach's learning degree of the diminutive causal structures during optimization on benchmark datasets. We specifically measure the cosine similarity between the output representations of the causal model and the GNN model. Comparisons between the backbone GNNs (Local Extremum GNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The overall framework of DCSGL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the reasoning process using IC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>s)p(t| s)log s∈ S p S ( s)p(t| s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Example of motif junction. In the graph, the white nodes and blue nodes respectively represent two distinct motifs, while the junction of these two motifs is indicated in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of graph classification accuracy in Motif-Variant. Motif-Variant (ID) denotes the ID version dataset. Bias represents the degree of distribution change between the training set and the test set. The best records are highlighted in bold, and underline denotes the second-best result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Results on Motif-Varient Bias = 0.9. Results on Motif-Varient Bias = 0.6. Results on Motif-Varient Balanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Accuracy and loss obtained over 200 epochs on multiple Motif-Variant datasets with distinct biases.</figDesc><graphic coords="14,44.14,210.06,160.69,97.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) and Figure 8(b) provide visualization of the output features. In each block of these two figures, each vertical t-SNE visualization results of Local Extremum GNN. t-SNE visualization results of DCSGL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of model outputs.</figDesc><graphic coords="15,315.38,306.43,76.02,63.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Hyperparameter Experiments.</figDesc><graphic coords="15,313.94,417.43,72.40,63.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 (</head><label>8</label><figDesc>Figure8(c) and 8(d) demonstrates the feature representations of all samples after dimensionality reduction using the t-SNE<ref type="bibr" target="#b76">[76]</ref> algorithm. Different colors in the figure represent different categories. It is evident from the figure that DCSGL can indeed learn more refined and accurate features. This outstanding performance is attributed to the injection of minute causal relationships into our model, enabling the model to continuously optimize its feature representation based on accurately determined knowledge, thereby ensuring the model's accuracy. The introduction of these subtle causal relationships enhances the model's flexibility in adapting to and understanding complex data structures, providing a more reliable foundation for its application in various task scenarios. In conclusion, these results thoroughly validate the superiority of our model and its robust capability in learning and representing causal relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Glossary of notations.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>G</cell><cell>Graph sample.</cell></row><row><cell>X</cell><cell>The causal part within the graph sample.</cell></row><row><cell>Y</cell><cell>Label</cell></row><row><cell>C</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of datasets used for experiment with diminutive causal structure from the topological domain.</figDesc><table><row><cell>Name</cell><cell>Graphs</cell><cell>Average Nodes</cell><cell>Classes</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell>Spurious-Motif</cell><cell>18,000</cell><cell>46.6</cell><cell>3</cell><cell>Classification</cell><cell>ACC</cell></row><row><cell>Motif-Variant</cell><cell>18,000</cell><cell>48.9</cell><cell>3</cell><cell>Classification</cell><cell>ACC</cell></row><row><cell cols="6">5.1. Experiments with Diminutive Causal Structure from Topo-</cell></row><row><cell cols="2">logical Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of graph classification accuracy in Spurious-Motif. Spurious-Motif (ID) denotes the ID version dataset. Bias represents the degree of distribution shift between the training set and the test set. Some of the results are cited from<ref type="bibr" target="#b13">[13]</ref>. The best records are highlighted in bold, and underline denotes the second-best result.</figDesc><table><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance of classification accuracy in node classification tasks. Some of the baselines are removed as they are specifically designed to address graph classification tasks. Mixed-N denotes the graph data mixed with motif from Spurious-Motif and Motif-Variant. The best records are highlighted in bold, and underline denotes the second-best result.</figDesc><table><row><cell>Method</cell><cell cols="2">Spurious-Motif-N Motif-Variant-N</cell><cell>Mixed-N</cell></row><row><cell>Local Extremum GNN [21]</cell><cell>76.24±1.70</cell><cell>75.13±1.98</cell><cell>74.58±1.63</cell></row><row><cell>GAT [36]</cell><cell>77.31±1.63</cell><cell>73.62±1.53</cell><cell>73.78±1.32</cell></row><row><cell>Group DRO [67]</cell><cell>75.01±1.89</cell><cell>73.10±1.55</cell><cell>74.03±1.39</cell></row><row><cell>IRM [68]</cell><cell>74.86±1.91</cell><cell>75.40±1.89</cell><cell>72.30±1.82</cell></row><row><cell>V-REx [26]</cell><cell>76.67±1.46</cell><cell>75.58±1.99</cell><cell>74.63±1.47</cell></row><row><cell>DIR [13]</cell><cell>76.50±1.68</cell><cell>75.66±2.01</cell><cell>76.30±1.53</cell></row><row><cell>RCGRL [14]</cell><cell>75.95±1.50</cell><cell>76.85±1.43</cell><cell>74.90±1.65</cell></row><row><cell>DCS-Only-D</cell><cell>76.12±1.69</cell><cell>75.65±1.71</cell><cell>75.68±1.70</cell></row><row><cell>DCS-Only-L</cell><cell>76.95±1.87</cell><cell>75.38±1.65</cell><cell>74.80±1.79</cell></row><row><cell>Average</cell><cell>76.54</cell><cell>75.52</cell><cell>75.24</cell></row><row><cell>DCSGL-T</cell><cell>78.37±1.86</cell><cell>78.91±1.49</cell><cell>77.31±1.09</cell></row><row><cell>DCSGL-A</cell><cell>78.01±1.50</cell><cell>77.40±1.66</cell><cell>77.38±1.35</cell></row><row><cell>DCSGL</cell><cell>80.16±1.73</cell><cell>80.01±1.70</cell><cell>78.08±1.54</cell></row><row><cell cols="4">structure. Clearly, we can directly assess the presence of this</cell></row><row><cell cols="4">conjunction based on the corresponding textual data in the</cell></row><row><cell cols="4">graph. If it is present, the output probability is set to 1. Con-</cell></row><row><cell cols="4">cerning ϕ(•), we use the statement containing 'but' as input. To</cell></row><row><cell cols="4">avoid introducing additional interference, such as the presence</cell></row><row><cell cols="4">of other types of contrastive conjunctions, we refrain from us-</cell></row><row><cell cols="4">ing statements without 'but' during training. This results in a</cell></row><row><cell cols="2">problem of missing negative samples.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Summary of datasets used for experiment with diminutive causal structure from the topological domain.</figDesc><table><row><cell>Name</cell><cell>Graphs</cell><cell>Average Nodes</cell><cell>Classes</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell>Graph-SST2</cell><cell>70,042</cell><cell>10.2</cell><cell>2</cell><cell>Classification</cell><cell>ACC</cell></row><row><cell>Graph-SST5</cell><cell>11,550</cell><cell>21.1</cell><cell>5</cell><cell>Classification</cell><cell>ACC</cell></row><row><cell>Graph-Twitter</cell><cell>6,940</cell><cell>19.8</cell><cell>3</cell><cell>Classification</cell><cell>ACC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Performance of graph classification accuracy in Graph-SST2, Graph-SST5, and Graph-Twitter. '(OOD)' denotes the OOD version datasets. The best records are highlighted in bold, and underline denotes the second-best result.</figDesc><table><row><cell>Method</cell><cell>Graph-SST2</cell><cell>Graph-SST2</cell><cell>Graph-SST5</cell><cell>Graph-SST5</cell><cell>Graph-Twitter</cell><cell>Graph-Twitter</cell></row><row><cell></cell><cell></cell><cell>(OOD)</cell><cell></cell><cell>(OOD)</cell><cell></cell><cell>(OOD)</cell></row><row><cell>ARMA[22]</cell><cell>89.19±0.87</cell><cell>81.44±0.59</cell><cell>48.13±0.98</cell><cell>37.26±0.89</cell><cell>63.19±1.13</cell><cell>62.56±1.65</cell></row><row><cell>GAT [36]</cell><cell>89.89±0.68</cell><cell>81.57±0.71</cell><cell>48.51±1.86</cell><cell>37.58±1.67</cell><cell>63.57±0.95</cell><cell>62.38±0.98</cell></row><row><cell>Top-k Pool [65]</cell><cell>89.31±1.26</cell><cell>79.78±1.35</cell><cell>49.72±1.42</cell><cell>36.26±1.86</cell><cell>63.41±1.95</cell><cell>62.95±1.09</cell></row><row><cell>GSN [66]</cell><cell>89.63±1.60</cell><cell>81.93±1.86</cell><cell>48.64±1.60</cell><cell>38.78±1.84</cell><cell>63.18±1.89</cell><cell>63.07±1.18</cell></row><row><cell>Group DRO [67]</cell><cell>89.94±1.60</cell><cell>81.29±1.44</cell><cell>47.44±1.12</cell><cell>37.78±1.12</cell><cell>62.23±1.43</cell><cell>61.90±1.03</cell></row><row><cell>IRM [68]</cell><cell>89.55±1.03</cell><cell>81.01±1.13</cell><cell>48.08±1.30</cell><cell>38.68±1.62</cell><cell>64.11±1.58</cell><cell>62.27±1.55</cell></row><row><cell>V-REx [26]</cell><cell>88.78±0.82</cell><cell>81.76±0.08</cell><cell>48.55±1.10</cell><cell>37.10±1.18</cell><cell>64.84±1.46</cell><cell>63.42±1.06</cell></row><row><cell>DIR [13]</cell><cell>89.91±0.86</cell><cell>81.93±1.26</cell><cell>49.16±1.31</cell><cell>38.67±1.38</cell><cell>65.14±1.37</cell><cell>63.49±1.36</cell></row><row><cell>DISC [14]</cell><cell>88.95±0.79</cell><cell>81.67±1.09</cell><cell>48.03±1.43</cell><cell>36.61±1.83</cell><cell>65.28±0.61</cell><cell>62.43±1.26</cell></row><row><cell>RCGRL [14]</cell><cell>90.73±0.40</cell><cell>82.31±1.01</cell><cell>48.56±1.53</cell><cell>37.98±1.63</cell><cell>66.58±0.53</cell><cell>62.93±1.35</cell></row><row><cell>DCS-Only-D</cell><cell>88.93±0.99</cell><cell>80.56±0.63</cell><cell>48.03±1.04</cell><cell>35.86±1.16</cell><cell>63.20±1.32</cell><cell>62.38±1.75</cell></row><row><cell>DCS-Only-L</cell><cell>89.25±1.15</cell><cell>81.03±0.85</cell><cell>48.30±1.07</cell><cell>36.32±0.77</cell><cell>63.30±1.09</cell><cell>61.80±1.85</cell></row><row><cell>Average</cell><cell>89.09</cell><cell>80.80</cell><cell>48.17</cell><cell>36.09</cell><cell>63.25</cell><cell>62.09</cell></row><row><cell>DCSGL-T</cell><cell>89.73±0.77</cell><cell>82.31±0.97</cell><cell>49.20±0.89</cell><cell>38.92±1.10</cell><cell>63.26±1.22</cell><cell>63.00±1.43</cell></row><row><cell>DCSGL-A</cell><cell>89.68±0.73</cell><cell>82.01±0.83</cell><cell>49.63±0.89</cell><cell>39.00±1.25</cell><cell>65.32±0.93</cell><cell>61.98±0.96</cell></row><row><cell>DCSGL</cell><cell>91.12±1.53</cell><cell>83.13±1.33</cell><cell>50.38±0.87</cell><cell>42.13±1.50</cell><cell>66.87±0.89</cell><cell>63.98±1.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Summary of computation cost</figDesc><table><row><cell></cell><cell>Method</cell><cell>Time</cell><cell>Additional</cell><cell>Memory</cell><cell>Additional</cell></row><row><cell></cell><cell></cell><cell>Cost (s)</cell><cell>Time (s)</cell><cell>Cost (Gb)</cell><cell>Memory (Gb)</cell></row><row><cell></cell><cell></cell><cell cols="3">Results on Spurious-Motif Dataset</cell><cell></cell></row><row><cell></cell><cell>Backbone</cell><cell>2.18</cell><cell>-</cell><cell>1.98</cell><cell>-</cell></row><row><cell>.</cell><cell>DIR DCSGL</cell><cell>4.31 4.01</cell><cell>2.13 1.83</cell><cell>3.61 2.56</cell><cell>1.63 0.58</cell></row><row><cell></cell><cell></cell><cell cols="3">Results on Graph-SST2 Dataset</cell><cell></cell></row><row><cell></cell><cell>Backbone</cell><cell>17.03</cell><cell>-</cell><cell>9.13</cell><cell>-</cell></row><row><cell></cell><cell>DIR</cell><cell>50.31</cell><cell>33.28</cell><cell>13.64</cell><cell>4.51</cell></row><row><cell></cell><cell>DCSGL</cell><cell>23.01</cell><cell>5.98</cell><cell>10.16</cell><cell>1.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Performance with different backbones. ∆ represents the improvement in accuracy after incorporating our method.</figDesc><table><row><cell>Method</cell><cell cols="2">Motif-Variant</cell><cell>Motif-Variant</cell></row><row><cell></cell><cell>Balanced</cell><cell>Bias=0.9</cell><cell>ID</cell></row><row><cell cols="3">DCSGL-Backbone 48.18±3.46 42.31±2.13</cell><cell>94.78±1.07</cell></row><row><cell>DCSGL</cell><cell cols="2">52.16±1.88 47.06±1.87</cell><cell>95.53±0.93</cell></row><row><cell>∆</cell><cell>3.98</cell><cell>4.75</cell><cell>0.75</cell></row><row><cell>FAGCN [73]</cell><cell cols="2">46.59±1.46 38.96±2.03</cell><cell>91.56±1.82</cell></row><row><cell>DCSGL-FAGCN</cell><cell cols="2">50.16±1.87 41.60±1.66</cell><cell>91.79±1.85</cell></row><row><cell>∆</cell><cell>3.57</cell><cell>2.64</cell><cell>0.23</cell></row><row><cell>EGC [74]</cell><cell cols="2">43.13±1.97 36.53±1.65</cell><cell>92.12±1.38</cell></row><row><cell>DCSGL-EGC</cell><cell cols="2">45.69±1.90 39.68±1.88</cell><cell>93.89±1.65</cell></row><row><cell>∆</cell><cell>2.56</cell><cell>3.15</cell><cell>1.77</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank the editors and reviewers for their valuable comments. This work is supported by the <rs type="programName">National Funding Program for Postdoctoral Researchers</rs>, Grant No. <rs type="grantNumber">GZC20232812</rs>, the <rs type="programName">Fundamental Research Program</rs>, Grant No. <rs type="grantNumber">JCKY2022130C020</rs>, the <rs type="programName">CAS Project for Young Scientists in Basic Research</rs>, Grant No. <rs type="grantNumber">YSBR-040</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rF6ydaW">
					<idno type="grant-number">GZC20232812</idno>
					<orgName type="program" subtype="full">National Funding Program for Postdoctoral Researchers</orgName>
				</org>
				<org type="funding" xml:id="_VjmM8bT">
					<idno type="grant-number">JCKY2022130C020</idno>
					<orgName type="program" subtype="full">Fundamental Research Program</orgName>
				</org>
				<org type="funding" xml:id="_RdBHMGS">
					<idno type="grant-number">YSBR-040</idno>
					<orgName type="program" subtype="full">CAS Project for Young Scientists in Basic Research</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V-Rex</forename></persName>
		</author>
		<idno>49.76±1.68</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lmkg: A large-scale and multi-source medical knowledge graph for intelligent medicine applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Knowledge-Based Systems</publisher>
			<biblScope unit="page">111323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A knowledge graph completion model based on contrastive learning and relation enhancement method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2022.109889</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2022.109889" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="page">109889</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eventkge: Event knowledge graph embedding with event causal transfer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2023.110917</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2023.110917" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="page">110917</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lightweight source localization for large-scale social networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3543507.3583299</idno>
		<ptr target="https://doi.org/10.1145/3543507.3583299" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sequeda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Houben</surname></persName>
		</editor>
		<meeting>the ACM Web Conference 2023<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023-04-30">30 April 2023 -4 May 2023. 2023</date>
			<biblScope unit="volume">2023</biblScope>
			<biblScope unit="page" from="286" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/94" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>aef38441efa3380a3bed3faf1f9d5d-Abstract</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comenet: Towards complete and efficient message passing for 3d molecular graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/0418973" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12-09">November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>e545b932939302cb605d06f43-Abstract-Conference. html</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno>arXiv-2209</idno>
		<title level="m">A molecular multimodal foundation model associating molecule graphs with natural language</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Logical complexity of graphs: A survey</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pikhurko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Model Theoretic Methods in Finite Combinatorics -AMS-ASL Joint Special Session</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Makowsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="2009">January 5-8, 2009. 2009</date>
			<biblScope unit="volume">558</biblScope>
			<biblScope unit="page" from="129" to="180" />
		</imprint>
	</monogr>
	<note>of Contemporary Mathematics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep graph generators: A survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ommi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Baghshah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3098417</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2021.3098417" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="106675" to="106702" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph learning: A survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAI.2021.3076021</idno>
		<ptr target="https://doi.org/10.1109/TAI.2021.3076021" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="127" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=i80OPhOCVH2" />
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hGXij5rfiHw" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust causal graph representation learning against confounding effects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i6.25925</idno>
		<ptr target="https://doi.org/10.1609/aaai.v37i6.25925" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2023">February 7-14, 2023. 2023</date>
			<biblScope unit="page" from="7624" to="7632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement causal structure learning on order graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i9.26274</idno>
		<ptr target="https://doi.org/10.1609/aaai.v37i9.26274" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2023">February 7-14, 2023. 2023</date>
			<biblScope unit="page" from="10737" to="10744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amortized causal discovery: Learning to infer causal graphs from time-series data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v177/lowe22a.html" />
	</analytic>
	<monogr>
		<title level="m">1st Conference on Causal Learning and Reasoning, CLeaR 2022, Sequoia Conference Center</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Eureka, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-04-13">11-13 April, 2022. 2022</date>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="509" to="525" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A differential geometric view and explainability of GNN on evolving graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=lRdhvzMpVYV" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probing GNN explainers: A rigorous theoretical and empirical analysis of GNN explanation methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v151/agarwal22b.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J R</forename><surname>Ruiz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-03-30">28-30 March 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="8969" to="8996" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explainable gnnbased models over knowledge graphs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J T</forename><surname>Cucala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=CrCvGNHAIrz" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inducing causal structure for interpretable neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7324" to="7338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ASAP: adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5997" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33014602</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33014602" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019. 2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Shanmugam</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0925-2312(01)00330-7</idno>
		<ptr target="https://doi.org/10.1016/S0925-2312(01)00330-7" />
	</analytic>
	<monogr>
		<title level="m">Causality: Models, reasoning, and inference : Judea pearl</title>
		<meeting><address><addrLine>cambridge, uk</addrLine></address></meeting>
		<imprint>
			<publisher>cambridge university press</publisher>
			<date type="published" when="2000">2000. 2001</date>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<title level="m">Causal inference in statistics: A primer</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/zhou22e.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07-23">17-23 July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="27222" to="27244" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian invariant risk minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01555</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01555" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">June 18-24, 2022, IEEE, 2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supporting medical relation extraction via causality-pruned semantic dependency forest</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2450" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<idno>arXiv-2205</idno>
		<title level="m">Supporting vision-language model inference with causality-pruning knowledge prompt</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Metamask: Revisiting dimensional confounder for self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38501" to="38515" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-instance causal representation learning for instance label prediction and out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/" />
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>e261e92e1cfb820da930ad8c38d0aead-Abstract-Conference. html</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimal local explainer aggregation for interpretable prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mintz</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/21458" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-03-01">February 22 -March 1, 2022. 2022</date>
			<biblScope unit="page" from="12000" to="12007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2006/aaai06-191.php" />
	</analytic>
	<monogr>
		<title level="m">Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference</title>
		<meeting>The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">July 16-20, 2006. 2006</date>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable graph learning via stochastic attention mechanism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15524" to="15543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in: international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Infogcl: Informationaware graph contrastive learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30414" to="30425" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph-wise common latent factor extraction for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cheung</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/20593" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-03-01">February 22 -March 1, 2022. 2022</date>
			<biblScope unit="page" from="6420" to="6428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual-decoder graph autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107564</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0950705121008261" />
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page">107564</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph convolution for semi-supervised classification: Improved linear separability and out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagannath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Label-invariant augmentation for semi-supervised graph classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno>55926-Abstract-Conference. html</idno>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/bcc" />
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bootstrapping informative graph augmentation via A meta learning approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/416</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2022/416" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</editor>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022, ijcai.org, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="3001" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Talent demandsupply joint prediction with dynamic heterogeneous graph enhanced meta-learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539139</idno>
		<ptr target="https://doi.org/10.1145/3534678.3539139" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Rangwala</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">August 14 -18, 2022. 2022</date>
			<biblScope unit="page" from="2957" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph convolutional reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning binarized graph representations with multi-faceted quantization reinforcement for top-k recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539452</idno>
		<ptr target="https://doi.org/10.1145/3534678.3539452" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Rangwala</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">August 14 -18, 2022. 2022</date>
			<biblScope unit="page" from="168" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph representation learning and its applications: A survey</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3390/s23084168</idno>
		<ptr target="https://doi.org/10.3390/s23084168" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4168</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Prado-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prenkaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.12089</idno>
		<title level="m">A survey on graph counterfactual explanations: Definitions, methods, evaluation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards fine-grained explainability for heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i7.26040</idno>
		<ptr target="https://doi.org/10.1609/aaai.v37i7.26040" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</editor>
		<meeting><address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2023">February 7-14, 2023. 2023</date>
			<biblScope unit="page" from="8640" to="8647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking explaining graph neural networks via non-parametric subgraph matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/wu23j.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="37511" to="37523" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Global explainability of gnns via logic combination of learned concepts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azzolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Longa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barbiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passerini</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=OTbRTIY4YS" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning causally invariant representations for out-ofdistribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/8" />
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>b21a7ea42cbcd1c29a7a88c444cce45-Abstract-Conference. html</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multivariate time series forecasting with transfer entropy graph</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="149" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mthetgnn: A heterogeneous graph embedding framework for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2021.12.008</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2021.12.008" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09613</idno>
		<title level="m">Rethinking causal relationships learning in graph neural networks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v6/pearl10a.html" />
	</analytic>
	<monogr>
		<title level="m">Causality: Objectives and Assessment (NIPS 2008 Workshop)</title>
		<title level="s">JMLR Proceedings</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>Whistler, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-12">December 12, 2008. 2010</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="39" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15445</idno>
		<title level="m">Explainability in graph neural networks: A taxonomic survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning representations by crystallized back-propagating errors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">L</forename><surname>Rutkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korytkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tadeusiewicz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-42505-9_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-42505-9_8" />
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Soft Computing -22nd International Conference, ICAISC 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</editor>
		<meeting><address><addrLine>Zakopane, Poland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">June 18-22, 2023. 2023</date>
			<biblScope unit="volume">14125</biblScope>
			<biblScope unit="page" from="78" to="100" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Posing fair generalization tasks for natural language inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1456</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1456" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="4484" to="4494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An algorithm for deciding if a set of observed independencies has a causal explanation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<ptr target="https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1" />
	</analytic>
	<monogr>
		<title level="m">UAI &apos;92: Proceedings of the Eighth Annual Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</editor>
		<meeting><address><addrLine>Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">July 17-19, 1992. 1992</date>
			<biblScope unit="page" from="2" to="667" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Causality</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1002/0471200611</idno>
		<ptr target="https://doi.org/10.1002/0471200611" />
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph u-nets, in: international conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3154319</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2022.3154319" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="657" to="668" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Distributionally robust neural networks, in: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24934" to="24946" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Geometric knowledge distillation: Topology compression for graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/c" />
	</analytic>
	<monogr>
		<title level="m">06f788963f0ce069f5b2dbf83fe7822-Abstract-Conference</title>
		<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>html</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generalization guarantee of training graph convolutional networks with graph topology sampling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/li22u.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07">July 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="13014" to="13051" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Gnnexplainer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="9240" to="9251" />
		</imprint>
	</monogr>
	<note>d80b7040b773199015de6d3b4293c8ff-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i5.16514</idno>
		<ptr target="https://doi.org/10.1609/aaai.v35i5.16514" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Do we need anisotropic graph neural networks?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Opolka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hl9ePdHO4_s" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Multivariate time-series classification with hierarchical variational graph pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2022.07.032</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2022.07.032" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="481" to="490" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
