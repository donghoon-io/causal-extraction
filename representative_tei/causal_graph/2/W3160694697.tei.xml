<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCR-Graph: Spatial-Causal Relationships based Graph Reasoning Network for Human Action Prediction</title>
				<funder>
					<orgName type="full">Program of National Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Decai</forename><surname>Li</surname></persName>
							<email>lidecai@sia.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>He</surname></persName>
							<email>heyuqing@sia.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Chinese Academy of Sciences Shenyang</orgName>
								<address>
									<settlement>Liaoning</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences Shenyang</orgName>
								<address>
									<settlement>Liaoning</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Chinese Academy of Sciences Shenyang</orgName>
								<address>
									<settlement>Liaoning</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Chunsheng Hua Liaoning University Shenyang</orgName>
								<address>
									<settlement>Liaoning</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SCR-Graph: Spatial-Causal Relationships based Graph Reasoning Network for Human Action Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Technologies to predict human actions are extremely important for applications such as human robot cooperation and autonomous driving. However, a majority of the existing algorithms focus on exploiting visual features of the videos and do not consider the mining of relationships, which include spatial relationships between human and scene elements as well as causal relationships in temporal action sequences. In fact, human beings are good at using spatial and causal relational reasoning mechanism to predict the actions of others. Inspired by this idea, we proposed a Spatial and Causal Relationship based Graph Reasoning Network (SCR-Graph), which can be used to predict human actions by modeling the action-scene relationship, and causal relationship between actions, in spatial and temporal dimensions respectively. Here, in spatial dimension, a hierarchical graph attention module is designed by iteratively aggregating the features of different kinds of scene elements in different level. In temporal dimension, we designed a knowledge graph based causal reasoning module and map the past actions to temporal causal features through Diffusion RNN. Finally, we integrated the causality features into the heterogeneous graph in the form of shadow node, and introduced a self-attention module to determine the time when the knowledge graph information should be activated. Extensive experimental results on the VIRAT datasets demonstrate the favorable performance of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the interest in human action prediction is increasing owing to its broad and important applications such as autonomous driving <ref type="bibr" target="#b0">[1]</ref>, human-robot cooperation <ref type="bibr" target="#b1">[2]</ref>,and security monitoring <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, human action prediction is a particularly challenging problem owing to three main reasons (Figure .1): <ref type="bibr" target="#b0">(1)</ref> Future actions of humans are internally driven with a certain purpose; however, because this purpose is not visible, we can only make inferences and judgments based on available external information <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b1">(2)</ref> The change of surrounding environment or the intervention of other people affects the next step in human action, by affecting the decision-making <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b2">(3)</ref> Even if human current action is the same in similar environment, different past actions can cause different action choices to be made in the future. Below, we will briefly review the relative works about human action prediction and introduce SRC-Graph method that proposed in this paper.</p><p>In recent years, various attempts have been made in human action prediction. The works in <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>[8] identified activity prediction with early detection of short-duration single action using hand-crafted features; however, the limited expression ability of these features limits the effect of action prediction. With the rapid development of deep learning, researchers began to use deep neural network (two stream CNNs <ref type="bibr" target="#b8">[9]</ref>, RNNs <ref type="bibr" target="#b9">[10]</ref>, 3D-CNNs <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>, etc.) to automatically extract video features, and predict actions based on these features. <ref type="bibr" target="#b12">[13]</ref> proposed a temporal deep model to better learn activity progression for performing activity detection and early detection tasks. But in this case, it is easy to be disturbed by sample noise due to the models inability to extract the key information of video accurately. <ref type="bibr" target="#b13">[14]</ref> designed a mem-LSTM model using CNN and LSTM to model the spatial and time dimensions. However, because of taking the raw video images as the input and ignoring mining the relationships behind, the prediction performances of these methods are limited in both accuracy and time horizon.</p><p>Recently, <ref type="bibr" target="#b0">[1]</ref> proposed a model that pays particular attention to the subjects in the videos with human detection and tracking over large time horizons. Good predictions have been made in predicting the person's behavior for the next second, so it cannot predict the human actions after a longer time ( 5 to 10 seconds). Li Fei-Fei and her team proposed an end-to-end, multi-task learning system utilizing rich visual features <ref type="bibr" target="#b2">[3]</ref>. They encoded a person by using rich semantic features about visual appearance, body movement and the persons interaction with the surroundings. Their study was motivated by the fact that humans derive such predictions by relying on similar visual cues, and it achieved remarkable better results and predict the future 4.8 seconds (12 frames) of person trajectory, However, the methods above mainly focus on adding together, the visual features about the human behavioral information, and the interaction of humans with their surroundings directly. The influence of the relationship between different objects and pedestrians on the future behavior of humans in the scene was not deeply explored. In addition, they did not consider the causality in action sequences. For example, as shown in Figure .1, if a person tend to a car while pulling a box , in the next moment, he will have exhibit a high probability of opening the trunk to place the box and then opening the door to get into the car.</p><p>In consideration of the above factors, in order to accurately predict the actions in a long period of time, we need to not only model the relationship between human behaviors and objects in the scene, but also to conduct the causal reasoning of transformation of actions according to the time series of the actions.</p><p>We proposed a spatial and causal reasoning graph network (SCR-Graph) to model the surrounding relationships in spatial dimension and the causal relationships in time dimension. This is structurally composed of two parts: The first part in spatial dimension is designed to capture the features of spatial topological relation between the subject and the surrounding environment elements. This is achieved by constructing a hierarchical heterogeneous graph neural network, so as to obtain the relation influence of the surrounding objects and others on the subject. The second part in time dimension is proposed to create a causal reasoning module, whose function is to constrain the action prediction results by using the action causal relationship reasoning, to make it fit the logical relationship between the action sequences. At the same time, considering that the switching time of actions is affected by multiple factors, we integrated the causality features (which were acquired by the Gate-Graph reasoning on knowledge graph in the time dimension) into the heterogeneous graph in the form of shadow node, and introduced a self-attention module to judge when the knowledge graph information should be activated during processing.</p><p>The main contributions of this study are summarized as follows:</p><p>(1) Inspired by the principles of humans powerful action prediction ability, we proposed a two-stream graph framework called SCR-Graph. Through construction of the surrounding relationships modeling module in spatial dimension and the causal reasoning module in time dimension, this model acquired the ability of logical reasoning in two dimensions, thus imitating humans.</p><p>(2) To improve the models performance and generalization ability, we construct Diffusion-RNN as a guidance to automatically determine the action prediction results. We further integrated the causality features into the heterogeneous graph in the form of shadow node.</p><p>(3) We conducted extensive experiments on action prediction datasets and demonstrated the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The introduction of unstructured data modeling and processing methods has greatly improved our algorithm. In this section, we briefly review the existing methods on two problems related to our work: graph neural networks and knowledge graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Neural Networks.</head><p>Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs <ref type="bibr" target="#b14">[15]</ref>. These can be divided into two types: graph convolution neural networks (GCN) and graph gate neural networks (Gate-GNN).</p><p>For graph convolution neural network, advances in this direction are often categorized as spectral approaches and non-spectral approaches. <ref type="bibr" target="#b15">[16]</ref> proposed the spectral network, whose convolution operation is defined in the Fourier domain by computing the eigen decomposition of the graph Laplacian. But a model trained on a specific structure could not be directly applied to a graph with a different structure, which limited their application scenarios. Non-spectral approaches define convolutions directly on the graph operating on spatially close neighbors <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b17">[18]</ref> proposed the GraphSAGE, a general inductive framework, which can be applied to different graph structures as it generates embeddings by sampling and aggregating features from a nodes local neighborhood. <ref type="bibr" target="#b18">[19]</ref> proposed a graph attention network (GAT) which incorporates the attention mechanism into the propagation step. It computes the hidden states of each node by attending over its neighbors, following a selfattention strategy. And <ref type="bibr" target="#b19">[20]</ref> extends attention mechanism to heterogeneous graphs to deal with those which contain different types of nodes and links.</p><p>Gate based graph neural network attempts to use the gate mechanism, similar to GRU <ref type="bibr" target="#b20">[21]</ref> or LSTM <ref type="bibr" target="#b21">[22]</ref>, in the propagation step to diminish the restrictions in the former GNN models and improve the long-term propagation of information across the graph structure <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr" target="#b22">[23]</ref> combined Graph Convolutional Networks (GCNs) and RNN to model spatial structures and dynamic patterns. Although this achieves the reasoning function of graph structure information in time dimension, but it cannot be used in directed graphs. To model the traffic flow as a diffusion process on a directed graph, <ref type="bibr" target="#b23">[24]</ref> proposed Diffusion Convolutional Recurrent Neural Network (DCRNN), which has the ability to capture the spatiotemporal dependencies on directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge Graphs</head><p>Knowledge graph is a kind of structured knowledge base, which describes the concepts, entities and their relationships in the objective world in a structured way <ref type="bibr" target="#b24">[25]</ref>. It can be applied to various kind of tasks, such as situation recognition <ref type="bibr" target="#b25">[26]</ref>, object detection <ref type="bibr" target="#b26">[27]</ref> and visual relationship extraction <ref type="bibr" target="#b27">[28]</ref>.</p><p>A knowledge based framework which can generalize to other tasks was proposed in <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b29">[30]</ref> introduced a graph search neural network (GSNN), which can exploit large knowledge graphs into an end-to-end framework for image classification <ref type="bibr" target="#b30">[31]</ref>, this was different from the methods which treat knowledge graph as a separate component in their frameworks. In addition, <ref type="bibr" target="#b30">[31]</ref> applies knowledge graphs for video classification, and propose a novel knowledge-based attention model. However, using knowledge graph to solve the problem of human action prediction has not been studied in depth yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Problem Definition: We assumed that we already know the tracking bounding box of all people and objects in the scene, based on which we can obtain the features of people and objects easily. Given continuous video frames with pedestrian tracking bounding boxes and object detection results, our model aims to predict each person's actions in the frame in the future.</p><p>Overall Framework: Fig. <ref type="figure" target="#fig_1">2</ref> shows the overall network architecture of our SC-Graph model.</p><p>(a) In spatial dimension: We constructed a hierarchical heterogeneous graph attention network (H-GAT) to get the interactive intention features of human-human and humanscene interactions through the topological relationship between humans and objects in the scene. In addition, to make the network capable of introducing causal reasoning features when necessary, and for better integration of time dimension features, we designed a shadow node module with fusion weight self-adjusting function.</p><p>(b) In time dimension: Based on the principle of statistics, we construct Diffusion-RNN that have the ability of graph causal reasoning to get the features and scores of different action nodes in the next moment. Then, we fuse the features of the top K nodes and send them to the shadow nodes to guide the results of action prediction more logically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graphs Construction methods</head><p>The construction of graph structure is an important foundation of graph neural network reasoning. The quality of graph structure construction determines the subsequent reasoning effect. As shown in Figure .3, the targets that may attract human interaction intention include people and objects in the scene. Therefore, building the relationship between humans and objects is a critical issue. What comes first is to reduce the influence of irrelevant people and objects on a human's action prediction in the complex environment. Here, we set up a human's behavior perception range circle according to each person's size in the picture. The radius Dis (id) of this circle is determined using Formula 1 and 2: 2  (1)</p><formula xml:id="formula_0">l (id) = m (id) 2 + n (id)</formula><formula xml:id="formula_1">Dis (id) = λ • l (id)<label>(2)</label></formula><p>Where m (id) , n (id) and l (id) are the width, height and diagonal length of pedestrian No. id in the picture respectivel, and λ is a constant coefficient.</p><p>Then, we connected the objects within the subject's perception scope with the subject. But considering the special influence of other people on the subject's interaction intention in the scene (for example, another person far away from the scene wave his hand to subject, which will also cause this subject's interaction intention), we constructed the graph structure in the form of full connection for humans. Using this rule, we can obtain the spatial dimension graph structure in any scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action-scene relationship reasoning in spatial dimension</head><p>After got the spatial relation graph structure, the next question is how to define and extract the features of each type of node. For "human" type nodes, the features that may have constraints on future actions include apparent features (human appearance features, semantic features of humanscene, etc.) and motion features (human trajectory features, skeleton point motion features, etc.). Here we use LSTM to encode all the human node related features, which is same to <ref type="bibr" target="#b2">[3]</ref>. And for object type nodes, the features contain objects catageray, size and location informations. As shown in Figure .5, the hierarchical features aggregation process on the graph can be divided into three steps: node level, type level and shadow level aggregation. First of all, node level aggregation is the process of aggregating node features of the same type. As each node play a different role and show different importance in learning node embedding for the specific task, we introduce node-level attention can learn the importance of same type neighbors for each node and aggregate the representation of these meaningful neighbors to form a node embedding <ref type="bibr" target="#b19">[20]</ref>.</p><p>Due to the heterogeneity of nodes, different types of nodes have different feature spaces. Therefore, for each type of nodes, we design the type-specific transformation matrix M φi to project the features of different types of nodes into the same feature space (Formula 3).</p><formula xml:id="formula_2">h i = M φi • h i<label>(3)</label></formula><p>Where h i and h i are the original and projected feature of node i , respectively. Then we leverage self-attention to learn the weight among various kinds of nodes. The importance of node pair (i, j) in node level can be formulated as Eq. 4 and get the weight coefficient e Φ ij via softmax function as Eq. 5</p><formula xml:id="formula_3">e Φ ij = att node (h i , h j ; Φ)<label>(4)</label></formula><formula xml:id="formula_4">α Φ ij = sof tmax j (e Φ ij )<label>(5)</label></formula><p>Then, the embedding of node i in each type can be aggregated by the neighbors projected features with the corresponding coefficients as follows (Eq. 6):</p><formula xml:id="formula_5">z Φ i = K k=1 σ( j∈N Φ i α Φ ij • h j )<label>(6)</label></formula><p>Where z Φ i is the learned embedding of node i for the meta-path Φ. And we extend node-level attention to multihead attention (K is the head number) so that the training process is more stable.</p><p>For the type-level attention, the importance of each metapath, denoted as w Φi , is shown as follows (Eq. 7):</p><formula xml:id="formula_6">w Φi = 1 |V | i∈V q T • tanh(W • z Φ i + b)<label>(7)</label></formula><p>Here, W is the weight matrix, b is the bias vector, q is the semantic-level attention vector. The weight of metapath Φ i , denotedas β Φi ,can be obtained by normalizing the above importance of all meta-paths using softmax function as Eq. 8:</p><formula xml:id="formula_7">β Φi = exp(w Φi ) P i=1 exp(w Φi )<label>(8)</label></formula><p>With the learned weights as coefficients, we can fuse these semantic-specific embeddings to obtain the embedding Z c as follows (Eq. 9):</p><formula xml:id="formula_8">Z c = P i=1 β Φi • Z Φi (9)</formula><p>Finally, in order to achieve the function of autonomously fusing the output features of causal inference at the right time, we introduce shadow nodes into the model. The aggregation function in shadow level is formulate by Eq. 10:</p><formula xml:id="formula_9">Z o = σ(Z c + att node (Z c ; Φ) • Z s )<label>(10)</label></formula><p>In which Z s is the shadow node features and Z o is the final embeded node features for action predicting. After processed by FC and softmax module, the action prediction results can be obtained. As it is a multi-lables classification problem, we choose the BEC loss which is designed as:</p><formula xml:id="formula_10">L(x, y) = N n=0 -ω n [y n • log σ(x n ) + (1 -y n ) • log(1 -σ(x n ))] (11)</formula><p>Here, x and y are the prediction result and ground truth, respectively. N is the number of categories of actions and ω n is the weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Causal relationship reasoning module in temporal dimension</head><p>As there are many causal relationships in human actions sequence (such as upload box then get in the trunk), and people often have multiple actions at the same time (such as talk while walking). How to model the multiple cooccurrence actions and causal reasoning in temporal dimension is the focus of this part.</p><p>We can represent the actions switching causality graph as a weighted directed graph G = (V, E, W), where V is a set of nodes |V | = N , E is a set of edges and W ∈ R N ×N is a weighted adjacency matrix representing the nodes proximity <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial dependency modeling:</head><p>In order to model the relationship between co-occurrence behaviors, we introduce the diffusion convolution into our method. The diffusion process can be represented as a weighted combination of infinite random walks on the graph <ref type="bibr" target="#b31">[32]</ref>.</p><p>The resulted diffusion convolution operation over a graph signal X ∈ R N ×P and a filter f θ is defined as:</p><formula xml:id="formula_11">X :,p f θ = K-1 k=1 (α(1-α) k (D -1 O W) k )X :,p f or p ∈ 1, ..., P<label>(12)</label></formula><p>Here, θ ∈ R K are the parameters for the filter and D -1 O W represent the transition matrice of the diffusion process. In which, D O = diag(Wl) is the out-degree diagonal matrix and l is the all one vector. P is the dimension of each node's feature <ref type="bibr" target="#b23">[24]</ref>.</p><p>With the convolution operation defined in Eq.12, we can build a diffusion convolutional layer that maps Pdimensional features to Q-dimensional outputs. The diffusion convolutional layer is formulated by Eq.13:</p><formula xml:id="formula_12">H :,q = a( P p=1 X :,p f Θ q,p,:,: ) f or ∈ 1, ..., Q (13)</formula><p>where H :,q is the output,{f Θ q,p,:,: } are the filters and a is the activation function.</p><p>Temporal dynamics modeling:</p><p>We leverage the Gated Recurrent Units (GRU) <ref type="bibr" target="#b32">[33]</ref> to model the temporal dependency. The Diffusion Convolutional Gated Recurrent Unit (DCGRU) is formulated by Eq.14-17.</p><formula xml:id="formula_13">r (t) = σ(Θ r G[X (t) , H (t-1) ] + b r )<label>(14)</label></formula><formula xml:id="formula_14">u (t) = σ(Θ u G[X (t) , H (t-1) ] + b u )<label>(15)</label></formula><formula xml:id="formula_15">C (t) = tanh(Θ C G[X (t) , (r (t) H (t-1) )] + b c ) (16)</formula><formula xml:id="formula_16">H (t) = u (t) H (t-1) + (1 -u (t) ) C (t)<label>(17)</label></formula><p>where X (t) , H (t) denote the input and output of at time t , rr (t) , u (t) are reset gate and update gate at time t, respectively. Θ r , Θ u , Θ C are parameters for the corresponding filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the performance of the proposed method on the ActEV/VIRAT <ref type="bibr" target="#b33">[34]</ref> <ref type="bibr" target="#b34">[35]</ref> dataset for action prediction. The extensive results thus obtained demonstrate the effectiveness of our method for the prediction of human actions. Finally, we conducted a component analysis of our framework according to several examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset: ActEV/VIRAT <ref type="bibr" target="#b33">[34]</ref> is a public dataset released by NIST in 2018 for activity detection research in streaming video. This dataset is an improved version of VIRAT <ref type="bibr" target="#b34">[35]</ref> , with more videos and annotations. It includes 455 videos at 30 fps from 12 scenes, more than 12 hours of recordings. Most of the videos have a high resolution of 1920x1080.</p><p>Features: All the pre-extracted features used in this model, including: human keypoints motion feature <ref type="bibr" target="#b35">[36]</ref>, human appearance feature <ref type="bibr" target="#b36">[37]</ref>, trajectory feature, humanobject spatial feature and human-scene feature, are extracted through pre-trained model and uniformly encoded through LSTM <ref type="bibr" target="#b2">[3]</ref>. Each feature is encoded as a vector of length 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head><p>(1) Knowledge graph construction:</p><p>We tested the construction effect of knowledge graph first, since whose quality has a great influence on the subsequent graph reasoning effect.</p><p>We use statistical method to count the actions switching relationship and calculate the switching probability. In details, we take every 12 frames in the training data set as a time segment to count the number of people's actions switchs between adjacent time segments, then we can get a actions switching matrix, in which each unit represents the times of actions switching (includes self swich to self). Based on this raw statistical data, we can calculate the swiching probability by softmax function in each row.</p><p>Based on the probability switching matrix, we can build the knowlegde graph, which is shown in Figure .6 (The Action-ID correspondence is list in Table.1) (2) Baseline method:  We compare our method with the recent baseline: Next <ref type="bibr" target="#b2">[3]</ref> is an end-to-end model utilizing rich visual features about the human behavioral information and interaction with their surroundings to predict human actions, which is similar with our work in this paper. This method uses LSTMs to extract features, and directly splits different types of features into a long vector. And based on the conbined feature, they can get the prediction results.</p><p>(3) Comparisons with baseline method:</p><p>As this is a multi-lable classification problem, we use Mean Average Precision (mAP) <ref type="bibr" target="#b37">[38]</ref> to evaluate the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Next  We test the two methods (Next and our method) with different input pre-extracted features, the experiment results are shown in Table .1. The column on the left shows the experimental results of Next, and the one on the right cor-responds ours. From Table .1, we can find that our method has a 2.4% -16.2% improvement under each condition.</p><p>After in-depth analysis into the Nexts results, we find that the P-Objects and P-Scene are noisy features, addition of which decrease the prediction accuracy of Next. Different from Next, our method is not sensitive to these noises. It is mainly because that our model uses attention modules and hierarchical aggregation mechanism to embed features, which makes it got the related features selection and strong noises anti-interference ability (the performace of our method with full features is 10.9% better than Next). ( <ref type="formula" target="#formula_3">4</ref>) Comparisons with different model types:</p><p>In this part, we try to analysis the impacts of different parts of our model on the prediction results and the following three experiments are conducted.  First, we examine the impact of human interactions on the prediction and build the graph structure of spatial relationships by utilizing human information alone. The human-objects relationships as well as causal relationship between actions are not considered. In this case, the mAP of result is 0.194 (Table .2) which is similarly to that of Next. This means that a proper designed feature aggregation mechanism is important and is helpful to the action prediction by using just simple features.</p><p>Then we embed both the human and objects nodes into our model by the hierarchical attention aggregation mechanism. As shown from Table .2, the result is improved to 0.208 mAP, which implies that the introduction of humanobjects relationships can provide useful information and enhance the ability of prediction model. Meanwhile, by comparing with the results of Next, we can find that directly introduce the scene information may produce noisy features, which will decrease the prediction performance. On the other hand, by modeling the action-scene relationship with our hierarchical graph attention module, the features of different scene elements can be effectively aggregated, which significantly improves the results of actions prediction.</p><p>Finally, we integrated the causality features into the heterogeneous graph of the action-scene relationship in the form of shadow node. Moreover, because of the designing of self-attention module, we can determine the time when the knowledge graph information should be activated. In this case, the prediction result acheives 0.213 mAP, which is 10.9% higher than Next. It means that introducing causal relationships between actions can further improve the prediction result of our method. (5) Qualitative analysis:</p><p>We compare the performance of our model outputs and the baseline while visualize the mid-level outputs of our model as shown in Figure .7. In spatial dimension, the yellow lines are relationships between humans, and the white represent relationships between human and objects. Brightness of the connecting line corresponds to the weight of attention. Accordingly, in the time dimension, the brightness of different action nodes means the probability of each action prediction output.</p><p>Figure .7(a) shows a simple scenario, in which a person is carrying a box to another one near the car. The spatial relationships between human-scenes are visualized, from which we can see that the person with box pays more attention to the car and the driver ahead (the lines between them are brighter than others). For the visualization of casual relationships, nodes in the knowledge graph are sequentially activated from "walking" and "carrying" to "talking", "transfer", "walking" and "carrying" during the period [t -2µ, t] which are consistent with human cognitions.</p><p>Figure .7(b) depicts a more complex scene and we take the person at the bottom right of the view as an example to illustrate the visualization performance of our method. As shown form Figure .7(b), spatial relationships between the person and the surrounding ones are closer than others. Meanwhile, it can be inferred that the person is more likely to cross the road with others together, which is inconsistent with ground truth and demonstrate the effectiveness of our method in the complex environment.</p><p>Figure .7(c) is a scene contains cooperation. We can find that the person with baggage pays simillar attentions to both the blue and white car, which may confuse the prediction result. But the transfer of high interaction intention of the partner in front enables our model to infer the correct answer, which illustrate the hierarchical graph attention module is useful to eliminate ambiguities. The result is consistent with the results obtained according to the causal reasoning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, considering the spatial and causal relational reasoning mechanism for action prediction in human beings, we proposed a spatial and causal relationship based graph reasoning network (SCR-Graph), which can be used to predict human actions by modeling the action-scene relationship and actions causal relationship in spatial and temporal dimensions, respectively. By introducing the hierarchical graph attention module in the spatial dimension, our model was able to fuse features of different kinds of scene elements using different strategies. In the temporal dimension, we designed a knowledge graph based causal reasoning module by sequential nodes activation process. Moreover, we proposed a fusion method for spatial and causal relationship features, with self-attention shadow nodes. The effectiveness of our method is evidenced by its favorable performance, as compared to existing methods.</p><p>In the future, we plan to explore the method to build an effective relational reasoning model in the original video, without annotation bounding boxes. This is a difficult problem because in a complex environment, it is difficult for the existing human and object detection and tracking algorithms to achieve reliable results. Therefore, our next goal is to design a more adaptable relational reasoning model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. What determines a human's future actions?</figDesc><graphic coords="1,322.53,465.21,57.93,56.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The overall framework of SCR-Graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Spatial relationship graph construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Temporal causal relationship graph construction. In time dimension, as the causality of actions are directed relation, constructing a directed knowledge graph can obtain better experimental results and make the model more convergent. The knowledge graph is a causality transfer graph between different actions which is obtained by analyzing the actions transfer of training dataset with statistical method. As shown in Figure.4, we calculate the transition probability matrix W of actions first, then build the weighted dirrected graph structure representing the actions switching relationships (more details is explaned in Section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Action-scene hierarchical features aggregation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The weighted directed knowledge graph.</figDesc><graphic coords="7,85.55,72.00,165.37,124.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Human, Objects and Shadow nodes 0.213</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The middle layer outputs visualization and comparison experiment results of our model.</figDesc><graphic coords="8,58.86,174.68,55.19,74.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Correspondence between Actions and IDs.</figDesc><table><row><cell cols="2">Action ID</cell><cell>Action</cell><cell>ID</cell><cell>Action</cell><cell>ID</cell></row><row><cell>BG</cell><cell>0</cell><cell>Talking</cell><cell cols="3">10 Talking phone 20</cell></row><row><cell cols="2">Walking 1</cell><cell cols="2">Transport 11</cell><cell>Tunning</cell><cell>21</cell></row><row><cell cols="2">Standing 2</cell><cell cols="2">Unloading 12</cell><cell>PickUp</cell><cell>22</cell></row><row><cell cols="2">Carrying 3</cell><cell>Pull</cell><cell cols="3">13 Using Tool 23</cell></row><row><cell cols="2">Gesturing 4</cell><cell>Loading</cell><cell>14</cell><cell>SetDown</cell><cell>24</cell></row><row><cell cols="4">Closing 5 Open Trunk 15</cell><cell>Crouching</cell><cell>25</cell></row><row><cell cols="4">Opening 6 Closing Trunk 16</cell><cell>sitting</cell><cell>26</cell></row><row><cell cols="2">Interacts 7</cell><cell>Riding</cell><cell cols="3">17 Object Transfer 27</cell></row><row><cell cols="4">Exiting 8 Texting Phone 18</cell><cell>Push</cell><cell>28</cell></row><row><cell cols="4">Entering 9 PP Interaction 19</cell><cell>PickUp</cell><cell>29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to baseline method Next on the ActEV/VIRAT dataset</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison results with different model types.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement: Research supported by the <rs type="funder">Program of National Natural Science Foundation</rs> of</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>China (No.U1609210, No.91748208) and Natural Science Foundation of China-Joint Funds of Liaoning Province (No.U1508208).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long-term on-board prediction of people in traffic scenes under uncertainty</title>
		<author>
			<persName><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4194" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Goal set inverse optimal control and iterative replanning for predicting human reaching motions in shared workspaces</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafi</forename><surname>Hayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Berenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="897" to="908" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Peeking into the future: Predicting future person activities and locations in videos</title>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5725" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anthropomorphism: opportunities and challenges in human-robot interaction</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Złotowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Proudfoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Yogeeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bartneck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="360" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction of human activity by discovering temporal sequence patterns</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1644" to="1657" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial action prediction networks</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action prediction from videos via memorizing hard-to-predict samples</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangqian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01926</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Domain-Specific Knowledge Graph Construction</title>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Kejriwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4173" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Object detection meets knowledge graphs</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kingsley</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05670</idno>
		<title level="m">Building a large-scale multimodal knowledge base system for answering visual queries</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Watch, think and attend: End-to-end video classification via dynamic knowledge evolution modeling</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable algorithms for data and network analysis</title>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Trecvid 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search</title>
		<author>
			<persName><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asad</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yooyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afzad</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A largescale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName><forename type="first">Sangmin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naresh</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Taek Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurajit</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungtae</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the stratification of multi-label data</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
