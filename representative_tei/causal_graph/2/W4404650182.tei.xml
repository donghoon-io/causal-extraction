<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterophilic Graph Neural Networks Optimization with Causal Message-passing</title>
				<funder ref="#_X9ddj8j">
					<orgName type="full">Nansha Key Area Science and Technology Project</orgName>
				</funder>
				<funder ref="#_rtSdNT3">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_xGQk7Zn">
					<orgName type="full">Guangzhou Industrial Information and Intelligent Key Laboratory Project</orgName>
				</funder>
				<funder ref="#_RgqdpSU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-27">27 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Botao</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Also with Hong</orgName>
								<orgName type="institution">Kong University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Keli</forename><surname>Zhang</surname></persName>
							<email>zhangkeli1@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Fugee</forename><surname>Tsung</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Also with Hong</orgName>
								<orgName type="institution">Kong University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
							<email>changh.heng@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Heng Chang</orgName>
								<orgName type="institution">University of Science and Technology (Guangzhou) Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab Shenzhen</orgName>
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<settlement>Ltd Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterophilic Graph Neural Networks Optimization with Causal Message-passing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-27">27 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3701551.3703568</idno>
					<idno type="arXiv">arXiv:2411.13821v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal structure</term>
					<term>Heterophiliy</term>
					<term>Graph neural network</term>
					<term>Message passing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we discover that causal inference provides a promising approach to capture heterophilic message-passing in Graph Neural Network (GNN). By leveraging cause-effect analysis, we can discern heterophilic edges based on asymmetric node dependency. The learned causal structure offers more accurate relationships among nodes. To reduce the computational complexity, we introduce intervention-based causal inference in graph learning. We first simplify causal analysis on graphs by formulating it as a structural learning model and define the optimization problem within the Bayesian scheme. We then present an analysis of decomposing the optimization target into a consistency penalty and a structure modification based on cause-effect relations. We then estimate this target by conditional entropy and present insights into how conditional entropy quantifies the heterophily. Accordingly, we propose CausalMP, a causal message-passing discovery network for heterophilic graph learning, that iteratively learns the explicit causal structure of input graphs. We conduct extensive experiments in both heterophilic and homophilic graph settings. The result demonstrates that the our model achieves superior link prediction performance. Training on causal structure can also enhance node representation in classification task across different base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Knowledge representation and reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The message-passing mechanism of graph neural network (GNN) inherently assumes homophily, which degrades in the heterophilic graphs. Heterophily refers to the characteristic of graphs whose edges are more likely to connect the nodes from different classes. In such graphs, the representations of node pairs become less distinguishable after being smoothed by their neighborhood features. This issue becomes particularly pronounced when there is lack in the node label information, such as link prediction task and few-shot node classification. The heterophilic edges can also act as the noise that hinder the optimization. Numerous studies have been proposed to improve message passing, aiming to learn more fair representations that are not affected by the heterophilic edges. They are usually dedicated to disentangling heterophilic information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> or improving the information gathering process in the graph <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. However, these techniques are taskspecific with low generalization ability. We also find most fail to demonstrate substantial performance improvements across both homophilic and heterophilic graphs.</p><p>Causal inference emerges as a promising approach for capturing the cause-effect among variables according to the distribution of observed data. While self-training is a widely used and straightforward strategy for relationship discovery <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32]</ref>, it focuses on strengthening correlations, which is inadequate for complex and heterophilic graphs. Causal inference can detect dependencies at a higher level <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>. It can be utilized to identify heterophilic edges, which exhibit asymmetric dependencies between connected node pairs. To illustrate the concept, we consider a camouflaged  fraudster detection case, as depicted in Fig. <ref type="figure" target="#fig_1">1</ref>. The nodes consist of fraudsters and benign users. Fraudsters can mimic the behaviors of benign users and establish connections with them, having similar node and structural embeddings. These heterophilic links make it challenging for GNN-based models to effectively detect fraudsters. However, there exists an information flow from benign users to fraudsters due to the mimicry behavior. Causal inference can detect the node dependencies where the fraudsters depend on the connected benign users, which suggest the presence of heterophily. If we revise the distinguished heterophilic edges between directed edges from benign users to fraudsters, the graph can mitigate the feature smoothing. The updated graph aligns more closely with the true information flow and improves the aggregation process within GNNs.</p><p>Causal inference typically involves conducting independence tests to determine the sub-structure of the causal graph. According to Pearl's causal analysis scheme <ref type="bibr" target="#b20">[21]</ref>, there are two higher levels than association (correlation) analysis: intervention-based and counterfactual inference. Although counterfactual inference is at the highest level, it is request non-existed knowledge that usually estimated by a generative process or searching for an estimation unobtainable data <ref type="bibr" target="#b36">[37]</ref>, which is time-consuming and computationally expensive. The intervention-based method can be realized by graph augmentation to uncover variable dependency <ref type="bibr" target="#b30">[31]</ref>. It does not require background knowledge and can improve the information aggregation process <ref type="bibr" target="#b17">[18]</ref>. The nature of the variable distribution is described by the node dependencies, whose inference process should ideally have better generalization ability. And the learned causal structure should be effective across heterophilic and homophilic graph.</p><p>However, there remains challenges of application of causal inference in GNNs. First, most causal inference methods for graph data primarily focus on graph classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref>or invariant learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The analysis of causality among node variables for a single input graph remains undefined. Besides, it is crucial to construct a message passing that maintains generalization ability.</p><p>The learned causal structure should align with node relationships, enhancing the performance of GNNs across various downstream tasks and graphs. Furthermore, to the best of our knowledge, there have been no studies that apply causal inference specifically to address heterophily. To tackle the challenges above, we propose a network that learns explicit Causal Message-Passing (CausalMP) of input graph by node dependency discovery. We quantify the intervention-based cause-effect by the dependencies between node pairs. Specifically, we construct a causality estimator among the node variables by comparing the conditional entropy in both directions of the edge. We demonstrate its effectiveness in indicating the presence of heterophily. Then, we modify the topology accordingly for the subsequent optimization. Experiments show that the proposed model achieves better performance in link prediction across both heterophilic and homophilic graphs. Moreover, the learned causal structure can improve the GNNs' training in node classification performance of different baseline methods, even when there are limited labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>ğ‘‘ğ‘œ-operator for causal inference. In causal inference, the ğ‘‘ğ‘œoperator is usually applied to evaluate the cause-effect relation. It represents interventions conducted on variables to assess their causal effects. By applying interventions, we can quantify the impact of intervened variables by observing the resulting variations in the distribution of other variables. We use ğ‘ƒ (ğ‘‹ 2 |do(ğ‘‹ 1 )) to describe the probability density of ğ‘‹ 1 â†’ ğ‘‹ 2 , which quantifies the effect on ğ‘‹ 2 resulting from ğ‘‹ 1 . ğ‘‘ğ‘œ-operator can be calculated by conditional probability in some scenarios. If there are three variables ğ‘‹ 1 , ğ‘‹ 2 , ğ‘‹ 3 and we wish to use the individual treatment effect (ITE) to quantify the effect of ğ‘‹ 1 on ğ‘‹ 2 , we can apply the ğ‘‘ğ‘œ-operator as:</p><formula xml:id="formula_0">ğ‘ƒ (ğ‘‹ 2 |do(ğ‘‹ 1 )) = âˆ‘ï¸ ğ‘‹ 3 ğ‘ƒ (ğ‘‹ 2 |ğ‘‹ 1 , ğ‘‹ 3 )ğ‘ƒ (ğ‘‹ 3 ).<label>(1)</label></formula><p>If ğ‘‹ 1 is binary treatment, then the ITE is calculated by:</p><formula xml:id="formula_1">ğ¼ğ‘‡ ğ¸ (ğ‘‹ 1 â†’ ğ‘‹ 2 ) = ğ‘ƒ (ğ‘‹ 2 |do(ğ‘‹ 1 = 1)) -ğ‘ƒ (ğ‘‹ 2 |do(ğ‘‹ 1 = 0)).<label>(2)</label></formula><p>In the multivariable situations, ğ‘‹ ğ‘– , ğ‘– = 1, 2, ... can represent variable groups. Then the structural equation model (SEM) can be employed for causal analysis. In SEM, the relationships among variables are depicted using directed edges in a graphical model denoted as M (ğ‘‹, ğ´ ğ‘ ), where ğ‘‹ represents the variables and ğ´ ğ‘ represents the causal graph in the form of an adjacency matrix. The model M typically starts from either a fully connected graph or an initialization based on prior knowledge graph, whose adjacency matrix denoted by ğ´. Various algorithms, such as PC, FCI, etc., have been proposed to search for subgraphs that are faithful to the true causal graph ğ´ ğ‘ . Subsequently, conditional independence tests are conducted on the observed data to maximize the averaged treatment effect (ATE) after pruning.</p><formula xml:id="formula_2">ğ´ğ‘‡ ğ¸ = E (ğ‘–,ğ‘— ) âˆˆğ´ [ğ¼ğ‘‡ ğ¸ (ğ‘‹ ğ‘– â†’ ğ‘‹ ğ‘— )].<label>(3)</label></formula><p>Estimation of causality. Intervention-based methods in structural equation modeling (SEM) exhibit superior performance in independence tests for uncovering causal relationships. However, these methods also demand greater computational resources. One such intervention-based analysis involves estimating the joint causal distribution of variables <ref type="bibr" target="#b21">[22]</ref> and maximizing their likelihood <ref type="bibr" target="#b27">[28]</ref>.</p><p>If the joint distribution of observed data can be factorized explicitly, the intervened SEM M do(ğ¼ ) can be written as:</p><formula xml:id="formula_3">ğ‘ƒ do(ğ¼ ) (ğ‘‹ ) = ğ‘– âˆˆğ¼ ğ‘ƒ (ğ‘‹ ğ‘– |ğ‘ƒğ‘ ğ´ ğ‘ ğ‘– ) ğ‘‹ ğ¼ =ğ‘¥ ğ¼<label>(4)</label></formula><p>where ğ‘ƒğ‘ ğ´ ğ‘ ğ‘– is the parent nodes of node ğ‘– in graph ğ´ ğ‘ , ğ‘‹ ğ¼ is the intervened variable.</p><p>In the Bayesian scheme, the distribution of the potential causal graphs can be denoted by ğ‘ƒ (ğ´ ğ‘ ), ğ´ ğ‘ âˆˆ A. With the observed graph data, we can represent the prior of the causal graph's parameters as ğ‘ƒ (ğœƒ ğ‘ |ğ´ ğ‘ ), ğœƒ ğ‘ âˆˆ Î˜ ğ‘ . Then the prior belief is written as ğ‘ƒ (ğ´ ğ‘ , ğœƒ ğ‘ ) = ğ‘ƒ (ğœƒ ğ‘ |ğ´ ğ‘ )ğ‘ƒ (ğ´ ğ‘ ). We define the likelihood of the node features as ğ‘ƒ (ğ‘‹ |ğœƒ ğ‘ , ğ´ ğ‘ ). The marginal likelihood is calculated by:</p><formula xml:id="formula_4">ğ‘ƒ (ğ‘‹ |ğ´ ğ‘ ) = âˆ« Î˜ ğ‘ ğ‘ƒ (ğ‘‹ |ğœƒ ğ‘ , ğ´ ğ‘ )ğ‘ƒ (ğœƒ ğ‘ |ğ´ ğ‘ )ğ‘‘ğœƒ ğ‘ .<label>(5)</label></formula><p>Then, it can be estimated by the Monte Carlo algorithm according to the specific model via the assumptions in the specific tasks. Heterophilic graph. Generally, heterophilic graph refers to the edge heterophily <ref type="bibr" target="#b6">[7]</ref>, where the edges often link nodes with different labels. We can use homophily ratio <ref type="bibr" target="#b42">[43]</ref> as the metric. Given a graph ğº (V, E), its homophily ratio is ğ‘… â„ = |{(ğ‘¢, ğ‘£) âˆˆ E : ğ‘¦ ğ‘¢ = ğ‘¦ ğ‘£ }|/|E |, where V is the node set, E is the edge set, ğ‘¦ ğ‘¢ , ğ‘¦ ğ‘£ are the labels of node ğ‘¢, ğ‘£ respectively. ğ‘… â„ ranges from 0 to 1. A value close to 1 implies strong homophily, while a value close to 0 indicates strong heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CAUSAL INFERENCE ON HETEROPHILY</head><p>The edges in the graph serve as indicators of association among the nodes. They can be treated as prior knowledge or skeletons for causal analysis. By optimizing the graph structure to align with the causal structure, we detect the heterophily and enhance the information-gathering of GNNs. This, in turn, aids GNNs in developing a better understanding of the node relationships during link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Information aggregation in GNN</head><p>In this Section, we start by forming the causal inference of the node dependency problem in GNNs. We present the assumptions regarding the properties of causality in the context of GNNs. Given a graph ğº (ğ‘‹, ğ´) with ğ‘ nodes, the features on each node ğ‘‹ ğ‘– âˆˆ R ğ· , ğ‘– âˆˆ [ğ‘ ] are considered as a group of variables. The adjacency matrix ğ´ âˆˆ R ğ‘ Ã—ğ‘ is the initialization of causal structure. A graph convolution layer can be denoted by:</p><formula xml:id="formula_5">X = AGG(ğ´, ğ‘‹ ) = ğœ (ğ‘Š ğ´ğ‘‹ + ğ‘) â‰œ ğœ (Î“ğ‘‹ + ğ‘) ,<label>(6)</label></formula><p>where AGG(â€¢) is the aggregation function, ğ‘Š , ğ‘ are the weight matrix and bias vector of the GNN layer, ğœ (â€¢) is the activation function, and Î“ is the connection coefficient matrix of the layer. Î“ refers to a quantification of the cause-effect, where Î“ ğ‘—ğ‘˜ = 0 if</p><formula xml:id="formula_6">ğ‘‹ ğ‘˜ âˆ‰ ğ‘ƒğ‘ ğ´ ğ‘ (ğ‘‹ ğ‘— )</formula><p>, where ğ‘ƒğ‘ ğ´ ğ‘ (â€¢) is the parents set of the given node in adjacency matrix ğ´ ğ‘ .</p><p>Local Markov property is a commonly applied assumption in the causal structure learning <ref type="bibr" target="#b22">[23]</ref>. It enables the implied conditional independencies being read off from a given causal structure <ref type="bibr" target="#b9">[10]</ref>. While, enumerate all the conditions in GNN is NP-hard problem.</p><p>Thus, we focus on the most primary cause-effect relationships in each convolution layer. For a node variable ğ‘‹ ğ‘– , we only consider the 1-hot neighbors and ignore the spouse nodes in Markov blanket, which is a common simplification in GNN. Specifically, we assume it is conditional independent of the rest of its neighbors. Then ğ‘‹ ğ‘– âŠ¥ ğ‘‹ \ğ‘–\ğ‘‹ (ğ‘– ) |ğ‘‹ N (ğ‘– ) , where N (ğ‘–) are the neighbor nodes of node ğ‘‹ ğ‘– in the input graph.</p><p>In local perspective of GNN, a center node ğ‘‹ ğ‘– , denoted as ğ‘Œ following, acquires information from parent nodes ğ‘‹ ğ‘— âˆˆ ğ‘ƒğ‘ ğ´ (ğ‘‹ ). Causal structure estimation is to identify its cause, represented by Î“ ğ‘– ğ‘— â‰  0. Although ground truth of causal structure is often unfeasible in practice, we can still learn invariant causal connections of center node, which possesses optimal generalization capabilities. Subsequently, we can make following assumption to provide an explicit definition of causality for node variables of the input graph.</p><formula xml:id="formula_7">Assumption 3.1. Aggregation Invariant. For the current ob- served causal connections Î“ = [Î“ 1 , ..., Î“ ğ‘ ] âˆˆ R ğ· Ã—ğ‘ , there exists a optimal subset ğ‘† * = ğ‘˜ : Î“ ğ‘˜ â‰  0 âŠ† {1, ..., ğ‘ }, that satisfies in any context ğœ‰ âˆˆ Î ğ‘‹ ğœ‰ = â„ ğ‘¥ (ğ¶ ğœ‰ ğ‘“ , ğœ‚) + ğ¸ ğœ‰ ğ‘¥ ğ‘Œ ğœ‰ = â„ ğ‘¦ (Î“ğ‘‹ ğœ‰ , ğ¶ ğœ‰ ğ‘“ , ğœ–) + ğ¸ ğœ‰ ğ‘¦ ,<label>(7)</label></formula><p>where ğ‘‹, ğ‘Œ are the node feature variables, ğ¶ ğ‘“ is the confounder, â„ ğ‘¥ (â€¢), â„ ğ‘¦ (â€¢) are the effect functions of the confounder on ğ‘‹, ğ‘Œ , and ğœ‚, ğœ– are the random noise with mean of 0, ğ¸ ğ‘¥ , ğ¸ ğ‘¦ are the random bias vectors. Here, ğ¶ ğ‘“ , ğ¸ ğ‘¥ , ğ¸ ğ‘¦ are jointly independent.</p><p>The aforementioned assumption implies the conditional distribution ğ‘Œ ğœ‰ |ğ‘‹ ğœ‰ ğ‘† * , ğ¶ ğœ‰ ğ‘“ for any given context, where each context corresponds to an intervention-based independence experiment. By making this assumption, we consider the existence of common knowledge that holds true across contexts, representing the causal connections. Besides, when applying an embedding model, the assumption guarantees the stability of the learned embeddings, ensuring a consistent joint entropy after graph augmentation. It is important to note that the causal connection ğ‘† * is not necessarily unique, indicating that the learned causal structure of the original graph can be variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Estimation of Intervention-based causality</head><p>In the Bayesian framework, the optimization involves maximizing the margin likelihood as Eq.( <ref type="formula" target="#formula_4">5</ref>). We aim to learn the parameter of the causal structure ğœƒ . The posterior of the final causal structure ğ´ ğ‘ and its parameters are denoted by</p><formula xml:id="formula_8">ğœƒ ğ‘ are ğ‘ƒ (ğ´ ğ‘ |ğ‘‹ ) âˆ ğ‘ƒ (ğ´ ğ‘ )ğ‘ƒ (ğ‘‹ |ğ´ ğ‘ ) and ğ‘ƒ (ğœƒ ğ‘ |ğ‘‹, ğ´ ğ‘ ) âˆ ğ‘ƒ (ğœƒ ğ‘ |ğ´ ğ‘ )ğ‘ƒ (ğ‘‹ |ğœƒ ğ‘ , ğ´ ğ‘ ).</formula><p>They can be estimated by ğ‘ƒ (ğ´|ğ‘‹ ), ğ‘ƒ (ğœƒ ğ‘ |ğ‘‹, ğ´) respectively after initialization. To approximate the optimization, we conduct Bayesian experimental design (BED) <ref type="bibr" target="#b29">[30]</ref> to model the node dependency in the form of entropy. Then utilize Monte Carlo estimator <ref type="bibr" target="#b29">[30]</ref> for the ğ‘‘ğ‘œ-intervention to quantify the point-wise causal relationship. The optimization problem is formalized as the following proposition. Proposition 3.2. Given the intervention strategy ğ¼ âˆˆ I, if we have the condition distribution ğ‘ƒ (ğ‘‹ -ğ¼ , ğ´|ğ‘‹ ), ğ‘ƒ (ğ´|ğ‘‹ ), the causal structure ğ´ ğ‘ and corresponding optimal intervention target ğ‘¥ ğ¼ can be obtained by optimizing:</p><formula xml:id="formula_9">ğ‘¥ ğ¼ , ğ´ ğ‘ = arg max ğ‘¥ ğ¼ ,ğ´ E ğ‘ƒ (ğ‘‹ -ğ¼ |ğ‘‘ğ‘œ (ğ‘‹ ğ¼ =ğ‘¥ ğ¼ ) ) [H(ğ‘ƒ (ğ‘‹ -ğ¼ , ğ´|ğ‘‹ ))-H(ğ‘ƒ (ğ´|ğ‘‹ ))] (8)</formula><p>where ğ‘‹ ğ¼ are the intervened node features, ğ‘‹ -ğ¼ are the non-intervened features, H(â€¢) is the entropy.</p><p>To approximate the optimization, the Bayesian experimental design (BED) approach can be employed, as described in <ref type="bibr" target="#b29">[30]</ref>. BED utilizes the uncertainties associated with the causal structure, which are captured by the posterior distribution. As such, we have the following proposition: Proposition 3.3. The optimization problem for the causal inference of node dependency in a graph ğº (ğ‘‹, ğ´) on GNN-based embedding networks ğ‘“ : ğº â†’ ğµ can be formulated as:</p><formula xml:id="formula_10">ğœ‰ * , ğœƒ ğ‘ = arg max ğœ‰,ğœƒ âˆ« ğ‘ˆ (ğµ|ğœ‰)ğ‘ƒ (ğµ|ğœ‰)ğ‘‘ğµ, ğ‘ˆ (ğµ|ğœ‰) = âˆ« Î˜ ğ‘ƒ (ğœƒ |ğµ, ğœ‰) log ğ‘ƒ (ğœƒ |ğµ, ğœ‰)ğ‘‘ğœƒ - âˆ« Î˜ ğ‘ƒ (ğœƒ ) log ğ‘ƒ (ğœƒ )ğ‘‘ğœƒ,<label>(9)</label></formula><p>where ğœ‰ * is the optimal intervention experiment, ğœƒ ğ‘ âˆˆ Î˜ is the parameter of the causal structure with a prior ğ‘ƒ (ğœƒ ), ğ‘ƒ (ğµ|ğœ‰) is the posterior of embeddings given the experiment ğœ‰, and ğ‘ˆ (ğµ|ğœ‰) is the utility function that quantifies the information gain of the causal structure.</p><p>According to the proposition, the learning of the causal structure can be achieved by maximizing the usefulness metric ğ‘ˆ (ğµ|ğœ‰). And the optimization process simultaneously improves the posterior probability ğ‘ƒ (ğµ|ğœ‰).</p><p>In the graph data, we transfer the optimization problem into Eq.( <ref type="formula" target="#formula_10">9</ref>). To proceed with this optimization, we require an estimation of the utility function based on the definition of causality. In the point-wise relationship, a conditional independence test can be applied to discover causality after intervention. Each noisy imputation is an independence experiment ğœ‰, and the utility function becomes ğ‘ˆ (ğ‘‹ -ğ¼ |ğ‘‘ğ‘œ (ğ‘‹ ğ¼ = ğ‘¥ ğ¼ )). By leveraging the Monte Carlo estimator <ref type="bibr" target="#b29">[30]</ref> and incorporating Assumption 3.1, we can estimate Eq.( <ref type="formula" target="#formula_10">9</ref>) by:</p><formula xml:id="formula_11">ğ‘¥ ğ¼ , ğ´ ğ‘ = ğ‘ğ‘Ÿğ‘” max ğ‘¥ ğ¼ ,ğ´ âˆ‘ï¸ ğ´âˆˆ A [ğ‘ƒ (ğ´, ğ‘‹ -ğ¼ ) log ğ‘ƒ (ğ´|ğ‘‹ -ğ¼ , ğ‘‘ğ‘œ (ğ‘‹ ğ¼ = ğ‘¥ ğ¼ ))].<label>(10)</label></formula><p>We can express Eq.( <ref type="formula" target="#formula_11">10</ref>) in terms of entropy, where we have the expression as Eq.( <ref type="formula">8</ref>).</p><p>To solve the optimization problem above, we introduce two penalties related to the terms in Eq.( <ref type="formula">8</ref>):</p><p>In the first step, for the entropy in the first term, we aim to maximize it to enhance the information gain and improve the description of causality in the inferred causal graph. This entropy can be calculated by considering the entropy of the neighbors conditioned on the intervened nodes, as expressed in Eq.(4). To optimize this term, we propose a causality estimator as follows.</p><p>Definition 3.4. Given a intervention ğ¼ âˆˆ I for the graph ğº (ğ‘‹, ğ´), the cause-effect significance of a center node ğ‘‹ ğ‘– to its neighbors ğ‘‹ ğ‘– â†’ ğ‘‹ ğ‘— in GNNs can be measured by:</p><formula xml:id="formula_12">ğ›¿H(ğ‘–, ğ‘—) = H ğ‘‹ ğ‘— |ğ‘‹ ğ‘– -H ğ‘‹ ğ‘– |ğ‘‹ ğ‘— , ğ‘– âˆˆ ğ¼, ğ‘— âˆˆ ğ‘ƒğ‘ ğ´ ğ‘– ,<label>(11)</label></formula><p>where The node pair (ğ‘–, ğ‘—) a high value of ğ›¿ (ğ‘–, ğ‘—) implies prominent cause-effect relationships in the graph, and also a high probability of heterophily. Specifically, ğ‘‹ ğ‘— is a cause of ğ‘‹ ğ‘– if it is positive inside the absolute-value sign. These edges are transformed into directed edges, while the opposite direction, represented by ğ‘‹ ğ‘— â†’ ğ‘‹ ğ‘– in the adjacency matrix, is set to zero. It is a heuristic with a greedy strategy to learn causal structure iteratively, during which the value of the first term in Eq.( <ref type="formula">8</ref>), which is equivalent to maximizing ATE defined in Eq.( <ref type="formula" target="#formula_2">3</ref>).</p><formula xml:id="formula_13">H(â€¢|â€¢) is the conditional entropy. ğ»(ğ‘¥ ! |ğ’© ! " , ğ’© ! # ) ğ»(ğ’© ! " |ğ‘¥ ! ) ğ»(ğ’© ! # |ğ‘¥ ! ) ğ¼(ğ‘¥ ! , ğ’© ! " ) ğ¼(ğ‘¥ ! , ğ’© ! # )<label>(</label></formula><p>In the second step, we seek to minimize the entropy of the second term of Eq.( <ref type="formula">8</ref>). In link prediction, a low entropy of the adjacency matrix indicates stability in the node embedding space. During the training of GNNs, we incorporate a distance penalty in the loss function to ensure consistency. This penalty ensures that the augmentation introduced by the causal structure does not disrupt the node representation, maintaining stability in the learned representations.</p><p>When the two terms in Eq.( <ref type="formula">8</ref>) are optimized simultaneously, the model can approach the optimal indicated by Eq.(9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Insight into heterophily</head><p>We construct a criterion for heterophilic links based on dependency estimated by their conditional entropy. Additionally, we provide insight into how conditional entropy quantifies heterophily. As a first step, we assume that the connections between node pairs are label-dependent. The expectation of a node ğ‘¥ ğ‘– connecting to a heterophilic neighbor is given by the Rayleigh quotient of the label, where the expected value is denoted as E[ğ‘ƒ + ğ‘– ] = 1 -ğ‘… â„ <ref type="bibr" target="#b12">[13]</ref>. If we adopt a GNN as a node embedding model ğ‘“ , it learns the conditional probability distribution that relates each node to the context ğ‘¥ N ğ‘– |ğ‘¥ ğ‘– , where the neighbors can be divided into homophilic ones and heterophilic ones according to their labels N ğ‘– = N + ğ‘– , N - ğ‘– . Then the conditional entropy is described as:</p><formula xml:id="formula_14">ğ» (N - ğ‘– |ğ‘¥ ğ‘– ) + ğ» (ğ‘¥ ğ‘– |N - ğ‘– ) = ğ» (ğ‘¥ ğ‘– , N + ğ‘– , N - ğ‘– ) -ğ» (N + ğ‘– |ğ‘¥ ğ‘– ) -ğ¼ (ğ‘¥ ğ‘– , N - ğ‘– )<label>(12)</label></formula><p>In heterophilic graphs, the lower bound of mutual information between heterophilic node pairs is negatively correlated with ğ‘ƒ + ğ‘– <ref type="bibr" target="#b19">[20]</ref>. This suggests that a heterophilic link refers to less mutual information, specifically a smaller ğ¼ (ğ‘¥ ğ‘– , N - ğ‘– ). Furthermore, the joint entropy ğ» (ğ‘¥ ğ‘– , N + ğ‘– , N - ğ‘– ) and the conditional entropy on the homophilic pair ğ» (N + ğ‘– |ğ‘¥ ğ‘– ) remain the same due to the static embedding model. Then, the conditional entropy on the left-hand side of the equation becomes larger.</p><p>The conditional entropy measures how well it can predict the heterophilic neighbors. When comparing the conditional entropy of a heterophilic pair to that of a homophilic pair, two situations arise. If the node pairs are less dependent on each other, both of the conditional entropy terms on the right-hand side of Eq.12 increase, resulting in a small difference between them, as Fig. <ref type="figure" target="#fig_2">2(b</ref>). However, if there exists a symmetric dependency relationship, only one of the terms increases, leading to a large difference between ğ» (N - ğ‘– |ğ‘¥ ğ‘– ) and ğ» (ğ‘¥ ğ‘– |N - ğ‘– ), as Fig. <ref type="figure" target="#fig_2">2(c</ref>). In the case of homophilic node pairs, the difference in conditional entropy between them remains small due to a similar representation distribution under label-dependent connectivity. Consequently, a large difference in conditional entropy suggests heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED MODEL</head><p>Based on the analysis in the previous section, we present the CausalMP to tackle the heterophily in graphs on the link prediction task. The main architecture is shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main modules</head><p>1. Intervention on graph data. To learn the dependency among the nodes from the observational data, we employ a node feature intervention approach, where Gaussian noise rescales the node features ğ‘‹ . ğ‘Ÿ ğ‘ ğ‘ nodes are rescaled by Gaussian noise, where ğ‘Ÿ ğ‘ is ratio of intervened nodes. According to the <ref type="bibr" target="#b23">[24]</ref>, it is sufficient to identify the causal connections under the noise intervention. The graph after the intervention is denoted by ğº I ( X, ğ´), where X is the intervened feature matrix.</p><p>2. Embedding learning. To identify node dependencies and capture causal-effect relationships, we conduct an independence test on the distribution of observational data following the intervention. To mitigate sensitivity to downstream task correlations, we employ an unsupervised GNN denoted as ğ‘“ : ğº â†’ ğµ âˆˆ R ğ‘ Ã—ğ· emb , where ğ· emb represents the dimension of the node embedding. Unsupervised learning enables the representations to capture the underlying causality among the nodes instead of correlation with the output.</p><p>3. Node dependency estimation and causal structure modification. We perform ğ‘€ intervention experiments on the original graph. The trained embedding network ğ‘“ maps the intervened graphs {ğº I [ğ‘š]}, ğ‘š âˆˆ [ğ‘€] to the embedding space {ğµ I [ğ‘š]}. Utilizing these ğ‘€ discrete observations, we can quantify the dependency between the center nodes (i.e., intervened nodes) and their neighbors, which is calculated by Eq.( <ref type="formula" target="#formula_12">11</ref>). In the Monte Carlo experiment, we discretize the embeddings into bins and use kernel density estimation (KDE) to estimate the joint probability density function (PDF). By leveraging the KDE estimates, we compute the conditional entropy between the node pairs.</p><p>For each edge, we obtain a corresponding dependency score ğ›¿H(ğ‘–, ğ‘—), where a larger value implies a more prominent causaleffect relationship. For a detected dependency ğ‘‹ ğ‘– â†’ ğ‘‹ ğ‘— , we convert the original undirected edge (ğ‘–, ğ‘—) to directed by setting ğ´( ğ‘—, ğ‘–) to 0. The threshold for pruning is ğœ‡ H +ğœ† 1 ğœ H . Here, ğœ† 1 is a coefficient, ğœ‡ H = E ğ‘– âˆˆ I,ğ‘— âˆˆ N (ğ‘– ) [ğ›¿H(ğ‘–, ğ‘—)],ğœ H = Var ğ‘– âˆˆ I,ğ‘— âˆˆ N (ğ‘– ) [ğ›¿H(ğ‘–, ğ‘—)] are mean and variance of the dependency scores.</p><p>Similarly, we can examine the presence of triangular relationships within the graph. We calculate the mutual information (MI) between the node pairs ğ‘€ğ¼ I (ğ‘–, ğ‘—), ğ‘˜ âˆˆ I , ğ‘–, ğ‘— âˆˆ ğ‘ƒğ‘ ğ´ (ğ‘˜). A large MI suggests a potential edge. The edges are added when MI exceeds the threshold ğœ‡ MI + ğœ† 2 ğœ MI , where ğœ‡ MI , ğœ MI represent the mean and variance all the MI scores, ğœ† 2 is a coefficient.</p><p>4. Optimization target. After modifying the graph structure ğ´, we obtain a causal structure ğ´ ğ‘ . As shown in Fig. <ref type="figure">3</ref>, we employ two encoder-decoder frameworks to address the link prediction task for ğ´ and ğ´ ğ‘ separately. We denote the GNN as ğ‘”(ğº) = DE(EN(ğº)), where EN(â€¢), DE(â€¢) are the encoder and decoder respectively. The reconstruction loss L recon is evaluated by the binary cross entropy between output and input logits. The consistency penalty is quantified by the mean squared error (MSE) as L cons = MSE(EN(ğ´ ğ‘ ), EN(ğ´)). The optimization target is weighted summation of the reconstruction loss of two graphs and consistency penalty L = L recon (ğ´) + ğ›¼ L recon (ğ´ ğ‘ ) + ğ›½L cons , where ğ›¼, ğ›½ are the coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CausalMP</head><p>In graph data, intervention I is applied to a subset of nodes. Additionally, the pruning strategy during node dependency estimation is a greedy approach. To address these limitations, we introduce iterations in the intervention experiments. It ensures that the selected center nodes and their neighbors encompass a significant portion of the nodes in the graph, then more node dependencies can be detected. A detailed algorithm of the proposed CausalMP is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Complexity analysis</head><formula xml:id="formula_15">+ |ğ¸|) = ğ‘‚ (ğ‘ 2 ).</formula><p>Compared to multi-view augmentation in graph learning, the overhead of CausalMP mainly results from the KDE estimation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 5.1 Experiment setup</head><p>To evaluate the improvement over GNNs on message-pathing, we conduct a comparative analysis on link prediction task. We adopt 85% /5%/10% split for training, validation, and testing. The negative link sampling capacity is the same as the positive ones. CausalMP is compared to popular baselines and SOTA models, including GAT <ref type="bibr" target="#b28">[29]</ref>, VGAE <ref type="bibr" target="#b10">[11]</ref>, Graph-InfoClust (GIC) <ref type="bibr" target="#b18">[19]</ref> and Linkless Link Prediction (LLP) <ref type="bibr" target="#b8">[9]</ref>. We also compare CausalMP with two additional models specifically designed for heterophilic graphs, namely LINKX <ref type="bibr" target="#b13">[14]</ref> and DisenLink <ref type="bibr" target="#b40">[41]</ref>, as well as another causality-based model, Counterfactual Link Prediction (CFLP) <ref type="bibr" target="#b38">[39]</ref>.</p><p>For the obtained explicit causal structure, we compared it with original graph in node classification. To amplify the contribution of structural information, we adopt the limited label setting as <ref type="bibr" target="#b25">[26]</ref>. Specifically, we used C-way 5-shot for small graphs and 100-shot for large graphs. We conduct the comparison on baselines models, i.e. GCN, GAT, and SOTA models for heterophilic graphs, i.e. LINKX, and GREET <ref type="bibr" target="#b16">[17]</ref>. As graph prompt learning also shows superiority in few-shot heterophilic node classification setting, we also apply the causal structure to GPPT <ref type="bibr" target="#b24">[25]</ref> and Gprompt <ref type="bibr" target="#b26">[27]</ref>.</p><p>We conduct experiments on 9 commonly used graph datasets, encompassing 4 homophilic graphs and 5 heterophilic graphs. For the implementation of CausalMP, we utilize CCA-SSG <ref type="bibr" target="#b37">[38]</ref> with default setting as the unsupervised embedding learning network. The encoder of the CausalMP is the same size as the embedding network, and we employ an MLP decoder both tasks. The details of datasets and experiment settings are shown in Appendix A. The link prediction performance is evaluated by AUC(%) and node classification by accuracy (%). We report the the average and variance of results for 5 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result</head><p>The results of link prediction are summarized in Table <ref type="table" target="#tab_1">1</ref>, where the bolded and underlined entries represent the best and second-best performance, respectively. The term "OoM" refers to out-of-memory issues of the device. Our observations are as follows: (a) The prevalent models and SOTA link prediction models (GIC, LLP) generally exhibit satisfactory and stable performance on homophilic graphs. However, they are unable to perform well and stably on heterophilic graphs. (b) Models specifically designed for heterophily (LINKX, DisenLink) sacrifice their superiority on homophilic graphs, indicating a trade-off in performance depending on the graph type. (c) CFLP demonstrates the applicability of causal analysis in heterophilic scenarios. However, it suffers from computational complexity. (d) The proposed CausalMP exhibits stable performance and outperforms the benchmarks on both homophilic and heterophilic graphs. Notably, it remains applicable even on large graphs (CS, Physics), showcasing its scalability.</p><p>The results of node classification with causal structure are presented in Table <ref type="table" target="#tab_2">2</ref>. It demonstrates that the models perform better when trained with the causal structure, both for homophilic and heterophilic graphs. It indicates that the improved message-passing facilitated by the causal structure improves node representation learning by GNNs. To provide insight into the node classification experiment, we vary the number of shots and the performance of the GCN is shown in Fig. <ref type="figure" target="#fig_4">4</ref>, which demonstrate that the learned causal structure contributes more when there is less available node information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Coefficients of the loss function.</head><p>In order to assess the impact of the coefficients ğ›¼ and ğ›½ in the optimization target, we conducted an ablation experiment, where we modified the loss function as:    where ğ›¼ represents the weight of the reconstruction loss, and ğ›½ represents the weight of the consistency between the representation of the original graph and the causal structure.</p><formula xml:id="formula_16">L = (1 -ğ›¼)L recon (ğ´) + ğ›¼ L recon (ğ´ ğ‘ ) + ğ›½L cons (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Experiment</head><p>Center node ratio. In CausalMP, the sampling ratio of center node ğ‘Ÿ ğ‘ âˆˆ (0, 1] is an important parameter in the intervention strategy. We investigate its impact on two heterophilic graphs (Actor, Cornell) with ğ‘Ÿ ğ‘ âˆˆ [0, 0.5]. We report the results of the performance and corresponding time consumption in Table <ref type="table" target="#tab_3">3</ref>. Our findings indicate that when the iteration number is fixed, larger values of ğ‘Ÿ ğ‘ lead to increased time consumption, particularly on larger graphs. Excessively large values of ğ‘Ÿ ğ‘ can result in a more dramatic modification of the structural information and performance degradation. Conversely, when ğ‘Ÿ ğ‘ is too small, there are not sufficient node dependencies uncovered to improve the message-passing. To strike a balance, we experientially select ğ‘Ÿ ğ‘ from [0.02, 0.1]. Optimization target weight. We first tune ğ›¼ through grid search in [0, 0.25, 0.5, 0.75, 1] with result shown in Fig. <ref type="figure" target="#fig_5">5</ref>. We discovered that assigning a too-large weight to either the original graph or the causal graph is not beneficial for training. Optimal performance is achieved when there is a balance between the two components. Similarly, the impact of ğ›½ is evaluated in Fig. <ref type="figure" target="#fig_6">6</ref>. We observe that as ğ›½ starts to increase from 0, the performance of CausalMP improves. This demonstrates the effectiveness of the consistency term. However, if ğ›½ keeps increasing, the performance starts to degrade. This is because an excessively large weight on the consistency term can interfere with the optimization of the reconstruction loss. While, CausalMP can achieve a stable and satisfying performance as long as extreme values of ğ›¼, ğ›½ are avoided. Thus, we set them as constant across different datasets.</p><p>Edge modification strategy. We conducted a comparative analysis by comparing several settings: Edge_Aug replaces the causal   Case study To gain a deeper understanding of how causal inference contributes to heterophilic graph learning, we conduct a case study on Texas to visualize the details. Texas is a highly heterophilic graph with 289 heterophilic edges and 36 homophilic edges.</p><p>As first train an unsupervised node embedding network ğ‘“ in CausalMP, we here calculate node dependency metric by Eq.11 on all the edges. We firstly only train the node embedding network ğ‘“ and calculate the node dependency metric by Eq.( <ref type="formula" target="#formula_12">11</ref>) on all the edges. The score on the homophilic edges is ğ›¿ 0 = 0.1534 Â± 0.1388, while ğ›¿ 1 = 0.2434 Â± 0.1894 on the heterophilic edges. We have 99.9% confidence that the mean values of ğ›¿ 0 and ğ›¿ 1 are different by Z-test.</p><p>Then, in the 5 iterations of causal structure learning, the numbers of edges transferred to directed edges are <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> of them are heterophilic edges, showing that ğ›¿ defined in Eq.( <ref type="formula" target="#formula_12">11</ref>) is an effective indicator for heterophily. ğ‘ ), the AUC becomes 83.98%, while 81.82% on the original graph ğº (ğ‘‹, ğ´), ğº (ğ‘‹, ğ´), which shows that the efficacy of message passing is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Heterophilic graph learning. Plenty of GNNs have been proposed to tackle the heterophilic graph. The prevalent models involved in improving the information gathering, such as MixHop <ref type="bibr" target="#b0">[1]</ref> and GPR-GNN <ref type="bibr" target="#b4">[5]</ref>. LINKX <ref type="bibr" target="#b13">[14]</ref> learns node feature and adjacency information separately, then concatenates them for final prediction, trained through simple minibatching. GOAT <ref type="bibr" target="#b11">[12]</ref> adaptive learns the relationship from virtually fully-connected nodes. Another promising approach is to improve the message-passing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. GReTo <ref type="bibr" target="#b41">[42]</ref> performs signed message passing by local context and target information. GOAL <ref type="bibr" target="#b39">[40]</ref> enriches the structural information by graph complementation to homophily-and heterophily-prone topology. ACMP <ref type="bibr" target="#b32">[33]</ref> construct message passing by interacting particle dynamics with a neural ODE solver implemented.</p><p>Causal inference. Traditional statistical causal inference can be categorized into score-based, constraint-based methods, and hybrid methods <ref type="bibr" target="#b21">[22]</ref>. It has primarily been gaining interest in the context of graph classification tasks to guide GNN training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref>, where they focus on identifying invariant substructures rather than dependency analysis. Causal inference is also applied to explanation tasks in GNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. <ref type="bibr" target="#b38">[39]</ref> estimates a counterfactual adjacency matrix of the original graph to enrich the link information and improve the GNN learning. <ref type="bibr" target="#b2">[3]</ref> defines the context information as the treatment and conducts augmentation on knowledge graph to improve representation learning and interpretability. <ref type="bibr" target="#b5">[6]</ref> propose a different definition of causality, namely causal lifting, and conduct the graph learning on the knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this study, we propose CausalMP, a novel scheme with causal inference embedded, devised to deal with heterophilic graph learning by GNNs. We conduct a theoretical analysis of intervention-based causality in GNN. We formulate an optimization problem by estimating cause-effect relationships through conditional entropy and we propose an indicator to locate heterophily. CausalMP iteratively transfers the detected dependencies into directed edges and add edges based on mutual information, optimizing GNNs under constrastive scheme. Through extensive experiments on both homophilic and heterophilic graphs, we demonstrate that CausalMP achieves superior link prediction performance than other baselines. And the learned causal structure contributes to node classifications especially in limited label situation.</p><p>In the future, we plan to explore the inclusion of 2-hop conditions in causal analysis, striking a balance between computational complexity and accuracy. Furthermore, we intend to leverage the learned causal structure for explanation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Detect heterophily by causal-effect estimation from asymmetric information flow that results from the mimic behaviors of fraudsters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Venn diagram of center node and its neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: of shot number in node classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of consistency term ğ›¼.</figDesc><graphic coords="8,53.81,84.21,121.27,82.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Influence of consistency term ğ›½.</figDesc><graphic coords="8,53.81,206.40,121.27,82.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Finally,</head><label></label><figDesc>We learn a causal structure ğ´ (ğ‘‡ ) ğ‘ of the original graph ğ´. If we re-initialize everything and pre-train a new CausalMP on ğº (ğ‘‹, ğ´ (ğ‘‡ ) ğ‘ ), ğº (ğ‘‹, ğ´ (ğ‘‡ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Embedding model ğ‘“ , link prediction model ğ‘”, causal structure of input graph ğ´ ğ‘ . Randomly initialize all the GNN models; Pre-train ğ‘” on ğº (ğ‘‹, ğ´), ğº (ğ‘‹, ğ´ Main scheme of CausalMP. In each iteration, we modify the detected dependencies into directed edge (red) and add edges (yellow) through mutual information. Both graphs are encoded and decoded by GNN with shared parameter that optimized by the weighed summation of three losses. number, which is ğ‘‚ (|ğ‘‰ |). Similarly, the number of edges involved in the dependency test would be ğ‘‚ (|ğ¸|). Then the time complexity of KDE would be ğ‘‚ (|ğ¸| â€¢ ğ‘€ 2 ). For the training of the embedding network and the encoder of the link prediction model, assuming no additional tricks or computations, the time complexity would be approximately ğ‘‚ (ğ‘ layer ğ‘‘ 2 hidden |ğ‘‰ | + ğ‘ layer ğ‘‘ hidden |ğ¸|), where ğ‘ layer is the layer number and ğ‘‘ hidden is dimension of hidden layer. If we employ inner-product for the decoder, it would take ğ‘‚ (|ğ‘‰ | 2 ), while ğ‘‚ (ğ‘ layer ğ‘‘ hidden |ğ¸|) for MLP decoder. Thus, the overall time complexity of CausalMP would not exceed ğ‘‚ (|ğ‘‰ | 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss</cell></row><row><cell></cell><cell></cell><cell>GNN Encoder</cell><cell></cell><cell>Decoder</cell><cell>Reconstruction loss of original graph</cell></row><row><cell>Causal dependency</cell><cell>Original graph</cell><cell cols="2">shared</cell><cell>shared</cell><cell>Embedding</cell></row><row><cell>learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>consistency</cell></row><row><cell></cell><cell></cell><cell>GNN Encoder</cell><cell>Representation</cell><cell>Decoder</cell><cell>Reconstruction loss of causal structure</cell></row><row><cell></cell><cell>Causal structure</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Initialize the causal structure ğ´ ğ‘ = ğ´; (0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Train the unsupervised embedding model ğ‘“ on ğº;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(0) ğ‘ );</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">for Iteration ğ‘¡ = 0 to ğ‘¡ = ğ‘‡ -1 do</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Random sample ğ‘ ğ¼ nodes;</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Impute noise intervention I on ğ´ ğ‘ by ğ‘€ times; (ğ‘¡ )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Map the intervened graphs to the embedding space by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ğ‘“ : ğ´ ğ‘ [ğ‘€] â†’ ğµ (ğ‘¡ ) [ğ‘€]; (ğ‘¡ )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Estimate the PDF of node embeddings ğ‘ƒ (ğµ ğ‘¡ ) by KDE;</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Determine directed edges by ğ›¿ threshold in Eq.(11);</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Determine added edges by MI threshold;</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Update causal structure ğ´ ğ‘ to ğ´ (ğ‘¡ ) ğ‘ (ğ‘¡ +1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ğ‘¡ +1) ğ‘</cell><cell>);</cell></row><row><cell></cell><cell></cell><cell></cell><cell>end</cell><cell></cell></row></table><note><p>Consider a graph with ğ‘ = |ğ‘‰ | nodes and |ğ¸| edges. During the intervention, the number of center nodes ğ¾ is proportional to the node Algorithm 1: CausalMP: Causal message-passing for heterophilic graph learning. Input: Graph ğº (ğ‘‹, ğ´), intervention strategy I, number intervention experiment ğ‘€, iteration of intervention experiment ğ‘‡ . Output: ; Optimize ğ‘” on ğº (ğ‘‹, ğ´), ğº (ğ‘‹, ğ´</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on link prediction (AUC/%)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Homophilic</cell><cell></cell><cell></cell><cell></cell><cell>Heterophilic</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>CS</cell><cell>Physics</cell><cell>Actor</cell><cell>Cornell</cell><cell>Texas</cell><cell>Chameleon</cell><cell>Squirrel</cell></row><row><cell>GAT</cell><cell cols="4">95.14Â±0.57 96.22Â±0.47 98.53Â±0.09 97.69Â±0.09</cell><cell cols="2">67.80Â±1.12 61.13Â±3.23</cell><cell>65.73Â±5.06</cell><cell>97.82Â±1.13</cell><cell>97.03Â±0.16</cell></row><row><cell>VGAE</cell><cell cols="4">94.78Â±0.69 95.50Â±0.32 96.65Â±0.14 94.87Â±0.11</cell><cell cols="3">70.82Â±0.81 58.18Â±9.47 66.75Â±10.09</cell><cell>98.18Â±0.22</cell><cell>96.59Â±0.24</cell></row><row><cell>GIC</cell><cell cols="2">96.17Â±0.45 97.12Â±0.24</cell><cell>OoM</cell><cell>OoM</cell><cell cols="2">70.29Â±0.29 58.01Â±3.41</cell><cell>66.19Â±7.32</cell><cell>95.30Â±0.29</cell><cell>95.00Â±0.23</cell></row><row><cell>LLP</cell><cell cols="6">95.29Â±0.19 95.14Â±0.36 97.48Â±0.26 98.79Â±0.05 80.37Â±1.07 68.20Â±7.96</cell><cell>71.88Â±3.95</cell><cell>97.52Â±0.37</cell><cell>95.13Â±0.44</cell></row><row><cell>LINKX</cell><cell cols="4">88.38Â±0.36 88.74Â±0.86 93.28Â±0.16 93.58Â±0.38</cell><cell cols="2">72.13Â±1.04 59.43Â±4.17</cell><cell>71.92Â±3.82</cell><cell>97.77Â±0.31</cell><cell>97.76Â±0.13</cell></row><row><cell cols="3">DisenLink 89.30Â±0.59 93.96Â±0.88</cell><cell>OoM</cell><cell>OoM</cell><cell cols="2">59.19Â±0.48 60.71Â±5.10</cell><cell>77.88Â±4.03</cell><cell>98.49Â±0.08</cell><cell>95.88Â±0.10</cell></row><row><cell>CFLP</cell><cell cols="2">93.44Â±0.82 93.82Â±0.56</cell><cell>OoM</cell><cell>OoM</cell><cell cols="2">80.41Â±0.32 73.14Â±5.42</cell><cell>66.02Â±3.84</cell><cell>98.29Â±0.16</cell><cell>98.39Â±0.04</cell></row><row><cell cols="9">CausalMP 96.84Â±0.43 97.20Â±0.43 98.81Â±0.03 98.18Â±0.02 86.81Â±0.55 73.59Â±5.38 79.26Â±5.38 99.03Â±0.13</cell><cell>98.11Â±0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The contribution of causal structure in node classification</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Heterophilic</cell><cell></cell><cell cols="2">Homophilic</cell></row><row><cell></cell><cell></cell><cell>Cornell</cell><cell>Texas</cell><cell>Chameron</cell><cell>Squirrel</cell><cell>Cora</cell><cell>CiteSeer</cell></row><row><cell>GCN</cell><cell cols="7">Original CausalMP 48.25Â±1.12 56.96Â±1.51 43.35Â±1.16 34.56Â±1.38 79.15Â±1.16 72.72Â±1.25 46.07Â±2.26 55.49Â±2.28 41.59Â±1.96 32.14Â±1.91 77.87Â±1.48 71.14Â±1.19</cell></row><row><cell>GAT</cell><cell cols="7">Original CausalMP 47.35Â±2.52 58.11Â±1.14 42.60Â±1.86 35.01Â±1.33 78.25Â±2.81 71.14Â±1.40 45.34Â±2.15 55.71Â±3.10 39.17Â±1.38 33.58Â±0.82 75.41Â±4.88 69.46Â±2.92</cell></row><row><cell>GREET</cell><cell cols="7">Original CausalMP 62.13Â±3.77 68.23Â±2.90 55.13Â±1.65 38.47Â±1.28 82.20Â±0.97 72.94Â±0.63 57.96Â±4.51 64.56Â±3.46 52.56Â±2.10 36.64Â±1.00 80.32Â±0.86 71.74Â±0.93</cell></row><row><cell>LINKX</cell><cell cols="7">Original CausalMP 59.09Â±3.63 65.76Â±3.27 57.36Â±0.60 39.21Â±0.46 83.44Â±1.58 72.92Â±1.20 56.40Â±4.19 62.08Â±4.72 55.89Â±1.02 37.59Â±0.92 81.90Â±1.43 71.87Â±1.39</cell></row><row><cell>GPPT</cell><cell cols="7">Original CausalMP 57.28Â±3.20 66.78Â±2.90 56.98Â±1.11 37.71Â±0.35 78.16Â±0.84 68.62Â±1.15 54.96Â±2.65 64.56Â±3.88 54.49Â±1.36 36.16Â±0.98 76.79 Â±0.92 66.56Â±1.71</cell></row><row><cell>Gprompt</cell><cell cols="7">Original CausalMP 59.14Â±2.42 52.78Â±2.17 57.23Â±1.02 39.78Â±0.91 78.85Â±0.50 72.69Â±1.78 55.62Â±1.42 49.78Â±3.45 55.17Â±1.41 37.14Â±0.80 77.35 Â±0.75 70.35Â±1.22</cell></row><row><cell cols="2">IMP(%)</cell><cell>2.82</cell><cell>2.74</cell><cell>2.30</cell><cell>1.92</cell><cell>1.74</cell><cell>1.65</cell></row><row><cell cols="2">Training ratio (%)</cell><cell>20</cell><cell>27</cell><cell>44</cell><cell>10</cell><cell>26</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The influence of the center node ratio</figDesc><table><row><cell></cell><cell>Cornell</cell><cell></cell><cell></cell><cell>Actor</cell><cell></cell></row><row><cell>Rc</cell><cell>AUC(%)</cell><cell cols="2">Time(s) Rc</cell><cell>AUC(%)</cell><cell>Time(s)</cell></row><row><cell cols="2">0.05 79.14Â±5.38</cell><cell>241</cell><cell>0</cell><cell>81.74Â±0.54</cell><cell>173</cell></row><row><cell>0.1</cell><cell>79.26+5.38</cell><cell>247</cell><cell cols="2">0.02 87.02Â±0.44</cell><cell>658</cell></row><row><cell>0.2</cell><cell>79.81Â±5.81</cell><cell>240</cell><cell cols="2">0.05 87.03Â±0.44</cell><cell>806</cell></row><row><cell cols="2">0.3 80.12Â±5.52</cell><cell>249</cell><cell>0.1</cell><cell>86.99Â±0.43</cell><cell>1048</cell></row><row><cell>0.4</cell><cell>80.10Â±6.05</cell><cell>261</cell><cell>0.2</cell><cell>86.98Â±0.41</cell><cell>1513</cell></row><row><cell>0.5</cell><cell>79.55Â±5.40</cell><cell>265</cell><cell>0.3</cell><cell>86.93Â±0.42</cell><cell>1965</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Graph update strategy CausalMP represents the full application of the proposed CausalMP. The link prediction results presented in Table4reveal that both edge addition based on mutual information and node dependency discovery are effective. Combining them into full CausalMP demonstrates superior performance, improving the graph learning of GNNs.</figDesc><table><row><cell>Method</cell><cell cols="4">Edge_Aug CausalMP-MI CausalMP-ğ›¿ CausalMP</cell></row><row><cell>Cora</cell><cell>94.72Â±0.50</cell><cell>96.57Â±0.40</cell><cell>96.47Â±0.38</cell><cell>96.84Â±0.43</cell></row><row><cell>CS</cell><cell>97.96Â±0.02</cell><cell>97.95Â±0.02</cell><cell>98.79Â±0.03</cell><cell>98.81Â±0.03</cell></row><row><cell>Texas</cell><cell>72.29Â±5.90</cell><cell>72.68Â±5.68</cell><cell>72.58Â±5.90</cell><cell>73.59Â±5.83</cell></row><row><cell cols="2">Chameleon 98.76Â±0.10</cell><cell>98.93Â±0.12</cell><cell>98.91Â±0.09</cell><cell>99.03Â±0.13</cell></row><row><cell>Squirrel</cell><cell>97.03Â±0.07</cell><cell>97.72Â±0.13</cell><cell>97.82Â±0.11</cell><cell>98.11Â±0.15</cell></row><row><cell cols="5">structure with an EdgeDrop augmentation. CausalMP-MI disabled</cell></row><row><cell cols="5">the cause-effect detection component, only retaining the adding</cell></row><row><cell cols="5">edges strategy based on mutual information. CausalMP-ğ›¿ disabled</cell></row><row><cell cols="5">the edge addition, only retaining the proposed dependency dis-</cell></row><row><cell cols="2">covery strategy.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is funded by <rs type="funder">National Natural Science Foundation of China</rs> Grant No. <rs type="grantNumber">72371271</rs>, the <rs type="funder">Guangzhou Industrial Information and Intelligent Key Laboratory Project</rs> (No. <rs type="grantNumber">2024A03J0628</rs>), the <rs type="funder">Nansha Key Area Science and Technology Project</rs> (No. <rs type="grantNumber">2023ZD003</rs>), and Project No. <rs type="grantNumber">2021JC02X191</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rtSdNT3">
					<idno type="grant-number">72371271</idno>
				</org>
				<org type="funding" xml:id="_xGQk7Zn">
					<idno type="grant-number">2024A03J0628</idno>
				</org>
				<org type="funding" xml:id="_X9ddj8j">
					<idno type="grant-number">2023ZD003</idno>
				</org>
				<org type="funding" xml:id="_RgqdpSU">
					<idno type="grant-number">2021JC02X191</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the baseline models, GIC learns node representation through unsupervised learning by maximizing the mutual information at both the graph level and cluster level. LLP is a relational knowledge distillation framework that matches each anchor node with other context nodes in the graph. LINKX separately embeds the adjacency matrix and node feature with multilayer perceptrons and transformation. CFLP conducts causal analysis at the counterfactual level that estimates the counterfactual adjacency matrix by searching other similar node pairs in the graph under opposite contexts (community), which is a time-consuming process. For the prompt learning based models in node classification, we adopt the same implementation as <ref type="bibr" target="#b25">[26]</ref>. We adopt edge prediction in pretraining GNN with 128 hidden dimensions on small graphs (Cornell, Texas), and SimGRACE <ref type="bibr" target="#b34">[35]</ref> for GNN with 512 hidden dimensions the others.</p><p>In the experiment on node dependency, the ratio of the center node in each iteration grid search within ğ‘Ÿ ğ‘ âˆˆ [0.02, 0.05, 0.1]. We conduct an intervention for ğ‘‡ = 5 iterations, repeating ğ‘€ = 8 times in each iteration to estimate PDF. The parameter of the loss function is set by ğ›¼ = 0.5, ğ›½ = 0.05. The training epoch of the embedding network and CausalMP in each iteration are set at 1000, 2000, and learning rate 1e-4, 1e-5 respectively with a decay of 1e-4.</p><p>The experiment is conducted on NVIDIA GeForce RTX 3090 24G GPU and Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge Graph Completion with Counterfactual Augmentation</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<meeting>the ACM Web Conference 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2611" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wiener graph deconvolutional network improves graph self-supervised learning</title>
		<author>
			<persName><forename type="first">Jiashun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fugee</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7131" to="7139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal Lifting and Link Prediction</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">479</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label-Wise Graph Convolutional Network for Heterophilic Graphs</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning on Graphs Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24934" to="24946" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linkless link prediction via relational distillation</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Shiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12012" to="12033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal structure learning and inference: a selective review</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quality Technology &amp; Quantitative Management</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GOAT: A Global Transformer on Large-scale Graphs</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renkun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bayan Bruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evennet: Ignoring odd-hop neighbors improves robustness of graph neural networks</title>
		<author>
			<persName><forename type="first">Runlin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4694" to="4706" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Orphicx: A causalityinspired latent variable model for interpreting graph neural networks</title>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13729" to="13738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond smoothing: Unsupervised graph representation learning with edge heterophily discriminating</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Vincent Cs Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4516" to="4524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting heterophily for graph neural networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1362" to="1375" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph InfoClust: Maximizing Coarse-Grain Mutual Information in Graphs</title>
		<author>
			<persName><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2021</title>
		<imprint>
			<publisher>Springer Science and Business Media Deutschland GmbH</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="541" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On local aggregation in heterophilic graphs</title>
		<author>
			<persName><forename type="first">Hesham</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somdeb</forename><surname>Majumdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03213</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causal inference. Causality: objectives and assessment</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="39" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using markov blankets for causal structure learning</title>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Pellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AndrÃ©</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology</title>
		<imprint>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gppt: Graph pre-training and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">All in One: Multi-Task Prompting for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3580305.3599256</idno>
		<ptr target="https://doi.org/10.1145/3580305.3599256" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD&apos;23)</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD&apos;23)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2120" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16534</idno>
		<idno>arXiv:2311.16534</idno>
		<title level="m">Graph Prompt Learning: A Comprehensive Survey and Beyond</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Targeted maximum likelihood based causal inference: Part I</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of biostatistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page" from="10" to="48550" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks</title>
		<author>
			<persName><surname>Julius Von KÃ¼gelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03962</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pgm-explainer: Probabilistic graphical model explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">My</forename><forename type="middle">T</forename><surname>Thai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12225" to="12235" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Insights into Noisy Pseudo Labeling on Graph Data</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Botao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fugee</forename><surname>Tsung</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XhNlBvb4XV" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ACMP: Allen-cahn message passing with attractive and repulsive forces for graph neural networks</title>
		<author>
			<persName><forename type="first">Yuelin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Ying-Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12872</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Xiangnan He, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simgrace: A simple framework for graph contrastive learning without data augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on causal inference</title>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From canonical correlation analysis to self-supervised graph neural networks</title>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from Counterfactual Links for Link Prediction</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26911" to="26926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Finding the Missing-half: Graph Complementary Learning for Homophily-prone and Heterophily-prone Graphs</title>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01820</idno>
		<title level="m">Link Prediction on Heterophilic Graphs via Disentangled Representation Learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GReTo: Remedying dynamic graph topology-task discordance via target homophily</title>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gengyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m">A DETAILS ON EXPERIMENT We conduct experiments on 9 commonly used graph datasets, encompassing 4 homophilic graphs, i.e. Cora, CiteSeer, CS, Physics, and 5 heterophilic graphs, i.e. Actor, Cornell, Texas, Chameleon, and Squirrel. The dataset statistics are shown in Table 5</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
