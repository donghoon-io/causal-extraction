<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLEAR: Can Language Models Really Understand Causal Graphs?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-24">24 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sirui</forename><surname>Chen</surname></persName>
							<email>chensirui@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengying</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
							<email>xyzeng@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengjie</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
							<email>luchaochao@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai AI Laboratory</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLEAR: Can Language Models Really Understand Causal Graphs?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-24">24 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.16605v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models' understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models' behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at <ref type="url" target="https://github.com/OpenCausaLab/CLEAR">https: //github.com/OpenCausaLab/CLEAR</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal reasoning is fundamental to how humans understand the world and solve challenges <ref type="bibr" target="#b58">(Sloman and Sloman, 2009)</ref>. The ability to reason causally allows us to explain phenomenon and predict the future <ref type="bibr" target="#b65">(Woodward, 2005;</ref><ref type="bibr" target="#b50">Pearl, 2009;</ref><ref type="bibr" target="#b8">Bunge, 2017)</ref>. There are various causal models used to investigate and represent causation, including mathematical equations, logical statements, and causal graphs <ref type="bibr" target="#b52">(Pearl and Mackenzie, 2018)</ref>. Among them, causal graph gains widespread adoption due to its intuitive and concise representation of complex causal relationships <ref type="bibr" target="#b48">(Pearl, 1995</ref><ref type="bibr" target="#b49">(Pearl, , 1998))</ref>. A causal graph is essentially a bayesian network where each node represents a variable, and the directed edges denote definite or possible causal relationships between variables <ref type="bibr" target="#b28">(Helmert, 2004)</ref>. Understanding causal graphs is essential, as it enables us to grasp the relationships between variables <ref type="bibr" target="#b35">(Kocaoglu et al., 2017)</ref>. Furthermore, causal graphs can be leveraged for probability calculation <ref type="bibr" target="#b34">(Kleinberg, 2013)</ref>, providing solutions to problems across all three rungs of the ladder of causation (i.e., association, intervention, and counterfactuals) <ref type="bibr" target="#b52">(Pearl and Mackenzie, 2018)</ref>. With the rapid advancement of language models, there has been a surge in research exploring their ability to solve graphrelated problems <ref type="bibr">(Zhang et al., 2023b;</ref><ref type="bibr" target="#b13">Chai et al., 2023;</ref><ref type="bibr" target="#b22">Fatemi et al., 2023;</ref><ref type="bibr" target="#b68">Ye et al., 2023;</ref><ref type="bibr">Zhang et al., 2023a;</ref><ref type="bibr" target="#b6">Besta et al., 2024;</ref><ref type="bibr">Chen et al., 2024c;</ref><ref type="bibr">Wang et al., 2024a;</ref><ref type="bibr" target="#b41">Luo et al., 2024)</ref>. In contrast to the abundant research on general graph problems, the ability of language models to understand causal graphs is yet to be investigated. Therefore, this paper aims to shed light on the question: Can language models really understand causal graphs?</p><p>Addressing this question poses three major challenges: (1) What does it mean for a model to understand causal graphs? (2) How to design a causal graph-based benchmark that can measure a model's understanding? (3) How to quantify a model's understanding when presented with causal graphs?</p><p>In this work, we first propose a framework to evaluate language models' understanding of causal graphs, by establishing four criteria: performance exceeding random guesses, robustness against question types, correct utilization of causal definitions, and performance constrained by task dependence. These criteria draw on insights from machine learning, philosophy, and psychology, providing a multidisciplinary approach to evaluating the comprehension of causal graphs by language models. Next, we construct the CLEAR, a novel benchmark created specifically for evaluating how well language models understand causal graphs. Finally, guided by our proposed framework of understanding in causal graphs, we systematically evaluate models' performance on CLEAR across all four criteria. To ensure a diverse evaluation, we select six leading models and utilize four prompts (e.g., in-context learning (IcL) <ref type="bibr" target="#b7">(Brown et al., 2020)</ref>). Our extensive experiments yield the following key findings:</p><p>1. The model's ability to handle different causal graph-based tasks is uneven, exhibiting notable weaknesses in specific areas (Figure <ref type="figure" target="#fig_0">1</ref>). 2. Language models have a preliminary understanding of causal graphs (Figure <ref type="figure">5</ref>), and are observed to focus on key information required to deduce the correct answer (Figure <ref type="figure" target="#fig_8">10</ref>). 3. Model performance is sensitive to the question type. A model's understanding of causal graphs might be artificially inflated if evaluation relies on limited types (Figure <ref type="figure" target="#fig_3">6</ref>). 4. Models exhibit a capacity for utilizing both explicit and implicit concepts related to causal graphs, and their proficiency with these concepts varies considerably (Figure <ref type="figure" target="#fig_5">7</ref>). 5. The performance of most models is not constrained by task dependency (i.e., although Task B depends on Task A, performance on Task B often exceeds that on Task A), showcasing a notable divergence in their performance trends. This might suggest heterogeneity in knowledge representation and applica-tion across different models (Figure <ref type="figure" target="#fig_6">8</ref>). Overall, we make four main contributions: • We make, to the best of our knowledge, the first-ever attempt to evaluate language models' capacity for understanding causal graphs. From the philosophical and psychological perspectives, understanding means: (1) More than just knowing isolated facts. It involves recognizing and grasping the relationships that weave together the various elements of a subject <ref type="bibr" target="#b37">(Kvanvig, 2003;</ref><ref type="bibr" target="#b10">Carter and Gordon, 2014;</ref><ref type="bibr" target="#b24">Grimm, 2021)</ref>. (2) Beyond the formula or definition. It encompasses the ability to not only grasp concepts or formulas but also to adeptly apply them in practical contexts <ref type="bibr" target="#b56">(Rumelhart, 1991;</ref><ref type="bibr" target="#b21">De Regt, 2004)</ref>. (3) Variation in degree. Understanding is not binary, its completeness depends on the individual's conceptual context and background knowledge <ref type="bibr" target="#b46">(Nickerson, 1985)</ref>.</p><p>Considering recent machine learning endeavors, <ref type="bibr" target="#b19">Choudhury et al. (2022)</ref> propose three criteria to assess if a reading comprehension model reaches human-level ability. They focus on whether a model could solve problems correctly, whether it uses information that humans would deem relevant, and whether its performance is consistently robust. Although the three conditions provided in <ref type="bibr" target="#b19">Choudhury et al. (2022)</ref>   (2) Level 2: Intermediate Task. These tasks represent the most common characteristics in causal graphs. Causal graph-based reasoning relies heavily on understanding these fundamental problems. (3) Level 3: Advanced Task. These tasks present complex, high-level challenges that are central to causal graph understanding. Solid arrows indicate the dependencies between tasks within the same level, while dashed arrows represent the tasks' dependencies across different levels. Task dependency design draws on established research <ref type="bibr" target="#b57">(Shpitser and Pearl, 2006;</ref><ref type="bibr" target="#b50">Pearl, 2009;</ref><ref type="bibr" target="#b5">Bareinboim and Pearl, 2012;</ref><ref type="bibr" target="#b51">Pearl et al., 2016;</ref><ref type="bibr" target="#b52">Pearl and Mackenzie, 2018;</ref><ref type="bibr" target="#b31">Jaber et al., 2019)</ref>.</p><p>derstanding, there is still room for improvement. For instance, these conditions fail to offer precise quantitative criteria and lack explicit clarification on what type of information is considered relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exploring Language Models'</head><p>Understanding of Causal Graphs</p><p>Numerous studies have identified understanding as a key factor in the pursuit of human-level artificial intelligence <ref type="bibr" target="#b42">(McCarthy, 2007;</ref><ref type="bibr" target="#b1">Adams et al., 2012;</ref><ref type="bibr" target="#b43">McClelland et al., 2020)</ref>. However, arriving at a definition of understanding within language models is an ongoing challenge. Evaluating models' understanding based on accuracy is currently the dominant approach and certainly essential <ref type="bibr" target="#b3">(Ashwani et al., 2024;</ref><ref type="bibr" target="#b27">He et al., 2024;</ref><ref type="bibr" target="#b67">Xu et al., 2024)</ref>, but this method suffers from inherent limitations. Real-world problems are complex, and data often contains noise <ref type="bibr" target="#b26">(Gupta and Gupta, 2019;</ref><ref type="bibr" target="#b45">Moran et al., 2020;</ref><ref type="bibr" target="#b4">Bansal et al., 2022)</ref>. These make it practically impossible for any model to be perfectly accurate all the time (even humans rarely achieve this) (Valverde-Albacete and Peláez-Moreno, 2014). While it is clear that understanding varies in degree <ref type="bibr" target="#b46">(Nickerson, 1985)</ref>, pinning down a specific threshold is difficult. This difficulty is compounded by the variability in task complexity and the subjective nature of interpreting "levels of understanding". Consequently, rather than define "what constitutes understanding of causal graphs in a language model", we think it might be more productive to consider "if a language model understands causal graphs, how should it behave?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Seeking Understanding of Causal Graphs in Model Behavior</head><p>To measure how well language models understand causal graphs, we develop a three-level evaluation hierarchy comprising 20 meticulously crafted causal graph-based tasks (as Figure <ref type="figure" target="#fig_1">2</ref> illustrates). These tasks include graphs' basic tasks (e.g., cycle), intermediate tasks (e.g., markov equivalent class), and advanced tasks (e.g., causal effect identification). Proficiency in these 20 tasks serves as a valid measure of a model's understanding of causal graphs. Therefore, combining the analyses from Section 2.1 and Section 2.2, we propose that a language model that exhibits understanding would demonstrate the following four behaviors in our tasks. <ref type="foot" target="#foot_0">1</ref> The performance of a model is denoted by P , random guess by P r , the original response of a model by R, and the ground truth by GT .</p><p>B1: Performance exceeding random guesses.</p><p>Existing work suggests that random guess implies a lack of extensive understanding of the given problem <ref type="bibr" target="#b9">(Capraro et al., 2012)</ref>. Moreover, using random guess as baseline is a common and reasonable practice in evaluating model performance <ref type="bibr" target="#b16">(Chen et al., 2023;</ref><ref type="bibr">Wang et al., 2024a;</ref><ref type="bibr">Chen et al., 2024b)</ref>. This behavior can be formulated as P &gt; P r .</p><p>B2: Robustness against question types. Numerous studies highlight that altering the question type or description of a graph, while preserving the original meaning of the problem, can signifi-cantly impact model performance <ref type="bibr" target="#b22">(Fatemi et al., 2023;</ref><ref type="bibr" target="#b30">Hu et al., 2023;</ref><ref type="bibr" target="#b41">Luo et al., 2024)</ref>. Therefore, we suppose that if a model's understanding of a causal graph and its related tasks is genuine, its performance should not be sensitive to superficial changes in the causal graph's question type.</p><p>B3: Correct utilization of causal definitions.</p><p>As De Regt ( <ref type="formula">2004</ref>) emphasizes, understanding implies the ability to utilize given definitions to solve problems. This behavior indicates that the model not only recognizes terms but also understands their meanings and how they relate to the given context. This behavior can be defined as: R ← def. = GT , where R ← def. means a model's response after adding a causal definition to the prompt. The definition can be conveyed either explicitly within the prompt or implicitly through the provision of examples (e.g., IcL) <ref type="bibr" target="#b39">(Li et al., 2022;</ref><ref type="bibr">Zheng et al., 2023;</ref><ref type="bibr" target="#b54">Richens and Everitt, 2024)</ref>.</p><p>B4: Performance constrained by task dependence. Task dependence consistently emerges as a crucial factor in studies focused on understanding <ref type="bibr" target="#b37">(Kvanvig, 2003;</ref><ref type="bibr" target="#b10">Carter and Gordon, 2014;</ref><ref type="bibr" target="#b24">Grimm, 2021)</ref>. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we determine that Task B is dependent on Task A if it requires knowledge acquired from Task A for resolution, whereas solving Task A does not necessitate knowledge from Task B. Mastery of the foundational task is thus deemed essential for succeeding in the dependent task. This performance constraint due to task dependence serves as a critical metric for assessing a model's depth of understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The CLEAR Benchmark</head><p>To explore the question: Can Language modEls reAlly undeRstand causal graphs? we propose CLEAR, the first benchmark dedicated to causal graph understanding. We ensure dataset diversity by accounting for various factors: the size, type, and density of causal graphs, as well as the richness of tasks and question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmark Construction</head><p>Generating random graphs. We begin by randomly creating a set of graphs. A graph is denoted as G = (V, E), where V and E represent set of nodes and edges. To ensure diversity, we cover both general and causal graphs, differentiated by structure into four types: undirected graph, directed graph, directed acyclic graph (DAG), and acyclic</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question types</head><p>Causal graph info Given a DAG (directed acyclic graph) with nodes V, W, L, A and directed edges W→A, V→W, W→L, V→A, L→A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Find all</head><p>Find all backdoor paths from node L to node A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Find one</head><p>Find the shortest backdoor path from node L to node A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How many</head><p>How many backdoor paths are from node L to node A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice selection</head><p>Which of the following is a backdoor path from W to L? A. W→A←L B. W←V→A←L C. W←A&lt;→V→L D. W→L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes or no</head><p>Is W←V→A a backdoor path from W to A?</p><p>Existence Does there exist a backdoor path from L to A?</p><p>Figure <ref type="figure">3</ref>: Six question types. Taking the backdoor path as an example, we design six question types in CLEAR.</p><p>A complete question is formulated by combining the causal graph info with a specific question type.</p><p>directed mixed graph (ADMG) <ref type="bibr" target="#b53">(Peters et al., 2017)</ref>.</p><p>The undirected and directed graphs, typical of general graph types, are employed primarily in basic tasks. Conversely, DAGs and ADMGs, which are causal in nature, are utilized in intermediate and advanced tasks. To control the complexity, we vary the number of nodes (n v ) from 4 to 9 and adjust the number of edges (n e ) from n v -1 to 10 for each n v . These graph types involve three types of edges: undirected edge, directed edge, and bi-directed edge. The undirected edges symbolize reciprocal relationships, while the bi-directed edges suggest the presence of confounding between nodes. For ADMGs that contain both directed and bi-directed edges, we maintain the ratio of bi-directed to directed edges at or below 0.5 to prevent excessive complexity. We denote nodes using letters, and to ensure neutrality and mitigate bias from the model's potential prior knowledge, the alphabetical order of V is randomized.</p><p>Generating causal reasoning questions. Based on the causal graphs, we generate questions with corresponding ground truth for various causal tasks and question types. The questions are produced using predefined templates. Specifically, we design 20 causal tasks and six question types.<ref type="foot" target="#foot_1">foot_1</ref> And as Figure <ref type="figure">3</ref> demonstrates, these question types can be divided into two types of subjective questions (i.e., find all and find one) and four types of objective questions (i.e., how many, yes or no, choice selection, and existence), providing a in-depth evaluation of models' understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Statistics</head><p>Our CLEAR benchmark includes 20 causal tasks, spanning all three complexity levels. We generate 2808 questions in total. For each causal task, we ensure that the number of questions exceeds 100 to support the validity of our experimental conclusions.  <ref type="bibr" target="#b60">(Touvron et al., 2023)</ref>), and limited-access models (GPT-3.5-Turbo (OpenAI, 2022), GPT-4 <ref type="bibr">(Achiam et al., 2023)</ref>, and Gemini Pro <ref type="bibr" target="#b59">(Team et al., 2023)</ref>). They originate from various creators and exhibit a spectrum of model scales. We use the default hyper-parameter settings for all models.</p><p>Prompts. In Section 4.2, 4.3 and 4.5, we employ the basic prompt (i.e., &lt;question&gt;). In Section 4.4, we adopt basic prompt, 1/3-shot IcL <ref type="bibr" target="#b7">(Brown et al., 2020)</ref>, and definition-guided prompt (i.e., &lt;instruction, definition, question&gt;).<ref type="foot" target="#foot_2">foot_2</ref> </p><p>Metrics. The evaluation metric is accuracy. Objective questions are assessed via answer extraction using GPT-4 and exact-match scoring. <ref type="foot" target="#foot_3">4</ref> Subjective questions are evaluated manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Random Guess</head><p>Figure <ref type="figure">4</ref> illustrates the models' performances on all causal tasks. Each cell in the figure represents a model's accuracy. From Figure <ref type="figure">4</ref>, we can conclude that: (1) Although limited (i.e., approximately 40% to 60% room for improvement), language models do exhibit preliminary understanding (i.e., exceed random guess) of causal graphs. The rightmost column of the figure indicates the models' average accuracies, demonstrating that all models outperform their random guesses. This suggests that they possess basic understanding of the causal graphs. Despite exceeding random guesses, there remains substantial room for improvement. Even the topperforming model, GPT-4, only reaches an accuracy of 60.5%, while the remaining models hover around 40.0%. (2) Language models demonstrate a good grasp of the fundamental elements that constitute a causal graph. All models achieve over 70.0% accuracy on the single node and single edge tasks, with GPT-4 even reaching 100.0% on the single node. These results provide valuable insights for designing future tasks involving causal graphs.</p><p>(3) The model's error response is the dominant factor contributing to its subpar performance compared with random guess. We adopt the error types defined in Chen et al. ( <ref type="formula">2024b</ref>) and observe the model exhibit errors such as causal hallucination, contradiction, and misunderstanding. <ref type="foot" target="#foot_4">5</ref>Figure <ref type="figure">5</ref> presents the models' average accuracies across three levels. We find that: (1) Language models excel at the basic task level. All models achieve an accuracy exceeding 50.0%, with the highest reaching 74.3%. Conversely, most average accuracies attained on the remaining two levels fail to surpass 40.0%. (2) The five models, excluding GPT-4, demonstrate similar performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Is the Model Robust?</head><p>To evaluate the models' robustness, we consider all six different question types. Different question types within a specific causal task, when presented with the same causal graph, aiming to probe the same core concept of causality. Importantly, we acknowledge the potential impact of question type on both the probabilities of random guesses and the phrasing of questions. Our objective is to conduct a preliminary investigation into how question types influence model robustness.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> shows the average accuracy of the models across different question types. We draw the following conclusions: (1) Model performance is sensitive to question type. All models excel in YN and EX question types but struggle with FA, FO, and HM. Wherein, Llama2-Chat-70B, InternLM2-Math-20B, and Gemini Pro exhibit performance discrepancies exceeding 35.0% across different question types. Although GPT-3.5-Turbo is not the top performer, it demonstrates the minimal performance difference, measuring at 22.8%. ( <ref type="formula">2</ref> model's understanding of causal graphs might be artificially inflated if evaluation relies on limited question types. The selection bias inherent in language models raises concerns about their robustness <ref type="bibr">(Zheng et al., 2024a;</ref><ref type="bibr">Chen et al., 2024a</ref>). If we only evaluate language models on CS, YN and EX, we risk overestimating their true capabilities. It is the diversity of question types that reveals the actual understanding capability of a model.<ref type="foot" target="#foot_5">foot_5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Definition Proficiency of the Model</head><p>To investigate whether the models could effectively utilize the provided definitions related to a causal graph, we further conduct experiments on seven tasks (i.e., 3NR, PT, BLP, BKP, CC, MRS, and FAS). 7 For these tasks, the average accuracies across all models on the objective questions are below 40%. Moreover, the seven tasks span all levels in Figure <ref type="figure" target="#fig_1">2</ref>, which can fully demonstrate the effectiveness of the experiments. For prompts, we select the basic prompt, 1/3-shot IcL, and definitionguided prompt. There is ample work validating the effectiveness of IcL <ref type="bibr" target="#b66">(Wu et al., 2023;</ref><ref type="bibr" target="#b63">Wang et al., 2023)</ref>. Therefore, to assess a model's ability to correctly apply or abstract a causal definition, IcL serves as an ideal reference.</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows the overall accuracy difference of each model across seven causal tasks using different prompts. 8 The baseline for comparison is the average accuracy of each model under the basic prompt. By analyzing this figure, we can draw the following conclusions: (1) The models exhibit notable differences in their understanding of definitions related to a causal graph. Providing the causal definition significantly enhances the performance of GPT-4, GPT-3.5-Turbo and Mixtral-8×7B. Notably, the improvement is most pronounced for GPT-4, which even surpasses both 1-shot IcL and 3shot IcL. The improvements on GPT-3.5-Turbo and Mixtral-8×7B are also remarkable, both outperforming 1-shot IcL. However, the remaining three models do not benefit from the provided definition. Specifically, InternLM2-Math-20B exhibits the most prominent accuracy decline. (2) Models capable of (explicitly) utilizing definitions correctly are often observed performance improvements (implicitly) through IcL. However, even if a model's performance can be considerably promoted by IcL, it does not necessarily mean the model can successfully apply (explicit) definitions. Despite the potential for accuracy gains (over 60% cumulatively) of Gemini Pro using 3-shot IcL, it struggles to correctly apply (explicitly) provided definitions, resulting in diminished performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">How Task Dependence Shapes Model Performance</head><p>Based on Figure <ref type="figure" target="#fig_1">2</ref>, we select three representative sets of dependent causal tasks and consider the YN question type. (1) Tasks within the same level: we choose CC→CT→CF, all located at intermediate task.</p><p>(2) Tasks across distinct levels: we choose a sequence spanning different levels: 3NR (basic task)→BKP (intermediate task)→BAS (advanced 7 These abbreviations are given in Table <ref type="table" target="#tab_2">1</ref>. Detailed definitions of all seven tasks are in Table <ref type="table">4</ref> (Appendix C.3). 8 We provide the detailed data in   Upon meticulous examination of Figure <ref type="figure" target="#fig_6">8</ref>, we have the following observations: (1) The performances of most models are not constrained by dependent causal tasks. Out of all models, only GPT-3.5-Turbo and GPT-4 in Figure <ref type="figure" target="#fig_6">8</ref>(b) exhibit the expected accuracy trend (i.e., 3NR≥BKP≥BAS). These results suggest that the models might not truly understand the causal relationships between tasks, but rather rely on other spurious correlations. It is also possible that not all models possess the capacity for human-level causal reasoning and knowledge transfer ability. (2) Different models exhibit varying performance trends when tackling the same group of dependent causal tasks. This highlights the heterogeneity of knowledge representation and application among different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Counterfactual Explainability</head><p>The analyses from Section 4.2 to Section 4.5 are based on directly calculating the accuracy of the models' outputs. To extend beyond mere accuracy, we leverage Captum <ref type="bibr" target="#b36">(Kokhlikyan et al., 2020;</ref><ref type="bibr" target="#b44">Miglani et al., 2023)</ref>, a Python library for model interpretability, to explore language models' understanding of causal graphs from a counterfactual perspective. We primarily use the perturbation based methods provided by Captum.<ref type="foot" target="#foot_6">foot_6</ref> As depicted in Figure <ref type="figure" target="#fig_7">9</ref>, we first query both Llama2-Chat-70B and Mixtral-8×7B, which are of comparable scale and have been widely adopted, using the original question to obtain their respective answers. Our main  Starting with the original question, we obtain answers for both models. Next, we establish the baseline using counterfactual perturbation. Finally, we calculate the token attribution of key information to understand its influence on the model's output.</p><p>focus is the impact of "Z→A" on the model's response. We suspect "X→R" and "M→Z", which are located near the "Z→A", could also potentially impact the model's response. Consequently, we use counterfactual perturbations to analyze the influences of these three statements on the model. We set the counterfactual perturbations as baseline (i.e., perturbation-based algorithm uses it as reference value), and the model's response as target string.</p><p>Finally, using the target function in Captum, we calculate the log probability of the model generating its answer given the question.</p><p>Figure <ref type="figure" target="#fig_8">10</ref> displays the token attributions of the models' responses. We find that "Z→A" is the most positive factor in getting the right answer "1" for Mixtral-8×7B, with "X→R" and "M→Z" also contributing positively. This confirms that Mixtral-8×7B correctly identifies and utilizes the relevant information in its reasoning process. In contrast, Llama2-Chat-70B produces a wrong answer. Neither "Z→A", "X→R" nor "M→Z" exhibit a significant positive impact on its answer, suggesting that Llama2-Chat-70B fails to identify key information.</p><p>The results support the claim in Section 4.2 that models have a preliminary understanding of causal graphs. More importantly, the results demonstrate a strong link between a model's understanding of the causal graph and its ability to focus on the essential information within the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Language models' understanding ability. Language models' understanding is being probed through various perspectives, such as causality <ref type="bibr" target="#b29">(Hobbhahn et al., 2022;</ref><ref type="bibr" target="#b33">Kim et al., 2023;</ref><ref type="bibr" target="#b3">Ashwani et al., 2024)</ref>, real-world problems <ref type="bibr" target="#b18">(Choi et al., 2023;</ref><ref type="bibr" target="#b27">He et al., 2024;</ref><ref type="bibr" target="#b67">Xu et al., 2024)</ref>, disciplines <ref type="bibr">(Castro Nascimento and Pimentel, 2023;</ref><ref type="bibr" target="#b25">Guo et al., 2024)</ref>. A common approach in these studies is to establish a benchmark, and then evaluate a model's performance. A more rigorous exploration of what means understanding in models is still needed.</p><p>Graph-based benchmarks. The capacity of language models to solve graph-based problems is attracting growing attention. <ref type="bibr">Wang et al. (2024a)</ref> propose the NLGraph, concentrating primarily on essential graph tasks. <ref type="bibr" target="#b41">Luo et al. (2024)</ref> introduce the GraphInstruct benchmark. <ref type="bibr" target="#b22">Fatemi et al. (2023)</ref> propose the GraphQA to explore the impact of different graph encoding methods. LLM4DyG <ref type="bibr">(Zhang et al., 2023a)</ref> addresses the dynamic graphs. Despite progress in applying models to graph tasks, their ability to reason about causality within graphs still requires further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper provides a comprehensive and in-depth exploration on the question: Can language models really understand causal graphs? We define a practical framework for accessing a model's understanding. We introduce CLEAR, a novel benchmark designed to evaluate a model's understanding of causal graphs, filling a significant gap in existing research. We validate our framework through extensive experiments and conclude five insightful findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Despite our best efforts to design a framework for causal graph understanding, construct a benchmark, and conduct thorough experiments on six models, we acknowledge that our work still has limitations. The language of CLEAR is relatively limited. Due to time and budget constraints, our benchmark only considers English. As language models are increasingly used worldwide, we acknowledge that a multilingual dataset could provide more meaningful findings. Moreover, the definition of understanding still requires further exploration. For instance, how to extend the concept of robustness to broader scenarios. Additionally, evaluating the understanding of large vision language models (LVLMs) will likely require considering a wider set of factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Considerations in Understanding Framework Design</head><p>A.1 Two-pronged Approach</p><p>In Section 2.2, we propose that we should consider "if a language model understands causal graphs, how should it behave?" We believe the question needs to be considered from two aspects:</p><p>(1) Human-centric perspective. To properly assess language models, we must first define understanding in a way that aligns with human cognition. This is crucial for ensuring language models truly achieve human capabilities.</p><p>(2) Model-centric perspective. While human-centric definitions provide a starting point, there exist foundational differences in information processing between human brains and language models <ref type="bibr" target="#b12">(Caucheteux et al., 2023)</ref>. Therefore, we need to explore practical definitions that are suitable to the characteristics of models.</p><p>To this end, we carefully design four criteria in Section 2.3. And from a model-centric perspective, we define understanding by examining behaviors related to B1: performance exceeding random guesses and B2: robustness against question types. From a human-centric perspective, we consider B3: correct utilization of causal definitions and B4: performance is constrained by task dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Design Details of CLEAR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Overall Statistics</head><p>The detailed statistics of CLEAR are in Table <ref type="table" target="#tab_5">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Question Templates</head><p>Templates for questions of CLEAR are listed in Table <ref type="table" target="#tab_6">3</ref>. While potentially impacting diversity <ref type="bibr" target="#b20">(Cobbe et al., 2021)</ref>, this method enables efficient data scaling and accesses whether a model can recognize subtle distinctions within the templates <ref type="bibr">(Chen et al., 2024b)</ref>. Definition-guided prompt. Taking BKP as an example, Figure <ref type="figure" target="#fig_10">11</ref> illustrates how to incorporate the definition of this causal task into the prompt (i.e., definition-guided prompt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details for Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backdoor path with definition</head><p>Prompt: You will be provided with a definition of a concept. Using this definition, answer the following question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition:</head><p>Given an ordered pair of variables (X, Y), a backdoor path is any path from X to Y that starts with an arrow pointing into X. This backdoor path is a non-causal path from X to Y. Question: Given a DAG (directed acyclic graph) with nodes V, W, L, A and directed edges W→A, V→W, W→L, V→A, L→A. Is W←V→A a backdoor path from W to A? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Qualitative Analysis of Error Response</head><p>Figure <ref type="figure" target="#fig_9">12</ref> provides cases for models' error responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Results of Definition Proficiency</head><p>Definitions for the seven selected tasks are provided in Table <ref type="table">4</ref>. The complete results of the four prompts are shown in Table <ref type="table" target="#tab_4">5</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Performance of six leading language models across 20 diverse tasks in CLEAR. Further details on the experimental results can be found in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Hierarchy and dependent relationships of tasks in CLEAR. We define three complexity levels. (1) Level 1: Basic Task. Mastering these concepts is a prerequisite for understanding any general graph. (2) Level 2: Intermediate Task. These tasks represent the most common characteristics in causal graphs. Causal graph-based reasoning relies heavily on understanding these fundamental problems. (3) Level 3: Advanced Task. These tasks present complex, high-level challenges that are central to causal graph understanding. Solid arrows indicate the dependencies between tasks within the same level, while dashed arrows represent the tasks' dependencies across different levels. Task dependency design draws on established research<ref type="bibr" target="#b57">(Shpitser and Pearl, 2006;</ref><ref type="bibr" target="#b50">Pearl, 2009;</ref><ref type="bibr" target="#b5">Bareinboim and Pearl, 2012;</ref><ref type="bibr" target="#b51">Pearl et al., 2016;</ref><ref type="bibr" target="#b52">Pearl and Mackenzie, 2018;</ref><ref type="bibr" target="#b31">Jaber et al., 2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Overall model performance. Each cell corresponds to the model's accuracy on that specific task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: How the question types affect model robustness. We compare the models' accuracies across different question types. FA stands for find all, FO for find one, HM for how many, CS for choice selection, YN for yes or no, and EX for existence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Explicit and implicit definition proficiency. We compare how well the model could utilize definitions, examining both explicitly and implicitly. Add def. indicates the definition-guided prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Task dependence's impact on model performance. We evaluate model performance across three groups of causal tasks categorized by their correlations. The orange dashed line represents the accuracy of random guess.</figDesc><graphic coords="8,313.98,269.26,115.99,84.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Counterfactual perturbation used in this case. Starting with the original question, we obtain answers for both models. Next, we establish the baseline using counterfactual perturbation. Finally, we calculate the token attribution of key information to understand its influence on the model's output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Token attribution. On the y-axis, underscores mark the tokenizer's divisions of each target string. The x-axis displays key information of the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C. 1</head><label>1</label><figDesc>Prompt Settings Basic prompt. Our basic prompt aligns with the definition in Chen et al. (2024b), referring to providing only the question requiring an answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Definition-guided prompt. We explicitly provide the model with definitions relevant to the questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>L1: BasicTask L3: AdvancedTask L2: IntermediateTask</head><label></label><figDesc>sufficiently define a model's un-</figDesc><table><row><cell>Single Node</cell><cell>Single Edge</cell><cell cols="2">Two Nodes Relationship</cell><cell>Topological Ordering</cell><cell>Causal Effect Identification</cell></row><row><cell>Cycle</cell><cell></cell><cell>Path</cell><cell cols="2">Three Nodes Relationship</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Backdoor Adjustment Set</cell></row><row><cell>C-Tree</cell><cell>C-Component</cell><cell>Directed Path</cell><cell>Backdoor Path</cell><cell>Blocked Path</cell><cell></cell></row><row><cell>C-Forest</cell><cell>Maximal Root Set</cell><cell>Markov Blanket</cell><cell>D-Separation</cell><cell>Markov Equivalent Class</cell><cell>Frontdoor Adjustment Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Concise statistics of the CLEAR benchmark. We tally the number of different causal tasks, organizing them by various levels. Type indicates question type.</figDesc><table><row><cell>Causal task</cell><cell cols="2"># Type # Sample</cell></row><row><cell>Basic Task</cell><cell></cell><cell></cell></row><row><cell>Single node (SN)</cell><cell>4</cell><cell>192</cell></row><row><cell>Single edge (SE)</cell><cell>4</cell><cell>192</cell></row><row><cell>Two nodes relationship (2NR)</cell><cell>5</cell><cell>120</cell></row><row><cell>Three nodes relationship (3NR)</cell><cell>5</cell><cell>120</cell></row><row><cell>Path (PT)</cell><cell>5</cell><cell>168</cell></row><row><cell>Cycle (CL)</cell><cell>4</cell><cell>144</cell></row><row><cell>Topological ordering (TO)</cell><cell>3</cell><cell>144</cell></row><row><cell cols="2">Intermediate Task</cell><cell></cell></row><row><cell>Blocked path (BLP)</cell><cell>3</cell><cell>144</cell></row><row><cell>D-separation (DS)</cell><cell>3</cell><cell>120</cell></row><row><cell>Markov equivalent class (MEC)</cell><cell>2</cell><cell>120</cell></row><row><cell>Markov blanket (MB)</cell><cell>3</cell><cell>144</cell></row><row><cell>Directed path (DP)</cell><cell>5</cell><cell>120</cell></row><row><cell>Backdoor path (BKP)</cell><cell>5</cell><cell>144</cell></row><row><cell>C-component (CC)</cell><cell>3</cell><cell>108</cell></row><row><cell>C-tree (CT)</cell><cell>1</cell><cell>120</cell></row><row><cell>C-forest (CF)</cell><cell>1</cell><cell>120</cell></row><row><cell>Maximal root set (MRS)</cell><cell>4</cell><cell>192</cell></row><row><cell>Advanced Task</cell><cell></cell><cell></cell></row><row><cell>Backdoor adjustment set (BAS)</cell><cell>4</cell><cell>132</cell></row><row><cell>Frontdoor adjustment set (FAS)</cell><cell>4</cell><cell>144</cell></row><row><cell>Causal effect identification (CEI)</cell><cell>1</cell><cell>120</cell></row><row><cell>Total</cell><cell>6</cell><cell>2808</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 1 presents the overview of the CLEAR.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Setups</cell></row><row><cell>Models. Our evaluation encompasses six models.</cell></row><row><cell>This selection includes both open-access models</cell></row><row><cell>(InternLM2-Math-20B (Ying et al., 2024), Mixtral-</cell></row><row><cell>8×7B (Jiang et al., 2024), and Llama2-Chat-</cell></row><row><cell>70B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note><p><p>(Appendix C.3</p>).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Detailed statistics of the CLEAR benchmark. We tally the number of different question types within each causal task, organizing them by various levels. YN indicates yes or no.</figDesc><table><row><cell>Causal task</cell><cell cols="7">Find all Find one How many Choice selection YN Existence Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Basic Task</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single node (SN)</cell><cell>48</cell><cell>-</cell><cell>48</cell><cell>48</cell><cell>48</cell><cell>-</cell><cell>192</cell></row><row><cell>Single edge (SE)</cell><cell>48</cell><cell>-</cell><cell>48</cell><cell>48</cell><cell>48</cell><cell>-</cell><cell>192</cell></row><row><cell>Two nodes relationship (2NR)</cell><cell>24</cell><cell>-</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>120</cell></row><row><cell>Three nodes relationship (3NR)</cell><cell>24</cell><cell>-</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>120</cell></row><row><cell>Path (PT)</cell><cell>24</cell><cell>72</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>-</cell><cell>168</cell></row><row><cell>Cycle (CL)</cell><cell>-</cell><cell>36</cell><cell>-</cell><cell>36</cell><cell>36</cell><cell>36</cell><cell>144</cell></row><row><cell>Topological ordering (TO)</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Question template for CLEAR.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>More thoughts about our framework are in Appendix A.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Appendix B provides more information on the dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>See Appendix C for details on these prompts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Prior studies have shown that strong language models (e.g., GPT-4) can be effective judges<ref type="bibr" target="#b40">(Lu et al., 2023;</ref> Zheng  et al., 2024b), demonstrating the validity of this approach.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The qualitative analysis is provided in Appendix C.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>There is a growing trend in benchmark design towards incorporating a wider variety of question types or providing more choices for models(Chen et al., 2024b; Wang et al.,  2024b;<ref type="bibr" target="#b55">Röttger et al., 2024)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>For further guidance, refer to the tutorial at: https:// captum.ai/tutorials/Llama2_LLM_Attribution.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt Given a DAG (directed acyclic graph) with nodes U, R, P, E, W, M, I, K and directed edges P-&gt;E, U-&gt;M, P-&gt;I, E-&gt;K, U-&gt;W, U-&gt;P, U-&gt;I, E-&gt;W, R-&gt;M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which of the following is one of children of node R?</head><p>A. M B. I C. K D. W</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model response:</head><p>The children of node R are the nodes that have an edge pointing to R. The edges pointing to R are: * U-&gt;R * P-&gt;R So, the children of R are U and P. Therefore, the correct answer is: A. M M is not a child of R, as there is no edge pointing from R to M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contradiction &amp; Misunderstanding</head><p>Ground truth: A (a) Example of contradiction and misunderstanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal hallucination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth: No</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Given an ADMG (acyclic directed mixed graph) with nodes B, F, C, K, directed edges B-&gt;C, B-&gt;K, F-&gt;K, C-&gt;K, F-&gt;C and bidirected edges F&lt;-&gt;C, C&lt;-&gt;K.</p><p>Is there a directed path from F to B?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model response:</head><p>Yes, there is a directed path from F to B in the given ADMG.</p><p>The path is:</p><p>Here's a step-by-step explanation:</p><p>1 Table <ref type="table">4</ref>: Definitions of the selected seven causal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal task Definition Basic Task</head><p>Three nodes relationship Given a DAG with three nodes X, Y, Z.</p><p>(1) A "chain" is a sequence of nodes connected by edges where each node has only one predecessor and one successor (except for the first and last nodes in the chain). The simplest chain in a causal graph can be illustrated as "X-&gt;Y-&gt;Z".</p><p>(2) A "fork" refers to a situation where one node has multiple outgoing edges leading to different successor nodes. The simplest fork in a causal graph can be illustrated as "X&lt;-Y-&gt;Z".</p><p>(3) A "v-structure" means one node is a child of the two others that themselves are not adjacent. The simplest v-structure in a causal graph can be illustrated as "X-&gt;Y&lt;-Z".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path</head><p>A path in a DAG is a sequence of (at least two) distinct nodes i1, . . . , im such that there is an edge between i k and i k+1 for all k = 1, . . . , m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocked path</head><p>In a DAG, a path p is said to be blocked by a set of nodes Z if and only if:</p><p>(1) p contains a chain i-&gt;m-&gt;j or a fork i&lt;-m-&gt;j such that the middle node m is in Z, or (2) p contains an inverted fork (or collider) i-&gt;m&lt;-j such that the middle node m is not in Z and such that no descendant of m is in Z.</p><p>Backdoor path Given an ordered pair of variables (X, Y), a backdoor path is any path from X to Y that starts with an arrow pointing into X. This backdoor path is a non-causal path from X to Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-component</head><p>Let G be a causal graph such that a subset of its bidirected arcs forms a spanning tree over all nodes in G. Then G is a C-component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximal root set</head><p>Let G be a causal graph and X is one node that belongs to G. If X does not have any descendant, then we call X a root set of G. Maximal set contains all the root sets of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advanced Task</head><p>Frontdoor adjustment set If a set of variables Z satisfies the front-door criterion relative to an ordered pair of variables (X, Y):</p><p>(1) Z intercepts all directed paths from X to Y;</p><p>(2) there is no unblocked back-door path from X to Z; and</p><p>(3) all back-door paths from Z to Y are blocked by X.</p><p>Then we call Z a frontdoor adjustment set, this set allows us to accurately estimate the causal effect of X on Y.</p><p>Table <ref type="table">5</ref>: Model performance on seven selected causal tasks.</p><p>Causal task Prompt Mixtral Llama2 InternLM2 GPT-3.5-Turbo GPT-4 Gemini Pro Basic Task</p><p>Three nodes relationship</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. 2023. Gpt-4 technical report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mapping the landscape of human-level artificial general intelligence</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itmar</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joscha</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Coop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rod</forename><surname>Furlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Goertzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Storrs</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Samsonovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Scheutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Schlesinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding human communication</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Rodman</surname></persName>
		</author>
		<author>
			<persName><surname>Sévigny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">10</biblScope>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Swagata</forename><surname>Ashwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshiteesh</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishith</forename><surname>Reddy Mannuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Singh Sengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Chaitanya Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dishant</forename><surname>Kathala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinija</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Chadha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18139</idno>
		<title level="m">Cause and effect: Can large language models truly understand causality? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data scaling laws in nmt: The effect of noise and architecture</title>
		<author>
			<persName><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1466" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal inference by surrogate experiments: z-identifiability</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph of thoughts: Solving elaborate problems with large language models</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Blach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Kubicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gerstenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Podstawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nyczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="17682" to="17690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causality and modern science</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Bunge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An investigation of preservice teachers&apos; use of guess and check in solving a semi open-ended mathematics problem</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Capraro</forename><surname>Song A An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabiola</forename><surname>Rangel-Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Harbaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Mathematical Behavior</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="116" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Objectual understanding and the value problem</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">C</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Philosophical Quarterly</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do large language models understand chemistry? a conversation with chatgpt</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1649" to="1655" />
			<date type="published" when="2023">2023</date>
			<publisher>Cayque Monteiro Castro Nascimento and André Silva Pimentel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evidence of a predictive coding hierarchy in the human brain listening to speech</title>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Caucheteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Rémi</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="441" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05845</idno>
		<title level="m">Graphllm: Boosting graph reasoning ability of large language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2024a. Quantifying and mitigating unimodal biases in multimodal large language models: A causal perspective</title>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.18346</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.00622</idno>
		<title level="m">Causal evaluation of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Theoremqa: A theorem-driven question answering dataset</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2024c. Exploring the potential of large language models (llms) in learning on graphs</title>
		<author>
			<persName><forename type="first">Zhikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark</title>
		<author>
			<persName><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine reading, fast and slow: When do models &quot;understand&quot; language?</title>
		<author>
			<persName><forename type="first">Ray</forename><surname>Sagnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="78" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discussion note: Making sense of understanding</title>
		<author>
			<persName><surname>Henk W De Regt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Definitions, traditions, and a general framework for understanding complex problem solving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Frensch</surname></persName>
		</author>
		<author>
			<persName><surname>Funke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex problem solving</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">N</forename><surname>Zalta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Metaphysics Research Lab, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>Summer 2021 edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Didolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.15485</idno>
		<title level="m">Learning beyond pattern matching? assaying mathematical understanding in llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dealing with noise problem in machine learning data-sets: A systematic review</title>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="466" to="474" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Can large language models understand real-world complex instructions?</title>
		<author>
			<persName><forename type="first">Qianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunzhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="18188" to="18196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A planning heuristic based on causal graph analysis</title>
		<author>
			<persName><forename type="first">Malte</forename><surname>Helmert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Investigating causal understanding in llms</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Hobbhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Seiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS ML Safety Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond text: A deep dive into large language models&apos; ability on understanding graph data</title>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop: New Frontiers in Graph Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal identification under markov equivalence: Completeness results</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2981" to="2989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blanche</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">Bou</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><surname>Bressand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04088</idno>
		<title level="m">Mixtral of experts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Can chatgpt understand causal language in science claims?</title>
		<author>
			<persName><forename type="first">Yuheun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis</title>
		<meeting>the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Causality, probability, and time</title>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Experimental design for learning causal graphs with latent variables</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Narine</forename><surname>Kokhlikyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kliushkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07896</idno>
		<title level="m">Captum: A unified and generic model interpretability library for pytorch</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The value of knowledge and the pursuit of understanding</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Kvanvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Moral functioning: Moral understanding and personality</title>
		<author>
			<persName><surname>Daniel K Lapsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Moral development, self, and identity</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Emergent world representations: Exploring a sequence model trained on a synthetic task</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aspen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graphinstruct: Empowering large language models with graph understanding and reasoning capability</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04483</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From here to human-level ai</title>
		<author>
			<persName><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1174" to="1182" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>James L Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page" from="25966" to="25974" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using captum to explain generative language models</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aobo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Markosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narine</forename><surname>Kokhlikyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)</title>
		<meeting>the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Noisier2noise: Learning to denoise from unpaired noisy data</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Coady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12064" to="12072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding understanding</title>
		<author>
			<persName><surname>Raymond S Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Education</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="239" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Chatgpt: Optimizing language models for dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Blog post</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graphs, causality, and structural equation models</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="284" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Basic books</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust agents learn causal world models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Richens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Röttger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Musashi</forename><surname>Hinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><forename type="middle">Rose</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16786</idno>
		<title level="m">Political compass or spinning arrow? towards more meaningful evaluations for values and opinions in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Understanding understanding. Memories, thoughts and emotions: Essays in honor of George Mandler</title>
		<author>
			<persName><surname>David E Rumelhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semimarkovian causal models</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Sloman</surname></persName>
		</author>
		<title level="m">Causal models: How people think about the world and its alternatives</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">100% classification accuracy considered harmful: The normalized information transfer factor explains the accuracy paradox</title>
		<author>
			<persName><forename type="first">J</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Valverde-Albacete</surname></persName>
		</author>
		<author>
			<persName><surname>Peláez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">84217</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2024a. Can language models solve graph problems in natural language?</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-instruct: Aligning language models with self-generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">2024b. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhranil</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaran</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01574</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Making things happen: A theory of causal explanation</title>
		<author>
			<persName><forename type="first">James</forename><surname>Woodward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1423" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Zhijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.13599</idno>
		<title level="m">a good pun is its own reword&quot;: Can large language models understand puns? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07134</idno>
		<title level="m">Natural language is all a graph needs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Huaiyuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhejian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06332</idno>
		<title level="m">Internlm-math: Open math large language models toward verifiable reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17110</idno>
		<title level="m">Llm4dyg: Can large language models solve problems on dynamic graphs? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">2023b. Graph meets llms: Towards large graph models</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop: New Frontiers in Graph Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">2024a. Large language models are not robust multiple choice selectors</title>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Step-back prompting enables reasoning via abstraction in large language models</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Huaixiu Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Eric Xing, et al. 2024b. Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
