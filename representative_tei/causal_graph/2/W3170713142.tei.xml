<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System</title>
				<funder ref="#_GzUdvuR #_Cq2Qw6e">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_NHYMnsH">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-06-07">7 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
							<email>fulifeng93@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
							<email>yijinfeng@jd.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<email>xiangnanhe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-07">7 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467289</idno>
					<idno type="arXiv">arXiv:2010.15363v2[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation</term>
					<term>Popularity Bias</term>
					<term>Causal Reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The general aim of the recommender system is to provide personalized suggestions to users, which is opposed to suggesting popular items. However, the normal training paradigm, i.e., fitting a recommender model to recover the user behavior data with pointwise or pairwise loss, makes the model biased towards popular items. This results in the terrible Matthew effect, making popular items be more frequently recommended and become even more popular. Existing work addresses this issue with Inverse Propensity Weighting (IPW), which decreases the impact of popular items on the training and increases the impact of long-tail items. Although theoretically sound, IPW methods are highly sensitive to the weighting strategy, which is notoriously difficult to tune.</p><p>In this work, we explore the popularity bias issue from a novel and fundamental perspective -cause-effect. We identify that popularity bias lies in the direct effect from the item node to the ranking score, such that an item's intrinsic property is the cause of mistakenly assigning it a higher ranking score. To eliminate popularity bias, it is essential to answer the counterfactual question that what the ranking score would be if the model only uses item property. To this end, we formulate a causal graph to describe the important cause-effect relations in the recommendation process. During training, we perform multi-task learning to achieve the contribution of each cause; during testing, we perform counterfactual inference to remove the effect of item popularity. Remarkably, our solution amends the learning process of recommendation which is agnostic to a wide range of models -it can be easily implemented in existing methods. We demonstrate it on Matrix Factorization (MF) and LightGCN [20], which are representative of the conventional and SOTA model for collaborative filtering. Experiments on five real-world datasets demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized recommendation has revolutionized a myriad of online applications such as e-commerce <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b59">60]</ref>, search engines <ref type="bibr" target="#b42">[43]</ref>, and conversational systems <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref>. A huge number of recommender models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49]</ref> have been developed, for which the default optimization choice is reconstructing historical user-item interactions. However, the frequency distribution of items is never even in the interaction data, which is affected by many factors like exposure mechanism, word-of-mouth effect, sales campaign, item quality, etc. In most cases, the frequency distribution is long-tail, i.e., the majority of interactions are occupied by a small number of popular items. This makes the classical training paradigm biased towards recommending popular items, falling short to reveal the true preference of users <ref type="bibr" target="#b1">[2]</ref>.</p><p>Real-world recommender systems are often trained and updated continuously using real-time user interactions with training data and test data NOT independent and identically distributed (non-IID) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b57">58]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> provides an evidence of popularity bias on a real-world Adressa dataset <ref type="bibr" target="#b17">[18]</ref>, where we train a standard MF and LightGCN <ref type="bibr" target="#b19">[20]</ref> and count the frequency of items in the top-ğ¾ recommendation lists of all users. The blue line shows the item frequency of the real non-IID test dataset, which is what we expect. As can be seen, more popular items in the training data are recommended much more frequently than expected, demonstrating a severe popularity bias. As a consequence, a model is prone to recommending items simply from their popularity, rather than user-item matching. This phenomenon is caused by the training paradigm, which identifies that recommending popular items more frequently can achieve lower loss thus updates parameters towards that direction. Unfortunately, such popularity bias will hinder the recommender from accurately understanding the user preference and decrease the diversity of recommendations. Worse still, the popularity bias will cause the Matthew Effect <ref type="bibr" target="#b35">[36]</ref> -popular items are recommended more and become even more popular.</p><p>To address the issues of normal training paradigm, a line of studies push the recommender training to emphasize the long-tail items <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. The idea is to downweigh the influence from popular items on recommender training, e.g., re-weighting their interactions in the training loss <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref>, incorporating balanced training data <ref type="bibr" target="#b6">[7]</ref> or disentangling user and item embeddings <ref type="bibr" target="#b57">[58]</ref>. However, these methods lack fine-grained consideration of how item popularity affects each specific interaction, and a systematic view of the mechanism of popularity bias. For instance, the interactions on popular items will always be downweighted than a long-tail item regardless of a popular item better matches the preference of the user. We believe that instead of pushing the recommender to the long-tail in a blind manner, the key of eliminating popularity bias is to understand how item popularity affects each interaction.</p><p>Towards this end, we explore the popularity bias from a fundamental perspective -cause-effect, which has received little scrutiny in recommender systems. We first formulate a causal graph (Figure <ref type="figure" target="#fig_1">2(c)</ref>) to describe the important cause-effect relations in the recommendation process, which corresponds to the generation process of historical interactions. In our view, three main factors affect the probability of an interaction: user-item matching, item popularity, and user conformity. However, existing recommender models largely focus on the user-item matching factor <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref> (Figure <ref type="figure" target="#fig_1">2</ref>(a)), ignoring how the item popularity affects the interaction probability (Figure <ref type="figure" target="#fig_1">2</ref>(b)). Suppose two items have the same matching degree for a user, the item that has larger popularity is more likely to be known by the user and thus consumed. Furthermore, such impacts of item popularity could vary for different users, e.g., some users are more likely to explore popular items while some are not. As such, we further add a direct edge from the user node (ğ‘ˆ ) to the ranking score (ğ‘Œ ) to constitute the final causal graph (Figure <ref type="figure" target="#fig_1">2</ref>(c)). To eliminate popularity bias effectively, it is essential to infer the direct effect from the item node (ğ¼ ) to the ranking score (ğ‘Œ ), so as to remove it during recommendation inference.</p><p>To this end, we resort to causal inference which is the science of analyzing the relationship between a cause and its effect <ref type="bibr" target="#b34">[35]</ref>. According to the theory of counterfactual inference <ref type="bibr" target="#b34">[35]</ref>, the direct effect of ğ¼ â†’ ğ‘Œ can be estimated by imagining a world where the user-item matching is discarded, and an interaction is caused by item popularity and user conformity. To conduct popularity debiasing, we just deduct the ranking score in the counterfactual world from the overall ranking score. Figure <ref type="figure" target="#fig_2">3</ref> shows a toy example where the training data is biased towards iPhone, making the model score higher on iPhone even though the user is more interested in basketball. Such bias is removed in the inference stage by deducting the counterfactual prediction.</p><p>In our method, to pursue a better learning of user-item matching, we construct two auxiliary tasks to capture the effects of ğ‘ˆ â†’ ğ‘Œ and ğ¼ â†’ ğ‘Œ . The model is trained jointly on the main task and two auxiliary tasks. Remarkably, our approach is model-agnostic and we implement it on MF <ref type="bibr" target="#b27">[28]</ref> and LightGCN <ref type="bibr" target="#b19">[20]</ref> to demonstrate effectiveness. To summarize, this work makes the following contributions:</p><p>â€¢ Presenting a causal view of the popularity bias in recommender systems and formulating a causal graph for recommendation. â€¢ Proposing a model-agnostic counterfactual reasoning <ref type="bibr">(MACR)</ref> framework that trains the recommender model according to the causal graph and performs counterfactual inference to eliminate popularity bias in the inference stage of recommendation. â€¢ Evaluating on five real-world recommendation datasets to demonstrate the effectiveness and rationality of MACR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><formula xml:id="formula_0">Let U = {ğ‘¢ 1 , ğ‘¢</formula><p>The goal of recommender training is to learn a scoring function ğ‘“ (ğ‘¢, ğ‘– |ğœƒ ) from ğ‘Œ , which is capable of predicting the preference of a user ğ‘¢ over item ğ‘–. Typically, the learned recommender model is evaluated on a set of holdout (e.g., randomly or split by time) interactions in the testing stage. However, the traditional evaluation may not reflect the ability to predict user true preference due to the existence of popularity bias in both training and testing. Aiming to focus more on user preference, we follow prior work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> to perform debiased evaluation where the testing interactions are sampled to be a uniform distribution over items. This evaluation also can examine a model's ability in handling the popularity bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first detail the key concepts about counterfactual inference (Section 3.1), followed by the causal view of the recommendation process (Section 3.2), the introduction of the MACR framework (Section 3.3), and its rationality for eliminating the popularity bias (Section 3.4). Lastly, we discuss the possible extension of MACR when the side information is available (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>â€¢ Causal Graph. The causal graph is a directed acyclic graph ğº = {ğ‘‰ , ğ¸}, where ğ‘‰ denotes the set of variables and ğ¸ represents the cause-effect relations among variables <ref type="bibr" target="#b34">[35]</ref>. In a causal graph, a capital letter (e.g., ğ¼ ) denotes a variable and a lowercase letter (e.g., ğ‘–) denotes its observed value. An edge means the ancestor node is a cause (ğ¼ ) and the successor node is an effect (ğ‘Œ ). Take Figure <ref type="figure" target="#fig_3">4</ref> as an example, ğ¼ â†’ ğ‘Œ means there exists a direct effect from ğ¼ to ğ‘Œ . Furthermore, the path ğ¼ â†’ ğ¾ â†’ ğ‘Œ means ğ¼ has an indirect effect on ğ‘Œ via a mediator ğ¾. According to the causal graph, the value of ğ‘Œ can be calculated from the values of its ancestor nodes, which is formulated as:</p><formula xml:id="formula_2">ğ‘Œ ğ‘–,ğ‘˜ = ğ‘Œ (ğ¼ = ğ‘–, ğ¾ = ğ‘˜),<label>(2)</label></formula><p>where ğ‘Œ (.) means the value function of ğ‘Œ . In the same way, the value of the mediator can be obtained through ğ‘˜ = ğ¾ ğ‘– = ğ¾ (ğ¼ = ğ‘–). In particular, we can instantiate ğ¾ (ğ¼ ) and ğ‘Œ (ğ¼, ğ¾) as neural operators (e.g., fully-connected layers), and compose a solution that predicts the value of Y from input I.</p><p>â€¢ Causal Effect. The causal effect of ğ¼ on ğ‘Œ is the magnitude by which the target variable ğ‘Œ is changed by a unit change in an ancestor variable ğ¼ <ref type="bibr" target="#b34">[35]</ref>. For example, the total effect (TE) of ğ¼ = ğ‘– on ğ‘Œ is defined as:</p><formula xml:id="formula_3">ğ‘‡ ğ¸ = ğ‘Œ ğ‘–,ğ¾ ğ‘– -ğ‘Œ ğ‘– * ,ğ¾ ğ‘– * ,<label>(3)</label></formula><p>which can be understood as the difference between two hypothetical situations ğ¼ = ğ‘– and ğ¼ = ğ‘– * . ğ¼ = ğ‘– * refers to a the situation where the value of ğ¼ is muted from the reality, typically set the value as null. ğ¾ ğ‘– * denotes the value of ğ¾ when ğ¼ = ğ‘– * . Furthermore, according to the structure of the causal graph, TE can be decomposed into natural direct effect (NDE) and total indirect effect (TIE) which represent the effect through the direct path ğ¼ â†’ ğ‘Œ and the indirect path ğ¼ â†’ ğ¾ â†’ ğ‘Œ , respectively <ref type="bibr" target="#b34">[35]</ref>. NDE expresses the value change of ğ‘Œ with ğ¼ changing from ğ‘– * to ğ‘– on the direct path ğ¼ â†’ ğ‘Œ , while ğ¾ is set to the value when ğ¼ = ğ‘– * , which is formulated as:</p><formula xml:id="formula_4">ğ‘ ğ·ğ¸ = ğ‘Œ ğ‘–,ğ¾ ğ‘– * -ğ‘Œ ğ‘– * ,ğ¾ ğ‘– * ,<label>(4)</label></formula><p>where ğ‘Œ ğ‘–,ğ¾ ğ‘– * = ğ‘Œ (ğ¼ = ğ‘–, ğ¾ = ğ¾ (ğ¼ = ğ‘– * )). The calculation of ğ‘Œ ğ‘– , ğ¾ ğ‘– * is a counterfactual inference since it requires the value of the same variable ğ¼ to be set with different values on different paths (see Figure <ref type="figure" target="#fig_3">4</ref>). Accordingly, TIE can be obtained by subtracting NDE from TE as following:</p><formula xml:id="formula_5">ğ‘‡ ğ¼ğ¸ = ğ‘‡ ğ¸ -ğ‘ ğ·ğ¸ = ğ‘Œ ğ‘–,ğ¾ ğ‘– -ğ‘Œ ğ‘–,ğ¾ ğ‘– * ,<label>(5)</label></formula><p>which represents the effect of ğ¼ on ğ‘Œ through the indirect path ğ¼ â†’ ğ¾ â†’ ğ‘Œ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Look at Recommendation</head><p>In Figure <ref type="figure" target="#fig_1">2</ref>(a), we first abstract the causal graph of most existing recommender models, where ğ‘ˆ , ğ¼ , ğ¾, and ğ‘Œ represent user embedding, item embedding, user-item matching features, and ranking score, respectively. The models have two main components: a matching function ğ¾ (ğ‘ˆ , ğ¼ ) that learns the matching features between user and item; and the scoring function ğ‘Œ (ğ¾). For instance, the most popular MF model implements these functions as an element-wise product between user and item embeddings, and a summation across embedding dimensions. As to its neural extension NCF <ref type="bibr" target="#b21">[22]</ref>, the scoring function is replaced with a fully-connected layer. Along this line, a surge of attention has been paid to the design of these functions.</p><p>For instance, LightGCN <ref type="bibr" target="#b19">[20]</ref> and NGCF <ref type="bibr" target="#b48">[49]</ref> employ graph convolution to perform matching feature learning, ONCF <ref type="bibr" target="#b20">[21]</ref> adopts convolutional layers as the scoring function. However, these models discards the user conformity and item popularity that directly affect the ranking score. A more complete causal graph for recommendation is depicted in Figure <ref type="figure" target="#fig_1">2</ref>(c) where the paths ğ‘ˆ â†’ ğ‘Œ and ğ¼ â†’ ğ‘Œ represent the direct effects from user and item on the ranking score. A few recommender models follow this causal graph, e.g., the MF with additional terms of user and item biases <ref type="bibr" target="#b27">[28]</ref> and NeuMF <ref type="bibr" target="#b21">[22]</ref> which takes the user and item embeddings as additional inputs of its scoring function. While all these models perform inference with a forward propagation, the causal view of the inference over Figure <ref type="figure" target="#fig_1">2</ref>(a) and Figure <ref type="figure" target="#fig_1">2</ref>(c) are different, which are ğ‘Œ ğ¾ ğ‘¢,ğ‘– and ğ‘Œ ğ‘¢,ğ‘–,ğ¾ ğ‘¢,ğ‘– , respectively. However, the existing work treats them equally in both training and testing stages. For briefness, we use Å·ğ‘¢ğ‘– to represent the ranking score, which is supervised to recover the historical interactions by a recommendation loss such as the BCE loss <ref type="bibr" target="#b53">[54]</ref>:</p><formula xml:id="formula_6">ğ¿ ğ‘‚ = âˆ‘ï¸ (ğ‘¢,ğ‘–) âˆˆğ· -ğ‘¦ ğ‘¢ğ‘– log(ğœ ( Å·ğ‘¢ğ‘– )) -(1 -ğ‘¦ ğ‘¢ğ‘– ) log(1 -ğœ ( Å·ğ‘¢ğ‘– )),<label>(6)</label></formula><p>where ğ· denotes the training set and ğœ (â€¢) denotes the sigmoid function. Å·ğ‘¢,ğ‘– means either ğ‘Œ ğ¾ ğ‘¢,ğ‘– or ğ‘Œ ğ‘¢,ğ‘–,ğ¾ ğ‘¢,ğ‘– . In the testing stage, items with higher ranking scores are recommended to users.</p><p>Most of these recommender model suffer from popularity bias (see Figure <ref type="figure" target="#fig_0">1</ref>). This is because Å·ğ‘¢ğ‘– is the likelihood of the interaction between user ğ‘¢ and item ğ‘–, which is estimated from the training data and inevitably biased towards popular items in the data. From the causal perspective, item popularity directly affects Å·ğ‘¢ğ‘– via ğ¼ â†’ ğ‘Œ , which bubbles the ranking scores of popular items. As such, blocking the direct effect from item popularity on ğ‘Œ will eliminate the popularity bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model-Agnostic Counterfactual Reasoning</head><p>To this end, we devise a model-agnostic counterfactual reasoning (MACR) framework, which performs multi-task learning for recommender training and counterfactual inference for making debiased recommendation. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the framework follows the causal graph in Figure <ref type="figure" target="#fig_1">2(c)</ref>, where the three branches correspond to the paths ğ‘ˆ â†’ ğ‘Œ , ğ‘ˆ &amp;ğ¼ â†’ ğ¾ â†’ ğ‘Œ , and ğ¼ â†’ ğ‘Œ , respectively. This ğ‘¢ would interact with items no matter whether the preference is matched. Considering the situation where two users are randomly recommended the same number of videos, one user may click more videos due to a broader preference or stronger conformity. Such "easy" user is expected to obtain a higher value of Å·ğ‘¢ and can be affected more by item popularity.</p><p>As the training objective is to recover the historical interactions ğ‘¦ ğ‘¢ğ‘– , the three branches are aggregated into a final prediction score:</p><formula xml:id="formula_7">Å·ğ‘¢ğ‘– = Å·ğ‘˜ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ),<label>(7)</label></formula><p>where ğœ (â€¢) denotes the sigmoid function. It scales Å·ğ‘¢ and Å·ğ‘– to be click probabilities in the range of [0, 1] so as to adjust the extent of relying upon user-item matching (i.e. Å·ğ‘˜ ) to recover the historical interactions. For instance, to recover the interaction between an inactive user and unpopular item, the model will be pushed to highlight the user-item matching, i.e. enlarging Å·ğ‘˜ .</p><p>Recommender Training. Similar to (6), we can still apply a recommendation loss over the overall ranking score Å·ğ‘¢ğ‘– . To achieve the effect of the user and item modules, we devise a multi-task learning schema that applies additional supervision over Å·ğ‘¢ and Å·ğ‘– . Formally, the training loss is given as:  where ğ›¼ and ğ›½ are trade-off hyper-parameters. Similar as ğ¿ ğ‘‚ , ğ¿ ğ¼ and ğ¿ ğ‘ˆ are also recommendation losses:</p><formula xml:id="formula_8">ğ¿ = ğ¿ ğ‘‚ + ğ›¼ * ğ¿ ğ¼ + ğ›½ * ğ¿ ğ‘ˆ ,<label>(8)</label></formula><formula xml:id="formula_9">ğ¿ ğ‘ˆ = âˆ‘ï¸ (ğ‘¢,ğ‘–) âˆˆğ· -ğ‘¦ ğ‘¢ğ‘– log(ğœ ( Å·ğ‘¢ )) -(1 -ğ‘¦ ğ‘¢ğ‘– ) log(1 -ğœ ( Å·ğ‘¢ )), ğ¿ ğ¼ = âˆ‘ï¸ (ğ‘¢,ğ‘–) âˆˆğ· -ğ‘¦ ğ‘¢ğ‘– log(ğœ ( Å·ğ‘– )) -(1 -ğ‘¦ ğ‘¢ğ‘– ) log(1 -ğœ ( Å·ğ‘– )).</formula><p>Counterfactual Inference. As aforementioned, the key to eliminate the popularity bias is to remove the direct effect via path ğ¼ â†’ ğ‘Œ from the ranking score Å·ğ‘¢ğ‘– . To this end, we perform recommendation according to:</p><formula xml:id="formula_10">Å·ğ‘˜ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ) -ğ‘ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ),<label>(9)</label></formula><p>where ğ‘ is a hyper-parameter that represents the reference status of Å·ğ‘˜ . The rationality of the inference will be detailed in the following section. Intuitively, the inference can be understood as an adjustment of the ranking according to Å·ğ‘¢ğ‘– . Assuming two items ğ‘– and ğ‘— with Å·ğ‘¢ğ‘– slightly lower than Å·ğ‘¢ ğ‘— , item ğ‘— will be ranked in front of ğ‘– in the common inference. Our adjustment will affect if item ğ‘— is much popular than ğ‘– where Å·ğ‘— &gt;&gt; Å·ğ‘– . Due to the subtraction of the second part, the less popular item ğ‘– will be ranked in front of j.. The scale of such adjustment is user-specific and controlled by Å·ğ‘¢ where a larger adjustment will be conducted for "easy" users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rationality of the Debiased Inference</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>(c), ğ¼ influences ğ‘Œ through two paths, the indirect path ğ¼ â†’ ğ¾ â†’ ğ‘Œ and the direct path ğ¼ â†’ ğ‘Œ . Following the counterfactual notation in Section 3.1, we calculate the NDE from ğ¼ to ğ‘Œ through counterfactual inference where a counterfactual recommender system (Figure <ref type="figure" target="#fig_6">6(b)</ref>) assigns the ranking score without consideration of user-item matching. As can be seen, the indirect path is blocked by feeding feature matching function ğ¾ (ğ‘ˆ , ğ¼ ) with the reference value of ğ¼ , ğ¾ ğ‘¢ * ,ğ‘– * . Formally, the NDE is given as:</p><formula xml:id="formula_11">ğ‘ ğ·ğ¸ = ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * ) -ğ‘Œ (ğ‘ˆ = ğ‘¢ * , ğ¼ = ğ‘– * , ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * ),</formula><p>where ğ‘¢ * and ğ‘– * denote the reference values of ğ‘ˆ and ğ¼ , which are typically set as the mean of the corresponding variables, i.e. the mean of user and item embeddings.</p><p>According to Equation 3, the TE from ğ¼ to ğ‘Œ can be written as:</p><formula xml:id="formula_12">ğ‘‡ ğ¸ = ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢,ğ‘– ) -ğ‘Œ (ğ‘ˆ = ğ‘¢ * , ğ¼ = ğ‘– * , ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * ).</formula><p>Accordingly, eliminating popularity bias can be realized by reducing ğ‘ ğ·ğ¸ from ğ‘‡ ğ¸, which is formulated as:  <ref type="formula" target="#formula_10">9</ref>. Recall that ğ‘‡ ğ¼ğ¸ = ğ‘‡ ğ¸ -ğ‘ ğ·ğ¸, the key difference of the proposed counterfactual inference and normal inference is using TIE to rank items rather than TE. Algorithm in Appendix A describes the procedure of our method.</p><formula xml:id="formula_13">ğ‘‡ ğ¸ -ğ‘ ğ·ğ¸ = ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢,ğ‘– ) -ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * ),<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>There are usually multiple causes for one item click, such as items' popularity, category, and quality. In this work, we focus on the bias revealed by the interaction frequency. As an initial attempt to solve the problem from the perspective of cause-effect, we ignoring the effect of other factors. Due to the unavailability of side information <ref type="bibr" target="#b38">[39]</ref> on such factors or the exposure mechanism to uncover different causes for the recommendation, it is also non-trivial to account for such factors.</p><p>As we can access such side information, we can simply extend the proposed MACR framework by incorporating such information into the causal graph as additional nodes. Then we can reveal the reasons that cause specific recommendations and try to further eliminate the bias, which is left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate the performance of our proposed MACR. Our experiments are intended to answer the following research questions:</p><p>â€¢ RQ1: Does MACR outperform existing debiasing methods? â€¢ RQ2: How do different hyper-parameter settings (e.g. ğ›¼, ğ›½, ğ‘) affect the recommendation performance? â€¢ RQ3: How do different components in our framework contribute to the performance? â€¢ RQ4: How does MACR eliminate the popularity bias?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>Datasets. Five real-world benchmark datasets are used in our experiments: ML10M is the widely-used <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59</ref>] dataset from MovieLens with 10M movie ratings. While it is an explicit feedback dataset, we have intentionally chosen it to investigate the performance of learning from the implicit signal. To this end, we transformed it into implicit data, where each entry is marked as 0 or 1 indicating whether the user has rated the item; Adressa <ref type="bibr" target="#b17">[18]</ref> and Globo <ref type="bibr" target="#b13">[14]</ref> are two popular datasets for news recommendation; Also, the datasets Gowalla and Yelp from LightGCN <ref type="bibr" target="#b19">[20]</ref> are used for a fair comparison. All the datasets above are publicly available and vary in terms of domain, size, and sparsity. The statistics of these datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Evaluation. Note that the conventional evaluation strategy on a set of holdout interactions does not reflect the ability to predict user's preference, as it still follows the long tail distribution <ref type="bibr" target="#b57">[58]</ref>. Consequently, the test model can still perform well even if it only considers popularity and ignores users' preference <ref type="bibr" target="#b57">[58]</ref>. Thus, the conventional evaluation strategy is not appropriate for testing whether the model suffers from popularity bias, and we need to evaluate on the debiased data. To this end, we follow previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58]</ref> to simulate debiased recommendation where the testing interactions are sampled to be a uniform distribution over items. In particular, we randomly sample 10% interactions with equal probability in terms of items as the test set, another 10% as the validation set, and leave the others as the biased training data <ref type="foot" target="#foot_0">1</ref> . We report the all-ranking performance w.r.t. three widely used metrics: Hit Ratio (HR), Recall, and Normalized Discounted Cumulative Gain (NDCG) cut at ğ¾. 4.1.1 Baselines. We implement our MACR with the classic MF (MACR_MF) and the state-of-the-art LightGCN (MACR_LightGCN) to explore how MACR boosts recommendation performance. We compare our methods with the following baselines:</p><p>â€¢ MF <ref type="bibr" target="#b27">[28]</ref>: This is a representative collaborative filtering model as formulated in Section 3.2. â€¢ LightGCN <ref type="bibr" target="#b19">[20]</ref>: This is the state-of-the-art collaborative filtering recommendation model based on light graph convolution as illustrated in Section 3.2. â€¢ ExpoMF <ref type="bibr" target="#b30">[31]</ref>: A probabilistic model that separately estimates the user preferences and the exposure. â€¢ CausE_MF, CausE_LightGCN <ref type="bibr" target="#b6">[7]</ref>: CausE is a domain adaptation algorithm that learns from debiased datasets to benefit the biased training. In our experiments, we separate the training set into debiased and biased ones to implement this method. Further, we apply CausE into two recommendation models (i.e. MF and LightGCN) for fair comparisons. Similar treatments are used for the following debias strategy. â€¢ BS_MF, BS_LightGCN <ref type="bibr" target="#b27">[28]</ref>: BS learns a biased score from the training stage and then remove the bias in the prediction in the testing stage. The prediction function is defined as: Å·ğ‘¢ğ‘– = Å·ğ‘˜ + ğ‘ ğ‘– , where ğ‘ ğ‘– is the bias term of the item ğ‘–. â€¢ Reg_MF, Reg_LightGCN <ref type="bibr" target="#b1">[2]</ref>: Reg is a regularization-based approach that intentionally downweights the short tail items, covers more items, and thus improves long tail recommendation. â€¢ IPW_MF, IPW_LightGCN: <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref> IPW Adds the standard Inverse Propensity Weight to reweight samples to alleviate item popularity bias. â€¢ DICE_MF, DICE_LightGCN: <ref type="bibr" target="#b57">[58]</ref> This is a state-of-the-art method for learning causal embedding to cope with popularity bias problem. It designs a framework with causal-specific data to disentangle interest and popularity into two sets of embedding. We used the code provided by its authors.</p><p>As we aim to model the interactions between users and items, we do not compare with models that use side information. We leave out the comparison with other collaborative filtering models, such as NeuMF <ref type="bibr" target="#b21">[22]</ref> and NGCF <ref type="bibr" target="#b48">[49]</ref>, because LightGCN <ref type="bibr" target="#b19">[20]</ref> is the stateof-the-art collaborative filtering method at present. Implementation details and detailed parameter settings of the models can be found in Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results (RQ1)</head><p>Table <ref type="table" target="#tab_2">2</ref> presents the recommendation performance of the compared methods in terms of HR@20, Recall@20, and NDCG@20. The boldface font denotes the winner in that column. Overall, our MACR consistently outperforms all compared methods on all datasets for all metrics. The main observations are as follows:</p><p>â€¢ In all cases, our MACR boosts MF or LightGCN by a large margin. Specifically, the average improvement of MACR_MF over MF on the five datasets is 153.13% in terms of HR@20 and the improvement of MACR_LightGCN over LightGCN is 241.98%, which are rather substantial. These impressive results demonstrate the effectiveness of our multi-task training schema and counterfactual reasoning, even if here we just use the simple item and user modules. MACR potentially can be further improved by designing more sophisticated models. â€¢ In most cases, LightGCN performs worse than MF, but in regular dataset splits, as reported in <ref type="bibr" target="#b19">[20]</ref>, LightGCN is usually a performing-better approach. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, with the same training set, we can see that the average recommendation frequency of popular items on LightGCN is visibly larger than MF. This result indicates that LightGCN is more vulnerable to popularity bias. The reason can be attributed to the embedding propagation operation in LightGCN, where the influence of popular items is spread on the user-item interaction graph which further amplifies the popularity bias. However, in our MACR framework, MACR_LightGCN performs better than MACR_MF. This indicates that our framework can substantially alleviate the popularity bias. â€¢ In terms of datasets, we can also find that the improvements over the Globo dataset are extremely large. This is because Globo is a large-scale news dataset, and the item popularity distribution is particularly skewed. Popular news in Globo is widely read, while some other unpopular news has almost no clicks. This result indicates our model's capability of addressing popularity bias, especially on long-tailed datasets. â€¢ As to baselines for popularity debias, Reg method <ref type="bibr" target="#b1">[2]</ref> have limited improvement over the basic models and even sometimes perform even worse. The reason is that Reg simply downweights popular items without considering their influence on each interaction.  CausE also performs badly sometimes as it relies on the debiased training set, which is usually relatively small and the model is hard to learn useful information from. BS and IPW methods can alleviate the bias issue to a certain degree. DICE achieved the best results among the baselines. This indicates the significance of considering popularity as a cause of interaction. In Appendix C.1, we also report our experimental results on Adressa dataset w.r.t. different values of ğ¾ in the metrics for more comprehensive evaluation. The hyper-parameter ğ‘ as formulated in Eq. ( <ref type="formula" target="#formula_10">9</ref>) controls the degree to which the intermediate matching preference is blocked in prediction. We conduct experiments on the Adressa dataset on MACR_LightGCN and MACR_MF and test their performance in terms of HR@20. As shown in Figure <ref type="figure" target="#fig_8">7</ref>, taking MACR_LightGCN as an instance, as ğ‘ varies from 0 to 29, the model performs increasingly better while further increasing ğ‘ is counterproductive. This illustrates that the proper degree of blocking intermediate matching preference benefits the popularity debias and improves the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>Compared with MACR_MF, MACR_LightGCN is more sensitive to ğ‘, as its performance drops more quickly after the optimum.     It indicates that LightGCN is more vulnerable to popularity bias, which is consistent with our findings in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effect of User Branch and Item Branch (RQ3</head><p>). Note that our MACR not only incorporates user/item's effect in the loss function but also fuse them in the predictions. To investigate the integral effects of user and item branch, we conduct ablation studies on MACR_MF on the Adressa dataset and remove different components at a time for comparisons. Specifically, we compare MACR with its four special cases: MACR_MF w/o user (item) branch, where user (or item) branch has been removed; MACR_MF w/o ğ¿ ğ¼ (ğ¿ ğ‘ˆ ), where we just simply remove ğ¿ ğ¼ (ğ¿ ğ‘ˆ ) to block the effect of user (or item) branch on training but retain their effect on prediction.</p><p>From Table <ref type="table" target="#tab_3">3</ref> we can find that both user branch and item branch boosts recommendation performance. Compared with removing the user branch, the model performs much worse when removing the item branch. Similarly, compared with removing ğ¿ ğ‘ˆ , removing ğ¿ ğ¼ also harms the performance more heavily. This result validates that item popularity bias has more influence than user conformity on the recommendation.</p><p>Moreover, compared with simply removing ğ¿ ğ¼ and ğ¿ ğ‘ˆ , removing the user/item branch makes the model perform much worse. This result validates the significance of further fusing the item and user influence in the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Debias Capability (RQ4).</head><p>We then investigate whether our model alleviates the popularity bias issue. We compare MACR_MF and MACR_LightGCN with their basic models, MF and LightGCN.    As shown in Figure <ref type="figure" target="#fig_11">8</ref>, we show the recommendation frequency of different item groups. We can see that our methods indeed reduce the recommendations frequency of popular items and recommend more items that are less popular. Then we conduct in Figure <ref type="figure" target="#fig_13">9</ref> an experiment to show the item recommendation recall in different item groups. In this experiment, we recommend each user 20 items and calculate the item recall. If an item appears ğ‘ times in the test data, its item recall is the proportion of it being accurately recommended to test users. We have the following findings.</p><p>â€¢ The most popular item group has the greatest recall increase, but our methods in Figure <ref type="figure" target="#fig_11">8</ref> show the recommendations frequency of popular items is reduced. It means that traditional recommender systems (MF, LightGCN) are prone to recommend more popular items to unrelated users due to popularity bias. In contrast, our MACR reduces the item's direct effect and recommends popular items mainly to suitable users. This confirms the importance of matching users and items for personalized recommendations rather than relying on item related bias. â€¢ The unpopular item group has relatively small improvement.</p><p>This improvement is mainly due to the fact that we recommend more unpopular items to users as shown in Figure <ref type="figure" target="#fig_11">8</ref>. Since these items rarely appear in the training set, it is difficult to obtain a comprehensive representation of these items, so it is difficult to gain a large improvement in our method.</p><p>To investigate why our framework benefits the debias in the recommendation, we explore what user branch and item branch, i.e., Å·ğ‘¢ and Å·ğ‘– , actually learn in the model. We compare ğœ ( Å·ğ‘¢ ) and ğœ ( Å·ğ‘– ) as formulated in Eq. <ref type="bibr" target="#b6">(7)</ref> , which is the output for the specific user ğ‘¢ or item ğ‘– from the user/item model after the sigmoid function, capturing user conformity and item popularity in the dataset. In Figure <ref type="figure" target="#fig_15">10</ref>, the background histograms indicate the proportion of users in each group involved in the dataset. The horizontal axis means the user groups with a certain number of interactions. The left vertical axis is the value of the background histograms, which corresponds to the users' proportion in the dataset. The right vertical axis is the value of the polyline, which corresponds to ğœ ( Å·ğ‘¢ ). All the values are the average values of the users in the groups. As we can see, with the increase of the occurrence frequency of users in the dataset, the sigmoid scores of them also increase. This indicates that the user's activity is consistent with his/her conformity level. A similar phenomenon can be observed in Figure <ref type="figure" target="#fig_17">11</ref> for different item groups. This shows our model's capability of capturing item popularity and users' conformity, thus benefiting the debias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we review existing work on Popularity Bias in Recommendation and Causal Inference in Recommendation, which are most relevant with this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Popularity Bias in Recommendation</head><p>Popularity bias is a common problem in recommender systems that popular items in the training dataset are frequently recommended. Researchers have explored many approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58]</ref> to analyzing and alleviating popularity bias in recommender systems. The first line of research is based on Inverse Propensity Weighting (IPW) <ref type="bibr" target="#b40">[41]</ref> that is described in the above section. The core idea of this approach is reweighting the interactions in the training loss. For example, Liang et al. <ref type="bibr" target="#b29">[30]</ref> propose to impose lower weights for popular items. Specifically, the weight is set as the inverse of item popularity. However, these previous methods ignore how popularity influence each specific interaction.</p><p>Another line of research tries to solve this problem through ranking adjustment. For instance, Abdollahpouri et al. <ref type="bibr" target="#b1">[2]</ref> propose a regularization-based approach that aims to improve the rank of long-tail items. Abdollahpouri et al. <ref type="bibr" target="#b2">[3]</ref> introduce a re-ranking approach that can be applied to the output of the recommender systems. These approaches result in a trade-off between the recommendation accuracy and the coverage of unpopular items. They typically suffer from accuracy drop due to pushing the recommender to the long-tail in a brute manner. Unlike the existing work, we explore to eliminate popularity bias from a novel cause-effect perspective. We propose to capture the popularity bias through a multi-task training schema and remove the bias via counterfactual inference in the prediction stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Causal Inference in Recommendation</head><p>Causal inference is the science of systematically analyzing the relationship between a cause and its effect <ref type="bibr" target="#b34">[35]</ref>. Recently, causal inference has gradually aroused people's attention and been exploited in a wide range of machine learning tasks, such as scene graph generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46]</ref>, visual explanations <ref type="bibr" target="#b31">[32]</ref>, vision-language multi-modal learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>, node classification <ref type="bibr" target="#b14">[15]</ref>, text classification <ref type="bibr" target="#b37">[38]</ref>, and natural language inference <ref type="bibr" target="#b15">[16]</ref>. The main purpose of introducing causal inference in recommender systems is to remove the bias <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>. We refer the readers to a systemic survey for more details <ref type="bibr" target="#b11">[12]</ref>.</p><p>Inverse Propensity Weighting. The first line of works is based on the Inverse Propensity Weighting (IPW). In <ref type="bibr" target="#b29">[30]</ref>, the authors propose a framework consisted of two models: one exposure model and one preference model. Once the exposure model is estimated, the preference model is fit with weighted click data, where each click is weighted by the inverse of exposure estimated in the first model and thus be used to alleviate popularity bias. Some very similar models were proposed in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Causality-oriented data. The second line of works is working on leveraging additional debiased data. In <ref type="bibr" target="#b6">[7]</ref>, they propose to create an debiased training dataset as an auxiliary task to help the model trained in the skew dataset generalize better, which can also be used to relieve the popularity bias. They regard the large sample of the dataset as biased feedback data and model the recommendation as a domain adaption problem. But we argue that their method does not explicitly remove popularity bias and does not perform well on normal datasets. Noted that all these methods are aimed to reduce the user exposure bias.</p><p>Causal embedding. Another series of work is based on the probability, in <ref type="bibr" target="#b30">[31]</ref>, the authors present ExpoMF, a probabilistic approach for collaborative filtering on implicit data that directly incorporates user exposure to items into collaborative filtering. ExpoMF jointly models both users' exposure to an item, and their resulting click decisions, resulting in a model which naturally down-weights the expected, but ultimately un-clicked items. The exposure is modeled as a latent variable and the model infers its value from data. The popularity of items can be added as an exposure covariate and thus be used to alleviate popularity bias. This kind of works is based on probability and thus cannot be generalized to more prevalent settings. Moreover, they ignore how popularity influences each specific interaction. Similar to our work, Zheng et al. <ref type="bibr" target="#b57">[58]</ref> also tries to mitigate popularity bias via causal approaches. The difference is that we analyze the causal relations in a fine-grained manner, consider the item popularity, user conformity and model their influence on recommendation. <ref type="bibr" target="#b57">[58]</ref> also lacks a systematic view of the mechanism of popularity bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we presented the first cause-effect view for alleviating popularity bias issue in recommender systems. We proposed the model-agnostic framework MACR which performs multi-task training according to the causal graph to assess the contribution of different causes on the ranking score. The counterfactual inference is performed to estimate the direct effect from item properties to the ranking score, which is removed to eliminate the popularity bias. Extensive experiments on five real-world recommendation datasets have demonstrated the effectiveness of MACR.</p><p>This work represents one of the initial attempts to exploit causal reasoning for recommendation and opens up new research possibilities. In the future, we will extend our cause-effect look to more applications in recommender systems and explore other designs of the user and item module so as to better capture user conformity and item popularity. Moreover, we would like to explore how to incorporate various side information <ref type="bibr" target="#b38">[39]</ref> and how our framework can be extended to alleviate other biases <ref type="bibr" target="#b11">[12]</ref> in recommender systems. In addition, we will study the simultaneous elimination of multiple types of biases such as popularity bias and exposure bias through counterfactual inference. Besides, we will explore the combination of causation and other relational domain knowledge <ref type="bibr" target="#b32">[33]</ref>.</p><p>(a) HR@K (b) NDCG@K (c) Recall@K Figure <ref type="figure" target="#fig_19">12</ref>: Top-K recommendation performance on Adressa datasets w.r.t. HR@K, NDCG@K and Recall@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A INFERENCE PROCEDURE</head><p>Algorithm 1 describes the procedure of our method and traditional recommendation system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS</head><p>We implement MACR in Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The embedding size is fixed to 64 for all models and the embedding parameters are initialized with the Xavier method <ref type="bibr" target="#b16">[17]</ref>. We optimize all models with Adam <ref type="bibr" target="#b26">[27]</ref> except for ExpoMF which is trained in a probabilistic manner as per the original paper <ref type="bibr" target="#b30">[31]</ref>. For all methods, we use the default learning rate of 0.001 and default mini-batch size of 1024 (on ML10M and Globo, we increase the mini-batch size to 8192 to speed up training). Also, we choose binarized cross-entropy loss for all models for a fair comparison. For the LightGCN model, we utilize two layers of graph convolution network to obtain the best results. For the Reg model, the coefficient for the item-based regularization is set to 1e-4 because it works best. For DICE, we keep all the optimal setting in their paper except replacing the regularization term ğ¿ ğ‘‘ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘ğ‘ğ‘›ğ‘ğ‘¦ from ğ‘‘ğ¶ğ‘œğ‘Ÿ with another option -ğ¿2. Because with our large-scale dataset, computing ğ‘‘ğ¶ğ‘œğ‘Ÿ will be out of memory for the 2080Ti GPU. It is also suggested in their paper. For ExpoMF, the initial value of ğœ‡ is tuned in the range of {0.1, 0.05, 0.01, 0.005, 0.001} as suggested by the author. For CausE, as their model training needs one biased dataset and another debiased dataset, we split 10% of the train data as we mentioned in Section 4.1 to build an additional debiased dataset for it. For our MACR_MF and MACR_LightGCN, the trade-off parameters ğ›¼ and ğ›½ in Eq. ( <ref type="formula" target="#formula_8">8</ref>) are both searched in the range of {1ğ‘’ -5, 1ğ‘’ -4, 1ğ‘’ -3, 1ğ‘’ -2} and set to 1e-3 by default. The ğ‘ in Eq. ( <ref type="formula" target="#formula_10">9</ref>) is tuned in the range of {20, 22, ..., 40}. The number  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Effect of hyper-parameters</head><p>As formulated in the loss function Eq. ( <ref type="formula" target="#formula_8">8</ref>), ğ›¼ is the trade-off hyperparameter which balances the contribution of the recommendation model loss and the item model loss while ğ›½ is to balance the recommendation model loss and the user loss. To investigate the benefit of item loss and user loss, we conduct experiments of MACR_MF on the typical Adressa dataset with varying ğ›¼ and ğ›½ respectively.</p><p>In particular, we search their values in the range of {1e-5, 1e-4, 1e-3, 1e-2}. When varying one parameter, the other is set as constant 1e-3. From Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table" target="#tab_5">5</ref> we have the following findings:</p><p>â€¢ As ğ›¼ increases from 1e-5 to 1e-3, the performance of MACR will become better. This result indicates the importance of capturing item popularity bias. A similar trend can be observed by varying ğ›½ from 1e-5 to 1e-3 and it demonstrates the benefit of capturing users' conformity. â€¢ However, when ğ›¼ or ğ›½ surpasses a threshold (1e-3), the performance becomes worse with a further increase of the parameters. As parameters become further larger, the training of the recommendation model will be less important, which brings the worse results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of popularity bias in recommender system. Items are organized into groups w.r.t. the popularity in the training set wherein the background histograms indicate the ratio of items in each group, and the vertical axis represents the average recommendation frequency.</figDesc><graphic coords="1,341.98,196.11,192.19,116.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal graph for (a) user-item matching; (b) incorporating item popularity; and (c) incorporating user conformity. I: item. U: user. K: matching features between user and item. Y: ranking score (e.g., the probability of interaction).</figDesc><graphic coords="2,67.53,88.67,66.45,66.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of counterfactual inference.</figDesc><graphic coords="2,320.36,83.69,235.42,72.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of causal graph where I, Y, and K denote cause, effect and mediator variable, respectively. Gray nodes mean the variables are at reference status (e.g., ğ¼ = ğ‘– * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The framework of MACR. The orange rectangles denote the main branch, i.e., the conventional recommender system. The blue and green rectangles denote the user and item modules, respectively.</figDesc><graphic coords="4,56.20,83.69,235.44,185.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Real world (b) Conterfactual world</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between real world and counterfactual world causal graphs in recommender systems.</figDesc><graphic coords="4,346.12,88.67,88.60,88.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of ğ‘ on MACR_LightGCN and MACR_MF w.r.t HR@20.</figDesc><graphic coords="6,325.09,290.63,109.87,76.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4. 3 . 1</head><label>31</label><figDesc>Effect of Hyper-parameters (RQ2). Our framework has three important hyper-parameters, ğ›¼, ğ›½, and ğ‘. Due to space limitation, we provide the results of parameter sensitivity analysis of ğ›¼, ğ›½ in Appendix C.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Frequency of different item groups recommended by LightGCN (MF) and MACR_LightGCN (MACR_MF).</figDesc><graphic coords="7,176.16,307.23,106.27,79.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average item recall in different item groups on Adressa. It indicates that LightGCN is more vulnerable to popularity bias, which is consistent with our findings in Section 4.2.</figDesc><graphic coords="7,63.17,307.10,106.27,80.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Average ğœ ( Å·ğ‘¢ ) comparison for different user groups on Adressa.</figDesc><graphic coords="7,442.33,207.42,107.18,79.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average ğœ ( Å·ğ‘– ) comparison for different item groups on Adressa.</figDesc><graphic coords="7,324.41,207.42,111.20,79.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Algorithm 1</head><label>1</label><figDesc>Inference Input: Backbone recommender ğ‘Œ ğ‘˜ , Item module ğ‘Œ ğ‘– , User module ğ‘Œ ğ‘¢ , User ğ‘¢, Item ğ‘–, Reference status ğ‘. Output: Å·ğ‘¢ğ‘– 1: /* Model Agnostic Counterfactual Reasoning */ 2: Å·ğ‘˜ = ğ‘Œ ğ‘˜ (ğ¾ (ğ‘¢, ğ‘–)); 3: Å·ğ‘– = ğ‘Œ ğ‘– (ğ‘–); 4: Å·ğ‘¢ = ğ‘Œ ğ‘– (ğ‘¢); 5: if ğ¼ğ‘ _ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘” then 6: Å·ğ‘¢ğ‘– = Å·ğ‘˜ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ); 7: else 8: Å·ğ‘¢ğ‘– = Å·ğ‘˜ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ) -ğ‘ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ); 9: end if 10: /* Traditional Recommender */ 11: Å·ğ‘¢ğ‘– = ğ‘Œ ğ‘˜ (ğ¾ (ğ‘¢, ğ‘–));</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 reports</head><label>12</label><figDesc>Figure 12  reports our experimental results on Adressa dataset w.r.t. HR@K, NDCG@K and Recall@K where ğ¾ = {1, 5, 10, 15, 20}. It shows the effectiveness of MACR which can improve MF and Light-GCN on different metrics with a large margin. Due to space limitation, we show the results on the Adressa dataset only, and the results on the other four datasets show the same trend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of five different datasets.Recall that the ranking score is calculated according to Equation7.As such, we have ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢,ğ‘– ) = Å·ğ‘˜ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ) and ğ‘Œ (ğ‘ˆ = ğ‘¢, ğ¼ = ğ‘–, ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * ) = ğ‘ * ğœ ( Å·ğ‘– ) * ğœ ( Å·ğ‘¢ ) where ğ‘ denotes the value Å·ğ‘˜ with ğ¾ = ğ¾ ğ‘¢ * ,ğ‘– * . In this way, we obtain the ranking schema for the testing stage as Equation</figDesc><table><row><cell></cell><cell>Users</cell><cell cols="3">Items Interactions Sparsity</cell></row><row><cell cols="2">Adressa 13,485</cell><cell>744</cell><cell>116,321</cell><cell>0.011594</cell></row><row><cell>Globo</cell><cell cols="3">158,323 12,005 2,520,171</cell><cell>0.001326</cell></row><row><cell cols="2">ML10M 69,166</cell><cell>8,790</cell><cell>5,000,415</cell><cell>0.008225</cell></row><row><cell>Yelp</cell><cell>31,668</cell><cell cols="2">38,048 1,561,406</cell><cell>0.001300</cell></row><row><cell cols="2">Gowalla 29,858</cell><cell cols="2">40,981 1,027,370</cell><cell>0.000840</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance evaluation of the compared methods with ğ¾ = 20. Rec means Recall. The bold-face font denotes the winner in that column. Note that the improvement achieved by MACR is significant (ğ‘-value &lt;&lt; 0.05).</figDesc><table><row><cell></cell><cell></cell><cell>Adressa</cell><cell></cell><cell></cell><cell>Globo</cell><cell></cell><cell></cell><cell>ML10M</cell><cell></cell><cell></cell><cell>Yelp2018</cell><cell></cell><cell></cell><cell>Gowalla</cell></row><row><cell></cell><cell>HR</cell><cell>Rec</cell><cell>NDCG</cell><cell>HR</cell><cell>Rec</cell><cell>NDCG</cell><cell>HR</cell><cell>Rec</cell><cell>NDCG</cell><cell>HR</cell><cell>Rec</cell><cell>NDCG</cell><cell>HR</cell><cell>Rec</cell><cell>NDCG</cell></row><row><cell>MF</cell><cell cols="3">0.111 0.085 0.034</cell><cell cols="3">0.020 0.003 0.002</cell><cell cols="3">0.058 0.009 0.008</cell><cell cols="3">0.071 0.006 0.009</cell><cell cols="3">0.174 0.046 0.032</cell></row><row><cell>ExpoMF</cell><cell cols="3">0.112 0.090 0.037</cell><cell cols="3">0.022 0.005 0.003</cell><cell cols="3">0.061 0.009 0.008</cell><cell cols="3">0.071 0.006 0.009</cell><cell cols="3">0.175 0.048 0.034</cell></row><row><cell>CausE_MF</cell><cell cols="3">0.112 0.084 0.037</cell><cell cols="3">0.023 0.005 0.003</cell><cell cols="3">0.054 0.008 0.007</cell><cell cols="3">0.066 0.005 0.008</cell><cell cols="3">0.166 0.045 0.032</cell></row><row><cell>BS_MF</cell><cell cols="3">0.113 0.090 0.038</cell><cell cols="3">0.021 0.005 0.003</cell><cell cols="3">0.060 0.009 0.008</cell><cell cols="3">0.071 0.006 0.010</cell><cell cols="3">0.175 0.046 0.033</cell></row><row><cell>Reg_MF</cell><cell cols="3">0.093 0.066 0.033</cell><cell cols="3">0.019 0.003 0.002</cell><cell cols="3">0.051 0.009 0.007</cell><cell cols="3">0.064 0.005 0.008</cell><cell cols="3">0.161 0.044 0.030</cell></row><row><cell>IPW_MF</cell><cell cols="3">0.128 0.096 0.039</cell><cell cols="3">0.021 0.004 0.003</cell><cell cols="3">0.041 0.006 0.005</cell><cell cols="3">0.072 0.006 0.010</cell><cell cols="3">0.174 0.048 0.033</cell></row><row><cell>DICE_MF</cell><cell cols="3">0.133 0.098 0.041</cell><cell cols="3">0.033 0.007 0.006</cell><cell cols="3">0.055 0.011 0.007</cell><cell cols="3">0.082 0.008 0.011</cell><cell cols="3">0.177 0.052 0.033</cell></row><row><cell>MACR_MF</cell><cell cols="3">0.140 0.109 0.050</cell><cell cols="3">0.112 0.046 0.026</cell><cell cols="3">0.140 0.041 0.024</cell><cell cols="3">0.135 0.026 0.019</cell><cell cols="3">0.252 0.077 0.050</cell></row><row><cell>LightGCN</cell><cell cols="3">0.123 0.098 0.040</cell><cell cols="3">0.017 0.005 0.003</cell><cell cols="3">0.038 0.006 0.005</cell><cell cols="3">0.061 0.004 0.009</cell><cell cols="3">0.172 0.045 0.032</cell></row><row><cell cols="4">CausE_LightGCN 0.115 0.082 0.037</cell><cell cols="3">0.014 0.005 0.003</cell><cell cols="3">0.036 0.005 0.005</cell><cell cols="3">0.061 0.005 0.009</cell><cell cols="3">0.173 0.046 0.033</cell></row><row><cell>BS_LightGCN</cell><cell cols="3">0.139 0.109 0.047</cell><cell cols="3">0.023 0.005 0.004</cell><cell cols="3">0.038 0.006 0.005</cell><cell cols="3">0.061 0.005 0.009</cell><cell cols="3">0.178 0.048 0.035</cell></row><row><cell>Reg_LightGCN</cell><cell cols="3">0.127 0.098 0.039</cell><cell cols="3">0.016 0.005 0.003</cell><cell cols="3">0.035 0.005 0.005</cell><cell cols="3">0.058 0.004 0.008</cell><cell cols="3">0.165 0.045 0.030</cell></row><row><cell>IPW_LightGCN</cell><cell cols="3">0.139 0.107 0.047</cell><cell cols="3">0.018 0.005 0.003</cell><cell cols="3">0.037 0.006 0.005</cell><cell cols="3">0.071 0.005 0.009</cell><cell cols="3">0.174 0.045 0.032</cell></row><row><cell>DICE_LightGCN</cell><cell cols="3">0.141 0.111 0.046</cell><cell cols="3">0.046 0.012 0.008</cell><cell cols="3">0.062 0.014 0.009</cell><cell cols="3">0.093 0.012 0.013</cell><cell cols="3">0.185 0.054 0.036</cell></row><row><cell cols="4">MACR_LightGCN 0.158 0.127 0.052</cell><cell cols="3">0.132 0.059 0.030</cell><cell cols="3">0.155 0.049 0.029</cell><cell cols="3">0.148 0.031 0.018</cell><cell cols="3">0.254 0.077 0.051</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of user and item branch on MACR_MF.</figDesc><table><row><cell></cell><cell cols="3">HR@20 Recall@20 NDCG@20</cell></row><row><cell>MACR_MF</cell><cell>0.140</cell><cell>0.109</cell><cell>0.050</cell></row><row><cell cols="2">MACR_MF w/o user branch 0.137</cell><cell>0.106</cell><cell>0.046</cell></row><row><cell cols="2">MACR_MF w/o item branch 0.116</cell><cell>0.089</cell><cell>0.038</cell></row><row><cell>MACR_MF w/o ğ¿ ğ¼</cell><cell>0.124</cell><cell>0.096</cell><cell>0.043</cell></row><row><cell>MACR_MF w/o ğ¿ ğ‘ˆ</cell><cell>0.138</cell><cell>0.108</cell><cell>0.048</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of ğ›¼ on MACR_MF.</figDesc><table><row><cell cols="3">HR@20 Recall@20 NDCG@20</cell></row><row><cell>1e-5 0.133</cell><cell>0.104</cell><cell>0.045</cell></row><row><cell>1e-4 0.139</cell><cell>0.108</cell><cell>0.049</cell></row><row><cell>1e-3 0.140</cell><cell>0.109</cell><cell>0.050</cell></row><row><cell>1e-2 0.137</cell><cell>0.108</cell><cell>0.048</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of ğ›½ on MACR_MF.</figDesc><table><row><cell cols="3">HR@20 Recall@20 NDCG@20</cell></row><row><cell>1e-5 0.139</cell><cell>0.108</cell><cell>0.049</cell></row><row><cell>1e-4 0.139</cell><cell>0.109</cell><cell>0.049</cell></row><row><cell>1e-3 0.140</cell><cell>0.109</cell><cell>0.050</cell></row><row><cell>1e-2 0.139</cell><cell>0.108</cell><cell>0.049</cell></row><row><cell cols="3">of training epochs is fixed to 1000. The L2 regularization coefficient</cell></row><row><cell>is set to 1e-5 by default.</cell><cell></cell><cell></cell></row><row><cell cols="3">C SUPPLEMENTARY EXPERIMENTS</cell></row><row><cell cols="2">C.1 Metrics with different Ks</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We refer to<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58]</ref> for details on extracting an debiased test set from biased data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">U19A2079</rs>, <rs type="grantNumber">61972372</rs>) and <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2020AAA0106000</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GzUdvuR">
					<idno type="grant-number">U19A2079</idno>
				</org>
				<org type="funding" xml:id="_Cq2Qw6e">
					<idno type="grant-number">61972372</idno>
				</org>
				<org type="funding" xml:id="_NHYMnsH">
					<idno type="grant-number">2020AAA0106000</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">MartÃ­n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Controlling popularity bias in learning-to-rank recommendation</title>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="42" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Managing popularity bias in recommender systems with personalized re-ranking</title>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In FLAIRS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A general framework for counterfactual learning-to-rank</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical biases in Information Retrieval metrics for recommender systems</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>BellogÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">IvÃ¡n</forename><surname>Cantador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr. J</title>
		<imprint>
			<biblScope unit="page" from="606" to="634" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal embeddings for recommendation</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>QuiÃ±onero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<editor>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Snelson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic reformulation of memory-based collaborative filtering: Implications on popularity biases</title>
		<author>
			<persName><forename type="first">RocÃ­o</forename><surname>CaÃ±amares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Should I follow the crowd? A probabilistic analysis of the effectiveness of popularity in recommender systems</title>
		<author>
			<persName><forename type="first">RocÃ­o</forename><surname>CaÃ±amares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">E</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><surname>Engelhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03240</idno>
		<title level="m">Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4613" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">News session-based recommendations using deep neural networks</title>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Gabriel De Souza Pereira Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adilson Marques Da</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><surname>Cunha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xiangnan He, Xin Xin, Qifan Wang, and Tat-Seng Chua Chua. 2021. Should Graph Convolution Trust Neighbors? A Simple Causal Inference Method</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empowering Language Understanding with Counterfactual Reasoning</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ã–zlem Ã–zgÃ¶bek, and Xiaomeng Su. 2017. The Adressa dataset for news recommendation</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Atle Gulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<idno>WI. 1042-1048</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trustsvd: Collaborative filtering with both the explicit and implicit influence of user trust and of item ratings</title>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="123" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Outer product-based neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How Sensitive is Recommendation Systems&apos; Offline Evaluation to Popularity?</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Jadidinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In REVEAL Workshop at RecSys</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What recommenders recommend: an analysis of recommendation biases and possible countermeasures</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Lerche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Kamehkhosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jugovac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Model User-adapt Interact</title>
		<imprint>
			<biblScope unit="page" from="427" to="491" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Degenerate feedback loops in recommender systems</title>
		<author>
			<persName><forename type="first">Ray</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AndrÃ¡s</forename><surname>GyÃ¶rgy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIES</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fism: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WSDM. 304- 312</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal inference for recommendation</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at UAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explaining Visual Models by Causal Attribution</title>
		<author>
			<persName><forename type="first">Ãlvaro</forename><surname>Parafita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MartÃ­nez</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>VitriÃ </surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marca</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4167" to="4175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale question tagging via joint question-topic embedding learning</title>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOIS</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Counterfactual VQA: A Cause-Effect Look at Language Bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge Uiversity Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Matthew effect in empirical data</title>
		<author>
			<persName><forename type="first">MatjaÅ¾</forename><surname>Perc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J R Soc Interface</title>
		<imprint>
			<date type="published" when="2014">2014. 2014. 20140378</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10860" to="10869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Counterfactual Inference for Text Classification Debiasing</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01395</idno>
		<title level="m">On the difficulty of evaluating baselines: A study on recommender systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Implicit user modeling for personalized search</title>
		<author>
			<persName><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Debiasing the human-recommender system feedback loop in collaborative filtering</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Khenissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olfa</forename><surname>Nasraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Shafto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of WWW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="645" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conversational recommender system</title>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10760" to="10770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Click&quot; Is Not Equal to&quot; Like&quot;: Counterfactual Recommendation for Mitigating Clickbait Issue</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The deconfounded recommender: A causal inference approach to recommendation</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06581</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast Adaptation for Cold-Start Collaborative Filtering with Meta-Learning</title>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A neural influence diffusion model for social recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learning for Matching in Search and Recommendation. Found. Trends Inf. Ret</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="102" to="288" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep Matrix Factorization Models for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hong-Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DeVLBert: Learning Deconfounded Visio-Linguistic Representations</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4373" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Causal Intervention for Leveraging Popularity Bias in Recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Disentangling User Interest and Conformity for Recommendation with Causal Embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Depeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Neural Autoregressive Approach to Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
