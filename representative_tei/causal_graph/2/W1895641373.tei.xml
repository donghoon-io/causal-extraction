<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Video and Text Parsing for Understanding Events and Answering Queries</title>
				<funder ref="#_Hn5CxEq">
					<orgName type="full">NSF CDI</orgName>
				</funder>
				<funder ref="#_wXpxTYE">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_DKbPxZu #_vTYvxVt">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Tae</roleName><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eun</forename><surname>Choe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song-Chun</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Video and Text Parsing for Understanding Events and Answering Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Joint video and text parsing</term>
					<term>Knowledge representation</term>
					<term>And-Or graph</term>
					<term>Event understanding</term>
					<term>Query answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against groundtruth as well as accuracy of query answering and obtained satisfactory results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION 1.Motivation and objective</head><p>Video understanding aims at automated recognition of the objects, scenes and events in the video. It is a crucial component in systems that improve human-computer interaction, such as (a) automatic text generation from videos to facilitate search and retrieval of visual information from the web, and (b) surveillance systems that answer human queries of who, what, when, where and why. However, understanding contents from video alone has been found to be challenging because of factors such as low resolution, deformation and occlusion; in addition, some aspects of objects and events are hidden, for example, a person's intentions and goals.</p><p>A significant portion of videos are accompanied by textual descriptions, which provide supplementary information for video understanding. as a few lines of on-screen text summarizing the news, which often have the least overlap. In all these cases, textual descriptions not only can help reduce the ambiguities in understanding the video, but may also provide additional information not observable in the video. In light of these benefits, there have been growing interests in processing video and text jointly to obtain a more accurate and comprehensive interpretation.</p><p>In contrast to the bag-of-words representation widely used in the video understanding and multimedia literature (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>), in this work we propose to represent the joint interpretation of video and text in the form of a parse graph. A parse graph is a labeled directed graph that can be seen as an extension of the constituency-based parse tree used in natural language syntactic parsing <ref type="bibr" target="#b4">[5]</ref>. It has been employed in computer vision to model objects <ref type="bibr" target="#b5">[6]</ref>, scenes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and events <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. A node in a parse graph represents an entity that can be an object, an event or a status of an object (i.e., fluent). An edge in a parse graph represents a relation between two entities. Together they represent the compositional structures of spatial information (for objects and scenes), temporal information (for actions and events) and causal information (causalities between events and object statuses). Such an explicit semantic representation facilitates subsequent applications such as text generation and query answering.</p><p>In order to handle different levels of overlap in the video and text, joint parsing must overcome the following challenges.</p><p>• When overlap exists between video and text, we need to solve the coreference problem, i.e., identifying the correspondence between the entities and relations observed in the video and those described in the text. For example, we may need to identify that a person detected in the video and a name mentioned in the text correspond to the same person. This problem can be complicated when multiple entities or relations of the same type appear in the same scene. • When there is little overlap between video and text, we need to fill in additional information to construct a coherent and comprehensive joint interpretation. For example, if the video shows a person waiting at a food truck and the text mentions a person buying food, then we need to infer that waiting at a food truck is typically followed by buying food in order to connect the video and text. • Occasionally, the content detected from the video may conflict with the content of the text, because of either erroneous parsing or inaccurate input. We need to resolve the conflicts and provide a consistent joint interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview of our work</head><p>We focus on joint parsing of surveillance videos and narrative text descriptions, but our framework can be employed to process other types of video-text data as well. Figure <ref type="figure" target="#fig_1">2</ref> shows our probabilistic generative model. Let vid and txt denote the input video and text respectively, pg vid the parse graph that interprets the input video, pg txt the parse graph that interprets the input text, and pg jnt the joint parse graph for both the video and the text. Our goal can be formulated as optimizing the posterior probability of the three parse graphs pg jnt , pg vid and pg txt given the input video vid and text txt.</p><p>P (pg jnt , pg vid , pg txt |vid, txt)</p><p>(1) ∝ P (pg jnt )P (pg vid , pg txt |pg jnt )P (vid|pg vid )P (txt|pg txt )</p><p>Our system for optimizing this posterior probability can be divided into three modules. First, a video parser generating candidate video parse graphs pg vid from the input video vid. The video parser encodes the conditional probability P (vid|pg vid ). It is also guided by a spatial-temporal-causal And-Or graph (S/T/C-AOG) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which models the compositional structures of objects, scenes and events. Second, a text semantic parser parsing the text descriptions txt into the text parse graph pg txt . We build our semantic parser on top of the Stanford Lexicalized Parser <ref type="bibr" target="#b10">[11]</ref>, focusing on parsing narrative event descriptions to extract information about objects, events, fluent changes and the relations between these entities. The text parser specifies the conditional probability P (txt|pg txt ). Third, a joint inference module producing a joint parse graph pg jnt based on the video and text parse graphs produced by the first two modules. The joint inference takes into account 1) the relationships between the joint parse graph and the video and text parse graphs, which are encoded in the conditional probability P (pg vid , pg txt |pg jnt ), and 2) the compatibility of the joint parse graph with the background knowledge represented in the S/T/C-AOG, as encoded in the prior probability P (pg jnt ). We employ three types of operators in constructing the joint parse graph:</p><p>• Solving the coreference problem by matching the video and text parse graphs • Filling in the potential gap between video and text by deduction.</p><p>• Resolving possible conflicts via revisions to the video and text parse graphs.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> show two examples of joint parsing from videos and texts with different levels of overlap. In each parse graph shown in the figures, we use different shapes to represent objects, events and fluents, and use different edge colors to represent different relations; the label on a node or edge indicates the semantic type of the denoted entity or relation. For each event node, the Agent relation connects it to the action initiator, and the Patient relation connects it to the action target. The set of event nodes are laid out horizontally based on their occurring time and vertically based on their compositional complexity. For a fluent node, we use levels to indicate its value at each time point. Figure <ref type="figure" target="#fig_2">3(a)</ref> shows a surveillance video and the human intelligence description of a parking lot scene, in which the video and text cover the same event and have significant overlap. Figure <ref type="figure" target="#fig_2">3(b)</ref> shows the results of video parsing, text parsing and joint inference. The video and text parses reinforce each other in the joint parse via overlapping (e.g., the stopping event, the building), and they also supplement each other by providing additional information (e.g., the taking-picture event is too subtle to recognize from the video but is provided in the text). Figure <ref type="figure" target="#fig_3">4</ref>(a) shows a surveillance video and text description of a courtyard scene. The video shows a few people waiting and taking turns, but their target is not recognized by the video parser; the text only mentions a food truck. The video and text parses have no direct overlap in terms of object entities, but these objects are involved in a common food purchase event. Figure <ref type="figure" target="#fig_3">4(b)</ref> shows the results of video parsing, text parsing and joint inference. The joint parse graph identifies the food truck as the target of people's actions and infers the types of the events as well as the fluent changes resulting from the events, thus providing a more coherent and comprehensive interpretation of the scene.</p><p>We encode the prior knowledge of parse graphs in the S/T/C-AOG and use it to guide both video parsing and joint inference, which is another contribution of this work. An AOG is an extension of a constituency grammar used in natural language parsing <ref type="bibr" target="#b4">[5]</ref> to represent hierarchical compositions of objects, scenes and events. An AOG has a hierarchical structure with alternating layers of And-nodes and Or-nodes. An And-node represents the configuration of a set of sub-entities to form a composite entity (e.g., waiting followed by ordering composes a buying event). An Or-node represents the set of alternative compositional configurations of an entity (e.g., a buying event can be either "waiting and ordering" or "ordering without waiting").</p><p>• A spatial And-Or graph (S-AOG) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> models the spatial decompositions of objects and scenes. • A temporal And-Or graph (T-AOG) <ref type="bibr" target="#b8">[9]</ref> models the temporal decompositions of events to sub-events and atomic actions.</p><p>• A causal And-Or graph (C-AOG) <ref type="bibr" target="#b9">[10]</ref> models the causal decompositions of events and fluent changes. These three types of AOGs are interconnected and together they form a joint S/T/C-AOG. A parse graph is an instance of an AOG that selects only one of the alternative configurations at each Or-node of the AOG. The prior probability of a parse graph is specified by the energy of configuration selections made at the Or-nodes of the AOG as well as the energy of the compositional configurations at the And-nodes of the AOG contained in the parse graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Evaluation</head><p>To empirically evaluate our system, we collected two datasets, each containing surveillance videos of daily activities in indoor or outdoor scenes and a set of text descriptions provided by five human subjects. We asked each human subject to provide text descriptions at three levels of details, so we can study how the amount of text descriptions and their degrees of overlap with the video content can impact the performance of joint parsing. The joint parse graphs produced by our system were then evaluated using two different approaches.</p><p>In the first approach, we compare the joint parse graphs against the ground truth parse graphs which were constructed based on the merged parses of all the text descriptions from all the human subjects. We measure the precision and recall where precision is the percentage of the joint parse graph that is correct (i.e., contained in the ground truth) and recall is the percentage of the ground truth that is covered by the joint parse graph.</p><p>One important application of video understanding is to answer queries from human users. Therefore, in the second evaluation approach, we measure the accuracy of a query answering system based on the joint parse graph produced by our system. We collected a set of natural language queries that mimics the natural humancomputer interaction and includes questions in the forms of who, what, when, where and why. The answers produced by the system were then compared against the correct answers provided by human subjects.</p><p>The evaluation results show that the joint parse graphs provide more accurate and comprehensive interpretations of the scenes than the video and text parse graphs. In addition, we find that additional text input improves the joint parse graphs but with diminishing return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1">Bag-of-words based approaches</head><p>There has been a large body of work in the areas of multimedia, computer vision and machine learning on joint processing of video/image and text. The majority of the existing work is based on the bag-of-words (BoW) representation for both video/image and text. In the BoW representation, a text is represented as a set of  unordered words; and a video/image is represented as a set of unordered visual words, each of which is a feature vector (e.g., SIFT and HoG) of an image patch. The BoW representation is easy to extract and process, but it ignores the compositional structures based on the words. Therefore, BoW-based approaches typically perform coarse-level processing of the video/image and text, e.g., classification or retrieval, instead of producing a full interpretation of the input video/image and text. For example, for joint video and text processing, Feng et al. <ref type="bibr" target="#b0">[1]</ref> applied a joint model of video/image and text for both automatic image annotation and retrieval; Iyengar et al. <ref type="bibr" target="#b11">[12]</ref> modeled videos and text jointly for the purpose of multimedia information retrieval; Yang et al. <ref type="bibr" target="#b12">[13]</ref> addressed the problem of video classification by joint modeling of videos and transcript texts; Xu et al. <ref type="bibr" target="#b13">[14]</ref> performed sports video semantic event detection based on joint analysis of videos and webcast texts. For joint processing of image and text, Paek et al. <ref type="bibr" target="#b14">[15]</ref> performed scene classification with a TF-IDF based approach applied to both images and the accompanying text; Barnard et al. <ref type="bibr" target="#b1">[2]</ref> and Blei <ref type="bibr" target="#b15">[16]</ref> discussed a series of probabilistic models to capture the dependencies between image elements and words; Berg and Forsyth <ref type="bibr" target="#b16">[17]</ref> demonstrated a method for identifying images containing categories of animals based on word and visual cues; Monay and Gatica-Perez <ref type="bibr" target="#b2">[3]</ref> applied the probabilistic latent semantic analysis model for annotated images to improve automatic image indexing; Wang et al. <ref type="bibr" target="#b3">[4]</ref> simultaneously modeled the image elements, text annotations and image class labels using a probabilistic topic model; Liu et al. <ref type="bibr" target="#b17">[18]</ref> presented an image retrieval system by leveraging large-scale web image and their associated textual descriptions; Jia et al. <ref type="bibr" target="#b18">[19]</ref> proposed a probabilistic model that encodes relations between loosely related images and text descriptions; Feng and Lapata <ref type="bibr" target="#b19">[20]</ref> learned a probabilistic topic model of visual and textual words for the purpose of automatic image caption generation. In contrast to the existing work, our work aims at deep semantic parsing of video and text, i.e., we try to identify the compositional structures contained in the video and text, based on which we extract and explicitly represent a full interpretation of the input video and text. Our scope of parsing includes spatial parsing (of objects and scenes), temporal parsing (of actions and events) and causal parsing (of events and fluents), some of which have rarely been explored in the previous work of joint processing of video/image and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">Parsing based approaches</head><p>Our work is built on the recent advancement on image/video parsing, which identifies the hierarchical compositional structures contained in the image or video. In particular, spatial parsing identifies the spatial compositional structures of objects and scenes <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>; temporal parsing recognizes the temporal compositional structures of events <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>; and causal parsing detects the causal relations between events and fluent changes <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Our work also involves semantic parsing of text descriptions, which is related to a variety of semantic parsing approaches studied in the natural language processing community, for example, a supervised approach <ref type="bibr" target="#b34">[35]</ref>, an unsupervised approach <ref type="bibr" target="#b35">[36]</ref>, and a dependency based approach <ref type="bibr" target="#b36">[37]</ref>. Semantic text parsing is typically built on top of a syntactic parser such as the Stanford Lexicalized Parser <ref type="bibr" target="#b10">[11]</ref>.</p><p>For joint parsing of video and text, Hobbs and Mulkar-Mehta <ref type="bibr" target="#b37">[38]</ref> described a preliminary framework that interprets the combination of video and text from news broadcasts, which is the work most closely related to ours. Compared with their work, our system extracts information directly from the raw data of video and text, and relies on a probabilistic generative model to handle uncertainty; besides, we have done comprehensive experiments to evaluate our system. The graph-based referential grounding approach proposed by Liu et al. <ref type="bibr" target="#b38">[39]</ref> is also related to our work, but they focused on spatial relations between objects while we study spatialtemporal-causal parsing of objects, scenes and events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Contributions</head><p>The contributions of our paper include three aspects:</p><p>• Although there has been a lot of previous work in video parsing and text semantic parsing, our work is the first to jointly parse video and text for more accurate and comprehensive parsing results. Our work of joint semantic parsing also goes beyond the traditional bag-of-words approaches to joint processing of video and text. Fig. <ref type="figure">5</ref>. The taxonomy in an example ontology for the courtyard scene. The symbol at the root node represents the universal type, which is a supertype of every other type.</p><p>of joint parsing in natural language query answering. Section 7 presents our experiments. Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REPRESENTATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ontology</head><p>An ontology is a "formal, explicit specification of a shared conceptualisation" <ref type="bibr" target="#b39">[40]</ref>. We use an ontology to specify the types of entities and relations that may appear in the parse graphs and therefore define the scope of our study. This ontology can be manually constructed using an ontology editor (e.g., <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>), adapted from an existing ontology (e.g., <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>), or automatically or semi-automatically learned from data (e.g., <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>).</p><p>As typical for an ontology, we organize the types of entities (i.e., concepts) into a taxonomy. A taxonomy is a directed acyclic graph in which a directed edge between two concepts denotes a supertype-subtype relationship. As will be shown in section 5, we rely on the taxonomy to compute semantic distances between concepts when doing joint inference. We divide the concepts into three main categories at the top level of the taxonomy.</p><p>• Object, which represents any physical objects including human. • Event, which includes any basic actions (e.g., sitting and walking) as well as any activities that are composed of a sequence of actions (e.g., buying). • Fluent, which is used in the AI community to refer to an attribute of a physical object that can change over time <ref type="bibr" target="#b48">[49]</ref>, e.g., the openness attribute of a door and the hunger attribute of a human. The change of fluents helps detect or explain human actions in a scene and thus is important in joint parsing. Figure <ref type="figure">5</ref> shows an example taxonomy for the courtyard scene.</p><p>We also specify a set of relations in the ontology. These relations can be organized into a relation taxonomy, allowing the computation of semantic distances between relations. A small set of core relations are introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge representation by And-Or graphs</head><p>We represent the compositional structures of objects, scenes and events using a spatial-temporal-causal And-Or graph (S/T/C-AOG). We will define probabilistic models on the AOG to address the stochasticity of the compositional structures in section 2.4. An AOG is an extension of a constituency grammar used in natural language parsing <ref type="bibr" target="#b4">[5]</ref>. Like a constituency grammar, an AOG represents possible hierarchical compositions of a set of entities. However, an AOG differs from a constituency grammar in that it contains additional annotations and relations over the entities. More specifically, an AOG consists of a hierarchical structure with alternating layers of And-nodes and Or-nodes. An And-node represents the decomposition of an entity of a specific type into a set of sub-entities. A Part-of relation is established between each sub-entity and the composite entity. The And-node also specifies a set of relations between the sub-entities, which configure how these sub-entities form the composite entity. An Or-node represents the alternative configurations of an entity of a specific type. AOGs has been employed in computer vision to model the hierarchical decompositions of objects <ref type="bibr" target="#b5">[6]</ref>, scenes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and events <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In this work, we use an AOG to model objects, scenes, events and perceptual causal effects in the joint interpretation of video and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Spatial And-Or graph</head><p>A spatial And-Or graph (S-AOG) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> models the spatial decompositions of objects and scenes. Figure <ref type="figure" target="#fig_4">6(a)</ref> shows an example S-AOG for indoor scenes. Each node in the S-AOG represents a type of scenes, objects or their parts. An And-node in the S-AOG represents a scene or object that is the spatial composition of a set of parts. For example, a table can be the spatial composition of a table top above four legs; and the background of an indoor scene can be the spatial composition of multiple 2D planes (i.e., the walls, floor and ceiling) that are hinged. An Or-node in the S-AOG represents alternative configurations of a scene or object. For example, a table can have many different styles; and an indoor scene may have a few different typical viewpoints. A leaf node in the S-AOG represents an atomic object that cannot be further decomposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Temporal And-Or graph</head><p>A temporal And-Or graph (T-AOG) <ref type="bibr" target="#b8">[9]</ref> models the hierarchical decompositions from events to sub-events and then to atomic actions. Figure <ref type="figure">7</ref>(a) shows an example T-AOG. Each node in the T-AOG represents a type of event or action occurred in a time interval. An And-node in the T-AOG represents an event that is the temporal composition of a set of sub-events. For example, a buying event can be the temporal composition of waiting followed by ordering. An Or-node in the T-AOG represents alternative configurations of an event, e.g., a buying event can be either "waiting and ordering" or "ordering without waiting". A leaf node in the T-AOG represents an atomic action which is defined by properties of the action initiator(s) and the action target(s) (e.g., the pose of a human, the state of an object) as well as the interactions between the initiator(s) and target(s). We use Agent to denote the relation between an action and the action initiator, and use Patient to denote the relation between an action and the action target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Causal And-Or graph</head><p>A causal And-Or graph (C-AOG) <ref type="bibr" target="#b9">[10]</ref> captures the knowledge of the Causal relation between events and fluent changes. Figure <ref type="figure">8</ref>(a) shows an example C-AOG. An And-node in a C-AOG represents a composition of conditions and events that can cause a fluent change. For example, to cause a door to open, it can be the case that the door is unlocked and someone pushes the door. An Or-node in a C-AOG represents alternative causes that can result in a fluent change. For example, the cause that a door is opened can be either that "someone unlocks and pushes the door" or that "the door is automatic and someone walks close". A leaf node in a C-AOG is either an event or a fluent.  between them during parsing, which improves parsing accuracy. Connecting the three types of AOGs also leads to a more coherent and comprehensive joint interpretation, which facilitates subsequent applications such as query answering of who, what, when, where and why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parse graphs</head><p>We represent an interpretation of the input video and/or text with a graphical representation called a parse graph. A parse graph is a realization of the S/T/C-AOG by selecting at each Or-node one of the alternative configurations. For example, Figure <ref type="figure" target="#fig_4">6</ref> We adopt the open world assumption <ref type="bibr" target="#b49">[50]</ref> which states that a parse graph may not be a complete interpretation of the scene and any statement not included or implied by the parse graph can be either true or false. Figure <ref type="figure" target="#fig_2">3</ref>(b) and 4(b) show the parse graphs that represent the interpretations of the videos and texts shown in Figure <ref type="figure" target="#fig_2">3</ref>(a) and 4(a). Note that for better visualization of the parse graph, we use different node shapes to denote different types of entities, and use different edge colors to denote different types of relations; for nodes with temporal annotations (e.g., events), we lay them out in accordance with the time line; for a fluent that may have its value changed over time, we use levels to indicate the value of the fluent at each time point.</p><p>We infer parse graphs from video and text through maximizing the posterior probability that we shall discuss in the following few sections. As an explicit semantic representation of the video and text, our parse graphs facilitate subsequent applications such as text generation and query answering. In addition, our parse graph can be naturally represented as a Resource Description Framework (RDF) data model <ref type="bibr" target="#b50">[51]</ref>, one of the standard knowledge representations in the semantic web. This allows easy distribution of the parse graph data in the semantic web and enables the use of a multitude of approaches and tools for the editing, storage, inference and retrieval of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Probabilistic Modeling</head><p>In this section we define a probabilistic distribution over parse graphs to account for their stochasticity. Since the valid parse graphs are specified by an AOG, we assign energy terms to the AOG to make it a probabilistic model of parse graphs (so an AOG embodies a stochastic grammar). First, at each Or-node of the AOG we specify an energy that indicates how likely each alternative configuration under the Or-node is selected in a parse graph. Second, at each And-node of the AOG we specify an energy for each relation between the child nodes of the And-node that captures the uncertainty of the relation. These energy terms can be either manually specified by domain experts or learned from data <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The energy of a parse graph is thus determined by the energy of configuration selections made at the Or-nodes and the energy of the relations between the child nodes of the And-nodes contained in the parse graph. Let V or (pg) be the set of Or-nodes in pg, E or (v) the energy associated to the configuration selected at the Or-node v, R(pg) the set of relations specified at the And-nodes in pg, and E R (r) the energy associated with the relation r. The energy of a parse graph is defined as:</p><formula xml:id="formula_0">E(pg) = v∈V or (pg) E or (v) + r∈R(pg) E R (r)<label>(2)</label></formula><p>We also allow part of a parse graph to be missing with a penalty to accommodate incomplete observation or description.</p><p>As introduced in section 2.2, we use a joint S/T/C-AOG to define the valid parse graphs. The energy of a parse graph in our framework is therefore the summation of four energy terms.</p><formula xml:id="formula_1">E STC (pg) = E S (pg) + E T (pg) + E C (pg) + r∈R * (pg) E R (r) (3)</formula><p>where E S (pg), E T (pg) and E C (pg) are the energy terms defined by the S-AOG, T-AOG and C-AOG respectively according to Eq.2, R * (pg) is the set of relations that connect the three types of AOGs (as discussed in section 2.2.4) and are not contained in any of the AOGs, and E R (r) is the energy associated with the relation r.</p><p>The prior probability of a parse graph pg is then defined as:</p><formula xml:id="formula_2">P (pg) = 1 Z e -ESTC(pg) (4)</formula><p>where Z is the normalization factor. Eq.4 defines one of the four factors in the posterior probability of the parse graphs given the input video and text (Eq.1). In the next three sections, we shall introduce and discuss the other three factors which are involved respectively in video parsing, text parsing and joint inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO PARSING</head><p>In video parsing, we aim to detect objects, scenes and events that form an interpretation of the input video.</p><p>Since our prior knowledge of objects, scenes and events is encoded in the S/T/C-AOG, we regard the S/T/C-AOG as the prior of the video parse graph pg vid and want to optimize the posterior of pg vid . Our objective energy function for video parsing is:</p><formula xml:id="formula_3">E v (pg vid ) = E STC (pg vid ) -log p(vid|pg vid )<label>(5)</label></formula><p>where E STC is defined in Eq.3, and p(vid|pg vid ) is the probability of the input video vid given the objects, scenes and events specified in the parse graph pg vid . Since video parsing often contains a significant level of uncertainty, we output a set of candidate video parse graphs with low energy instead of a single best parse graph. To justify the use of the objective energy function E v as defined in Eq.5, note that if we assume that the set of nodes and edges in the joint parse graph pg jnt is a superset of that in the video parse graph pg vid , then it can be shown that exp(-E v (pg vid )) is a factor of the joint posterior probability P (pg jnt , pg vid , pg txt |vid, txt) (Eq.1); therefore, a video parse graph that has a higher energy according to Eq.5 would very likely lead to a low joint posterior probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial parsing</head><p>We first perform spatial parsing on each video frame following the approach proposed in <ref type="bibr" target="#b6">[7]</ref> called hierarchical cluster sampling. The approach consists of two stages.</p><p>In the first stage, the approach performs bottom-up clustering in which lower-level visual entities (e.g., line segments) of a video frame are composed into possible higher level objects according to the S-AOG. To keep the number of candidate objects tractable, compositions with high energy are pruned. In the second stage, the parser applies the Metropolis-Hastings algorithm in the space of spatial parse graphs. The sampler performs two types of operations to change the current spatial parse graph: the first is to add a candidate object composed in the first stage into the parse graph, where the proposal probability is defined based on the energy of the candidate object as well as the compatibility of the object with the rest of the parse graph; the second type of operation is to randomly prune the current parse graph. At convergence the approach outputs the optimal spatial parse graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal parsing</head><p>We perform temporal parsing following the approach proposed in <ref type="bibr" target="#b8">[9]</ref>, which is based on the Earley parser <ref type="bibr" target="#b53">[54]</ref>. The input video is divided into a sequence of frames. The agents, objects and fluents in each frame are identified using the spatial parser and special detectors.</p><p>The temporal parser reads in frames sequentially, and at each frame maintains a so-called state set that contains pending derivations of And-nodes that are consistent with the input video up to the current frame (i.e., for each And-node in the state set, only a subset of its child nodes have been detected from the video up to the current frame). At the beginning of parsing, the Andnodes at the top level of the T-AOG are added into the state set of frame 0, with all their child nodes pending.</p><p>With each new frame read in, three basic operations are iteratively executed on the state set of the new frame.</p><p>The prediction operation adds into the state set new And-nodes that are expected according to the existing pending derivations in the state set. The Scanning operation checks the detected agents, objects and fluents in the new frame and advances the pending derivations in the state set accordingly. The Completion operation identifies And-nodes whose derivations are complete and advances the pending derivations of their parent And-nodes. During this process, we prune derivations that have high energies to make the sizes of state sets tractable. After all the frames are processed, a set of candidate parses of the whole video can be constructed from the state sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal parsing</head><p>After all the events are detected in temporal parsing, we then perform causal parsing. For each fluent change detected in the video using special detectors, we collect the set of events that occur within a temporal window right before the fluent change and run the Earley parser again based on the sub-graph of the C-AOG rooted at the detected fluent change.</p><p>After spatial, temporal and causal parsing is done, we combine the resulting spatial, temporal and causal parse graphs into a complete video parse graph. Figure <ref type="figure" target="#fig_8">10</ref> shows a parse graph of the example input video shown in Figure <ref type="figure" target="#fig_3">4</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TEXT PARSING</head><p>In parallel to video parsing, we perform text semantic parsing to convert the input text description into the text parse graph pg txt . Our semantic parsing approach is similar to many existing information extraction approaches, but our objective is distinctive in two aspects. First, we focus on the domain of narrative event descriptions to extract information about objects, events, fluent-changes and the relations between these entities. Important relations in this domain include spatial, temporal, functional and causal relations. Second, information extracted from the text is expressed using the same set of semantic types and parse graph representation that are also used in video parsing. This allows us to connect the two parse graphs and perform joint inference. We assume that text descriptions are unambiguous and therefore only a single text parse graph is produced from the input text. However, our approach can also handle text ambiguity and produce multiple candidate text parse graphs with different probabilities. Our semantic parsing of text is a processing pipeline consisting of four main steps: text filtering, part-of-speech tagging and dependency inference, dependencies filtering, and parse graph generation, which will be introduced in the following four subsections. In the case where the text descriptions are accompanied with time stamps or durations, we add temporal annotations to the corresponding event nodes in the resulting parse graph. We will use the parsing process of the sentence "Tom is buying food" as a running example in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Filtering</head><p>This is a preprocessing step that locates and labels certain categories of words or phrases in the input text to facilitate subsequent text parsing steps. It first performs the named entity recognition (NER) to identify text elements related to names of persons, time, organizations and locations. Existing NER tools such as Stanford NER <ref type="bibr" target="#b54">[55]</ref> can be used. Second, it recognizes certain compound noun phrases that refer to a single entity (e.g., "food truck" and "vending machine"). The words in a compound noun phrase are then grouped together into a single word. Currently this is done by utilizing the chunker tool in Apache OpenNLP <ref type="bibr" target="#b55">[56]</ref>. Our example sentence is not changed in this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Part-of-Speech Tagging and Dependency Inference</head><p>This step performs part-of-speech (POS) tagging and syntactic dependency parsing using the Stanford Lexicalized Parser <ref type="bibr" target="#b10">[11]</ref>. During POS tagging, each word is ROOT-0 ROOT 0  The parser then performs dependency analysis to identify relations between the words in the sentence. It generates a dependency graph in which each node is a word and each directed edge represents a basic grammatical relation between the words. The Stanford Dependencies <ref type="bibr" target="#b56">[57]</ref> include more than 50 types of grammatical relations such as subject, modifier, complement, direct and direct object. For example, the following dependency identifies Tom as the subject of the buying event.</p><p>nsubj(buying,Tom).</p><p>Together with POS labels, these dependency relations allow us to extract information from the text more easily.</p><p>For our example sentence, the result of this step is shown in Figure <ref type="figure" target="#fig_0">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dependencies Filtering</head><p>In this step, we map each word in the sentence to an entity type defined in the ontology or to a literal type. Examples of such mappings are shown in Table <ref type="table" target="#tab_1">1</ref>. This step is currently implemented using a look-up table but can also be automated by utilizing the WordNet dictionary <ref type="bibr" target="#b43">[44]</ref>.</p><p>In some cases, it is important to check the POS tag of a word to determine the correct entity type. For example, in the following two sentences:  The door is open(/JJ). They open(/VBP) the door.</p><p>the word "open" in the first sentence refers to a fluent state, while in the second sentence it refers to an action. The POS labels "JJ" and "VBP" allow us to map the words to the correct entity types. Note that since we focus on a small set of entity types as specified in our ontology, words that can denote multiple entity types under the same POS tags are rare.</p><p>For our example sentence, the resulting dependency tree of this step is: nsubj(Buying, Human name) aux(Buying, Aux) root(ROOT-0, Buying) dobj(Buying, Food)</p><p>We further replace each entity type with its top level supertype specified in the ontology. In our example, "Buying" is replaced with "Event" and "Food" is replaced with "Object".</p><p>Compared with the original dependency tree which describes the relations between words, we have now added the entity types of these words. This provides additional cues to infer more precise semantic relations between the entities denoted by the words. For instance, from the above example, the dependency "nsubj(Event, Human name)" implies that the person (Tom) is the agent of the event (buying). Without the entity types, the dependency relation "nsubj" could be ambiguous, e.g., "nsubj(teacher, Tom)" from the sentence "Tom is a teacher" refers to an occupation attribute and not an agent-event relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parse Graph Generation</head><p>In order to extract semantic relations from dependencies more accurately, we need to consider not only individual dependencies but also their contexts (the semantic types of the words and the neighboring dependencies). Therefore, we design an attribute grammar to parse the set of dependencies and infer the semantic relations. The final text parse graph is then generated from the semantic relations inferred in this step as well as the entity types produced from the previous step.</p><p>In the attribute grammar, the terminal symbols are the mapped dependencies from the previous step. A set of production rules describe how the terminal symbols can be generated from ROOT, the start symbol. The Earley parsing algorithm <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b57">[58]</ref> is used to parse the dependencies using the attribute grammar, which employs topdown dynamic programming to search for a valid set of production rules that generate the dependencies. The parse tree from the attribute grammar for our example sentence is shown in Figure <ref type="figure" target="#fig_10">12</ref>.</p><p>In each node of the parse tree, information from the input sentence is organized into an attribute-value structure, which may include attributes such as the event or action type, person's name, etc. Such information structures are propagated from the lower-level nodes to the higher-level nodes of the parse tree via the attribute functions associated with the production rules of the Object.type := Human where " * " is a wild-card character that stands for any subtext of the attribute name. After all the information from the input sentence is propagated to the toplevel nodes in the parse tree of the attribute grammar, the final text parse graph is then generated based on these top-level nodes. For example, the top-level node Agent(Event,Object) in Figure <ref type="figure" target="#fig_10">12</ref> contains the following information:</p><p>Event.id = "buying-3" Event.type = Buy Object.id = "Tom-1" Object.type = Human Object.name = "Tom" from which we can construct a part of the text parse graph containing the Agent relation between the buying event and the human as well as the name attribute of the human. The final text parse graph of the example sentence is shown in Figure <ref type="figure" target="#fig_11">13</ref>.</p><p>In addition to extracting the Agent and Patient relations as shown in the running example, by using the attribute grammar we also extract the spatial, temporal and causal relations as well as functional properties of objects based on the function words in the input sentence. Examples of such relations and the function words used to extract such relations are shown in Table <ref type="table">2</ref>. Some function words, such as "at", "from" and "to", have multiple meanings, but their semantic context in the input sentence allows the correct relations to be inferred during parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">JOINT INFERENCE</head><p>In joint inference, we construct the joint parse graph from the video and text parse graphs produced by the first two modules. Although spatial-temporal-causal parsing of videos and semantic parsing of text have been studied in the literature, this is the first time to integrate them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Challenges in Joint Inference</head><p>The joint parse graph shall not be a simple union of the video parse graph and the text parse graph. We need to consider the following three difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Coreference. The entities and relations covered by the two input parse graphs may be overlapping.</p><p>Figure <ref type="figure" target="#fig_16">14</ref> shows such an example. We need to identify and merge the overlapping entities and relations. One challenge is that the same entity or relation might be annotated with different semantic types in the two parse graphs. For example, in Figure <ref type="figure" target="#fig_16">14</ref> the food truck entity is annotated with "Vehicle" in the video parse graph and "Food Truck" in the text parse graph. In addition, the spatialtemporal annotations of the same entity may not be exactly the same in the two parse graphs. As another challenge, for each entity there might be multiple candidate matching targets. For example, a person mentioned in the text may be matched to a few different persons in the video. 2) Missing information. Some information is not explicitly presented in the input video and text, but may be deduced using the prior knowledge encoded in the S/T/C-AOG. Such information is necessary to fill in the gap between the input video and text or is useful in downstream tasks like query answering. So we should include such information in the joint parse graph. Figure <ref type="figure" target="#fig_12">15</ref> shows an example in which the fluent "Has Food" is neither observable in the video nor stated in the text, but can be deduced from the event of buying food. 3) Conflicts. The parse graphs from the input video and text may contain conflicting information. Such conflicts manifest themselves once overlapping entities and relations from the two parse graphs are merged. Figure <ref type="figure" target="#fig_16">16</ref> shows an example parse graph that contains conflicting information. We need to detect such conflicts using the prior knowledge and correct them in the joint parse graph.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buy Location Patient Agent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>Food_truck Book Time Fig. <ref type="bibr" target="#b15">16</ref>. An example of conflicting information in the parse graph. Here the conflict is between "buy book" and "buy from food truck". Figure <ref type="figure" target="#fig_13">17</ref> shows a schematic diagram of how the joint parse graph is constructed from the video and text parse graphs, which demonstrates the three difficulties in joint inference. There are two scenarios. In the first scenario (Figure <ref type="figure" target="#fig_16">17(a)</ref>) where there is significant overlapping between the video and text parse graphs, we first need to identify the overlapping and solve the coreference problem, and then we can deduce missing information and correct conflicts to obtain a more accurate and comprehensive joint parse graph. In the second scenario (Figure <ref type="figure" target="#fig_16">17(b)</ref>) where there is no overlapping between the video and text parse graphs, at first no coreference can be identified to connect the two parse graphs; only after the two parse graphs are expanded by deducing implicit information do they become overlapped, and then we can connect them by solving coreference and produce a coherent joint parse graph.</p><p>To solve the three difficulties, we propose three types of operators: matching, which matches and merges subgraphs of the video and text parse graphs to solve the coreference problem; deduction, which inserts implicit information to fill in the potential gap between video and text and makes the joint parse graph more comprehensive; and revision, which resolves possible conflicts between video and text. In the rest of this section, we first define the conditional probability P (pg vid , pg txt |pg jnt ) in section 5.2, which specifies the relation between the joint parse graph and the video and text parse graphs; then we introduce the joint inference algorithm in section 5.3, which takes the union of the video and text parse graphs as the initial joint parse graph and iteratively changes it using the three types of operators to maximize the joint posterior probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relating the three parse graphs</head><p>To relate the video parse graph pg vid , the text parse graph pg txt and the joint parse graph pg jnt , we define the conditional probability P (pg vid , pg txt |pg jnt ). By multiplying it with the prior probability of the joint parse graph P (pg jnt ) (Eq.4), we get the joint probability of the three parse graphs, which is used to guide our joint inference. Intuitively, the conditional probability P (pg vid , pg txt |pg jnt ) models the generation of the video and text parse graphs given the joint parse graph, by considering how likely different types of entities and relations in the joint parse graph would be included in the video and text parse graphs, while penalizing the changes of the semantic, temporal and spatial annotations of these entities and relations in the video and text parse graphs. We do not assume independence of the generation of the video parse graph and that of the text parse graph, because in many scenarios they are indeed generated jointly, e.g., in TV news reporting the news editor would choose the footage and narrative that cover the same aspect of the whole news story. To make the maximum a posteriori estimation in joint inference tractable, we make the assumption that the inclusion and change of each entity or relation is independent of that of any other entity or relation in the parse graph, so we can factorize the conditional probability as defined below.</p><formula xml:id="formula_4">P (pg vid , pg txt |pg jnt ) = x∈pgjnt 1 Z x e -E(x) (6) × 1 Z ψ e -α ψ (pg vid ∪pgtxt)\pgjnt</formula><p>where x ranges over all the entities and relations in the joint parse graph pg jnt , Z x and Z ψ are normalization factors, α ψ is a constant representing the penalty of having an entity or relation in the video or text parse graph that is not contained in the joint parse graph, and E(x) is defined as follows.</p><formula xml:id="formula_5">E(x) =              α v (x) + d(x j , x v ) if x ∈ pg vid \ pg txt α t (x) + d(x j , x t ) if x ∈ pg txt \ pg vid α vt (x) + d(x j , x v ) + d(x j , x t ) if x ∈ pg vid ∩ pg txt α φ (x) if x / ∈ pg vid ∪ pg txt</formula><p>We enforce the constraint that a relation in the joint parse graph can be contained in the video or text parse graph only if the two entities connected by the relation are also contained in the video or text parse graph. The notations used in the energy function are explained below.</p><p>• α v (x) is the energy that x is contained in the video parse graph pg vid but not in the text parse graph pg txt . Ideally, this energy shall be set to a low value for elements that represent visible low-level details of objects, scenes and events, for example, the atomic actions of a drinking event (e.g., reaching to, lifting, holding and putting down a cup of water), which are seldom mentioned in the text description. • α t (x) is the energy that x is contained in the text parse graph pg txt but not in the video parse graph pg vid . Ideally, this energy shall be set to a low value for elements representing objects and events or their attributes that are either not visible in the video (e.g., a person's name, whether he is hungry) or hard to detect by the video parser (e.g., a person's gender can be hard to identify in a low resolution surveillance video). -The semantic distance d o (x, y) is defined as the distance between the semantic types of x and y in the ontology. Several approaches, e.g., <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, can be used to measure the semantic distance given the taxonomy in an ontology. We require the distance to be very large if the two semantic types are disjoint (i.e., having no common instance). -The temporal distance d t (x, y) is defined as the time difference between the temporal annotations of x and y if both have temporal annotations. -The spatial distance d s (x, y) is defined as the Euclidean distance between the spatial annotations of x and y if both have spatial annotations.</p><p>Currently we manually set the values of all the constants in the definition, but it is possible to learn them from annotated data (e.g., <ref type="bibr" target="#b61">[62]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint inference algorithm</head><p>We start with the simple union of the video and text parse graphs and then iteratively apply three types of operators that make changes to the joint parse graph.</p><p>• Matching. We match a node a from the video parse graph with a node b from the text parse graph, and merge them into a single node c which inherits all the edges attached to either node a or node b. To reduce the search space, we only match nodes that have a small distance between them (as defined in Eq.7). The semantic type of the new node c is set to be the more special one of the semantic types of node a and node b. For example, if node a is of semantic type "Vehicle" and node b is of semantic type "Food Truck", then the semantic type of node c is set to "Food Truck" because "Food Truck" is a sub-type of "Vehicle". The temporal and spatial annotations of nodes a and b are averaged before being assigned to the new node c. After the merging, if there exist two edges connecting to node c that have the same direction and a small distance (as defined in Equation <ref type="formula">7</ref>), then the two edges are also merged. • Deduction. We insert a new subgraph into the joint parse graph, such that the prior probability of the joint parse graph as defined by the S/T/C-AOG is increased. Specifically, for each entity in the joint parse graph, we find the type of this entity in the S/T/C-AOG and insert an instantiation of its immediate surrounding subgraph in the AOG into the joint parse graph. Because the energy defined in section 5.2 actually penalizes the size of the joint parse graph, we require the insertion to be minimal, i.e., each inserted node or edge does not duplicate any existing node or edge. This can be achieved by following the deduction operator with a series of matching operators to match and merge the inserted nodes and edges with the existing parse graph. • Revision. We either remove a subgraph from the parse graph, or change the annotation of a node or edge in the parse graph, in order to solve a conflict as defined in the S/T/C-AOG (e.g., an instantiation of zero probability, or instantiation of more than one child of an Or-node). Because the energy defined in section 5.2 penalizes such revisions, we require the removal or change to be minimal, i.e., no extra node or edge is removed or changed that is unnecessary to solve the conflict.</p><p>Our objective function is the joint posterior probability defined in Eq.1. Because both the prior probability P (pg jnt ) (Eq.4) and the conditional probability P (pg vid , pg txt |pg jnt ) (Eq.6) can be factorized by the entities and relations in the parse graph, we can compute the change to the joint posterior probability caused by each operator.</p><p>For the surveillance data that we focus on in this paper, a typical parse graph contains a few tens of nodes and edges, so we use depth-first search with pruning to optimize the joint parse graph. For data of larger scales, it is straightforward to extend our algorithm with beam search. Given the initial joint parse graph made of a simple union of the video and text parse graphs, we first apply the matching operators until no further matching is possible, then apply the revision operators to solve conflicts, and finally apply the deduction operators (with the follow-up matching operators) to deduce implicit information. Each operator typically contains a number of possibilities (e.g., multiple nodes in the text parse graph can be matched with a specific node in the video parse graph), and we exam all the possibilities in a depth-first manner. In order to reduce the search space, we prune the possibilities that have significantly higher energy than the others.</p><p>Stop criterion for deduction. When applying the deduction operator, in some cases we may deduce multiple candidate subgraphs that have similar energy and are mutually exclusive (e.g., different children of an Ornode). In other words, there is significant uncertainty in the deduction as measured by information entropy. For example, we may observe a human in the video but cannot see what he is doing because of low resolution or occlusion, and based on the background knowledge we may infer a number of actions of the human that are equally possible. Since we adopt the open world assumption, it is reasonable to keep agnostic here and not add any new information. Specifically, we cancel the deduction operator if the entropy of the deduced subgraph is higher than a threshold:</p><formula xml:id="formula_6">H(pg de |pg jnt ) = - N i=1 P (pg i de |pg jnt ) log P (pg i de |pg jnt )</formula><p>&gt; log N c where pg jnt is the joint parse graph before applying a deduction operator, pg de denotes the subgraph inserted by the deduction operator, N is the number of possible candidate subgraphs that can be deduced by the operator, and c &gt; 1 is a pre-specified constant.</p><p>Note that in video parsing we produce multiple candidate video parse graphs. So for each video parse graph we run the joint inference algorithm to find a joint parse graph, and then we output the parse graph triple pg vid , pg txt , pg jnt with the highest joint posterior probability. Although the algorithm described here only outputs a single optimal joint parse graph for each pair of input video and text parse graphs, it is easy to adapt the algorithm to output multiple candidate joint parse graphs along with their joint posterior probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANSWERING NATURAL LANGUAGE QUERIES</head><p>The joint parse graph is a semantic representation of the objects, scenes and events contained in the input video and text, which is very useful in many applications. In this section, we show how the joint parse graph can be used in semantic queries to answer questions, such as (a) questions in the forms of who, what, when, where and why; and (b) summary of scenes or events, e.g. how many persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Text query parsing</head><p>Given a simple plain text question about the objects, scenes and events in the input video and text, we parse the question into a formal semantic query representation. Since our joint parse graph can be represented in RDF, we represent a query using SPARQL, the standard query language for RDF <ref type="bibr" target="#b62">[63]</ref>.</p><p>The steps of text query parsing are identical to those in the text parsing module introduced in section 4. The attribute grammar for analyzing the dependencies is extended to include interrogative wh-words such as who, what, when, where and why. These wh-words indicate what the objective of the query is, and the rest of the text query defines the query conditions. For example, the question "Who has a cap?" is parsed into the following dependencies and attributes: nsubj(Event, who) root(ROOT-0, Event); Event.type = Possess det(Object, Det) dobj(Event, Object); Object.type = Cap</p><p>The first dependency indicates that the query is asking for the agent of an event. The rest of the dependencies specify what the event type ("Possess") and the event patient ("Cap") are. These query conditions are then converted into the SPARQL format: SELECT ?name ?who1 WHERE { ?who1 hasName ?name. ?cap4 rdf:type Cap. ?has2 rdf:type Possess. ?has2 hasAgent ?who1. ?has2 hasPatient ?cap4. } Queries that require aggregates can also be expressed in SPARQL via aggregate functions (COUNT, MAX, etc.) and grouping. For instance, the question "how many persons are there in the scene" is parsed into the following SPARQL query: SELECT (COUNT(DISTINCT ?agent) as ?count) WHERE { ?agent rdf:type Human } After the SPARQL query is generated, it is evaluated by a SPARQL query engine which essentially performs graph matching of the patterns in the query condition with the joint parse graph stored in the RDF knowledge base. One implementation of the SPARQL query engine is included in Apache Jena <ref type="bibr" target="#b63">[64]</ref>. The values obtained from the query engine are then processed to produce the answers to the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Computing answer probabilities from multiple interpretations</head><p>Our system can produce multiple joint parse graphs, each associated with its posterior probability. These joint parse graphs correspond to different interpretations of the input video and text. To answer a query accurately, we can execute the query on all the interpretations. The collected answers are then compared. We associate the answers from different interpretations by matching their semantic types and spatial-temporal annotations.</p><p>For answers that are matched, their probabilities are combined. Formally, the probability of an answer a is computed as:</p><formula xml:id="formula_7">P (a) = pg P (pg)1(pg |= a)</formula><p>where P (pg) denotes the posterior probability of the joint parse graph pg and 1(pg |= a) is the indicator function of whether parse graph pg entails the answer a. In this way, different possible answers can be ranked based on their marginal probabilities. A schematic diagram of the process is shown in Figure <ref type="figure" target="#fig_14">18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Text query GUI tool</head><p>We developed a GUI tool for text query answering. A screenshot of this tool is shown in Figure <ref type="figure" target="#fig_15">19</ref>. With this tool, the user types a query in natural language text and the system parses the query and retrieves the result, which is shown on the left of the GUI. If the answer is an event, the text description of the event is also presented in the bottom right of the GUI (the text descriptions are generated automatically from the joint parse graphs, which will be discussed in section 7.2.1). The corresponding video segment of the retrieved answer is shown on the top-right with overlays surrounding the agent and patient of the event. With this tool, the user can efficiently query about the events occurring in the video and review the relevant video segments. In our experiment, each query response typically took a few seconds.</p><p>User enters text query here SPARQL Answers with probabilities Display of retrieved agent and object</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video preview control</head><p>Text description of event </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets</head><p>We collected two datasets of surveillance videos and text descriptions in indoor (hallway) and outdoor (courtyard) scenes. Compared with other types of datasets (such as movies with screenplays and news report footages with closed captions), surveillance videos contain a well-defined set of objects, scenes and events, which is amenable to controlled experiments; the text accompanying surveillance videos provides descriptions of the scenes shown in the video, while other types of datasets may contain text that are not descriptive of the scenes such as dialogs and comments; in addition, on surveillance videos we can control the level of details in the text descriptions and study its impact on the performance of joint parsing.</p><p>For each of the two datasets, we first recorded the video of the scene and divided the video into fifteen clips. Each video clip contains one or more events and lasts from a few seconds to over one minute. Based on the objects and events captured in the video, we manually constructed the ontology and the S/T/C-AOG for each dataset. We then asked five human subjects from different countries and background to provide text descriptions by watching the video clips. The human subjects were asked to only describe the events and the involved agents, objects and fluents in the scenes using plain English. The ontology was also provided to the human subjects to restrict the scope of their description. The human subjects were instructed to provide descriptions at three different levels of details, so we can study how the amount of text descriptions and their degrees of overlap with the video content can impact  the performance of joint parsing. At the first level, the human subjects were told to give the simplest descriptions consisting of one or two sentences; at the second level, they were asked to give more details (typically less than ten sentences); at the third level, they were asked to provide information that is not directly observable in the video but can be inferred (typically zero to two sentences). Figure <ref type="figure" target="#fig_17">20</ref> shows two examples from our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Qualitative results</head><p>We ran our system to produce video parses, text parses and joint parses from the datasets. We then compared the information contained in the three types of parse graphs and made the following observations.</p><p>• Prominent objects and events are typically contained in both the video and the text parse graphs, e.g., a person drinking at the water fountain. • Low-level visible details of objects, events and fluents are often contained in the video parse graphs but not in the text parse graphs, e.g., a person approaching the water fountain before drinking and the door closing after a person enters. • Two types of information are typically contained in the text parse graphs but not in the video parse graphs. The first is information invisible in the video but can be inferred, e.g., a person is thirsty (because he is drinking water). The second type of information is visible in the video but is not correctly detected by the video parser. • The joint parse graphs may contain information that can be found in neither the video nor the text parse graphs, e.g., low-level details of objects and events that are not detected by the video parser, or invisible information that is not mentioned in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated text from the joint parse graph</head><p>A person is thirsty. He approaches a fountain. He drinks water at a fountain He leaves a fountain at drinks water at a fountain. He leaves a fountain at the bottom of the image between 0:40 and 0:45.</p><p>A person is thirsty. He approaches a fountain. He bends down. He drinks water at a fountain. He nd bends down because he wants to drink water. He leaves a fountain at the bottom of the image between 0:40 and 0:45.</p><p>A person is hungry. He waits for food between 0:00 and 1:02. He talks at a food truck between 0:00 and 0:38. He walks to a food truck between 0:39 and 0:44. He walks from a food truck between 0:52 and 1:02.</p><p>A person is hungry. He waits for food between 0:00 and 1:02. He stands between 0:00 and 0:08. H lk f d k b 0 00 d 0 08 He talks at a food truck between 0:00 and 0:08. He bends down between 0:09 and 0:16. He squats between 0:17 and 0:38. He walks to a food truck between 0:39 and 0:45. He receives uck.</p><p>food between 0:46 and 0:51. He walks away from a food truck between 0:51 and 1:02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Examples of generated text from the joint parse graphs. For each input video, we show the results produced from joint parsing using level 1 text and using text of all three levels.</p><p>the input text. Such information is deduced in joint inference from other information in the video and text parse graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Text generation</head><p>We extend the text generator described in Yao et al. <ref type="bibr" target="#b64">[65]</ref> to generate text descriptions from our joint parse graphs.</p><p>The generated text presents the content of parse graphs in a user-friendly manner and helps non-expert users to understand the results from joint parsing. The text generation process goes through two modules: sentence planner and sentence realizer. The sentence planner selects the content to be expressed, specifies hard sentence boundary, and provides information about the content. During this process, the information from the parse graph is converted to a functional description <ref type="bibr" target="#b65">[66]</ref>, which specifies elements of a sentence such as the event, subjects, objects, and their functional properties. With a functional description as the input, the sentence realizer generates the sentence by determining a grammatical form and performing word substitution. A head-driven phrase structure grammar (HPSG) <ref type="bibr" target="#b66">[67]</ref> of English syntax is used to generate text sentences.</p><p>Table <ref type="table">3</ref> shows some examples of the generated text. By comparing the input text and the generated text, it can be seen that when the input text is simple, the majority of the information in the joint parse graph comes from video parsing as well as deduction in joint inference; with more input text, an increasing part of the joint parse graph comes from text parsing.</p><p>We also evaluated the generated text based on the joint parse graph against the merged text descriptions from human subjects, using quantitative metrics such as BLEU <ref type="bibr" target="#b67">[68]</ref> and METEOR <ref type="bibr" target="#b68">[69]</ref> which were developed for evaluating machine translation. We find the resulting scores uninformative about the quality of joint parsing, because 1) the automatic text generator often chooses different words and expressions than human (e.g., "bend because he wants to drink" vs. "bend to drink"); 2) the generated text often contains information that is not included in the human text descriptions, such as the information from video parsing and deduction (e.g., low-level visual details). These differences are heavily penalized by the machine translation evaluation metrics, although they either are unimportant or should be rewarded for evaluating joint parse graphs. Therefore, we adopt two alternative quantitative evaluation approaches as introduced in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Quantitative evaluation</head><p>On each video clip in the datasets, for each human subject we used the text descriptions of the video clip as the text input to produce three joint parse graphs. The first joint parse graph is based on the level-1 text descriptions; the second is based on the combined text from level 1 and 2; and the third is based on the text of all three levels. We consider four baseline parse graphs: the video parse graph and the three text parse graphs based on level 1 text, level 1+2 text and text of all three levels. We evaluated the joint parse graphs as well as the baseline parse graphs using two different evaluation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Comparison against ground truth</head><p>Ground truth. For each video clip, we constructed the ground truth parse graph by merging the parses of all the text descriptions from all the human subjects except the one whose text description was used as the input in the current run of experiments. The merged parse graph was manually checked to ensure correctness. Precision and recall. We compared the parse graphs to be evaluated against the ground truth parse graph and measured the precision, recall and F-score. Let pg represent the parse graph to be evaluated, pg * the ground truth parse graph, pg ∩ pg * the overlap between the two parse graphs, and pg the size of parse graph pg.</p><formula xml:id="formula_8">Precision = pg ∩ pg * pg Recall = pg ∩ pg * pg *</formula><p>F-score is defined as the harmonic mean of precision and recall. In computing precision and recall, in order to estimate the overlap between the parse graph to be evaluated and the ground truth parse graph, we ran depth-first search using the matching operator described in section 5.3 to match the nodes and edges in the two parse graphs. Since there might be information that is correct but missing from our ground truth parse graph constructed from merged human text descriptions, we also manually checked the parse graph to be evaluated when computing its precision.</p><p>Figure <ref type="figure" target="#fig_19">21</ref> shows the precision, recall and F-score of the baseline parse graphs and joint parse graphs averaged over all the video clips and human subjects of each dataset. It can be seen that all the precision scores are close to 1, suggesting the correctness of the information contained in all types of parse graphs. On the other hand, the recall and F-score of different types of parse graphs vary significantly. Each level of joint parse graphs has higher recall and hence higher F-score than the video parse graphs and the text parse graphs of the same level, which demonstrates the advantage of joint parsing. In particular, the joint parse graphs based on level-1 text have significantly higher recall and F-score than both the video parse graphs and the level-1 text parse graphs, implying that the information from the video and the level-1 text is highly complementary; it also shows that providing a simple text description (i.e., level-1 text) to a video can already boost the performance of video understanding to the level close to a full description of the video (i.e., all three levels of text combined).</p><p>Adding more text (level 2 and 3) into joint parsing can be seen to further improve the joint parse graphs but with diminishing return.</p><p>Comparing the results of the two datasets, we can see that the recall scores of the text and joint parse graphs on the courtyard dataset are much lower than those on the hallway dataset. This is because the courtyard dataset contains more complicated events and therefore the text descriptions from the five human subjects are often more different. Below is an example of such difference on a courtyard video clip:</p><p>1) The person walks.</p><p>2) The person walks and talks.</p><p>3) The person walks out of a building.</p><p>4) The person walks across the courtyard. Since none of the text descriptions cover all the details of the event, the average recall of the text parse graphs is low. The joint parse graphs are produced with the text parse graphs as input, so their average recall is also lower on the courtyard dataset.</p><p>To compensate for this phenomenon in computing recall, we changed the ground truth parse graphs by keeping only the entities and relations that are mentioned by a minimal number of human subjects. The number of human subjects that mention an entity or relation indicates the degree of consensus among the human subjects regarding the existence and importance of the entity or relation. The maximal degree of consensus is 4/4 in our experiments because we used the text from four of the five human subjects to construct the ground truth parse graphs (excluding the provider of the input text of joint parsing). Figure <ref type="figure" target="#fig_20">22</ref> shows the average recall scores of all types of parse graphs evaluated against the re-constructed ground truth parse graphs based on different degrees of consensus. It can be seen that all the recall scores are improved with the increase of the ground truth degree of consensus. Most of the improvements are very large, implying that the text descriptions from different human subjects are indeed quite different, leading to significant changes of the ground truth parse graphs at different degrees of consensus. At the highest degree of consensus, the recall scores of most of the parse graphs are over 0.9, suggesting that these parse graphs cover the most important entities and relations in the scene. Among all types of parse graphs, only the video parse graphs on the hallway dataset do not have large improvement of recall, suggesting the limitation of the video parser on the hallway dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Degree of deduction.</head><p>We further studied the influence of the degree of deduction on joint parsing. On a subset of the courtyard dataset that contain complex events, we adjusted the value of α φ to control the degree of deduction during joint inference and then measured the precision and recall of the resulting joint parse graphs. Figure <ref type="figure" target="#fig_21">23</ref> shows the results. It can be seen that with an increasing degree of deduction, at first the recall is greatly improved without decreasing the precision, suggesting that the deduced information is mostly correct; however, with further deduction the recall is improved modestly while the precision drops dramatically, which is caused by the increase of erroneous information being deduced; eventually, the recall stops improving while the precision continues to drop, implying that all the additional information being deduced is false. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Evaluation based on query answering</head><p>In section 6 we have introduced how the joint parse graph produced by our system can be used in query answering. Here we evaluate the quality of the joint parse graph by measuring the accuracy of query answering. This evaluation approach of video understanding is novel in that it directly measures the utility of the video understanding system in human-computer interaction, which goes beyond the conventional evaluation frameworks such as those based on classification or detection.</p><p>For each video clip in our datasets, we constructed a natural language query set that mimics the scenario in which a human user continuously queries the computer in order to acquire information of the objects, scene and events captured in the video clip. The natural language queries can take the forms of who, what, when, where and why. We also asked a human subject to provide the correct answers for each query set. Table <ref type="table">4</ref> shows an example query set and the correct answers. We then entered the queries into our system and retrieved the answers. Please see <ref type="url" target="http://www.stat.ucla.edu/">http://www.stat. ucla.edu/</ref> ∼ tukw/JointParsing/demo.html#demo2 for a demonstration video of a user continuously asking TABLE <ref type="table">4</ref> An example sequence of natural language queries and the correct answers.</p><p>queries through our GUI tool. We compared the answers produced by the system against the correct answers provided by human. For each who/what/where/why query, we matched the system answers with the human answers and computed the precision (the percentage of the system answers that are correct) and recall (the percentage of the human answers that are returned by the system). For each when query, we checked the overlap between the time range returned by the system and that provided by human, based on which we computed the precision (the percentage of the time range returned by the system that is correct) and recall (the percentage of the correct time range that is covered by the system answer). We then computed the F-score which is the harmonic mean of precision and recall.</p><p>Figure <ref type="figure" target="#fig_5">24</ref> shows the average F-score of the what, where, when and why queries and of all the queries based on the video, text and joint parse graphs. The results of the who queries are not shown because the video clips used in the experiments either contain or designate a single person and therefore the who queries become trivial to answer. From the results we can see that query answering based on the joint parse graphs clearly outperforms that based on the video and text parse graphs. Note that in comparison with the experimental results from the first evaluation approach (Figure <ref type="figure" target="#fig_19">21</ref>), in the query answering experiments the joint parse graphs have a significantly larger advantage over the video and text parse graphs. This is because answering a query involves perfect matching of the conditions of the query with one or more subgraphs of the parse graph, and therefore any small error in the parse graph would be magnified in the evaluation of query answering. Among the four types of queries shown in Figure <ref type="figure" target="#fig_5">24</ref>, the joint parse graphs have the largest advantage on the why queries over the video and text parse graphs, because the causal relations required for answering the why queries are typically not detected in video parsing and not always mentioned in the input text but can be inferred by the deduction operator in joint inference. The advantage of the joint parse graphs on the where queries is also large, because in video parsing many events being queried are not detected, and in the text the location information is sometimes skipped (as can be seen in the examples in Figure <ref type="figure" target="#fig_17">20</ref>), while in joint parsing additional location information can be deduced. For the when queries, the average F-score of the text parse graphs is well below 1.0 although the time annotations provided by human subjects in the input text are quite accurate, which is because some of the events being queried are not described in the input text and therefore the corresponding when queries cannot be answered based on the text parse graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND DISCUSSION</head><p>We propose a framework for parsing video and text jointly for understanding events and answering queries. Our framework produces a parse graph of video and text that is based on the spatial-temporal-causal And-Or graph (S/T/C-AOG) and represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents). We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the model we propose a joint parsing system that consists of three modules: video parsing, text parsing and joint inference. In the joint inference module, we produce the joint parse graph by performing matching, deduction and revision on the video and text parse graphs. We show how the joint parse graph produced by our system can be used to answer natural language queries in the forms of who, what, when, where and why. The empirical evaluation of our system shows satisfactory results.</p><p>For future work, first, we will further improve both video parsing and text parsing with respect to parsing quality and efficiency. Second, currently we manually construct the probabilistic S/T/C-AOG to model objects, scenes and events; in the future we want to investigate automatic approaches to learning AOG from data. Third, we plan to apply our system to video and text data more challenging than surveillance videos with human intelligence descriptions, e.g., news report data, which contains more diversified contents and larger gap between video and text and requires more sophisticated background knowledge in joint parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Fig. 1 .</head><label>11</label><figDesc>Fig. 1. Examples of videos accompanied with texts. (a) A television show footage and its screenplay; (b) A surveillance video and the human intelligence descriptions; (c) A news report footage and its closed captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The graphical model for joint parsing of video and text. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) An example surveillance video and the text descriptions of a parking lot scene. (b) The video parse graph, text parse graph and joint parse graph. The video and text parses have significant overlap. In the joint parse graph, the green shadow denotes the subgraph from the video and the blue shadow denotes the subgraph from the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) An example surveillance video and the text descriptions of a courtyard scene. (b) The video parse graph, text parse graph and joint parse graph. The video and text parses have no overlap. In the joint parse graph, the green shadow denotes the subgraph from the video and the blue shadow denotes the subgraph from the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) An example spatial And-Or graph (S-AOG). Each node represents a type of scenes, objects or their parts;an And-node represents the spatial composition of its child nodes; an Or-node represents a selection among its child nodes; a leaf node represents an atomic object. (b) An example spatial parse graph from<ref type="bibr" target="#b6">[7]</ref>, which is a realization of the S-AOG by making selections at Or-nodes.</figDesc><graphic coords="7,331.81,65.32,206.80,199.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2. 2 . 4</head><label>24</label><figDesc>Joint S/T/C-AOG Although S-AOG, T-AOG and C-AOG have been proposed and investigated separately before, in this work we show for the first time that the three types of AOGs are interrelated and form a joint S/T/C-AOG, as shown in Figure 9. An action represented by a leaf node in the T-AOG is connected via the Agent, Patient and Location relations to the action initiator, target and location, which are objects modeled in the S-AOG. An event that has a Causal relation with a fluent change in the C-AOG is itself modeled by the T-AOG. There may also be a Trigger relation between a fluent in the C-AOG and an event in the T-AOG. Finally, there are HasFluent relations between objects in the S-AOG and fluents in the C-AOG. The connections across the three types of AOGs enable the propagation of information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .Fig. 9 .</head><label>789</label><figDesc>Fig. 7. (a) An example temporal And-Or graph (T-AOG). Each node represents a type of event or action; an And-node represents the temporal composition of its child nodes; an Or-node represents a selection among its child nodes; a leaf node represents an atomic action defined by the properties of and interactions between the action initiator(s) and target(s). (b) An example temporal parse graph, which is a realization of the T-AOG by making selections at Or-nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(b), 7(b) and 8(b) show respectively a valid spatial, temporal and causal parse graph as realizations of the S/T/C-AOGs given in Figure 6(a), 7(a) and 8(a). A parse graph can be represented as a labeled directed graph. Each node in a parse graph represents an entity whose label indicates the semantic type of the entity. A node may also have a set of spatial-temporal annotations indicating the spatialtemporal coordinate or range of the entity represented by the node. A parse graph may also contain a special type of nodes representing attribute values in the form of strings or numbers. A directed edge between two nodes in a parse graph represents a relation between the two entities denoted by the two nodes. The label on the edge indicates the type of the relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. A candidate parse graph of the video in our example input shown in Figure 4(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Tom- 1 /Fig. 11 .</head><label>111</label><figDesc>Fig.11. An example dependency tree from step 2 of text parsing.Entity type or literal typeWords in the input text</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. An illustration of step 3 and 4 of text parsing: dependencies filtering of step 3 is shown in the bottom half of the figure; attribute grammar parsing of step 4 is shown in the top half of the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. The parse graph of the example input text "Tom is buying food".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. An example of information that can be deduced from the parse graph. The dashed shapes and arrows represent inferred entities and relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig.17. A schematic diagram of how the joint parse graph pg jnt is constructed from the video parse graph pg vid and text parse graph pg txt , when (a) there is overlapping between the video and text parse graphs, and (b) the video and text parse graphs have no overlapping but become overlapped after some steps of deduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. The schematic diagram of querying multiple interpretations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Text Query GUI. See http://www.stat.ucla.edu/ ∼ tukw/JointParsing/demo.html#demo2 for a demonstration video.</figDesc><graphic coords="18,51.82,85.13,245.71,154.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( 1 )</head><label>1</label><figDesc>The person drinks water.<ref type="bibr" target="#b1">(2)</ref> The person walks to a water fountain. He bends down to drink water and then down to drink water and then leaves.(3) The person is thirsty.(1) The person buys food at the food truck.(2) The person walks to the (2) The person walks to the food truck. He orders his food and waits. He picks up his food and walks away.(3) The person is hungry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Examples from the hallway (top) and courtyard (bottom) datasets. The text descriptions shown on the right side are divided into three levels of details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>person drinks water. He walks to a p fountain. He bends down to drink water an then leaves. He is thirsty. (Level 1) 0:00-1:02 The person waits for food. (Level 1-3) 0:00-1:02 The person waits for food. 0 00 0 08 H d d lk 0:00-0:08 He stands and talks. 0:09-0:16 He bends down. 0:17-0:38 He squats. 0:39-0:45 He walks to the food truck. 0:46-0:51 He receives food. 0:51-0:54 He walks. 0:55-1:02 He walks away from the food tru 0:00-1:02 He is hungry. 0:00 :0 e s u g y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. The average precision, recall and F-score of the video, text and joint parse graphs in comparison against the ground truth parse graphs constructed from merged human text descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 22 .</head><label>22</label><figDesc>Fig.<ref type="bibr" target="#b21">22</ref>. The average recall of the video, text and joint parse graphs evaluated against the ground truth parse graphs re-constructed based on different degrees of consensus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. The influence of the degree of deduction on the precision and recall of joint parse graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>.6 Outline of the paper</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>┬ ┬</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Object</cell><cell></cell><cell></cell><cell></cell><cell>Event</cell><cell></cell><cell></cell><cell></cell><cell>Fluent</cell></row><row><cell>LivingBeing</cell><cell></cell><cell cols="2">Artifact</cell><cell>IndividualAction</cell><cell></cell><cell cols="3">GroupAction</cell><cell>HumanFluent</cell><cell>DoorFluent</cell></row><row><cell>Vehicle Vehicle</cell><cell></cell><cell>F it Furniture</cell><cell>Machine Machine</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell></cell><cell></cell><cell></cell><cell>Buy Wait Order Wa</cell><cell>lk</cell><cell>Sit</cell><cell>Discuss</cell><cell>SitTogether</cell><cell>Hunger Tiredness</cell><cell>Openness</cell></row><row><cell>FoodTruck Car</cell><cell>Chair</cell><cell>Table</cell><cell cols="2">Bench VendingMachine</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">• Although spatial, temporal and causal AOGs and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">their parsing techniques have been separately in-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">vestigated before, our work is the first to propose</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">the joint S/T/C-AOG which crosses the spatial,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">temporal and causal dimensions and enables the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">propagation of information between them during</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">parsing.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">• We propose novel evaluation approaches of video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">understanding based on parse graph comparison</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">and query answering through storylines. In partic-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">ular, the query-answering-based evaluation mimics</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">the natural human-computer interaction via ques-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">tions of who, what, when, where and why, which</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">goes beyond the conventional evaluation frame-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">works such as those based on classification or de-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tection.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">1The rest of the paper is organized as follows. Section</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">2 introduces our knowledge representations. Section 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">and 4 present the approaches of video parsing and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">text parsing respectively. Section 5 introduces our joint</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">inference module. Section 6 introduces the application</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Example mappings between words and entity types or literal types.</figDesc><table /><note><p>assigned a grammatical category such as noun, verb, adjectives, adverb, article, conjunct and pronoun. The tagger uses the Penn treebank tag set which has 45 tags.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>• α vt (x) is the energy that x is contained in both the video parse graph pg vid and the text parse graph pg txt . Ideally, this energy shall be set to a low value for elements representing visible high-level objects, scenes and events.• α φ (x) is the energy that x is contained in neither the video parse graph pg vid nor the text parse graph pg txt . Ideally, this energy shall be set to a low value for elements representing low-level details of objects, scenes and events that are invisible or hard to detect in the video. • x j , x v and x t are the corresponding entities or relations of x in the joint parse graph pg jnt , video parse graph pg vid and text parse graph pg txt respectively.</figDesc><table /><note><p><p><p>• d is a distance measure that combines the semantic, temporal and spatial distances as defined below. It models the difference in the annotations of the entity or relation x in the joint parse graph and the video or text parse graph.</p>d(x, y) = α o d o (x, y) + α t d t (x, y) + α s d s (x, y) (7)</p>where α o , α t and α s are constant weights, and the three distance measures d o (x, y), d t (x, y) and d s (x, y) are explained below.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The work is supported by the <rs type="funder">DARPA</rs> grant <rs type="grantNumber">FA 8650-11-1-7149</rs>, the <rs type="funder">ONR</rs> grant <rs type="grantNumber">N00014-10-1-0933</rs> and <rs type="grantNumber">N00014-11-C-0308</rs>, and the <rs type="funder">NSF CDI</rs> grant <rs type="grantNumber">CNS 1028381</rs>. We want to thank <rs type="person">Mingtian Zhao</rs>, <rs type="person">Yibiao Zhao</rs>, <rs type="person">Ping Wei</rs>, <rs type="person">Amy Morrow</rs>, <rs type="person">Mohamed R. Amer</rs>, <rs type="person">Dan Xie</rs> and <rs type="person">Sinisa Todorovic</rs> for their help in automatic video parsing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wXpxTYE">
					<idno type="grant-number">FA 8650-11-1-7149</idno>
				</org>
				<org type="funding" xml:id="_DKbPxZu">
					<idno type="grant-number">N00014-10-1-0933</idno>
				</org>
				<org type="funding" xml:id="_vTYvxVt">
					<idno type="grant-number">N00014-11-C-0308</idno>
				</org>
				<org type="funding" xml:id="_Hn5CxEq">
					<idno type="grant-number">CNS 1028381</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video clip Queries Correct answers</head><p>Who is there in the designated area?</p><p>A human.</p><p>What is he doing? Buying food; waiting; walking; picking up food.</p><p>Where does he buy food?</p><p>At the food truck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why does he buy food?</head><p>He is hungry.</p><p>When does he pick up food? 0:46-0:51.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple bernoulli relevance models for image and video annotation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling semantic aspects for cross-media image indexing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1802" to="1817" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch Ütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A stochastic grammar of images</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends. Comput. Graph. Vis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="362" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image parsing with stochastic scene grammar</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parsing video events with goal inference and intent prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Using causal induction in humans to learn and infer causality from video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>CogSci</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The stanford parser: A statistical parser</title>
		<ptr target="http://nlp.stanford.edu/software/lex-parser.shtml" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint visual-text modeling for automatic retrieval of multimedia documents</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ircing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pytlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virga</surname></persName>
		</author>
		<editor>MULTIMEDIA</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Harmonium models for semantic video representation and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using webcast text for semantic event detection in broadcast sports video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integration of visual and text-based approaches for the content labeling and classification of photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Probabilistic models of text and images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Animals on the web</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Textual query of personal photos facilitated by large-scale web data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1022" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning cross-modality similarity for multinomial data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic caption generation for news images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context and hierarchy in a probabilistic image model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards scalable representations of object categories: Learning a hierarchy of parts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bottom-up/top-down image parsing with attribute grammar</title>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hierarchical and contextual model for aerial image parsing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="283" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coupled hidden markov models for complex action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Coupled hidden semi markov models for activity recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conversation scene analysis based on dynamic bayesian network and image-based gaze detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Otsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI-MLMI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recognition of composite human activities through context-free grammar based representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognition of multi-object events using attribute grammars</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An extended grammar system for learning and recognizing complex visual events</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="255" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Costsensitive top-down/bottom-up inference for multiscale activity recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CASE E : a hierarchical event representation for the analysis of videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hakeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic event logic for interval-based event recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using abduction for video-text coreference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mulkar-Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BOEMIE 2008 Workshop on Ontology Evolution and Multimedia Information Extraction</title>
		<meeting>BOEMIE 2008 Workshop on Ontology Evolution and Multimedia Information Extraction</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards mediating shared perceptual basis in situated dialogue</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A translation approach to portable ontology specifications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Acquis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="220" />
			<date type="published" when="1993-06">Jun. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The evolution of protégé: an environment for knowledge-based systems development</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gennari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Musen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fergerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Grosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crubézy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="123" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Webode: An integrated workbench for ontology representation, reasoning, and exchange</title>
		<author>
			<persName><forename type="first">O</forename><surname>Corcho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández-L Ópez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D M G</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vicente</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>EKAW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Orient: Integrate ontology engineering into industry tooling environment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">WordNet: A lexical database for English</title>
		<ptr target="http://wordnet.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cyc</title>
		<author>
			<persName><forename type="first">D</forename><surname>Foxvog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory and Applications of Ontology: Computer Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="259" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ontology learning for the semantic web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maedche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="79" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ontology learning from text: An overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ontology Learning from Text: Methods, Applications and Evaluation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ontology Learning and Population from Text: Algorithms, Evaluation and Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Commonsense</forename><surname>Reasoning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2002-12">December 2002</date>
		</imprint>
	</monogr>
	<note>nd Edition</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">RDF/XML syntax specification (revised)</title>
		<ptr target="http://www.w3.org/TR/REC-rdf-syntax/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning and-or templates for object modeling and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of event and-or grammar and semantics from video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An efficient context-free parsing algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Earley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="61" />
			<date type="published" when="1983-01">Jan. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Incorporating nonlocal information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Apache OpenNLP</title>
		<ptr target="http://opennlp.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Semantic video search using natural language queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hakeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Comparison of ontology-based semantic-similarity measures</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sundlass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annu Symp Proc</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="384" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Computing semantic similarity using ontologies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumptner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP Labs, Tech. Rep</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic similarity in biomedical ontologies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Couto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Detecting visual text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">SPARQL query language for RDF</title>
		<ptr target="http://www.w3.org/TR/rdf-sparql-query/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Apache jena</title>
		<ptr target="http://jena.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">I2T: Image parsing to text description</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generation that exploits corpusbased statistical knowledge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Langkilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-COLING</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Head-driven phrase structure grammar</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
