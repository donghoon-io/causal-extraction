<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning causal graphs using variable grouping according to ancestral relationship</title>
				<funder ref="#_9WYdAA2">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Cai</surname></persName>
							<email>cai.ming.52d@st.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University Kyoto</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hisayuki</forename><surname>Hara</surname></persName>
							<email>hara.hisayuki.8k@kyoto-u.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Liberal Arts and Sciences</orgName>
								<orgName type="institution">Kyoto University Kyoto</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning causal graphs using variable grouping according to ancestral relationship</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal discovery</term>
					<term>causal DAG</term>
					<term>conditional independence test</term>
					<term>DirectLiNGAM</term>
					<term>divide-and-conquer</term>
					<term>linear regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery has drawn increasing attention in recent years. Over the past quarter century, several useful algorithms for learning causal graphs have been proposed. However, when the sample size is small relative to the number of variables, the accuracy of estimating causal graphs using existing methods decreases. In addition, some methods are not feasible when the sample size is smaller than the number of variables.</p><p>To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches (e.g., [1], [2]). For learning the entire causal graph, the divide-and-conquer approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal structure learning algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal graphs, especially when the sample size is small relative to the number of variables and the model is sparse. However, existing methods are either computationally expensive or do not provide sufficient accuracy when the sample size is small.</p><p>This paper proposes a new algorithm for grouping variables according to the ancestral relationships among the variables, assuming that the causal model is LiNGAM <ref type="bibr" target="#b2">[3]</ref>, where the causal relationships are linear, and the exogenous variables are mutually independent and distributed as continuous non-Gaussian distributions. We call the proposed algorithm CAG (Causal Ancestral-relationship-based Grouping). The time complexity of the ancestor finding in CAG is shown to be cubic to the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM [4] without grouping variables and other divideand-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the model is sparse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, inference using high-dimensional causal models for observational data has played a pivotal role in various fields, such as econometrics <ref type="bibr" target="#b4">[5]</ref>, biology <ref type="bibr" target="#b5">[6]</ref>, and psychology <ref type="bibr" target="#b6">[7]</ref>. For the high dimensional causal inference, the structural causal model (SCM, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>) defined by a directed acyclic graph (DAG) has been extensively used. Since the causal relationships among variables are usually unknown, we also need to learn the causal DAG before we use the SCM.</p><p>Over the past quarter century, several practical algorithms for learning causal DAGs without latent confounders have been proposed. These algorithms are classified into several types.</p><p>The constraint-based methods use the conditional independence (CI) relationships among variables to determine causal directions. The PC algorithm <ref type="bibr" target="#b9">[10]</ref> is a typical example. Using hypothesis tests, the PC algorithm first estimates CI relationships between all pairs of variables. Then, it estimates the skeleton of a causal DAG and the directions of edges in the skeleton in turn. However, the time complexity of the PC algorithm, in the worst case, is exponential to the number of variables. Hence, the PC algorithm is not feasible for highdimensional data.</p><p>The greedy equivalent search (GES) <ref type="bibr" target="#b10">[11]</ref> is an algorithm that learns causal structures using a model selection criterion such as BIC <ref type="bibr" target="#b11">[12]</ref>. This type of method is called the scorebased method. As Chickering et al. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> have proven, however, GES belongs to NP-hard, and its application to highdimensional causal models is also impractical.</p><p>It is important to note that the constraint-based and scorebased methods can only identify causal DAGs up to the Markov equivalence class, the set of all DAGs compatible with the inferred CI relationships. To fully identify causal DAGs requires additional constraints on the causal model.</p><p>Shimizu et al. <ref type="bibr" target="#b2">[3]</ref> considered the linear structural equation model as the causal model, where exogenous variables are mutually independent, and the distributions of the exogenous variables are continuous and non-Gaussian. They called the model linear non-Gaussian acyclic model (LiNGAM) and showed that it is possible to fully identify the causal DAG that defines LiNGAM using independent component analysis (ICA-LiNGAM) <ref type="bibr" target="#b14">[15]</ref>. However, ICA-LiNGAM tends to converge to a locally optimal solution when the dimension of the model is high, resulting in lower estimation accuracy (e.g., <ref type="bibr" target="#b3">[4]</ref>). To overcome this difficulty, Shimizu <ref type="bibr" target="#b3">[4]</ref> proposed DirectLiNGAM that estimates causal DAGs using linear regression between variables and CI tests.</p><p>Hoyer et al. <ref type="bibr" target="#b15">[16]</ref> and Zhang and Hyvärinen <ref type="bibr" target="#b16">[17]</ref> generalized LiNGAM to nonlinear and showed that when the model is arXiv:2403.14125v1 [stat.ML] 21 Mar 2024 nonlinear, the causal DAG is identifiable even if the exogenous variables are Gaussian distributed.</p><p>However, the estimation accuracy of existing methods tends to decrease as the sample size decreases relative to the number of variables. Furthermore, DirectLiNGAM is not feasible when the sample size is smaller than or equal to the number of variables (e.g., <ref type="bibr" target="#b17">[18]</ref>).</p><p>Cai et al. <ref type="bibr" target="#b0">[1]</ref> proposed the scalable causation discovery algorithm (SADA) to address this problem. The main idea of SADA is to group variables into several subsets according to the CI relationships between the variables, then learn the causal DAG of each group, merge each result, and return the entire causal DAG estimate. In this method, the causal DAG learning algorithm is applied to each smaller group of variables, reducing the sample size required for it to work. Through simulation studies, Cai et al. <ref type="bibr" target="#b0">[1]</ref> showed that SADA improves LiNGAM concerning estimation accuracy. In SADA, however, the time complexity of grouping variables is of exponential order for the number of variables. In addition, SADA has a severe drawback in that even if the correct CI relationships are known, the marginal model for the variables in each group is not necessarily the causal model defined by the sub-DAG induced by the variables in each group. This means the estimator of a causal DAG estimated by SADA may not be consistent depending on the true causal DAG (e.g., <ref type="bibr" target="#b1">[2]</ref>).</p><p>Zhang et al. <ref type="bibr" target="#b1">[2]</ref> proposed another algorithm for grouping variables called causality partitioning (CAPA). CAPA requires only low-order CI relationships among variables to group variables. Letting p be the number of variables and σ max be the maximum order of the conditioning set, the time complexity of CAPA is O(p σmax+2 ). CAPA guarantees that if the correct low-order CI relationships are known, the marginal model for each group of variables is the causal model defined by the sub-DAG induced by the variables in each group. However, especially in the case of DAGs with high outdegree and low indegree, CAPA tends not to group variables finely enough to improve estimation accuracy even if the true conditional independence relationships are known.</p><p>Maeda and Shimizu <ref type="bibr" target="#b18">[19]</ref> proposed the repetitive causal discovery (RCD), which is intended to be applied to the model with latent confounders. RCD can also be applied to the models without latent confounders. RCD first determines the ancestral relationships among variables using linear regression and CI tests and creates a list of ancestor sets for each variable. The parent-child relationships between variables are determined from the CI relationships among variables in each estimated ancestor set. RCD can be interpreted as one of the divide-and-conquer approaches because RCD splits the entire variable into families of ancestor sets for each variable and then learns causal DAGs for each group.</p><p>In this paper, we propose another scheme for grouping variables according to the ancestral relationships between the variables. We also assume LiNGAM for the causal model. The proposed method uses the algorithm to find variables' ancestor sets in RCD. When the true causal DAG is connected, and the ancestor sets are correctly estimated, the variable grouping obtained by the proposed method is the family of maximal elements in the family of the union of each variable and its ancestors. The time complexity of the proposed algorithm for grouping variables is the third order of the number of variables. Suppose the true ancestor sets are correctly estimated. In that case, the marginal model for the variables in each group obtained by the proposed method is always the LiNGAM defined by the sub-DAG induced by the variables in each group, and no edge exists across different groups. Therefore, the entire causal DAG can be consistently estimated by applying the causal structure learning algorithm such as DirectLiNGAM to each group and merging the results. We call the proposed procedure for grouping variables the causal ancestral-relationship-based grouping (CAG).</p><p>Extensive computer experiments show that CAG outperforms the original DirectLiNGAM without grouping variables, CAPA, and RCD regarding estimation accuracy and computation time when the sample size is small relative to the number of variables and the true causal DAG is sparse.</p><p>The rest of this paper is organized as follows: Section II summarizes some existing causal structure learning algorithms and clarifies the position of the proposed method. Section III describes the details of the proposed method. Section IV confirms the proposed methods's usefulness through computer experiments. Section V concludes the paper. The pseudo-code for the proposed method is provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LiNGAM and Variants</head><p>Let X = (x 1 , . . . , x p ) ⊤ be a p-dimensional random vector. In the following, we identify X with the variable set. Assume that the causal relationship among the variables is acyclic and linear. LiNGAM <ref type="bibr" target="#b2">[3]</ref> is expressed by</p><formula xml:id="formula_0">X = BX + e,<label>(1)</label></formula><p>where the error term e = (e 1 , . . . , e p ) ⊤ is assumed to be independently distributed as continuous non-Gaussian distributions. B = {b ij } is a p × p coefficient matrix. b ij represents the direct causal effect from x j to x i . b ij = 0 indicates the absence of the direct causal effect from x j to x i . We note that it is possible to transform the matrix B into a strictly lower triangular matrix by permuting the rows and columns when a DAG defines the causal relationship between X.</p><p>The model ( <ref type="formula" target="#formula_0">1</ref>) is rewritten by</p><formula xml:id="formula_1">X = (I -B) -1 e,<label>(2)</label></formula><p>where I denotes the p × p identity matrix. Noting that (2) is equivalent to the independent component analysis (ICA) model, Shimizu et al. <ref type="bibr" target="#b2">[3]</ref> showed that B is identifiable and proposed an algorithm for estimating B. The algorithm is known as ICA-LiNGAM. However, as the number of variables increases, ICA-LiNGAM is more likely to converge to a locally optimal solution, which reduces the accuracy of causal DAG estimation. To overcome this problem, Shimizu et al. <ref type="bibr" target="#b3">[4]</ref> proposed another algorithm for estimating B named DirectLiNGAM. When linear regression analyses are conducted in two ways for each pair of variables, with one of them as the dependent variable and the other as the independent variable, if the independent variable and the residuals are mutually independent in only one of the models, the independent variable in that model will precede the dependent variable in causal order. After determining the causal order of all variables, DirectLiNGAM estimates the set of parents of each variable by sparse estimating a linear regression model with each variable as the dependent variable and all variables preceding it in the causal order as independent variables. This approach ensures no edges violate the estimated causal order. DirectLiNGAM is also less accurate when the number of variables is large relative to the sample size. DirectLiNGAM tends to output redundant edges even when the sample size is large.</p><p>The computational cost of DirectLiNGAM is higher than that of ICA-LiNGAM. The time complexity of Di-rectLiNGAM is O(np 3 M 2 + p 4 M 3 ), where n is the sample size, p is the number of variables, and M is the maximal rank found by the low-rank decomposition used in the kernel-based independence measure <ref type="bibr" target="#b3">[4]</ref>. Since DirectLiNGAM requires estimating a linear regression model, DirectLiNGAM is not feasible when n is smaller than p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scalable Causation Discovery Algorithm (SADA)</head><p>Cai et al. <ref type="bibr" target="#b0">[1]</ref> proposed a divide-and-conquer approach called the scalable causation discovery algorithm (SADA) to enhance the scalability of causal DAG learning algorithms. SADA first splits the variable set X into two or more subsets. Then, a causal DAG learning algorithm such as DirectLiNGAM is applied to each group of variables, and finally, all the results are merged to estimate the entire causal DAG. Because SADA applies the causal DAG learning algorithm to each group with a smaller number of variables, if the variables are grouped correctly, SADA is expected to improve the estimation accuracy when the sample size is small relative to the number of the entire variable set. SADA is feasible even if the sample size is smaller than the number of variables as long as it is larger than the number of variables in each group.</p><p>The procedure for grouping X into two subsets by SADA is as follows. SADA first randomly selects two variables x i , x j ∈ X satisfying x i ⊥ ⊥ x j | (X \ {x i , x j }) and finds the smallest X ⊆ X\{x i , x j } with respect to the inclusion relation satisfying x i ⊥ ⊥ x j | X. The disjoint subsets X 1 , X 2 , and C are computed according to the following procedure.</p><p>1) Initialize with</p><formula xml:id="formula_2">X 1 = {x i }, X 2 = {x j }, C = { X}. 2) For all w ∈ X \ (X 1 ∪ X 2 ∪ C) (a) if w ⊥ ⊥ X 2 | Ĉ for ∃ Ĉ ⊆ C, then X 1 ← X 1 ∪ {w} (b) if w ⊥ ⊥ X 1 | Ĉ for ∃ Ĉ ⊆ C, then X 2 ← X 2 ∪ {w} (c) else C ← C ∪ {w} 3) For all s ∈ C (a) if s ⊥ ⊥ X 2 | Ĉ for ∃ Ĉ ⊆ C \ {s}, then X 1 ← X 1 ∪ {s} and C ← C \ {s} (b) if s ⊥ ⊥ X 1 | Ĉ for ∃ Ĉ ⊆ C \ {s}, then X 2 ← X 2 ∪ {s} and C ← C \ {s} 4) Return X 1 , X 2 and C</formula><p>In step 2, the intermediate set C tends to be large, and step 3 downsizes C as much as possible.</p><p>The output of SADA is two subsets V 1 = X 1 ∪ C and V 2 = X 2 ∪ C. This algorithm can be repeated recursively to group variables into smaller subsets. In the implementation, the lower bound of the number of variables in each group is set to θ, and SADA is applied to V 1 or V 2 to create even smaller groups only when</p><formula xml:id="formula_3">|V 1 | &gt; θ or |V 2 | &gt; θ.</formula><p>If the number of variables is less than or equal to θ, or if we cannot find</p><formula xml:id="formula_4">x i , x j ∈ V k satisfying x i ⊥ ⊥ x j | (V k \ {x i , x j }), k = 1, 2, no further grouping is made.</formula><p>We illustrate the SADA procedure to split X into two subsets where the true causal DAG is the one in Fig 1 . Table <ref type="table">I</ref> shows the process of grouping variables x 1 , . . . , x 9 into two subsets. Since both x 3 ⊥ ⊥ x 9 | {x 1 , x 2 , x 4 . . . x 8 } and x 3 ⊥ ⊥ x 9 hold in the model defined by the DAG in Fig. <ref type="figure">1</ref>, we can initialize with X 1 = {x 3 }, X 2 = {x 9 } and C = ∅. We then use the CI tests to determine one by one to which of X 1 , X 2 , and C the remaining variables belong. This procedure returns</p><formula xml:id="formula_5">X 1 = {x 1 , x 2 , x 3 , x 5 , x 6 }, X 2 = {x 7 , x 8 , x 9 }, C = {x 4 } and hence V 1 = {x 1 , x 2 , x 3 , x 4 , x 5 , x 6 } and V 2 = {x 4 , x 7 , x 8 , x 9 }.</formula><p>In this case, we note that the marginal models for V 1 and V 2 are defined by the sub-DAGs induced by V 1 and V 2 , respectively. If the initial values of X 1 , X 2 , and C or the scanning order of w ∈ X \ (X 1 ∪ X 2 ∪ C) changes, V 1 and V 2 may also change.</p><formula xml:id="formula_6">x 1 x 2 x 3 x 4 x 5</formula><p>x 6</p><p>x 7</p><p>x 8</p><p>x 9</p><p>Fig. <ref type="figure">1</ref>. A causal DAG with nine variables.</p><p>Suppose that X is grouped into V 1 , . . . , V K by the SADA's variable grouping procedure. SADA applies a causal structure learning algorithm such as DirectLiNGAM to each of the K groups. Let Ĝ1 = (V 1 , E 1 ), . . . , ĜK = (V K , E K ) be the estimated K DAGs for each group, where</p><formula xml:id="formula_7">E k ⊂ V k × V k , k = 1, . . . , K are the set of directed edges in Ĝk . Then, the entire causal DAG is estimated by Ĝ = (X, E 1 ∪ • • • ∪ E K ).</formula><p>However, SADA has several practical problems. In SADA, the marginal model Ĝk , k = 1, . . . , K is not necessarily the causal model induced by V k even if we knew the correct CI relationships between variables. Assume that the true causal DAG is Fig 2 and that θ ≤ 3. Since x 1 ⊥ ⊥ x 4 | (x 2 , x 3 ), we can initialize with X 1 = {x 1 }, X 2 = {x 4 }, and C = {x 2 , x 3 }, which does not need further variable checking. As a result, we group the original variable set into</p><formula xml:id="formula_8">V 1 = {x 1 , x 2 , x 3 } and V 2 = {x 2 , x 3 , x 4 }. For V 1 , the marginal model is defined by TABLE I THE GROUPING PROCESS OF THE VARIABLES IN FIG. 1 IN SADA. Step X X 1 C X 2 Initial x 1 , x 2 , x 4 , x 5 , x 6 , x 7 , x 8 x 3 ϕ x 9 Check x 1 x 2 , x 4 , x 5 , x 6 , x 7 , x 8 x 1 , x 3 ϕ x 9 Check x 2 x 4 , x 5 , x 6 , x 7 , x 8 x 1 , x 2 , x 3 ϕ x 9 Check x 4 x 5 , x 6 , x 7 , x 8 x 1 , x 2 , x 3 x 4 x 9 Check x 5 x 6 , x 7 , x 8 x 1 , x 2 , x 3 x 5 x 4 x 9 Check x 6 x 7 , x 8 x 1 , x 2 , x 3 x 5 , x 6 x 4 x 9 Check x 7 x 8 x 1 , x 2 , x 3 x 5 , x 6 x 4 x 7 , x 9 Check x 8 ϕ x 1 , x 2 , x 3 x 5 , x 6 x 4 x 7 , x 8 , x 9</formula><p>the sub-DAG induced by x 1 , x 2 and x 3 . However, the marginal model with respect to V 2 is not the model defined by the sub-DAG induced by (x 2 , x 3 , x 4 ) because x 2 ̸ ⊥ ⊥ x 3 .</p><p>x 1</p><formula xml:id="formula_9">x 2 x 3 x 4</formula><p>Fig. <ref type="figure">2</ref>. An example of SADA not working.</p><p>SADA does not guarantee that it always returns a DAG. Cycles may appear when DAG estimates for each variable group are merged. Suppose V 1 and V 2 share x 1 and x 2 . If</p><formula xml:id="formula_10">x 1 → x 2 ∈ E 1 , x 2 → x 1 ∈ E 2 , the cycle x 1 → x 2 → x 1</formula><p>appears after merging Ĝ1 and Ĝ2 . Therefore, SADA needs to remove redundant edges in cycles.</p><p>In addition, SADA requires too many CI tests for grouping variables. Consider the simple model where x 1 , . . . , x p are mutually independent and no edge exists in the true DAG. In this case, any x i and x j satisfy x i ⊥ ⊥ x j | (X \ {x i , x j }). In the worst case, 2 p-2 CI tests are required to ensure Ĉ = ∅. Thus, SADA is not feasible for high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Causality Partitioning (CAPA)</head><p>Zhang et al. <ref type="bibr" target="#b1">[2]</ref> proposed a refined grouping scheme called the causality partitioning (CAPA). Define the order of the CI test by the number of variables in the conditioning set. CAPA reduces time complexity by setting an upper limit on the order of CI tests and using only CI tests of orders below that limit.</p><p>When the maximum order of the CI test is σ max , the time complexity of CAPA is shown to be O(p σmax+2 ). In implementation, σ max is set to small numbers, like two or three.</p><p>The procedure for grouping X into two subsets by CAPA is as follows. Let σ be the current order of the conditioned set. Let M σ = {M σ ij } be a p × p matrix such that</p><formula xml:id="formula_11">M σ ij = 0 x i ⊥ ⊥ x j | Z, ∃Z ⊂ X \ {x i , x j } s.t. |Z| = σ 1 otherwise.</formula><p>Let G σ = (X, E σ ) be an undirected graph with M σ as its adjacency matrix. Let</p><formula xml:id="formula_12">d i = p r=1,r̸ =i M σ ir , i = 1, . . . , p be the degrees of x i in G σ . 1) Initialize with σ = 0 2) Initialize with A = B = C = D = ∅ 3)</formula><p>For all x i ∈ X in ascending order by d i , i = 1, . . . , p do at most one of the followings so that</p><formula xml:id="formula_13">A ̸ = ∅, B ̸ = ∅ at output (a) A ← A ∪ {x i } if ∀x j ∈ B, (x i , x j ) / ∈ E σ (b) B ← B ∪ {x i } if ∀x j ∈ A, (x i , x j ) / ∈ E σ 4) C ← X \ (A ∪ B) 5) For all x i ∈ A ∪ B D ← D ∪ {x i } if ∃x j ∈ C, (x i , x j ) ∈ E σ 6) X 1 = A ∪ C ∪ D, X 2 = B ∪ C ∪ D 7) Return {X 1 , X 2 } if max(|X 1 |, |X 2 |) &lt; |X| Else if σ = σ max , exit Else σ ← σ + 1 and go to 2 Let S = X 1 ∩ X 2 .</formula><p>When we know the true CI relationship between X, S always d-separates X 1 \ S and X 2 \ S in the true causal DAG. Therefore, the marginal model for X 1 and X 2 is guaranteed to be defined by the sub-DAG induced by X 1 and X 2 , respectively. In general, the ascending order of d i , i = 1, . . . , n is not unique, and the output {X 1 , X 2 } may change depending on the choice of the ascending order. In Step 3, both 3(a) and 3(b) may be possible depending on x i , in which case one of them is randomly selected, which may also change the output.</p><p>We illustrate the CAPA procedure with σ = 1 to create two subsets where the true causal DAG is the one in Fig 1 . 
When σ = 1, G σ is an undirected graph in Fig. <ref type="figure">3</ref>. Table <ref type="table">II</ref> shows an example of the results of Step 3 and Step 4 when x 3 , x 5 , x 6 , x 9 , x 2 , x 8 , x 1 , x 7 , x 4 is used as the order of x i by d i . Since C = {x 4 }, we have D = {x 1 , x 5 , x 6 , x 7 }. Therefore X 1 and X 2 are</p><formula xml:id="formula_14">X 1 = {x 1 , x 2 , x 3 , x 4 , x 5 , x 6 , x 7 } X 2 = {x 1 , x 4 , x 5 , x 6 , x 7 , x 8 , x 9 }, respectively.</formula><p>In this example,</p><formula xml:id="formula_15">|X 1 | = 7, |X 2 | = 7</formula><p>, so the size of each group is not so small. CAPA can also be repeated recursively to split variables into smaller subsets. CAPA tends to fail to group finely in causal DAGs that contain vertices with high outdegree and low indegree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Repetitive Causal Discovery (RCD)</head><p>RCD <ref type="bibr" target="#b18">[19]</ref> is a novel causal structure learning algorithm that can be applied even when the model contains latent confounders. RCD also assumes LiNGAM, i.e., the causal</p><formula xml:id="formula_16">x 1 x 2 x 3 x 4 x 5</formula><p>x 6</p><p>x 7</p><p>x 8</p><p>x 9</p><p>Fig. <ref type="figure">3</ref>. Gσ with σ = 1 for the DAG in Fig. <ref type="figure">1</ref> TABLE II THE GROUPING PROCESS OF THE VARIABLES IN FIG. <ref type="figure">1</ref>.</p><formula xml:id="formula_17">order d i A B C x 3 1 x 3 ∅ ∅ x 5 1 x 3 x 5 ∅ x 6 1 x 3 x 5 , x 6 ∅ x 9 1 x 3 x 6 , x 6 , x 9 ∅ x 2 2 x 2 , x 3 x 5 , x 6 , x 9 ∅ x 8 2 x 2 , x 3 x 5 , x 6 , x 8 , x 9 ∅ x 1 2 x 1 , x 2 , x 3 x 5 , x 6 , x 8 , x 9 ∅ x 7 2 x 1 , x 2 , x 3 x 5 , x 6 , x 7 , x 8 , x 9 ∅ x 4 4 x 1 , x 2 , x 3 x 5 , x 6 , x 7 , x 8 , x 9 x 4</formula><p>relationships are linear, and the exogenous variables are independently distributed as continuous non-Gaussian distributions. The processes in RCD are divided into the following three parts: ancestral relationship finding, parental relationship finding, and confounder determining. The first step in the RCD is to determine the ancestral relationship between variables by repeatedly conducting simple linear regressions and independence tests on each variable pair, in a similar way as when determining causal order in DirectLiNGAM, and to create a list of ancestor sets for each variable. Let Anc i be the estimated ancestor set of x i .</p><p>Once ancestral relationships are estimated, RCD extracts parent-child relationships between each pair of variables using CI tests. Assume x j ∈ Anc i . For x i and x j , if x i ̸ ⊥ ⊥ x j | Anc i \ {x j }, then x j can be determined as a parent of x i . If a variable pair in the RCD's output for which the direction of causality cannot be identified exists, we conclude that there are latent confounders between them.</p><p>Our study assumes that the causal model does not contain latent confounders. RCD can also be applied to the model without latent confounders. RCD is interpreted as another divide-and-conquer approach to improve scalability because it estimates the causal DAG by estimating the parent-child relationship between variables within each variable's ancestor set. Our proposed method also uses the algorithm to find ancestral relationships in RCD to group variables.</p><p>RCD may return a graph that contains cycles even when the sample size is large. This is because errors in estimating ancestral relationships lead to an incorrect estimation of the parent-child relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Positioning of the Proposed Method</head><p>The proposed method also assumes LiNGAM <ref type="bibr" target="#b0">(1)</ref>. Like SADA and CAPA, the proposed method begins by grouping variables into multiple subsets based on the ancestral relationships between variables. The ancestral relationships are estimated in the same way as in RCD. When the true causal DAG is connected, the proposed method defines the variable grouping by the maximal elements in the family of the union of each variable and its ancestors. Thus, if the ancestral relationships are correctly estimated, variables are partitioned into as many groups as the number of sink nodes in the true causal DAG. The proposed method groups finely for DAGs with high outdegree and low indegree. The proposed method's time complexity for variable grouping is O(p 3 ), which is the same order as CAPA with σ max = 1.</p><p>When the ancestral relationships among the variables are correctly estimated, the marginal model for variables in each group obtained by the proposed method is the LiNGAM defined by the sub-DAG induced by the variables in each group.</p><p>In RCD, the estimated ancestral relationships are used to estimate parent-child relationships among variables. In contrast, the estimated ancestral relationships are only used to group variables in the proposed method. For each group, DirectLiNGAM is applied to estimate sub-DAGs, which are then merged to estimate the entire causal DAG.</p><p>Section III provides a more comprehensive description of our proposed method. Section IV compares the performance of the proposed method, the original DirectLiNGAM, CAPA, and RCD in the absence of latent confounders by computer experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CAUSAL ANCESTRAL-RELATIONSHIPS-BASED GROUPOING (CAG)</head><p>This section will introduce our proposed algorithm in detail. Let X = (x 1 , . . . , x p ) ⊤ be a p-dimensional random vector. Let G = (X, E) be the true causal DAG, where E ⊂ X × X is the set of edges in G. We assume that X follows LiNGAM (1) without latent confounders. Let Anc i and P a i be the sets of ancestors and parents of x i in G, respectively. CA ij := Anc i ∩ Anc j is the set of common ancestors of x i and x j in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Finding the Ancestral Relationships Between the Variables</head><p>This subsection summarizes the algorithm for finding the ancestral relationships between variables in X in RCD <ref type="bibr" target="#b18">[19]</ref>. To determine the ancestral relationship between x i and x j , i ̸ = j, Maeda and Shimizu <ref type="bibr" target="#b18">[19]</ref> considered the simple regression models</p><formula xml:id="formula_18">x i = β ij x j + u i , x j = β ji x i + u j ,<label>(3)</label></formula><p>where u i and u j are error terms. Maeda and Shimizu <ref type="bibr" target="#b18">[19]</ref> focused on the independence relationship between the independent variable and the error term in each model. The following proposition holds for the ancestral relationship between x i and x j .</p><p>Proposition 1 (Maeda and Shimizu <ref type="bibr" target="#b18">[19]</ref>). One of the following four conditions holds for the ancestral relationship between x i and x j .</p><formula xml:id="formula_19">1) If x i ⊥ ⊥ x j , then x i / ∈ Anc j ∧ x j / ∈ Anc i . 2) If x j ⊥ ⊥ u i , then x i ∈ Anc j . 3) If x i ⊥ ⊥ u j , then x j ∈ Anc i . 4) If x i ̸ ⊥ ⊥ u j ∧ x j ̸ ⊥ ⊥ u i , then CA ij ̸ = ∅.</formula><p>This algorithm first checks which conditions 1 through 4 in Proposition 1 holds for each variable pair (x i , x j ). For implementation, the error terms u i and u j are replaced with the OLS residuals. If (x i , x j ) satisfies condition 4, the determination of the ancestral relationship between (x i , x j ) is withheld. Let R be the set of variable pairs (x i , x j ) that satisfies condition 4. Let CA * ij be the set of common ancestors of (x i , x j ) found during checking Proposition 1 for all pairs (x i , x j ).</p><p>Assume that (x i , x j ) ∈ R. Then, CA * ij forms part of the backdoor path for x i and x j . To remove the influence of CA * ij from x i and x j , consider the regression models</p><formula xml:id="formula_20">x i = α ⊤ ij • CA * ij + v i , x j = α ⊤ ji • CA * ij + v j ,<label>(4)</label></formula><p>where we identify CA * ij with the vector of common ancestors of (x i , x j ). Furthermore, consider the following regression models for the error terms v i and v j ,</p><formula xml:id="formula_21">v i = β ij v j + u i , v j = β ji v i + u j .<label>(5)</label></formula><p>Then Proposition 1 is generalized as follows.</p><p>Proposition 2 (Maeda and Shimizu <ref type="bibr" target="#b18">[19]</ref>). One of the following four conditions holds for the ancestral relationship between</p><formula xml:id="formula_22">(x i , x j ) ∈ R. 1) If v i ⊥ ⊥ v j , then x i / ∈ Anc j ∧ x j / ∈ Anc i . 2) If v j ⊥ ⊥ u i , then x i ∈ Anc j . 3) If v i ⊥ ⊥ u j , then x j ∈ Anc i . 4) If v i ̸ ⊥ ⊥ u j and v j ̸ ⊥ ⊥ u i , then CA ij \ CA * ij ̸ = ∅.</formula><p>Proposition 1 is the case where CA ij = ∅. For implementation, the error terms v and u are replaced with OLS residuals. If (x i , x j ) satisfies condition 4, the determination of the ancestral relationship between x i and x j is withheld. After checking Proposition 2 for all (x i , x j ) ∈ R, update CA * ij and R. If R ̸ = ∅, recheck Proposition 2. Theoretically, if the model contains no latent confounders, the procedure in Proposition 2 can be repeated until R = ∅ to completely determine the ancestral relationship of all (x i , x j ).</p><p>The pseudo-code of this algorithm is described in Algorithm 1 in Appendix A. Since the linearity is assumed between variables, Pearson's correlation test <ref type="bibr" target="#b19">[20]</ref> can be used to test the marginally independence, x i ⊥ ⊥ x j and v i ⊥ ⊥ v j . For testing the independence of independent variables and residuals, Shimizu and Maeda <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> used the Hilbert-Schmidt independence criterion (HSIC) <ref type="bibr" target="#b21">[22]</ref>. This paper uses a Kernel-based conditional independence (KCI) test <ref type="bibr" target="#b22">[23]</ref> because the accuracy of the test remains the same, and the computation time is faster.</p><p>Simply checking the conditions of Proposition 1 or Proposition 2 may yield ancestor relationships such that the estimated graph contains directed cycles due to errors in testing. The proposed method adds a heuristic procedure to avoid directed cycles. See Algorithm 1 in the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grouping Variables and Merging Results</head><p>This subsection introduces a procedure for grouping X into several subsets according to the estimated ancestral relationships among variables and merging the estimated causal DAGs for each group.</p><p>Let Anc i be the estimated ancestor set of x i . Define V by the output of the following procedures.</p><p>1) Let V 0 be the family of maximal sets in</p><formula xml:id="formula_23">{x i } ∪ Anc i , i = 1, . . . , p 2a) If |v| &gt; 1 for all v ∈ V 0 , then V ← V 0 2b) Otherwise, V ← V 0 For all v ∈ V 0 with |v| = 1 V ← (V \ v) ∪    v ′ ∈V0,v ′ ̸ =v {v ∪ v ′ }    3) Return V Definition 1.</formula><p>Define V as the grouping of X obtained by the ancestral relationships among X.</p><p>We call the series of procedures for obtaining V the causal ancestral-relationship-based grouping (CAG). If the ancestral relationships among X are correctly estimated and V = V 0 , each element of V is the union of a sink node in G and its ancestors. We also note that v∈V v = X. Now, we have the following theorem.</p><p>Theorem 1. Assume that we know the correct ancestral relationships between variables. Define G v be the sub-DAG of G induced by v ∈ V. Then, the marginal model for v is the LiNGAM defined by G v .</p><p>Proof. The probability density function of X is written by</p><formula xml:id="formula_24">p(x) = p i=1 p(x i | P a i ) = i:xi∈v p(x i | P a i ) • j:xj / ∈v p(x j | P a j ).</formula><p>We note that when x i ∈ v, P a i does not include the variables x j / ∈ v. By integrating p(x) out according to the reverse causal order of X \ v, we have</p><formula xml:id="formula_25">p(v) = i:xi∈v p(x i | P a i ),</formula><p>which is the LiNGAM defined by G v .</p><p>Theorem 1 claims that we can consistently estimate the sub-DAGs G v for v ∈ V by applying a causal structure learning algorithm such as DirectLiNGAM to v. Once we obtain the estimates Ĝv = (v, E v ), where</p><formula xml:id="formula_26">E v = (v × v) ∩ E, the entire causal graph G can be estimated by Ĝ = (X, v∈V E v ).</formula><p>When the sample size is small, many ancestral relationships cannot be detected due to the type II error of the CI test in the CAG, so there may be many variables for which the ancestor set is empty, resulting in many groups consisting of a single variable in V 0 . Using such grouping would make the causal DAG estimates too sparse. So here, the group consisting of a single variable in V 0 is merged with the other groups according to the above procedure 2b. Note that Theorem 1 holds even if V 0 contains a group consisting of one variable.</p><p>Table <ref type="table" target="#tab_0">III</ref> presents the correct Anc i and v i for i = 1, . . . , 9 of the DAG in Figure <ref type="figure">1</ref>. In this case, we can easily see that V 0 = V = {v 3 , v 5 , v 6 , v 9 }. By applying the causal structure learning algorithm to each element of V, we can consistently estimate the sub-DAGs as shown in Fig. <ref type="figure" target="#fig_0">4</ref>. By merging the sub-DAGs as (X, E 3 ∪ E 5 ∪ E 6 ∪ E 9 ), we can obtain the true causal DAG in Figure <ref type="figure">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variables</head><p>Anc</p><formula xml:id="formula_27">i v i x 1 ϕ x 1 x 2 x 1 x 1 , x 2 x 3 x 1 , x 2 x 1 , x 2 , x 3 x 4</formula><p>x 1 , x 7</p><p>x 1 , x 4 , x 7 x 5</p><p>x 1 , x 4 , x 7</p><p>x 1 , x 4 , x 5 , x 7 x 6</p><p>x 1 , x 4 , x 7</p><p>x 1 , x 4 , x 5 , x 6 x 7 ϕ x 7 x 8</p><p>x 7</p><p>x 7 , x 8 x 9</p><p>x 7 , x 8 x 7 , x 8 , x 9 In the same way as SADA, the estimated entire DAG may contain cycles due to errors in learning DAGs for each group, even if the correct ancestral relationships are estimated. The proposed method implements topological sorting on the merged graph to check if a directed cycle exists in the merged graph. In SADA, the significance of each coefficient b ij for all edges in a cycle is assessed by the Wald test, and the least significant edge in the cycle is removed from the cycle. In the proposed method, since the ancestral relationships between variables are estimated, directed cycles can also be eliminated by removing edges in the cycle that contradict the estimated ancestral relationships. Another possible approach would be removing the edge in the cycle with the smallest absolute value of coefficient |b ij |. The computer experiment in Section IV compares the performance of these methods for removing cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time Complexity of Variable Grouping and Minimum Required Sample Size</head><p>It suffices to focus on the number of CI tests to evaluate the time complexity of variable grouping by CAG. The case that requires the most CI tests is when the true causal DAG is fully connected, i.e., Anc i = P a i for all x i ∈ X. In that case, since the proposed algorithm performs CI tests while removing a source node one by one from a DAG, the number of required CI tests is</p><formula xml:id="formula_28">p 2 + p -1 2 + • • • + 2 2 = O(p 3 ).</formula><p>As mentioned in Section II-A, the time complexity of Di-rectLiNGAM is O(np 3 M 2 + p 4 M 3 ). Even allowing for the time complexity of grouping variables, if n is small relative to p, CAG is expected to reduce overall computation time compared to the original DirectLiNGAM without variable grouping.</p><p>Let n max be the largest cardinality of v ∈ V. Then, the required sample size is reduced to n max + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPUTER EXPERIMENTS</head><p>This section details the results of computer experiments to compare the estimation accuracy and computation time of CAG with the original DirectLiNGAM without variable grouping, RCD in the absence of latent confounders, and CAPA with σ max = 0, 1, 2. In these experiments, CAG and CAPA used DirectLiNGAM as the causal structure learning algorithm for each variable group. In the following, we will refer to these procedures as CAG-LiNGAM and CAPA-LiNGAM, respectively. The accuracy of CAG-LiNGAM using true ancestral relationships was also computed for reference. As mentioned in Section II-D, RCD can also be applied to the model in the presence of latent confounders. This paper assumes that no latent confounder exists, so in this experiment for RCD, the procedure for determining latent confounders was omitted to estimate ancestral and parent-child relationships among variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>The number of variables p and the number of edges |E| were set to (p, |E|) = (10, 5), <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9)</ref>, <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b18">19)</ref>, (30, 29) and (40, 39). The sample sizes n were set to 11, 25, 50, 100 and 500.</p><p>The error terms e i were generated from the uniform distribution U (-1, 1). The true causal DAGs were randomly generated with (p, |E|) fixed at each iteration. In these experiments, the true causal DAGs were assumed to be sparse, and the indegrees of all vertices were set to at most one. The nonzero elements of the coefficient matrix B were randomly generated from the uniform distribution U ([-1, -0.5] ∪ [0.5, 1]) at each iteration. The number of iterations was set to 100.</p><p>Let |E c | be the number of correctly detected edges in the 100 estimated DAGs. Denote the number of redundant and missing edges in the 100 estimated DAGs by |E r | and |E m |, respectively. Note that if an edge in an estimated DAG is in the wrong direction, both |E r | and |E m | will add one. We evaluated the performance of CAG-LiNGAM and the other methods using the following indices (e.g., Zhu et al. <ref type="bibr" target="#b23">[24]</ref>).</p><p>1) The precision</p><formula xml:id="formula_29">P re := |E c | |E c | + |E r |</formula><p>is the ratio of correctly estimated edges among the edges in the estimated DAG;</p><p>2) The recall</p><formula xml:id="formula_30">Rec := |E c | |E c | + |E m | = |E c | |E|</formula><p>is the ratio of correctly estimated edges among the edges in the true DAG; 3) F -measure F := 2 × P re × Rec P re + Rec is the harmonic mean of the precision and the recall; 4) T ime is the running time in seconds for estimating 100</p><p>DAGs. Let α P be the significance level of Pearson's correlation test in CAG and RCD. Let α CI be the significance level of KCI in CAG, RCD, and CAPA. α P and α CI are set to α P ∈ {0.01, 0.05, 0.1, 0.5}, α CI ∈ {0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5}, respectively.</p><p>Experiments for p = 40 were conducted on a machine with a 3.0 GHz Intel Core i9 processor and 256 GB memory, and experiments for p = 30 were performed on a machine with a 3.0 GHz Intel Core i9 processor and 128 GB memory. The other experiments were conducted on a machine with a 2.1 GHz Dual Xeon processor and 96 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results and Discussion</head><p>Table <ref type="table" target="#tab_4">IV</ref> shows P re, Rec, F and T ime for each method for some (p, |E|, n). The experiments for CAG-LiNGAM and RCD were conducted with 4 × 7 combinations of significance levels of Pearson's correlation test and KCI for each (p, |E|, n). The experiments for CAPA-LiNGAM were conducted with 7 significance levels of KCI for each (p, |E|, n). Table <ref type="table" target="#tab_4">IV</ref> shows only the results of these methods for the significance level that maximizes F -measure.</p><p>In Table <ref type="table" target="#tab_4">IV</ref>, CAG represents CAG-LiNGAM, while CAG * refers to CAG-LiNGAM using true ancestral relationships. DLi stands for the original DirectLiNGAM without grouping variables. W, Anc, and abs under CAG and RCD represent cycle elimination methods based on the Wald test, the estimated ancestral relationship, and the absolute value of the coefficient, respectively. The numbers 0, 1, and 2 under CAPA are the σ max values. In the table, the bold numbers highlight the best performance in each experimental group (p, |E|, n), excluding CAG * .</p><p>As seen from Table <ref type="table" target="#tab_4">IV</ref>, CAG-LiNGAM outperforms the original DirectLiNGAM without the variable grouping, RCD, and CAPA-LiNGAM when the sample size is small relative to the number of variables in terms of F -measure and the recall. Although CAG requires O(p 3 ) time complexity to group variables, CAG-LiNGAM often shows shorter computation time than the original DirectLiNGAM when the sample size is small. When the sample size is large, CAG-LiNGAM takes longer computation time than the original DirectLiNGAM, but CAG-LiNGAM is superior to the original DirectLiNGAM in terms of precision and F -measure.</p><p>CAG with Wald tests takes longer computation time but is more accurate in terms of F -measure than CAG with Anc and abs. As the sample size increases, the accuracy of estimating ancestral relationships and DirectLiNGAM increases, and hence, the frequency of cycles appearing in the estimated DAG decreases, so the difference in estimation accuracy between the three CAG variants becomes smaller.</p><p>When the sample size is small, CAPA-LiNGAM may show higher precision than CAG-LiNGAM. However, the recall and F -measure are lower in many cases. This may be because when the sample size is small, it is difficult for CAPA to detect conditional independence relationships due to the type II error of the CI tests. Then, G σ becomes overly sparse, resulting in overly sparse causal DAG estimates.</p><p>When σ max is 0 and 1, the time complexity of the variable grouping of CAPA is less than or equal to that of CAG. However, the computation time for CAPA-LiNGAM is not much different from that for CAG-LiNGAM, or CAG-LiNGAM is faster than CAPA-LiNGAM. Besides, CAG-LiNGAM outperforms CAPA-LiNGAM in terms of F -measure.</p><p>It is noteworthy that RCD shows higher F -measure than CAG-LiNGAM when the sample size is large, although it takes more computation time. Conversely, when the sample size is small, CAG-LiNGAM shows higher F -measure than RCD, although it takes more computation time.</p><p>The original DirectLiNGAM shows lower precision than divide-and-conquer algorithms. This may be because the original DirectLiNGAM does not group variables, leading to redundant edges across different variable groups. In general, DAGs estimated by DirectLiNGAM tend to have more redundant edges.</p><p>Even if the true causal DAG is sparse, the subgraph induced by each variable group becomes relatively denser. In the case of directed trees, the ratio of the number of edges to the number of variable pairs is (p -1)/ p C 2 = 2/p. This means that the smaller the number of variables in each group is, the denser the sub-DAG induced by each group is. When variables are correctly grouped into smaller subsets, the true sub-DAGs induced by each group become denser, and there are no edges across groups. Therefore, if ancestral relationships can be accurately estimated, applying DirectLiNGAM to each group is expected to avoid more redundant edges than using it for the entire variable.</p><p>The CAG uses the estimated ancestral relationships for grouping variables but does not use them to estimate the causal DAG for each group. However, since ancestral relationships partially define the structure of the causal DAG, information on ancestral relationships could be used to estimate causal DAGs. The RCD can be interpreted as using information on ancestral relationships to estimate causal DAGs. Let A c and A m be the number of correctly detected and missing ancestors in the estimated ancestor lists Anc i , i = 1, . . . , p, respectively. Define Anc rec by the recall of the list of ancestors,</p><formula xml:id="formula_31">Anc rec = A c A c + A m .</formula><p>Figure <ref type="figure" target="#fig_1">5</ref> plots Anc rec of the ancestor list for some (p, |E|, n) when using the significance levels of the CI tests that maximize the F -measure of the CAG-LiNGAM with Wald tests. This figure shows that when the sample size is small, the accuracy of the estimation of ancestor lists is quite low. When (p, |E|, n) = <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11)</ref>, <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11)</ref>, <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">25)</ref>, <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">25)</ref>, (40, 39, 50), Anc rec are exactly zeros. However, Table <ref type="table" target="#tab_4">IV</ref> shows that the accuracy of CAG-LiNGAM is not that bad. This fact suggests that applying DirectLiNGAM to each group split by the CAG may correct errors in the estimated ancestral relationships within each group. Table <ref type="table" target="#tab_2">V</ref> shows Anc rec computed from the ancestral relationships estimated by CAG and Anc rec computed from the estimated DAG in the experiments. We can see that Anc rec is significantly improved by applying DirectLiNGAM to each group.</p><p>Table <ref type="table" target="#tab_2">VI</ref> summarizes the following six indices calculated from CAG's experimental results when p = 10.   <ref type="table" target="#tab_2">VI</ref> shows that when the sample size is small, Imp &lt; W or, but Imp/Err &gt; W or/Corr. By applying Di-rectLiNGAM, the number of corrected ancestral relationships is less than that of worsened ones. However, the proportion of corrected ancestral relationships is relatively larger than that of worsened ancestral relationships, which may have improved the accuracy of causal DAG estimation.</p><p>Even with large sample sizes, Anc rec improves by applying DirectLiNGAM. However, Table <ref type="table" target="#tab_4">IV</ref> shows that the accuracy of CAG is inferior to that of RCD. When the sample size is large, DirectLiNGAM tends to include redundant edges in the estimated DAG, which may make CAG less accurate than RCD. This experiment set 28 different significance levels for the CAG's CI tests. Table VII presents the significance levels that maximize the F -measure of CAG-LiNGAM. A small significance level is often chosen when the sample size is small. In practice, however, the results are almost the same regardless of the significance level chosen. When the sample size is small, ancestral relationships can hardly be detected regardless of the significance level. In this experiment of CAG-LiNGAM with Wald tests, when (p, |E|, n) = <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11)</ref>, <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11)</ref>, <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">25)</ref>, no ancestral relationship could be detected within 100 repetitions. From Definition 1 in Section III-A, if no ancestral relationship was detected, the variable group is the family of all p C 2 variable pairs. Interestingly, even in such cases, CAG-LiNGAM shows higher F -measure than the other methods. When the sample size is moderately small, the F -measure values are large when the significance level of Pearson's correlation test is 0.01 and the significance level of KCI is about 0.2 to 0.5. When the sample size is large, the smaller the significance level, the higher F -measure.  As mentioned, CAG * is the CAG-LiNGAM when the true ancestral relationships are known. When the sample size is small, the estimation accuracy of CAG-LiNGAM is inferior to that of CAG * because CAG is less accurate. However, when p = 10 and the sample size is large, CAG-LiNGAM shows higher precision and F -measure than CAG * . As seen from Figure <ref type="figure" target="#fig_1">5</ref> and Table V, the accuracy of ancestral relationship estimation improves as the sample size increases. However, even when (p, |E|, n) = (10, 9, 500), Anc rec is 0.713, which is not close enough to 1. From Table VII, when (p, |E|, n) = (10, 9, 500), α P = 0.01 and α CI = 0.001 are used for the significance levels of the CI tests. Even with a large sample size, if small significance levels are used in the CAG, some ancestral relationships cannot be detected due to type II errors in the CI test. As a result, the number of variables in the estimated groups may be smaller than those in the true CAG grouping, resulting in fewer redundant edges than CAG * . Grouping by CAG is not the finest grouping that guarantees the identifiability of true causal DAGs. The results suggest that using finer groupings may improve the accuracy of causal DAG estimation when the sample size is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes CAG as a divide-and-conquer algorithm for learning causal DAGs. This algorithm can help improve the performance of the original DirectLiNGAM, especially when the sample size is small relative to the number of variables. CAG is based on the algorithm for finding ancestral relationships among variables in RCD. CAG-LiNGAM guarantees the identifiability of true causal DAGs. Detailed computer experiments confirm that CAG-LiNGAM may outperform the original DirectLiNGAM, RCD, and CAPA-LiNGAM in terms of F -measure when the true causal DAG is sparse, and the sample size is small relative to the number of variables.</p><p>If the true causal is connected and the correct CI relationships are estimated, CAG creates the same number of variable groups as sink nodes in the causal DAG. Therefore, when the number of sink nodes in the true causal graph is small, the number of variables in each group cannot be sufficiently small, and thus, improvement in accuracy may not be expected. Other computer experiments have shown that even if the true causal DAG is sparse, the estimation accuracy is not necessarily high when the indegrees of some vertices are larger than one. CAPA is the opposite of CAG, where the number of variable groups is small when the maximum indegree of the true causal DAG is small, while it may be possible to group variables into many variable groups even when the maximum indegree of the true causal DAG is large. As noted at the end of the previous section, the divideand-conquer algorithm is expected to be more accurate in estimating causal DAGs if finer groupings are used. The hybrid algorithm of CAG and CAPA may give fine groupings even when the indegree of the true causal DAG is larger.</p><p>We also found that the RCD has higher estimation accuracy than CAG when the sample size is large. RCD estimates causal DAGs using the estimated ancestral relationships among variables. As the sample size increases, the estimation accuracy of ancestral relationships improves. Therefore, using the estimated ancestral relationships to estimate causal DAGs may improve estimation accuracy. However, RCD is computationally expensive when the sample size is large. Using the information on the estimated ancestral relationships in DirectLiNGAM may improve the estimation accuracy of causal DAGs at a relatively low computational cost, even when the sample size is large.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of the process of the CAG</figDesc><graphic coords="7,58.83,380.51,231.33,187.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Ancrec with Wald tests for some sample sizesThe lines in {red, blue, orange, black, pink} represent (p, |E|) = (10, 5),<ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b8">9)</ref>,<ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b18">19)</ref>, (30.29), (40, 39), respectively.</figDesc><graphic coords="9,67.75,529.68,213.48,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Improvement (Imp): the number of ancestral relationships in the output DAGs that DirectLiNGAM corrects • Worsening (W or): the number of ancestral relationships in the output DAGs that DirectLiNGAM worsens</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE III</head><label>III</label><figDesc>Anc i AND v i OF THE DAG IN FIG. 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V Ancrec</head><label>V</label><figDesc>BEFORE AND AFTER APPLYING DIRECTLINGAM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Before</cell><cell></cell><cell></cell><cell>After</cell></row><row><cell cols="2">(p, |E|, n)</cell><cell>W</cell><cell></cell><cell>Anc</cell><cell>abs</cell><cell>W</cell><cell>Anc</cell><cell>abs</cell></row><row><cell cols="2">(10, 5, 11)</cell><cell>0</cell><cell></cell><cell>0.049</cell><cell>0</cell><cell cols="3">0.438 0.405 0.466</cell></row><row><cell cols="2">(10, 5, 25)</cell><cell cols="4">0.300 0.300 0.300</cell><cell cols="3">0.519 0.525 0.524</cell></row><row><cell cols="2">(10, 5, 50)</cell><cell cols="4">0.513 0.513 0.513</cell><cell cols="3">0.677 0.679 0.679</cell></row><row><cell cols="2">(10, 5, 100)</cell><cell cols="4">0.723 0.723 0.723</cell><cell cols="3">0.852 0.852 0.852</cell></row><row><cell cols="2">(10, 5, 500)</cell><cell cols="4">0.919 0.919 0.919</cell><cell cols="3">0.998 0.998 0.998</cell></row><row><cell cols="2">(10, 9, 11)</cell><cell>0</cell><cell></cell><cell>0</cell><cell>0</cell><cell cols="3">0.404 0.568 0.447</cell></row><row><cell cols="2">(10, 9, 25)</cell><cell>0</cell><cell></cell><cell>0.323</cell><cell>0</cell><cell cols="3">0.648 0.483 0.701</cell></row><row><cell cols="2">(10, 9, 50)</cell><cell cols="4">0.346 0.346 0.346</cell><cell cols="3">0.540 0.557 0.547</cell></row><row><cell cols="2">(10, 9, 100)</cell><cell cols="4">0.514 0.514 0.514</cell><cell cols="3">0.791 0.817 0.805</cell></row><row><cell cols="2">(10, 9, 500)</cell><cell cols="4">0.713 0.713 0.713</cell><cell cols="3">0.999 0.999 0.999</cell></row><row><cell cols="2">(20, 19, 25)</cell><cell cols="4">0.001 0.177 0.001</cell><cell cols="3">0.514 0.366 0.577</cell></row><row><cell cols="2">(20, 19, 50)</cell><cell cols="4">0.320 0.289 0.320</cell><cell cols="3">0.545 0.558 0.594</cell></row><row><cell cols="2">(20, 19, 100)</cell><cell cols="4">0.359 0.359 0.359</cell><cell cols="3">0.638 0.669 0.651</cell></row><row><cell cols="2">(20, 19, 500)</cell><cell cols="4">0.596 0.596 0.596</cell><cell cols="3">0.993 0.998 0.998</cell></row><row><cell cols="2">(30, 29, 50)</cell><cell cols="4">0.268 0.241 0.268</cell><cell cols="3">0.450 0.562 0.515</cell></row><row><cell cols="2">(30, 29, 100)</cell><cell cols="4">0.371 0.291 0.371</cell><cell cols="3">0.745 0.636 0.782</cell></row><row><cell cols="2">(30, 29, 500)</cell><cell cols="4">0.515 0.515 0.515</cell><cell cols="3">0.993 0.997 0.993</cell></row><row><cell cols="2">(40, 39, 50)</cell><cell cols="4">0.001 0.222 0.225</cell><cell cols="3">0.627 0.546 0.508</cell></row><row><cell cols="2">(40, 39, 100)</cell><cell cols="4">0.356 0.276 0.356</cell><cell cols="3">0.697 0.624 0.785</cell></row><row><cell cols="2">(40, 39, 500)</cell><cell cols="4">0.470 0.470 0.470</cell><cell cols="3">0.997 0.994 0.997</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="9">THE IMPROVEMENT AND WORSENING OF ANCESTRAL RELATIONSHIPS</cell></row><row><cell></cell><cell cols="7">AFTER APPLYING DIRECTLINGAM.</cell></row><row><cell>(p, |E|, n)</cell><cell cols="2">Elimination</cell><cell cols="2">Imp W or</cell><cell>Corr</cell><cell>Err</cell><cell>Imp/Err</cell><cell>W or/Corr</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>265</cell><cell>1029</cell><cell>3858</cell><cell>642</cell><cell>0.4128</cell><cell>0.2667</cell></row><row><cell>(10,5,11)</cell><cell>Anc</cell><cell></cell><cell>228</cell><cell>858</cell><cell>3858</cell><cell>642</cell><cell>0.3551</cell><cell>0.2224</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>286</cell><cell>1042</cell><cell>3858</cell><cell>642</cell><cell>0.4455</cell><cell>0.2701</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>239</cell><cell>700</cell><cell>3956</cell><cell>544</cell><cell>0.4393</cell><cell>0.1769</cell></row><row><cell>(10,5,25)</cell><cell>Anc</cell><cell></cell><cell>227</cell><cell>636</cell><cell>3956</cell><cell>544</cell><cell>0.4173</cell><cell>0.1608</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>238</cell><cell>720</cell><cell>3956</cell><cell>544</cell><cell>0.4375</cell><cell>0.1820</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>204</cell><cell>268</cell><cell>4154</cell><cell>346</cell><cell>0.5896</cell><cell>0.0645</cell></row><row><cell>(10,5,50)</cell><cell></cell><cell></cell><cell>191</cell><cell>253</cell><cell>4154</cell><cell>346</cell><cell>0.5520</cell><cell>0.0609</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>211</cell><cell>280</cell><cell>4154</cell><cell>346</cell><cell>0.6098</cell><cell>0.0674</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>794</cell><cell>1226</cell><cell>2627</cell><cell>1873</cell><cell>0.4239</cell><cell>0.4667</cell></row><row><cell>(10,9,11)</cell><cell>Anc</cell><cell></cell><cell>374</cell><cell>559</cell><cell>2627</cell><cell>1873</cell><cell>0.1997</cell><cell>0.2128</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>794</cell><cell>1226</cell><cell>2627</cell><cell>1873</cell><cell>0.4239</cell><cell>0.4667</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>657</cell><cell>678</cell><cell>2780</cell><cell>1720</cell><cell>0.3820</cell><cell>0.2439</cell></row><row><cell>(10,9,25)</cell><cell>Anc</cell><cell></cell><cell>519</cell><cell>565</cell><cell>2780</cell><cell>1720</cell><cell>0.3017</cell><cell>0.2032</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>747</cell><cell>804</cell><cell>2780</cell><cell>1720</cell><cell>0.4343</cell><cell>0.2892</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>704</cell><cell>373</cell><cell>3078</cell><cell>1422</cell><cell>0.4951</cell><cell>0.1212</cell></row><row><cell>(10,9,50)</cell><cell>Anc</cell><cell></cell><cell>570</cell><cell>273</cell><cell>3078</cell><cell>1422</cell><cell>0.4008</cell><cell>0.0887</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>728</cell><cell>412</cell><cell>3078</cell><cell>1422</cell><cell>0.5120</cell><cell>0.1339</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>682</cell><cell>130</cell><cell>3479</cell><cell>1021</cell><cell>0.6680</cell><cell>0.0374</cell></row><row><cell>(10,9,100)</cell><cell>Anc</cell><cell></cell><cell>586</cell><cell>122</cell><cell>3479</cell><cell>1021</cell><cell>0.5739</cell><cell>0.0351</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>703</cell><cell>118</cell><cell>3479</cell><cell>1021</cell><cell>0.6885</cell><cell>0.0339</cell></row><row><cell></cell><cell>Wald</cell><cell></cell><cell>626</cell><cell>32</cell><cell>3730</cell><cell>770</cell><cell>0.8130</cell><cell>0.0086</cell></row><row><cell>(10,9,500)</cell><cell>Anc</cell><cell></cell><cell>626</cell><cell>33</cell><cell>3730</cell><cell>770</cell><cell>0.8078</cell><cell>0.0088</cell></row><row><cell></cell><cell>abs</cell><cell></cell><cell>626</cell><cell>32</cell><cell>3730</cell><cell>770</cell><cell>0.8104</cell><cell>0.0086</cell></row><row><cell cols="9">• Errors in estimation (Err): the number of ancestral</cell></row><row><cell cols="9">relationships that are wrongly estimated in CAG</cell></row><row><cell cols="9">• Correct estimation (Corr): the number of ancestral rela-</cell></row><row><cell cols="8">tionships that are correctly estimated in CAG</cell></row><row><cell cols="9">• Imp/Err: the ratio of ancestral relationships incorrectly</cell></row><row><cell cols="9">estimated by CAG that DirectLiNGAM corrected</cell></row><row><cell cols="9">• W or/Corr: the ratio of ancestral relationships correctly</cell></row><row><cell cols="9">estimated by CAG that DirectLiNGAM worsened</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VII THE</head><label>VII</label><figDesc>SIGNIFICANCE LEVELS USED IN CAG-LINGAM IN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell>W</cell><cell></cell><cell></cell><cell>Anc</cell><cell></cell><cell>abs</cell></row><row><cell>(p, |E|, n)</cell><cell>α P</cell><cell>α CI</cell><cell>α P</cell><cell>α CI</cell><cell>α P</cell><cell>α CI</cell></row><row><cell>(10, 5, 11)</cell><cell cols="2">0.01 0.001</cell><cell>0.01</cell><cell>0.3</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(10, 5, 25)</cell><cell>0.01</cell><cell>0.5</cell><cell>0.01</cell><cell>0.5</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>(10, 5, 50)</cell><cell>0.01</cell><cell>0.4</cell><cell>0.01</cell><cell>0.4</cell><cell>0.01</cell><cell>0.4</cell></row><row><cell>(10, 5, 100)</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.3</cell></row><row><cell>(10, 5, 500)</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(10, 9, 11)</cell><cell>0.05</cell><cell>0.01</cell><cell>0.05</cell><cell>0.01</cell><cell>0.05</cell><cell>0.01</cell></row><row><cell>(10, 9, 25)</cell><cell cols="2">0.01 0.001</cell><cell>0.5</cell><cell>0.5</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(10, 9, 50)</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.3</cell></row><row><cell>(10, 9, 100)</cell><cell>0.01</cell><cell>0.2</cell><cell>0.01</cell><cell>0.2</cell><cell>0.01</cell><cell>0.2</cell></row><row><cell>(10, 9, 500)</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(20, 19, 25)</cell><cell cols="2">0.05 0.001</cell><cell>0.01</cell><cell>0.5</cell><cell cols="2">0.05 0.001</cell></row><row><cell>(20, 19, 50)</cell><cell>0.01</cell><cell>0.4</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.4</cell></row><row><cell>(20, 19, 100)</cell><cell>0.01</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>(20, 19, 500)</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(30, 29, 50)</cell><cell>0.01</cell><cell>0.4</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.4</cell></row><row><cell>(30, 29, 100)</cell><cell>0.01</cell><cell>0.2</cell><cell>0.01</cell><cell>0.1</cell><cell>0.01</cell><cell>0.2</cell></row><row><cell>(30, 29, 500)</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell></row><row><cell>(40, 39, 50)</cell><cell cols="2">0.01 0.001</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.4</cell></row><row><cell>(40, 39, 100)</cell><cell>0.01</cell><cell>0.2</cell><cell>0.01</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>(40, 39, 500)</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell><cell cols="2">0.01 0.001</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work was supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">JP21K11797</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9WYdAA2">
					<idno type="grant-number">JP21K11797</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pseudo-code of ancestor finding</head><p>This section presents the pseudocode of the ancestor-finding algorithm. Lines 5-6 are the procedure that avoids cyclic ancestral relationships and speeds up the ancestor finding. P earson(x i , x j ) represents the P-value of the Pearson test for the correlation between x i and x j . KCI(u, v) represents the P-value of KCI for the independence between u and v.</p><p>We can declare that all ancestral relationships have been identified if no new ancestral relationships are discovered in a single loop because of the no confounder existing assumption. This means, unlike the original RCD <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, even if some pairs of variables halt at a point where additional searching for unknown common ancestors is required in Line 35, they will ultimately be treated as having no ancestral relationship. for each pair of variables x i and x j do 5:</p><p>CA ← Anc i ∩ Anc j 10:</p><p>if pvalue l ≤ α P then 13:</p><p>if pvalue j i &gt; α CI and pvalue i j ≤ α CI then 16:</p><p>Add x i into Anc j 17:</p><p>else if pvalue j i ≤ α CI and pvalue i j &gt; α CI then if pvalue l ≤ α P then 28: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo-code of Grouping and Merging</head><p>This section presents the pseudocode for grouping variables and merging the results. In the implementation, a type II error of CI tests can result in many groups with only one variable. In that case, the output DAG will be too sparse. To avoid this, the procedure in lines 3 through 9 merges groups with one element into the other groups. When the sample size is small, no ancestral relationship may be detected, and the number of elements in all groups is one. In such a case, all p C 2 variable pairs are defined as the variable grouping.</p><p>After grouping, line 13 shows the process of learning causal sub-DAGs and merging them. Finally, lines 15-17 eliminate cycles in the merged DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Grouping and Merging</head><p>Input: The set AN C L of ancestor lists Output: The set of edges E 1: V ← ∅ 2: Let V 0 be defined as the family of maximal elements in</p><p>for each v ∈ V 0 with |v| = 1 do 7:</p><p>end for 9: end if 10: for each v in V do Eliminate cycles in G 17: end if 18: return G</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sada: A general framework to support robust causation discovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning causal structures based on divide and conquer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3232" to="3243" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directlingam: A direct method for learning a linear non-gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research-JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
		<title level="m">Causality in economics and econometrics. SSRN eLibrary</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter singlecell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning causes: Psychological explanations of causal explanation1</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and machines</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="60" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Causal Inference in Statistics: A Primer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>Hoboken, New Jersey, U.S.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning bayesian networks is np-complete</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from data: Artificial intelligence and statistics V</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in informatics and control</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="207" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2599</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-dimensional causal discovery under non-gaussianity</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rcd: Repetitive causal discovery of linear non-gaussian acyclic models with latent confounders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="735" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the effects of non-normality on the distribution of the sample product-moment correlation coefficient</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Repetitive causal discovery of linear non-gaussian acyclic models in the presence of latent confounders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3775</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04477</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
