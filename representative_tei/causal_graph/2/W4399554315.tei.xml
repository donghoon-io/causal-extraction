<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title>
				<funder ref="#_4Dc8DHQ">
					<orgName type="full">National Nuclear Security Administration</orgName>
				</funder>
				<funder ref="#_Wqepay5">
					<orgName type="full">U.S. Department of Energy, Office of Science, Office of Biological and Environment Research</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Department of Energy Office of Science</orgName>
				</funder>
				<funder ref="#_Qv8hTgG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_sjZHC9X">
					<orgName type="full">Exascale Computing Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-03">3 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ashka</forename><surname>Shah</surname></persName>
							<email>shahashka@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Committee on Computational and Applied Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
								<orgName type="institution" key="instit3">University of Chicago</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adela</forename><surname>Depavia</surname></persName>
							<email>adepavia@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Committee on Computational and Applied Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
								<orgName type="institution" key="instit3">University of Chicago</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathaniel</forename><surname>Hudson</surname></persName>
							<email>hudsonn@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Committee on Computational and Applied Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
								<orgName type="institution" key="instit3">University of Chicago</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
							<email>foster@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Committee on Computational and Applied Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
								<orgName type="institution" key="instit3">University of Chicago</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rick</forename><surname>Stevens</surname></persName>
							<email>stevens@cs.uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Committee on Computational and Applied Mathematics</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
								<orgName type="institution" key="instit3">University of Chicago</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-03">3 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.06348v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized waywithout necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of Directed Acyclic Graphs (DAG), to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees under the Maximal Ancestral Graph (MAG) class. We leverage the idea of a superstructure-a set of learned or existing candidate hypotheses-to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to 10 4 variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces. Code is available at <ref type="url" target="https://github.com/shahashka/causal_discovery_via_partitioning">https://github.com/shahashka/causal_discovery_via_partitioning</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal discovery aims to find meaningful causal relationships using large-scale observational data. Causal relationships are often represented as a graph, where nodes are random variables and directed edges are causeeffect relationships between random variables <ref type="bibr">(Spirtes et al., 2000b)</ref>. Causal graphs have high expressive power as they allow us to investigate complex relationships between many variables simultaneously-making them relevant for many problems in science, economics, and decision systems <ref type="bibr" target="#b29">(Pearl, 1995)</ref>.</p><p>Exploring the graph search space to find the causal graph is an NP-hard problem. Causal discovery algorithms have benefited from some performance enhancements and parallel strategies <ref type="bibr" target="#b31">(Ramsey, 2015;</ref><ref type="bibr" target="#b23">Laborda et al., 2023;</ref><ref type="bibr" target="#b25">Lee &amp; Kim, 2019)</ref>. Recent work explores a distributed divide-and-conquer version of causal discovery by partitioning variables into subsets, locally estimating graphs, and merging graphs to resolve a causal graph. Existing divide-and-conquer methods do not provide theoretical guarantees for consistency; meaning in the infinite data limit they do not necessarily find the Markov Equivalence Class of the true causal graph. Existing algorithms also rely on an extra learning step to merge graphs which can be computationally expensive. Finally, these algorithms ignore the violations to causal assumptions when learning on subsets of variables <ref type="bibr">(Spirtes et al., 2000b;</ref><ref type="bibr" target="#b11">Eberhardt, 2017)</ref>.</p><p>To address these limitations in literature, we propose a causal partition. A causal partition is a graph partition of the hypothesis space, defined by a superstructure, into overlapping variable sets. A causal partition allows for merging locally estimated graphs without an additional learning step. We can efficiently create a causal partition from any disjoint partition. This means that a causal partition can be an extension to any graph partitioning algorithm.</p><p>We are interested in causal discovery for high-dimensional scientific problems; in particular, biological network inference. Biological networks are organized into hierarchical scale-free sub-modules <ref type="bibr" target="#b1">(Albert, 2005;</ref><ref type="bibr" target="#b44">Wuchty et al., 2006;</ref><ref type="bibr" target="#b32">Ravasz, 2009)</ref>. The causal partition allows us to leverage the inherent, interpretable communities in these networks for scaling.</p><p>Our contributions are as follows: (A) We define a novel causal partition which leverages a superstructure and extends any disjoint partition. (B) We prove, under certain assumptions, that learning with a causal partition is consistent without an additional learning procedure. (C) We show the efficacy of our algorithm on synthetic biologically-tuned networks up to 10,000 nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Causal discovery algorithms are categorized into two types: (i) Constraint-based algorithms use conditional independence tests to determine dependence between nodes <ref type="bibr">(Spirtes et al., 2000b;</ref><ref type="bibr">a)</ref>, and (ii) Score-based algorithms greedily optimize a score function over the space of potential graphs <ref type="bibr" target="#b7">(Chickering, 2002;</ref><ref type="bibr" target="#b17">Hauser &amp; Bühlmann, 2012)</ref>. To address the intractable search space for causal discovery, many "hybrid" methods first constrain the search space with a constraint-based method, and then greedily optimizing the subspace using a score-based method <ref type="bibr" target="#b41">(Tsamardinos et al., 2006;</ref><ref type="bibr" target="#b28">Nandy et al., 2018)</ref>. <ref type="bibr" target="#b30">Perrier et al. (2008)</ref> formalize this approach by defining the superstructure G = (V, E) where for a true causal graph G * = (V, E * ), E * ⊆ E. The superstructure can be found using a constraint-based method like the PC algorithm, which is sound and complete. The superstructure can also be informed by domain knowledge e.g., for gene regulatory networks genes that are functionally related likely constrain underlying regulatory relationships <ref type="bibr" target="#b6">(Cera et al., 2019)</ref>. Incorporating prior knowledge into causal discovery allows us to infer which hypotheses or known relationships are best supported by data.</p><p>Another approach to scaling causal discovery algorithms is the divide-and-conquer approach. In this approach, random variables are partitioned into subsets. Causal discovery is run on each subset in parallel before a final merge to resolve a graph over the full variable set. <ref type="bibr" target="#b18">Huang &amp; Zhou (2022)</ref> and <ref type="bibr" target="#b15">Gu &amp; Zhou (2020)</ref> use hierarchical clustering of the data to obtain a disjoint partition of variables. Similarly, <ref type="bibr" target="#b26">Li et al. (2014)</ref> partition the node set using the Girvan-Newman community detection algorithm. Similar to our work, <ref type="bibr" target="#b47">Zeng &amp; Poh (2004)</ref> use an overlapping partition, however, they do not provide any theoretical guarantees for learning. <ref type="bibr" target="#b40">Tan et al. (2022)</ref> use an ancestral partition to restrict candidate parents for exact causal discovery using dynamic programming. <ref type="bibr" target="#b23">Laborda et al. (2023)</ref> employ ring-based distributed parallelism. Our work differs from these because we use a superstructure G to partition nodes into overlapping subsets using a novel causal graph partition with theoretical guarantees. The causal partition avoids any additional learning step to combine subsets. We show that a causal partition can be an extension to any disjoint partition, allowing us to learn effectively on graphs of varying topologies. Finally, <ref type="bibr" target="#b48">Zhang et al. (2024)</ref>  a superstructure to partition the variable set and define a causal partition with similar properties to ours. However, the variable set can only be partitioned into two subsets using an optimal edge cut, meaning the scaling potential of this algorithm is limited. Our work has no constraints on the number of subsets, and as a result we can scale up to 10,000 variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>also leverage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Discovery</head><p>Causal discovery considers a set of data sampled from the joint distribution of random variables X ≜ (X 1 , . . . , X p ) where p is the number of random variables in the system. Each random variable X i ∈ R n is defined as a real-valued column vector where each value is an individual observation for random variable X i .</p><p>We assume these relationships can be represented by a Directed Acyclic Graph (DAG). This DAG is a tuple G * = (V, E * ) where V is the node (or vertex) set made up of p nodes corresponding to the random variables, and E * ⊂ V × V is the set of directed edges between nodes. For each directed edge (X i , X j ) ∈ E * , we refer to the source node of the edge (X i ) as the "cause" and the target node of the edge (X j ) as the "effect". The joint distribution of random variables is given by a probability density function that factorizes as:</p><formula xml:id="formula_0">P (X 1 ...X p ) = p i P X i |P a G * (X i )<label>(1)</label></formula><p>Where  <ref type="bibr">Zhang, 2008a)</ref>. As a helpful reference, we include relevant definitions in Table <ref type="table" target="#tab_0">1</ref> .</p><formula xml:id="formula_1">P a G * (X i ) is the set of parents of node i in G * . Nodes that are d-separated in G * imply a conditional independence in P . Let X, Y ∈ V and Z ⊆ V / {X, Y }. If Z d-separates X from Y in DAG G * ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Classes for Latent Variables</head><p>While the causal graph can be represented by a DAG, we consider alternative graphical representations that consider latent (unobserved) variables. Namely, we consider two well-studied graph classes: (i) Maximal Ancestral Graphs (MAGs) and (ii) Partial Ancestral Graphs (PAG) <ref type="bibr" target="#b33">(Richardson &amp; Spirtes, 2003;</ref><ref type="bibr">Zhang, 2008a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.1 (mixed graph, MAG).</head><p>A mixed graph G consists of a set of nodes V and a set of directed edges E ⊂ V × V and a set of bi-directed edges B ⊂ V × V . If (X i , X j ) ∈ E we say there is a directed edge between X i and X j and we write X i → X j . If {X i , X j } ∈ B we say there is a bi-directed edge and write X i ↔ X j . A mixed graph is called a maximal ancestral graph (MAG) if it contains no almost directed cycles and there is no inducing path between non-adjacent nodes.</p><p>An almost directed cycle is a cycle that contains both directed and bi-directed edges. An inducing path is defined as follows:</p><p>Definition 3.2 (Inducing path). Given L ⊂ V , an inducing path relative to L between vertices u and v is a path Π = {u, q 1 , . . . , q k , v} such that every non-endpoint node in Π ∩ {V \ L} is a collider on Π and an ancestor of at least one of u or v.</p><p>Some examples of inducing paths are illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The idea of d-separation in DAGs can be extended to m-separation in mixed graphs <ref type="bibr">(Zhang, 2008a</ref> We will prove, that under certain assumptions, we can reconstruct the CPDAG representing the MEC (H * ) of a the true DAG (G * ) from PAGs estimated on subsets of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal Discovery on Subsets of Variables</head><p>We now describe the problem setup for learning over subsets of variables. Column-wise subsets of X are marked with a subscript: e.g., for a subset of nodes S, the corresponding subset of data is X S = {X n i } i∈S . The presence of latent variables outside the subset S complicates our learning procedure. We must use MAGs rather than DAGs to represent graphs estimated on subsets of variables to ensure consistency of our algorithm. To this end we define a latent projection, as used by <ref type="bibr">Zhang (2008a)</ref>, of the true graph G * onto a subset of nodes S. An example is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Definition 3.4 (Latent MAG). Let G be a DAG with variables V and S ⊂ V , where V contains no selection variables. <ref type="foot" target="#foot_1">2</ref> The latent MAG L MAG (G, S) is the MAG that contains all nodes in S and satisfies:</p><formula xml:id="formula_2">1. u, v ∈ S and u → v ∈ G ⇒ u → v ∈ L MAG (G, S) 2. (projected edge) ∈ L MAG (G, S) if there is an inducing path between u and v relative to V \S in G * . The edge is directed u → v if u is an ancestor to v in G * . The edge is directed v → u if v is an ancestor to u in G * . Otherwise the edge is bi-directed u ↔ v.</formula><p>Latent projections are well-studied objects in the causal discovery literature, see <ref type="bibr" target="#b42">(Verma &amp; Pearl, 2022;</ref><ref type="bibr" target="#b12">Faller et al., 2023;</ref><ref type="bibr" target="#b34">Richardson et al., 2023;</ref><ref type="bibr">Zhang, 2008a)</ref>  Next, we assume that the structure learner employed on each subset is a complete and consistent PAG learner, even in the presence of confounder variables. Algorithms known to satisfy these assumptions include the seminal FCI algorithm <ref type="bibr">(Zhang, 2008b)</ref>. Assumption 1. We have a consistent structure learning algorithm A that operates on data matrix X S for a subset of random variables S ⊆ V . When the distribution P satisfies faithfulness, then in the infinite data limit</p><formula xml:id="formula_3">A (X S ) = P AG[L MAG (G * , S)]</formula><p>In particular, by definition of the latent MAG and latent PAG operators, Assumption 1 implies the output of A satisfies several properties.</p><p>Lemma 1. Given A satisfying Assumption 1, 1. For any x i , x j ∈ S, the output A (X S ) has an edge between x i and x j if and only if there is an inducing path in G * relative to V \ S between them.</p><p>2. For any triple x i , x j , x k ∈ S that form an unshielded collider in G * as x i → x j ← x k , the output A (X S ) will have an edge between x i and x j as well as x j and x k , and both of these edges will have an arrowhead at x j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">For any</head><formula xml:id="formula_4">u, v ∈ S such that u ∼ G * v, if u ∼ A (S) v with an arrowhead at v in A (X S ), then u → v in G * .</formula><p>The proofs for Lemma 1 are deferred to Appendix B. These properties, at a high level, allow us to determine the alignment of the adjacencies and the unshielded colliders in locally estimated graphs A (X S ) to the underlying DAG G * . These will be important for resolving the CPDAG H * using locally estimated graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Defining a Causal Partition</head><p>Here, we outline the properties of our novel causal partition, which admits a divide-and-conquer algorithm to estimate H * a CPDAG corresponding to G * by learning over subsets. Since learning on the entire variable set with A (X V ) can be computationally intractable, we use an initial structure over the entire variable set to help partition V into subsets. We first assume access to an initial superstructure G.</p><p>Assumption 2. We have access to superstructure G = (V, E), an undirected graph, that constrains the true graph G * . This means all edges in G * are in G, but not all edges in G are necessarily in G * .<ref type="foot" target="#foot_2">foot_2</ref> </p><p>Now we consider some overlapping partition {S 1 , . . . , S N } of V , and the output {A (X Si )} N i=1 . Using Assumption 1, we show that given a partition with a particular structure defined below, one can recover H * from {A (X Si )} N i=1 . Definition 3.5 (Causal Partition). We say an overlapping partition {S 1 , . . . , S N } is causal with respect to superstructure G and ground-truth DAG G * if, given any learner A satisfying Assumption 1, all of the following hold:</p><p>(i) The partition is edge-covering with respect to the superstructure G.</p><p>(ii) For any vertices u, v such that u ̸ ∼ G * v and u ∼ G v, there exists some subset S i such that u, v ∈ S i and A (X Si ) does not contain an edge between u and v.</p><p>(iii) For any unshielded collider u → v ← w in G * , there exists some subset S i such that {u, v, w} ⊆ S i .</p><p>In particular, property (ii) in Definition 3.5 is crucial to the divide-and-conquer strategy proposed in this work, as it allows the algorithm to identify and discard projected edges learned on a subset S i (as in Defn 3.4) by comparing the output A (X Si ) to results on other subsets. In Section 5.1, we show that given a superstructure satisfying Assumption 2, a simple and computationally tractable procedure yields a causal partition satisfying all above properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Guarantees in the Infinite Data Limit</head><p>Now we prove that given any causal partition {S 1 , . . . , S N } with respect to DAG G * and superstructure G, one can recover H * a CPDAG corresponding to G * . Our main theorem states that Algorithm 1 recovers H * from local output {A (X Si )} N i=1 . Theorem 1. Given superstructure G satisfying Assumption 2, a learner A satisfying Assumption 1, and {S 1 , . . . , S N } a causal partition with respect to G and G * , let H * denote the output of Algorithm 1</p><formula xml:id="formula_5">H * = Screen(G, {A (X Si )} N i=1</formula><p>). Then H * satisfies the following properties: Property (i) in Theorem 1 states that H * contains the same adjacencies as G * . Properties (ii) and (iii) combine to imply that an unshielded collider u → v ← w appears oriented in H * if and only if that unshielded collider exists in G * . These combined properties ensure that H * is the CPDAG that represents the MEC of G * .</p><formula xml:id="formula_6">(i) ∀u, v ∈ V , u ∼ H * v if and only if u ∼ G * v; (ii) For any unshielded collider u → v ← w in H * , it holds that u → v ← w in G * ; and (iii) For any unshielded collider u → v ← w in G * , u ∼ H * v</formula><p>The proof of Theorem 1, included in Appendix B, relies on the fact that by definition of a causal partition, for any u, v not adjacent in G * , there must be a subset S i such that u, v ∈ S i and the local output A (S i ) does not contain an edge between u and v. This allows us to "screen" projected edges from true edges as edges that are not consistent across all locally estimated graphs.</p><p>We note that Screen is computationally lightweight. The dominant cost is</p><formula xml:id="formula_7">O(N • m ′ • d),</formula><p>for N the number of partitions, m ′ the total number of learned edges, and d the maximum degree in the learned graph. Of note, m ′ ≤ p 2 for p the number of random variables, and in real-world applications learned graphs tend to be sparse so typical instances have m ′ ≪ p 2 <ref type="bibr" target="#b2">(Barabási, 2013)</ref>.</p><p>We highlight that because we only assume access to observational data, we can only recover cause-effect relationships contained in the Markov equivalence class of G * , and therefore our guarantees relate to learning H * a CPDAG representing the MEC of G * . In Section 8, we discuss potential extensions of our framework for settings where both interventional and observational data are available, which might allow one to further reduce the size of the learned equivalence class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Practical Algorithm for Causal Discovery with a Causal Partition</head><formula xml:id="formula_8">1: Screen(G, {H i } N i=1 ) Input: a superstructure G, a set of PAGS {H i = (S i , E i )} N i=1 Result: H * = (V, E * ) a CPDAG Initialize V = ∪ N i=1 S i ; E candidates ← ∪ N i=1 E i ; E * ← ∅; // Discard edges not in superstructure. E candidates ← E candidates ∩ {u * - * v | u ∼ G v}; foreach u, v such that {u * - * v} ∈ E candidates do if ∀i s.t. S i ⊇ {u, v}, u ∼ A(S i ) v then</formula><p>// If an edge between u and v appears in the output on all subsets, add undirected edge to output graph.</p><formula xml:id="formula_9">E * ← E * ∪ {u -v}; // Orient unshielded colliders foreach i ∈ [N ] do foreach Unshielded u * → v ← * w in H i do if u -v and v -w in E * then 9 discard ← {u -v, v -w}; 10 orient ← {u → v, v ← w}; 11 E * ← {E * \ discard} ∪ orient; return H * = (V, E * )</formula><p>Here, we describe a practical procedure for causal discovery motivated by the idealized results studied in Section 4. We discuss how partitions satisfying Defn. 3.5 can be efficiently constructed, and detail a full end-to-end algorithm for causal discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Efficient Creation of a Causal Partition</head><p>The causal partition structure, described in Defn. 3.5, is crucial to the guarantees of Theorem 1 in the infinite data limit. While the first property of a causal partitionedge coverage with respect to superstructure G-is easy to ensure, it is not obvious how to satisfy properties (ii) and (iii) without knowledge of the ground truth G * . Here we present a simple and intuitive method for constructing causal partitions. This construction is efficient and adapts to arbitrary superstructure topologies.</p><p>Given a graph G = (V, E) and S ⊆ V , let ∂ out (S) denote the outer vertex boundary of set S in G:</p><formula xml:id="formula_10">∂ out (S) ≡ {v ∈ V (G) \ S : ∃u ∈ S such that v ∼ G u} where v ∼ G u if any of (u, v), (v, u) or {u, v} ∈ E.</formula><p>Given any initial vertex-covering partition of the superstructure G, we consider the overlapping partition formed by expanding subsets via the addition of vertices from the outer boundary. Definition 5.1. Let {S 1 , . . . , S N } be a vertex-covering partition of graph G. The causal expansion of {S 1 , . . . , S N } with respect to G is defined as {S ′ 1 , . . . , S ′ N } with subsets</p><formula xml:id="formula_11">S ′ i = S i ∪ ∂ out (S i ).</formula><p>As the name suggests, we show that a causal expansion satisfies the properties of a causal partition. The proof is deferred to Appendix B.</p><p>Lemma 2. Given G a superstructure satisfying Assumption 2, {S 1 , . . . , S N } a vertex-covering partition of G. Then the causal expansion {S ′ 1 , . . . , S ′ N } is a causal partition with respect to G and G * .</p><p>This simple construction, illustrated in Fig. <ref type="figure" target="#fig_4">2</ref>, offers several advantages. Firstly, this method can be run on any vertex-covering initial partition {S 1 , . . . , S N }. Graph partitioning algorithms form an extensive field <ref type="bibr" target="#b14">(Girvan &amp; Newman, 2002;</ref><ref type="bibr" target="#b8">Clauset et al., 2004;</ref><ref type="bibr" target="#b35">Schaeffer, 2007;</ref><ref type="bibr" target="#b27">Malliaros &amp; Vazirgiannis, 2013;</ref><ref type="bibr" target="#b16">Harenberg et al., 2014)</ref>, and depending on the topology of G different partitioning may be more appropriate to a specific superstructure. The causal expansion allows a user to first partition the superstructure G using whatever method is most appropriate to the application, and then easily derive a corresponding causal partition.</p><p>The causal expansion is computationally efficient, both to construct and in its incorporation into the full causal discovery procedure, described in Algorithm 2. Given an initial partition {S 1 , . . . , S N }, constructing its causal expansion takes time linear in the size of the superstructure G. In Appendix E, we discuss how connectivity properties of the initial partition {S 1 , . . . , S N } dictate the size of the largest subset.  Algorithm 2: causal_discovery(V, X, G)</p><formula xml:id="formula_12">Input: a set of variables V , a matrix of observations X, superstructure G Result: Gout = (V, E) a CPDAG 1 {D 1 , . . . , D N } ← disjoint_partition(G); /* construct causal expansion */ 2 S i ← D i ∪ ∂out(D i )(∀1 ≤ i ≤ N ); 3 {G S i = A (X S i )} N i=1 ; 4 return Gout ← Screen(G, {G S i });</formula><p>Now, we describe our divide-and-conquer causal discovery algorithm with an expansive causal partition as described in Section 5.1. Algorithm 2 requires a set of variables V , a data matrix X and a superstructure G. In Section 6.3 we also study the case where G is derived from data using the PC algorithm. Any causal learner can be plugged into A , but for consistent learning we require that the assumptions for A allow for causal insufficiency (confounders may be present) and causal faithfulness. Any graph partitioning algorithm can be plugged into disjoint_partition. A complexity analysis of divide-and-conquer with an expansive causal partition is shown in Appendix D. In the next sections we show the use of this practical algorithm on random and biologically-tuned networks with synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Results on Random Networks</head><p>In this section we describe experiments for evaluating Algorithm 2 on synthetic random networks with finite data. For causal discovery on subsets (i.e., A ) we evaluate with four different algorithms: (1) Peters-Clark (PC) <ref type="bibr">(Spirtes et al., 2000b)</ref>, (2) Greedy Equivalence Search (GES) <ref type="bibr" target="#b17">(Hauser &amp; Bühlmann, 2012)</ref>, (3) Really Fast Causal Inference (RFCI) <ref type="bibr" target="#b9">(Colombo et al., 2012)</ref>, and (4) DAGMA <ref type="bibr" target="#b5">(Bello et al., 2022)</ref>. Note that only RFCI is a PAG learner that satisfies Assumption 1. The other algorithms are DAG learners that assume causal sufficiency; still we include them in this evaluation because (a) they are popular causal discovery benchmarks, and (b) even with the violation to causal sufficiency, we observe good performance with the causal partition. For details on how the DAG subgraphs are merged see Appendix F. For disjoint_partition in Algorithm 2 we use greedy modularity based community detection <ref type="bibr" target="#b8">(Clauset et al., 2004)</ref>. We benchmark our algorithm with another divide-and-conquer method PEF <ref type="bibr" target="#b15">(Gu &amp; Zhou, 2020)</ref>.</p><p>For evaluation, we use the following metrics: (1) True Positive Rate (TPR) of correct edges in the estimated graph, Ĝ, compared to the edges in G * ; and (2) Structural Hamming Distance (SHD), which is the number of incorrect edges. An incorrect edge is any edge in G * that is missing in Ĝ or any edge in Ĝ that is not in G * . Additionally for larger networks we include (3) False Positive Rate (FPR); and (4) Time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Default parameters:</head><p>We use the following parameters by default unless stated otherwise. The graph topology is constructed by generating two scale-free networks using the Barabási-Albert generative model <ref type="bibr" target="#b3">(Barabási &amp; Bonabeau, 2003)</ref>; both graphs have p = 50 nodes each, with one graph being constructed with a m = 1 edge per node and the second graph being constructed with m = 2 edges per node (edge connections are established via preferential attachment as per the generative model). We use n = 100, 000 samples from the joint, multivariate Gaussian distribution (details on the DAG and data generating process are in Appendix F.1). The fraction of additional edges in a perfect superstructure G is 10% of the edges in G * (all edges in G are undirected). For causal discovery on subsets we set A to PC, GES, RFCI or DAGMA. Finally, for disjoint_partition in Algorithm 2 we use greedy modularity <ref type="bibr" target="#b8">(Clauset et al., 2004)</ref> from the networkx Python library. The parameter settings for A and each partitioning algorithm are detailed in Appendix F.2 and F.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Number of samples</head><p>In this experiment, we test the consistency of Algorithm 2 with increasing samples n. We use a perfect superstructure and add a fraction 10% extra extraneous edges to G that are not in G * . Results are shown in Fig. <ref type="figure" target="#fig_5">3</ref>. As the sample size increases, we see the convergence of No Partition with the MEC of G * . This empirically supports our theoretical result that Algorithm 2 is consistent in the infinite data limit. Interestingly, even when the A does not permit latent variables (as in PC, GES, DAGMA), we still see convergence of No Partition with Expansive Causal. We also show results for an Edge Cover partition; this partition only accounts for edge coverage of G ((i) in Defn 3.5). We see the Edge Cover partition performs comparably to the Expansive Causal partition. This implies that of the properties of a causal partition described in Defn. 3.5, edge coverage appears to be the most important. With the exception of the PC algorithm, we also outperform the benchmark PEF .   This experiment assumes a perfect superstructure G. We increase the fraction of extraneous edges in G and not in G * . In Fig. <ref type="figure" target="#fig_6">4</ref>, we see comparable learning of Edge Cover, Expansive Causal, and No Partition. This means that although G * is increasingly obscured by G, and even though partitioning is done on G, we can still estimate close to the MEC H * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Imperfect superstructure G</head><p>In this experiment we use the PC algorithm to estimate the superstructure G. Since the superstructure now relies on the data, it is imperfect and does not include all edges in G * . We vary the "perfection" of the superstructure by increasing the the significance level α of the PC algorithm. A larger α means a denser superstructure and a structure that is more likely to include more edges in G * . Results are shown in Fig. <ref type="figure" target="#fig_7">5</ref>. We turn off the superstructure screening step, as in Screen, for this experiment. For the GES and DAGMA causal learning algorithms, Expansive Causal outperforms Edge Cover slightly -unlike in previous experiments. Still, the edge coverage property of the causal expansion accounts for most of the improvement in accuracy compared to a disjoint partition. The causal partition may provide additional benefits to learning when the superstructure G is imperfect. Example communities extracted from the 10,000 node network. These three communities account for 3% of the total nodes and 2.81% of the total edges. The size of the node is proportional to its degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Number of Nodes</head><p>In this experiment we highlight the scalability of our algorithm by evaluating on networks with 10,000 nodes. We test two network structure cases here: (A) one hundred communities each with 100 nodes with a Barabasi-Albert scale-free topology; this is identical to the preceding experiments, except with more communities, and (B) hierarchical scale-free graphs which are characterized by highly connected hub nodes that are preferentially attached to other hubs. This is similar to gene regulatory networks <ref type="bibr" target="#b45">(Yu &amp; Gerstein, 2006)</ref>, but these structures are more sparse than typical biological networks. For both networks we obtain 10,000 samples from the multivariate Gaussian distribution. Unlike the preceding experiments, in this experiment we show results for when the sample size is equal to the number of variables: n = p.</p><p>In Table <ref type="table" target="#tab_6">2</ref>   and TPR while incurring a small cost in compute time compared to the fastest algorithm Disjoint. This further supports the claim that Edge Cover may be sufficient for many problems. For GES, however, we see the increased benefit of the Expansive Causal partition compared to Edge Cover for the TPR. Since No Partition runs in a few minutes on this network, it may be unclear why partitioning is needed. In the next example we will see how the compute times increase dramatically when the network has a much more complex community structure.  maintaining the second highest accuracy score. Compared to No Partition, Expansive Causal provides 2.13x speedup and Edge Cover provides 13.8x speedup. Note that PEF did not converge in 72 hours.<ref type="foot" target="#foot_4">foot_4</ref> </p><p>In Expansive Causal 100 Comms and Edge Cover 100 Comms we set the number of subsets to one hundred and ensure the size of the largest subset is smaller for each partitioning algorithm (Fig. <ref type="figure" target="#fig_9">7a</ref>). We see significant speedup (12.9x for Expansive Causal 100 Comms and 606x for Edge Cover 100 Comms ) compared to No Partition. However, this does come at a cost to accuracy as seen in Table <ref type="table" target="#tab_7">3</ref>. We present an initial study of the subset size, speedup, and accuracy trade off in Appendix G. No Partition achieves the best accuracy, particularly with respect to SHD and FPR, however now the compute time is close to 25 hours, motivating the need for partitioning. Finally, we note that given n = p for these experiments it is probable that the limitations we see for Expansive Causal are due to the statistical problems that arise in this data setting.</p><p>We leave understanding the sample inefficiency of partitioning algorithms to future work.</p><p>We conclude that our methods Expansive Causal and Edge Cover provide a faster time to solution on large graphs, are relatively robust to dense and imperfect superstructures, and provide comparable accuracy compared to No Partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Empirical Results on Synthetically Tuned E.coli Networks</head><p>This section contains results for biological networks. We use the topologies of E. coli biological networks due to their availability and popularity. To better benchmark the algorithms, we leverage a proximitybased topology generative model from the literature proposed by <ref type="bibr" target="#b19">Hufbauer et al. (2020)</ref>. The model was designed with the goal of generating structures with the following properties: (i) small-world (ii) exponential degree distribution (i.e., scale-free), and (iii) presence of inherit community structures. Coincidentally, these properties are also relevant for real-world biological networks <ref type="bibr" target="#b4">(Barabasi &amp; Oltvai, 2004;</ref><ref type="bibr" target="#b22">Koutrouli et al., 2020)</ref>, thus we take advantage of this generative method. We seed this tuning algorithm with the known E. coli regulatory network reconstructed from experimental data in <ref type="bibr" target="#b13">Fang et al. (2017)</ref> to generate synthetic networks with E. coli-like topology. See Fig. <ref type="figure" target="#fig_10">8b</ref> for a visualization of the highly connected hub nodes of an example tuned network. We impose a random causal ordering on the topology and generate data from the DAG using the multivariate Gaussian distribution described in Appendix F.1.</p><p>A comparison of all algorithms is shown in Table <ref type="table" target="#tab_8">4</ref> for A = GES. Expansive Causal provides 1.7x speedup compared to No Partition. While there is a significant speedup, we note the decrease in accuracy for all divide-and-conquer algorithms. Still compared to other methods based on partitioning shown here, using a causal partition accelerates causal discovery and provides the best trade off in accuracy. We expect that scaling up to larger gene set sizes (e.g., 10 4 genes for eukaryotic cells) will be severely expensive for methods without partitioning since these networks are more dense and complex than those evaluated in Section 6.4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions &amp; Future Directions</head><p>We propose a divide-and-conquer causal discovery algorithm based on a novel causal partition. Our algorithm leverages a superstructure-i.e., a known or inferred structured hypothesis space. We prove the consistency of our algorithm under assumptions for the causal learner and in the infinite data limit using the Maximal Ancestral Graph (MAG) class. Unlike existing works, our algorithm allows for the merging of locally estimated graphs without an additional learning step. Motivated by a complex scientific application space, we also show an example for gene regulatory network inference for a small organism (E.coli). This example shows the applicability of our work to real-world networks, but we leave evaluation of our method on larger organisms (e.g, eukaryotes) to future work.</p><p>One limitation of our work is the reliance on a perfect superstructure to create a causal partition. Although in Section 6.3 we empirically explore the impact that using imperfect superstructures generated by the PC algorithm has on the learned output, more experiments (including other methods of generating superstructures) are needed to fully characterize the impact of learning when the superstructure does not constrain the edge set of the true causal graph. Further, while the divide-and-conquer method developed in this work can substantially reduce the runtime of performing causal discovery on large variable sets, characterizing the trade-off between time-savings and sample complexity remains an area of open work. An important open question for future research is how the partitioning described in this paper impacts the sample efficiency of causal discovery algorithms. As discussed in Section 4, another interesting direction for future research is extending the divide-and-conquer framework presented in this paper to the setting where both interventional and observational data are available. There exist causal discovery algorithms which can leverage a mixture of observational and interventional data (such as the I-FCI and Ψ-FCI algorithms) <ref type="bibr" target="#b21">(Kocaoglu et al., 2019;</ref><ref type="bibr" target="#b20">Jaber et al., 2020)</ref>. If one uses one of these algorithms to perform local learning on the variable subsets, then this might allow one to strengthen the guarantees presented in this work beyond only recovering cause-effect pairs in the MEC of G * . Given these limitations and open questions, we believe that this work provides a meaningful contribution to causal discovery at scale and to knowledge discovery for domains with high-dimensional structured hypothesis spaces. Examples of Non-Inducing Paths q 1 u q 2 v q 1 u q 4 v q 2 q 3 Nodes in Figure <ref type="figure">9</ref>: Examples of non-inducing paths. The example in (a) illustrates the case described in Lemma 9. This path is not inducing because q1, q2 are non-endpoint paths in S, but they are not both colliders on the path. The example in (b) illustrates Case 2 in the proof of Lemma 10. The definition of an inducing path requires that q1 be an ancestor of u and q4 be an ancestor of u, but this implies the existence of a cycle in G * contains u, q1, v, and q4. Thus this path cannot exist.</p><p>We consider four cases, parameterized by the distance from the endpoints u, v to ∂ out S ′ i . Note that these four cases cover all possible positionings of u and v within S ′ i . Thus to prove Lemma 10, we must show that each case implies the existence of some S ′ ∈ {S ′ 1 , . . . , S ′ N }, not necessarily equal to</p><formula xml:id="formula_13">S ′ i i, such that u ̸ ∼ A (S ′ ) v. Case 1. max{dist G * (u, ∂ out S ′ i ), dist G * (v, ∂ out S ′ i )} &gt; 2 Case 2. dist G * (u, ∂ out S ′ i ) = dist G * (v, ∂ out S ′ i ) = 2. Case 3. dist G * (u, ∂ out S ′ i ) = 2, and dist G * (v, ∂ out S ′ i ) = 1. Case 4. dist G * (u, ∂ out S ′ i ) = dist G * (v, ∂ out S ′ i ) = 1.</formula><p>We now show that in each case, there exists some</p><formula xml:id="formula_14">S ′ ∈ {S ′ 1 , . . . , S ′ N } such that u ̸ ∼ A (S ′ ) v. Case 1. max{dist G * (u, ∂ out S ′ i ), dist G * (v, ∂ out S ′ i )</formula><p>} &gt; 2 implies that for any path Π between u, v, either Π ⊆ S ′ i , or that Π contains a prefix {u, q 1 , q 2 } ⊆ S ′ i , or that Π contains a suffix {q k-1 , q k , v} ⊆ S ′ i . In all of these cases, Lemma 9 implies that Π is not an inducing path between u and v in G * relative to</p><formula xml:id="formula_15">V \ S ′ i . Thus u ̸ ∼ A (S ′ i ) v. Case 2. dist G * (u, ∂ out S) = dist G * (v, ∂ out S) = 2 implies that for any path Π between u, v, either Π ⊆ S ′ i or that Π contains a prefix {u, q 1 } ⊆ S ′ i and suffix {q k , v} ⊆ S ′ i . If Π ⊆ S ′</formula><p>i , then it is not an inducing path. Consider the case when Π contains a prefix {u, q 1 } ⊆ S ′ i and suffix {q k , v} ⊆ S ′ i and assume for the sake of contradiction that Π is an inducing path between u and v in G * relative to V \ S ′ i . Both q 1 and q k are non-endpoint vertices on Π ∩ S ′ i . They must therefore be colliders on Π as well as ancestors of at least one of u or v. Since q 1 be a collider on Π, it must be that u → q 1 so u ∈ anc G * (q 1 ), where anc G * (x) ≡ {z ∈ V : z is an ancestor of x in G * }.</p><p>Moreover, q 1 must be an ancestor of either u or v, and because u ∈ anc G * (q 1 ) it cannot be that q 1 is an ancestor of u as this would imply the existence of a cycle in G * . Thus it must be that q 1 ∈ anc G * (v). However, we similarly conclude that as q k be a collider on Π, it must be that q k ← vso v ∈ anc G * (q k ). Moreover q k must be an ancestor of either u or v, and q k cannot be an ancestor of v as G * is acyclic, so q k ∈ anc G * (u).</p><p>However we have thus concluded that u ∈ anc G * (q 1 ), q 1 ∈ anc G * (v), v ∈ anc G * (q k ), and q k ∈ anc G * (u). This implies the existence of a cycle in G * , and thus cannot occur. Thus we conclude that no such path Π can be an inducing path between u and v in G * relative to data exist; many, including the PC algorithm, are more easily parallelized than greedy score-based learners and thus can be run on the global variable set in reasonable time <ref type="bibr" target="#b46">(Zarebavani et al., 2019;</ref><ref type="bibr" target="#b24">Le et al., 2016)</ref> . However, when the superstructure G is learned from data, it may be imperfect, i.e. there may exist edges in G * which are not in G. If the superstructure is missing a large fraction of the ground-truth edges, the step in Screen, which discards edges not in the superstructure may significantly reduce the rate of true positive edges returned by the algorithm, with the effect growing more severe with more imperfect superstructures. Thus in the finite sample limit, if working with a superstructure which is suspected to be highly imperfect, one option is to simply omit the step in Screen, which discards edges not in the superstructure. In Section 6.3, we examine the impact of learning imperfect superstructures from data, and show while imperfect superstructures do impact learning significantly, the expansive causal partition is most effective out of all partition schemes.</p><formula xml:id="formula_16">V \ S ′ i . Thus u ̸ ∼ A (S ′ i ) v. Algorithm 3: Screen_Finite_Data(G, {H i } N i=1 , X) Input: a superstructure G, a set of PAGS {Hi = (Si, Ei)} N i=1 , a matrix of observations X. Result: H * = (V, E * ) a PAG Initialize V = ∪ N i=1 Si; E candidates ← ∪ N i=1 Ei; E * ← ∅ foreach u, v</formula><p>Potential cycles: When the result of learning over a subset is not a latent projection, the algorithm presented in Section 4 may fail to return a DAG. In particular, even if the output A (X Si ) is a DAG on every subset S i , the output of Screen may contain cycles. However, it is possible to localize these cycles; if the output A (X Si ) is a DAG on every subset S i , then any cycle in the output of Screen(G, {A (X Si )} N i=1 ) will have some edge (u, v) such that one of the two endpoints lies in the overlap of partition {S 1 , . . . , S N }, i.e. ∃i ̸ = j such that {u, v} ∩ {S i ∩ S j } ̸ = ∅.</p><p>Using this observation about the location of all cycles in the output of Screen, adopt the following procedure.</p><p>If the output of Screen contains a cycle, we find all edges in that cycle which intersect with the overlap of partition {S 1 , . . . , S N }. We then rank these edges using a scoring function and discard the lowest-ranked edge. While a variety of edge scoring functions may be deployed for this step, in this work we assess edges using the log-likelihood induced by the linear structural equation</p><formula xml:id="formula_17">X j = p i=1 W (G) ij X i + ε j (2)</formula><p>where W (G) ij denotes the weighted adjacency matrix of a DAG G and ε j ∼ N (0, σ 2 j ) denotes additive Gaussian noise. Then joint distribution of (X 1 . . . X p ) is a multivariate Gaussian distribution N (0, Σ) where Σ = W W T . The log-likelihood under this model is </p><formula xml:id="formula_18">l(W, Σ) = p j=1 - n 2 log(σ j ) 2 - 1 2σ 2 j ||X j -XW j || 2<label>(</label></formula><formula xml:id="formula_19">(u, v) ∈ C do if u ∈ V or v ∈ V then Ê ← Ê ∪ {(u, v)} ; ê ← arg min (u,v)∈ Ê loglikelihood_score(u, v, G, X); G.removeEdge(ê); return G;</formula><p>Algorithm 5: loglikelihood_score(i, j, G, X) Input: a node i, a node j, a graph G, a matrix of observations X Result: a score based on the likelihood of graph given the data in the presence and absence of edge (i, j) // least squares estimates of Eq. 2</p><formula xml:id="formula_20">Ŵ (G i,j ) ← LSE(X j , G i,j ) Ŵ (G 0,j ) ← LSE(X j , G 0,j ) Σ ← cov(X) // covariance matrix of X // log-likelihoods from Eq. 3 l ij ← l( Ŵ (G i,j ) , Σ) l 0 ← l( Ŵ (G 0,j ) , Σ) score ← l ij -l 0 return score</formula><p>In order to score an edge (i, j), we compare the log-likelihood at the least squares estimates (LSE) of the regression coefficients ( Ŵij ) in Eq. 2 of two different DAGs: G i,j which contains edge (i, j), and G 0,j in which we remove edge (i, j) so that i is no longer a parent of j. Edge (i, j) is then scored by how much including i as a parent of j increases the log-likelihood of X j under the linear structural equation. The likelihood based score is outlined in Algorithm 5. The full procedure for cycle resolution is outlined in Algorithm 4.</p><p>In the case when the detected cycle has length two, i.e. there exist edges (i, j) and (j, i), we adopt the methodology of <ref type="bibr" target="#b15">Gu &amp; Zhou (2020)</ref> and use the risk inflation criterion (RIC) to determine whether to discard one or both of the edges forming the cycle. In this setting we compare three models: G i,j in which i is a parent of j, G j,i in which j is a parent of i, and G 0 in which neither edge appears. We then compute the RIC score for each model, which balances the log-likelihood with a sparsity-promoting term penalizing the total edges in the graph. If the model G 0 out-performs both G i,j and G j,i , then both edges are removed from the graph. If at least one of the models G i,j , G j,i out-performs G 0 , then the better-performing edge is retained and the other edge is discarded. For further details on using the RIC score to assess edges, we direct readers to <ref type="bibr" target="#b15">Gu &amp; Zhou (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Computational Complexity of the Divide-and-Conquer Method</head><p>The motivation for the divide-and-conquer method developed in this work is that existing causal discovery algorithms' computational complexity grows prohibitively large as the number of variables in the dataset increases. The computational cost of the divide-and-conquer method includes the expense of producing the initial partition {D i } N i=1 , constructing the causal expansion {D i ∪ ∂ out (D i )} N i=1 , running some causal discovery algorithm on each of the subsets D i ∪ ∂ out (D i ), and merging the results using Algorithm 1.</p><p>For most reasonable choices of partitioning algorithms, the dominant cost in the divide-and-conquer procedure will be running causal discovery on each subset in the expansive causal partition. A key aspect of the divide-and-conquer method's speedup is that the problem of running causal discovery on each subset of the expansive causal partition is embarassingly parallelizable. For any causal discovery algorithm, let F(•) describe the worst-case runtime as a function of the number of random variables in the input, and let p denote the number of random variables in the global dataset. Running causal discovery on N subsets in parallel on a machine with N processors reduces the dominant cost from F(p) to max i∈[N ] F(|D i | + |∂ out (D i )|). In settings where P &lt; N processors are used, the wall-clock time for performing the parallelized causal discovery on subsets using P processors is given by Brent's law <ref type="bibr" target="#b36">(Smith, 1993)</ref>  As an example, we describe the computational complexity of the full divide-and-conquer procedure in a simplified setting. Consider a variable set V of cardinality p, and assume the superstructure G reflects strong community structure. Specifically, consider the stochastic block model setting: each variable in V belongs to one of two equally-sized hidden communities and that for any two variables u, v ∈ V , G contains an edge between u and v with probability q in if u and v belong to the same community, and probability q out if they belong to different communities. We consider a regime where these communities can be efficiently recovered with high probability: assume q in = α/p, q out = β/p for α, β &gt; 0 and We consider running causal discovery using the FCI algorithm, which satisfies Assumption 1. While the FCI algorithm does not perform every pairwise conditional independence test, in the worst case its runtime still scales exponentially with the size of the input variable set, e.g. F(p) = c p for c ∈ (1, 2) <ref type="bibr" target="#b38">(Spirtes, 2001)</ref>. When run in parallel on two processors, the wall-clock time will be exponential in the size of the subsets of the expansive causal partition: conditioned on exact recovery, for any fixed δ ∈ (0, 1 -β/4), |D i ∪ ∂ out (D i )| ≤ (1/2 + β/4 + δ)p with high probability. This corresponds to reducing runtime from c p to c (1/2+β/4+δ)p . The complexity of merging the results of the causal discovery output using Algorithm 1 is a function of the number of learned edges and maximum learned degree. In the SBM setting, with high probability its runtime is O(p(p + q)p 3 ) = O(α(α + β)p 2 ).</p><p>In total, the time to run the divide-and-conquer procedure when parallelizing the causal discovery step is O p log 2 (p) + c (1/2+β/4+δ)p + α(α + β)p 2 with high probability. In the large variable regime, the exponential term c (1/2+β/4+δ)p dominates the runtime of the divide-and-conquer procedure. We compare this with the runtime of the FCI algorithm on the global variable set, which is O(c p ). For small values of β, the divide-and-conquer method reduces runtime compared with the runtime of FCI on the global variable set from c p to ≈ √ c p , which represents considerable computational savings in the regimes of interest when p is large. This small-β regime corresponds to the setting where few intra-cluster edges are present. the divide-and-conquer methodology, but rather because of the use of the GES learner in this setting. Still, since we can control the size of the subsets with the disjoint partition we can still achieve accuracy and time benefits with GES as shown in our empirical results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of latent MAGS L MAG (G * , S). Inducing paths Π relative to V \ S are highlighted in green. (a) For x1, x2 ∈ S, any edge (x1, x2) in G * is an inducing path relative to V \ S between x1 and x2. (b) Π is an inducing path relative to V \ S between x1 and x5 because all non-endpoint nodes on the path are in V \ S. (c) Π is an inducing path relative to V \ S between x1 and x5 because every non-endpoint is either in V \ S (nodes x2, x4), or is in S and is a collider on the path and is an ancestor of at least one of x1 or x5 (node x3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and v ∼ H * w and both edges have an arrowhead at v in H * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Expansive causal partition {S ′ 1 , S ′ 2 } made from initial disjoint partition {S1, S2}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experiment increasing the number of samples n. Error bars are 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Experiment increasing fraction of extraneous edges in a perfect superstructure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Increase in density of the imperfect superstructure by increasing the significant level α of the PC algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Topological structure of 10,000 node graphs defined by case (A). This network has 10,000 nodes. The nodes are divided into 100-node subsets, of which there are 100. To generate the edges in the network, we first generate a Barabasi-Albert scale-free graph on each 100-node subset, and then randomly add edges connecting these subsets together. (a) Distribution of subset sizes for each partitioning algorithm. (b) Example communities extracted from the 10,000 node network. These three communities account for 3% of the total nodes and 2.81% of the total edges. The size of the node is proportional to its degree.</figDesc><graphic coords="10,75.32,244.47,224.64,168.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Topological structure of 10,000 node graphs defined by case (B): a 10,000 node hierarchical scale-free network . (a) Distribution of subset sizes for each partitioning algorithm. (b) Example communities extracted from the 10,000 node network with the Disjoint partition. These three communities account for 25% of the total nodes and 20.5% of the total edges. Each community has a set of hub nodes that connected to a large number of other nodes (as seen by the large clusters of nodes in the visualization).</figDesc><graphic coords="12,75.32,70.51,224.64,168.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Topological structure of a synthetically-tuned E. coli network. (a) Distribution of subset sizes for each partitioning algorithm. (b) Example communities extracted from the 2,332 node network with the Disjoint partition. These three communities account for 40.7% of the total nodes and 40% of the total edges.</figDesc><graphic coords="13,75.32,165.32,224.64,168.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>i | + |∂ out (D i )|i | + |∂ out (D i )|) .For all causal discovery algorithms satisfying Assumption 1 known to the authors at the time of publication, the computational complexity F(•) is dramatically super-linear such thatN i=1 F(|D i | + |∂ out (D i )|) ≪ F(p),as shown below for the FCI algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>. This regime is well-studied, and in this setting known clustering algorithms exactly recovery the underlying partition with high probability in nearly-linear time O(p log 2 (p)) (see e.g. Abbe &amp; Sandon (2015); Wang et al. (2020)). Employing these clustering algorithms, with high probability we recover an initial partition D 1 , D 2 such that |D 1 | = |D 2 | = p/2. Moreover, in expectation |∂ out (D 1 )| = |∂ out (D 2 )| = pβ/4, so conditioned on exact recovery the size of the clusters in the causal expansion are (1/2 + β/4)p in expectation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table of Relevant Notation The outer vertex boundary of a set of nodes S. Xi ∼ G ′ Xj Nodes Xi and Xj are adjacent in some graph G ′ .</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>G  *</cell><cell>Underlying true causal graph represented by a DAG.</cell></row><row><cell>H  *</cell><cell>CPDAG representing MEC of G  *  .</cell></row><row><cell>G</cell><cell>Superstructure.</cell></row><row><cell>X</cell><cell>The complete observed data matrix (of dimensionality n × p).</cell></row><row><cell>Xi ∈ X</cell><cell>Observational data for the i th random variable; also used to denote</cell></row><row><cell></cell><cell>nodes in graphical models.</cell></row><row><cell>(Xi, Xj)</cell><cell>Directed edge from random variables (nodes) Xi to Xj.</cell></row><row><cell>A</cell><cell>Consistent causal learner that outputs an PAG on subsets S.</cell></row><row><cell>{S1, . . . , SN }</cell><cell>Partition over node set V , where S ⊂ V .</cell></row><row><cell>∂out(S)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Average time and accuracy results on 10,000 node graphs with ∼ 10,000 edges comprised of 100 scale free networks each of size 100 (averaged over over 5 networks). The number of samples n is 10,000. Dash (-) indicates the algorithm did not complete in 24 hours. This experiment was run with 2x AMD EPYC 7713 CPU @2GHz with a total of 128 cores and 256 GB of RAM.</figDesc><table><row><cell cols="2">Structure Learner Partitioning Algorithm</cell><cell>SHD ↓</cell><cell cols="2">TPR ↑ FPR ↓</cell><cell>Time (min) ↓</cell></row><row><cell></cell><cell>No Partition</cell><cell>857.2 ± 35.6</cell><cell cols="2">0.930 ± 0.002 9.0e-6</cell><cell>11.695 ± 0.230</cell></row><row><cell></cell><cell>Disjoint</cell><cell>4382.8 ± 201.8</cell><cell>0.650 ± 0.011</cell><cell>1.0e-5</cell><cell>0.119 ± 0.005</cell></row><row><cell>A = GES</cell><cell>Edge Cover</cell><cell>1306.4 ± 35.4</cell><cell>0.895 ± 0.002</cell><cell>1.3e-5</cell><cell>0.148 ± 0.007</cell></row><row><cell></cell><cell>Expansive Causal</cell><cell cols="2">1550.6 ± 54.3 0.947 ± 0.003</cell><cell>1.5e-5</cell><cell>0.163 ± 0.009</cell></row><row><cell></cell><cell>PEF</cell><cell>3775.8 ± 119.3</cell><cell>0.697 ± 0.01</cell><cell>3.7e-5</cell><cell>103.767 ± 3.067</cell></row><row><cell></cell><cell>No Partition</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Disjoint</cell><cell>8519.2 ± 227.0</cell><cell cols="2">0.322 ± 0.007 4.9e-5</cell><cell>4.891 ± 8.716</cell></row><row><cell>A = PC</cell><cell cols="2">Edge Cover 7050.8 ± 195.0</cell><cell>0.443 ± 0.004</cell><cell>6.3e-5</cell><cell>10.343 ± 11.363</cell></row><row><cell></cell><cell>Expansive Causal</cell><cell cols="2">8683.0 ± 401.0 0.504 ± 0.014</cell><cell cols="2">7.4e-5 450.744 ± 187.366</cell></row><row><cell></cell><cell>PEF</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>No Partition</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Disjoint</cell><cell>10294.0 ± 191.5</cell><cell>0.491 ± 0.013</cell><cell>6.5e-5</cell><cell>0.715 ± 0.311</cell></row><row><cell>A = RFCI</cell><cell cols="3">Edge Cover 9683.4 ± 288.4 0.691 ± 0.005</cell><cell>8.9e-5</cell><cell>5.430 ± 4.210</cell></row><row><cell></cell><cell>Expansive Causal</cell><cell>9924.5 ± 113.8</cell><cell>0.647 ± 0.003</cell><cell>9.0e-5</cell><cell>107.788 ± 11.204</cell></row><row><cell></cell><cell>PEF</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>No Partition</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Disjoint</cell><cell>3722.6 ± 274.7</cell><cell cols="2">0.694 ± 0.015 1.0e-6</cell><cell>1.172 ± 0.107</cell></row><row><cell>A = DAGMA</cell><cell>Edge Cover</cell><cell cols="2">925.6 ± 240.8 0.925 ± 0.019</cell><cell>2.0e-6</cell><cell>2.502 ± 0.234</cell></row><row><cell></cell><cell>Expansive Causal</cell><cell>1828.0 ± 36.5</cell><cell>0.858 ± 0.006</cell><cell>3.0e-6</cell><cell>3.593 ± 0.607</cell></row><row><cell></cell><cell>PEF</cell><cell>4135.6 ± 84.6</cell><cell>0.667 ± 0.005</cell><cell>3.1e-5</cell><cell>122.405 ± 5.736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Average time and accuracy results on 10,000 node hierarchical scale-free graphs with ∼ 10,000 edges (averaged over 5 networks). The number of samples n is 10,000. We show results only for A = GES. This experiment was run with an AMD Zen 3 (Milan) 7543P CPU @2.8 GHz with 64 cores and 512 GB of RAM.</figDesc><table><row><cell>Partitioning Algorithm</cell><cell>SHD ↓</cell><cell cols="3">TPR ↑ FPR ↓ Time (hrs.) ↓</cell></row><row><cell>No Partition</cell><cell cols="4">333.0 ± 36.8 0.976 ± 0.003 3.0e-6 25.46 ± 17.24</cell></row><row><cell>Edge Cover</cell><cell>1214.6 ± 40.6</cell><cell>0.913 ± 0.004</cell><cell>9.0e-6</cell><cell>1.84 ± 0.02</cell></row><row><cell>Expansive Causal</cell><cell>987.2 ± 27.1</cell><cell>0.928 ± 0.002</cell><cell>7.0e-6</cell><cell>11.96 ± 5.72</cell></row><row><cell cols="2">Edge Cover 100 Comms 3506.4 ± 563.9</cell><cell>0.752 ± 0.040</cell><cell>1.4e-5</cell><cell>0.04 ± 0.02</cell></row><row><cell>Expansive Causal 100 Comms</cell><cell>2510.8 ± 72.3</cell><cell>0.821 ± 0.002</cell><cell>8.0e-6</cell><cell>1.97 ± 0.28</cell></row></table><note><p><p><p><p><p><p><p>In Table</p>3</p>we show results for (B) with A = GES. The subsets of this network are larger and more dense compared to (A) (see Fig.</p>6b</p>compared to Fig.</p>7b</p>); the GES algorithm takes significantly longer on this network topology. Our Expansive Causal achieves a faster time to solution compared to No Partition while</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results</figDesc><table><row><cell>No Partition 805</cell><cell>0.859</cell><cell>8.5e-5</cell><cell>11.8</cell></row><row><cell>PEF 1,766</cell><cell>0.692</cell><cell>8.3e-5</cell><cell>22.3</cell></row><row><cell>Disjoint 3,903</cell><cell>0.479</cell><cell>1.2e-4</cell><cell>23.9</cell></row><row><cell>Edge Cover 1,791</cell><cell>0.698</cell><cell>1.1e-4</cell><cell>7.1</cell></row><row><cell>Expansive Causal 1,717</cell><cell>0.701</cell><cell cols="2">6.4e-5 6.9</cell></row></table><note><p><p>for a synthetically-tuned E.coli network made up of 2,332 nodes and 5,691 edges. n=10,000 samples. We show results only for A = GES. This experiment was run with an Intel(R) Xeon(R) Gold 6242 CPU @ 2.80GHz with 64 cores and 192 GB of RAM.</p>Partitioning Algorithm SHD ↓ TPR ↑ FPR ↓ Time (hrs) ↓</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>such that {u * - * v} ∈ E candidates do // If an edge between u and v appears in the learned output on all subsets containing u and v, add edge to output graph. if ∀i s.t. Si ⊇ {u, v}, u ∼ A(S i ) v then // If edge appears oriented in output, add oriented edge to E * .</figDesc><table><row><cell></cell><cell>if ∃i such that Ei ∋ {u  * → v} then</cell></row><row><cell>5</cell><cell>E  *  ← E  *  ∪ {u → v};</cell></row><row><cell></cell><cell>else</cell></row><row><cell>7</cell><cell>E  *  ← E  *  ∪ {u •-• v};</cell></row><row><cell cols="2">H  *  ← (V, E  *  )</cell></row><row><cell cols="2">while H  *  contains cycle C do</cell></row><row><cell>H</cell><cell></cell></row></table><note><p>* ← score_and_discard(H * , C, {S1, . . . , SN }, X); return H * = (V, E * );</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Input: a graph G, C a list of edges comprising a cycle in G, {S i }</figDesc><table><row><cell cols="2">Algorithm 4: score_and_discard</cell><cell></cell></row><row><cell></cell><cell></cell><cell>N i=1 a partition of the nodes of G, a</cell></row><row><cell></cell><cell>matrix of observations X</cell><cell></cell></row><row><cell cols="2">Result: a modified copy of G which does not contain cycle C</cell><cell></cell></row><row><cell>V ←</cell><cell>N i,j=1 {S i ∩ S j } ;</cell><cell>// overlapping nodes</cell></row><row><cell cols="2">Ê ← {} ;</cell><cell>// overlapping edges</cell></row><row><cell cols="2">foreach</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Additionally, we will use * as a "wild card" end mark. For example u * → v means that the end mark at u can be any of three outlined in the Defn. 3.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>There is no selection bias in our setting, since data is sampled from the full vertex set V which retains causal sufficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This assumption is not required to prove identifiability of H * , rather it allows us to define the causal partition when the superstructure is not fully connected, and therefore, when we can exploit the communities in the superstructure to enable scaling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Published in Transactions on Machine LearningResearch (03/2025)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>PEF does not leverage an initial superstructure to partition, as a result the time for partitioning is longer than our proposed methods which use an artificial superstructure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>The first term in the expression is the dominant cost in time if the probelm is run on N processes. The second term is the time if the problem is run in serial divided by the actual number if processes available P</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors thank <rs type="person">Valerie Hayot-Sasson</rs> for her help with running simulations. The authors thank <rs type="person">Bryon Aragam</rs> for helpful discussions. AS is supported by the <rs type="funder">Exascale Computing Project</rs> (<rs type="grantNumber">17-SC-20-SC</rs>), a collaborative effort of the <rs type="funder">U.S. Department of Energy Office of Science</rs> and the <rs type="funder">National Nuclear Security Administration</rs>. This research used resources of the <rs type="institution" subtype="infrastructure">Argonne Leadership Computing Facility</rs>. <rs type="institution">Argonne National Laboratory</rs>'s work on the Exploration of the <rs type="projectName">Potential for Artificial Intelligence and Machine Learning to Advance Low-Dose Radiation Biology Research (RadBio-AI</rs>) project was supported by the <rs type="funder">U.S. Department of Energy, Office of Science, Office of Biological and Environment Research</rs>, under contract <rs type="grantNumber">DE-AC02-06CH11357</rs>. AD gratefully acknowledges the support of <rs type="funder">NSF</rs> <rs type="grantNumber">DGE 2140001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sjZHC9X">
					<idno type="grant-number">17-SC-20-SC</idno>
				</org>
				<org type="funded-project" xml:id="_4Dc8DHQ">
					<orgName type="project" subtype="full">Potential for Artificial Intelligence and Machine Learning to Advance Low-Dose Radiation Biology Research (RadBio-AI</orgName>
				</org>
				<org type="funding" xml:id="_Wqepay5">
					<idno type="grant-number">DE-AC02-06CH11357</idno>
				</org>
				<org type="funding" xml:id="_Qv8hTgG">
					<idno type="grant-number">DGE 2140001</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">Argonne Leadership Computing Facility</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://openreview.net/forum?id=FecsgPCOHk">https://openreview.net/forum?id=FecsgPCOHk</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Definitions</head><p>Definition A.1 (Collider on a path). Given a path P = (X 1 , . . . , X k ) on a mixed graph G, a non-endpoint vertex X i is a collider on path P if both edges adjacent to X i on the path have a directed or bi-directed edge pointing to X i . Examples include X i-1 → X i , X i ← X i+1 and X i-1 → X i , X i ↔ X i+1 . A non-endpoint vertex which is not a collider is said to be a non-collider on that path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Deferred Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Deferred Proofs from Section 3.3</head><p>Here we prove the properties in Lemma 1. 1. For any x i , x j ∈ S, the output A (X S ) has an edge between x i and x j if and only if there is an inducing path in G * relative to V \ S between them.</p><p>Proof. We begin by noting that by definition, x i and x j are adjacent in L M AG (G * , S) if and only if there is an inducing path in G * relative to V \ S between them <ref type="bibr">(Zhang, 2008a)</ref>. Moreover, by definition the PAG A (X S ) = P AG[L M AG (G * , S)] has the same adjacencies as any member of [L M AG (G * , S)], and therefore the same adjacencies as L M AG (G * , S). Thus x i and x j are adjacent in A (X S ) if and only if there is an inducing path in G * relative to V \ S between them.</p><p>2. For any triple x i , x j , x k ∈ S that form an unshielded collider in G * as x i → x j ← x k , the output A (X S ) will have an edge between x i and x j as well as x j and x k , and both of these edges will have an arrowhead at x j .</p><p>Proof. We first note that for {x i , x j , x k } ⊆ S, the edges x i → x j and x k → x j are inducing paths in G * relative to V \ S and thus the pairs x i , x j and x k , x j are adjacent in both L M AG (G * , S) and A (X S ). To show that both edges will have an arrowhead at x j in A (X S ), it thus remains to show the edges have arrowheads at x k in every [L M AG (G * , S)]. By property (1) of Definition 3.4, both edges will have arrowheads at x k in L MAG (G * , S). Given {x i , x j x k } form an unshielded collider in G * , all sets that d-separate x i from x k in G * do not contain x j . Thus given {x i , x j , x k } ⊆ S, all sets that m-separate x i form x k in L MAG (G * , S) do not contain x j , and thus the collider is unshielded in L MAG (G * , S) <ref type="bibr">(Zhang, 2008a)</ref>.</p><p>By definition of the MEC of a MAG, every element in [L M AG (G * , S)] has the same unshielded colliders, so every element in [L MAG (G * , S)] has arrowheads at x k <ref type="bibr">(Zhang, 2008b)</ref>. Thus the PAG A (X S ) = P AG[L MAG (G * , S)] has arrowheads at x k on both edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">For any</head><p>Assume for the sake of contradiction that v → u in G * . By the definition of P AG[L MAG (G * , S)], given u ∼ A (X S ) v with an arrowhead at v in A (X S ), it holds that u and v are adjacent with an arrowhead at v for every element of [L MAG (G * , S)] <ref type="bibr">(Zhang, 2008a)</ref>. In particular, u and v are adjacent with an arrowhead at v in L M AG (G * , S). By definition of the latent MAG, u and v are adjacent with an arrowhead at v in L M AG (G * , S) implies that one of the following hold:</p><p>and there is an inducing path in G * between u and v relative to V \ S, or (3) there is some other inducing path between u and</p><p>Thus in all three cases we arrive at a contradiction, and so we conclude that v ̸ → u in G * , and thus that u → v in G * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Deferred Proofs from Section 4</head><p>In this section, we consider superstructure G satisfying Assumption 2, a learner A satisfying Assumption 1, {S 1 , . . . , S N } a causal partition with respect to G and G * , and H * the output of Algorithm 1 on G, {A (X Si )} N i=1 . We begin by proving property (i) in Theorem 1.</p><p>By the definition of a causal partition, {S 1 , . . . , S N } is edge-covering with respect to G and thus ∃i ∈ [N ] such that u, v ∈ S i . Moreover, given u, v ∈ S i , the edge between the two nodes in G * is an inducing path with respect to V \ S i and so by statement (1) in Lemma 1, u ∼ A (X S i ) v. Thus u ∼ G v and u ∼ A (X S i ) v so u * - * v ∈ E candidates . Moreover, for any subset S j ∋ u, v, the edge between u and v in G * is an inducing path with respect to V \ S j , so u * - * v ∈ E j for all j such that u, v ∈ S j . Thus an edge between u and v will be added to E * , so u ∼ H * v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversely, consider any</head><p>, then E candidates will not contain an edge between u and v and thus neither will E * . If u ∼ G v and ∃i ∈ [N ] such that u ∼ A (X S i ) v, then E candidates will contain an edge between u and v. However because {S 1 , . . . , S N } is a causal partition, by property (ii) of Definition 3.5 there exists some j ∈ [N ] such that u, v ∈ S j and u and v do not have an edge between them in E i the edges of output A (S j ). Thus no edge between u and v will be added to E * . We thus conclude that u ∼ H * v if and only if u ∼ G * v.</p><p>In order to prove property (ii) of Theorem 1, we will use the following lemma:</p><p>We now prove property (ii) of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5. For any unshielded collider</head><p>The proof follows directly from application of Lemma 4.</p><p>We conclude with the proof of property (iii):</p><p>It thus remains to show that the v-structure edges are oriented correctly in H * . By the definition of a causal partition, ∃i such that {u, v, w} ⊆ S i . Thus by statement (2) in Lemma 1, u * → v and w * → v in A (X Si ). Thus the condition in Line 5 is satisfied so both u → v and w → v will be added to E * , and thus the edges are oriented correctly in H * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Deferred Proofs from Section 5.1</head><p>Throughout this section, we assume superstructure G satisfies Assumption 2. Consider {S 1 , . . . , S N } be a vertex-covering partition of G and denote by {S ′ 1 , . . . , S ′ N } the causal expansion of {S 1 , . . . , S N } with respect to G.</p><p>In order to prove Lemma 2, we introduce several auxiliary lemmas. Proving that the causal expansion satisfies properties (i) and (iii) of Definition 3.5 is straightforward. These arguments are contained in Lemmas 7 and 8 respectively:</p><p>Thus by definition of the expansive causal partition, {u, v, w} ⊆ S ′ i .</p><p>Proving that the causal expansion satisfies property (ii) of Definition 3.5 is more involved. We first establish the following helper lemma:</p><p>Lemma 9. Consider any S ⊆ V and any u, v ∈ S such that u ̸ ∼ G * v in DAG G * . Then any path Π ⊆ S such that length(Π) &gt; 1 is not an inducing path between u and v in G * relative to V \ S. Moreover, any path Π = (u, q 1 , q 2 , . . . , q k-1 , q k , v) such that either {u, q 1 , q 2 } ⊆ S or {q k-1 , q k , v} ⊆ S is not an inducing path between u and v in G * relative to V \ S.</p><p>Proof of Lemma 9. Both conditions on Π imply the existence of non-endpoints q, q ′ ∈ S adjacent along path Π. By definition of an inducing path, q and q ′ must both therefore be colliders on Π. This implies that the edge between q and q ′ in path Π must have an arrowhead at both q and q ′ in G * . However G * is a DAG and cannot contain bi-directed edges, so q and q ′ cannot both be colliders on Π, and Π is therefore not an inducing path.</p><p>We now use Lemma 9 to prove that the causal expansion satisfies property (ii) of Definition 3.5:</p><p>Recall that by Assumption 1, for any subset S ′ i , u ∼ A(S ′ i ) if and only if there is an inducing path between u and v in G * relative to V \ S ′ i . Thus to prove Lemma 10, it suffices to show that ∃i ∈ [N ] such that no inducing path exists between u and v in G * relative to V \ S ′ i . By Lemma 9, any path Π in G * of length greater than 1 such that Π ⊆ S ′ i cannot be such an inducing path. As u ̸ ∼ G * v, all paths between u and v in G * have length at least 1. Thus to prove Lemma 10, it suffices to show that ∃i ∈</p><p>For any u ∈ S ⊆ V , denote by dist G * (u, ∂ out S) the shortest-path distance from u to any node v ∈ ∂ out (S). In other words, dist G * (u, ∂ out S) is the minimum number of edges between a node u and any node w ̸ ∈ S. Note that for any u ∈ S, dist G * (u, ∂ out S) ≥ 1.</p><p>Case 3. Recall that by definition of the expansive causal partition, S ′ i = S i ∪ ∂ out (S i ) for original vertexcovering partition {S 1 , . . . , S N }, where the outer boundary ∂ out (S i ) is defined by the edges in superstructure</p><p>Moreover, as G satisfies Assumption 2, this implies v ∼ G z. Thus by definition of the expansive causal partition it must be that v ∈ S ′ i \S i . As the original partition {S 1 , . . . , S N } is vertex-covering, this implies ∃j ∈</p><p>) &gt; 1, then either Case 1 or Case 2 respectively imply that u ̸ ∼ A (S ′ j ) v, which would conclude the proof. It thus remains to consider the case where dist(v, ∂ out (S ′ j )) = 2 and dist(u, ∂ out (S ′ j )) = 1. We thus have the following setup: by assumption of Case 3, dist G * (u, ∂ out S ′ i ) = 2 and dist G * (v, ∂ out S ′ i ) = 1. Then by the above arguments, we have shown j</p><p>Thus by Assumption 1, there must exist Π i and inducing path between u and v with respect to V \ S ′ i and Π j an inducing path between u and v with respect to</p><p>By definition of an inducing path q i must be a collider on Π i in G * , so u ∈ anc G * (q i ), and q i must be an ancestor of either v or u. As G * is acyclic and u ∈ anc G * (q i ), q i cannot be an ancestor of u and must therefore be an ancestor of v: q i ∈ anc G * (v).</p><p>Similarly, as dist G * (v, ∂ out S ′ j ) = 2, Π j must contain a suffix {q j , v} ⊆ Π j ∩ S ′ j such that q j ̸ = u. Moreover by an analogous argument to the above, v ∈ anc G * (q j ) and q j ∈ anc G * u.</p><p>We have therefore concluded the following: ∃q i , q j ∈ V such that u ∈ anc G * (q i ), q i ∈ anc G * (v), v ∈ anc G * (q j ), and q j ∈ anc G * (u). However this implies the existence of a cycle in G * , which contradicts the assumption that G * is a DAG. Thus it cannot hold that both u ∼</p><p>As superstructure G satisfies Assumption 2 this implies u ∼ G z and thus by definition of the expansive causal partition, implies u ∈ S ′ i \ S i . As the original partition was vertex-covering, this implies ∃j ̸ = i such that u ∈ S j . Thus by definition of the expansive causal partition, u ∈ S ′ j and dist G * (u,</p><p>) v, and so the statement of Lemma 10 holds.</p><p>Lemma 2 follows directly from Lemmas 7, 8, and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Finite Sample Effects</head><p>While the theoretical results in Section 4 only apply to the infinite data regime, in this section we discuss heuristics for addressing the effects of learning with finite samples and describe a practical algorithm for real-world causal discovery problems. In the finite data setting, there two key ways that finite samples cause divergence from the idealized assumptions studied in Section 4: (1) the superstructure may be imperfect and (2) the result of learning over a local subset may not be a latent projection and therefore the merged graph may contain cycles. We describe our finite sample screening procedure in Algorithm 3. In score_and_discard, we resolve cycles by discarding the edge corresponding to a the lowest score, where the score is related to the log-likelihood of the data with and without each edge in the cycle.</p><p>Imperfect Superstructure : In real-world causal discovery applications, one may wish to learn a superstructure G from data <ref type="bibr" target="#b10">(Constantinou et al., 2023)</ref>. Several algorithms for learning a superstructure from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Controlling Maximum Subset Size</head><p>A key factor in accuracy-timing trade-offs is controlling the size of the largest subset in the partition. Here we observe that the largest subset produced by the causal expansion in Section 3.4 is governed by specific connectivity properties of the initial partition on which it is built. In particular, for graphs with strong community structure, if the initial partition is strongly correlated with community structure, then the resultant subsets in the causal expansion will not be much larger than any of the subsets in the input.</p><p>For the causal expansion defined in Section 3.4, the maximum size of any subset is controlled by the sizes of the subsets in the input partition and their corresponding vertex expansion values. For any set S such that |S| ≤ |V |/2, the vertex expansion of S in graph G is defined as</p><p>In particular, if the superstructure G has strong community structure and the initial partition {S 1 , . . . , S N } is constructed appropriately, then the subsets of the causal expansion will not be dramatically larger than those in the initial partition. See Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 DAG generation</head><p>Ground truth DAGs, G * , are synthetically created using a Barabasi-Albert scale-free model <ref type="bibr" target="#b3">(Barabási &amp; Bonabeau, 2003)</ref>. A random topological ordering is imposed on the nodes so that the graph is acyclic. Data is generated assuming a Gaussian noise model: (X 1 , ..., X p ) T = ((X 1 , ..., X p )W ) T + ϵ where ϵ ∼ N (0, σ 2 p ). W is an upper-triangular matrix of edge weights where w ij ̸ = 0 if and only if i → j is an edge in G * . The variance σ 2 is uniformly sampled from (0, 1]. Each column vector X i represents the data distribution for a variable corresponding to a node i in G * . For our experiments we create graphs (p=50) with two communities, where each community has a Barabasi-Albert scale-free topology and communities are connected using preferential attachment. Any cycles created by this are removed to ensure the graph is a DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Partitioning Algorithm Parameter settings</head><p>In this section we describe the parameter settings for the partitioning algorithms: Disjoint, Edge Cover, Expansive Causal and PEF. The Disjoint partition is networkx.greedy_modularity. For networks with 100 communities of size 100 nodes, we set the best_n and cutoff both to 100. For all other experiments we do not set these parameters. These parameters are also used for the Edge Cover and Expansive Causal partitions, and there are no additional parameters that need to be set here. For PEF , the minimum size of each community is set in order to select the optimal partitioning. For networks with 100 communities of size 100 nodes, this is set to 1% of the size of the node set. For all other experiments this is set to 5% of the size of the node set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Causal Discovery Algorithm Parameter settings</head><p>In this section we describe all the parameter settings for the causal discovery algorithm A used for local learning. The four algorithms we chose are GES, PC, RFCI, and DAGMA. For GES, PC, and RFCI we use the R implementations in the pcalg library. For PC and RFCI the significance level α was set to 0.001. For GES, fixedGaps which controls which edges are added in the greedy search is set to all the edges not in the superstructure, maxDegree is not limited, and adaptive is set to "triples". For DAGMA, we used the Right: Accuracy and time trade-off for 1,000 graph with ten communities of size 100 with scale-free topology with 1,000 samples. We see that certain subset sizes take unexpectedly long for GES learner.</p><p>LinearModel with L2 loss. During optimization lambda1 was set to 0.02, while all other parameters were set to the default. These parameters were consistent across all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Details on merging DAG subgraphs</head><p>Causal discovery algorithms GES, PC and DAGMA do not satisfy Assumption 1 and instead output a DAG (for the PC algorithm we randomly choose a graph in the MEC). Despite this, we still employ a screen operation when merging subgraphs in a similar manner to Algorithm 1. However, given there are no consistency guarantees over the observed subset of nodes for these algorithms, it is possible that after merging the resultant directed graph contains cycles. Therefore for these algorithms we employ the algorithm discussed in Appendix C which accounts for the presence of cycles. For these learners Algorithm 3 now takes in a set of DAGs instead of PAGs and returns a DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Time and Accuracy trade offs</head><p>The computational bottleneck for divide-and-conquer algorithms is the size of the largest subset: max i |S i | for a partition {S 1 . . . S N }. This is because we expect causal discovery algorithms to converge to an estimated graph faster for smaller variables sets (the graph space defined by a smaller variable set is smaller). However, we observe that the convergence of GES appears to be a function of both size of the subset, and the topology of G * . Fig. <ref type="figure">10</ref> (left) shows the time to solution and TPR as the size of the biggest subset increases. For this study, we use a 1,000 node hierarchical scale-free graph. This is equivalent to the types of graphs in Section 6.4. To control the size of the subsets we fix the number of communities and resolution for the greedy modularity disjoint partition -we sweep through five different disjoint partitions, increasing size of the largest subset. Here, we see expected scaling behavior -as the size of the largest subset increases so does the time solution. The largest time to solution is for the non-partitioned method on the entire 1,000 node graph. This means that partitioning the graph always enables some scaling. The Expansive Causal and Edge Cover partitions are extensions of each Disjoint partition. We observe that for our proposed partition methods (Expansive Causal Partition and Edge Cover) we do not increase the size of the largest subset significantly (this aligns with notes in Appendix E), and we benefit from a significant boost in accuracy. In Fig. <ref type="figure">10</ref> (right) we run the same study but with a 1,000 node graph with 10 communities, each with size of 100 and scale-free topology. This is equivalent to the types of graphs in Section 6.1 through Section 6.3, but with more communities. Here, we observe good scaling when the size of the largest partition is small and close to the size of the natural communities. However beyond this, the time to solution increases to be even larger than the non-partitioned method. This suggest that certain 'bad' subsets incur a longer convergence time for the GES causal discovery algorithm. We hypothesize this is related to violation of causal sufficiency of these subsets -subsets that contain more confounders (unobserved common causes) outside may result in sub-optimal convergence in the GES learner. Note that this result is not due to our causal partition or</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Sandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 56th Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="670" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scale-free networks in cell biology</title>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cell science</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="4947" to="4957" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network science. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page">20120375</biblScope>
			<date type="published" when="1987">1987. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale-free networks</title>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Bonabeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific american</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="60" to="69" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network biology: understanding the cell&apos;s functional organization</title>
		<author>
			<persName><forename type="first">Albert-Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><forename type="middle">N</forename><surname>Oltvai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Functionally related genes cluster into genomic regions that coordinate transcription at a distance in saccharomyces cerevisiae</title>
		<author>
			<persName><forename type="first">Alanna</forename><surname>Cera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">K</forename><surname>Holganza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Abu Hardan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irvin</forename><surname>Gamarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Eldabagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Deschaine</surname></persName>
		</author>
		<author>
			<persName><surname>Elkamhawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Exequiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Sisso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Arnone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Msphere</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="1128" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding community structure in very large networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristopher</forename><surname>Mark Ej Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66111</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The impact of prior knowledge on causal structure learning</title>
		<author>
			<persName><forename type="first">Zhigao</forename><surname>Anthony C Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neville</forename><forename type="middle">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Kitson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Introduction to the foundations of causal discovery</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Leena</forename><forename type="middle">Chennuru</forename><surname>Philipp M Faller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atalanti</forename><forename type="middle">A</forename><surname>Vankadara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Mastakouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><surname>Janzing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09552</idno>
		<title level="m">Self-compatibility: Evaluating causal discovery without ground truth</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global transcriptional regulatory network for escherichia coli robustly connects gene expression to transcription factor activities</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Mih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Yurkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colton</forename><forename type="middle">J</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">O</forename><surname>Palsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="10286" to="10291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning big Gaussian Bayesian networks: Partition, estimation and fusion</title>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6340" to="6370" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Community detection in large-scale networks: a survey and empirical evaluation</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Harenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Gjeltema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ranshous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Harlalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramona</forename><surname>Seay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanchana</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagiza</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="426" to="439" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partitioned hybrid learning of Bayesian network structures</title>
		<author>
			<persName><forename type="first">Jireh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1695" to="1738" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A proximity-based generative model for online social network topologies</title>
		<author>
			<persName><forename type="first">Emory</forename><surname>Hufbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Khamfroush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Computing, Networking and Communications (ICNC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="648" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal discovery from soft interventions with unknown targets: Characterization and learning</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9551" to="9561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterization and learning of causal graphs with latent variables from soft interventions</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A guide to conquer the biological network era using graph theory</title>
		<author>
			<persName><forename type="first">Mikaela</forename><surname>Koutrouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Paez-Espino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">A</forename><surname>Pavlopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in bioengineering and biotechnology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A ring-based distributed algorithm for learning high-dimensional bayesian networks</title>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">D</forename><surname>Laborda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Torrijos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">M</forename><surname>Puerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">A</forename><surname>Gámez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Symbolic and Quantitative Approaches with Uncertainty</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast pc algorithm for high dimensional causal discovery with multi-core pcs</title>
		<author>
			<persName><forename type="first">Thuc</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1483" to="1495" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parallel simulated annealing with a greedy algorithm for bayesian network structure learning</title>
		<author>
			<persName><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seoung</forename><surname>Bum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1157" to="1166" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A graph partitioning approach for bayesian network structure learning</title>
		<author>
			<persName><forename type="first">Shuohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuihua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Chinese Control Conference</title>
		<meeting>the 33rd Chinese Control Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2887" to="2892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clustering and community detection in directed networks: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fragkiskos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="95" to="142" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-dimensional consistency in score-based and hybrid structure learning</title>
		<author>
			<persName><forename type="first">Preetam</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3151" to="3183" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finding optimal Bayesian network given a super-structure</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Perrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiya</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoru</forename><surname>Miyano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling up greedy causal search for continuous variables</title>
		<author>
			<persName><forename type="first">Ramsey</forename><surname>Joseph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07749</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting hierarchical modularity in biological networks</title>
		<author>
			<persName><forename type="first">Erzsébet</forename><surname>Ravasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Systems Biology</title>
		<imprint>
			<biblScope unit="page" from="145" to="160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal inference via ancestral graph models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Oxford Statistical Science Series</title>
		<imprint>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nested markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Thomas S Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="334" to="361" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph clustering</title>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Satu</surname></persName>
		</author>
		<author>
			<persName><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer science review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="64" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The design and analysis of parallel algorithms</title>
		<author>
			<persName><forename type="first">Justin R</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Constructing bayesian network models of gene expression networks from microarray data</title>
		<author>
			<persName><forename type="first">Pater</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Valerio Aimale</surname></persName>
		</author>
		<author>
			<persName><surname>Wimberly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An anytime algorithm for causal inference</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning the structure of bayesian networks with ancestral and/or heuristic partition</title>
		<author>
			<persName><forename type="first">Xiangyuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">584</biblScope>
			<biblScope unit="page" from="719" to="751" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and causal inference: The works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="221" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A nearly-linear time algorithm for exact community recovery in stochastic block model</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony Man-Cho</forename><surname>So</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10126" to="10135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The architecture of biological networks. Complex systems science in biomedicine</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wuchty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erszébet</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="165" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Genomic analysis of the hierarchical structure of regulatory networks</title>
		<author>
			<persName><forename type="first">Haiyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="14724" to="14731" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CUDA-based parallel PC algorithm for causal structure learning on GPU</title>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Zarebavani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foad</forename><surname>Jafarinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matin</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saber</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><surname>Cupc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="542" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Block learning bayesian network structure from data</title>
		<author>
			<persName><forename type="first">Yi-Feng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Leng</forename><surname>Poh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Hybrid Intelligent Systems (HIS&apos;04)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards effective causal partitioning by edge cutting of adjoint graph</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Causal reasoning with ancestral graphs</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
