<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2010-07-09">9 July 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ali</forename><surname>Shojaie</surname></persName>
							<email>shojaie@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Michailidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Penalized likelihood methods for estimation of sparse high-dimensional directed acyclic graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-07-09">9 July 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/biomet/asq038</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive lasso</term>
					<term>Directed acyclic graph</term>
					<term>High-dimensional sparse graphs</term>
					<term>Lasso</term>
					<term>Penalized likelihood estimation</term>
					<term>Small n large p asymptotics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Directed acyclic graphs are commonly used to represent causal relationships among random variables in graphical models. Applications of these models arise in the study of physical and biological systems where directed edges between nodes represent the influence of components of the system on each other. Estimation of directed graphs from observational data is computationally NP-hard. In addition, directed graphs with the same structure may be indistinguishable based on observations alone. When the nodes exhibit a natural ordering, the problem of estimating directed graphs reduces to the problem of estimating the structure of the network. In this paper, we propose an efficient penalized likelihood method for estimation of the adjacency matrix of directed acyclic graphs, when variables inherit a natural ordering. We study variable selection consistency of lasso and adaptive lasso penalties in high-dimensional sparse settings, and propose an error-based choice for selecting the tuning parameter. We show that although the lasso is only variable selection consistent under stringent conditions, the adaptive lasso can consistently estimate the true graph under the usual regularity assumptions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Graphical models are efficient tools for the study of statistical models through a compact representation of the joint probability distribution of the underlying random variables. The nodes of the graph represent the random variables, while the edges capture the relationships among them. Both directed and undirected edges are used to represent interactions among random variables. However, there is a conceptual difference between these two types of graphs: while undirected graphs are used to represent conditional dependence, directed graphs often represent causal relationships (see <ref type="bibr" target="#b14">Pearl, 2000)</ref>. Directed acyclic graphs, also known as Bayesian networks, are a special class of directed graphs, where all the edges of the graph are directed and the graph has no cycles. Such graphs are the main focus of this paper and unless otherwise specified, any reference to directed graphs below refers to directed acyclic graphs.</p><p>Directed graphs are used in graphical models and belief networks and have been the focus of research in the computer science literature <ref type="bibr" target="#b14">(Pearl, 2000)</ref>. Important applications involving directed graphs also arise in the study of biological systems, including cell signalling pathways and gene regulatory networks <ref type="bibr" target="#b13">(Markowetz &amp; Spang, 2007)</ref>.</p><p>Estimation of directed acyclic graphs is an NP-hard problem, and estimation of the direction of edges may not be possible due to observational equivalence. Most of the earlier estimation methods involve greedy algorithms that search through the space of possible graphs. A number of methods are available for estimating directed graphs with a small to moderate number of nodes. The max-min hill-climbing algorithm <ref type="bibr" target="#b22">(Tsamardinos et al., 2006)</ref>, and the PC-algorithm <ref type="bibr" target="#b21">(Spirtes et al., 2000)</ref> are two such examples. However, the space of directed graphs grows superexponentially with the number of nodes <ref type="bibr" target="#b16">(Robinson, 1977)</ref>, and estimation using search-based methods, especially in high-dimensional settings, becomes impractical. Bayesian methods (e.g <ref type="bibr" target="#b3">Heckerman et al., 1995)</ref> are also computationally very intensive and therefore not particularly appropriate for high-dimensional settings. Recently, <ref type="bibr" target="#b6">Kalisch &amp; Bühlmann (2007)</ref> proposed an implementation of the PC-algorithm with polynomial complexity in high-dimensional sparse settings. When the variables inherit a natural ordering, estimation of directed graphs is reduced to estimating their structure or skeleton. Applications with a natural ordering of variables include estimation of causal relationships from temporal observations, estimation of transcriptional regulatory networks from gene expression data and settings where additional experimental data can determine the ordering of variables. Examples of such applications are given in § 6.</p><p>For Gaussian random variables, conditional independence relations among random variables are represented using an undirected graph, known as the conditional independence graph. The edges of this graph represent conditional dependencies among random variables, and correspond to nonzero elements of the inverse covariance matrix, also known as the precision matrix. Different penalization methods have been recently proposed to obtain sparse estimates of the precision matrix. <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref> considered an approximation to the problem of sparse inverse covariance estimation using the lasso penalty. They showed, under appropriate assumptions, that their proposed method correctly determines the neighbourhood of each node. <ref type="bibr" target="#b0">Banerjee et al. (2008)</ref> and <ref type="bibr" target="#b2">Friedman et al. (2008)</ref> explored different aspects of the problem of estimating the precision matrix using the lasso penalty, while <ref type="bibr" target="#b24">Yuan &amp; Lin (2007)</ref> considered other choices for the penalty. <ref type="bibr" target="#b17">Rothman et al. (2008)</ref> proved consistency in Frobenius norm, as well as in matrix norm, of the 1 -regularized estimate of the precision matrix when p n, while <ref type="bibr" target="#b9">Lam &amp; Fan (2009)</ref> extended their result and considered estimation of matrices related to the precision matrix, including the Cholesky factor of the inverse covariance matrix, using general penalties. Penalization of the Cholesky factor of the inverse covariance matrix has also been considered by <ref type="bibr" target="#b4">Huang et al. (2006)</ref> and <ref type="bibr" target="#b12">Levina et al. (2008)</ref>, who used the lasso penalty in order to obtain a sparse estimate of the inverse covariance matrix. This method is based on the regression interpretation of the Cholesky factorization model and therefore requires the variables to be ordered a priori.</p><p>In this paper, we consider the problem of estimating the skeleton of directed acyclic graphs, where the variables exhibit a natural ordering. The known ordering of variables is exploited to reformulate the likelihood as a function of the adjacency matrix of the graph, which results in an efficient algorithm for estimation of structure of directed graphs using penalized likelihood methods. Although the results of this paper are presented for the case of Gaussian observations, the proposed method can also be applied to non-Gaussian observations, provided the underlying causal mechanisms in the network are linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">REPRESENTATION OF DIRECTED ACYCLIC GRAPHS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2•1. Background and notation</head><p>Consider a graph G = (V, E), where V corresponds to the set of nodes with p elements and E ⊂ V × V to the edge set. The nodes of the graph represent random variables X 1 , . . . , X p and the edges capture the associations among them. An edge is called directed if ( j, i) / ∈ E whenever (i, j) ∈ E, and undirected when (i, j) ∈ E if and only if ( j, i) ∈ E. We denote by pa i the set of parents of node i and for j ∈ pa i , we denote j → i. The skeleton of a directed graph is the undirected graph that is obtained by replacing directed edges in E with undirected ones. We represent E using the adjacency matrix A of the graph; i.e. a p × p matrix whose ( j, i)th entry is nonzero if there is an edge between nodes j and i.</p><formula xml:id="formula_0">X 1 X 1 X 2 X 2 X 3 X 3 X 4 ρ 21 ρ 32 (b) (a)</formula><p>The estimation of directed graphs is a challenging problem due to the so-called observational equivalence with respect to the same probability distribution. More specifically, regardless of the sample size, it may not be possible to infer the direction of causation among random variables from observational data. As an illustration, consider the simple graph in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Reversing the direction of all edges of the graph results in a new graph, which is isomorphic to the original graph, and hence not distinguishable from observations alone.</p><p>The second challenge in estimating directed graphs is that conditional independence among random variables may not reveal the skeleton. The notion of conditional independence in directed graphs is represented using the concept of d-separation <ref type="bibr" target="#b14">(Pearl, 2000)</ref>, or equivalently, the moral graph, obtained by removing the directions of the edges and joining the parents of each node <ref type="bibr" target="#b10">(Lauritzen, 1996)</ref>. Therefore, estimation of the conditional independence structure reveals the structure of the moral graph, which includes additional edges between parents of each node. For instance, X 2 and X 3 are connected in the moral graph of the graph in Fig. <ref type="figure" target="#fig_0">1(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2•2. The latent variable model</head><p>The causal effect of random variables in a directed acyclic graph is often explained using structural equation models <ref type="bibr" target="#b14">(Pearl, 2000)</ref>. In particular,</p><formula xml:id="formula_1">X i = f i (pa i , Z i ) (i = 1, . . . , p), (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where the random variables Z i are the latent variables representing the unexplained variation in each node. To model the association among the nodes, we consider a simplification of (1) with linear f i . More specifically, let ρ i j represent the effect of node j on i for j ∈ pa i . Then</p><formula xml:id="formula_3">X i = j∈pa i ρ i j X j + Z i (i = 1, . . . , p). (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>If the random variables are Gaussian, equations ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_3">2</ref>) are equivalent, in the sense that ρ i j are the coefficients of the linear regression model of X i on X j , for j ∈ pa i . It is known in the normal case that ρ i j = 0, if j / ∈ pa i . Consider the graph in Fig. <ref type="figure" target="#fig_0">1(a)</ref>; denoting the influence matrix of the graph by , (2) can be written in compact form as X = Z , where for the example above, we have</p><formula xml:id="formula_5">= ⎛ ⎝ 1 0 0 ρ 12 1 0 ρ 12 ρ 23 ρ 23 1 ⎞ ⎠ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="522">ALI SHOJAIE AND GEORGE MICHAILIDIS</head><p>Let the latent variables Z i be independent with mean μ i and variance σ 2 i . Then, E(X ) = μ and = var(X ) = D T , where D = diag (σ 2 i ) and T denotes the transpose of the matrix . The following result from <ref type="bibr" target="#b19">Shojaie &amp; Michailidis (2009)</ref> establishes the relationship between the influence matrix , and the adjacency matrix A of the graph. The second part of the lemma establishes a compact relationship between and A in the case of directed graphs, which is explored in § 3 to directly formulate the problem of estimating the skeleton of the graph.</p><p>LEMMA 1. For any graph G, with the adjacency matrix A, and influence matrix ,</p><formula xml:id="formula_6">(i) = A 0 + A 1 + A 2 + • • • = ∞ r =0 A r</formula><p>, where A 0 ≡ I . (ii) If G is a directed acyclic graph, then has full rank and = (I -A) -1 . Remark 1. Part (ii) of Lemma 1 and the fact that = D T imply that for any directed acyclic graph, if D ii &gt; 0 for all i, then is full rank. More specifically, let φ j ( ) denote the jth eigenvalue of matrix . Then, φ min ( ) &gt; 0, or φ max ( -1 ) &lt; ∞. Similarly, since -1 = -T D -1 -1 , then having full rank implies that φ min ( -1 ) &gt; 0, or equivalently φ max ( ) &lt; ∞. This result also applies to all subgraphs of a directed graph.</p><p>The properties of the proposed latent variable model established in Lemma 1 are independent of the choice of probability distribution. In fact, since the latent variables Z i in (2) are assumed independent, given the entries of the adjacency matrix, the distribution of each random variable X i in the graph only depends on the values of pa i . Hence, regardless of the choice of the probability distribution, the joint distribution of the random variables is compatible with G <ref type="bibr">(Pearl, 2000, p. 16</ref>). Therefore, based on the equivalence of conditional independence and d-separation, if the joint probability distribution of random variables on a directed graph is generated according to the latent variable model (2), zeros of the adjacency matrix, A, determine conditional independence relations among random variables. As mentioned before, under the normality assumption, the latent variable model is equivalent to the general structural equation model. Although we focus on Gaussian random variables in the remainder of this paper, the estimation procedure proposed in § 3 can be applied to a variety of other distributions, if one is willing to assume the linear structure in equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PENALIZED LIKELIHOOD ESTIMATION OF DIRECTED GRAPHS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3•1. Problem formulation</head><p>Consider the latent variable model of § 2•2 and denote by X the n × p data matrix. We assume, without loss of generality, that the X i s are centred and scaled, so that μ i = 0 and</p><formula xml:id="formula_7">σ 2 i = 1 (i = 1, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , p).</head><p>Denote by ≡ -1 the precision matrix of a p-vector of Gaussian random variables and consider a general penalty function J ( ). The penalized estimate of is then given by</p><formula xml:id="formula_8">ˆ = arg min 0 {-log det( ) + tr ( S) + λJ ( )},<label>(3)</label></formula><p>where S = n -1 X T X denotes the empirical covariance matrix and λ is the tuning parameter controlling the size of the penalty. Applications in biological and social networks often involve sparse networks. It is therefore desirable to find a sparse solution for (3). This becomes more important in the small n, large p setting, where the unpenalized solution is unreliable. The lasso penalty and the adaptive lasso penalty <ref type="bibr" target="#b25">(Zou, 2006</ref>) are singular at zero and therefore result in sparse solutions. We consider these two penalties in order to find a sparse estimate of the adjacency matrix. However, the optimization algorithm proposed here can also be used with other choices of penalty function, if the penalty is applied to each individual component of the adjacency matrix.</p><p>Using the latent variable model of § 2•2, and the relationship between the covariance matrix and the adjacency matrix of directed graphs established in Lemma 1, the problem of estimating the adjacency matrix of the graph can be directly formulated as an optimization problem based on A. Specifically, if the underlying graph is directed, and the ordering of the variables is known, then, A is a lower triangular matrix with zeros on the diagonal. Let A = {A : A i j = 0, j i}. Then, using the facts that det(A) = 1 and σ 2 i = 1, A can be estimated by: Â = arg min (4)</p><p>In this paper, we consider the general weighted lasso problem, where</p><formula xml:id="formula_9">J (A) = i, j=1: p, j&lt;i w i j |A i j |.</formula><p>The lasso and adaptive lasso problems are special cases of this general penalty. In the case of the lasso, w i j = 1. In the original proposal of <ref type="bibr" target="#b25">Zou (2006)</ref>, the weights for the adaptive lasso are obtained by setting w i j = | Ãi j | -γ , for some initial estimate of the adjacency matrix Ã and some power γ . We consider the following modification of the original weights:</p><formula xml:id="formula_10">w i j = 1 ∨ | Ãi j | -γ , (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where the initial estimates Ã are obtained from the regular lasso estimates, and x ∨ y represents the maximum of x and y. Aside from the truncation of weights from below, which is implemented to facilitate the study of asymptotic properties, the main difference between the adaptive lasso penalty using ( <ref type="formula" target="#formula_10">5</ref>) and the proposal of <ref type="bibr" target="#b25">Zou (2006)</ref> is the use of the lasso estimates to construct the weights. In § § 4 and 5, we show that this modification, which could also be considered a two-stage or iterated lasso penalty, results in improvements over the regular lasso penalty in terms of both asymptotic properties and numerical performance.</p><p>The objective functions for the lasso and adaptive lasso problems are convex. However, since the 1 penalty is nondifferentiable, these problems can be reformulated using matrices A + = max(A, 0) and A -=min(A, 0). To that end, let W be the p × p matrix of weights for the adaptive lasso, or the matrix of ones for the lasso estimation problem. Problem (4) can then be formulated as min</p><formula xml:id="formula_12">A + ,A -0 tr {S(I -A + + A -) T (I -A + + A -) + λ(A + + A -)W + (A + + A -)1 u + }, (6)</formula><p>where 0 is interpreted componentwise, is a large positive number and 1 u + is the indicator matrix for upper triangular elements of a p × p matrix, including the diagonal elements. The last term of the objective function, tr { (A + + A -)1 u + }, prevents the upper triangular elements of the matrices A + and A -being nonzero.</p><p>Problem (6) corresponds to a quadratic optimization problem with nonnegativity constraints and can be solved using standard interior point algorithms. However, such algorithms do not scale well with dimension and are only applicable if p ranges in the hundreds. In § 3•2, we present an alternative formulation of the problem, which leads to more efficient algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3•2. Optimization algorithm</head><p>Consider again the problem of estimating the adjacency matrix of directed graphs with the general weighted lasso penalty. Let a i be the ith row of matrix A, and denote by lthe set of indices up to l -1, i.e. l -= { j : j = 1, . . . , l -1}. Then (4) can be written as</p><formula xml:id="formula_13">Â = arg min A∈A p i=1 a i Sa T i -2a i S i + λ|a i |W i .<label>( 7 )</label></formula><p>It can be seen that the objective function in ( <ref type="formula" target="#formula_13">7</ref>) is separable and therefore it suffices to solve the optimization problem over each row of matrix A. Then, taking advantage of the lower triangular structure of A and noting that A 11 = 0, solving ( <ref type="formula" target="#formula_13">7</ref>) is equivalent to solving the p -1 optimization problems:</p><p>Âi,i -= arg min</p><formula xml:id="formula_14">θ ∈R i-1 ⎧ ⎨ ⎩ θ T S i -,i -θ -2S i,i -θ + λ i-1 j=1 |θ j |w i j ⎫ ⎬ ⎭ (i = 2, . . . , p). (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>In addition,</p><formula xml:id="formula_16">S i -,i -= n -1 (X i -) T X i -and S i,i -= n -1 (X i ) T X i -,</formula><p>and hence the problem in ( <ref type="formula" target="#formula_14">8</ref>) can be reformulated as the following 1 -regularized least-squares problems:</p><formula xml:id="formula_17">Âi,i -= arg min θ ∈R i-1 ⎧ ⎨ ⎩ n -1 X i -X i -θ 2 2 + λ i-1 j=1 |θ j |w i j ⎫ ⎬ ⎭ (i = 2, . . . , p). (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>The formulation in ( <ref type="formula" target="#formula_17">9</ref>) indicates that the ith row of matrix A includes the coefficient of projection of X i on X j ( j = 1, . . . , i -1), in agreement with the discussion in § 2•2. It also reveals a connection between estimation of the underlying graphs and the neighbourhood selection approach of <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref>: when the underlying graph is directed, the approximate solution of the neighbourhood selection problem is exact, if the regression model is fitted on the set of parents of each node instead of all other nodes in the graph. Using ( <ref type="formula" target="#formula_17">9</ref>), the problem of estimating directed graphs can be solved very efficiently. In fact, it suffices to solve p -1 lasso problems for estimation of least-squares coefficients, with dimensions ranging from 1 to p -1. To solve these problems, we use the efficient pathwise coordinate optimization algorithm of <ref type="bibr" target="#b1">Friedman et al. (2007)</ref>, implemented in the R-package glmnet (R Development Core Team, 2010). The proposed procedure is summarized in Algorithm 1.</p><p>Algorithm 1. Penalized likelihood estimation of directed graphs.</p><p>(1) Given the ordering O, order the columns of observation matrix X in increasing order.</p><p>(2) For i = 2, 3, . . . , p, 2.1. Denote y = X i , X = X i -. Given the weight matrix W , let w = W i,i -, and solve</p><formula xml:id="formula_19">Âi,i -= arg min ⎧ ⎨ ⎩ n -1 y -X θ 2 2 + λ i i-1 j=1 |θ j |w j ⎫ ⎬ ⎭ 3•3.</formula><p>Analysis of computational complexity As mentioned in § 1, the space of all possible directed graphs is super-exponential in the number of nodes and hence it is not surprising that the PC-algorithm, without any restriction on the space, has exponential complexity. <ref type="bibr" target="#b6">Kalisch &amp; Bühlmann (2007)</ref> recently proposed an efficient implementation of the PC-algorithm for sparse graphs; its complexity when the maximal neighbourhood size q is small, is bounded with high probability by O( p q ). Although this is a considerable improvement over the original algorithm, in many applications it can become fairly expensive. For instance, gene regulatory networks and signaling pathways include many hub genes, which lead to large values for q.</p><p>The reformulation of the directed graph estimation problem in (9) requires solving p -1 lasso regression problems. The cost of solving a lasso problem comprised of k covariates and n observations using the pathwise coordinate optimization algorithm is O(nk); hence, the total cost of estimating the adjacency matrix of the graph is O(np 2 ), which is the same as the cost of calculating the empirical covariance matrix S. Moreover, the formulation in (9) includes a set of non-overlapping subproblems. Therefore, for problems with very large number of nodes and/or observations, the performance of the algorithm can be further improved by parallelizing the estimation of these subproblems. The adaptive lasso version of the problem is similarly solved using the modification of the regular lasso problem proposed in <ref type="bibr" target="#b25">Zou (2006)</ref>, which results in the same computational cost as the regular lasso problem.</p><p>To evaluate the performance of these algorithms, we compared the average CPU time, over 10 simulation runs, for estimation of directed graphs with different numbers of nodes, p = 100, 1000, and different sample sizes, n = 100, 1000. The computational time for the PCalgorithm increases with larger values of the average neighbourhood size and the significance level α. Therefore, to control the computational complexity of the PC-algorithm, these parameters are set to 5 and 0•01, respectively. In addition, in order to compare equivalent quantities, we only consider the CPU time that the PC-algorithm requires for estimation of the graph skeleton. Based on this simulation study, while for p = 100, the computation time for the PC-algorithm is comparable to the time for the penalized likelihood algorithm, in a graph with p = 1000 and n = 1000, the average CPU time for the PC-algorithm could be up to two orders of magnitude larger than the equivalent time for Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ASYMPTOTIC PROPERTIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4•1. Preliminaries</head><p>Next, we establish theoretical properties of the lasso and adaptive lasso estimates of the adjacency matrix of directed graphs. Asymptotic properties of the lasso-type estimates with fixed design matrices have been studied by a number of researchers <ref type="bibr" target="#b8">(Knight &amp; Fu, 2000;</ref><ref type="bibr" target="#b25">Zou, 2006;</ref><ref type="bibr" target="#b5">Huang et al., 2008)</ref>, while random design matrices have been considered by <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref>. <ref type="bibr" target="#b17">Rothman et al. (2008)</ref> and <ref type="bibr" target="#b9">Lam &amp; Fan (2009)</ref>, among others, have studied asymptotic properties of estimates of covariance and precision matrices.</p><p>As discussed in § 3•2, the problem of estimating the adjacency matrix of a directed graph is equivalent to solving the p -1 non-overlapping penalized least-squares problems described in (9). In order to study the asymptotic properties of the proposed estimators, we focus on the asymptotic consistency of network estimation, i.e. the probability of correctly estimating the network structure, in terms of Type I and Type II errors. We allow the total number of nodes in the graph to grow as an arbitrary polynomial function of the sample size, while assuming that the true underlying network is sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4•2. Assumptions</head><p>Let X = (X 1 , . . . , X p ) be a collection of p zero-mean Gaussian random variables with covariance matrix , and let X and S be defined as in § 3•1. To simplify the notation, denote by θ i = A i,ithe entries of the ith row of A to the left of the diagonal. Further, let θ i,I be the estimate for the ith row, with values outside the set of indices I set to zero; i.e. θ i,I ≡ A i,iand A i, j = 0, j / ∈ I.</p><p>The following assumptions are used in establishing the consistency of network estimation.</p><p>Assumption 1. For some a &gt; 0, p = p(n) = O(n a ) as n → ∞, and there exists a 0 b &lt; 1 such that max i∈V card (pa i ) = O(n b ) as n → ∞.</p><p>Assumption 2. There exists ν &gt; 0 such that for all n ∈ N and all i ∈ V , var (X i | X i -) ν.</p><p>Assumption 3. There exists δ &gt; 0 and some ξ &gt; b, with b defined above, such that for all i ∈ V and for every j ∈ pa i , |π i j | δn -(1-ξ )/2 , where π i j is the partial correlation between X i and X j after removing the effect of the remaining variables.</p><p>Assumption 4. There exists &lt; ∞ such that for all n ∈ N and every i ∈ V and j ∈ pa i , θ j,pa i 2 .</p><p>Assumption 5. There exists κ &lt; 1 such that for all i ∈ V and j / ∈ pa i ,</p><formula xml:id="formula_20">| k∈pa i sign (θ i,pa i k )θ j,pa i k | &lt; κ.</formula><p>Assumption 1 determines the permissible rates of increase in the number of variables and the neighbourhood size, as a function of n, Assumption 2 prevents singular or near-singular covariance matrices, and Assumption 3 guarantees that true partial correlations are bounded away from zero.</p><p>Assumption 4 limits the magnitude of the shared ancestral effect between each node in the network and any of its parents. This is less restrictive than the equivalent assumption for the neighbourhood selection problem, where the effects over all neighbouring nodes are assumed to be bounded. In fact, in the case of gene regulatory networks, empirical data indicate that the average number of upstream-regulators per gene is less than two <ref type="bibr" target="#b11">(Leclerc, 2008)</ref>. Thus, the number of parents of each node is small, while each hub node can affect many downstream nodes.</p><p>Assumption 5 is referred to as neighbourhood stability and is equivalent to the irrepresentability assumption of <ref type="bibr" target="#b5">Huang et al. (2008)</ref>. It has been shown that the lasso estimates are not in general variable selection consistent if this assumption is violated. <ref type="bibr" target="#b5">Huang et al. (2008)</ref> considered the adaptive lasso estimates with general initial weights and proved their variable selection consistency under a weaker form of irrepresentability assumption, referred to as adaptive irrepresentability. We will show that when the initial weights for the adaptive lasso are derived from the regular lasso estimates as in (5), the assumption of neighbourhood stability, and the less stringent Assumption 4 are not required for establishing variable selection consistency of the adaptive lasso. This relaxation in assumptions required for variable selection consistency, is a result of the consistency of the regular lasso estimates, and the special structure of directed graphs. Similar results can be obtained for the adaptive lasso estimates of the precision matrix, as well as regression models with fixed and random design matrices, under additional mild assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4•3. Asymptotic consistency of directed graph estimation</head><p>Our first result studies the variable selection consistency of the lasso estimate. In this theorem, (i) corresponds to sign consistency, (ii) and (iii) establish control of Type I and Type II errors and (iv) addresses the consistency of network estimation. We denote by Ê the estimate of the edge set of the graph, and by x ∼ y the asymptotic equivalence between x and y.</p><p>THEOREM 1. Suppose that Assumption 1-5 hold and λ ∼ dn -(1-ζ )/2 for some b &lt; ζ &lt; ξ and d &gt; 0. Then for the lasso estimate there exist constants c (i) , . . . , c (iv) &gt; 0 such that for all i ∈ V , as n → ∞,</p><formula xml:id="formula_21">(i) pr{sign ( θi,pa i j ) = sign (θ i,pa i j ) for all j ∈ pa i } = 1 -O{exp (-c (i) n ζ )}, (ii) pr( pa i ⊆ pa i ) = 1 -O{exp (-c (ii) n ζ )}, (iii) pr(pa i ⊆ pa i ) = 1 -O{exp (-c (iii) n ζ )} and (iv) pr( Ê = E) = 1 -O{exp (-c (iv) n ζ )}.</formula><p>Proof . The proof of this theorem follows from arguments similar to those presented in <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref> with minor modifications and replacing conditional independence for undirected graphs with d-separation for directed graphs, and hence is omitted.</p><p>The next result establishes similar properties for the adaptive lasso estimates, without the assumptions of neighbourhood stability. The proof of Theorem 3 makes use of the consistency of sparse estimates of the Cholesky factor of covariance matrices, established in Theorem 9 of <ref type="bibr" target="#b9">Lam &amp; Fan (2009)</ref>. For completeness, we restate a simplified version of the theorem for our lasso problem, for which σ i = 1 (i = 1, . . . p), and the eigenvalues of the covariance matrix are bounded; see Remark 1. Throughout this section, we denote by s the total number of nonzero elements of the true adjacency matrix A.</p><p>THEOREM 2 <ref type="bibr" target="#b9">(Lam &amp; Fan 2009)</ref></p><formula xml:id="formula_22">. If n -1 (s + 1) log p = o(1) and λ = O{(log p/n) 1/2 }, then Â -A F = O p {(n -1 s log p) 1/2 }.</formula><p>It can be seen from Theorem 2 that the lasso estimates are consistent as long as n -1 (s + 1) log p = o(1). To take advantage of this result, we replace Assumption 1 with the following assumption.</p><formula xml:id="formula_23">Assumption 1 . For some a &gt; 0, p = p(n) = O(n a ) as n → ∞. Also, max i∈V card (pa i ) = O(n b ) as n → ∞, where sn 2b-1 log n = o(1) as n → ∞.</formula><p>Assumption 1 further restricts the number of parents of each node and also enforces a restriction on the total number of nonzero elements of the adjacency matrix. Condition sn 2b-1 log n = o(1) implies that b &lt; 1/2. Therefore, although the consistency of the adaptive lasso in Theorem 3 is established without making any further assumptions on the structure of the network, it is achieved at the price of requiring a higher degree of sparsity in the network. THEOREM 3. Consider the adaptive lasso estimation problem, where the initial weights are calculated using the regular lasso estimates of the adjacency matrix of the graph in (9). Suppose Assumptions 1 , 2 and 3 hold and λ ∼ dn -(1-ζ )/2 for some b &lt; ζ &lt; ξ and d &gt; 0. Also suppose that the initial lasso estimates are found using a penalty parameter λ 0 that satisfies λ 0 = O{(log p/n) 1/2 }. Then there exist constants c (i) , . . . , c (iv) &gt; 0 such that for all i ∈ V , as n → ∞, (i)-(iv) in Theorem 1 hold.</p><p>Proof . See the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4•4. Choice of the tuning parameter</head><p>Both lasso and adaptive lasso estimates of the adjacency matrix depend on the choice of the tuning parameter λ. Different methods have been proposed for selecting the value of the tuning parameter, including crossvalidation <ref type="bibr" target="#b17">(Rothman et al., 2008)</ref> and the Bayesian information criterion <ref type="bibr" target="#b24">(Yuan &amp; Lin, 2007)</ref>. However, choices of λ that result in the optimal classification error do not guarantee a small error for network reconstruction. We propose next a choice of λ for the general weighted lasso problem with weights w i j . Let Z * q denote the (1q)th quantile of standard normal distribution, and define</p><formula xml:id="formula_24">λ i (α) = 2n -1/2 Z * α/{2 p(i-1)} . (<label>10</label></formula><formula xml:id="formula_25">)</formula><p>The following result establishes that such a choice controls the probability of falsely joining two distinct ancestral sets, defined next.</p><p>DEFINITION 1. For every node i ∈ V , the ancestral set of node i, AN i , consists of all nodes j, such that j is an ancestor of i or i is an ancestor of j or i and j have a common ancestor k. THEOREM 4. Under the assumptions of Theorems 1 and 3 above, for the lasso and adaptive lasso, respectively, for all n ∈ N the solution of the general weighted lasso estimation problem with tuning parameter determined in (10) satisfies pr(there exists i ∈ V : ÂN i AN i ) α.</p><p>Proof . See the Appendix.</p><p>Theorem 4 is true for all values of p and n, but it does not provide any guarantee for the probability of false positive error for individual edges in the graph. We also need to determine the optimal choice of penalty parameter λ 0 for the first phase of the adaptive lasso, where the weights are estimated using the lasso. Since the goal of the first phase is to achieve prediction consistency, crossvalidation can be used to determine the optimal choice of λ 0 . On the other hand, it is easy to see that the error-based proposal in (10) satisfies the requirement of Theorem 2 and can therefore be used to define λ 0 . It is, however, recommended to use a higher value of significance level in estimating the initial weights, in order to prevent an oversparse solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5•1. Preliminaries</head><p>In this section, we consider examples of estimating directed graphs of a varying number of edges from randomly generated data. To randomly generate data, one needs to generate lower-triangular adjacency matrices with sparse nonzero elements, ρ i j . In order to control the computational complexity of the PC-algorithm, we use the random directed graph generator in the R-package pcalg <ref type="bibr" target="#b6">(Kalisch &amp; Bühlmann, 2007)</ref>, which generates graphs with given values of the average neighbourhood size. The sparsity levels of graphs with different sizes are set according to the theoretical bounds in § 4, as well as the recommendations of <ref type="bibr" target="#b6">Kalisch &amp; Bühlmann (2007)</ref> for the neighbourhood size. We use an average neighbourhood size of five, while limiting the total number of true edges to be equal to the sample size n.</p><p>Different measures of structural difference can be used to evaluate the performance of estimators. The structural Hamming distance represents the number of edges that are not in common between the estimated and true graphs, i.e. SHD = card ( Ê\E) + card (E\ Ê), where Ê and E are defined as in Theorem 1. The main drawback of this measure is its dependency on the number of nodes, as well as the sparsity of the network. The second measure of goodness of estimation considered here is the Matthew's correlation coefficient, which is commonly used to assess the performance of binary classification methods, and is defined as</p><formula xml:id="formula_26">MCC = (TP × TN) -(FP × FN) {(TP + FP)(TP + FN)(TN + FP)(TN + FN)} 1/2 , (<label>11</label></formula><formula xml:id="formula_27">)</formula><p>where TP, TN, FP and FN denote the total number of true positive, true negative, false positive and false negative edges, respectively. The value of Matthew's correlation coefficient ranges from -1 to 1 with larger values corresponding to better fits, and -1 and 1 representing worst and best fits, respectively. Finally, in order to compare the performance of different estimation methods with theoretical bounds established in § 4•3, we also report the values of true and false positive rates.</p><p>Graphical lasso PC-algorithm Lasso Adaptive lasso True graph p = 50 Fig. <ref type="figure">2</ref>. True directed graph along with estimates from Gaussian observations. The greyscale represents the percentage of inclusion of edges.</p><p>The performances of the PC-algorithm and our proposed estimators based on the choice of tuning parameter in ( <ref type="formula" target="#formula_24">10</ref>), vary with different values of significance level α. In the following experiments, we first investigate the appropriate choice of α for each estimator. We then compare the performance of the estimators with an optimal choice of α. The results reported in this section are based on estimates obtained from 100 replications; to offset the effect of numerical instability, we consider an edge present only if | Âi j | &gt; 10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5•2. Estimation of directed graphs from normally distributed observations</head><p>We begin with an example that illustrates the differences between estimation of directed graphs and conditional independence graphs. The first two images in Fig. <ref type="figure">2</ref> represent a randomly generated directed graph of size p = 50 and the greyscale image of the average precision matrix estimated based on a sample of size n = 100 using the graphical lasso algorithm <ref type="bibr" target="#b2">(Friedman et al., 2008)</ref>. The image is obtained by calculating the proportion of times that a specific edge is present in 100 replications. To control the probability of falsely connecting two components of the graph, the value of the tuning parameter for the graphical lasso is defined based on the error-based proposal in <ref type="bibr">Banerjee et al. (2008, Theorem 2)</ref>. It can be seen that the conditional independence graph has many more edges, 8% false positive rate compared to 1% for the lasso and adaptive lasso, and does not reveal the true structure of the underlying directed graph. Therefore, although methods of estimating conditional independence graphs are computationally efficient, they should not be used in applications, such as estimation of gene regulatory networks, where the underlying graph is directed.</p><p>In simulations throughout this section, the sample size is fixed at n = 100, and estimators are evaluated for an increasing number of nodes, p = 50, 100, 200. Figure <ref type="figure" target="#fig_2">3</ref> shows the mean and standard deviation of the Hamming distances, expressed in base 10 logarithmic scale, for estimates based on the PC-algorithm, as well as the proposed lasso, and adaptive lasso methods for different values of the tuning parameter α and different network sizes. It can be seen that for all values of p and α, the adaptive lasso estimate produces the best results, and the proposed penalized likelihood methods outperform the PC-algorithm. This difference becomes more significant as the size of the network increases.</p><p>As mentioned in § 2, it is not always possible to estimate the direction of the edges of a directed graph and therefore, the estimate from the PC-algorithm may include undirected edges. Since our penalized likelihood methods assume knowledge of the ordering of variables and estimate the structure of the network, in the simulations considered here, we only estimate the skeleton of the network using the PC-algorithm. We then use the ordering of the variables to determine the direction of the edges. The performance of the PC-algorithm for estimation of partially completed directed graphs may therefore be worse than the results reported here. In our simulation results, observations are generated according to the linear structural equation model ( <ref type="formula" target="#formula_3">2</ref>) with standard normal latent variables and ρ i j = ρ = 0•8. Additional simulation studies with different values of σ and ρ indicate that changes in σ do not have a considerable effect on the performance of the proposed models. As the magnitude of ρ decreases, the performance of the proposed methods, as well as the PC-algorithm deteriorates, but the findings remain unchanged.</p><p>The above simulation results suggest that the optimal performance of the PC-algorithm is achieved when α = 0•01. The performance of the lasso and adaptive lasso methods is less sensitive to the choice of α; however, a value of α = 0•10 seems to deliver more reliable estimates. Our extended simulations indicate that the performance of the adaptive lasso does not vary significantly with the value of power γ and therefore we present the results for γ = 1.</p><p>Figure <ref type="figure">2</ref> represents images of estimated and true directed graphs created based on the above considerations for tuning parameters for p = 50. Similar results were also observed for larger values of p, p = 100, 200, and are excluded due to space considerations. Plots in Fig. <ref type="figure">4</ref> compare the performance of the three methods with the optimal settings of tuning parameters, over a range of values of p. It can be seen that the values of Matthew's correlation coefficient confirm the above findings based on the Hamming distance. On the other hand, false positive and true positive rates only focus on one aspect of estimation at a time and do not provide a clear distinction between the methods.</p><p>The representation of conditional independence in directed graphs adapted in our algorithm is not restricted to normally distributed random variables; if the underlying structural equations are linear, the method proposed in this paper can correctly estimate the underlying graph. In order to assess the sensitivity of the estimates to the underlying distribution, we performed two simulation studies with nonnormal observations. In both simulations, observations were generated according to a linear structural model. In the first simulation, the latent variables were generated from a mixture of a standard normal and a t-distribution with three degrees of freedom, while in the second simulation, a t-distribution with four degrees of freedom was used. The performance of the proposed algorithm for nonnormal observations was similar to the case of Gaussian observations, with the adaptive lasso providing the best estimates, and the performance of penalized methods improving in sparse settings. 5•3. Sensitivity to perturbations in the ordering of the variables Algorithm 1 assumes a known ordering of the variables. The superior performance of the proposed penalized likelihood methods in comparison to the PC-algorithm may be explained by the fact that additional information about the order of the variables significantly simplifies the problem of estimating directed graphs. Therefore, when such additional information is available, estimates using the PC-algorithm suffer from a disadvantage. However, as the underlying network becomes more sparse, the network includes fewer complex structures and the ordering of variables should play a less significant role.</p><p>Next, we study the performances of the proposed methods, and the PC-algorithm in problems where the ordering of variables is unknown. To this end, we generate normally distributed observations from the latent variable model of § 2•2. We then randomly permute the order of variables in the observation matrix and use the permuted matrix to estimate the original directed graph. Figure <ref type="figure">5</ref> illustrates the performance of the three methods for choices of α described in § 5•2. It can be seen that for small, dense networks, the PC-algorithm outperforms the proposed methods. This is expected since the change in the order of variables causes the proposed algorithm to include unnecessary moral edges, while failing to recognize some of the existing associations. However, as the size of the network and correspondingly the degree of sparsity increase, the local structures become simpler and therefore the ordering of the variables becomes less crucial. Thus, the performance of penalized likelihood algorithms is improved compared to that of the PC-algorithm. For the high-dimensional sparse case, where the computational cost of the PC-algorithm becomes more significant, the penalized likelihood methods provide better estimates.  <ref type="bibr" target="#b2">Friedman et al. (2008)</ref> analyzed this dataset using the graphical lasso algorithm. They estimated the graph for a range of values of the 1 penalty and reported moderate agreement, around 50% false positive and false negative rates, between one of their estimates and the findings of <ref type="bibr" target="#b18">Sachs et al. (2003)</ref>. True and estimated signaling networks using the PC-algorithm, with α = 0•01, and the lasso and adaptive lasso algorithms, with α = 0•1, along with the corresponding performance measures, are given in Fig. <ref type="figure" target="#fig_4">6</ref>. The estimated network using the PC-algorithm includes a number of undirected edges. As in the simulation studies, we only estimate the structure of the network using the PC-algorithm and determine the direction of edges by enforcing the ordering of nodes. It can be seen that the adaptive lasso and lasso provide estimates that are closer to the true structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6•2. Transcription regulatory network of E. coli</head><p>Transcriptional regulatory networks play an important role in controlling the gene expression in cells and incorporating the underlying regulatory network results in more efficient estimation and inference <ref type="bibr" target="#b19">(Shojaie &amp; Michailidis, 2009</ref><ref type="bibr" target="#b25">, 2010)</ref>. <ref type="bibr" target="#b7">Kao et al. (2004)</ref> proposed the network component analysis method to infer the transcriptional regulatory network of Escherichia coli, E. coli. They also provided whole genome expression data over time, with n = 24, as well as information about the known regulatory network of E. coli.</p><p>In this application, the set of transcription factors are known a priori and the goal is to find connections among transcription factors and regulated genes through analysis of whole genome transcriptomic data. The algorithm proposed in this paper can be used by exploiting the natural hierarchy of transcription factors and regulated genes. <ref type="bibr" target="#b7">Kao et al. (2004)</ref> provide gene expression data for seven transcription factors and 40 regulated genes, i.e. p = 47. Figure <ref type="figure">7</ref> presents the known regulatory network of E. coli along with the estimated networks and the corresponding performance measures using the PC-algorithm, lasso and adaptive lasso. The values of α are set as in § 6•1. The relatively poor performance of the algorithms in this example can be partially attributed to the small sample size. However, it is also known that no single source of where Z j is independent of {X k ; k ∈ pa i \{ j}}. Then by (A3),</p><formula xml:id="formula_28">G j { θi (β)} = -2n -1 Z T j R i (β) - k∈pa i \{ j} θ j,pa i \{ j} 2n -1 X T k R i (β).</formula><p>By Lemma A1, it follows that for all k ∈ pa</p><formula xml:id="formula_29">i \{ j}, |G k { θi (β)}| = |2n -1 X T k R i (β)| λw ik , and hence, G j { θi (β)} -2n -1 Z T j R i (β) + λ k∈pa i \{ j} |θ j,pa i \{ j} |w ik .</formula><p>Using the fact that |θ j,pa i \{ j} | 1, it suffices to show that pr</p><formula xml:id="formula_30">⎡ ⎣ sup β 0 -2n -1 Z T j R i (β) &lt; -λ k∈pa i w ik ⎤ ⎦ = 1 -O exp -c (i) n ζ , n → ∞.</formula><p>It is shown in Lemma A.2. of <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref> that for any q &gt; 0, there exists c (i) &gt; 0 such that for all j ∈ pa i with θ i j &gt; 0, pr inf</p><formula xml:id="formula_31">β 0 2n -1 Z T j R i (β) &gt; qλ = 1 -O exp -c (i) n ζ , n → ∞. ( A 4 )</formula><p>However, by definition w ik 1 and therefore, k∈pa i w ik card (pa i ) 1, and (i) follows from (A4).</p><p>To prove (ii), note that the event pa i pa i is equivalent to the event that there exists a node j ∈ i -\pa i such that θi j 0. In other words, denoting the latter event by D, pr( pa i ⊆ pa i ) = 1pr(D). However, by Lemma A1, and since w i j 1, pr(D) = pr(there exists j ∈ i -\pa i such that |G j ( θi,pa i )| w i j λ) pr{there exists j ∈ i -\pa i such that |G j ( θi,pa i )| qλ and w i j λ qλ for some q 1} pr(there exists j ∈ i -\pa i such that w i j q for some q 1).</p><p>But w i j = 1 ∨ | θi j | -γ , with θi j the lasso estimate of the adjacency matrix from (9). Hence, letting F be the event that there exists j ∈ i -\pa i such that w i j q for some q &gt; 0, and using Lemma A1 we can write pr(F) = pr there exists j ∈ i -\pa i such that θi j q -1/γ for some q 1 pr there exists j ∈ i -\pa i such that θi j q for some q &gt; 0 pr there exists j ∈ i -\pa i such that θi j 0 = pr(there exists j ∈ i -\pa i such that |G j ( θi,pa i )| λ 0 ).</p><p>Since card (pa i ) = o(n), we can assume, without loss of generality, that card (pa i ) &lt; n, which implies that θi,pa i is an almost sure unique solution to (A1) with I = pa i . Let E = {max j∈i -\pa i |G j ( θi,pa i )| &lt; λ 0 }. Then conditional on the event E, it follows from the first part of Lemma A1 that θi,pa i is also a solution of the unrestricted weighted lasso problem (A1) with I = i -. Since θi,pa i j = 0, for all j ∈ i -\pa i , it follows from the second part of Lemma A1 that θi j = 0, for all j ∈ i -\pa i . Hence, pr there exists j ∈ i -\pa i such that θi j 0 1pr(E) = pr max j∈i -\pa i |G j ( θi,pa i )| λ 0 , where G j ( θi,pa i ) = -2n -1 X T j (X i -X θi,pa i ). Since card (V ) = O(n a ) for some a &gt; 0, Bonferroni's inequality implies that to verify (ii) it suffices to show that there exists a constant c (ii) &gt; 0 such that for all j ∈ i -\pa i , pr(|G j ( θi,pa i )| λ 0 ) = O exp -c (ii) n ζ , n → ∞, where R j ∼ N (0, σ 2 j ), σ 2 j 1 and R j is independent of X l , l ∈ pa i . Similarly, with R i satisfying the same requirements as R j , we get X i = k∈pa i θ i,pa i k X k + R i . Denote by X pa i the columns of X corresponding to pa i and let θ pa i be the column vector of coefficients with dimension card (pa i ) corresponding to pa i . Then, pr{|G j θi,pa i )| λ 0 } = pr -2n -1 X T j (X i -X θi,pa i ) λ 0 = pr -2n -1 X pa i θ j,pa i pa i + R j T X pa i θ i,pa i pa i -θi,pa i pa i + R i λ 0 pr -2n -1 θ i,pa i pa i -θi,pa i pa i T X T pa i X pa i θ j,pa i pa i λ 0 /3 + pr -2n -1 θ i,pa i pa i -θi,pa i pa i ) T X T pa i R j λ 0 /3 + pr -2n -1 X pa i θ j,pa i pa i + R j T R i λ 0 /3 ≡ I + II + III, say. Let 1 pa i denote a vector of 1s of dimension card (pa i ). Then using the fact that |θ j,pa i l | 1, for all l ∈ pa i , we can write I pr{2 θ i,pa i pa i -θi,pa i pa i</p><p>∞ n -1 (X pa i 1 pa i ) T X pa i 1 pa i λ 0 /3}. Then X T pa i X pa i ∼ W card (pa i ) ( pa i , n) where W m ( , n) denotes a Wishart distribution with mean n . Hence, from properties of the Wishart distribution, we get (X pa i 1 pa i ) T X pa i 1 pa i ∼ W 1 (1 T pa i pa i 1 pa i , n). Since pa i also forms a directed acyclic graph, the eigenvalues pa i are bounded, see Remark 1, and hence 1 T pa i pa i 1 pa i card (pa i )φ max pa i . ( A 5 )</p><p>Therefore, if Z ∼ χ 2 1 , then n -1 (X pa i 1 pa i ) T X pa i 1 pa i is stochastically smaller than card (pa i )φ max ( pa i )Z . On the other hand, by Theorem 2, A -Ã F = O p {(n -1 s log p)</p><p>1/2 }, and hence, By Assumption 1 , sn 2b-1 log n = o(1) and hence by Slutsky's Theorem and properties of the χ 2distribution, there exists c (I) &gt; 0 such that for all j ∈ i -\pa i , I = O{exp (-c (I) n ζ )} as n → ∞. Using a similar argument, II pr(2n -1 θ i,pa i pa i -θi,pa i pa i ∞ |1 pa i X T pa i R j | λ 0 /3). But columns of X pa i have mean zero and are all independent of R j , so it suffices to show that there exists c (II) &gt; 0 such that for all j ∈ i -\pa i and for all k ∈ pa i , pr 2n -1 θ i,pa i pa i -θi,pa i</p><formula xml:id="formula_32">θ i,pa i pa i -θi,pa i pa i ∞ = O p (n -1 s log p) 1/2 . (<label>A</label></formula><formula xml:id="formula_33">pa i ∞ card (pa i ) X T k R j λ 0 /3 = O exp -c (II) n ζ as n → ∞. (A7)</formula><p>By (A6) and Assumption 1 , the random variable on the left-hand side of (A7) is stochastically smaller than 2n -1 |X k R j |. By independence of X k and R j , E(X k R j ) = 0. Also, using Gaussianity of both X k and R j , there exists g &lt; ∞ such that E{exp (|X k R j |)} g. Since λ 0 = O{(log p/n) 1/2 }, by Bernstein's inequality <ref type="bibr" target="#b23">(Van der Vaart &amp; Wellner, 1996)</ref>, pr(2n -1 |X k R j | &gt; λ 0 /3) exp(-c (II) n ζ ) for some c (II) &gt; 0 and hence (A7) is satisfied. Finally, pr{| -2n -1 (X pa i θ j,pa i pa i + R i ) T R j | λ 0 /3} = pr{| -2n -1 X T i R j | λ 0 /3}, and using the Bernstein's inequality we conclude that there exists c (III) &gt; 0 such that for all j ∈ i -\pa i and for all k ∈ pa i , III = O{exp (-c (III) n ζ )} as n → ∞. The proof of (ii) is then completed by taking c (ii) to be the minimum of c (I) , . . . , c (III) .</p><p>To prove (iii), note that pr(pa i ⊆ pa i ) = 1pr(there exists j ∈ pa i such that θi j = 0), and let E = {max k∈i -\pa i |G j ( θi,pa i )| &lt; λw i j }. It then follows from an argument similar to the proof of (ii) that conditional on E, θi,pa i is an almost sure unique solution of the unrestricted adaptive lasso problem (A1) with</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Directed graphs. (a) A simple directed graph; and (b), an illustration of observational equivalence in directed graphs.</figDesc><graphic coords="3,291.29,75.34,83.17,53.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A∈A[</head><label></label><figDesc>tr {(I -A) T (I -A)S} + λJ (A)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Logarithm, in base 10, of the Hamming distances for estimation of directed graphs using the PC-algorithm (black solid), lasso (black dashes) and adaptive lasso (grey dot-dashes) from normal observations.</figDesc><graphic coords="12,69.89,243.22,394.70,109.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5. Matthew's correlation coefficient, true and false positives for estimation of directed graphs using the PC-algorithm (black solid), lasso (black dashes) and adaptive lasso (grey dot-dashes) with random ordering.</figDesc><graphic coords="13,88.49,71.26,394.34,111.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Known and estimated networks for human cell signalling data. True and false edges are marked with solid and dashed arrows, respectively. MCC, Matthew's correlation coefficient; FN, False negative; FP, False positive; SHD, structural Hamming distance.</figDesc><graphic coords="14,53.09,89.14,417.74,95.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>6 ) Noting that card (pa i ) = O(n b ), b &lt; 1/2 and p = O(n a ), a &gt; 0, (A5) and (A6) imply thatθ i,pa i pa i -θi,pa i pa i ∞ card (pa i )φ max ( pa i ) = O p (sn 2b-1 a log n) 1/2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,70.85,80.98,417.74,194.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,70.73,309.94,417.74,194.90" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>transcriptomic data is expected to successfully reveal the regulatory network and better estimates are obtained by combining different sources of data. It can be seen that the PC-algorithm can only detect one of the true regulatory connections, and both the lasso and adaptive lasso offer significant improvements, mostly due to the considerable drop in the false negative rate, from 97% for the PC-algorithm to 63% for the adaptive lasso. In this case, the lasso and adaptive lasso estimates are very similar, and the choice of the best estimate depends on the performance evaluation criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>Thanks go to the authors of <ref type="bibr" target="#b6">Kalisch &amp; Bühlmann (2007)</ref> and <ref type="bibr" target="#b1">Friedman et al. (2007)</ref> for making the R-packages pcalg and glmnet available. We are especially thankful to Markus Kalisch and ALI SHOJAIE AND GEORGE MICHAILIDIS Trevor Hastie for help with technical difficulties with these packages. We would like to thank the editor, an associate editor, and a referee, whose comments and suggestions significantly improved the clarity of the manuscript. This research was partially funded by grants from the National Institutes of Health, U.S.A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical proofs</head><p>The following lemma is a consequence of the Karush-Kuhn-Tucker conditions for the general weighted lasso problem and is used in the proof of Theorems 3 and 4. LEMMA A1. Let θi,I be the general weighted lasso estimate of θ i,I , i.e.</p><p>θi,I = arg min</p><p>and let w i be the vector of initial weights in the adaptive lasso estimation problem. Then a vector θ with θk = 0, for all k / ∈ I is a solution of (A1) if and only if for all j ∈ I, G j (θ ) =sign ( θ j )w i j λ if θ j 0 and |G j (θ )| w i j λ if θ j = 0. Moreover, if the solution is not unique and |G j (θ )| &lt; w i j λ for some solution θ , then θ j = 0 for all solutions of (A1).</p><p>Proof . The proof of the lemma is identical to the proof of Lemma (A1) in <ref type="bibr" target="#b13">Meinshausen &amp; Bühlmann (2006)</ref>, except for inclusion of general weights w i j , and is therefore omitted.</p><p>Proof of Theorem 3. To prove (i), note that by Bonferroni's inequality, and the fact that card (pa i ) = o(n) as n → ∞, it suffices to show that there exists some c (i) &gt; 0 such that for all i ∈ V and for every j ∈ pa i , pr{sign ( θi,pa i j</p><p>Let θi,pa i (β) be the estimate of θ i,pa i in (A1), with the jth component fixed at a constant value β,</p><p>where</p><p>, θi,pa i (β) is identical to θi,pa i . Thus, if sign ( θi,pa i j</p><p>) sign (θ i j ), there would exist some β with sign (β) sign (θ i j ) 0 such that θi,pa i (β) is a solution to (A2). Since θ i j 0, for all j ∈ pa i , it suffices to show that for all β with sign (β) sign (θ i j ) &lt; 0, with high probability, θi,pa i (β) cannot be a solution to (A2).</p><p>Without loss of generality, we consider the case where θ i j &gt; 0; θ i j &lt; 0 can be shown similarly. Then if β 0, from Lemma A1, θi,pa i (β) can be a solution to (A2) only if G j { θi (β)} -λw i j . Hence, it suffices to show that for some c (i) &gt; 0 and all j ∈ pa i with θ i j &gt; 0, pr sup</p><p>Then for every j ∈ pa i we can write</p><p>)</p><p>pr there exists j ∈ pa i such that θi j = 0 pr there exists j ∈ pa i such that θi j = 0 + pr(E c ).</p><p>From (i), there exists a c 1 &gt; 0 such that pr(there exists j ∈ pa i such that θi j = 0) = O{exp (-c 1 n ζ )} and it was shown in (ii) that pr(E c ) = O{exp (-c 2 n ζ )} for some c 2 &gt; 0. Thus (iii) follows from Bonferroni's inequality.</p><p>The claim in (iv) follows from (ii) and (iii), and Bonferroni's inequality as p = O(n a ).</p><p>Proof of Theorem 4. We first show that if AN i ∩ AN j = ∅, then i and j are independent. Since = T and is lower triangular,</p><p>We assume without loss of generality that i &lt; j. The argument for j &gt; i is similar. Suppose for all k = 1, . . . , i, that ik = 0 or jk = 0. Then by (A8) i and j are independent. However, by Lemma 1, jk is the influence of kth node on j, and this is zero only if there is no path from k to j. If i is an ancestor of j, we have i j 0. On the other hand, if there is no node k ∈ isuch that k influences both i and j, i.e. k is a common ancestor of i and j, then for all k = 1, . . . , i we have ik jk = 0 and the claim follows. Using Bonferroni's inequality twice and Lemma A1, we get pr(there exists i ∈ V such that ÂN i AN i ) p max i∈V pr(there exists j ∈ i -\AN i such that j ∈ pa i )</p><p>However, by definition w i j 1, and hence it suffices to show that (i -1) p max i∈V, j∈i -\AN i pr{|G j ( θi,AN i )| λ} α.</p><p>Note that G j ( θi,AN i ) = -2n -1 X T j (X i -X θi,AN i ) and X j is independent of X k for all k ∈ AN i . Therefore, conditional on X AN i , G j ( θi,AN i ) ∼ (0, 4R 2 /n), where R 2 = n -1 X i -X θi,AN i 2 2 n -1 X i 2 2 = 1, by definition of θi,AN i and the fact that columns of the data matrix are scaled.</p><p>It follows that for all j ∈ i -\AN i , pr{|G j ( θi,AN i )| λ | X AN i } 2{1 -(n 1/2 λ/2)}, where is the cumulative distribution function for standard normal random variable. Using the choice of λ proposed in (10), we get pr{|G j ( θi,AN i )| λ | X AN i } α{(i -1) p} -1 , and the result follows.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'aspremont</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="516" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathwise coordinate optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: the combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Covariance matrix selection and estimation via penalised normal likelihood</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pourahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive Lasso for sparse high-dimensional regression models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sinica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1603" to="1618" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PC-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transcriptome-based determination of multiple transcription regulator activities in Escherichia coli by using network component analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boscolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sabatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="641" to="646" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asymptotics for lasso-type estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1356" to="1378" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparsity and rate of convergence in large covariance matrix estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4254" to="4278" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survival of the sparsest: robust gene networks are parsimonious</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molec. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse estimation of large covariance matrices via a nested Lasso penalty</title>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="245" to="263" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the Lasso</title>
		<author>
			<persName><forename type="first">F</forename><surname>Markowetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Spang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006">2007. 2006</date>
		</imprint>
	</monogr>
	<note>Ann. Statist.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Core</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://www.R-project.org" />
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Counting unlabeled acyclic digraphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorial Mathematics V: Proc. Fifth Australian Conf., R. Melbourne Inst</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H C</forename><surname>Little</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="28" to="43" />
		</imprint>
	</monogr>
	<note type="report_type">Technol</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse permutation invariant covariance estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Statist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="494" to="515" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="504" to="506" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis of gene sets based on the underlying regulatory network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="407" to="426" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network enrichment analysis in complex experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Appl. Genet. Molec. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wellner</surname></persName>
		</author>
		<title level="m">Weak Convergence and Empirical Processes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model selection and estimation in the Gaussian graphical model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The adaptive lasso and its oracle properties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006-07">2006. July 2009. March 2010</date>
		</imprint>
	</monogr>
	<note>Received</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
