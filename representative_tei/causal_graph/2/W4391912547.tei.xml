<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Is a Causal Graph?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-24">24 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philip</forename><surname>Dawid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Is a Causal Graph?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-24">24 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.09429v1[math.ST]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>directed acyclic graph</term>
					<term>extended conditional independence</term>
					<term>augmented DAG</term>
					<term>moralisation</term>
					<term>structural causal model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article surveys the variety of ways in which a directed acyclic graph (DAG) can be used to represent a problem of probabilistic causality. For each of these we describe the relevant formal or informal semantics governing that representation. It is suggested that the cleanest such representation is that embodied in an augmented DAG, which contains nodes for non-stochastic intervention indicators in addition to the usual nodes for domain variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphical representations of probabilistic <ref type="bibr" target="#b0">(Pearl, 1988</ref><ref type="bibr" target="#b1">, Lauritzen, 1996</ref><ref type="bibr" target="#b2">, Cowell et al., 1999)</ref> and causal <ref type="bibr" target="#b3">(Pearl, 2009)</ref> problems are ubiquitous. Such a graph has nodes representing relevant variables in the system, which we term domain variables, and arcs between some of the nodes. The most commonly used type of graph for these purposes, to which we will confine attention in this article 1 , is a Directed Acyclic Graph (DAG), in which the arcs are arrows, and it is not possible to return to one's starting point by following the arrows. An illustration of such a DAG <ref type="bibr" target="#b4">(Dawid and Evett, 1997</ref>) is given in Figure <ref type="figure" target="#fig_0">1</ref>. Now there is no necessary relationship between a geometric object, such as a graph, and a probabilistic or causal model-they inhabit totally different mathematical universes. Any such relationship must therefore be specified externally, and then constitutes a way of interpreting the graph as saying something about the problem at hand. It is important to distinguish between the syntax of a graph, i.e., its internal, purely geometric properties, and its semantics, describing its intended interpretation.</p><p>In this article we consider a variety of ways-some formal, some less so-in which DAGs have been and can be interpreted. In particular, we emphasise the importance of a clear understanding of what is the intended interpretation in any specific case. We caution against the temptation to interpret purely syntactical elements, such as arrows, semantically (the sin of "reification"), or to slide unthinkingly from one interpretation of the graph to another.</p><p>The material in this article is largely a survey of previously published material <ref type="bibr" target="#b5">(Dawid, 2002</ref><ref type="bibr" target="#b6">, 2003</ref><ref type="bibr">, 2010a</ref><ref type="bibr">,b, 2015</ref><ref type="bibr">, 2021</ref><ref type="bibr" target="#b15">, 2022</ref><ref type="bibr" target="#b9">, Dawid and Didelez, 2010</ref><ref type="bibr" target="#b10">, Geneletti and Dawid, 2011</ref><ref type="bibr">, Dawid et al., 2017</ref><ref type="bibr">, Dawid and Musio, 2022</ref><ref type="bibr">, Constantinou and Dawid, 2017</ref><ref type="bibr">, Dawid et al., 2022)</ref>, which should be consulted for additional detail and discussion. In particular we do not here showcase the wealth of important applications of the methods discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outline</head><p>In § 2 we describe how a DAG can be used to represent properties of probabilistic independence between variables, using a precise semantics based on the method of moralisation. An alternative representation, involving functional rather than probabilistic dependencies, is also presented. Section 3 discusses some informal and imprecise ways in which a DAG might be considered as representing causal relations. In preparation for a more formal approach, § 4 describes an understanding of causality as a reaction to an external intervention in a system, and presents an associated language, based on an extension of the calculus of conditional independence, for expressing and manipulating causal concepts. Turning back to a DAG model, in § 5 we introduce a precise semantics for its causal interpretation, again based on moralisation, this time used to express extended condtional independence properties. To this end we introduce augmented DAGs, which contain nodes for non-stochastic intervention indicators, as well as for stochastic domain variables. In § 6 we describe the causal semantics of a "Pearlian" <ref type="bibr" target="#b3">(Pearl, 2009</ref>) DAG, and show how these can be explicitly represented by an augmented DAG, where again a version involving functional relationships-the Structural Causal Model (SCM) -is available.</p><p>Section 7 considers a different type of causal problem, that is not about the response of a system to an intervention, but rather aims to assign, to an observed putative cause, responsibility for the emergence of an observed outcome. This requires new understandings and semantics that can not be represented by a probabilistic DAG, but can be by using a structural causal model. However this is problematic, since there can be distinct SCMs that are observationally equivalent but lead to different answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic DAG</head><p>The most basic way of interpreting a DAG is as representing qualitative probabilistic relations of conditional independence between its variables. Such a situation occupies the lowest rung of the "ladder of causaton" <ref type="bibr" target="#b18">(Pearl and Mackenzie, 2018)</ref>. The semantics governing such a representation, while precise, are not totally straightforward, being described by either of two logically equivalent criteria, known as "d-separation" <ref type="bibr">(Pearl, 1986, Verma and</ref><ref type="bibr" target="#b20">Pearl, 1990)</ref> and "moralisation" <ref type="bibr" target="#b21">(Lauritzen et al., 1990</ref>). Here we describe moralisation, using the specific DAG D of Figure <ref type="figure" target="#fig_0">1</ref> for illustration.</p><p>Suppose we ask: Is the specific conditional independence property</p><formula xml:id="formula_0">(B, R) ⊥ ⊥ (G1, Y1) | (A, N )</formula><p>(read as "the pair (B, R) is independent of (G1, Y1), conditional on (A, N )"-see <ref type="bibr" target="#b22">Dawid (1979)</ref>) represented by the graph? To address this query we proceed as follows:</p><p>1. Ancestral graph We form a new DAG D ′ by removing from D every node that is not mentioned in the query and is not an ancestor of a mentioned node<ref type="foot" target="#foot_0">foot_0</ref> , as well as any arrow involving a removed node. Figure <ref type="figure">2</ref> shows the result of applying this operation to Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Moralisation</head><p>If in D ′ there are two nodes that have a common child (i.e., each has an arrow directed from it to the child node) but they are not themselves joined by an arrow-a configuration termed an "immorality"-then an undirected edge is inserted between them. Then every remaining arrowhead is removed, yielding an undirected graph G ′ . In our example this yields Figure <ref type="figure" target="#fig_1">3</ref>. 3. Separation Finally, in G ′ we look for a continuous path connecting an element of the first set in our query (here (B, R)) to an element of the second set (here (G1, Y1)) that does not contain an element of the third set (here (A, N )). If there is no such path, we conclude that the queried conditional independence property is represented in the original DAG. Since this is the case in our example, the property (B, R) ⊥ ⊥ (G1, Y1) | (A, N ) is indeed represented in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instrumental variable</head><p>The DAG of Figure <ref type="figure" target="#fig_2">4</ref> can be used to represent a problem in which we have an "instrumental variable", with the nodes interpreted as follows: Such a problem invites a causal interpretation, which we will take up in § 5 below. But if we interpret Figure <ref type="figure" target="#fig_2">4</ref> purely as a probabilistic DAG, we see that it represents exactly the following conditional independencies:</p><formula xml:id="formula_1">X Exposure Y Outcome Z Instrument U Unobserved confounding variables 3 Y X U Z Example: Instrumental Variable X: cause Y: effect Z: instrument U: unobserved</formula><formula xml:id="formula_2">U ⊥ ⊥ Z (1) Y ⊥ ⊥ Z | (U, X)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Markov equivalence</head><p>Given a collection of conditional independence relations for a set of variables, there may be 0, 1, or several DAGs that represent just these relations. Two DAGs are termed Markov equivalent when they represent the identical conditional independencies. It can be shown <ref type="bibr">(Frydenberg, 1990, Verma and</ref><ref type="bibr" target="#b24">Pearl, 1991</ref>) that this will be the case if they have the same skeleton (i.e., the undirected graph obtained by deleting the arrowheads) and the same immoralities.</p><p>Example 1 Consider the following DAGs on three nodes:</p><formula xml:id="formula_3">1. A → B → C 2. A ← B ← C 3. A ← B → C 4. A → B ← C.</formula><p>These all have the same skeleton. However, whereas DAGs 1, 2 and 3 have no immoralities, 4 has one immorality. Consequently, 1, 2 and 3 are all Markov equivalent, but 4 is not Markov equivalent to these. Indeed, 1, 2 and 3 all represent the conditional independence property A ⊥ ⊥ C | B, whereas 4 represents the marginal independence property A ⊥ ⊥ C. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bayesian network</head><p>The purely qualitative graphical structure of a probabilistic DAG can be elaborated with quantitative information. With each node in the DAG we associate a specified conditional distribution for its variable, given any values for its parent variables. There is a one-one correspondence between a collection of all such parent-child distributions and a joint distribution for all the variables that satisfies all the conditional independencies represented by the graph. The graphical structure also supports elegant algorithms for computing marginal and conditional distributions <ref type="bibr" target="#b2">(Cowell et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Structural probabilistic model</head><p>Suppose we are given the conditional distribution p(Y | X). It is then possible to construct (albeit non-uniquely) a fictitious "error variable" E, having a suitable distribution P , and a suitable deterministic function f of (X, E), such that the distribution of f (x, E) is just p(Y | X = x). For example, using the probability integral transform, if Fx is the cumulative distribution function of p(Y | X = x), we can take f (x, e) = F -1</p><p>x (e)<ref type="foot" target="#foot_2">foot_2</ref> and E uniform on [0, 1]. However such a representation is highly non-unique. Indeed, we could alternatively take f (x, e) = F -1</p><p>x (ex), where e is now a vector (ex), and the multivariate distribution of E is an arbitrarily dependent copula, such that each entry (Ex) is uniform on [0, 1].</p><p>Given any such construction, for purely distributional purposes we can work with the functional equation Y = f (X, E), with E ∼ P independently of X. This operation can be extended to apply to a complete Bayesian network, by associating with each domain variable V a new error variable EV , with suitable distribution Pv, all these being independent, and with V being modelled as a suitable determistic function of its graph parent domain variables and EV . We term such a deterministic model a Structural Probabilistic Model (SPM), by analogy with the Structural Causal Model (SCM) <ref type="bibr" target="#b18">(Pearl and Mackenzie, 2018)</ref>-see § 6.1 below. In an SPM, all stochasticity is confined to the error variables.</p><p>An SPM can be represented graphically by introducing new nodes for the error variables, as illustrated in Figure <ref type="figure">5</ref> for the case of Figure <ref type="figure" target="#fig_2">4</ref>, it being understood that the error variables are modelled as random, but all other parent-child relations are deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y X Z U</head><formula xml:id="formula_4">X: cause Y: effect Z: instrument U: unobserved E Z E X E U E Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5: SPM representation of instrumental variable model</head><p>It is easy to check that the conditional independencies represented between the domain variables, as revealed by the moralisation criterion, are identical, both in the original graph and in an SPM extension of it. So, when each graph is endowed with its own parent-child relationships (stochastic in the case of the original graph, deterministic for the structural graph), both graphs describe the identical joint distribution for the domain variables. For purely probabilistic purposes, nothing is gained by introducing error variables to "mop up" the stochastic dependencies of a simple DAG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Reification?</head><p>It is important to keep in mind that a probabilistic DAG is nothing but a very indirect way of representing a collection of conditional independence properties. In particular, the arrows in the DAG have no intrinsic meaning, being there only to support the moralisation procedure. It is indeed somewhat odd that the property of conditional independence, which is essentially a symmetric one, can be represented at all by means of arrows, which have built-in directionality. The example of Markov equivalence between 1, 2 and 3 in Example 1 shows that the specific direction of an arrow in a DAG should not be taken as meaningful itself. Rather, an arrow has a transient status rather like that of a construction line in an architect's plan, or of a contour line on a map: instrumentally helpful, but not representing anything visible in the house or on the ground.</p><p>Nevertheless, on looking at a DAG, it is hard to avoid the temptation to imbue its arrows with a direct meaning in relation to the system studied. This is the philosophical error of reification, which confuses the map with the territory <ref type="bibr" target="#b25">(Korzybski, 1933)</ref>, wrongly interpreting a purely instrumental property of a representation as if it had a direct counterpart in the external system. In the case of a probabilistic DAG model, this frequently leads to endowing it with an unjustified causal interpretation, where the presence of an arrow A → B is considered to represent the presence of a causal influence of A on B. Such a confusion underlies much of the enterprise of "causal discovery", where observational data are analysed to uncover their underlying probabilistic conditional indepencies, these are represented by a probabilistic DAG, and that DAG is then reinterpreted as representing causal relations. This is not to say that a DAG can not be interpreted causally-but to do so will require a fresh start, with new semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Informal causal semantics</head><p>Common causal interpretations of a DAG involve statements such as:</p><p>• an arrow represents a direct cause • a directed path represents a causal pathway Or, as described for example by <ref type="bibr" target="#b26">Hernán and Robins (2006)</ref>: "A causal DAG D is a DAG in which: 1. the lack of an arrow from Vj to Vm can be interpreted as the absence of a direct causal effect of Vj on Vm (relative to the other variables on the graph) 2. all common causes, even if unmeasured, of any pair of variables on the graph are themselves on the graph.</p><p>Thus Figure <ref type="figure" target="#fig_2">4</ref> might be interpreted as saying:</p><p>• U is a common cause of X and Y</p><p>• Z affects the outcome Y only through X</p><p>• Z does not share common causes with Y</p><p>• Z has a direct causal effect on X</p><p>In the above, we have marked syntactical terms, relating to the graph itself (the "map"), in bold-face, and terms involving external concepts (the "territory"), in teletype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic Causality</head><p>Or, one can start by ignoring the DAG, and try to relate causal concepts directly to conditional independence. Such an approach underlies the enterprise of causal discovery, where discovered conditional independencies are taken to signify causal relations. Common assumptions made here are:<ref type="foot" target="#foot_3">foot_3</ref> </p><p>Weak Causal Markov assumption: If X and Y have no common cause (including each other), they are probabilistically independent Causal Markov assumption: A variable is probabilistically independent of its non-effects, given its direct causes</p><p>Conbining such assumptions with the formal semantics by which a DAG represents conditional independence, we obtain a link between the DAG and causal concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A problem</head><p>How could we check if a DAG, endowed with a causal interpretation along the above lines, is a correct representation of the external problem it is intended to model? To even begin to do so, we would already need to have an independent understanding of the external concepts, as marked in bold-face-which thus can not make reference to the graph itself. But these informal causal concepts are woolly and hard to pin down. I am aware of almost no work that even attempts to do so-a recent exception is <ref type="bibr" target="#b27">Sadeghi and Soo (2023)</ref>. But without this, using such informal causal semantics risks generating confusion rather than clarification.</p><p>To avoid confusion, we need to develop a more formal causal semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interventional causality and extended conditional</head><p>Our approach to this begins by introducing an explicit, non-graphical, understanding of causality, expressed in terms of the probabilistic response of a system to an (actual or proposed) intervention. A causal effect of A on B exists if the distribution of B, after an external intervention sets A to some value a, varies with a. Formally we introduce a nonstochastic variable FA, having the same set of values as A, such that FA = a describes the regime in which A is set to value a by an external intervention. <ref type="foot" target="#foot_4">6</ref> Then A has no causal effect on B just when the distribution of B, given FA = a, does not depend on a.</p><p>If FA were a stochastic variable, this would just be the usual property of independence of B from FA, notated as</p><formula xml:id="formula_5">B ⊥ ⊥ FA.<label>(3)</label></formula><p>Now not only does this understanding of independence remain intuitively meaningful in the current case that FA is a non-stochastic variable, but the formal mathematical properties of independence and conditional independence can be rigorously extended to such cases <ref type="bibr" target="#b22">(Dawid, 1979</ref><ref type="bibr" target="#b28">, 1980</ref><ref type="bibr">, Constantinou and Dawid, 2017)</ref>: we term this extended conditional independence (ECI). The standard axioms of conditional independence apply, with essentially no changes<ref type="foot" target="#foot_5">foot_5</ref> , to ECI. Since much of the enterprise of statistical causality involves drawing causal conclusions from observational data, we further endow FA with an additional state ∅, read as "idle": FA = ∅ denotes the regime in which no intervention is applied to A, but it is allowed to develop "naturally", in the observational setting. The distinction between A and FA is that the state of A describes what value was taken, while the state of FA describes how that value was taken.</p><p>We regard the main thrust of causal statistical inference as aiming to deduce properties of a hypothetical interventional regime, such as the distribution of B given FA = a, from observational data, obtained under the idle regime FA = ∅. But since there is no logically necessary connexion between the distributions under different regimes, suitable assumptions will need to be made-and justified-to introduce such connexions.</p><p>The simplest such assumption-which, however will only very rarely be justifiable-is that when we consider the distribution of B given A, we need to know what was the value a that A took, but not how it came to take that value (i.e., not whether this was in the observational regime FA = ∅, or in the interventional regimes, FA = a), the conditional distribution of B being the same in both cases. That is,</p><formula xml:id="formula_6">p(B = b | A = a, FA = ∅) = p(B = b | A = a, FA = a).</formula><p>(4) This is the property of ignorability. When this strong property holds, we can directly take the observed distribution of B given A = a for the desired interventional distribution of B</p><p>given FA = a: the distribution of B given A is a "modular component", transferable between different regimes. We note that (4) can be expressed succinctly using ECI notation, as</p><formula xml:id="formula_7">B ⊥ ⊥ FA | A<label>(5)</label></formula><p>As exemplified by ( <ref type="formula" target="#formula_5">3</ref>) and ( <ref type="formula" target="#formula_7">5</ref>), ECI supplies a simple and powerful language for expressing and manipulating causal concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Augmented DAG</head><p>We can now introduce a formal graphical causal semantics, so moving onto the second rung of the ladder of causation. This is based on ECI, represented by an augmented DAG, which is just like a regular DAG, except that some of its nodes may represent non-stochastic variables, such as regime indicators.<ref type="foot" target="#foot_6">foot_6</ref> Just as certain collections of purely probabilistic conditional independence properties can usefully and precisely be represented (by moralisation) by means of a regular DAG, so we may be able to construct an augmented DAG to represent (by exactly the same moralisation criterion) causal properties of interest expressed in terms of ECI.</p><p>Consider for example the simple augmented DAG</p><formula xml:id="formula_8">FA → A → B</formula><p>This represents (by moralisation, as always) the ECI property of ( <ref type="formula" target="#formula_7">5</ref>), and so is a graphical representation of the ignorability assumption. Note that it is the whole structure that, with ECI, imparts causal meaning to the arrow from A to B-the direction of that arrow would not otherwise be meaningful in itself.</p><p>Example 2 Consider the following augmented DAGS, modifications of the first three DAGs of Example 1 to allow for an intervention on A:</p><formula xml:id="formula_9">1. FA → A → B → C 2. FA → A ← B ← C 3. FA → A ← B → C</formula><p>We saw in Example 1 that, without the node FA, these DAGs are all Markov equivalent. With FA included, we see that DAGs 2 and 3 are still Markov equivalent, since they have the same skeleton and the same unique immorality FA → A ← B; but they are no longer Markov equivalent to DAG 1, which does not contain that immorality. All three DAGs represent the ECI A ⊥ ⊥ C | (FA, B), which says that, in any regime<ref type="foot" target="#foot_7">foot_7</ref> , A is independent of C given B; however, while DAGs 2 and 3 both represent the ECI (B., C) ⊥ ⊥ FA, which implies that A does not cause either B or C, DAG 1 instead represents the ECI (B, C) ⊥ ⊥ FA | A, expressing the ignorability of the distribution of (B, C) given A.</p><p>Note moreover that the Markov equivalence of DAGs 2 and 3 means that we can not interpret the direction of the arrow between B and C as meaningful in itself. In particular, in DAG 3 the arrow B → C does not signify a causal effect of B on C: in this approach, causality is only described by ECI properties, and their moralisation-based graphical representations. □</p><p>Example 3 To endow the instrumental variable problem of Figure <ref type="figure" target="#fig_2">4</ref> with causal contentspecifically, relating to the causal effect of X on Y -we might replace it by the augmented DAG of Figure <ref type="figure" target="#fig_3">6</ref>, where the node FX now allows us to consider an intervention on X. This DAG still represents the probabilistic conditional independencies (1) and ( <ref type="formula" target="#formula_2">2</ref>), in any regime. But now it additionally represents genuine causal properties:  <ref type="formula">6</ref>) says that X has no causal effect on U and Z, these having the same joint distribution in all regimes (including the idle regime). Property ( <ref type="formula">7</ref>) entails the modular conditional ignorability property that the distribution of Y given (z, u, x) (which in fact depends only on (u, x), by ( <ref type="formula">1</ref>)) is the same, both in the interventional regime where X is set to x, and in the observational regime where X is not controlled. Although rarely stated so explicitly, these assumptions are basic to most understandings of an instrumental variable problem, and its analysis (which we shall not however pursue here).</p><formula xml:id="formula_10">(U, Z) ⊥ ⊥ FX (6) Y ⊥ ⊥ FX | (Z, U, X) (7) Y X Z U Example: Instrumental Variable X: cause Y: effect Z: instrument U: unobserved F X</formula><p>If we wanted, we could work with an augmented version of the SPM representation of Figure <ref type="figure">5</ref>, as in Figure <ref type="figure" target="#fig_4">7</ref>. This entails exactly the same ECI properties as Figure <ref type="figure" target="#fig_3">6</ref> for the Yet another representation of the problem is given in Figure <ref type="figure" target="#fig_5">8</ref>. Here V denotes an additional unobserved variable of no direct interest. It can again be checked that both Figure <ref type="figure" target="#fig_3">6</ref> and Figure <ref type="figure" target="#fig_5">8</ref> represent the identical ECI properties between the variables of interest, X, Y, Z, U and FX . This identity shows that the arrow Z → X in Figure <ref type="figure" target="#fig_3">6</ref> should not be taken as signifying a direct causal effect of Z on X: we could equally well regard X and Z as being associated through a common cause, V . <ref type="bibr" target="#b26">Hernán and Robins (2006)</ref> regard Figure <ref type="figure" target="#fig_3">6</ref> and Figure <ref type="figure" target="#fig_5">8</ref> as essentially different-as indeed they would be if interpreted in these informal terms-and conclude (correctly) that it is not necessary, for analysing the instrumental variable problem, to require that Z have a direct effect on X. From our point of view there is no essential difference between Figure <ref type="figure" target="#fig_3">6</ref> and Figure <ref type="figure" target="#fig_5">8</ref>, since even in Figure <ref type="figure" target="#fig_3">6</ref>   As it stands this looks like a probabilitic DAG, representing purely conditional independence properties, such as C ⊥ ⊥ D | (A, B). <ref type="bibr" target="#b3">Pearl (2009)</ref>, however, would endow it with additional causal meaning, using an interventional interpretation as in § 4. He would regard it as asserting that, for any node, its conditional distribution, given its parents, would be the same, in a purely observational setting, and in any interventional setting that sets the values of some or all of the variables other than itself. For example, it requires:</p><formula xml:id="formula_11">F X Y X Z U X: cause Y: effect Z: instrument U: unobserved E Z E X E U E Y</formula><p>The distribution of C, given (A, B), does not depend on whether A and B arose naturally, or were set by external intervention.</p><p>While this is a perfectly clear formal interpretation of Figure <ref type="figure">9</ref>, it is problematic in that, if we are just presented with that DAG, we may not know whether it is meant to be interpreted as representing only probabilistic properties, or is supposed to be further endowed with Pearl's causal semantics. This ambiguity can lead to confusion: a particular danger is to see, or construct, a probabilistic DAG, and then slide, unthinkingly, into giving it an unwarranted Pearlian causal interpretation.</p><p>We can avoid this problem by explicit inclusion of regime indicators, one for each domain variable, as for example in Figure <ref type="figure" target="#fig_0">10</ref>. Not only is this clearly not intended as a probabilistic DAG, but the Pearlian causal semantics, which in the case of Figure <ref type="figure">9</ref> require external specification, are now explicitly represented in the augmented DAG, by moralisation. For example, Figure <ref type="figure" target="#fig_0">10</ref> represents the ECI C ⊥ ⊥ (FA, FB) | (A, B, FC , FD, FE). When FC = c ̸ = ∅, C has a one-point distribution at c, and this ECI holds trivially. But for FC = ∅ we recover the property quoted in italics above (under any settings, idle or interventional, of FD and FE).</p><p>We also note that an augmented Pearlian DAG can have no other Markov equivalent such DAG, since no arrow can be reversed without creating or destroying an immorality. In this sense every arrow now carries causal meaning.</p><formula xml:id="formula_12">F C Augmented Pearlian DAG F A F D F E F B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F M c h f D 6 0 t f Y C H S + 9 s z 2 i S 0 8 d Q y 8 = " &gt; A A A C p H i c j V H Z b h M x F H W G r Y S l K T z y Y p i C U i m K M i n b C 1 K b S B U P P B R B 2 k r j Y e T x 3 R W v c n 2 A N F o f o G v 4 R X + g 7 / B k w Z E U J G 4 l q W j s 3 i 5 t z C C O z 8 a / e h E V 6 5 e u 3 5 j 4 2 b 3 1 u 0 7 d z d 7 W / e O n K 4 s g x n T Q t u T g j o Q X M H M c y / g x F i g s h B w X J x P W / 3 4 I 1 j H t X r v F w Y y S U 8 V n 3 N G f a D y X p 9 w V e I p r v s H + f 7 g I J / s N A H v D y Y B T 1 8 R k M Y v H P i d J u / F y X C 0 L P x v E K N V H e Z b n Z K U m l U S l G e C O p c m I + O z m l r P m Y C m S y o H h r J z e g p p g I p K c F m 9 / F K D H w e m x H N t w 1 Y e L 9 k / E z W V z i 1 k E Z y S + j P 3 t 9 a S l 2 l p 5 e c v s 5 o r U 3 l Q 7 O K i e S W w 1 7 j t D y 6 5 B e b F I g D K L A 9 v x e y M W s p 8 6 G K X K P j E t J R U l T V h 3 D Q 1 k Y X + X G 8 T A 9 a Q h 8 v V w u 2 m W T e H N u s m H W e / A n</formula><p>G C y a A 9 g g x w P L 7 M 3 q S 7 6 2 7 8 2 4 6 J 5 C W O d 0 P s / + Z y N B 4 m z 4 f P 3 j 6 N 9 y a r C W 2 g B</p><formula xml:id="formula_13">+ g R 6 q M E v U B 7 6 D U 6 R D P E 0 B f 0 F X 1 D 3 6 M n 0 Z v o X T S 7 s E a d V e Y + W q v o w 0 9 Q m 8 v a &lt; / l a t e x i t &gt; C ? ? (F A , F B ) | (A, B, F C = ;) E A B C D Figure 10: Augmented Pearlian DAG</formula><p>However, just as we should not automatically interpret a regular DAG as Pearlian, so we should not unthinkingly simply augment it by adding an intervention indicator for each domain variable-which would have the same effect. We must consider carefully whether the many very strong properties embodied in any Pearlian or augmented DAG, relating probabilistic properties (parent-child distributions) across distinct regimes, are justifiable in the specific applied context we are modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Structural Causal Model</head><p>We can also reintepret a SPM, such as in Figure <ref type="figure">5</ref>, using Pearlian semantics<ref type="foot" target="#foot_8">foot_8</ref> , as a causal model-a Structural Causal Model (SCM). This would then assert that the distributions of the error variables, and the functional dependence of each unintervened domain variable on its parents, are the same in all regimes, whether idle or subject to arbitrary interventions.</p><p>Again, to avoid confusion with a SPM, it is advisable to display a SCM as an augmented DAG, by explicitly including intervention indicators (as in Figure <ref type="figure" target="#fig_4">7</ref>, but having an intervention indicator associated with every domain variable.)</p><p>However, construction and inclusion of fictitious error variables is of no consequence, since, if we concentrate on the domain variables alone, their probabilistic structure, in any regime, will be exactly the same as for the fully probabilistic augmented DAG. In particular, no observations on the domain variables, under any regime or regimes, can resolve the arbitrariness in the specifications of the error distributions and the functional relationships in a SCM. On this second rung of the ladder of causation, the additional structure of the SCM once again gains us nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Causes of Effects</head><p>So far we have only considered modelling and analysing problems relating to the "effects of causes (EoC)", where we consider the downstream consquenced of an intervention. An entirely different class of causal questions relates to "causes of effects (CoE)" <ref type="bibr">(Dawid and Musio, 2022)</ref>, where, in an individual case, both a putative causal variable X and a possible consequence Y of it have both been observed, and we want to investigate whether X was indeed the cause of Y . Such problems arise in the legal context, for example when an individual sues a pharmaceutical company claiming that it was because she took the company's drug that she developed a harmful condition.</p><p>At the simplest level we may only have information about the joint distribution of (X, Y ) and their values, x and y, for the case at hand. But this is not enough to address the CoE question, which refers, not to an unknown fact or variables, but to an unknown relationship: was it causal? To try to undersand this question takes us into new territory-the third rung of the ladder of causation.</p><p>Although by no means totally satisfactory, this question is most commonly understood as relating to a counterfactual situation. Suppose both X and Y are binary, and we have observed X = Y = 1. We mught express the "event of causation" as</p><p>The outcome variable would have been different (i.e., Y = 0) if the causal variable had been different (i.e., X = 0) But the hypothesis here, X = 0, contradicts the known fact that X = 1-it is counterfactual . Since in no case can we observe both X = 1 and X = 0-what has been called "the fundamental problem of causal inference" <ref type="bibr" target="#b30">(Holland, 1986)</ref>-it would seem impossible to address this question, at any rate, on the basis of purely factual knowledge of the joint distribution of (X, Y ). So a more complicated framework is required, necessitating more complex assumptions and analyses <ref type="bibr" target="#b31">(Dawid, 2007)</ref>.</p><p>One approach builds on the idea of "potential responses", popularised by <ref type="bibr" target="#b32">Rubin (1974</ref><ref type="bibr" target="#b33">Rubin ( , 1978) )</ref> initally for addressing EoC questions-which, as we have seen, can progress perfectly well without them. They do however seem essential for formulating counterfactual questions. For the simple example above, we duplicate the response Y , replacing it by the pair (Y0, Y1), with Yx conceived of as a potential response, that would be realised if in fact X = x. Then the probability of causation, PC, can be defined as the conditional probability, given the data, of the event of causation:</p><formula xml:id="formula_14">PC = P(Y0 = 0 | X = 1, Y1 = 1). (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>There is however a difficulty: on account of the fundamental problem of causal inference, no data, of any kind, could ever identify the joint distribution of (Y0, Y1), so PC is not estimable. It turns out that data supplying the distribution of the observable variables (X, Y ) can be used to set interval bounds on PC, and these bounds can sometimes be refined if we can collect data on additional variables <ref type="bibr" target="#b12">(Dawid et al., 2017, Dawid and</ref><ref type="bibr">Musio, 2022)</ref>, but only in very special cases can we obtain a point value for PC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Graphical representation</head><p>Because PC cannot be defined in terms of observable domain variables, we cannot represent a CoE problem by means of a regular or augmented DAG on these variables. It is here that the expanded SCM version appears to come into its own. Thus consider the simple SCM of Figure <ref type="figure" target="#fig_0">11</ref>. In it, we have</p><formula xml:id="formula_16">X = fX (EX ), Y = fY (X, EY ), E Y E X F X Y X Figure 11: Simple SCM with EY ⊥ ⊥ X.</formula><p>The deterministic structure of a SCM supports the introduction of potential responses (something that can not be done with a purely probabilistic DAG): Yx = fY (x, EY ).<ref type="foot" target="#foot_9">foot_9</ref> Then X, Y0, Y1 are all functions of (EX , EY ), and thus have a joint distribution entirely determined by that of (EX , EY ). And given this joint distribution we can compute PC in (8). Likewise, given a more general SCM we can compute answers to CoE questions.</p><p>It would seem then that this approach bypasses the difficulties alluded to above. This, however, is illusory, since such a solution is available only when we have access to a fully specified SCM. As discussed in § 2.4 and § 6.1, there will be many distinct PSMs or SCMs consistent with a given probabilistic or augmented DAG model for the domain variables. Since the probabilistic structure is the most that we can learn from empirical data, we will never be able to choose between these distinct SCM versions of the problem. However, different SCMs will lead to different answers to the CoE questions we put to them. Thus for Figure <ref type="figure" target="#fig_0">11</ref>, we have PC = P(Y0 = 0 | Y1 = 1), since Y ⊥ ⊥ X, and this will depend on the dependence between Y0 and Y1 as embodied in the SCM. But because of the fundamental problem of causal inference, this dependence can never be identified from empirical data. So different SCMs inducing the same probabilistic structure, which are entirely indistinguishable empirically, will lead to different answers. When we allow for all possible such SCMs, we are led back to the interval bounds for PC discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We have surveyed a variety of directed graphical models, with varying syntax (including or omitting error variables or regime indicators) and semantics (formal or informal, modelling probabilistic or causal situations). Different semantics are relevant to different rungs of the ladder of causation.</p><p>When presented with a simple DAG it may not be obvious how it is supposed to be interpreted, and there is an ever-present danger of misinterpretation, or of slipping too easily from one interpretation (e.g., purely probabilistic) to another (e.g., causal). This can largely be avoided by always using a simple DAG to model a probabilistic problem (on the first rung of the ladder), and an augmented DAG to model a causal problem (on the second rung). In both cases, the moralisation procedure provides the semantics whereby interpretive properties can be read off the graph.</p><p>DAGs such as SCMs, that involve, explicity or implicitly, error variables and functional relationships, can be used on all three rungs of the ladder. However they can not be identified empirically. For rungs one and two this is unimportant, and all equivalent versions, inducing the same underlying purely probabilistic DAG, will yield the same answers as obtainable from that underlying DAG. For rung 3, which addresses the probability of causation in an individual instance, only an approach based on SCMs is available. However, different but empirically indistinguishable SCMs now deliver different answers to the same causal question. Taking this into account we may have to be satisfied with an interval bound on the desired, but unidentifiable, probability of causation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Directed acyclic graph D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: Ancestral subgraph D ′</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Instrumental variable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Instrumental variable: Augmented DAG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Instrumental variable: Augmented DAG with error variables domain variables and the intervention variable. With suitably chosen distributions for the error variables and functional dependence of each domain variable on its parents, we can recover the same joint distribution for the domain variables, in any regime, as in the original probabilistic augmented DAG. Inclusion of the extra structure brings nothing new to the table.Yet another representation of the problem is given in Figure8. Here V denotes an additional unobserved variable of no direct interest. It can again be checked that both Figure6and Figure8represent the identical ECI properties between the variables of interest, X, Y, Z, U and FX . This identity shows that the arrow Z → X in Figure6should not be taken as signifying a direct causal effect of Z on X: we could equally well regard X and Z as being associated through a common cause, V .<ref type="bibr" target="#b26">Hernán and Robins (2006)</ref> regard Figure6and Figure8as essentially different-as indeed they would be if interpreted in these informal terms-and conclude (correctly) that it is not necessary, for analysing the instrumental variable problem, to require that Z have a direct effect on X. From our point of view there is no essential difference between Figure6and Figure8, since even in Figure6the arrow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Instrumental variable: Alternative augmented DAG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure 9: Pearlian DAG</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>i.e., there is no directed path from it to a mentioned node</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The dotted outline on node U serves as a reminder that U is unobsderved, but is otherwise of no consequence.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>using a suitable definition of the inverse function F -1 x when Fx is not continuous</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Even when all terms involved are fully understood, one might question just why these assumptions should be regarded as appropriate</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We shall here only consider "surgical interventions", such that F A = a ⇒ A = a.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>In ECI we interpret A ⊥ ⊥ B | C as the property that the conditional distribution of A, given B = b and C = c, depends only on c. Here we can allow B and C to include non-stochastic variables; however, A must be fully stochastic.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We may indicate a non-stochastic variable by a square node, and a stochastic variable by a round node. However this distinction does not affect how we use the DAG.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>In fact this only has bite for the idle regime, since, in an interventional regime F A = a, A has a degenerate distribution at a, so that the conditional independence is trivially satisfied.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>where the possibility of intervention is envisaged for each of the domain variables, but not for the fictitious error variables</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>Indeed, we could replace E Y with the pair Y := (Y 0 , Y 1 ), and the function F Y with the "look-up" function L Y , where L Y (x, Y ) = Yx.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>San</publisher>
			<pubPlace>Mateo, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graphical Models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Probabilistic Networks and Expert Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Second ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using a Graphical Method to Assist the Evaluation of Complicated Patterns of Evidence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Evett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forensic Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Influence Diagrams for Causal Modelling and Inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">437</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Corrigenda</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal Inference Using Influence Diagrams: The Problem of Partial Compliance (with Discussion)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Highly Structured Stochastic Systems</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Hjort</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</editor>
		<meeting>the Highly Structured Stochastic Systems</meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="45" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beware of the DAG</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<ptr target="http://tinyurl.com/33va7tm" />
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research Workshop and Conference Proceedings</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="59" to="86" />
		</imprint>
	</monogr>
	<note>Proceedings of the Proceedings of the NIPS 2008 Workshop on Causality</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing and Doing: The Pearlian Synthesis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Heuristics, Probability and Causality: A Tribute to Judea Pearl</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Dechter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Geffner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</editor>
		<imprint>
			<publisher>London</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying the Consequences of Dynamic Treatment Strategies: A Decision-Theoretic Overview</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Surveys</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="184" to="231" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Defining and Identifying the Effect of Treatment on the Treated</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Geneletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality in the Sciences; Illari</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="728" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical Causality from a Decision-Theoretic Perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-010814-020105</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and its Application</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="303" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Probability of Causation. Law, Probability and Risk</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murtas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="163" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extended Conditional Independence and Applications in Causal Inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2618" to="2653" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decision-Theoretic Foundations for Statistical Causality</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2020-0008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="39" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Tale Wags the DAG. In Probabilistic and Causal Inference: The Works of Judea Pearl</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501714.3501744</idno>
		<editor>Dechter, R.</editor>
		<editor>Geffner, H.</editor>
		<editor>Halpern, J.Y.</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computing Machinery and Morgan &amp; Claypool</publisher>
			<biblScope unit="page" from="557" to="574" />
		</imprint>
	</monogr>
	<note>chapter 28</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effects of Causes and Causes of Effects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musio</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-070121-061120</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and its Application</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="261" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bounding Causes of Effects with Mediators. Sociological Methods and Research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musio</surname></persName>
		</author>
		<idno type="DOI">10.1177/00491241211036161</idno>
	</analytic>
	<monogr>
		<title level="j">Online first</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2022-03">2022. March, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The Book of Why; Basic Books</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Constraint-Propagation Approach to Probabilistic Reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lemmer</surname></persName>
		</editor>
		<meeting>the Uncertainty in Artificial Intelligence<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal Networks: Semantics and Expressiveness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Levitt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lemmer</surname></persName>
		</editor>
		<meeting>the Uncertainty in Artificial Intelligence<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Independence Properties of Directed Markov Fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Leimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="491" to="505" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional Independence in Statistical Theory (with Discussion)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Chain Graph Markov Property</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frydenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="333" to="353" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Equivalence and Synthesis of Causal Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Bonissone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</author>
		<author>
			<persName><surname>Lemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1991">1991</date>
			<publisher>Amsterdam</publisher>
			<pubPlace>North-Holland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Science and Sanity: An Introduction to Non-Aristotelian Systems and General Semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korzybski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1933">1933</date>
			<publisher>International Non-Aristotelian Library Publishing Compan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instruments for Causal Inference: An Epidemiologist&apos;s Dream?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="360" to="372" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Axiomatization of Interventional Probability Distributions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Soo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04479</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional Independence for Statistical Operations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="598" to="617" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extended Conditional Independence and Applications in Causal Inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2618" to="2653" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Statistics and Causal Inference (with Discussion)</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="945" to="970" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hypotheticals and Potential Responses: A Philosophical Examination of Statistical Causality</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><surname>Counterfactuals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality and Probability in the Sciences</title>
		<title level="s">Texts in Philosophy</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Russo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</editor>
		<imprint>
			<publisher>College Publications: London</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="503" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian Inference for Causal Effects: The Rôle of Randomization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="34" to="68" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What Can Group Level Data Tell Us About Individual Causality?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-75460-0_13</idno>
	</analytic>
	<monogr>
		<title level="m">Statistics in the Public Interest</title>
		<editor>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Memory</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fienberg; Carriquiry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tanur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Eddy</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="235" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
