<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHARACTERIZATION OF CAUSAL ANCESTRAL GRAPHS FOR TIME SERIES WITH LATENT CONFOUNDERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-05">5 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Andreas</forename><surname>Gerhardus</surname></persName>
							<email>andreas.gerhardus@dlr.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">German Aerospace Center</orgName>
								<orgName type="department" key="dep2">Institute of Data Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CHARACTERIZATION OF CAUSAL ANCESTRAL GRAPHS FOR TIME SERIES WITH LATENT CONFOUNDERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-05">5 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.08417v2[stat.ME]</idno>
					<note type="submission">Submitted to the Annals of Statistics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Primary 62A09</term>
					<term>62D20</term>
					<term>62M10; secondary 68T30</term>
					<term>68T37 Causal graph</term>
					<term>ancestral graph</term>
					<term>time series</term>
					<term>latent variable</term>
					<term>causal inference</term>
					<term>causal discovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a novel class of graphical models for representing time lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences-without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. In recent decades causal graphical models have become a standard tool for reasoning about causal relationships, e.g. <ref type="bibr">Pearl (2009)</ref>, <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, <ref type="bibr" target="#b19">Koller and Friedman (2009)</ref>. The most basic and popular class of models are directed acyclic graphs (DAGs). In their interpretation as causal Bayesian networks these graphs specify interventional distributions and causal effects in terms of the observational distribution, e.g. <ref type="bibr" target="#b37">Spirtes, Glymour and Scheines (1993)</ref>, <ref type="bibr" target="#b28">Pearl (1995)</ref>, <ref type="bibr" target="#b29">Pearl (2000)</ref>. DAGs can only model acyclic causal relationships among variables that are not subject to latent confounding, i.e., such that there are no unobserved common causes of observed variables. The latter assumption is known as causal sufficiency and intuitively means that all variables relevant for describing the system's causal relationships are modeled explicitly. If causal sufficiency cannot be asserted, as is often the case, then one approach is to instead work with maximal ancestral graphs (MAGs), see <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>, <ref type="bibr">Zhang (2008a)</ref>. This larger class of graphs retains a well-defined causal interpretation in presence of latent confounding.</p><p>MAGs can even represent selection variables, i.e., unobserved variables that determine which sample points belong to the observed population. In this paper, we rule out selection variables by assumption. It is then sufficient to work with a subclass of MAGs that, following <ref type="bibr" target="#b25">Mooij and Claassen (2020)</ref>, are called directed maximal ancestral graphs (DMAGs). Assuming the absence of selection variables is common both in the literature on causal effect estimation and causal discovery, e.g. <ref type="bibr" target="#b41">Zhang (2006)</ref>, <ref type="bibr">Perković et al. (2018a)</ref> and <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, <ref type="bibr">Malinsky and Spirtes (2018)</ref>, <ref type="bibr">Gerhardus and Runge (2020)</ref>. As an advantage, DMAGs convey significantly stronger inferences about the presence of causal ancestral relationships than MAGs. Moreover, for time series there is exactly one sample point per time step and hence potential selection bias would at least not go unnoticed.</p><p>To use any of these model classes for causal reasoning one needs to already know the system's causal structure in form of the respective graph. If this knowledge is not available and experiments are infeasible, then one must rely on observational causal discovery, e.g. <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, <ref type="bibr" target="#b32">Peters, Janzing and Schölkopf (2017)</ref>, which refers to learning causal relationships from observational data under suitable enabling assumptions. So-called independence-based methods, also called constraint-based methods, attempt to learn the causal graph from independencies in the observed probability distribution. In general, learning the graph from independencies is an under-determined problem since distinct graphs may describe the same set of independencies. This non-uniqueness is known as Markov equivalence. Without more assumptions it is then only possible to learn those features of the causal graph that it shares with all its Markov equivalent graphs. These shared features can in turn be represented by certain graphs, which for the case of MAGs are partial ancestral graphs (PAGs), see <ref type="bibr" target="#b0">Ali, Richardson and Spirtes (2009)</ref>, <ref type="bibr">Zhang (2008b)</ref>. There are sound and complete causal discovery algorithms for learning PAGs, e.g. the FCI algorithm, see <ref type="bibr" target="#b39">Spirtes, Meek and Richardson (1995)</ref>, <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, <ref type="bibr">Zhang (2008b)</ref>. Here, sound refers to correctness of the method and complete to it learning all shared features. The refinement of PAGs obtained by restricting from MAGs to DMAGs are called directed partial ancestral graphs (DPAGs) in <ref type="bibr" target="#b25">Mooij and Claassen (2020)</ref>.</p><p>The causal graphical model framework outlined above does not inherently rely on temporal information, and the non-temporal setting so far is its major domain of application. However, dynamical systems and time series data are ubiquitous and of great interest to science and beyond. In this setting, Granger causality (see <ref type="bibr" target="#b16">Granger (1969)</ref>) is a widely-used framework for causal analyses. This framework employs a predictive notion of causality, according to which a time series X has a causal influence on time series Y if the past of X helps in predicting the present of Y given that the pasts of all time series other than X are already known. Granger causality has two central limitations: First, it requires the absence of latent confounders, i.e., unobserved time series that are a common cause of two observed time series. Second, it cannot in general deal with contemporaneous causal influences, i.e., causal influences on time scales below the sampling interval. For an in-depth discussion of these limitations see, e.g., <ref type="bibr">Peters, Janzing and Schölkopf (2017, chapter 10)</ref>.</p><p>Since the causal graphical model framework is not subject to these two limitations, in recent years there has been a growing interest in adapting it to the time series setting. Generally, there are three ways to do this. The first approach, e.g. <ref type="bibr" target="#b8">Eichler and Didelez (2007)</ref>, <ref type="bibr" target="#b7">Eichler (2010)</ref>, <ref type="bibr" target="#b9">Eichler and Didelez (2010)</ref> and <ref type="bibr" target="#b6">Didelez (2008)</ref>, <ref type="bibr" target="#b24">Mogensen and Hansen (2020)</ref>, uses a graph in which there is one vertex per component time series. The edges then summarize the causal influences at all time lags, thus giving a conveniently compressed graphical representation of the causal relationships. However, the information about time lags of individual cause-and-effect relationships is lost. The second approach uses graphs with one vertex per component time series and time step, thus resolving the time lags. There are various causal discovery methods that implement this approach, e.g. <ref type="bibr" target="#b5">Chu and Glymour (2008)</ref>, <ref type="bibr" target="#b18">Hyvärinen et al. (2010)</ref>, <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, <ref type="bibr">Malinsky and Spirtes (2018)</ref>, <ref type="bibr" target="#b34">Runge (2020)</ref>, <ref type="bibr" target="#b26">Pamfil et al. (2020)</ref>, <ref type="bibr">Gerhardus and Runge (2020)</ref>, and application works from diverse domains, e.g. <ref type="bibr" target="#b20">Kretschmer et al. (2016)</ref>, <ref type="bibr" target="#b17">Huckins et al. (2020)</ref>, <ref type="bibr" target="#b36">Saetia, Yoshimura and Koike (2021)</ref>. By resolving time lags it becomes possible to obtain a data-driven process understanding and to study the effect of interventions on particular time steps of variables. However, learning a time-resolved graph is statistically more challenging than learning a time-collapsed graph and one might need to compromise on the number of resolved time steps. <ref type="bibr" target="#b2">Assaad, Devijver and Gaussier (2022)</ref> proposes a third, intermediate approach with two vertices per component time series (one for the present time step and one for the entire past).</p><p>We follow the second approach. In this case, the temporal information inherent to time series restricts the connectivity pattern (i.e., absence and presence of edges, edge orientations) of the resulting time-resolved graphs. Namely, since we here consider graphical models in which directed edges signify causal influences (DAGs, DMAGs and DPAGs), the directed edges must not point backwards in time. In addition, we assume time invariant causal relationships. This invariance, known as causal stationarity, implies that the graph's edges are repetitive in time. For DAGs that represent time series without latent confounders, which we call time series DAGs (ts-DAGs), these are the only restrictions on the connectivity pattern.</p><p>For DMAGs that represent time series with latent confounders, the corresponding restrictions on the connectivity pattern have, however, not yet been worked out. Although there are works on independence-based time series causal discovery with latent confounding, see <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, <ref type="bibr">Malinsky and Spirtes (2018)</ref>, <ref type="bibr">Gerhardus and Runge (2020)</ref>, no characterization of the associated class of graphical models has been given. This is the conceptual gap that we close in the present work, i.e., we completely characterize which DMAGs are obtained by marginalizing ts-DAGs and hence can serve as causal graphical model for causally stationary time series with latent confounders. We call the novel graphs defined by this characterization time series DMAGs (ts-DMAGs) and show that these novel graphs constitute a strictly smaller model class than the previously considered model classes. We further show that, without imposing additional assumptions, one can draw stronger causal inferences from ts-DMAGs than from the previously considered graphs. We also introduce time series DPAGs (ts-DPAGs) as representations of Markov equivalence classes of ts-DMAGs. Time series DPAGs are more informative than the graphs learned by current latent time series causal discovery algorithms. As a remark, since contemporaneous causal interactions are allowed without restrictions other than acyclicity, the time series case considered here formally subsumes and hence is more general than the (acyclic) non-temporal case.</p><p>The structure of this paper is as follows: In Sec. 2 we summarize basic graphical concepts and introduce our notation. In Sec. 3 we first specify the considered type of causally stationary time series processes. We then introduce ts-DMAGs, a class of causal graphical models for representing the causal relationships and independencies among only the observed variables of such processes at finitely many regularly spaced observed time steps. In Sec. <ref type="bibr" target="#b45">4</ref> we analyze ts-DMAGs and first derive several properties that they necessarily have. With Theorems 1 and 2 we then completely characterize ts-DMAGs by a single necessary and sufficient condition. We further show that ts-DMAGs are a strict subset of the classes of graphical models that have previously been considered in the literature (see <ref type="bibr">Sec. 4.8)</ref>. For this reason, and as we demonstrate with examples, one can draw stronger causal inferences from ts-DMAGs than from the previously considered graphs. We further introduce the concept of stationarification in order to illuminate various discussions. In Sec. 5 we put these developments to use in the context of causal discovery by defining ts-DPAGs as representations of the Markov equivalence classes of ts-DMAGs. We show that these graphs contain more causal information than the output of current causal discovery algorithms. Moreover, we point out an incorrect claim in the literature that, as we argue, has misguided recent developments (see the discussion below <ref type="bibr">Theorem 3)</ref>. We also present an algorithm that learns ts-DPAGs from data. We give further theoretical results and all proofs in the Supplementary Material <ref type="bibr" target="#b13">(Gerhardus, 2023)</ref>.</p><p>2. Basic graphical concepts and notation. Our notation and terminology is a mixture of those used in <ref type="bibr" target="#b21">Maathuis and Colombo (2015)</ref>, <ref type="bibr">Perković et al. (2018b)</ref> and <ref type="bibr" target="#b25">Mooij and Claassen (2020)</ref> as well as some idiosyncratic notation.</p><p>A graph G = (V, E) consists of a set of vertices V together with a set of edges E ⊆ V × V. The vertices i, j ∈ V are adjacent if (i, j) ∈ E or (j, i) ∈ E. We then say that there is an edge between i and j and that i is an adjacency of j, and similiary for i and j interchanged.</p><p>Throughout this paper we only consider directed partial mixed graphs. These are graphs that satisfy three conditions: First, there is at most one edge between any pair of vertices. Second, no vertex is adjacent to itself. Third, there are at most four types of edges: directed edges (→), bidirected edges (↔), partially directed edges (•→), and non-directed edges (•-•). The third condition is formalized by a decomposition of E as E = E → ∪ E ↔ ∪ E •→ ∪ E •-• that specifies the edge types (also called edge orientations). This decomposition is considered part of the specification of a concrete graph. A directed mixed graph is a partial mixed graph without partially directed and non-directed edges, and a directed graph is a directed mixed graph without bidirected edges. The skeleton of a graph is the object obtained when disregarding the information about the decomposition of</p><formula xml:id="formula_0">E into E → ∪ E ↔ ∪ E •→ ∪ E •-• .</formula><p>Given directed partial mixed graphs G = (V, E) and G ′ = (V ′ , E ′ ), we say that G ′ is a subgraph of G and that G is a supergraph of G ′ , denoted as</p><formula xml:id="formula_1">G ′ ⊆ G or G ⊇ G ′ , if V ′ ⊆ V and (i, j) ∈ E ′ • with • ∈ {→, ↔, •→, •-•} implies (i, j) ∈ E • .</formula><p>Given a directed partial mixed graph G = (V, E), its induced subgraph on V ′ ⊆ V is the graph G ′ = (V ′ , E ′ ) such that (i, j) ∈ E ′</p><p>• with • ∈ {→, ↔, •→, •-•} if and only if i, j ∈ V ′ and (i, j) ∈ E • . We denote a directed edge (i, j) ∈ E → as i→j or j←i and say i→j (j←i) is in G if (i, j) ∈ E → ; similarly for the other edge types. We view edges as composite objects of the symbols at their ends-the edge marks-which are tails, heads, or circles. For example, i•→j has a circle-mark at i and a head mark at j, and i→j has a tail mark at i. Tails and heads are non-circle marks and unambiguous orientations. Circle marks are ambiguous orientations. The symbol ' * ' is a wildcard for all three marks. For example, * → may be →, ↔, or •→.</p><p>A walk in G is an ordered sequence π = (i 1 , i 2 , . . . , i n ) of vertices such that i k and i k+1 are adjacent in G for all k = 1, . . . , n -1. The integer n ≥ 1 is the length of π and a vertex in this sequence is said to be on π. A path is a walk on which every vertex occurs at most once. For a path π = (i 1 , i 2 , . . . , i n ) the vertices i 1 and i n are the end-point vertices of π, all other vertices on π are the non end-point vertices of π. We refer to π as a path between i 1 and i n and graphically represent it by i 1 * - * i 2 * - * . . . * - * i n where i k * - * i k+1 is the unique edge between i k and i k+1 . Such a graphical representation can also specify a path. We say that π is out of i 1 if i 1 →i 2 in G and that π is into i 1 if i 1 ← * i 2 in G; similarly for the other end-point vertex. For 1 ≤ a &lt; b ≤ n we write π(i a , i b ) for the path (i a , i a+1 , . . . i b ) and π(i b , i a ) for the path (i b , i b-1 , . . . i a ). Both of these are subpaths of π. Given walks π 1 = (i 1 , i 2 , . . . , i n ) and π 2 = (j 1 , j 2 , . . . , j m ) with i n = j 1 we write π 1 ⊕ π 2 for the walk (i 1 , i 2 , . . . , i n , j 2 , . . . , j m ). A vertex i k on path π is a collider on π if it is a non end-point vertex of π and π(i k-1 , i k+1 ) is i k-1 * →i k ← * i k+1 , else it is a non-collider on π. If the vertices i and k are non-adjacent, then the path i * - * j * - * k is an unshielded triple and the path i * →j← * k an unshielded collider. A path of length n = 1 is called trivial. The path π = (i 1 , i 2 , . . . , i n ) is a directed path if i k →i k+1 in G for all 1 ≤ k ≤ n -1 or i k ←i k+1 in G for all 1 ≤ k ≤ n -1. In the former case we speak of a directed path from i 1 to i n , in the latter case of a directed path from i n to i 1 .</p><p>If the edge i→j is in G, then i is a parent of j and j is a child of i. The vertex i is an ancestor of j and j is a descendant of i if i = j or if there is a directed path from i to j. The set of parents and ancestors of a vertex i in G are respectively denoted as pa(i, G) and an(i, G). We say vertex i is an ancestor of a set S of vertices and S is a descendant of i if at least one element of S is a descendant of i. Similary, vertex i is a descendant of a set S of vertices and S is an ancestor of i if at least one element of S is an ancestor of i.</p><p>A directed partial mixed graph G has a directed cycle if there are distinct vertices i and j with i ∈ an(j, G) and j ∈ an(i, G). A directed acyclic graph (DAG) D is a directed graph without directed cycles. A directed partial mixed graph G has an almost directed cycle if the edge i↔j is in G and i ∈ an(j, G). A directed ancestral graph is a directed mixed graph without directed cycles and almost directed cycles. An inducing path between i and j is a path π between i and j such that all non end-point vertices of π are colliders on π and ancestors of i or j. A directed maximal ancestral graph (DMAG) M is a directed ancestral graph that has no inducing paths between non-adjacent vertices. Every DAG is a DMAG. Directed partial ancestral graphs (DPAGs) P are directed partial mixed graphs that represent Markov equivalence classes of DMAGs, see Def. 5.2 in Sec. 5.1 for a formal definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A class of causal graphical models for time series with latent confounders. In this section we first formally specify the considered type of time series processes, see Sec. 3.1. We then explain how, if there are no unobserved variables, certain DAGs with an infinite number of vertices <ref type="bibr">(Def. 3.4)</ref> can model these processes as causal Bayesian networks, see Secs. <ref type="bibr">3.2 and 3.3. Importantly,</ref><ref type="bibr">Def. 3.6 in Sec. 3.4</ref> introduces so-called time series DMAGs (ts-DMAGs). These graphs are projections of the infinite DAGs and represent the causal relationships and independencies among only a subset of observed variables at a finite number of regularly sampled or regularly subsampled observed time steps. Time series DMAGs constitute the novel class of causal graphical models which is the central topic of this paper.</p><p>3.1. Structural vector autoregressive processes. We consider multivariate time series {V t } t∈Z , where</p><formula xml:id="formula_2">V t = (V 1 t , . . . , V nV t ) with the component time series V i = {V i t } t∈Z for 1 ≤ i ≤ n V ,</formula><p>that are generated by an acyclic structural vector autoregressive process with contemporaneous influences, e.g. <ref type="bibr">Malinsky and Spirtes (2018)</ref>. That is to say, for all t ∈ Z (time index) and</p><formula xml:id="formula_3">1 ≤ i ≤ n V (variable index) the value of V i t is determined as (1) V i t := f i (PA i t , ϵ i t )</formula><p>, where f i is a measurable function that depends on all its arguments, the random variables ϵ i t (so-called "noise" variables) are jointly independent (with respect to both indices) and have a distribution that may depend on i but not on t, and</p><formula xml:id="formula_4">PA i t ⊆ {V k t-τ | 1 ≤ k ≤ n V , 0 ≤ τ ≤ p ts } \ {V i t }.</formula><p>Here, the order p ts of the process is the smallest integer for which the set inclusion in the previous sentence holds (for all i and t). We demand that 0 ≤ p ts &lt; ∞.</p><p>We allow contemporaneous causal influences (i.e., V k t-τ ∈ PA i t with τ = 0). Further, for all ∆t ∈ Z we assume the sets PA i t and PA i t-∆t to be consistent in the sense that</p><formula xml:id="formula_5">V k t-τ ∈ PA i t if and only if V k t-τ -∆t ∈ PA i t-∆t .</formula><p>Acyclicity means the system of equations is recursive. The attribute structural asserts that eq. ( <ref type="formula">1</ref>) is a structural causal model (SCM), e.g. <ref type="bibr" target="#b3">Bollen (1989)</ref>, <ref type="bibr">Pearl (2009)</ref>, <ref type="bibr" target="#b32">Peters, Janzing and Schölkopf (2017)</ref>, which we indicate by the ':=' symbol. Because of this causal interpretation we refer to the variables PA i t as causal parents of V i t and to the consistency of PA i t and PA i t-∆t as causal stationarity. The restriction of PA i t to variables V k t-τ with τ ≥ 0 ensures that there is no causal influence backward in time.</p><p>3.2. Time series DAGs. The causal parentships specified by an SCM are graphically represented by the SCM's causal graph, e.g. <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, <ref type="bibr">Pearl (2009)</ref>, <ref type="bibr" target="#b32">Peters, Janzing and Schölkopf (2017)</ref>. The causal graph is a directed graph with one vertex per variable, typically excluding the noise variables, and directed edges from each variable to all variables of which it is a causal parent. The same construction applies to structural processes as in eq. ( <ref type="formula">1</ref>). However, as we capture by the below three notions, the resulting "temporal causal graphs" carry more structure than their non-temporal counterparts.</p><p>First, the random variable V i t corresponds to a particular time step t of a particular component time series V i . This correspondence is captured by the following notion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEFINITION 3.1 (Time series structure</head><formula xml:id="formula_6">). A graph G = (V, E) has a time series structure if V = I × T, where I = {1, 2, . . . , n} with n ≥ 1 is the variable index set and T = {t ∈ Z | t s ≤ t ≤ t e } with t s ∈ Z ∪ {-∞} and t e ∈ Z ∪ {+∞} and t s ≤ t e is the time index set.</formula><p>We say that a vertex (i, t) ∈ V is at time t and, if t a ≤ t ≤ t b , to be in the time window</p><formula xml:id="formula_7">[t a , t b ]. We further say (i, t i ) ∈ V is before (j, t j ) ∈ V and (j, t j ) ∈ V is after (i, t i ) ∈ V if t i &lt; t j . An edge ((i, t i ), (j, t j )) ∈ E has length or lag |t i -t j |.</formula><p>We call edges of length zero contemporaneous and call all other edges lagged.</p><p>Second, below eq. ( <ref type="formula">1</ref>) we explicitly restricted the causal parents PA i t to only contain vertices that are before or at time t. This restriction is captured by the following notion. <ref type="bibr">DEFINITION 3.2 (Time order)</ref></p><formula xml:id="formula_8">. A directed partial mixed graph G = (V, E) with time series structure is time ordered if ((i, t i ), (j, t j )) ∈ E → implies t i ≤ t j . FIG 1.</formula><p>Two illustrations of the same ts-DAG D that represents a structural process as in eq. (1) of order p D = p ts = 2 with three component time series V 1 , V 2 , and V 3 . Given the implicit assertion that there is no edge of length larger than those depicted, the ts-DAG is uniquely specified by showing a segment of p ts + 1 successive time steps. The horizontal dots indicate that the structure is repeated into the infinite past and infinite future.</p><p>In a time ordered graph G the ancestral relationship (i, t i ) ∈ an((j, t j ), G) implies t i ≤ t j . This fact shows that also indirect causal influences are correctly restricted to not go backwards in time as soon as this restriction is imposed on direct causal influences.</p><p>Third, the property of causal stationarity (see <ref type="bibr">Sec. 3.1)</ref> restricts the edges to be repetitive in time. This restriction is captured by the following notion. <ref type="bibr">DEFINITION 3.3 (Repeating edges)</ref>. A directed partial mixed graph G = (V, E) with time series structure has repeating edges if the following holds: If</p><formula xml:id="formula_9">((i, t i ), (j, t j )) ∈ E • with • ∈ {→, ↔, •→, •-•} and (i, t i + ∆t), (j, t j + ∆t) ∈ V, then ((i, t i + ∆t), (j, t j + ∆t)) ∈ E • .</formula><p>REMARK <ref type="bibr">(on Def. 3.3)</ref>. Section 3 is concerned with DAGs and DMAGs only. In these graphs there are by definition no edges of the types •→ or •-•. However, in Sec. 5 we will apply the concept of repeating edges also to DPAGs. Since these graphs (DPAGs) can contain edges •→ or •-•, we already here formulate Def. Due to time order and repeating edges, a ts-DAG D is fully specified by its variable index set together with its edges that point to a vertex at time t. Hence, if the longest edge of D is of finite length p D &lt; ∞ then one unambiguously specifies D by drawing all vertices within the time window [tp D , t] and the edges between these; see Fig. <ref type="figure">1</ref> for an example. In slight abuse of notation we sometimes denote vertices by the random variable that they represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Time series DAGs as causal graphs for structural vector autoregressive processes.</head><p>Since the structural process in eq. ( <ref type="formula">1</ref>) is acyclic by assumption, i.e., since the system of equations is recursive, its causal graph is acyclic (hence the terminology). In combination with the discussions in the previous subsection we thus get the following result. LEMMA 3.5. The causal graph of an acyclic and causally stationary structural vector autoregressive process as in eq. ( <ref type="formula">1</ref>) is a ts-DAG.</p><p>This observation has been made before, for example in <ref type="bibr" target="#b35">Runge et al. (2012)</ref> and <ref type="bibr" target="#b31">Peters, Janzing and Schölkopf (2013)</ref> where ts-DAGs have respectively been called time series graphs and full time graphs. We note that, because of time order, the assumption of acyclicity restricts only the ts-DAG's contemporaneous edges.</p><p>In the non-time series setting, an acyclic SCM defines a unique distribution over the SCM's variables (pushforward of the noise distribution by the structural assignments). According to the causal Markov condition, see <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, the SCM's causal graph is a Bayesian network for this so-called entailed distribution <ref type="bibr">(Pearl, 2009)</ref>, which in turn implies that d-separations (see <ref type="bibr" target="#b27">Pearl (1988)</ref>, denoted by '⊥ ⊥') in the causal graph imply the corresponding independencies in the distribution <ref type="bibr">(Verma and Pearl, 1990;</ref><ref type="bibr" target="#b12">Geiger, Verma and Pearl, 1990)</ref>. The causal faithfulness condition, see <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, assumes the reverse implication, i.e., that all independencies imply the corresponding d-separations. Then, d-separations and independencies are in one-to-one correspondence.</p><p>Although acyclic, the time series setting specified by eq. ( <ref type="formula">1</ref>) is more complicated: Since time is indexed by t ∈ Z (as opposed to, e.g., t ∈ N), there is no initial "starting" distribution that can be pushforwarded to explicitly define a unique entailed distribution. Instead, we need to ask whether eq. ( <ref type="formula">1</ref>) implicitly defines a distribution; and if yes, how many. Following the terminology in <ref type="bibr" target="#b4">Bongers, Blom and Mooij (2018)</ref>, this question asks for solutions to eq. ( <ref type="formula">1</ref>), that is, for stochastic processes which satisfy eq. ( <ref type="formula">1</ref>) almost surely. The existence of such solutions as well as their uniqueness (up to almost sure equality) and properties are non-trivial and not considered here. Rather, for the purpose of this paper we assume that eq. ( <ref type="formula">1</ref>) is solved by a (not necessarily unique) strictly stationary stochastic process whose finite-dimensional distributions satisfy the causal Markov and causal faithfulness condition with respect to its ts-DAG. This assumption is common in the literature, cf. <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, <ref type="bibr">Malinsky and Spirtes (2018)</ref>, <ref type="bibr">Gerhardus and Runge (2020)</ref>, and is here only needed for the connection to causality. The results of the present paper are, technically, about marginalizing the independence (i.e., d-separation) models of ts-DAGs and remain valid also without that additional assumption. The issue of solving eq. ( <ref type="formula">1</ref>) is an important aspect to consider in future work.</p><p>3.4. Time series DMAGs. In most real-world scenarios, unobserved common causes cannot be excluded. As mentioned in Sec. 1 for the non-time series setting, directed maximal ancestral graphs (DMAGs) are often used for causal modeling in the presence of unobserved variables. This use of DMAGs as causal graphical models was pioneered in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>, which defines a marginalization / projection procedure that from a DAG G over vertices V, of which only a subset O ⊆ V is observed, constructs a DMAG M O (D) over the observed variables O only (see also <ref type="bibr">Zhang (2008a)</ref>). The projection of D to M O (D) has two properties: First, both graphs have the same ancestral relationships among vertices in O. Second, d-separations in D among vertices in O are in one-to-one correspondence to the similar concept of m-separation in M O (D) (also denoted by '⊥ ⊥'). These two properties ensure that if D is a causal graph then also M O (D) carries causal meaning and can be used for causal reasoning as explained in <ref type="bibr">Zhang (2008a)</ref>.</p><p>Below, we generalize the construction of such "causal" DMAGs to the time series setting. To begin, we first note that for time series there are two types of unobserved variables:</p><p>• Unobservable variables: Some component time series L 1 , . . . , L nL with 0 ≤ n L &lt; n V may be unobserved entirely by the experimental setup. We call these L i unobservable and call the other component time series</p><formula xml:id="formula_10">O 1 , . . . , O nO with n O = n V -n l ≤ ∞ observable.</formula><p>The variable index set I of the ts-DAG D accordingly decomposes as I = I O ∪ I L . This first type of unobserved variables is similar to the case of unobserved variables in the nontime series setting. • Temporally unobserved variables: In addition, throughout the paper we will treat only a finite number of time steps T O as observed. This construction is specific to the time series setting and means that at times T \ T O also the observable time series are treated as unobserved. The rational for doing so is that in practice only finitely many observations are available and hence one can only reason about DMAGs of finite temporal extension. Throughout the paper we restrict the set T O of observed time steps to take one of the following two forms:</p><p>• Regular sampling: All time steps within a time interval [tp, t] for some non-negative integer p &lt; ∞ and a reference time step t are observed, i.e.,</p><formula xml:id="formula_11">T O = {t -τ | 0 ≤ τ ≤ p}. • Regular subsampling: Every n-th time step, for n ≥ 2 an integer, within [t -p, t] with p &lt; ∞ is observed, i.e., T O = {t -τ | 0 ≤ τ ≤ p, τ mod n = 0}.</formula><p>The time window length p is not restricted relative to the order p ts of the data-generating process, i.e., we allow all of p &lt; p ts and p = p ts and p &gt; p ts . The reference time step t is arbitrary since the ts-DAG D has repeating edges. We are led to the following definition.  <ref type="bibr">(D)</ref> and also referred to as a ts-DMAG, is the DMAG on the vertex set O that is obtained by applying the MAG latent projection defined in <ref type="bibr">Zhang (2008a</ref><ref type="bibr">Zhang ( , pp. 1442-3) -3)</ref> to D with L = V \ O being the set of latent vertices.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the construction of ts-DMAGs as projections of ts-DAGs. We stress that all vertices prior to the observed time window (i.e., before time tp) are treated as unobserved, even if they are observable and hence would be observed for a larger value of p.</p><p>REMARK (on Def. 3.6). The time series DMAG M O (D) is defined as the MAG latent projection of an infinite object, namely of the ts-DAG D. An implementation of this projection in a procedure that always terminates in finite time is possible but non-trivial. Such a procedure will be discussed in <ref type="bibr" target="#b15">Gerhardus et al. (2023)</ref>. For the present paper, however, this procedure is not needed because all theoretical results and examples either do not require the explicit construction of ts-DMAGs or one can carry out the required projections by hand. Time series DMAGs are the central objects of interest in this paper and a significant part of the paper deals with deriving their properties. We will see that the repeating edges property of ts-DAGs D plays an essential role in this regard. As a first step, the following lemma notes which of the defining properties of ts-DAGs carry over to ts-DMAGs. While according to part 1 of Lemma 3.7 every ts-DMAG is a DMAG with time series structure, part 2 implies that the reverse is not true. Namely, DMAGs with time series structure that are not time ordered cannot be ts-DMAGs. We thus see that ts-DMAGs are a proper subclass of DMAGs with time series structure. The following example shows that ts-DMAGs M O (D)  Despite this fact, the repeating edges property of the ts-DAG D strongly restricts the connectivity pattern of the ts-DMAG M O (D). We will work out these restrictions in Sec. 4.</p><p>4. Characterization of ts-DMAGs. The main goal of this section is to characterize the space of ts-DMAGs, i.e., to find conditions that specify exactly which DMAGs with time series structure are ts-DMAGs. Theorem 1 in Sec. 4.6 achieves this goal by providing a single condition that is both necessary and sufficient. The theorem uses the notion of canonical ts-DAGs, see Def. <ref type="bibr">4.11 in Sec. 4.5. In Sec. 4.4</ref> we introduce stationarified ts-DMAGs and, more generally, the concept of stationarification. This concept simplifies the definition of canonical ts-DAGs and is useful to describe the output of two recent time series causal discovery algorithms (see Sec. 5.5). In Sec. 4.8 we show that ts-DMAGs constitute a strict subset of the classes of graphical models that have so far been used in the literature for describing time lag specific causal relationships and independencies in time series with latent confounders. Section 4.3 discusses several properties that ts-DMAGs necessarily have, but which can also be obeyed by DMAGs that are not ts-DMAGs. These properties are useful for the discussions in Secs. 4.8 and 5. In Sec. 4.2 we show that regular sampling and regular subsampling are equivalent from a graphical point of view. Section 4.7 gives a characterization of the space of stationarified ts-DMAGs. At first, however, we spell out the motivation for the analysis. 4.1. Motivation. When using a class of graphs to represent causal knowledge, it is desirable to know which graphs belong to this class and which do not. Otherwise, it is impossible to fully characterize which causal claims a given graph of that class conveys. Another, a posteriori motivation has been mentioned in the previous paragraph: In Sec. 4.8 we will see that ts-DMAGs are a strict subset of the previously employed model classes. Thus, when using ts-DMAGs as targets of inference in causal discovery or to reason about causal effects, it is, respectively, possible to learn more qualitative causal relationships (see Secs. 5.4 and 5.5 for an in-depth discussion) and to identify more causal effects (see <ref type="bibr">Example 5.8</ref>) from data without having imposed any additional assumption or restriction.  1. For every n &gt; 1 there is a ts-DAG</p><formula xml:id="formula_12">D ′ such that M IO×T n O (D) = M IO×T 1 O (D ′ ). 2. For every n &gt; 1 there is a ts-DAG D ′ such that M IO×T 1 O (D) = M IO×T n O (D ′</formula><p>). Lemma 4.1 implies: Every property that ts-DMAGs necessarily have in case of regular sampling is also necessarily obeyed in case of regular subsampling (part 1) and vice versa (part 2). Moreover, every set of additional properties that, when imposed on a DMAG M with time series structure, is sufficient for M to be a ts-DMAG in case of regular sampling is also sufficient in case of regular subsampling (part 2) and vice versa (part 1).</p><p>Due to this equivalence we from here on restrict to regular sampling, without losing generality, and write</p><formula xml:id="formula_13">M p (D) for M O (D) where O = I O × T O and T O = {t -τ | 0 ≤ τ ≤ p}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.</head><p>Properties of ts-DMAGs. In this subsection, we discuss several properties that ts-DMAGs M p (D) necessarily have. These properties are such that a certain graphical property persists when the involved vertices are shifted in time. We use the following definitions. DEFINITION 4.2 (Time-shift persistent graphical properties). A partial mixed graph G = (V, E) with time series structure has. . . 1. . . . repeating adjacencies if the following holds: If ((i, t i ), (j, t j )) ∈ E and (i, t i + ∆t), (j, t j + ∆t) ∈ V then ((i, t i + ∆t), (j, t j + ∆t)) ∈ E. 2. . . . past-repeating adjacencies if the following holds: If ((i, t i ), (j, t j )) ∈ E and (i, t i + ∆t), (j, t j + ∆t) ∈ V with ∆t &lt; 0 then ((i, t i + ∆t), (j, t j + ∆t)) ∈ E. 3. . . . repeating orientations if the following holds: If ((i, t i ), (j, t j )) ∈ E • with • ∈ {→, ↔, •→, •-•} and ((i, t i + ∆t), (j, t j +∆t)) ∈ E then ((i, t i +∆t), (j, t j +∆t)) ∈ E • .</p><p>A DMAG M = (V, E) with time series structure has 4. . . . repeating ancestral relationships if the following holds: If (i, t i ) ∈ an((j, t j ), M) and (i, t i + ∆t), (j, t j + ∆t) ∈ V then (i, t i + ∆t) ∈ an((j, t j + ∆t), M). 5. . . . repeating separating sets if the following holds: If (i, t i ) ⊥ ⊥ (j, t j ) | S and {(i, t i + ∆t), (j, t j + ∆t)} ∪ S ∆t ⊆ V, where S ∆t is obtained by shifting every vertex in S by ∆t time steps, then (i,</p><formula xml:id="formula_14">t i + ∆t) ⊥ ⊥ (j, t j + ∆t) | S ∆t .</formula><p>REMARK (on Def. 4.2). Section 4 is concerned with DAGs and DMAGs only. However, in Sec. 5 we will apply the concepts of repeating adjacencies, past-repeating adjacencies and repeating orientations also to DPAGs (which are a special case of partial mixed graphs). Hence, we already here formulate the definition in sufficient generality.</p><p>Figure <ref type="figure" target="#fig_5">3</ref> illustrates the five properties introduced by Def. 4.2 as well as their distinctions. Below we will make frequent use of the implications expressed by the following lemma. <ref type="bibr">LEMMA 4.3. 1.</ref> Repeating edges is equivalent to the combination of repeating adjacencies and repeating orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Repeating adjacencies implies past-repeating adjacencies. 3. Repeating ancestral relationships implies repeating orientations. 4. In graphs with time index set T = Z, repeating edges implies repeating ancestral relationships and repeating separating sets.</p><p>These implications further show that the combination of repeating adjacencies and repeating ancestral relationships implies repeating edges. Importantly, repeating orientations does not imply repeating ancestral relationships, see part b) of Fig. <ref type="figure" target="#fig_5">3</ref> for an example.</p><p>Since ts-DAGs have repeating edges, according to Lemma 4.3 they in fact also have all five properties given in Def. 4.2. How about ts-DMAGs? While these in general do not inherit repeating edges from the underlying ts-DAG, see part 3 of Lemma 3.7, the following lemma shows that ts-DMAGs do feature some of the weaker time-shift persistent properties.   ). That ts-DMAGs have repeating orientations and repeating separating sets has already been found and used in <ref type="bibr">Entner and Hoyer (2010). 4.4. Stationarified ts-DMAGs. Example 4.5</ref> shows that in a ts-DMAG M p (D) there may be an edge (i, t i + ∆t) * - * (j, t j + ∆t) with ∆t &lt; 0 even if the vertices (i, t i ) and (j, t j ) are non-adjacent in M p (D). This is the case even though one then knows that (i, t i + ∆t) and (j, t j + ∆t) can be d-separated in underlying ts-DAG D, just not by a set of vertices that is within the observed time window. One might thus view such an edge (i, t i + ∆t) * - * (j, t j + ∆t) in M p (D) as an artifact of the chosen time window and hence prefer to manually remove the edge by subjecting the ts-DMAG to the following operation. <ref type="bibr">DEFINITION 4.6 (Stationarification)</ref>. Let G = (V, E) be a directed partial mixed graph with time series structure. The stationarification of G, denoted as stat(G), is the graph stat(G) = (V ′ , E ′ ) defined as follows:</p><p>1. It has the same set of vertices as G, i.e., V ′ = V. 2. There is an edge ((i, t i ), (j,</p><formula xml:id="formula_15">t j )) ∈ E ′ • with • ∈ {→, ↔, •→, •-•} if and only if ((i, t i + ∆t), (j, t j + ∆t)) ∈ E • in G for all ∆t with (i, t i + ∆t), (j, t j + ∆t) ∈ V.</formula><p>REMARK (on Def. 4.6). Section 4 is concerned with DAGs and DMAGs only. In these graphs there are by definition no edges of the types •→ or •-•. However, in Sec. 5 we will apply the concept of sationarification also to DPAGs. Since these graphs (DPAGs) can contain edges •→ or •-•, we already here formulate the definition in sufficient generality.  <ref type="figure" target="#fig_1">2</ref>) and a DMAG with time series structure M 2 (the same as in part a) of Fig. <ref type="figure" target="#fig_5">3</ref>) together with their stationarifications. Note that although M 2 has repeating adjacencies its contemporaneous edges are not in stat(M 2 ) because these edges do not have the same orientation.</p><p>To see that the process of stationarification indeed achieves what it is supposed to do, consider the ts-DMAG M 1 in part a) of <ref type="bibr">Fig. 4.</ref> In this graph there is the edge O Stationarification removes an edge (i, t i + ∆t) * - * (j, t j + ∆t) also if (i, t i ) and (j, t j ) are adjacent but if the edges (i, t i ) * - * (j, t j ) and (i, t i + ∆t) * - * (j, t j + ∆t) have different orientations (note the "•" subscripts on E ′</p><p>• and E • in part 2 of Def. 4.6). This effect, illustrated by parts c) and d) of Fig. <ref type="figure" target="#fig_8">4</ref>, ensures that stat(G) is the unique largest subgraph of G with repeating edges. For graphs with repeating orientations (as, e.g., ts-DMAGs) this effect does not occur and stationarification only concerns adjacencies (as, e.g., in parts a) and b) of Fig. <ref type="figure" target="#fig_8">4</ref>).</p><p>Since ts-DMAGs M p (D) have repeating orientation and past-repeating adjacencies, their stationarifications stat(M p (D)) can be characterized with the following simpler condition. LEMMA 4.7. The stationarification stat(M p (D)) of a ts-DMAG M p (D) is the unique subgraph of M p (D) in which the vertices (i, t jτ ) and (j, t j ) with τ ≥ 0 are adjacent if and only if the vertices (i, tτ ) and (j, t) are adjacent in M p (D).</p><p>Because the stationarification stat(G) is a subgraph of G, a time series structure and time order naturally carry over from G to stat(G). Moreover, we can prove the following. We thus refer to stat(M p (D)) as a stationarified ts-DMAG and abbreviate stat(M p (D)) as M p st (D). However, as the following example shows, a stationarified ts-DMAG M p st (D) may not be the MAG latent projection of any ts-DAG, i.e., may not be a ts-DMAG.  The vertices (i, tτ i ) and (j, tτ j ) with 0 ≤ τ j ≤ τ i ≤ p are adjacent in a stationarified ts-DMAG M p st (D) if and only if they can not be d-separated by any set of observable vertices within [tpτ j , t] in the underlying ts-DAG D (instead of [tp, t], which is what a ts-DMAG would assert). The orientation of edges, however, retains the standard meaning: Tail and head marks respectively convey (non-)ancestorship according to the ts-DAG D. The following lemma says that stationarification does not change ancestral relationships.</p><p>LEMMA 4.10. The ts-DMAG M p (D) and its stationarification M p st (D) agree on ancestral relationships, i.e., (i, t i ) ∈ an((j, t j ), M p (D)) if and only if (i, t i ) ∈ an((j, t j ), M p st (D)).</p><p>Since M p (D) and D by construction of the MAG latent projection agree on ancestral relationships, Lemma 4.10 implies that also the stationarified ts-DMAG M p st (D) agrees with the ancestral relationships of D. Thus, M p st (D) has repeating ancestral relationships. In summary, edges in the ts-DMAG M p (D) that are not also in M p st (D) are due to marginalizing over observable vertices before tp. Such edges disappear when p is sufficiently increased, see also <ref type="bibr">Gerhardus (2023, Sec. B.8)</ref>. However, as we will show in Sec. 5.3, these additional edges play a useful role in causal discovery. In Sec. 5.5 we will further use the concept of stationarification to describe the SVAR-FCI causal discovery algorithm from <ref type="bibr">Malinsky and Spirtes (2018)</ref> and the LPCMCI causal discovery algorithm from Gerhardus and Runge (2020). 4.5. Canonical ts-DAGs. In the current subsection we return to the goal of characterizing the space of ts-DMAGs. To this end, we first recall the concept of canonical DAGs. <ref type="bibr">DEFINITION 4.11 (Canonical DAG. From Sec. 6.1 of Richardson and Spirtes (2002)</ref>, specialized to the case of directed ancestral graphs</p><formula xml:id="formula_16">). Let G = (V, E) be a directed ancestral graph. The canonical DAG D c (G) of G is the graph D c (G) = (V ca , E ca ) defined as follows: 1. Its vertex set is V ca = V ∪ L, where L = {l ij | (i, j) ∈ E ↔ }. 2. Its edge set E ca = E ca → consists of the edges • i→j for all (i, j) ∈ E → and • l ij →i for all l ij ∈ L and • l ij →j for all l ij ∈ L.</formula><p>Intuitively, the canonical DAG D c (G) of a directed ancestral graph G is obtained by replacing each bidirected edge i↔j in G with i←l ij →j where l ij is an additionally inserted, unobserved vertex. The canonical DAG D c (G) is a DAG and has the convenient property that there are no edges pointing into unobserved vertices and hence that there are also no edges between two unobserved vertices. Despite this simple structure of unobserved vertices, the following result shows that canonical DAGs are expressive enough to generate all DMAGs. LEMMA 4.12 (Theorem 6.4 in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>, specialized to directed ancestral graphs). If M is a DMAG over vertex set O, then the MAG latent projection For ts-DMAGs, however, there is no definitional characterization. In addition, because not every DMAG with time series structure is a ts-DMAG (see the explanation below Lemma 3.7), characterizing ts-DMAGs is a non-trivial task. In the remaining parts of the current subsection and Sec. 4.6, we show that ts-DMAGs can be characterized by an appropriate generalization of the condition G = M O (D c (G)). The first step of such a generalization is to find an appropriate generalization of canonical DAGs.  <ref type="formula">4</ref>) and a DMAG with time series structure M 2 together with their canonical ts-DAGs. In Dc(M 1 ) there is no unobservable time series because in M 1 there is no bidirected edge that is repetitive in time and hence there is no bidirected edge in stat(M 1 ). The unobservable time series L (2,1,1) in Dc(M 2 ) in the notation of Def. 4.13 corresponds to (2, 1, 1) ∈ J and results from the edge</p><formula xml:id="formula_17">M O (D c (M)) of the canonical DAG D c (M) of M equals M, i.e., M = M O (D c (M)).</formula><formula xml:id="formula_18">O 2 t-1 ↔O 1 t in stat(M 2 ) = M 2 .</formula><p>The generalization of canonical DAGs to the time series setting is non-trivial for the following reason. Consider an edge (i, t i ) * - * (j, t j ) in a DMAG M with time series structure that is not in the DMAG's stationarification stat(M). If, depending on the orientation of the edge (i,</p><formula xml:id="formula_19">t i ) * - * (j, t j ) in M, either (i, t i )→(j, t j ) or (i, t i )←(j, t j ) or (i, t i )←(l ij , t ij )→(j, t j )</formula><p>with (l ij , t ij ) unobserved were included in a "canonical ts-DAG" D c (M), then the repeating edges property of ts-DAGs would require the same structure to be present at all other time steps too. Hence, in D c (M) there would be (i,</p><formula xml:id="formula_20">t i + ∆t)→(j, t j + ∆t) or (i, t i + ∆t)←(j, t j + ∆t) or (i, t i + ∆t)←(l ij , t ij + ∆t)→(j, t j + ∆t) for all ∆t ∈ Z. Con- sequently, in the MAG latent projection M O (D c (M)) of D c (M)</formula><p>there would be an edge (i, t i + ∆t) * - * (j, t j + ∆t) of the same type for all ∆t. But then also in the stationarification stat(M O (D c (M))) of M O (D c (M)) there would be the edge (i, t i + ∆t) * - * (j, t j + ∆t) for all ∆t. Hence, M could not equal M O (D c (M)).</p><p>Given these considerations, the canonical ts-DAG D c (M) of a ts-DMAG M should instead only take into account the edges in the stationarification stat(M) of M. We are thus lead to the following definition, which for use further below is not restricted to ts-DMAGs but more generally applies to acyclic directed mixed graphs. . Let G be an acyclic directed mixed graph with time series structure and let</p><formula xml:id="formula_21">V = I × T with T = {t -τ | 0 ≤ τ ≤ p} be its set of vertices. Denote with E stat the set of edges of stat(G). The canonical ts-DAG associated to G, denoted as D c (G), is the graph D c (G) = (V ca , E ca ) defined as follows: 1. Its set of vertices is V ca = (I ∪ J) × Z, where J = {(i, j, τ ) | ((i, t -τ ), (j, t)) ∈ E stat ↔ }. The variable index set is I ∪ J and the time index set is Z. 2. Its set of edges E ca = E ca → are for all ∆t ∈ Z • (i, t -τ + ∆t)→(j, t + ∆t) for all ((i, t -τ ), (j, t)) ∈ E stat → and • ((i, j, τ ), t + ∆t)→(i, t + ∆t) for all (i, j, τ ) ∈ J and • ((i, j, τ ), t -τ + ∆t)→(j, t + ∆t) for all (i, j, τ ) ∈ J.</formula><p>Figure <ref type="figure" target="#fig_12">5</ref> illustrates canonical ts-DAGs. Intuitively, the canonical ts-DAG D c (G) of G is obtained in three steps: First, replace G by its stationarification stat(G). Second, in stat(G) replace every bidirected edge (i, t jτ )↔(j, t j ) with (i, t jτ )←((i, j, τ ), t jτ )→(j, t j ) where ((i, j, τ ), t jτ ) is an additionally inserted, unobserved vertex. Third, repeat this structure into the infinite past and future according to the repeating edges property. This intuition identifies the vertices ((i, j, τ ), s) with (i, j, τ ) ∈ J and s ∈ Z as analogs of the unobserved vertices l ij ∈ L in standard canonical DAGs (see Def. 4.11 above) and, in addition, means that the time series indexed by J are treated as unobservable. The key difference between standard canonical DAGs and canonical ts-DAGs is the first of the three steps, i.e., the application of stationarification. A similarity is that also in canonical ts-DAGs there are no edges into unobservable vertices and hence no edges between two unobservable vertices.</p><p>Canonical ts-DAGs are indeed ts-DAGs and, by means of the following result, yield the desired generalization of Lemma 4.12. LEMMA 4.14. Let D be a ts-DAG with variable index set I.</p><formula xml:id="formula_22">Let I O ⊆ I and T O = {t -τ | 0 ≤ τ ≤ p} with p ≥ 0. Then, M O (D) = M O (D c (M O (D))) with O = I O × T O .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REMARK (on Lemma 4.14). The lemma involves two different MAG latent projections:</head><p>First, the projection of the ts-DAG D to the ts-DMAG M O (D). Second, the projection of the canonical ts-DAG</p><formula xml:id="formula_23">D c (M O (D)) of M O (D) to M O (D c (M O (D))).</formula><p>In the first projection, the time series indexed by I \ I O are unobservable. In the second projection, the time series indexed by the set J are unobservable. In both projections, all vertices before tp and after t are temporally unobserved. However, since the set of observed variables is the same in both projections (namely O), no confusion arises when writing</p><formula xml:id="formula_24">M p (D) = M p (D c (M p (D))) instead of M O (D) = M O (D c (M O (D)))</formula><p>. From here on we adopt this notation. Lemma 4.14 says that the composition of creating the canonical ts-DAG and then projecting back to the original vertices is the identity operation on the space of ts-DMAGs, see Fig. <ref type="figure" target="#fig_13">6</ref>. This result is far from obvious for two reasons: First, if an edge (i,</p><formula xml:id="formula_25">t i ) * - * (j, t j ) in a ts- DMAG M p (D) is not in the stationarified ts-DMAG M p st (D) then in the canonical ts-DAG D c (M p (D)) there is neither (i, t i )→(j, t j ) nor (i, t i )←(j, t j ) nor (i, t i )←(l ij , t ij )→(j, t j ) with (l ij , t ij ) unobservable. Hence, the edge (i, t i ) * - * (j, t j ) needs to appear in the MAG latent projection M p (D c (M p (D))) of D c (M p (D)</formula><p>) in a non-trivial way, namely because of marginalizing over the temporally unobserved vertices. Second, this marginalization over the vertices before tp must not create superfluous edges. EXAMPLE 4.15. The example in Fig. <ref type="figure" target="#fig_14">7</ref> illustrates Lemma 4.14. This example also shows that the original ts-DAG D and the canonical ts-DAG D c (M p (D)) need not be equal.</p><p>We stress that Lemma 4.14 holds for arbitrary ts-DAGs D. In particular, in D there may be what in <ref type="bibr">Malinsky and Spirtes (2018)</ref> is referred to as "auto-lag confounders", namely unobservable autocorrelated component time series L, i.e., L t-τ →L t with L unobservable. 4.6. A necessary and sufficient condition that characterizes ts-DMAGs. Lemma 4.14 readily implies the following characterization of ts-DMAGs as a subclass of DMAGs with time series structure by a single necessary and sufficient condition.  </p><formula xml:id="formula_26">(D c (M 2 )) in part f), which is a proper supergraph of M 2 . The edge O 2 t-1 ↔O 1 t in M 1 (D c (M 2 )) is due to the green colored inducing path O 2 t-1 ←O 2 t-2 ←O 1 t-2 ←L (1,1,1) t-2 →O 1 t-1 ←L (1,1,1) t-1 →O 1 t in D c (M 2 ).</formula><p>Importantly, the DMAGs considered in <ref type="bibr">Examples 4.18 and 4.</ref>19 obey all necessary conditions given in <ref type="bibr">Lemmas 3.7 and 4.4</ref>. The condition M = M p (D c (M)) is thus strictly stronger than even the combination of all these necessary conditions. This observation clearly demonstrates the significance and non-triviality of Theorem 1.</p><p>As an alternative to Theorem 1, we also characterize ts-DMAGs as a subclass of directed mixed graphs with time series structure. THEOREM 2. Let G be a directed mixed graph with time series structure and time index set</p><formula xml:id="formula_27">T = {t -τ | 0 ≤ t ≤ p}. Then G is a ts-DMAG, i.e., there is a ts-DAG D such that G = M p (D) if and only if G is acyclic and G = M p (D c (G)).</formula><p>Theorem 2 is even stronger than Theorem 1 because Theorem 2 does not require the graph G to be ancestral and/or maximal. Acyclicity, however, is needed because the definition of canonical ts-DAGs D c (G) requires G to be acyclic, as does the notion of d-separation.  </p><formula xml:id="formula_28">M p (D c (M p st (D))) of the canonical ts-DAG D c (M p st (D)) of the stationarification M p st (D) = stat(M p (D)) of M p (D), i.e., M p (D) = M p (D c (M p st (D))).</formula><p>According to Lemma 4.20 one can always uniquely determine M p (D) from M p st (D). A ts-DMAG M p (D) and its stationarification M p st (D) thus carry the exact same information about the underlying ts-DAG D. In this sense M p (D) and M p st (D) are, if interpreted in the correct way, equivalent descriptions.</p><p>Lastly, we also arrive at two characterizations of stationarified ts-DMAGs.</p><p>LEMMA 4.21. Let M be a DMAG with time series structure and time index set</p><formula xml:id="formula_29">T = {t -τ | 0 ≤ t ≤ p}. Then, M is a stationarified ts-DMAG, i.e., there is a ts-DAG D such that M = M p st (D) if and only if M = M p st (D c (M)).</formula><p>LEMMA 4.22. Let G be a directed mixed graph with time series structure and time index set</p><formula xml:id="formula_30">T = {t -τ | 0 ≤ t ≤ p}. Then, G is a stationarified ts-DMAG, i.e., there is a ts-DAG D such that G = M p st (D) if and only if G is acyclic and G = M p st (D c (G)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.8.</head><p>Comparison with previously considered model classes. The author is aware of two distinct classes of graphical models based on DMAGs that have so far been used to represent time-lag specific causal relationships in time series with latent confounders. Here, we show that both these model classes are strictly larger than the class of ts-DMAGs.</p><p>The first previously used model class, employed by the tsFCI algorithm from <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, are DMAGs with time series structure that are time ordered and have repeating orientations as well as past-repeating adjacencies. <ref type="bibr">Lemmas 3.7 and 4.4</ref> show that ts-DMAGs fall into this model class. The reverse, however, is not true: The graphs in part b) of Fig. 5. Markov equivalence classes of ts-DMAGs and causal discovery. This section discusses the implications of the concepts and results of Sec. 4 for causal discovery. To this end, Def. 5.7 in Sec. 5.4 introduces time series DPAGs (ts-DPAGs) as graphs that represent Markov equivalence classes of ts-DMAGs. Time series DPAGs are refinements of DPAGs obtained by incorporating our background knowledge about the data generating processnamely that the data are generated by a process as in eq. ( <ref type="formula">1</ref>) and that the observed time steps are regularly (sub-)sampled. We further introduce several alternative refinements of DPAGs, see Secs. 5.1 and 5.2, concretely DPAGs which represent Markov equivalence classes of stationarified ts-DMAGs and DPAGs which incorporate only some of the necessary properties of ts-DMAGs as background knowledge. As we show, these alternative DPAGs carry less information about the underlying ts-DAG than ts-DPAGs do. Using the introduced terminology, in Sec. 5.5 we discuss and compare three algorithms for independence-based causal discovery in time series with latent confounders and show that none of them learns ts-DPAGs. That is, all of these algorithms are conceptually suboptimal as they fail to learn causal properties of the underlying ts-DAG that in principle can be learned. As opposed to that, Algorithm 1 in Sec. 5.6 does learn ts-DPAGs and in this sense is complete. Another important result is Theorem 3 in Sec. 5.3, according to which DPAGs based on stationarified DMAGs carry less causal information than DPAGs based on non-stationarified DMAGs. Theorem 3 corrects an erroneous claim that has appeared in the literature, see the explanation below Theorem 3 in Sec. 5.3 and the discussion of the SVAR-FCI algorithm in Sec. 5.5 for more details.</p><p>5.1. Background knowledge and DPAGs. Markov equivalent DMAGs by definition have the same m-separations and thus cannot be distinguished by statistical independencies. They might, however, be distinguished if additional assumptions are made. One type of such assumptions is background knowledge, i.e., the assertion that DMAGs with certain properties can be excluded as these are in conflict with a priori knowledge about the system of study. <ref type="bibr">DEFINITION 5.1 (Background knowledge, cf. Mooij and Claassen (2020)</ref>). A background knowledge A is a Boolean function on the set of all DMAGs. If A(M) = 1, then M is said to be consistent with A, else it is said to be inconsistent with A. • P has the same skeleton (i.e., the same set of adjacencies) as M and • every non-circle mark in P is also in M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A DPAG</head><formula xml:id="formula_31">P for M is called maximally informative (m.i.) with respect to [M] ′ ⊆ [M] if • every non-circle mark in P is in every element of [M] ′ and</formula><p>• for every circle mark in P there are M 1 , M 2 ∈ [M] ′ such that in M 1 there is a tail mark and in M 2 ∈ [M] ′ there is a head mark instead of the circle mark. 3. The maximally informative (m.i.) DPAG with respect to A, denoted as P(M, A), is the m.i. DPAG of M with respect to [M] A . 4. The conventional m.i. DPAG for M is the m.i. DPAG P(M) = P(M, A ∅ ), where A ∅ is the "empty" background knowledge for which A ∅ = 1 constant.</p><p>To compare different background knowledges and the accordingly refined DPAGs, we employ the following terminology. DEFINITION 5.3 (Stronger/weaker background knowledge, more/less informative DPAG). Let A 1 and A 2 be background knowledges, and let P 1 and P 2 be DPAGs for M. We say. . .</p><formula xml:id="formula_32">• . . . A 1 is stronger than A 2 and A 2 is weaker than A 1 if A 1 (M) = 1 implies A 2 (M) = 1.</formula><p>• . . . P 1 is more informative than P 2 and P 2 is less informative than P 1 if every circle mark in P 1 is also in P 2 .</p><p>It follows that P(M, A 1 ) is more informative than P(M, A 2 ) if A 1 is stronger than A 2 . By construction P(M, A) is the most informative DPAG for M that can be learned from statistical independencies together with the background knowledge A. The first background knowledge A D is as much background knowledge as is available in the time series setting defined in Sec. 3.1. In Sec. 5.4 we will use A D to define ts-DPAGs. The second background knowledge A stat D is the equivalent background knowledge when working with stationarified ts-DMAGs M p st (D) instead of ts-DMAGs M p (D). We will use A stat D to compare causal discovery based on M p (D) with causal discovery based on M p st (D). Given that a ts-DMAG M p (D) and its stationarification M p st (D) are in one-to-one correspondence, see Sec. 4.7, one might also expect the corresponding DPAGs to carry the same information. Interestingly, as we will show in Sec. 5.3, this expectation is incorrect. The third and fourth background knowledges A ta and A to equally apply to both standard and stationarified ts-DMAGs. They are included for comparison with existing causal discovery algorithms.</p><p>The four specified background knowledges compare as follows: Since both ts-DMAGs M p (D) and stationarified ts-DMAGs M p st (D) are time ordered and have repeating ancestral relationships, both A D and A stat D are stronger than A ta . Since repeating ancestral relationships imply repeating orientations, A ta is stronger than A to . For stationarified ts-DMAGs M p st (D), however, A ta and A to are equivalent (as follows from Lemma 4.3). In our notation this equivalence is expressed as P(M p st (D), A ta ) = P(M p st (D), A to ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.</head><p>DPAGs of ts-DMAGs M p (D) carry more information than DPAGs of stationarified ts-DMAGs M p st (D). In this subsection we show that, when working with the background knowledges specified in Def. 5.4, DPAGs of ts-DMAGs can never carry less but may carry more information about the underlying ts-DAG than DPAGs of stationarified ts-DMAGs. This is so despite the fact that, as explained in Sec. 4.7, a ts-DMAG and its stationarification are in one-to-one correspondence. Towards proving the claim we first note the following. LEMMA 5.5. Let D be a ts-DAG and let A ∈ {A D , A ta , A to }. Then, the graph stat(P(M p (D), A)) is a DPAG for M p st (D).</p><p>In particular, both DPAGs P(M p st (D), A stat ) and stat(P(M p (D), A)) have the same adjacencies. Moreover, it is well-defined to ask whether one of the two DPAGs is more informative than the other. The following result answers this question. THEOREM 3. Let D be a ts-DAG and let (A, A stat ) either be (A to , A to ) or (A ta , A ta ) or (A D , A stat D ). Then: 1. Every non-circle mark (head or tail) in P(M p st (D), A stat ) is also in stat(P(M p (D), A)). 2. Every non-circle mark in P(M p st (D), A stat ) is also in P(M p (D), A). 3. There are cases in which a non-circle mark that is in stat(P(M p (D), A)) is not also in P(M p st (D), A stat ). 4. There are cases in which a non-circle mark that is in P(M p (D), A) is not also in P(M p st (D), A stat ), even regarding adjacencies that are shared by both graphs.</p><p>Theorem 3 contradicts the opposite claim in <ref type="bibr">Malinsky and Spirtes (2018)</ref> according to which more unambiguous edge orientations (heads or tails) may be inferred if, as licensed by the assumption of causal stationarity, the property of repeating adjacencies is enforced in causal discovery; see Sec. 5.5 for more details. The following example illustrates Theorem 3.   <ref type="bibr">Zhang (2006, pp. 81f</ref>). We hence get the DPAG P(M p (D), A ta ) shown in part d). Since there are no circle marks left, P(M p (D), A ta ) here equals the DPAG P(M p (D), A D ) in part e). <ref type="foot" target="#foot_2">2</ref> The graphs stat(P(M p (D), A to )), stat(P(M p (D), A ta )) and stat(P(M p (D), A D )) are respectively obtained by removing the edge between O    <ref type="figure">A</ref>)), if one prefers graphs with repeating edges-rather than with DPAGs P(M p st (D), A stat ) of stationarified ts-DMAGs. One might argue, though, that the additional ambiguous orientations (i.e., circle marks) which P(M p st (D), A stat ) has as compared to P(M p (D), A) might turn into unambiguous orientations (i.e., head or tail marks) in P(M p st (D), A stat ) for an increased length p &gt; p of the observed time window. <ref type="foot" target="#foot_3">3</ref> However, increasing p to p also increases the number of observed vertices and thus yields a higherdimensional causal discovery problem. Having more observed vertices typically hurts finitesample performance of causal discovery, see, e.g., the simulation studies in <ref type="bibr">Gerhardus and Runge (2020)</ref>. On the other hand, algorithms that work with stationarified ts-DMAGs rather than ts-DMAGs may scale more favorably with the length p of the observed time window because they remove the edges O i t-∆t-τ * - * O j t-∆t for all ∆t as soon as the edge O i t-τ * - * O j t is removed and therefore typically make fewer independence tests. From a practical perspective there thus is a trade-off between working with ts-DMAGs vs. working with stationarified ts-DMAGs, which calls for empirical evaluation in future work. REMARK (on Def. 5.7). The equivalence of regular sampling and regular subsampling, see Sec. 4.2, carries over to ts-DPAGs. We hence restrict to regular sampling without loss of generality and write P p (D) for</p><formula xml:id="formula_33">P O (D) with O = I O × T O and T O = {t -τ | 0 ≤ τ ≤ p}.</formula><p>The following example discusses a case in which the use of the strongest background knowledge A D leads to strictly more unambiguous edge orientations than A ta . We thus cannot replace A D with A ta in the definition of ts-DPAGs without losing information.    <ref type="foot" target="#foot_4">4</ref> Thus, the causal effect of O 1 t-1 on O 1 t is identifiable and can be estimated from observations by adjusting for the empty set. Importantly, if we interpret M 1 (D) not as a ts-DMAG but as a "standard" DMAG, then the causal effect of O 1 t-1 on O 1 t would be unidentifiable as follows from Lemma 10 in <ref type="bibr">Zhang (2008a)</ref>.</p><p>Example 5.8 clearly demonstrates the importance of our characterization of ts-DMAGs for the tasks of causal discovery and causal inference. Moreover, the following result shows that ts-DPAGs are complete with respect to ancestral relationships. LEMMA 5.9. If in a ts-DPAG P p (D) there is an edge (i, t i )•- * (j, t j ), then there are ts-DAGs D 1 and D 2 such that both ts-DMAGs M p (D 1 ) and M p (D 2 ) are Markov equivalent to the ts-DMAG M p (D) and (i, t i ) ∈ an((j, t j ), D 1 ) and (i, t i ) / ∈ an((j, t j ), D 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.</head><p>Existing causal discovery algorithms do not learn ts-DPAGs. To the best of the author's knowledge, so far there is no causal discovery algorithm that learns ts-DPAGs P p (D). Hence, all existing causal discovery algorithms fail to learn some causal relationships that can be learned. This failure also applies to the independence-based algorithms tsFCI from <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, SVAR-FCI from <ref type="bibr">Malinsky and Spirtes (2018)</ref> and LPCMCI from Gerhardus and Runge (2020). <ref type="foot" target="#foot_5">5</ref> Below, we discuss and compare these three algorithms conceptually (but also note the practical considerations discussed at the end of Sec. 5.3).</p><p>The tsFCI algorithm from <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref> refines the well-known FCI algorithm, see <ref type="bibr" target="#b39">Spirtes, Meek and Richardson (1995)</ref>, <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref>, <ref type="bibr">Zhang (2008b)</ref>, to structural processes as in eq. ( <ref type="formula">1</ref>). To this end, see the blue colored instructions in parts 2(a) and 2(b) of Algorithm 1 in <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, tsFCI imposes time order from the start and enforces repeating orientations at all steps. In addition, see the blue colored instructions in parts 1(b) and 1(c) of Algorithm 1 in <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>, tsFCI excludes future vertices from conditioning sets and uses repeating separating sets to avoid unnecessary independence tests (these latter two modifications are, however, only relevant computationally and statistically but not conceptually). Importantly, <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref> introduces two variants of the algorithm. The first variant, which we call tsFCI l (with 'l' for 'lagged'), assumes that in the data-generating ts-DAG there are no contemporaneous edges and hence orients all contemporaneous edges in the DPAG as bidirected. This first variant is as specified by Algorithm 1 in <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref>. However, in section 6 of <ref type="bibr" target="#b10">Entner and Hoyer (2010)</ref> (see, in particular, their footnote 3) the authors explain the minor modifications that have to be done when not making the additional assumption of no contemporaneous causation. Moreover, there they also show an application of the resulting more general variant. We refer to this second variant as tsFCI l+c (with 'l + c' for 'lagged plus contemporaneous'). To summarize, in our terminology tsFCI l+c attempts to learn the DPAG P(M p (D), A to ) of the ts-DMAG M p (D). Since P(M p (D), A to ) may contain circle marks that are not in the ts-DPAG P p (D) = P(M p (D), A D ), see Examples 5.6 and 5.8, tsFCI l+c does not learn all ancestral relationships that can be learned when using the available background knowledge A D . From Example 5.6 we even conclude that tsFCI l+c learns fewer orientations as can be learned with the weaker background knowledge A ta .</p><p>As compared to tsFCI l+c , the more recent SVAR-FCI algorithm from Malinsky and Spirtes (2018) enforces repeating adjacencies by removing the edges</p><formula xml:id="formula_34">O i t-∆t-τ * - * O j t-∆t</formula><p>for all ∆t as soon as the edge O i t-τ * - * O j t is removed-even in cases where there is no associated separating set in the observed time window. <ref type="foot" target="#foot_6">6</ref> This modification is achieved by the respective second lines in the "then"-clauses in steps 5 and 11 of Algorithm 3.1 in Malinsky and Spirtes (2018). Consequently, SVAR-FCI finds a skeleton which has repeating adjacencies, i.e., SVAR-FCI finds the skeleton of the stationarified ts-DMAG M p st (D) rather than the skeleton of the ts-DMAG M p (D). On the skeleton of M p st (D) the algorithm then applies the FCI orientation rules, augmented with the background knowledge of time order and repeating orientations. In our terminology SVAR-FCI hence attempts to learn the DPAG P(M p st (D), A to ) of the stationarified ts-DMAG M p st (D). Now recall Theorem 3, which says that all unambiguous edge orientations in this DPAG P(M p st (D), A to ) are also in P(M p (D), A to )-the one learned by tsFCI l+c -while there are cases in which the opposite is not true. Thus, if ground-truth knowledge of (conditional) independencies is given, SVAR-FCI can never learn more unambiguous edge orientations than tsFCI l+c while there are cases in which it learns strictly fewer. The additional edge removals thus actually have the opposite effect of what was intended in <ref type="bibr">Malinsky and Spirtes (2018)</ref>. Moreover, also SVAR-FCI fails to learn all identifiable ancestral relationships of the underlying ts-DAG. EXAMPLE 5.10. Assume ground-truth knowledge about (conditional) independencies. When applied to the ts-DMAG in part a) of Fig. <ref type="figure" target="#fig_23">9</ref>, tsFCI l+c returns the graph in part c) whereas SVAR-FCI returns the graph in part h) with strictly fewer unambiguous edge marks. This difference is relevant: From tsFCI l+c 's output we can conclude that O  <ref type="bibr">4.4)</ref>. In fact, see Theorem 3, one can always post-process the output of tsFCI l+c by stationarification stat(•) to obtain a graph that, compared to the graph learned by SVAR-FCI, has the same adjacencies and the same or more unambiguous edge orientations. In the current example, this post-processing step amounts to removing the edge O 1 t-1 •→O 2 t-1 from the graph in part c) of Fig. <ref type="figure" target="#fig_23">9</ref>.</p><p>Algorithm 1 Learning ts-DPAGs 1: Apply any causal discovery algorithm on the time window [tp, t] that determines a DPAG P * for M p (D) which is at least as informative as the conventional m.i. DPAG P(M p (D)). The tsFCI l+c algorithm meets this requirement, whereas SVAR-FCI and LPCMCI do not meet this requirement. 2: Let M * be the set of all DMAGs that are represented by and are Markov equivalent to the DPAG P * . This step can, e.g., be done with the Zhang MAG listing algorithm described in <ref type="bibr" target="#b22">Malinsky and Spirtes (2016)</ref>. 3: Let M be the set of all DMAGs M ∈ M * for which M = M p (Dc(M)). This step can be executed as follows: For each M ∈ M * , first, construct the canonical ts-DAG Dc(M) by applying Def. 4.13 and, second, apply the MAG latent projection to determine the ts-DMAG M p (Dc(M)) and, third, check for equality of the graphs M and M p (Dc(M)). 4: Let P be the m.i. DPAG with respect to the set M. Note that parts 1 and 2 of Def. 5.2 specify how P is determined from M. 5: return ts-DPAG P = P p (D)</p><p>The LPCMCI algorithm from Gerhardus and Runge (2020) applies several modifications to SVAR-FCI that significantly improve the finite-sample performance. The infinite sample properties are unchanged, however. Thus also LPCMCI in general learns fewer orientations than tsFCI l+c and fails to learn all ancestral relationships that can be learned. 5.6. ts-DPAGs can be learned from data. In this subsection we show that ts-DPAGs can, at least in principle, be learned from data. In fact, using the characterization of ts-DMAGs by Theorem 1, we can immediately write down Algorithm 1 for this purpose.</p><p>Practically, however, finding the set of candidate DMAGs M * in step 2 is expected to become computationally infeasible for large graphs P * . This expectation is based on the empirical finding in <ref type="bibr" target="#b22">Malinsky and Spirtes (2016)</ref> according to which the Zhang MAG listing algorithm (there used not for causal discovery but for causal effect estimation and in a nontemporal setting) becomes too slow for graphs with about 15 to 20 vertices. On the contrary, when using tsFCI l+c in step 1, the DPAG P * already incorporates the background knowledge A to of time order and repeating orientations. Hence, P * will tend to have fewer circle marks than a typical DPAG in the non-temporal setting. Therefore, Algorithm 1 might be feasible for yet larger graphs. Nevertheless, it would be desirable to instead derive orientation rules that impose the background knowledge A D directly on P * and thus entirely circumvent the need to determine the set M * . Moreover, recalling from the remark on Def. 3.6, an implementation of the projection procedure required for step 3 is possible but non-trivial and will be left to future work, see <ref type="bibr" target="#b15">Gerhardus et al. (2023)</ref>. The following example illustrates Algorithm 1. EXAMPLE 5.11. Consider the graph P(M 1 (D), A ta ) in part b) of <ref type="bibr">Fig. 10,</ref><ref type="bibr">which</ref> in this example equals P(M 1 (D), A to ). Given ground-truth knowledge of (conditional) independencies, this graph is the output of tsFCI l+c (and of SVAR-FCI and LPCMCI) on any ts-DAG D that projects to M 1 (D) in part a) of the figure. Such D exists, e.g. the canonical ts-DAG D c (M 1 (D)) of M 1 (D). There is exactly one circle mark in</p><formula xml:id="formula_35">P * = P(M 1 (D), A ta ), namely on O 1 t-1 •→O 1 t .</formula><p>This circle mark can be oriented either as a tail (O </p><formula xml:id="formula_36">* = {M 1 , M 2 }. Moving to step 3, the first candidate M 1 passes the check M 1 = M 1 (D c (M 1 )), whereas (see Example 5.8) M 2 ̸ = M 1 (D c (M 2 )</formula><p>) for the second candidate M 2 . Thus M = {M 1 }. Since there is only a single element M 1 in M, this DMAG M 1 according to parts 1 and 2 of Def. 5.2 equals the ts-DPAG P 1 (D). Noting that P 1 (D) (learned by Algorithm 1) has an additional unambiguous edge mark as compared to P(M 1 (D), A to ) (learned by tsFCI l+c , SVAR-FCI and LPCMCI), we see that Algorithm 1 is indeed more informative than the existing algorithms. REMARK (on <ref type="bibr">Example 5.11)</ref>. The example has two non-generic properties. First, see <ref type="bibr">Gerhardus (2023, Fig. C and Example B.15)</ref>, in general there can be circle marks in the ts-DPAG P p (D). Second, the ts-DMAG M p (D) = M 1 (D) here has repeating edges and thus equals M p st (D) = M 1 st (D). Only if the equality M p (D) = M p st (D) holds, then also SVAR-FCI and LPCMCI learn the DPAG P * = P(M p (D), A to ), which then also necessarily equals P(M p (D), A ta ). In general, however, M p (D) and M p st (D) are not equal and neither SVAR-FCI nor LPCMCI may be used for step 1 of Algorithm 1, see Fig. <ref type="figure" target="#fig_23">9</ref> and Example 5.6. 6. Discussion. In this paper, we developed and analyzed ts-DMAGs, a class of graphical models for representing time-lag specific causal relationships and independencies among finitely many regularly (sub-)sampled time steps of causally stationary multivariate time series with unobserved components. As a central result, Theorems 1 and 2 completely characterize ts-DMAGs. Examples demonstrated that ts-DMAGs constitute a strictly smaller class of graphical models than the graphs that have previously been employed in the literature, see Sec. 4.8 for details. At the same time, using ts-DMAGs does not require additional assumptions or restrictions on the data-generating process. From ts-DMAGs one can thus draw stronger causal inferences than from the previously employed model classes, both in causal discovery and causal effect estimation. In addition, we defined ts-DPAGs as representations of Markov equivalence classes of ts-DMAGs. Time series DPAGs contain as much information about the ancestral relationships as can in principle be learned from observational data under the standard assumptions of independence-based causal discovery. We then showed that current time series causal discovery algorithms do not learn ts-DPAGs, i.e., they fail to learn some causal relationships that can be learned. As opposed to that, Algorithm 1 does learn ts-DPAGs. With Theorem 3 we corrected the incorrect claim from the literature that causal discovery on stationarified DMAGs gives more unambiguous edge orientations than causal discovery on non-stationarified DMAGs-in fact, the opposite is true. We envision that these results will be used to improve time series causal inference methods that resolve time lags, which in turn can have applications in diverse scientific and technical domains.</p><p>The results presented here point to various directions of future research. First, it would be valuable to consider the causal discovery problem in more detail. In particular, it is desirable to develop orientation rules that impose the background knowledge of an underlying ts-DAG A D without the need for listing all DMAGs consistent with A D . Second, it remains open to characterize the causal inferences that can be drawn from ts-DMAGs and ts-DPAGs. As shown by Example 5.8, deriving such a characterization is a non-trivial task that goes beyond the corresponding task in the non-temporal setting. Third, one may analyze the additional restrictions and causal inferences that follow when, as opposed to this work, assumptions on the connectivity pattern of the underlying ts-DAG are imposed. Lastly, it is desirable to generalize our results to cases with cyclic causal relationships and selection variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENT TO "CHARACTERIZATION OF CAUSAL ANCESTRAL GRAPHS FOR TIME SERIES WITH LATENT CONFOUNDERS"</head><p>BY ANDREAS GERHARDUS 1,a</p><p>1 German Aerospace Center, Institute of Data Science, a andreas.gerhardus@dlr.de</p><p>This Supplementary Material contains: First, a glossary of abbreviations and frequently used symbols. Second, theoretical results that were omitted from the main text because of space constraints. Third, proofs of all theoretical results presented in the main text together with various auxiliary results that are used in these proofs.</p><p>A. Glossary of abbreviations and frequently used symbols. The following glossary might be helpful for reading the main paper and this Supplementary Material. B. Omitted results. This section presents several theoretical results that were omitted from the main text due to space constraints. B.1. ts-DMAGs are a generalization of DMAGs. Consider an arbitrary DMAG M nt with vertex set O nt and without time series structure (the subscript "nt" stands for "nontemporal"). As proven in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>, there is DAG D nt over some vertex set V nt ⊃ O nt such that M nt = M Ont (D nt ). Now define D as the ts-DAG that consists of disconnected copies of D nt at every time step s ∈ Z, i.e., there are no lagged edges in D and for all s ∈ Z its induced subgraph on V nt × {s} is D nt . It then immediately follows that the ts-DMAG M IO×TO with I O = V nt and T O = {t} equals M nt .</p><p>This consideration identifies ts-DMAGs as a proper generalization of DMAGs and thereby shows that all statements about ts-DMAGs also to apply to DMAGs as a special case. , the graph obtained by removing all vertices after t and all edges involving these vertices from D. Then, M IO×TO (D) = M IO×TO (D ≤t ), i.e., before applying the MAG latent projection one may simply ignore the part of D that is after t.</p><p>PROOF OF LEMMA B.1. Let (i, t i ) and (j, t j ) with t i , t j ≤ t be distinct vertices in D. Then, (i, t i ) and (j, t j ) are non-adjacent in D if and only if (i, t i ) ⊥ ⊥ (j, t j ) | S in D with S = pa((i, t i ), D) ∪ pa((j, t j ), D) \ {(i, t i ), (j, t j )}, see <ref type="bibr">Verma and Pearl (1990, Lemma 1)</ref>. Moreover, all vertices in S are before or at t due to time order of D, i.e., S is a subset of V ≤t = I × T ≤t . Consequently, (i, t i ) and (j, t j ) are non-adjacent in D if and only if there is a subset S ′ ⊆ V ≤t such that (i, t i ) ⊥ ⊥ (j, t j ) | S ′ in D. This observation implies that the graphs M V≤t (D) and D ≤t have the same skeleton, and the equality M V≤t (D) = D ≤t follows because both M V≤t (D) and D ≤t have the same ancestral relationships among vertices in V ≤t as D. From M V≤t (D) = D ≤t the statement follows with the commutativity of the marginalization process as stated by Theorem 4.20 in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>. This result, which follows from time order and d-separation, has the intuitive interpretation that the future need not be known in order to reason about the past and present. B.3. Temporal confounding. As explained in Sec. 3.4 of the main text, in the construction of M IO×TO (D) all vertices before tp, i.e., in (-∞, tp -1] are on purpose treated as unobserved-even if they are observable and hence become observed for some p &gt; p. As the following example shows, such temporally unobserved observable vertices before tp can act as latent confounders of observed vertices. EXAMPLE B.2. In the ts-DAG D 1 shown in part a) of Fig. <ref type="figure" target="#fig_1">2</ref> of the main text, the temporally unobserved vertex O 1 t-3 confounds the observed vertices</p><formula xml:id="formula_37">O 1 t-2 and O 2 t-2 through the path O 1 t-2 ← O 1 t-3 → O 2 t-2</formula><p>. This argument remains valid even without the unobservable time series L 1 .</p><p>In the time series setting one thus effectively always deals with the case of latent confounding, even if component time series are observable. This observation further demonstrates the importance of conceptually understanding the latent variable setting as approached in this paper.</p><p>We also note that it is precisely this type of confounding that gives rise to edges which are in M p (D) but not in M p st (D). PROOF OF LEMMA B.4. 1. Consider an edge ((i, t i ), (j, t j )) ∈ E • in stat(G) and let ∆t be such that (i, t i + ∆t), (j, t j + ∆t) ∈ V. Using the second point in Def. 4.6 twice, we first get ((i, t i + ∆t ′ ), (j, t j + ∆t ′ )) ∈ E • in G for all ∆t ′ for which (i, t i + ∆t ′ ), (j, t j + ∆t ′ ) ∈ V and thus ((i, t i + ∆t), (j,</p><formula xml:id="formula_38">t j + ∆t)) ∈ E • in stat(G).</formula><p>2. Let G ′ ⊆ G have repeating edges and assume G ′ ⊆stat(G). Since both G ′ and stat(G) are subgraphs of G, adjacencies that are shared by G ′ and stat(G) correspond to edges of the same type. Thus, G ′ ⊆stat(G) implies that there is an adjacency in G ′ which is not in stat(G), i.e., ((i, t i ), (j, t j )) ∈ E in G ′ and ((i, t i ), (j, t j )) / ∈ E in stat(G). Since G ′ has repeating edges by assumption, ((i, t i ), (j, t j )) ∈ E in G ′ implies that ((i, t i + ∆t), (j, t j + ∆t)) ∈ E in G for all ∆t for which (i, t i + ∆t), (j, t j + ∆t) ∈ V. But then the second point in Def. 4.6 gives ((i, t i ), (j, t j )) ∈ E in stat(G). Contradiction. B.5. Why the case of no unobservable vertices remains special. As discussed in Sec. B.3, even if there are no unobservable time series one in general still is in the setting of latent confounding. It is worth noting, though, that the case of no unobservable vertices remains special:  B.6. Different DMAGs with the same stationarification cannot both be ts-DMAGs. The following result is an immediate consequence of the one-to-one correspondence between a ts-DMAG and its stationarification (see Sec. 4.7 in the main text). LEMMA B.6. Let M 1 and M 2 be DMAGs with time series structure such that M 1 ̸ = M 2 and stat(M 1 ) = stat(M 1 ). Then, at least one of M 1 and M 2 is not a ts-DMAG. t-1 , O 1 t ) is not subject to unobserved confounding, that is, in D ′ there is no inducing path (relative to the set of observed vertices) between O 1 t-1 and O 1 t that is into O 1 t-1 . Consequently, from M 1 (D) we can conclude that the causal effect of O 1 t-1 on O 1 t is identifiable and can be estimated from observations by adjusting for the empty set. Moreover, since the ts-DPAG P 1 (D) in part c) of Fig. <ref type="figure" target="#fig_25">10</ref> is equal to the ts-DMAG M 1 (D), we can draw this conclusion not only from M 1 (D) but also from P 1 (D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REMARK (on</head><p>PROOF OF LEMMA B.8. First, we prove that the pair (O 1 t-1 , O 1 t ) is not subject to unobserved confounding. To this end, we begin by deriving the existence of certain paths in D ′ : For i = 1, 2, 3 let π ′ i be a copy of π i that is shifted backwards in time by one time step. These paths π ′ i exist due to the repeating edges property of D ′ . Then, the concatenation ) would be a collider-free path such that all its non end-point vertices, if any, are unobserved. Consequently, there would need to be an edge between O 2 t-1 and O 1 t in M 1 (D ′ ). Since the combination of points 6 and 7 in this enumeration contradicts point 4 of the enumeration, such a path π cannot exist. Consequently, O 1 t-1 and O 1 t are d-separated in G. The second rule of the do-calculus, e.g. <ref type="bibr">Pearl (2009)</ref>, thus gives that the interventional distribution</p><formula xml:id="formula_39">1. Since O 1 t-1 is an ancestor of O 1 t in D ′ according to the edge O 1 t-1 →O 1 t in M 1 (D ′ ), in D ′ there is a directed path π 1 from O 1 t-1 to O 1 t .</formula><formula xml:id="formula_40">ρ 1 = π ′ 1 (O 1 t-1 , O 1 t-2 ) ⊕ π ′ 2 ⊕ π ′ 3 is a collider-free path between O 1 t-1 and O 2 t-1 that is into both O 1 t-</formula><formula xml:id="formula_41">P (O 1 t | do(O 1 t-1 = o 1 t-1 )</formula><p>) is expressed in terms of the observational distribution as</p><formula xml:id="formula_42">P (O 1 t | do(O 1 t-1 = o 1 t-1 )) = P (O 1 t | O 1 t-1 = o 1 t-1 ). □ B.</formula><p>8. Increasing the number of observed time steps. The main text consides ts-DMAGs and ts-DPAGs on observed time windows [tp, t], where p ≥ 0 is arbitrary but fixed. In Sec. B.8.1 we first compare ts-DMAGs and ts-DPAGs on time windows of different length. We show that, as expected, the ts-DMAGs and ts-DPAGs on the longer time window can never contain less but may contain more information about the underlying ts-DAG than the ts-DMAGs and ts-DPAGs on the shorter time window. In Sec. B.8.2 we then define the notions of limiting ts-DMAGs and ts-DPAGs by allowing conditioning sets from the entire past. All proofs are given in Sec. F. B.8.1. Comparison of ts-DMAGs and ts-DPAGs on different observed time windows. Since the reference time step t is arbitrary and only time differences are relevant, we need only compare ts-DMAGs and ts-DPAGs on [tp, t] and [tp, t] with p &gt; p. To this end we use the following notation. DEFINITION B.9 (Subgraph of a ts-DMAG / ts-DPAG induced on time window). Let D be a ts-DAG, let p ≥ p ≥ 0, and let tp ≤ t 1 ≤ t 2 ≤ t. The induced subgraph of M p(D) (subgraph of P p(D)) on its subset of vertices within [t 1 , t 2 ] is denoted as M p,[t1,t2] (D) (denoted as P p,[t1,t2] (D)).</p><p>The additionally observed vertices in [t-p, t-p-1] enlarge the set of potential conditions and thus may lead to more d-separations among the originally observed vertices. We thus get the following result. LEMMA B.10. Let D be a ts-DAG and let p &gt; p ≥ 0. Then, up to relabeling vertices:</p><formula xml:id="formula_43">1. For all 0 ≤ ∆t &lt; p -p: M p,[t-p-∆t,t-∆t] (D) is a subgraph of M p (D). 2. M p,[t-p,t-p+p] (D) equals M p (D).</formula><p>3. There are cases in which M p,[t-p,t] (D) is a proper subgraph of M p (D).</p><p>Moving to a semantic level, we confirm the intuition that M p(D) cannot contain less but may contain more information about D than M p (D).</p><p>LEMMA B.11. Let D 1 and D 2 be ts-DAGs and let p &gt; p ≥ 0. Then:</p><formula xml:id="formula_44">1. If M p(D 1 ) = M p(D 2 ), then M p (D 1 ) = M p (D 2 ). 2. There are cases in which M p(D 1 ) ̸ = M p(D 2 ) and M p (D 1 ) = M p (D 2 ).</formula><p>In other words: Every inference about D that can be drawn from M p (D) can also be drawn from M p(D), whereas the converse need not be true. EXAMPLE B.12. Figure <ref type="figure">B</ref> shows the ts-DMAGs M 1 (D) and M 2 (D). These graphs conform with parts 1 and 2 of Lemma B.10 and prove its part 3. Further, the edge</p><formula xml:id="formula_45">O 1 t-2 →O 2 t in M 2 (D) shows that O 1 t-2 is an ancestor of O 2 t in D,</formula><p>which is a conclusion that cannot be drawn from M 1 (D).</p><p>Since ts-DPAGs P p (D) by definition have the same adjacencies as the corresponding ts-DMAGs M p (D), the effect of increasing p on their adjacencies is the same as for ts-DMAGs. Regarding edge orientations, Lemma B.11 raises the expectation that all unambiguous edge marks in P p (D) should also be in P p(D). This is expectation is indeed correct. LEMMA B.13. Let D be a ts-DAG and let p &gt; p ≥ 0. Let (i, t i ) and (j, t j ) be adjacent in both P p (D) and P p(D). Then: 1. If there is a non-circle mark on (i, t i ) * - * (j, t j ) in P p (D), then the same non-circle mark is also on (i, t i ) * - * (j, t j ) in P p(D). 2. There are cases in which there is a non-circle mark on (i, t i ) * - * (j, t j ) in P p(D) that is not on (i, t i ) * - * (j, t j ) in P p (D).</p><p>LEMMA B.14. Let D be a ts-DAG and let p &gt; p ≥ 0. Then:</p><p>1. Every circle edge mark in P p,[t-p,t] (D) is also in P p (D).</p><p>2. There are cases in which there is a non-circle edge mark in P p, [t-p,t] (D) that is not also in P p (D).</p><p>EXAMPLE B.15. Figure <ref type="figure">C</ref> shows the ts-DPAGs P<ref type="foot" target="#foot_7">foot_7</ref> (D) and P 2 (D) for a ts-DAG D. These graphs conform with parts 1 of Lemmas B.13 and B.14 and prove parts 2 of both these lemmas. For example, in  [t-p,t] (D) as well as the sequence ∆p → P p+∆p,[t-p,t] (D) convergence with respect to the discrete metric 1 on the space of ts-DMAGs, respectively space of ts-DPAGs. DEFINITION B.17 (Limiting ts-DMAG / ts-DPAG). Let D be a ts-DAG and let p ≥ 0. The limiting ts-DMAG M p lim (D), respectively limiting ts-DPAG P p lim (D), is the limit of the sequence ∆p → M p+∆p,[t-p,t] (D), respectively ∆p → P p+∆p,[t-p,t] (D), with respect to the discrete metric on the space of ts-DMAGs, respectively space of ts-DPAGs.</p><formula xml:id="formula_46">P 1 (D) there is O 1 t-1 •→O 1 t while in P 2 (D) there is O 1 t-1 →O</formula><p>See <ref type="bibr">Fig. D</ref>   t-p →O 2 t in M p lim (D). In <ref type="bibr">Malinsky and Spirtes (2018)</ref> this effect is discussed under the names of "auto-lag confounders" and "infinite-lag associations". </p><formula xml:id="formula_47">{V i t | 1 ≤ i ≤ n V , t ∈ Z}, i.e.</formula><p>, is indexed by the set I × Z where I = {1, . . . , n V }. This form shows that the causal graph D has time series structure with time index set Z, where vertex (i, t) ∈ I×Z corresponds to random variable V i t . Further, pa((i, t), D) = PA i t by definition of causal graphs and </p><formula xml:id="formula_48">PA i t ⊆ {V k t-τ | 1 ≤ k ≤ n V , 0 ≤ τ ≤ p ts } \ {V i t }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REMARK (on Lemma C.1)</head><p>. This claim is a well-known result, see for example <ref type="bibr">Zhang (2008a)</ref>, <ref type="bibr">Zhang (2008b)</ref>, which straightforwardly follows from the definition of the MAG latent projection procedure in <ref type="bibr">Zhang (2008a)</ref> as well as from the definitions in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>. However, since we did not find a formal proof spelled out in the literature, we here include a proof for completeness. 2. Assume M IO×TO (D) is not time ordered, i.e., assume there is (j, t j )→(i, t i ) in M IO×TO (D) with t j &gt; t i . This assumptions means (j, t j ) ∈ an((i, t i ), M IO×TO (D)) and thus, by Lemma C.1, (j, t j ) ∈ an((i, t i ), D). The latter in turn implies that in D there is directed path π from (j, t j ) to (i, t i ). This path must at least contain one edge (k, t k )→(l, t l ) with t k &gt; t l , which contradicts time order of D. </p><formula xml:id="formula_49">ϕ : I × Z → I ′ × Z (i, t -∆t) → i, t -∆t n for ∆t mod n = 0 (i, ∆t mod n), t -∆t n for ∆t mod n ̸ = 0 .</formula><p>Here, (∆t mod n) with negative ∆t is defined as the smallest non-negative integer ∆t+n•m with m ∈ Z. Note that ϕ is bijective and, hence, invertible. We define D ′ as the directed graph This construction is such that D and D ′ are as graphs equal up to relabeling their vertices according to ϕ. As a consequence, D ′ is acyclic and its d-separations are the same as those of D. Moreover, D ′ is indeed a ts-DAG: First, its time series structure is given by the decomposition of</p><formula xml:id="formula_50">V ′ into V ′ = I ′ × Z, i.e., I ′ is its variable index set. Second, time order follows because if b ∈ V ′ is after a ∈ V ′ , then ϕ -1 (b) is after ϕ -1 (a) together with the fact that D is time ordered. Third, if for a = (i ′ , t ′ ) ∈ V ′ we write ϕ -1 (a) = (i, t), then ϕ -1 ((i ′ , t ′ -∆t ′ )) = (i, t -n • ∆t ′ ).</formula><p>This observation implies that D ′ has repeating edges. Hence, D ′ is a ts-DAG and the statement follows since ϕ(</p><formula xml:id="formula_51">I O × T n O ) = I O × T 1 O . 2.</formula><p>The desired ts-DAG D ′ is constructed by stretching all edges of D by a factor of n in time and adding (n -1) further copies of this stretched version of D to D ′ , respectively shifted by 1 up to (n -1) time steps with respect to the first copy, without any edges between the n copies. Formally: D ′ is the ts-DAG over the vertex set PROOF OF <ref type="bibr">LEMMA 4.3.</ref> In all statements that involve the repeating ancestral relationships or repeating separating sets property, we implicitly assume the graph to be a DMAG (because else these properties would be undefined).</p><formula xml:id="formula_52">V ′ = I × Z, where I is the variable index set of D, such that (i, t ′ -∆t ′ )→(j, t ′ ) in D ′ if and only if (∆t ′ mod n) = 0 and (i, t ′ -∆t ′ /n)→(j, t ′ ) in D.</formula><p>1. &amp; 2. These statements immediately follow from the definitions of the involved properties.</p><p>3. This statement follows because the ancestral relationships between an adjacent pair of vertices uniquely specifies the type of the edge between this pair of vertices.</p><p>4. This statement follows because for T = Z repeating edges implies that the graph is invariant under time shifts, i.e., invariant under the mapping ϕ ∆t : I × T → I × T with ϕ ∆t ((i, t i )) = (i, t i + ∆t) for all ∆t ∈ Z.</p><p>PROOF OF <ref type="bibr">LEMMA 4.4</ref> 4. Let (i, t i ) and (j, t j ) be non-adjacent in M p (D) and without loss of generality assume t i ≤ t j . Consequently, there is a set of vertices S in M p (D) with S ∩ {(i, t i ), (j, t j )} = ∅ such that (i, t i ) ⊥ ⊥ (j, t j ) | S in M p (D). Due to time order of D, no (k, t k ) with t j &lt; t k can be an ancestor of (i, t i ) or of (j, t j ). Lemma S5 in the supplementary material of <ref type="bibr">Gerhardus and Runge (2020)</ref> then asserts that (i, LEMMA D.1. Let G be a directed partial mixed graph with time series structure that has repeating orientations and past-repeating adjacencies. Then, stat(G) is the unique subgraph G in which (i, t i ) and (j, t j ) with τ = t jt i ≥ 0 are adjacent if and only if (i, tτ ) and (j, t) are adjacent in G.</p><formula xml:id="formula_53">t i ) ⊥ ⊥ (j, t j ) | S ′ in M p (D) with S ′ = S ∩ {(l, t l ) | t l ≤ t j }.</formula><p>PROOF OF LEMMA D.1. The statement uniquely determines stat(G) for the following reason: First, as is immediate from Def. 4.6 and asserted by the statement, stat(G) is a subgraph of G. Second, the statement specifies the set of edges that are in G but not in stat(G). Consequently, stat(G) is obtained by deleting a specified set of edges from G.</p><p>It remains to be shown that stat(G) has the asserted property: First, consider two vertices (i, t i ) and (j, t j ) with τ = t jt i ≥ 0 that are adjacent in stat(G). Since stat(G) has repeating edges we thus (i, tτ ) and (j, t) are adjacent in stat(G), which in turn gives that (i, tτ ) and (j, t) are adjacent in G because it is a supergraph of stat(G). Second, consider two vertices (i, tτ ) and (j, t) that are adjacent in G. The past-repeating adjacencies property of G then implies that (i, t i + ∆t) and (j, t j + ∆t) are adjacent in M p (D) for all ∆t with (i, t i + ∆t), (j, t j + ∆t) ∈ V. Moreover, since G has repeating orientations, all these edges have the same orientation. By the second point in Def. 4.6 we thus get that (i, t i ) and (j, t j ) are adjacent in stat(G). First, we show that D [t-p,t] and stat(M IO×TO (D)) have the same adjacencies: Let (i, tτ ) and (j, t) with 0 ≤ τ = t jt i be distinct vertices in D, where without loss of generality (j, t) / ∈ an((i, tτ ), D). If (i, tτ ) and (j, t) are adjacent in D, then there is no set S with S ∩ {(i, tτ ), (j, t)} = ∅ that d-separates them. If (i, tτ ) and (j, t) are non-adjacent in D, then (i, tτ ) ⊥ ⊥ (j, t) | S with S = pa((j, t), D) \ {(i, tτ )}. By time order of D and the definition of p ts , all vertices in pa((j, t), D) \ {(i, tτ )} are within [tp ts , t]. Since p ts ≤ p by assumption we thus get: (i, tτ ) and (j, t) can be d-separated in D by a set of vertices in I O × T O if and only if they are non-adjacent in D. In combination with repeating edges of D and Lemma 4.7 the desired claim follows.</p><p>Second, D [t-p,t] and M IO×TO (D) also have the same edge orientations because they have the same ancestral relationships according Lemma C.1. LEMMA D.2. Let M be a DMAG with time series structure that is time ordered and has repeating orientations and past-repeating adjacencies. Then, stat(M) is a DMAG.</p><p>PROOF OF LEMMA D.2. We have to show that stat(M) does not have directed cycles, does not have almost directed cycles, and is maximal.</p><p>No (almost) directed cycles: Assume that stat(M) has a directed or an almost direct cycle. Then, since stat(M) is a subgraph of M, also M has the same directed or almost directed cycle. But then M is not a DMAG. Contradiction.</p><p>Maximality: Assume the opposite, i.e., assume in stat(M) there are non-adjacent vertices (i, t i ) and (j, t j ), where without loss of generality τ = t jt i ≥ 0, between which there is an inducing path π. We note that stat(M) is time ordered because it is a subgraph of the time ordered graph M. Since by definition of inducing paths all vertices on π are ancestors of (i, t i ) or (j, t j ), we get that all vertices on π are within the time window [tp, t j ]. The repeating edges property of stat(M) now shows that π t-tj , defined as the ordered sequence of vertices obtained by shifting all vertices on π forward in time by tt j time steps, is a path in stat(M) whose edges are orientated in the same way as the corresponding edges of π. Moreover, by combining part 1 of Lemma 4.3 with part 1 of Lemma B.4 we see that the stationarification stat(M) has repeating ancestral relationships. Hence, π t-tj is an inducing path between (i, tτ ) and (j, t) in stat(M). Since stat(M) ⊆ M, π t-tj is also in M an inducing path between (i, tτ ) and (j, t). Maximality of M thus requires (i, tτ ) and (j, t) to be adjacent in M. According to Lemma D.1 we then obtain that (i, tτ ) and (j, t) are adjacent in stat(M). Since (i, t i ) and (j, t j ) are non-adjacent in stat(M) by assumption, this observation contradicts repeating edges of stat(M).</p><p>PROOF OF LEMMA 4.8. Apply Lemma D.2 to M = M p (D).</p><p>PROOF OF LEMMA 4.10. Assume (i, t i ) ∈ an((j, t j ), M p st (D)). This assumption means that in M p st (D) there is a directed path π from (i, t i ) to (j, t j ). Since M p st (D) is a subgraph of M p (D), this path π is also in M p (D). Hence, (i, t i ) ∈ an((j, t j ), M p (D)). Assume (i, t i ) ∈ an((j, t j ), M p (D)). This assumption by Lemma C.1 implies that (i, t i ) ∈ an((j, t j ), D), and hence there is a directed path π from (i, t i ) to (j, t j ) in D. Since D is time ordered, all vertices on π are within [tp, t]. Let ((k 1 , t 1 ), . . . , (k n , t n )) with (k 1 , t 1 ) = (i, t i ) and (k n , t n ) = (j, t j ) be the ordered sequence of observed vertices on π. For all 1 ≤ m ≤ n -1 let π m be the ordered sequence of vertices obtained by shifting π((k m , t m ), (k m+1 , t m+1 )) by tt m+1 time steps forward in time. These paths π m are directed paths from (k m , t -(t m+1t m )) to (k m+1 , t) in D and all their non end-point vertices unobservable. Hence, the paths π m cannot be blocked by any set of observable variables, which implies that in M p (D) there are the edges (k m , t -(t m+1t m ))→(k m+1 , t). According to Lemma 4.7, we thus get that (k m , t -(t m+1t m ))→(k m+1 , t) are in M p st (D), which due to repeating edges of M p st (D) in turn gives (k m , t m )→(k m+1 , t m+1 ) in M p st (D). These edges combine to a directed path from</p><formula xml:id="formula_54">(k 1 , t 1 ) = (i, t i ) to (k n , t n ) = (j, t j ) in M p st (D), hence (i, t i ) ∈ an((j, t j ), M p st (D)).</formula><p>LEMMA D.3. Let G be a directed partial mixed graph with time structure that has repeating edges. Then, stat(G) = G. PROOF OF <ref type="bibr">LEMMA 4.</ref>12. This statement is implied by Theorem 6.4 in <ref type="bibr" target="#b33">Richardson and Spirtes (2002)</ref>. D.5. Proof of Lemma 4.14. We split the proof into three parts that are respectively given in Secs. D. <ref type="bibr">5.2, D.5.3 and D.5.4</ref>. Moreover, we collect several auxiliary results and definitions in D.5.1. For ease of notation, in Sec. D.5.4 we do not always denote vertices by the tuples of their variable and time indices but sometimes just with a single character, for example v instead of (i, t). D.5.1. Auxiliary results and definitions.</p><formula xml:id="formula_55">PROOF OF LEMMA D.3. Apply part 2 of Lemma B.4 for (G ′ , G) = (G, G) to see that G is a subgraph of stat(G). Since stat(G) is a subgraph of G,</formula><p>LEMMA D.5. Let D be a DAG over vertices V and let O ⊆ V be the subset of observed vertices. Then:</p><formula xml:id="formula_56">1. If i→j in M O (D)</formula><p>, then for every subset S ⊆ O that does not contain i or j there is path π between i and j in D that is active given S and into j. 2. If i↔j in M O (D), then for every subset S ⊆ O that does not contain i or j there is path π between i and j in D that is active given S and into both i and j.</p><p>REMARK (on Lemma D.5). This result might have appeared in the literature before. Also note that the presence of i→j in M does not imply that for all S as above there is path between i and j in D that is active given S and out of i. As an example, consider the DAG over V = {i, j, k, l} constituted by i→j→k together with i←l→k and choose O = {i, j, k}: Although i→k in M, for S = {j} the only active path in D is i←l→k.</p><p>PROOF OF LEMMA D.5. 1. The fact that i * - * j is in M O (D) is by <ref type="bibr">Theorem 4.2 in Richardson and Spirtes (2002)</ref> equivalent to the existence of an inducing path ρ relative to O in D between i and j. Assume ρ is out of j. Then, because j is not an ancestor of i according to i * →j in M O (D), there is at least one collider on ρ. By definition of inducing paths, all colliders on ρ are ancestors of i or j. Let k be the collider on ρ that is closest to j on ρ. Because ρ is out of j, j is an ancestor of k. Transitivity of ancestorship thus implies that j is an ancestor of i or j. Both options are a contradiction because there are no directed cycles and because i * →j is in M O (D). Hence, ρ is into j. The statement now follows by combining Lemmas 6. <ref type="bibr">1.1. and 6.1.2 in Spirtes, Glymour and Scheines (2000)</ref>.</p><p>2. Arguments similar to those in the proof of part 1 of Lemma D.5 show that there is an inducing path ρ relative to O in D between i and j that is into both i and j. The statement for i↔j in M follows by Lemma 6. <ref type="bibr">1.2 in Spirtes, Glymour and Scheines (2000)</ref>.</p><p>LEMMA D.6. Let D be a DAG over vertices V and let O ⊆ V be the subset of observed vertices. Let i→j be an edge in M O (D) and S ⊆ O \ {i, j}. Then: If in D there is no path between i and j that is active given S and out of i, then i is an ancestor of S in D.</p><p>PROOF OF LEMMA D.6. We know that i is an ancestor of j in D because i→j in M O (D). Hence, in D there is a directed path π from i to j. Assuming that in D there is no path between i and j that is active given S and out of i, π must be blocked by S. Consequently, S contains a vertex of π and thus a descendant of i. DEFINITION D.7 (Observable vertices within a time window). The set of observable vertices within a time window [t 1 , t 2 ], where t 1 ≤ t 2 , are denoted by O(t 1 , t 2 ). DEFINITION D.8 (Observable vertices within a time window not on a given path). O(t 1 , t 2 ) [π] is the set of all vertices in O(t 1 , t 2 ) less the non end-point vertices of the path π. DEFINITION D.9 (Almost adjacent). Two distinct observable vertices (i, t i ) and (j,</p><formula xml:id="formula_57">t j ) in D c (M p (D)) are almost adjacent if there is an unobservable vertex (k, t k ) such that (i, t i )←(k, t k )→(j, t j ) in D c (M p (D)). LEMMA D.10. If (i, t i ) and (j, t j ) are almost adjacent in D c (M p (D)), then there is a unique unobservable vertex (k, t k ) such that (i, t i )←(k, t k )→(j, t j ) in D c (M p (D)).</formula><p>PROOF OF LEMMA D.10. Existence follows because (i, t i ) and (j, t j ) are almost adjacent, uniqueness follows in combination with the definition of canonical ts-DAGs (see Def. 4.13 in the main text). LEMMA D.11. Let (i, t i ) and (j, t j ) with t i ≤ t j be distinct observable vertices in D c (M p (D)). Then, (i, t i ) and (j, t j ) are adjacent or almost adjacent in D c (M p (D)) if and only if (i, t -(t jt i )) and (j, t) are adjacent in M p (D).</p><formula xml:id="formula_58">PROOF OF LEMMA D.11. If. The premise is that (i, t -(t j -t i )) * - * (j, t) in M p (D),</formula><p>where * - * is → or ← or ↔. Past-repeating adjacencies and repeating orientations of M p (D) then imply (i, t -(t jt i ) -∆t) * - * (j, t -∆t) for all 0 ≤ ∆t ≤ p -(t jt i ), where * - * is the same edge type as between (i, t -(t jt i )) and (j, t). Hence, all these edges are also in stat(M p (D)). If * - * is → or ←, then the definition of canonical ts-DAGs implies</p><formula xml:id="formula_59">(i, t -(t j -t i ) -∆t ′ ) * - * (j, t -∆t ′ ) in D c (M p (D)) for all ∆t ′ ∈ Z. In particular, (i, t i ) and (j, t j ) are adjacent in D c (M p (D)). If * - * is ↔, then the definition of canonical ts-DAGs implies that (i, t -(t j -t i ) -∆t ′ )←((i, j, t j -t i ), t -∆t ′ -(t j -t i ))→(j, t -∆t ′ ) in D c (M p (D)) or t i = t j and (i, t -∆t ′ )←((j, i, 0), t -∆t ′ )→(j, t -∆t ′ ) in D c (M p (D)</formula><p>) for all ∆t ′ ∈ Z. In particular, (i, t i ) and (j, t j ) are almost adjacent in D c (M p (D)).</p><p>Only if. Since the vertices (i, t -(t jt i )) and (j, t) are non-adjacent in M p (D) they are also non-adjacent in M p st (D). The statement now follows with the definition of canonical ts-DAGs. LEMMA D.12. Let (i, t i ) and (j, t j ) be distinct observable vertices that are adjacent or almost adjacent in D c (M p (D)). Then:</p><formula xml:id="formula_60">1. (i, t i )→(j, t j ) in D c (M p (D)) if and only if (i, t i ) ∈ an((j, t j ), D). 2. (i, t i )←(j, t j ) in D c (M p (D)) if and only if (j, t j ) ∈ an((i, t i ), D). 3. (i, t i ) and (j, t j ) are almost adjacent in D c (M p (D)) if and only if (i, t i ) /</formula><p>∈ an((j, t j ), D) and (j, t j ) / ∈ an((i, t i ), D).</p><p>PROOF OF LEMMA D.12. Assume without loss of generality that t i ≤ t j , else exchange (i, t i ) and (j, t j ). From Lemma D.11 it then follows that (i, t -(t jt i )) and (j, t) are adjacent in M p (D). The definition of edges in DMAGs in combination with repeating ancestral relationships of D further implies that</p><formula xml:id="formula_61">• (i, t -(t j -t i ))→(j, t) in M p (D) if and only if (i, t i ) ∈ an((j, t j ), D), • (i, t -(t j -t i ))←(j, t) in M p (D) if and only if (j, t j ) / ∈ an((i, t i ), D), • (i, t -(t j -t i ))↔(j, t) in M p (D) if and only if (i, t i ) /</formula><p>∈ an((j, t j ), D) and (j, t j ) / ∈ an((i, t i ), D). PROOF OF LEMMA D.14. Assume (i, t i ) ∈ an((j, t j ), M p st (D)). This assumption means that in</p><formula xml:id="formula_62">M p st (D) there is a directed path π = ((k 1 , t 1 ), . . . , (k n , t n )) from (k 1 , t 1 ) = (i, t i ) to (k n , t n ) = (j, t j ). Since M p st (D)</formula><p>has repeating edges and is time ordered, the fact that</p><formula xml:id="formula_63">(k m , t m )→(k m+1 , t m+1 ) is in M p st (D) implies (k m , t m )→(k m+1 , t m+1 ) in D c (M p st (D)). Consequently, π is also in D c (M p st (D)</formula><p>) and we find that (i, t i ) ∈ an((j, t j ), D c (M p st (D))). Let (i, t i ), (j, t j ) be vertices in M p st (D) and assume (i,</p><formula xml:id="formula_64">t i ) ∈ an((j, t j ), D c (M p st (D))). This assumption means in D c (M p st (D)) there is a directed path π = ((k 1 , t 1 ), . . . , (k n , t n )) from (k 1 , t 1 ) = (i, t i ) to (k n , t n ) = (j, t j ).</formula><p>Since by definition of canonical ts-DAGs there are no edges into unobservable vertices, all vertices on π are observed and thus also in M p st (D). Moreover, again by definition of canonical ts-DAGs, any edge of D c (M p st (D)) that is between vertices in M p st (D) is also in M p st (D). Hence, π is also in M p st (D) and we find (i, t i ) ∈ an((j, t j ), M p st (D)). 1. If (i, t i ) ∈ an((j, t j ), D) and t jt i ≤ p, then (i, t i ) ∈ an((j, t j ), D c (M p (D))).</p><p>2. If (i, t i ) ∈ an((j, t j ), D c (M p (D))), then (i, t i ) ∈ an((j, t j ), D).</p><p>PROOF OF LEMMA D.15. 1. Let (i, t i ) ∈ an((j, t j ), D) with τ = t jt i ≤ p, where τ ≥ 0 due to time order of D. The repeating ancestral relationships property of D then gives (i, tτ ) ∈ an((j, t), D), which implies (i, tτ ) ∈ an((j, t), M p (D)) by Lemma C.1 and thus (i, tτ ) ∈ an((j, t), M p st (D)) by Lemma 4.10 and finally (i, tτ ) ∈ an((j, t), D c (M p (D))) by Lemma D.14.</p><p>2.</p><formula xml:id="formula_65">Let (i, t i ) ∈ an((j, t j ), D c (M p (D))). This premise means that in D c (M p (D)) there is a directed path π = ((k 1 , t 1 ), . . . , (k n , t n )) from (k 1 , t 1 ) = (i, t i ) to (k n , t n ) = (j, t j ), where t m ≤ t m+1 due to time order of D c (M p (D)). Using repeating ancestral relationships of D c (M p (D)), we thus get that (k m , t -(t m+1 -t m )) ∈ an((k m+1 , t), D c (M p (D))) for all 1 ≤ m ≤ n -1.</formula><p>Since by definition of canonical ts-DAGs there are no edges into unobservable vertices, all vertices on π are observable. Moreover, again due to definition of canonical ts-DAGs, D c (M p (D)) cannot contain edges with a lag larger than p. These observations require 0 ≤ |t m+1t m | = t m+1t m ≤ p and thus shows that both (k m , t -(t m+1 -t m )) and (k m+1 , t) are vertices in M p st (D). Using Lemma D.14 we therefore get LEMMA D.17. Let (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t be distinct observable vertices in D c (M p (D)) and let S ⊆ O(tp, t) \ {(i, t i ), (j, t j )}. Then: If (i, t i ) and (j, t j ) are dconnected given S in D c (M p (D)), then (i, t i ) and (j, t j ) are d-connected given S in D.</p><formula xml:id="formula_66">(k m , t-(t m+1 -t m )) ∈ an((k m+1 , t), M p st (D)), which in turn gives (k m , t-(t m+1 -t m )) ∈ an((k m+1 , t), M p</formula><p>REMARK (on Lemma D.17). The statement makes sense because D and D c (M p (D)) have the same observable time series. PROOF OF LEMMA D.17. Let (i, t i ) and (j, t j ) be d-connected given S ⊆ O(tp, t) \ {(i, t i ), (j, t j )} in D c (M p (D)). Then, in D c (M p (D)) there is path π between (i, t i ) and (j, t j ) that is active given S. Since no node in S is after t, no node on π is after t because else due to time order of D c (M p (D)) there would be a collider on π after t that, again due to time order, cannot be unblocked by S. Let ((k 1 , t 1 ), . . . , (k n , t n )) with (k 1 , t 1 ) = (i, t i ) and (k n , t n ) = (j, t j ) be the ordered sequence of observable vertices on π (not necessarily temporally observed, so some of these vertices may be before tp). Since in D c (M p (D)) there are no edges into unobservable vertices and no edges with a lag larger than p, the subpaths π m = π((k m , t m ), (k m+1 , t m+1 )) with 1 ≤ m ≤ n -1 are either of the form (k m , t m )→(k m+1 , t m+1 ) or (k m , t m )←(k m+1 , t m+1 ) or (k m , t m )←(l m , t lm )→(k m+1 , t m+1 ) with (l m , t lm ) unobservable. In all cases |t mt m+1 | ≤ p. Now associate to each π m a path ρ m in D between (k m , t m ) and (k m+1 , t m+1 ) in the following way:</p><p>Case 1:</p><formula xml:id="formula_67">If π m is (k m , t m )→(k m+1 , t m+1 ), then (k m , t -(t m+1 -t m ))→(k m+1 , t) in D c (M p (D)</formula><p>) by repeating edges of D c (M p (D)) and hence (k m , t-(t m+1 -t m ))→(k m+1 , t) in M p (D) by Lemmas D.11 and D.12. According to Lemma D.5 there thus is path between (k m , t -(t m+1t m )) and (k m+1 , t) in D that is into (k m+1 , t) and active given S m,t-tm+1 , where S m,t-tm+1 is obtained by shifting S m = S \ {(t m , k m ), (t m+1 , k m+1 )} forward in time by tt m+1 time steps. Let p m be the set of all such paths. If any path in p m is out of (k m , t -(t m+1t m )), then let ρ m,t-tm+1 be any such path and let ρ m the path obtained by shifting ρ m,t-tm+1 backwards in time by tt m+1 time steps. If no path p m is out of (k m , t -(t m+1t m )), then let ρ m,t-tm+1 be any path in p m and let ρ m the path obtained by shifting ρ m,t-tm+1 backwards in time by tt m+1 time steps. In this latter case (k m , t -(t m+1t m )) is an ancestor of S m,t-tm+1 in D according to Lemma D.6. By repeating ancestral relationships of D the vertex (k m , t m ) is then an ancestor of S.</p><p>Case 2:</p><formula xml:id="formula_68">If π m is (k m , t m )←(k m+1 , t m+1</formula><p>), do the same as for case 1 with the roles of (k m , t m ) and (k m+1 , t m+1 ) exchanged.</p><p>Case 3: t) in M p (D). According to Lemma D.5 there thus is path between (k m , t -(t m+1 -t m )) and (k m+1 , t) in D that is into both (k m , t -(t m+1t m )) and (k m+1 , t) and active given S m,t-tm+1 . Let ρ m,t-tm+1 be any such path and let ρ m the path obtained by shifting ρ m,t-tm+1 backwards in time by tt m+1 time steps.</p><formula xml:id="formula_69">If π m is (k m , t m )←(l m , t lm )→(k m+1 , t m+1 ) and t m ≤ t m+1 , then (k m , t - (t m+1 -t m ))←(l m , t -(t m+1 -t lm ))→(k m+1 , t) in D c (M p (D)) and hence (k m , t - (t m+1 -t m ))↔(k m+1 ,</formula><p>Case 4: If π m is (k m , t m )←(l m , t lm )→(k m+1 , t m+1 ) and t m &gt; t m+1 , do the same as for case 3 with the roles of (k m , t m ) and (k m+1 , t m+1 ) exchanged.</p><p>The paths ρ m exist due to repeating adjacencies of D and they are active given S m due to repeating separating sets of D. Moreover, due to repeating orientations of D all edges on ρ m are oriented in the same way as the corresponding edges on ρ m,t-tm+1 . Consequently: If (k m+1 , t m+1 ) is a collider on π and thus π m and π m+1 meet head-to-head at (k m+1 , t m+1 ), then, first, (k m+1 , t m+1 ) is an ancestor of S because π is active given S and, second, ρ m and ρ m+1 meet head-to-head at (k m+1 , t m+1 ). Moreover, if (k m+1 , t m+1 ) is a non-collider on π and thus π m and π m+1 do not meet head-to-head at (k m+1 , t m+1 ), then, first, (k m+1 , t m+1 ) is not in S because π is active given S and, second, ρ m and ρ m+1 may or may not meet head-to-head at (k m+1 , t m+1 ). Importantly, if they do meet head-to-head, then (k m+1 , t m+1 ) is an ancestor of S. By applying <ref type="bibr">Lemma 3.3.1 in Spirtes, Glymour and Scheines (2000)</ref> to the ordered sequence of paths (ρ 1 , . . . , ρ n ) we thus obtain a path between (k 1 , t 1 ) = (i, t i ) and (k n , t n ) = (j, t j ) in D that is active given S, and hence (i, t i ) and (j, t j ) are d-connected given S in D. As the third and final part of the proof of Lemma 4.14 we here show that any adjacency in M p (D) is also in M p (D c (M p (D))). We note that the proof of Lemma D.17 crucially relies on the particular form of D c (M p (D)) due to which two subsequent observable vertices on a path in D c (M p (D)) are at most p time steps apart and adjacent or almost adjacent. A general ts-DAG D does, however, not necessarily have these properties, which is why this part of the proof becomes more complicated. We begin by proving the converse of Lemma D.17 restricted to collider-free paths in D.</p><p>LEMMA D.19. Let π be a collider-free path in D between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t. Then, the ordered sequence ((k 1 , t 1 ), . . . , (k n , t n )) of observable vertices on π with (k 1 , t 1 ) = (i, t i ) and (k n , t n ) = (j, t j ) has a unique subsequence ((l 1 , s 1 ), . . . , (l m , s m )) with the following properties:</p><formula xml:id="formula_70">1. (l 1 , s 1 ) = (i, t i ) and (l m , s m ) = (j, t j ), 2. |s α -s α+1 | ≤ p for all 1 ≤ α ≤ m -1,</formula><p>3. all non end-point vertices of π((l α , s α ), (l α+1 , s α+1 )) are unobservable or are before max(s α , s α+1 ) -p for all 1 ≤ α ≤ m -1.</p><p>REMARK (on Lemma D.19). While the uniqueness of ((l 1 , s 1 ), . . . , (l m , s m )) is not needed for the subsequent proofs, we have included it to be able to refer to the subsequence ((l 1 , s 1 ), . . . , (l m , s m )) instead of a such subsequence.</p><p>PROOF OF LEMMA D.19. Existence: Assume without loss of generality that t i ≤ t j , else exchange (i, t i ) and (j, t j ). Since π is collider-free, time order of D thus implies that no vertex on π is after t j . We now prove the statement by induction over n, where n is the number of observable vertices on π:</p><p>Induction base case: n = 2 In this case (i, t i ) and (j, t j ) are the only observable vertices on π. Clearly, the sequence ((i, t i ), (j, t j )) has the desired properties.</p><p>Induction step: n → n + 1 In this case π has n + 1 ≥ 3 observable vertices and the statement has already been proven for paths that have at most n observable vertices. Let π 1 be the subpath of π from (i, t i ) to (k n , t n ) (the observable vertex on π other than (j, t j ) itself that is closest to (j, t j ) on π) and let π 2 be the subpath of π from (k 2 , t 2 ) (the observable vertex on π other than (i, t i ) itself that is closest to (i, t i ) on π) to (j, t j ). We distinguish three collectively exhaustive cases:</p><formula xml:id="formula_71">• Case |t j -t n | = t j -t n ≤ p:</formula><p>This premise implies |t i -t n | ≤ p. Hence, by assumption of induction the statement applies to π 1 . The desired sequence is obtained by appending (j, t j ) to the sequence obtained by applying the statement to π 1 . • Case |t jt 2 | = t jt 2 ≤ p: By assumption of induction the statement then applies to π 2 . Moreover, |t it 2 | ≤ p. The desired sequence is obtained by prepending (i, t i ) to the sequence obtained by applying the statement to π 2 .</p><p>• Case |t jt n | = t jt n &gt; p and |t jt 2 | = t jt 2 &gt; p:</p><p>Since π is collider-free and D is time ordered, this premise implies that all observable non end-point vertices on π are before t jp. Hence, the sequence ((i, t i ), (j, t j )) has the desired properties.</p><p>Moreover, π α is collider-free because π is collider-free. All non end-point vertices on π α are unobserved, i.e., unobservable or temporally unobserved due to part 3 of Lemma D.19. Consequently, π α cannot be blocked by any set of observed variables, which is why v 1 and v 2 are adjacent in M p (D). The statement then follows from Lemma D.11.</p><p>DEFINITION D.21 (Canonically induced path). Let π be a collider-free path in D between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t and let ((l 1 , s 1 ), . . . , (l m , s m )) be as in Lemma D.19 applied to π. The canonical path induced by π, denoted π ci , is the (unique) path in D c (M p (D)) between (i, t i ) and (j, t j ) with the following properties:</p><formula xml:id="formula_72">1. For all 1 ≤ α ≤ m the vertex (l α , s α ) is on π ci . 2. For all 1 ≤ α ≤ m -1 the subpath π ci ((l α , s α )), (l α+1 , s α+1 )) is a) (l α , s α )→(l α+1 , s α+1 ) if and only if (l α , s α ) ∈ an((l α+1 , s α+1 ), D), b) (l α , s α )←(l α+1 , s α+1 ) if and only if (l α+1 , s α+1 ) ∈ an((l α , s α ), D), c) (l α , s α )←(l ′ α , s ′ α )→(l α+1 , s α+1 ) with (l ′ α , s ′ α )</formula><p>unobservable if and only if (l α , s α ) / ∈ an((l α+1 , s α+1 ), D) and (l α+1 , s α+1 ) / ∈ an((l α , s α ), D).</p><p>REMARK (on Def. D.21). Existence follows from Lemma D.12 because according to Lemma D.20 the vertices (l α , s α ) and (l α+1 , s α+1 ) are adjacent or almost adjacent for all 1 ≤ α ≤ m -1. Uniqueness follows from Lemma D.10 because in D c (M p (D)) there is at most one edge between any pair of vertices. LEMMA D.22. Let π be a collider-free path in D between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t and let π ci be the canonical path induced by π. Then: π ci ((l α-1 , s α-1 ), (l α , s α )) to be (l α-1 , s α-1 )←(l α , s α ), which is a contradiction. Hence π((l α-1 , s α-1 ), (l α , s α )) is into (l α , s α ).</p><p>We similarly we find that π((l α , s α ), (l α+1 , s α+1 )) is into (l α , s α ) and thus that (l α , s α ) is a collider on π, a contradiction.</p><p>4. We may without loss of generality assume that (k 1 , t 1 ) is closer to (i, t i ) on π than (k 2 , t 2 ) is to (i, t i ) on π. Write t 12 = max(t 1 , t 2 ).</p><p>Since π is collider-free, also its subpath π((k 1 , t 1 ), (k 2 , t 2 )) is collider-free. For showing that π((k 1 , t 1 ), (k 2 , t 2 )) is an inducing path relative to O(t 12p, t)[π ci ((k 1 , t 1 ), (k 2 , t 2 ))] it is thus sufficient to show that none of its non end-point vertices is an element of the set O(t 12 -p, t)[π ci ((k 1 , t 1 ), (k 2 , t 2 ))]. To this end, let (k 3 , t 3 ) be a non end-point vertex on π((k 1 , t 1 ), (k 2 , t 2 )).</p><p>Since ((l 1 , s 1 ), . . . , (l m , s m )) is the sequence of all observable vertices on π ci , there are α 1 and α 2 with 1 ≤ α 1 &lt; α 2 ≤ m such that (k 1 , t 1 ) = (l α1 , s α1 ) and (k 2 , t 2 ) = (l α2 , s α2 ). Therefore, either (k 3 , t 3 ) equals (l α3 , s α3 ) for some α 1 &lt; α 3 &lt; α 2 or (k 3 , t 3 ) is a non endpoint vertex on π((l α3 , s α3 ), (l α3+1 , s α3+1 )) for some α 3 with α 1 ≤ α 3 &lt; α 2 . In the former case, (k 3 , t 3 ) is not in O(t 12p, t 12 )[π ci ((k 1 , t 1 ), (k 2 , t <ref type="formula">2</ref>))] because it is a non end-point vertex on π ci ((k 1 , t 1 ), (k 2 , t 2 )). In the latter case, according to part 3 of Lemma D.19, (k 3 , t 3 ) is unobservable or before max(s α3 , s α3+1 )p. Because π is collider-free and both (l α3 , s α3 ) and (l α3+1 , s α3+1 ) are on π((k 1 , t 1 ), (k 2 , t 2 )), time order of D implies max(s α3 , s α3+1 ) ≤ t 12 . Thus, (k</p><formula xml:id="formula_73">3 , t 3 ) is not in O(t 12 -p, t)[π ci ((k 1 , t 1 ), (k 2 , t 2 ))].</formula><p>5. This claim follows from parts 1 and 2 of Lemma D.22: Since π is collider-free and active given S, no vertex on π is in S. Thus, since all observable vertices on π ci are also on π, no vertex on π ci is in S. Since π ci is collider-free this observation shows that π ci is active given S.</p><p>LEMMA D.23. Let (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t be distinct observable vertices in D and let S ⊆ O(tp, t) \ {(i, t i ), (j, t j )}. Then: If in D there is a collider-free path π between (i, t i ) and (j, t j ) that is active given S, then (i, t i ) and (j, t j ) are d-connected given S in D c (M p (D)).</p><p>PROOF OF LEMMA D.23. According to part 5 of Lemma D.22 the canonically induced path π ci of π d-connects (i, t i ) and (j, t j ) given S in D c (M p (D)).</p><p>In order to show that any adjacency in M p (D) is also in M p (D c (M p (D)))-and thus to finish the proof of Lemma 4.14-it remains to prove a statement that extends Lemma D.23 to the case in which π, the d-connecting path in D, has n c ≥ 1 colliders c 1 , . . . , c nc (ordered starting with the collider closest to (i, t i )). One might think that such a generalization follows readily now, namely by cutting π into n c + 1 collider-free paths π a,a+1 = π(c a , c a+1 ) with 0 ≤ a ≤ n c , where we let c 0 = (i, t i ) and c nc+1 = (j, t j ), and then applying <ref type="bibr">Lemma 3.3.1 in Spirtes, Glymour and Scheines (2000)</ref> to the canonically induced paths π a,a+1 ci of π a,a+1 with 0 ≤ a ≤ n c . While we do use a similar approach, the proof is complicated by two facts: First, the canonically induced path π a,a+1 ci of π a,a+1 only exists if π a,a+1 is between observable vertices that are at most p time steps apart, see Def. D.21. However, some of the colliders on π, i.e., some of the c a with a ≤ 1 ≤ n c might be unobservable and/or more than p time steps apart from the neighboring colliders or end-point vertices of π, i.e., from c a-1 or c a+1 . Second, even if π a,a+1 ci exists, it may be out of one of its end-point vertices although π a,a+1 is into this vertex. In case this vertex is a collider on π and an element of S, <ref type="bibr">Lemma 3.3.1 in Spirtes, Glymour and Scheines (2000)</ref> does not apply. We will address the first of these complications by noting that, in order for π to be active given S, every collider on π must be an ancestor of S and thus an ancestor of an observed vertex within the time window [tp, t]. Hence, in D there are directed paths from the c a with a ≤ 1 ≤ n c to some observed vertices. By joining these directed paths with the π a,a+1 we get collider-free paths πa,a+1 in D between observed vertices, the canonically induced paths πa,a+1 ci of which exist. DEFINITION D.24 (Collider extension structure). Let π be a (non collider-free) path between the distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t in D that is active given S ⊆ O(tp, t) \ {(i, t i ), (j, t j )}. Let c 1 , . . . , c nc with n c ≥ 1 be the collider on π, ordered starting with the collider closest to (i, t i ). A collider extension structure of π with respect to S and O(tp, t) is a collection of paths ρ 1 , . . . , ρ nc such that for all 1 ≤ a ≤ n c and 1 ≤ b ≤ n c with a ̸ = b all of the following holds:</p><p>1. One of these options holds: a) ρ a is the trivial path consisting of c a = d a only and</p><formula xml:id="formula_74">c a = d a ∈ O(t -p, t). b) ρ a is a non-trivial directed path from c a to some vertex d a ∈ O(t -p, t). 2. If v is on ρ a and in S, then v = d a .</formula><p>3. d a is an ancestor of S. 4. ρ a intersects with π at c a only. 5. ρ a and ρ b do not intersect. LEMMA D.25. Given the assumptions and notation of Def. D.24, one of the following statements holds:</p><p>1. There is a collider extension structure of π with respect to S and O(tp, t). 2. In D there is a path π ′ between (i, t i ) and (j, t j ) with at most n c -1 colliders that is active given S.</p><p>PROOF OF LEMMA D.25. We divide the proof into three steps.</p><p>Step 1: The first, second and third property of collider extension structures holds Let 1 ≤ a ≤ n c . Since c a is a collider on π and π is active given S, there is S a ∈ S (which may be equal to c a ) such that c a ∈ an(S a , D). Hence, there is a (possibly trivial, namely if and only if c a = S a ) directed path λ a from c a to S a . On λ a let d a be the vertex closest to c a that is in O(tp, t) (which may be c a itself). This vertex d a exists because S a ∈ S ⊆ O(tp, t), such that d a is an ancestor of S by means of the subpath λ(d a , S a ). Let ρ a be the subpath λ a (c a , d a ), which is the trivial path consisting of the single vertex c a = d a ∈ O(tp, t) or is a non-trivial directed path from c a to d a ∈ O(tp, t). Moreover, by definition of d a together with the fact that S ⊆ O(tp, t), no vertex on ρ a other than d a is in S. The collection of paths ρ 1 , . . . , ρ nc thus fulfills the first three conditions of a collider extension structure of π with respect to S and O(tp, t).</p><p>The first two of these conditions have two immediate implications that will be important later in this proof: First, if ρ a is non-trivial then c a / ∈ S. Second, if ρ a is non-trivial then it is active given S \ {d a }.</p><p>Step 2: The fourth property of collider extension structures or existence of π ′ Assume there is 1 ≤ a ≤ n c such that π and ρ a do not intersect at c a only. Then, ρ a must be non-trivial and hence we get c a / ∈ S. Let e a be the vertex on ρ a closest to c a other than c a itself that is also on π. If e a is on π((i, t i ), c a ), then let v 1 = (i, t i ) and v 2 = (j, t j ), else let v 1 = (j, t j ) and v 2 = (i, t i ). Let π ′ be the concatenation π(v 1 , e a ) ⊕ ρ a (e a , c a ) ⊕ π(c a , v 2 ). By definition of e a , π ′ is a path (rather than a walk) in D between (i, t i ) and (j, t j ). We now show that π ′ is active given S and has at most n c -1 colliders.</p><p>All colliders on π ′ are ancestors of S: Since ρ a (e a , c a ) is a non-trivial directed path from c a to e a , every collider on π ′ is a collider on π(v 1 , e a ) or a collider on π(c a , v 2 ) or equals e a . Every collider on π(v 1 , e a ) or π(c a , v 2 ) is a collider on π and hence, because π is active given S, an ancestor of S. Lastly, e a is an ancestor of S a ∈ S by means of the path λ a (e a , S a ).</p><p>No non end-point non-collider on π ′ is in S: All non end-point non-colliders on π(v 1 , e a ) or π(c a , v 2 ) are non end-point non-colliders on π and hence, because π is active given S, not in S. All vertices on ρ a (e a , c a ) other than, perhaps, e a are not in S because, as shown in step 1 of this proof, all vertices on ρ a other than d a are not in S. Lastly, assume that e a is in S and a non end-point non-collider on π ′ . Because ρ a (e a , c a ) is into e a , this assumption requires that π(v 1 , e a ) is a non-trivial path out of e a . Consequently, e a is a non end-point non-collider on π, which is a contradiction because e a ∈ S and π is active given S.</p><p>Number of colliders: There are no colliders on ρ a (e a , c a ) because it is a directed path. If v 1 = (i, t i ), then there are at most a -1 colliders on π(v 1 , e a ) and exactly n ca colliders on π(c a , v 2 ). If v 1 = (j, t j ), then there are at most n ca colliders on π(v 1 , e a ) and exactly a -1 colliders on π(c a , v 2 ). The junction point c a is a non-collider on π ′ because ρ a (e a , c a ) is out of c a . Regarding e a , there are two cases:</p><p>1. First, assume e a is a non-collider on π ′ . Then, there are at most (a -1) + (n ca) = n c -1 colliders on π ′ . 2. Second, assume e a is a collider on π ′ . This assumption requires π(v 1 , e a ) to be into e a .</p><p>Let r be that particular root node on π(v 1 , c a ) which is closest to c a on π. Then, π(r, c a ) is non-trivial (because c a is a collider on π and hence not a root on π) and directed from r to c a (by combining the facts that c a is a collider on π, that r is a root on π, and that no other root node on π is between r and c a ). Assume that e a is on π(r, c a ). Then, π(e a , c a ) would be a non-trivial (because e a ̸ = c a ) directed path from e a to c a , which contradicts acyclicity because c a is an ancestor e a by means of ρ a (c a , e a ). Hence, e a is on π(v 1 , r).</p><p>Since π(v 1 , e a ) is into e a , we thus see that e a is on π</p><formula xml:id="formula_75">(v 1 , c a-1 ) if v 1 = (i, t i )</formula><p>and that e a is on π(c a+1 , v 1 ) if v 1 = (j, t j ). Consequently, there are at most a -2 colliders on π(v 1 , e a ) if v 1 = (i, t i ) and there are most n c -(a + 1) colliders on π(v 1 , e a ) if v 1 = (j, t j ) . In summary, there are most (a</p><formula xml:id="formula_76">-2) + (n c -a) + 1 = (n c -(a + 1)) + (a -1) + 1 = n c -1 colliders on π ′ .</formula><p>Thus, if the collection of paths ρ 1 , . . . , ρ nc does not fulfill the fourth condition of a collider extension structure of π with respect to S and O(tp, t), then there is path π ′ as in point 2 of this lemma. To complete this proof it is therefore sufficient to show the following statement: If the collection of paths ρ 1 , . . . , ρ nc fulfills the first four conditions of a collider extension structure of π with respect to S and O(tp, t), then this collection of paths also fulfills the fifth condition of a collider extension structure (and hence is a collider extension structure) or there is path π ′ as in point 2 of the lemma.</p><p>Step 3: The fifth property of collider extension structures holds or existence of π ′ Assume there are 1 ≤ a, b ≤ n c with a &lt; b such that ρ a and ρ b intersect. Then, at least one of these paths must be non-trivial because c a ̸ = c b . If one of them is trivial and the other one is non-trivial, say ρ a is trivial and ρ b is non-trivial, then ρ b must contain c a and hence intersects with π at a vertex other than c a , namely c b . Since this conclusion violates the fourth condition of a collider extension structure of π with respect to S and O(tp, t), we do not need to consider this situation as explained in the last paragraph of step 2. Consequently, we can assume that both ρ a and ρ b are non-trivial and intersect π at, respectively, c a and c b only.</p><p>The fact that both ρ a and ρ b are non-trivial implies S = S\{c a , c b }, see step 1 of this proof. Let f ab be the vertex closest to c a on ρ a that is also on ρ b . Because ρ a and ρ b respectively intersect π at c a and c b only, f ab is neither c a nor c b and both ρ a (c a , f ab ) and ρ b (f ab , c b ) are non-trivial paths. Moreover, f ab is an ancestor of S by means of the directed path λ a (f ab , S a ) from f ab to S a ∈ S. Let π ′ be the concatenation π((i, t i ), c a ) ⊕ ρ a (c a , f ab ) ⊕ ρ b (f ab , c b ) ⊕ π(c b , (j, t j )), which by the assumptions on ρ a and ρ b as well as the definition of f ab is a path (rather than a walk) in D between (i, t i ) and (j, t j ). Consider the four constituting subpaths of π ′ :</p><p>1. First, π((i, t i ), c a ) is active given S \ {(i, t i ), c a } = S because π is active given S. 2. Second and similar to the first point, π(c b , (j, t j )) is active given S \ {c b , (j, t j )}. 3. Third, ρ a (c a , f ab ) is active given S \ {d a } since ρ a is active given S \ {d a } (see the last paragraph in part 1 of this proof). There are two cases: a</p><formula xml:id="formula_77">) If f ab = d a , then ρ a (c a , f ab ) is active given S \ {c a , f ab } = S \ {f ab } = S \ {d a }. b) If f ab ̸ = d a , then f ab /</formula><p>∈ S (because no vertex on ρ a other than d a is in S) and d a is not on ρ a (c a , f ab ). Because ρ a (c a , f ab ) is collider-free, we thus see that ρ a (c a , f ab ) is active given S \ {c a , f ab } = S = {d a } ∪ (S \ {d a }). Thus, ρ a (c a , f ab ) is active given S \ {c a , f ab }. Since the junction points c a and c b are non-colliders on π ′ and not in S, whereas the third junction point f ab is a collider on π ′ and an ancestor of S, Lemma 3.3.1 in <ref type="bibr">Spirtes, Glymour and Scheines (2000)</ref> asserts that π ′ is active given S. Lastly, there are exactly (a -1)</p><formula xml:id="formula_78">+ 1 + (n c -b) = n c -(b -a) ≤ n c -1 colliders on π ′ .</formula><p>Thus, if ρ 1 , . . . , ρ nc fulfills the first four conditions of a collider extension structure of π with respect to S and O(tp, t), then it also fulfills the fifth condition or there is a path π ′ as in point 2 of the lemma.</p><p>By induction over the number of colliders n c , using Lemma D.23 as the induction base case and Lemma D.25 for the induction step, we thus arrive at the following conclusion: For the purpose of proving Lemma 4.14 it is thus sufficient to consider d-connecting paths π in D for which there is a collider extension structure of π with respect to S and O(tp, t). This reasoning allows to overcome the first complication mentioned above in the following way (see Lemma D.29). DEFINITION D.26 (Notation for remaining parts of the proof). Given the assumptions and notation of Def. D.24, let ρ 1 , . . . , ρ nc be a collider extension structure of π with respect to S and O(tp, t). We let ρ 0 and ρ nc+1 be the trivial paths that, respectively, consist of c 0 = d 0 = (i, t i ) and c nc+1 = d nc+1 = (j, t j ) only. Moreover, for all 0 ≤ a 1 &lt; a 2 ≤ n c + 1 we let π a1,a2 = π(c a1 , c a2 ) and πa1,a2 = ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 ⊕ ρ a2 . REMARK (on Def. D.26). The concatenation πa1,a2 is a path (rather than a walk) in D according to the fourth and fifth property of collider extension structures, and both end-points of πa1,a2 are in O(tp, t) (according to the first property of collider extension structures). LEMMA D.27. Given the assumptions and notation of Def. D.26, for all 0 ≤ a ≤ n c :</p><p>1. πa,a+1 is active given S \ {d a , d a+1 }. 2. πa,a+1 ci , the canonical induced path of πa,a+1 , is active given S \ {d a , d a+1 }. REMARK (on Lemma D.27). These two statements concern different graphs: The first statement is with respect to D, the second statement is with respect to D c (M p (D)).</p><p>PROOF OF LEMMA D.27. 1. Since ρ a and ρ a+1 are trivial paths or non-trivial paths out of, respectively, c a and c a+1 and since π a,a+1 is collider-free, πa,a+1 is collider-free. Thus, assuming that πa,a+1 is blocked given S \ {d a , d a+1 }, there is a non end-point non-collider v on πa,a+1 with v ∈ S \ {d a , d a+1 }. This vertex cannot be on ρ a (d a , c a ) because according to the definition of collider extension structures the opposite requires v = d a . Since v can similarly not be on ρ a+1 , v must be a non end-point vertex on π a1,a2 . However, then v must be a non end-point non-collider on π, which is a contradiction because v ∈ S and π is active given S. At this point we face the second complication mentioned above: We can now not straight away apply <ref type="bibr">Lemma 3.3.1 in Spirtes, Glymour and Scheines (2000)</ref> to the ordered sequence of paths π0,1 ci , . . . , πnc,nc+1 ci in order to infer the existence of a path between d 0 = (i, t i ) and</p><formula xml:id="formula_79">d nc+1 = (j, t j ) in D c (M p (D)</formula><p>) that is active given S. To recall, the reason is that πa,a+1 ci may be out of one of its end-point vertices although πa,a+1 is into this vertex. To resolve this complication, we now show that such a situation requires the existence of a certain inducing path in D and hence the existence of an additional edge in D c (M p (D)) which can be used to bypass that vertex. DEFINITION D.28 (Canonical paths). Let π be a path in D between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i t j ≤ p. A path π c between (i, t i ) and (j, t j ) in D c (M p (D)) is canonical with respect to π if all of the following holds:</p><p>1. All observable vertices on π c are also on π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If</head><formula xml:id="formula_80">(k 1 , t 1 ), (k 2 , t 2 ) and (k 3 , t 3 ) are distinct observable vertices on π c and (k 2 , t 2 ) is on π c ((k 1 , t 1 ), (k 3 , t 3 )), then (k 2 , t 2 ) is on π((k 1 , t 1 ), (k 3 , t 3 )). 3. π c is collider-free. 4. If (k 1 , t 1 ) and (k 2 , t 2 ) are distinct observable vertices on π c , then π((k 1 , t 1 ), (k 2 , t 2 )) is an inducing path relative to O(max(t 1 , t 2 ) -p, t)[π c ((k 1 , t 1 ), (k 2 , t 2 ))].</formula><p>REMARK (on Def. D.28). First, Lemma D.22 implies that the canonically induced path π ci of a collider-free path π between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ p is canonical with respect to π. Second, while the first property in Def. D.28 is implied by the second property and would thus not be needed, we have included it in the definition for clarity. LEMMA D.29. Given the assumptions and notation of Def. D.26, let 0 ≤ a 1 &lt; a 2 &lt; a 3 ≤ n c + 1. Assume that πa1,a2 c is canonical with respect to πa1,a2 and active given S \ {d a1 , d a2 }, and that πa2,a3 c is canonical with respect to πa2,a3 and active given S \ {d a2 , d a3 }. Then: If at least one of πa1,a2 c and πa2,a3 c is out of d a2 , then there is a path πa1,a3 c that is canonical with respect to πa1,a3 and active given S \ {d a1 , d a3 }.</p><p>PROOF OF LEMMA D.29. We here assume that πa2,a3 c is out of d a2 , the case in which πa2,a3 c is into d a2 and πa1,a2 c out of d a2 follows equivalently. To simplify notation we write t(v) for the time step of a vertex v, i.e., v = (•, t(v)). We divide the proof into 14 steps.</p><p>Step 1: No vertex on πa1,a2 or πa2,a3 is after t If there would be a vertex on πa1,a2 (on πa2,a3 ) that is after t, then this path would have a collider after t because of time order of D and because both d a1 and d a2 (both d a2 and d a3 ) are not after t. Again using time order, this collider could not be unblocked by S \ {d a1 , d a2 } (by S \ {d a2 , d a3 }) because by definition of S all vertices in S are not after t (see <ref type="bibr">Def. D.24)</ref>. This conclusion contradicts the assumption that πa1,a2 (on πa2,a3 ) is active given S \ {d a1 , d a2 } (given S \ {d a2 , d a3 }).</p><p>Step Step 3: Definition and properties of g a2,a3 and properties of πa2,a3 (c a2 , g a2,a3 ) Let g a2,a3 be the vertex next to d a2 on πa2,a3 c (this may be d a3 ). Since πa2,a3 c is directed from d a2 to d a3 , the path πa2,a3 c is into g a2,a3 . Because in D c (M p (D)) there are no edges into unobservable vertices, we see that g a2,a3 is observable. Moreover, using time order of D c (M p (D)) and that πa2,a3 c is directed from d a2 to d a3 , we see that g a2,a3 is not before d a2 and not after d a3 . Hence, g a2,a3 is within the observed time window [tp, t].</p><p>Since πa2,a3 c is canonical with respect to πa2,a3 and g a2,a3 is on πa2,a3 c , the vertex g a2,a3 is on πa2,a3 = ρ a2 (d a2 , c a2 ) ⊕ π a2,a3 ⊕ ρ a3 . If g a2,a3 were on ρ a2 , then g a2,a3 would in D be an ancestor of d a2 by means of ρ a2 (g a2,a3 , d a2 ). This ancestorship would violate acyclicity of D because g a2,a3 is a descendant of d a2 according to step 2. Hence, g a2,a3 is on πa2,a3 (c a2 , d a3 ) = π a2,a3 ⊕ ρ a3 excluding c a2 . Moreover, πa2,a3 (c a2 , g a2,a3 ) is a non-trivial subpath of πa2,a3 (c a2 , d a3 ) and of πa2,a3 (d a2 , g a2,a3 ). Lastly, πa2,a3 (c a2 , g a2,a3 ) is into c a2 because π is into c a2 .</p><p>Since πa2,a3 c is canonical with respect to πa2,a3 and t(d a2 ) ≤ t(g a1,a2 ) by time order, πa2,a3 (d a2 , g a2,a3 ) is an inducing path relative to O(t(g a2,a3 )p, t)[π a2,a3 c (d a2 , g a2,a3 )]. Here, the simplification O(t(g a2,a3 ) -p, t)[π a2,a3 c (d a2 , g a2,a3 )] = O(t(g a2,a3 ) -p, t) applies because πa2,a3 c (d a2 , g a2,a3 ) by definition of g a2,a3 consists of its end point vertices d a2 and g a2,a3 only. Using the defining properties of inducing paths, we thus see that the path πa2,a3 (d a2 , g a2,a3 ) has the following two properties:</p><p>1. First, if v is an observable non end-point non-collider on πa2,a3 (d a2 , g a2,a3 ), then t(v) &lt; t(g a2,a3 ) -p or t(v) &gt; t. Because step 1 excludes t(v) &gt; t, in fact t(v) &lt; t(g a2,a3 ) -p. 2. Second, if v is a collider on πa2,a3 (d a2 , g a2,a3 ), then v is in D an ancestor of d a2 or g a2,a3 .</p><p>Since d a2 is in D an ancestor of g a2,a3 , the vertex v is, in fact, an ancestor of g a2,a3 .</p><p>Both of these statements are also true for πa2,a3 (c a2 , g a2,a3 ) because it is a subpath of πa2,a3 (d a2 , g a2,a3 ).</p><p>Step 4: All observable vertices on ρ a2 other than d a2 are before t(g a2,a3 ) -p Let v ̸ = d a2 be an observable vertex on ρ a2 . Since g a2,a3 is not on ρ a2 , see step 2, v is then a non end-point vertex on πa2,a3 (d a2 , g a2,a3 ). Moreover, since ρ a2 is directed from c a2 to d a2 , the vertex v is a non-collider on πa2,a3 (d a2 , g a2,a3 ). From step 3 we then get that t(v) &lt; t(g a2,a3 ) -p or t(v) &gt; t, and step 1 further excludes the case t(v) &gt; t.</p><p>Step 5: Definition and properties of h a1,a2 and properties of πa1,a2 (h a1,a2 , c a2 ) Let h a1,a2 be the observable vertex on πa1,a2 c closest to d a2 other than d a2 itself that is not more than p time steps before g a2,a3 , i.e., for which t(g a1,a2 )p ≤ t(h a1,a2 ) (note that h a1,a2 may be d a1 ).</p><p>Since πa1,a2 c is canonical with respect to πa1,a2 , the vertex h a1,a2 is on πa1,a2 = ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 ⊕ ρ a2 . Due to step 4 and t(g a1,a2 )p ≤ t(h a1,a2 ), the vertex h a1,a2 cannot be on ρ a2 unless h a1,a2 = d a2 , which is, however, excluded by definition. Hence, h a1,a2 is on πa1,a2 (d a1 , c a2 ) = ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 excluding c a2 (because c a2 is on ρ a2 ). Moreover, πa1,a2 (h a1,a2 , c a2 ) is a non-trivial subpath of πa1,a2 (d a1 , c a2 ) and of πa2,a3 (h a1,a2 , d a2 ). Lastly, πa1,a2 (h a1,a2 , c a2 ) is into c a2 because π a1,a2 is into c a2 .</p><p>Write t hd = max(t(h a1,a2 ), t(d a2 )) and t hg = max(t(h a1,a2 ), t(g a2,a3 )). Since πa1,a2 c is canonical with respect to πa1,a2 , the path πa1,a2 (h a1,a2 , d a2 ) is an inducing path relative to O(t hdp, t)[π a1,a2 c (h a1,a2 , d a2 )]. In particular, πa1,a2 (h a1,a2 , d a2 ) has the following two properties:</p><p>1. First, if v is an observable non end-point non-collider on πa1,a2 (h a1,a2 , d a2 ), then t(v) &lt; t hd -p or t(v) &gt; t or v is on πa1,a2 c (h a1,a2 , d a2 ). The case t(v) &gt; t is excluded by step 1.</p><p>2. Second, assume w is unobservable. Then, w is a non end-point vertex of πa1,a2 c (d a1 , h a1,a2 ) and of non end-point vertex of πa2,a3 c (g a2,a3 , d a3 ). Moreover, as follows immediately from the definition of canonical ts-DAGs, every unobservable vertex in D c (M p (D)) is adjacent to exactly two vertices, both of which are observable. We thus find that πa1,a2 (g a2,a3 , d a3 ) are collider-free, only h a1,a2 or g a2,a3 can potentially be colliders on π a1,a3 c . First, because πa2,a3 c (g a2,a3 , d a3 ) is a trivial path or a non-trivial path and out of g a2,a3 , see step 7, g a2,a3 is a non-collider on πa1,a3 c . Second, assume that h a1,a2 is a collider on πa1,a3 c . This premise requires that κ a2 is h a1,a2 ←g a2,a3 or h a1,a2 ←u a2 →g a2,a3 and that πa1,a2 c (d a1 , h a1,a2 ) is non-trivial and into h a1,a2 . In combination with the facts that πa1,a2 c is collider-free and that h a1,a2 ̸ = c a2 this form of πa1,a2 c (d a1 , h a1,a2 ) requires that πa1,a2 c (h a1,a2 , c a2 ) is a non-trivial directed path from h a1,a2 to c a2 . Hence, h a1,a2 is an ancestor of c a2 in D c (M p (D)) and, thus, also an ancestor in D. Together with step 2 this ancestral relationship shows that h a1,a2 is in D an ancestor of g a2,a3 . Lemma D.12 in combination with the definition of κ a2 then requires κ a2 = h a1,a2 →g a2,a3 , a contradiction.</p><p>Step 11: All observable vertices on πa1,a3 c are also on πa1,a3 Recall from above that πa1,a3 c = πa1,a2 c (d a1 , h a1,a2 ) ⊕ κ a2 ⊕ πa2,a3 c (g a2,a3 , d a3 ) and πa1,a3 = ρ a1 (d a1 , c a1 )⊕π a1,a2 ⊕π a2,a3 ⊕ρ a3 (the latter because π a1,a2 ⊕π a2,a3 = π a1,a3 ). According to step 7, every vertex on πa2,a3 c (g a2,a3 , d a3 ) is on π a2,a3 ⊕ ρ a3 and hence on πa1,a3 . According to step 8, every observable vertex on πa1,a2 c (d a1 , h a1,a2 ) is on ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 and hence on πa1,a3 . Lastly, due to the three particular forms that the path κ a2 may have, see step 9, every observable vertex on κ a2 is on πa2,a3 c (g a2,a3 , d a3 ) or πa1,a2 c (d a1 , h a1,a2 ) and hence on πa1,a3 .</p><p>Step 12: πa1,a3 c fulfills point 2. in Def. D.28 For reference below we note the following results:</p><p>1. If w 1 and w 2 are on πa1,a2 (d a1 , h a1,a2 ) or on πa1,a3 (d a1 , h a1,a2 ), then πa1,a2 (w 1 , w 2 ) = πa1,a3 (w 1 , w 2 ). This equality follows because h a1,a2 and thus also w 1 and w 2 are on ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 .</p><p>is canonical with respect to πa2,a3 , also on πa2,a3 (v 1 , v 3 ). Moreover, using the fourth result at the beginning of this step, πa2,a3 (v 1 , v 3 ) = πa1,a3 (v 1 , v 3 ). Hence, v 2 is on πa1,a3 (v 1 , v 3 ). 4. Fourth, assume that v 2 is on πa1,a3 c (g a2,a3 , d a3 ) excluding g a2,a3 and that v 1 is not on πa1,a3 c (g a2,a3 , d a3 ). This premise implies that also v 3 is on πa1,a3 c (g a2,a3 , d a3 ) = πa1,a2 c (g a2,a3 , d a3 ). Following the same steps as in the previous case with v 1 replaced by g a2,a3 , we get that v 2 is on πa1,a3 (g a2,a3 , v 3 ) = πa2,a3 (g a2,a3 , v 3 ). Moreover, v 1 is on πa1,a2 c (d a1 , h a1,a2 ) = πa1,a3 c (d a1 , h a1,a2 ) and hence, using that πa1,a2 c is canonical with respect to πa1,a2 , on πa1,a2 (d a1 , h a1,a2 ). This observation shows that πa1,a3 (g a2,a3 , v 3 ) is a subpath of πa1,a3 (v 1 , v 3 ) and hence that v 2 is on πa1,a3 (v 1 , v 3 ). 5. Fifth, assume that v 2 is h a1,a2 or g a2,a3 . This premise implies that v 1 is on the path πa1,a2 c (d a1 , h a1,a2 ) = πa1,a3 c (d a1 , h a1,a2 ) and hence, because πa1,a2 c is canonical with respect to πa1,a2 , on πa1,a3 (d a1 , h a1,a2 ) = πa1,a2 (d a1 , h a1,a2 ), where the latter equality follows by the first result at the beginning of this step. Moreover, the premise implies that v 3 is on πa2,a3 c (g a1,a2 , d a2 ) = πa1,a3 c (g a1,a2 , d a2 ) and hence, because πa2,a3 c is canonical with respect to πa2,a3 , on πa1,a3 (g a1,a2 , d a2 ) = πa2,a3 (g a1,a2 , d a2 ), where the latter equality follows by the third result at the beginning of this step. These considerations show that πa1,a3 (v 1 , v 3 ) decomposes as πa1,a3 (v 1 , h a1,a2 ) ⊕ πa1,a3 (h a1,a2 , g a2,a3 ) ⊕ πa1,a3 (g a2,a3 , v 3 ). Hence, v 2 is on πa1,a3 (v 1 , v 3 ) irrespective of whether v 2 = h a1,a2 or v 2 = g a2,a3 .</p><p>Step 13: πa1,a3 c fulfills point 4. in Def. D.28 Let v 1 and v 2 be two distinct observable vertices on πa1,a3 c such that, without loss of generality, v 1 is closer to d a1 on πa1,a3 c than v 2 is to d a1 on πa1,a3 c . We distinguish three collectively exhaustive cases:</p><p>1. First, assume v 2 is on πa1,a3 c (d a1 , h a1,a2 ). Then, both v 1 and v 2 are on πa1,a2 c (d a1 , h a1,a2 ) = πa1,a3 c (d a1 , h a1,a2 ) and hence πa1,a2 c (v 1 , v 2 ) = πa1,a3 c (v 1 , v 2 ). Since πa1,a2 c is canonical with respect to πa1,a2 , we thus find that πa1,a2 (v 1 , v 2 ) is an inducing path relative to O(max(t(v 1 ), t(v 2 ))p, t)[π a1,a3</p><p>(v 1 , v 2 ). Since an inducing path relative to some set O 1 of observed vertices is also an inducing path relative to another set O 2 of observed with O 2 ⊆ O 1 , we get that πa1,a3 (v 1 , h a1,a2 ) is an inducing path relative to O(t v1hp, t)[π a1,a3 c (v 1 , v 2 )]. b) Because h a1,a2 is on ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 and g a2,a3 is on π a2,a3 ⊕ ρ a3 , see steps 5 and 3, the path πa1,a3 (h a1,a2 , g a1,a2 ) decomposes as πa1,a2 (h a1,a2 , c a2 ) ⊕ πa2,a3 (c a2 , g a1,a2</p><p>). This decomposition shows that πa1,a3 (h a1,a2 , g a1,a2 ) = ψ, with ψ as considered in step 6. Hence, πa1,a3 (h a1,a2 , g a1,a2</p><p>) is an inducing path relative to O(t hgp, t) and, thus, an inducing path relative to O(t hgp, t)[π a1,a3 c (v 1 , v 2 )]. c) By following the same steps as in the second case of this enumeration with v 1 replaced by g a1,a2 , we get that πa2,a3 (g a2,a3 , v 2 ) is an inducing path relative to O(t gv2p, t)[π a1,a3 c (g a2,a3 , v 2 )], where t gv2 = max(t(g a1,a2 ), t(v)). Moreover, since πa1,a3 c (g a2,a3 , v 3 ) is a subpath of πa1,a3 c (v 1 , v 2 ), we get that πa2,a3 (g a2,a3 , v 2 ) is inducing path relative O(t gv2p, t)[π a1,a3 c (v 1 , v 2 )].</p><p>To proof the desired inducing path property of πa1,a3 (v 1 , v 2 ), we now separately consider its colliders and observable non end-point non-collider: a) First, let v be a collider on πa1,a3 (v 1 , v 2 ). Then, v is a collider on one of the three constituting subpaths or is h a1,a2 or g a2,a3 . In all cases, using the inducing path properties of the constituting subpaths, v is in D an ancestor of v 1 or h a1,a2 or g a2,a3 or v 2 . Since, as shown above in this step, both h a1,a2 and g a2,a3 are in D ancestors of v 1 or v 2 , we get that v is in D an ancestor of v 1 or v 2 . b) Second, let v be an observable non end-point non-collider on πa1,a3 (v 1 , v 2 ). Since h a1,a2 and g a2,a3 are in D ancestors of v 1 or v 2 , time order of D guarantees that t(h a1,a2 ) ≤ t v1v2 and t(g a2,a3 ) ≤ t v1v2 , where t v1v2 = max(t(v 1 ), t(v 2 )). These inequalities imply t v1h ≤ t v1v2 , t hg ≤ t v1v2 , and t gv2 ≤ t v1v2 . Thus, using the inducing path properties of the constituting subpaths in combination with t(v) ≤ t, see step 1, we find that t(v) &lt; t v1v2p if v is not a non end-point vertex of πa1,a3 c (v 1 , v 2 ) Thus, πa1,a3 (v 1 , v 2 ) is an inducing path relative to O(t v1v2p, t)[π a1,a3 c (v 1 , v 2 )].</p><p>We may without loss of generality assume that π has a collider extension structure, because if not then according to Lemma D.25 there is path π ′ between (i, t i ) and (j, t j ) in D with at most n c colliders that is active given S and hence, by assumption of induction, (i, t i ) and (j, t j ) are d-connected given S in D c (M p (D)). Therefore, the assumptions and notation of Def. D.26 apply.</p><p>Consider the following algorithmic procedure:</p><p>1. For all 0 ≤ a ≤ n c let πa,a+1 c be πa,a+1 ci , that is, let πa,a+1 c be the canonically induced path of πa,a+1 .  and guarantees the existence of a path between (i, t i ) and (j, t j ) in D c (M p (D)) that is active given S.</p><p>Hence, (i, t i ) and (j, </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3.3 in sufficient generality. By combining the three notions introduced in Defs. 3.1, 3.2 and 3.3 we define the following class of graphical models, which plays an important role throughout the paper. DEFINITION 3.4 (Time series DAG). A time series DAG (ts-DAG) is a DAG D = (V, E) with time series structure V = I × T with T = Z that is time ordered and has repeating edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG 2 .</head><label>2</label><figDesc>FIG 2. The ts-DAG D 1 in part a) implies the ts-DMAG M I O ×T O (D 1 ) in part b) for I O = {1, 2} and T O = {t -2, t -1, t} (regular sampling). The ts-DAG D 2 in part c) implies the ts-DMAG M I O ×T O (D 2 ) in part d) for I O = {2, 3} and T O = {t -4, t -2, t} (regular subsampling). Color coding: Observed vertices are light blue, unobservable vertices are dark gray, temporally unobserved observable vertices are light gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>DEFINITION 3 . 6 (</head><label>36</label><figDesc>Time series DMAG). Let D = (V, E) be a ts-DAG with variable index set I, let I O ⊆ I, and let T O ⊊ Z be regularly sampled or regularly subsampled. The time series DMAG implied by D over O = I O × T O , denoted as M O (D) or M IO×TO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LEMMA 3 . 7 . 3 .</head><label>373</label><figDesc>Let M O (D) be a ts-DMAG. Then: 1. M O (D) has a time series structure. 2. M O (D) is time ordered. There are cases in which M O (D) does not have repeating edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 2 .</head><label>2</label><figDesc>Equivalence of regular subsampling and regular sampling. In Sec. 3.4 we restricted the set of observed time steps T O to regular sampling or regular subsampling. While different at first sight, these two cases are equivalent in the following sense. LEMMA 4.1. Let D be a ts-DAG and 1 ≤ n steps ∈ N. For 1 ≤ n ∈ Z define the set T n O = {tm • n | 0 ≤ m ≤ n steps -1}. Then, with equality up to relabeling vertices:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG 3 .</head><label>3</label><figDesc>FIG 3.  Examples of time ordered DMAGs with time series structure for illustrating the properties from Def. 4.2 and the repeating edges property from Def. 3.3. In each case we state which of these properties apply. a) Repeating adjacencies, repeating separating sets, past-repeating adjacencies. b) Repeating orientations, repeating separating sets, past-repeating adjacencies. c) Repeating orientations, repeating ancestral relationships. d) All but repeating separating sets. e) All but repeating repeating edges and repeating adjacencies. f) All.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>LEMMA 4 . 4 . 1 . 2 . 3 . 4 .</head><label>441234</label><figDesc>Time series DMAGs M p (D) have repeating ancestral relationships. Time series DMAGs M p (D) have repeating orientations. Time series DMAGs M p (D) have repeating separating sets. Time series DMAGs M p (D) have past-repeating adjacencies. 5. There are cases in which a ts-DMAG M p (D) does not have repeating adjacencies. The ts-DMAGs in parts b) and d) of Fig. 2 indeed satisfy the properties asserted by parts 1 through 4 of Lemma 4.4. Moreover, part 5 of Lemma 4.4 clarifies why ts-DMAGs may fail to have repeating edges: They do not necessarily have repeating adjacencies but only the weaker property of past-repeating adjacencies. The following example illustrates this fact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>EXAMPLE 4 . 5 .</head><label>45</label><figDesc>Consider the ts-DAG D 1 in part a) of Fig. 2. In this graph the d-separation O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIG 4 .</head><label>4</label><figDesc>FIG 4. A ts-DMAG M 1 = M 2 (D 1 ) (the same as in part b) of Fig.2) and a DMAG with time series structure M 2 (the same as in part a) of Fig.3) together with their stationarifications. Note that although M 2 has repeating adjacencies its contemporaneous edges are not in stat(M 2 ) because these edges do not have the same orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>LEMMA 4 . 8 .</head><label>48</label><figDesc>The stationarification stat(M p (D)) of a ts-DMAG M p (D) is a DMAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>EXAMPLE 4 .</head><label>4</label><figDesc>9. The stationarified ts-DMAG M 2 st (D 1 ) in part b) of Fig. 4 implies the d-separation O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma 4 .</head><label>4</label><figDesc>12 means that every DMAG is the MAG latent projection of some DAG. Moreoever, the condition M = M O (D c (M)) yields a characterization of DMAGs in the sense that a directed ancestral graph G is a DMAG if and only if it meets the condition G = M O (D c (G)). Because DMAGs are already characterized by definition, 1 the alternative characterization by the condition G = M O (D c (G)) is of limited use in this case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FIG 5 .</head><label>5</label><figDesc>FIG 5. A ts-DMAG M 1 = M 2 (D 1 ) (the same as in part b) of Fig.2and part a) of Fig.4) and a DMAG with time series structure M 2 together with their canonical ts-DAGs. In Dc(M 1 ) there is no unobservable time series because in M 1 there is no bidirected edge that is repetitive in time and hence there is no bidirected edge in stat(M 1 ). The unobservable time series L(2,1,1) in Dc(M 2 ) in the notation of Def. 4.13 corresponds to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FIG 6 .</head><label>6</label><figDesc>FIG 6. Conceptual illustration of Lemma 4.14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIG 7 .</head><label>7</label><figDesc>FIG 7. A ts-DAG D together with the ts-DMAG M 1 (D) and the canonical ts-DAG Dc(M 1 (D)) of the ts-DMAG. Marginalizing Dc(M 1 (D)) to the observed vertices gives back M 1 (D). Same color coding as in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>EXAMPLE 4 .</head><label>4</label><figDesc>16. The DMAG M 2 in part c) of Fig. 5 is a ts-DMAG. This conclusion follows because the canonical ts-DAG D c (M 2 ) in part d) of the figure projects to M 2 . EXAMPLE 4.17. One may use Theorem 1 to confirm that none of the four DMAGs in parts a) -d) of Fig. 3 is a ts-DMAG. In these cases this conclusion also follows because each of these four graphs violates at least one of the necessary conditions in Lemmas 3.7 and 4.4. EXAMPLE 4.18. The DMAG M 1 in part a) of Fig. 8 is not a ts-DMAG because its canonical ts-DAG D c (M 1 ) in part b) projects to the ts-DMAG M 1 (D c (M 1 )) in part c), which is a proper subgraph of M 1 . This example also demonstrates that the equality stat(M) = M p st (D c (M)) is not sufficient for M to be a ts-DMAG. EXAMPLE 4.19. The DMAG M 2 in part d) of Fig. 8 is not a ts-DMAG since its canonical ts-DAG D c (M 2 ) in part e) projects to the ts-DMAG M 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>FIG 8 .</head><label>8</label><figDesc>FIG 8. Two examples of DMAGs with time series structure, M 1 (the same as in part part e) of Fig.3) and M 2 (the same as in part f) of Fig.3), that are not ts-DMAGs although they obey all necessary conditions in Lemmas 3.7 and 4.4. See also the discussions inExamples 4.18 and 4.19.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>4. 7 .</head><label>7</label><figDesc>Implications for stationarified ts-DMAGs. A ts-DMAG M p (D) by definition uniquely determines its stationarification M p st (D). How about the opposite? That is, can a ts-DMAG M p (D) be uniquely determined from its stationarification M p st (D)? At first it seems perfectly conceivable that different ts-DMAGs have the same stationarification, which would make it impossible to uniquely determine M p st (D) from M p (D). However, as a corollary to the observation D c (G) = D c (stat(G)) and Lemma 4.14 we get the following result. LEMMA 4.20. Let D be a ts-DAG. Then, the ts-DMAG M p (D) equals the MAG latent projection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>3 and parts a) and d) of Fig. 8 fall into the model class used by tsFCI but are not ts-DMAGs. The second previously used model class, employed by the SVAR-FCI algorithm from Malinsky and Spirtes (2018) and LPCMCI from Gerhardus and Runge (2020), are DMAGs with time series structure that are time ordered and have repeating edges. From Lemma 3.7 and Def. 4.6 we see that each ts-DMAG M p (D) is associated to a graph in this model, namely to the stationarified ts-DMAG M p st (D) = stat(M p (D)). Lemma 4.20 further implies that the mapping ι : M p (D) → M p st (D) is injective. Conversely, not all graphs in the model class used by SVAR-FCI and LPCMCI are ts-DMAGs: The graph in part d) of Fig. 8 is an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Combining Definition 2</head><label>2</label><figDesc>in<ref type="bibr" target="#b25">Mooij and Claassen (2020)</ref> with the definition of PAGs in Andrews,<ref type="bibr" target="#b1">Spirtes and Cooper (2020)</ref>, we refine DPAGs by background knowledge as follows. DEFINITION 5.2 (DPAGs refined by background knowledge). Let M be a DMAG, let [M] be its Markov equivalence class, and for a background knowledge A let [M] A be the subset of [M] that is consistent with A, i.e., [M] A = {M ∈ [M] | A(M) = 1}. Then: 1. A directed partial mixed graph P is a DPAG for M if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>5. 2 .</head><label>2</label><figDesc>Considered background knowledges. In the below discussions we are interested in the following background knowledges.DEFINITION 5.4 (Specific background knowledges). The background knowledge of. . . • . . . an underlying ts-DAG is the background knowledge A D for which A D (M) = 1 if and only if M is a ts-DMAG, i.e., A D (M) = 1 if and only if there is a ts-DAG D with M = M p (D). • . . . an underlying ts-DAG for stationarifications is the background knowledge A stat D for which A stat D (M) = 1 if and only if M is a stationarified ts-DMAG, i.e., A stat D (M) = 1 if and only if there is a ts-DAG D with M = M p st (D). • . . . time order and repeating ancestral relationships is the background knowledge A ta for which A ta (M) = 1 if and only if M is time ordered and has repeating ancestral relationships. • . . . time order and repeating orientations is the background knowledge A to for which A to (M) = 1 if and only if M is time ordered and has repeating orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>EXAMPLE 5.6. Part a) and b) of Fig. 9 respectively show a ts-DMAG M p (D) and its conventional m.i. DPAG P(M p (D)). To derive P(M p (D)) one may, for example, apply the FCI orientation rules, see Zhang (2008b), to the skeleton of M p (D). Part c) of the same figure shows P(M p (D), A to ), where the head at O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>(D), A to ) = P(M p st (D), A ta ) always holds. With the characterization of stationarified ts-DMAGs inLemma 4.21 (or Lemma 4.22)  we can further show that in this example the graph in h) equals P(M p st (D), A D ) in part i). Note that the ts-DMAG M 1 (D) in part a) is indeed a ts-DMAG. For example, its canonical ts-DAG D c (M 1 (D)) projects to M 1 (D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>FIG 9 .</head><label>9</label><figDesc>FIG 9. An example for illustrating Theorem 3, see also the discussion in Example 5.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>5. 4 .</head><label>4</label><figDesc>Time series DPAGs. In Sec. 5.3 we showed that DPAGs of ts-DMAGs always carry more information about the underlying ts-DAG than DPAGs of stationarified ts-DMAGs. Because of this fact we choose to define time series DPAGs as the former type of DPAGs. DEFINITION 5.7 (Time series DPAG). Let D be a ts-DAG with variable index set I, let I O ⊆ I, and let T O ⊊ Z be regularly sampled or regularly subsampled. The time series DPAG implied by D over O = I O × T O , denoted as P O (D) or P IO×TO (D) and also referred to as a ts-DPAG, is the m.i. DPAG P(M O (D), A D ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>FIG 10 .</head><label>10</label><figDesc>FIG 10. A case in which P p (D) has a non-circle mark that is not in P(M p (D), A ta ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>EXAMPLE 5.8. The ts-DMAG M 1 (D) in part a) of Fig. 10 gives rise to P(M 1 (D), A ta ) in part b) with a circle mark at O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>B. 2 .</head><label>2</label><figDesc>Future vertices are not relevant for determining ts-DMAGs. In the MAG latent projection of D to M IO×TO (D) the vertices L = V \ (I O × T O ) are unobserved, see Def. 4.6 in the main text. Since t is the upper bound of the set of observed time steps T O , this form of L means that in particular all vertices after t, i.e., in [t + 1, +∞) are unobserved. However, for determining M IO×TO (D) these vertices are irrelevant: LEMMA B.1. Let D be a ts-DAG with vertex set V = I × Z. Denote with D ≤t the subgraph of D induced on V ≤t = I × T ≤t with T ≤t = {s ∈ Z | s ≤ t}, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>B. 4 .</head><label>4</label><figDesc>An axiomatic characterization of stationarifications. As we have already noted below Def. 4.6 in the main text, the definition of stationarification implies the following result. LEMMA B.3. stat(G) is the unique largest subgraph of G that has repeating edges. PROOF OF LEMMA B.3. Combine both parts of Lemma B.4. Indeed, we could alternatively have defined stationarifications by this property and then derived that stationarifications fulfill the properties as given in Def. 4.6. LEMMA B.4. 1. stat(G) has repeating edges. 2. If G ′ is a subgraph of G and has repeating edges, then G ′ is a subgraph of stat(G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>LEMMA B. 5 .</head><label>5</label><figDesc>Let D be a ts-DAG with variable index set I, let I O = I, and let T O = {tτ | 0 ≤ τ ≤ p} where p ≥ p ts with p ts the largest lag in D. Then, stat(M IO×TO (D)) equals the subgraph of D induced on I O × T O . REMARK (on Lemma B.5). The proof is given in Sec. D.3 below. In other words: If all component time series are observable (I O = I) and there are enough regularly sampled time steps to capture all direct causal influences (choice of T O and p ≥ p ts ), the stationarified ts-DMAG stat(M IO×TO (D)) equals the segment of D on T O . For ts-DMAGs M IO×TO (D) the same is not necessarily true.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>FIG A .</head><label>.</label><figDesc>FIG A. Two different DMAGs with time series structure that have the same stationarification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>Lemma B.6). The proof is given in Sec. D.7 below. EXAMPLE B.7. The two DMAGs with time series structure in parts a) and b) of Fig. A have the same stationarification (namely the graph in part c) of the figure). Therefore, at most one of them can be a ts-DMAG. Indeed, using Theorem 1 (or Theorem 2) from the main text we confirm that M 2 is not a ts-DMAG and that M 1 is a ts-DMAG. B.7. Additional results on Example 5.8. Here, we formalize and prove the following claim made in Example 5.8 in the main text. LEMMA B.8. Let D ′ be a ts-DAG such that its ts-DMAG M 1 (D ′ ) equals the ts-DMAG M 1 (D) in part a) of Fig. 10 in the main text. Then, in D ′ the pair of observed vertices (O 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>FIG B .</head><label>.</label><figDesc>FIG B. Illustration of ts-DMAGs M p (D) of the same ts-DAG D for different p, see also the discussion in Example B.12. The component time series L 1 is unobservable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>FIG D .</head><label>.</label><figDesc>FIG D. Illustration of limiting ts-DMAGs and limiting ts-DPAGs. The underlying ts-DAGs D 1 and D 2 are, respectively, those shown in parts a) of Fig. B and Fig. C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>4. P p lim (D) has repeating edges. 5. P p lim (D) is a DPAG for M p lim (D). Unlike the examples shown in parts c) and d) of Fig. D, in general there may be circle marks in a limiting ts-DPAG. Lastly, given that M p lim (D) and P p lim (D) have repeating edges, one might hope to give meaning to sending p to infinity in M p lim (D) and P p lim (D) by restricting attention to edges that involve a vertex at time t. However, as the following example shows, such a construction is not possible in general. EXAMPLE B.19. Consider the ts-DAG D in part a) of Fig. B. Since L 1 is unobservable and autocorrelated, for all p there is O 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>C</head><label></label><figDesc>. Proofs for Sec. 3 and for Lemma B.5 and Lemma B.6. C.1. Proofs for Sec. 3.3. PROOF OF LEMMA 3.5. See the explanations in Secs. 3.2 and 3.3 of the main text. Formally: The set of random variables involved in the structural process defined in Sec. 3.1 is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>PROOF OF LEMMA C.1. Only if. Assume i ∈ an(j, D). This assumption means that in D there is a directed path π from i to j. Let (k 1 , . . . , k n ) with k 1 = i and k n = j be the ordered sequence of nodes on π that are in O. Consequently, for all 1 ≤ m ≤ n -1 the vertices k m and k m+1 can in D not be d-separated by any set of observed variables, and hence there are the edges k m →k m+1 in M O (D) for all 1 ≤ m ≤ n -1. These edges give a directed path from k 1 = i to k n = j in M O (D), and hence i ∈ an(j, M O (D)). If. Assume i ∈ an(j, M O (D)). This assumption means that in M O (D) there is a directed path π from i to j.Let (k 1 , . . . , k n ′ ) with k 1 = i and k n ′ = j be the ordered sequence of nodes on π. By definition of edge orientations in M we thus get k m ∈ an(k m+1 , D) for all 1 ≤ m ≤ n ′ -1, and hence i ∈ an(j, D) by transitivity of ancestorship. PROOF OF LEMMA 3.7. 1. The vertex set of M IO×TO (D) is I O × T O . This decomposition defines the time series structure of M IO×TO (D), namely: I O is its variable index set and T O is its time index set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>3 .D. 1 .</head><label>31</label><figDesc>See part b) of Fig. 2 in the main text for an example. D. Proofs for Sec. 4. Proofs for Sec. 4.2. PROOF OF LEMMA 4.1. 1. The desired ts-DAG D ′ is constructed by treating the vertices at all time steps other than tm • n with m ∈ Z as members of unobservable time series in D ′ , by shifting all vertices of D within a time window [tm • n -(n -1), tm • n] to time tm • n in D ′ for all m ∈ Z, and by then relabeling the time steps according to tm • n → tm. Formally: Let I with I O ⊆ I denote the variable index set of D. Define I ′ = I ∪ K with K = I × {1, . . . , n -1} and consider the following map:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>FIG E . 2 )</head><label>.2</label><figDesc>FIG E. Illustration of the constructions involved in the proof of Lemma 4.1 for n = 2 and n steps = 3. The vertically arranged numbers to the left of the four ts-DAGs are their respective variable indices. a) The same ts-DAG as in part c) of Fig. 2, here denoted D 1 . Here, I O = {2, 3} ⊆ I = {1, 2, 3} and T O = {t-4, t-2, t}. The corresponding implied ts-DMAG M I O ×T O (D 1 ) is shown in part d) of Fig. 2. b) The ts-DAG D ′ 1 constructed from D 1 as defined in the proof of part 1 of Lemma 4.1. Here, I ′ O = I O = {2, 3} ⊆ I ′ = I ∪ K with K = I × {1} and T ′ O = {t -2, t -1, t}. Note that while in D 1 the hatched vertices are temporally unobserved, in D ′ 1 they are unobservable. The corresponding implied ts-DMAG M I ′ O ×T ′ O (D ′ 1 ) is the same as M I O ×T O (D 1 ) up to relabeling vertices. c) The same ts-DAG as in part a) of Fig. 2, here denoted D 2 . Here, I O = {1, 2} ⊆ I = {1, 2, 3} and T O = {t -2, t -1, t}. The corresponding implied ts-DMAG M I O ×T O (D 2 ) is shown in part b) of Fig. 2. d) The ts-DAG D ′ 2 constructed from D 2 as defined in the proof of part 2 of Lemma 4.1. Here, I ′ O = I O = {1, 2} ⊆ I ′ = I and T ′ O = {t -4, t -2, t}. The corresponding implied ts-DMAG M I ′ O ×T ′ O (D ′ 2 ) is the same as M I O ×T O (D 2 ) up to relabeling vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>See parts c) and d) of Fig. E for illustration. The statement is apparent from this construction. D.2. Proofs for Sec. 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>5 .</head><label>5</label><figDesc>Now take any ∆t ∈ Z with 0 ≤ ∆t ≤ tt j and let S ′ ∆t be obtained by shifting all vertices in S forward in time by ∆t time steps. By construction of S ′ all nodes in S ′ ∆t are within [tp, t]. The repeating separating sets property of M p (D), as asserted by part 2 of Lemma 4.4. and already proven, then implies (i, t i + ∆t) ⊥ ⊥ (j, t j + ∆t) | S ′ ∆t . This fact proves the contraposition of the statement. Part 1 of Lemma 4.4 implies that M p (D) has repeating orientations. Thus, part 1 of Lemma 4.3 shows that M p (D) would also necessarily have repeating edges if M p (D) necessarily had repeating adjacencies, thereby contradicting part 3 of Lemma 3.7. D.3. Proofs for Sec. 4.4 and of Lemma B.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head></head><label></label><figDesc>PROOF OF LEMMA 4.7. Apply Lemma D.1 to G = M p (D). PROOF OF LEMMA B.5. Denote the subgraph of D induced on I × T O as D [t-p,t] . This graph clearly has the same set of vertices and the same time series structure as M IO×TO (D) because I O = I by assumption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>as immediately implied by the second point in Def. 4.6, this observation shows stat(G) = G. LEMMA D.4. Let G be a directed partial mixed graph with time structure. Then stat(stat(G)) = stat(G). PROOF OF LEMMA D.4. Combine part 1 of Lemma B.4 with Lemma D.3. D.4. Proofs for Sec. 4.5 other than Lemma 4.14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>Now proceed as in the proof of the if part of Lemma D.11. D.5.2. Part 1: M p (D) and M p (D c (M p (D))) have the same ancestral relationships. As the first part of the proof of Lemma 4.14 we here show that M p (D) and M p (D c (M p (D))) have the same ancestral relationships. LEMMA D.13. D c (G) = D c (stat(G)). PROOF OF LEMMA D.13. An inspection of Defs. 4.6 and 4.13 in the main text reveals that D c (G) is uniquely determined by stat(G). The statement thus follows because stat(G) = stat(stat(G)) according to Lemma D.4. LEMMA D.14. The stationarified ts-DMAG M p st (D) has the same ancestral relationships among vertices in M p st (D) as the canonical ts-DAG D c (M p (D)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head></head><label></label><figDesc>These considerations show that M p st (D) and D c (M p st (D)) have the same ancestral relationships among vertices in M p st (D). The statement follows because D c (M p st (D)) = D c (M p (D)) according to Lemma D.13. LEMMA D.15. Consider a ts-DAG D and the canonical ts-DAG D c (M p (D)). Then:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head></head><label></label><figDesc>(D)) by Lemma 4.10 and thus (k m , t -(t m+1t m )) ∈ an((k m+1 , t), D) by Lemma C.1 and thus (k m , t m ) ∈ an((k m+1 , t m+1 ), D) by repeating ancestral relationships of D. The statement now follows from transitivity of ancestorship. LEMMA D.16. M p (D) and M p (D c (M p (D))) have the same ancestral relationships. PROOF OF LEMMA D.16. Combine Lemma C.1 with Lemma D.15. D.5.3. Part 2: Any adjacency in M p (D c (M p (D))) is also in M p (D). As the second part of the proof of Lemma 4.14 we here show that any adjacency in M p (D c (M p (D))) is also in M p (D). Together with the fact that both these graphs have the same ancestral relationships, as already proven in Sec. D.5.2, we then get that M p (D c (M p (D))) is a subgraph of M p (D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head></head><label></label><figDesc>LEMMA D.18. M p (D c (M p (D))) is a subgraph of M p (D). PROOF OF LEMMA D.18. As an immediate consequence of Lemma D.17, every adjacency in M p (D c (M p (D))) is also in M p (D). The statement then follows with Lemma D.16 because the orientation of edges in M p (D) and M p (D c (M p (D))) are uniquely determined by the ancestral relationships. D.5.4. Part 3: Any adjacency in M p (D) is also in M p (D c (M p (D))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>4 .</head><label>4</label><figDesc>Fourth and similar to the third point, ρ a (f ab , c b ) is active given S \ {f ab , c b }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>2 .</head><label>2</label><figDesc>Since πa,a+1 is collider-free, as shown in the proof of part 1 of Lemma D.27, and since both d a and d a+1 are by definition in O(tp, t), the canonical induced path πa,a+1 ci exists. The statement follows by combining part 1 of Lemma D.27 with part 5 of Lemma D.22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>2 :</head><label>2</label><figDesc>c a2 and d a2 are in D ancestors of d a3 Because πa2,a3 c is out of d a2 and collider-free (the latter by means of being canonical with respect to πa2,a3 ), the path πa2,a3 c is directed from d a2 to d a3 . Hence, all vertices on πa2,a3 c are ancestors of d a3 and descendants of d a2 in D c (M p (D)) and thus, by part 2 of Lemma D.15, also in D. Because c a2 is an ancestor of d a2 in D by means of ρ a2 , we thus see that c a2 is in D an ancestor of every vertex on πa2,a3 c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>2 .</head><label>2</label><figDesc>Set m to n c . 3. Let σ : {0, 1, . . . , m + 1} → {0, 1, . . . , n c + 1} be the identity map. 4. While there is an integer a with 1 ≤ a ≤ m such that at least one of πσ(a-1),σ(a) d σ(a) : a) Let b be the smallest such a. σ(b -1) = 0 and σ(b + 1) = n c + 1, then return πσ(b-1),σ(b+1) m by one. e) Let σ ′ : {0, 1, . . . , m + 1} → {0, 1, . . . , n c + 1} be such that σ ′ (a) = σ(a) for all 1 ≤ a &lt; b and σ ′ (a) = σ(a + 1) for all b ≤ a ≤ m + 1. f) Replace σ by σ ′ . 5. Return the ordered sequence of paths πσ(D.27 and Lemma D.22, the canonically induced paths πa,a+1 ci are for all 0 ≤ a ≤ n c active given S \ {d a , d a+1 } and canonical with respect to πa,a+1 . Lemma D.29 thus guarantees that all paths πa1,a3 c with 0 ≤ a 1 &lt; a 3 ≤ n c + 1 constructed in step 4(b) of the above procedure are active given S \ {d a1 , d a3 } and canonical with respect to πa1,a3 . Thus, if the algorithm terminates in step 4(c), it returns a path π0,nc+1 c between d 0 = (i, t i ) and d nc+1 = (j, t j ) in D c (M p (D)) that is active given S = S \ {d 0 , d nc+1 }. If the algorithm terminates in step 5, then the following is true: 1. πσ(0),σ(1) c is a path in D c (M p (D)) between d σ(0) = d 0 = (i, t i ) and d σ(1) that is into d σ(1) and active given S \ {d σ(0) , d σ(1) }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>2 .</head><label>2</label><figDesc>For all 1 ≤ a ≤ m -1 the path πσ(a),σ(a+1) c is a path in D c (M p (D)) between d σ(a) and d σ(a+1) that is into both d σ(a) and d σ(a+1) and active given S \ {d σ(a) , d σ(a+1) }.3.πσ(m),σ(m+1)c is a path in D c (M p (D)) between d σ(m) and d σ(m+1) = d nc+1 = (j, t j ) that is into d σ(m) and active given S \ {d σ(m) , d σ(m+1) }.Further, by definition of collider extension structures, d a is an ancestor of S for all 1 ≤ a ≤ n c .Lemma 3.3.1 in Spirtes, Glymour and Scheines (2000)  thus applies to the ordered sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head></head><label></label><figDesc>t j ) are d-connected given S in D c (M p (D)). LEMMA D.31. M p (D) is a subgraph of M p (D c (M p (D))).PROOF OF LEMMA D.31. As an immediate consequence of Lemma D.30, every adjacency in M p (D) is also in M p (D c (M p (D))). The statement follows with Lemma D.16 because the orientation of edges are uniquely determined by the ancestral relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>do not in general have repeating edges.</figDesc><table><row><cell>EXAMPLE 3.8. The ts-DMAG in part b) of Fig. 2 does not have repeating edges because there is the edge O 1 t-2 ↔O 2 t-2 although O 1 t-1 and O 2 t-1 (and O 1 t and O 2 t ) are non-adjacent.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 t on O 1 t-1 •→O 1 t . According to the stronger background knowledge A D one can orient this edge as O 1 t-1 →O 1 t because the opposite hypothesis gives the graph in part d) in Fig. 8, which by means of Theorem 1 was shown to not be a ts-DMAG, see Example 4.19. Thus, from the ts-DPAG P 1 (D) we can conclude that O 1 t-1 has a causal influence on O 1 t whereas from P(M 1 (D), A ta ) we can only conclude that this causal influence might but also might not exist. Furthermore, see Gerhardus (2023, Lemma B.8), in the ts-DMAG M 1 (D) the pair (O 1 t-1 , O 1</figDesc><table /><note><p>t ) cannot suffer from latent confounding.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>2 t-1 generically has a causal effect on O 1 t whereas from SVAR-FCI's output we can only conclude that O 2 t-1 might but also might not generically have a causal effect on O 1 t . As another difference, tsFCI l+c gives the edge O 1 t-1 •→O 2 t-1 whereas SVAR-FCI gives that O 1 t-1 and O 2 t-1 are nonadjacent. At first one might think that SVAR-FCI is at the advantage in this regard, because the absence of an edge between O 1 t-1 and O 2 t-1 correctly conveys that the pair (O 1 t-1 , O 2 t-1 ) is not confounded by unobservable variables. Note, however, that tsFCI l+c conveys the same conclusion by means of having learned that O 1 t and O 2 t are non-adjacent (cf. last paragraph in Sec.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>↔O 1 t ), giving rise to a DMAG M 2 . Both of these candidates are represented by and Markov equivalent to P * , hence M</figDesc><table><row><cell>DMAG M 1 , or as a head (O 1 t-1</cell><cell>1 t-1 →O 1 t ), giving rise to a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>. Since O 2 t-1 is an ancestor of O 2 t in D ′ according to the edge O 2 t-1 →O 2 t in M 1 (D ′ ), in D ′ there is a directed path π 3 from O 2 t-1 to O 2 t . This path cannot intersect O 1 t-1 because else O 2 t-1 would be an ancestor of O 1 t-1 by means of the subpath π 3 (O 2 t-1 , O 1 t-1 ), which together with the fact that O 1 t-1 is an ancestor of O 2 t-1 according to the edge O 1 t-1 →O 2 t-1 in M 1 (D ′ ) contradicts acyclicity. The path π 3 can also not intersect O 1 t because the subpath π 3 (O 2 t-1 , O 1 t ) would then be a directed path from O 2 t-1 to O 1 t such that all its non end-point vertices, if any, are unobserved. Consequently, there would need to be the edge O 2 t-1 →O 1 t in M 1 (D ′ ). Moreover, due to time, order π 3 can also not contain any vertex O 1 s or O 2 s with s ≤ t -2. We conclude that all non end-point vertices of π 3 , if any, are unobservable.</figDesc><table><row><cell>This path cannot intersect O 2 t be-t by means of the subpath π 1 (O 2 t would be an ancestor of O 1 cause else O 2 t , O 1 t ), which together with the fact that O 1 t is an ancestor of O 2 t according to the edge O 1 t →O 2 t in M 1 (D ′ ) contradicts acyclicity. The path π 1 can also not intersect O 2 t-1 because the sub-path π 1 (O 2 t-1 , O 1 t ) would then be a directed path from O 2 t-1 to O 1 t such that all its non end-point vertices, if any, are unobserved. Consequently, there would need to be the edge O 2 t-1 →O 1 t in M 1 (D ′ ). Moreover, due to time order, π 1 can also not contain any vertex O 1 s or O 2 s with s ≤ t -2. We conclude that all non end-point vertices of π 1 , if any, are unobservable. 2. Since O 1 t-1 is an ancestor of O 2 t-1 in D ′ according to the edge O 1 t-1 →O 2 t-1 in M 1 (D ′ ), in D</cell></row></table><note><p>′ there is a directed path π 2 from O 1 t-1 to O 2 t-1 . This path can, due to time order, neither intersect O 1 t nor O 2 t . Moreover, also due to time order, π 2 cannot contain any vertex O 1 s or O 2 s with s ≤ t -2. We conclude that all non end-point vertices of π 2 , if any, are unobservable. 3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>1  and O 2 t-1 such that all its non end-point vertices are unobserved. In particular, ρ is an inducing path. Now suppose that, contrary to the claim to be proven, inD ′ there is an inducing path ρ 2 between O 1 t-1 and O 1 t that is into O 1 t-1 .Then, according to Lemma 32 in Zhang (2008a) the concatenation ρ 1 ⊕ ρ 2 has a subsequence ρ which is an inducing path between O 2 , 2. is collider-free because π is active given the empty set, 3. is also a path in D ′ because G is a subgraph of D ′ , 4. needs to intersect at least one of O 2</figDesc><table><row><cell>and O 1</cell><cell>t-1</cell></row><row><cell cols="2">t-1 and O 2 t because else it would be an inducing path t-1 in D ′ , t that is O 1 t-1 and O 1 between O 1 5. is into O 1 t because else it would need to be directed from O 1 t to O 1 t-1 , which contradicts time order, 6. does not intersect O 2 t because else O 2 t would need to be an ancestor of O 1 t (which is not possible to due to the edge O 1 t →O 2 t in M 1 (D ′ )) or of O 1 t-1 (which is not possible due to time order), 7. does not intersect O 2 t-1 because else the subpath π(O 2 t-1 , O 1 t</cell></row></table><note><p><p><p>t in D ′ . However, then there would need to be an edge between O 2 t-1 and O 1 t in M 1 (D ′ ) (according to the ancestral relationships, this edge would be O 2 t-1 ↔O 1 t ), which is a contradiction. We conclude that there is no inducing path between</p>O 1 t-1 and O 1 t which is into O 1 t-1 , that is, the pair (O 1 t-1 , O</p>1 t ) is not subject to unobserved confounding. Second, we prove that the causal effect of O 1 t-1 and O 1 t is identifiable and can be estimated by adjusting for the empty set. To this end, assume that O 1 t-1 and O 1 t are not d-separated in the graph G that is obtained by removing from D ′ all edges out of O 1 t-1 . Then, there is at least one path π between O 1 t-1 and O 1 t in G that is active given the empty set. This path 1. is into O 1 t-1 because in G there are no edges out of O 1 t-1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Limiting ts-DMAGs and limiting ts-DPAGs. Lemmas B.10 and B.13 imply the following behaviour when p is kept fixed while p ≥ p increases beyond any bound.</figDesc><table><row><cell>instead.</cell><cell>1 t</cell></row><row><cell>B.8.2.</cell><cell></cell></row></table><note><p><p><p>LEMMA B.16. Let D be a ts-DAG and p ≥ 0. Then:</p>1. There is p ≥ p with M p′ ,[t-p,t] (D) = M p,[</p>t-p,t] (D) for all p′ ≥ p. 2. There is p ≥ p with P p′ ,[t-p,t] (D) = P p,[t-p,t] (D) for all p′ ≥ p. Lemma B.16 implies that the sequence ∆p → M p+∆p,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>for examples. Similar to stationarified ts-DMAGs, limiting ts-DMAGs M p lim (D) are not in general DMAGs for the underlying ts-DAGs D and carry different meaning. Namely, vertices (i, t i ) and (j, t j ) with t i ≤ t j ≤ t are adjacent in M p lim (D) if and only if there is no finite set of observable variables within (-∞, t] that d-separates (i, t i ) and (j, t j ) in D. The same statement applies to limiting ts-DPAGs.</figDesc><table><row><cell>LEMMA B.18. 1. M p lim (D) has repeating edges. 2. M p lim (D) is a subgraph of M p st (D). 3. M p</cell></row></table><note><p>lim (D) is a DMAG.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>by definition of the data generating process. Hence, D is time ordered. The repeating edges property follows because the data generating process by definition is causally stationary, i.e., because V k Proofs for Sec. 3.4. LEMMA C.1. Let D be a DAG with vertex set V = O ∪ L. Then, for i, j ∈ O, i ∈ an(j, D) if and only if j ∈ an(i, M O (D)).</figDesc><table /><note><p>t-τ ∈ PA i t if and only if V k t-τ -∆t ∈ PA i t-∆t . Lastly, acyclicity of D is definitional for the data generating process. C.2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>. 1. This statement follows because M p (D) and D have the same ancestral relationships between vertices in M p (D) (according to Lemma C.1) in combination with the fact that D has repeating ancestral relationships (as implied by part 4 of Lemma 4.3). 2. Combine part 1 of Lemma 4.4 with part 3 of Lemma 4.3.3. Theorem 4.18 in Richardson and Spirtes (2002)  implies that for sets S of vertices in M</figDesc><table /><note><p><p>p (D) the m-separation (i, t i ) ⊥ ⊥ (j, t j ) | S holds in M p (D) if and only if the d-separation (i, t i ) ⊥ ⊥ (j, t j ) | S</p>holds in D. The statement now follows because D has repeating separating sets as implied by part 4 of Lemma 4.3.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As directed ancestral graphs without inducing paths between non-adjacent vertices, see Sec.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>In general, the DPAGs P(M p (D), A D ) and P(M p (D), A ta ) are not equal and can contain circle marks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>There are examples with this property, but it is unknown to the author whether this property is a general fact.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Interestingly, we can draw this conclusion although O 1 t-1 →O 1 t is not visible, thereby suggesting that the notion of visibility fromZhang (2008a)  needs refinement for ts-DMAGs and ts-DPAGs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>The same is true for the score-based and hybrid algorithms from<ref type="bibr" target="#b11">Gao and Tian (2010)</ref> andMalinsky and  Spirtes (2018).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>To clarify:Malinsky and Spirtes (2018)  does not mention tsFCI l+c but refer to tsFCI l when writing 'tsFCI'.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_7"><p>The discrete metric d(•, •) is defined by d(x, y) = 1 if x = y and d(x, y) = 0 else. FIG C. Illustration of ts-DPAGs P p (D) of the same ts-DAG D for different p.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_8"><p>Note that the proof of Lemma F.2 uses part 1 of Lemma B.11 but not part 2 of Lemma B.11, such that the proofs of these two lemmas are not circular.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I thank <rs type="person">Jakob Runge</rs> for helpful discussions and suggestions. I thank <rs type="person">Tom Hochsprung</rs> and <rs type="person">Wiebke Günther</rs> for careful proofreading and suggestions on how to make the paper more accessible. I thank two anonymous reviewers and two anonymous Associate Editors for suggestions and questions that helped me to improve the paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>Supplement to "Characterization of causal ancestral graphs for time series with latent confounders" This Supplementary Material contains: First, a glossary of abbreviations and frequently used symbols. Second, theoretical results that were omitted from the main text due to space constraints. Third, proofs of all theoretical results presented in the main text together with various auxiliary results that are used in these proofs.</p><p>Uniqueness: Let ((l 1 , s 1 ), . . . , (l m , s m )) and ((l ′ 1 , s ′ 1 ), . . . , (l ′ m ′ , s m ′ )) be two such subsequences. We proof their equality by induction over α, where α is the index of the subsequences.</p><p>Induction base case: α = 1 The equality (l 1 , s 1 ) = (l ′ 1 , s ′ 1 ) follows due to the first property demanded in Lemma D.19 applied to both sequences.</p><p>Induction step: α → α + 1 ≤ min(m, m ′ ) By the assumption of induction (l q , s q ) = (l ′ q , s ′ q ) for all 1 ≤ q ≤ α is given and we have to show (l α+1 , s α+1 ) = (l ′ α+1 , s ′ α+1 ). Assume the opposite, i.e., assume (l α+1 , s α+1 ) ̸ = (l ′ α+1 , s ′ α+1 ). Without loss of generality further assume that (l α+1 , s α+1 ) is on the subpath π((l ′ α+1 , s ′ α+1 ), (j, t j )), else exchange the two subsequences. Let r with α &lt; r &lt; m ′ be such that the vertices (l ′ q , s ′ q ) with α &lt; q ≤ r are non end-point vertices on π((l α , s α ), (l α+1 , s α+1 )) and (l ′ r+1 , s ′ r+1 ) is on π((l α+1 , s α+1 ), (j, t j )). Such an r exists because both ((l 1 , s 1 ), . . . , (l m , s m )) and ((l ′ 1 , s ′ 1 ), . . . , (l ′ m ′ , s m ′ )) are subsequences of ((k 1 , t 1 ), . . . , (k n , t n )). Note that the (l ′ q , s ′ q ) with α &lt; q ≤ r are observable because all vertices of the subsequence ((l ′ 1 , s ′ 1 ), . . . , (l ′ m ′ , s m ′ )) are observable. The third property demanded in Lemma D.19 applied to the sequence ((l 1 , s 1 ), . . . , (l m , s m )) thus requires s ′ q &lt; max(s α , s α+1 )p for all α &lt; q ≤ r. We distinguish two cases:</p><p>• Suppose s α+1 ≤ s α . Then s ′ q &lt; s αp for all α &lt; q ≤ r. For q = α + 1 we thus get s ′ α+1 &lt; s αp, which contradicts the second property demanded in Lemma D.19 applied to</p><p>q &lt; s α+1p for all α &lt; q ≤ r. By time order of D in combination with the facts that π is collider-free and that (l α , s α ) is on π((i, t i ), (l α+1 , s α+1 )), the premise s α+1 &gt; s α implies that (l α+1 , s α+1 ) is on π between the root node of π and (j, t j ). Consequently, all vertices on π((l α+1 , s α+1 ), (j, t j )) are not before s α+1 . Since (l ′ r+1 , s ′ r+1 ) is on π((l α+1 , s α+1 ), (j, t j )) we thus find s α+1 ≤ s ′ r+1 , which implies s ′ q &lt; s ′ r+1p for all α &lt; q ≤ r. For q = r we thus get s ′ r &lt; s ′ r+1p, which contradicts the second property demanded in Lemma D.19 applied to ((l ′ 1 , s ′ 1 ), . . . , (l ′ m ′ , s m ′ )). Consequently, the assumption (l α+1 , s α+1 ) ̸ = (l ′ α+1 , s ′ α+1 ) leads to a contradiction and (l α+1 , s α+1 ) = (l ′ α+1 , s ′ α+1 ) must be true. This induction terminates when α + 1 = min(m, m ′ ). The first property demanded in Lemma D.19 applied to both sequences then requires m = m ′ and (l m , s m ) = (l ′ m , s ′ m ) = (j, t j ), which completes the proof REMARK (on the proof of Lemma D.19). For the special case in which π is a directed path we can also immediately see that the sequence ((k 1 , t 1 ), . . . , (k n , t n )) of all observable nodes on π has the desired properties. We need the above proof to also cover the case in which π is into both (i, t i ) and (j, t j ).</p><p>LEMMA D.20. Let π be a collider-free path in D between distinct observable vertices (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t and let ((l 1 , s 1 ), . . . , (l m , s m )) be as in Lemma D.19 applied to π. Then, (l α , s α ) and (l α+1 , s α+1 ) are adjacent or almost adjacent in D c (M p (D)) for all 1 ≤ α ≤ m -1.</p><p>PROOF OF LEMMA D.20. Let π α be the path in D obtained by shifting the subpath π((l α , s α ), (l α+1 , s α+1 )) forward in time by tmax(s α , s α+1 ) ≥ 0 time steps. This π α is a path between the vertices v 1 = (l α , t -(max(s α , s α+1 )s α )) and v 2 = (l α , t -(max(s α , s α+1 ) -s α+1 )), both of which are observable because all vertices in the sequence ((l 1 , s 1 ), . . . , (l m , s m )) are observable and within [tp, t] because of part 2 of Lemma D.19.</p><p>If v is on πa1,a2 c (h a1,a2 , d a2 ), then t(v) &lt; t(g a1,a2 ) -p by definition of h a1,a2 . Note that t hd -p ≤ t hg -p and t(g a1,a2 )p ≤ t hgp. Hence, in any case, t(v) &lt; t hgp. 2. Second, if v is a collider on πa1,a2 (h a1,a2 , d a2 ), then v is in D an ancestor of h a1,a2 or d a2 .</p><p>Because d a2 is in D an ancestor of g a2,a3 , the vertex v is, in fact, in D an ancestor of h a1,a2 or g a2,a3 .</p><p>Both of these statements are also true for πa1,a2 (h a1,a2 , c a2 ) because πa1,a2 (h a1,a2 , c a2 ) is a subpath of πa1,a2 (h a1,a2 , d a2 ).</p><p>Step 6: g a2,a3 and h a1,a2 are adjacent or almost adjacent in D c (M p (D)) Consider the concatentation ψ = πa1,a2 (h a1,a2 , c a2 ) ⊕ πa2,a3 (c a2 , g a2,a3 ). This concatenation is a path (rather than a walk) in D because πa2,a3 (c a2 , g a2,a3 ) is a subpath of π a2,a3 ⊕ ρ a3 and because different ρ a do not intersect (by definition of collider extension structures). The junction point c a2 is a collider on ψ because both πa1,a2 (h a1,a2 , c a2 ) and πa2,a3 (c a2 , g a2,a3 ) are into c a2 , see steps 5 and 3. We now show that ψ is an inducing path relative to O(t hgp, t hg ).</p><p>To this end, we separately look at the colliders and non end-point non-colliers on ψ.</p><p>Colliders: According to steps 3 and 5, every collider on πa1,a2 (h a1,a2 , c a2 ) and on πa2,a3 (c a2 , g a2,a3 ) is in D an ancestor of h a1,a2 or g a2,a3 . The junction point c a2 is in D an ancestor of g a2,a3 according to step 1.</p><p>Non end-point non-colliders: Let v be a non end-point non-collider on ψ. Since c a2 is a collider on ψ, the vertex v is then a non end-point non-collider on πa1,a2 (h a1,a2 , c a2 ) or a non end-point non-collider on πa2,a3 (c a2 , g a2,a3 ). With steps 3 and 5 we then get t(v) &lt; t hgp.</p><p>Consequently, ψ is an inducing path relative to the set of observable vertices within O(t hg -p, t hg ). By shifting this structure forward in time by tt hg time steps, we see that the forward shifted copies of g a2,a3 and h a1,a2 are adjacent in M p (D). Hence, g a2,a3 and h a1,a2 are adjacent or almost adjacent in D c (M p (D)) according to Lemma D.11.</p><p>For reference further below we note that ψ is also on inducing path relative to O(t hg -p, t). This statement follows because, as shown, if v is a non end-point non-collider on ψ, then t(v) &lt; t hgp and thus v is not in O(t hgp, t) \ O(t hgp, t hg ).</p><p>Step 7: Properties of πa2,a3 c (g a2,a3 , d a3 ) Since πa2,a3 c is directed from d a2 to d a3 , see step 2, πa2,a3 c (g a2,a3 , d a3 ) is the trivial path consisting of the single vertex g a2,a3 = d a3 or a non-trivial directed path from g a2,a3 to d a3 and, hence, out of g a2,a3 and into d a3 . In particular, πa2,a3 c (g a2,a3 , d a3 ) is collider-free. Consider any vertex v on πa2,a3 c (g a2,a3 , d a3 ). Because in D c (M p (D)) there are no edges into unobservable vertices and because g a2,a3 is observable, v is observable. Moreover, because πa2,a3 c is canonical with respect to πa2,a3 , the vertex v is on πa2,a3 (g a2,a3 , d a3 ). Since g a2,a3 is on π a2,a3 ⊕ ρ a3 excluding c a2 , see step 3, also v is on π a2,a3 ⊕ ρ a3 excluding c a2 . In particular, v is not on ρ a2 .</p><p>Step 8: Properties of πa1,a2</p><p>c is canonical with respect to πa1,a2 , the vertex v is on πa1,a2 (d a1 , h a1,a2 ). Since h a1,a2 is on ρ a1 (d a1 , c a1 )⊕π a1,a2 excluding c a2 , see step 5, also v is on ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 excluding c a2 . In particular, we conclude that v is not on ρ a2 .</p><p>We now show that πa1,a2 c (d a1 , h a1,a2 ) and πa2,a3 c (g a2,a3 , d a3 ) do not intersect. Assume the opposite, i.e., let w be on both πa1,a2 c (d a1 , h a1,a2 ) and πa2,a3 c (g a2,a3 , d a3 ). There are two cases:</p><p>1. First, assume w is observable. Then, according to step 7 and the previous discussion in the current step, w is on ρ a1 (d a1 , c a1 )⊕π a1,a2 excluding c a2 and on π a2,a3 ⊕ρ a3 excluding c a2 . These observations contradict each other because ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 and π a2,a3 ⊕ ρ a3 intersect at c a2 only.</p><p>The combination of the four steps 11, 12, 10 and 13 shows that πa1,a3 c is canonical with respect to πa1,a3 .</p><p>Step 14: πa1,a3</p><p>, d a3 ) and assume πa1,a3 c is blocked given S \ {d a1 , d a3 }. Since πa1,a3 c is collider-free, see step 10, this premise means there is a non end-point vertex v on πa1,a3 c with v ∈ S \ {d a1 , d a3 }. There are three collectively exhaustive cases:</p><p>1. First, assume v is on πa1,a2 c (d a1 , h a1,a2 ). Then, because πa1,a2 c is canonical with respect to πa1,a2 , the vertex v is on πa1,a2 (d a1 , h a1,a2 ). Since πa1,a2 (d a1 , h a1,a2 ) is a subpath of ρ a1 (d a1 , c a1 ) ⊕ π a1,a2 excluding c a2 , see step 5, v is not on ρ a2 . In particular, v ̸ = d a2 and thus v ∈ S \ {d a1 , d a2 }. Moreover, since v ̸ = d a1 and h a1,a2 ̸ = d a2 by the respective definitions of v and h a1,a2 , the vertex v is a non end-point vertex on πa1,a2 c . Since πa1,a2 c is collider-free by means of being canonical with respect to πa1,a2 and since v ∈ S \ {d a1 , d a2 }, we arrive at a contradiction to the assumption that πa1,a2 c is active given S \ {d a1 , d a2 }. 2. Second, assume v is πa2,a3 c (g a2,a3 , d a3 ). Then, because πa2,a3 c is canonical with respect to πa2,a3 , the vertex v is on πa2,a3 (g a2,a3 , d a3 ). Since πa2,a3 (g a2,a3 , d a3 ) is a subpath of π a2,a3 ⊕ ρ a2 excluding c a2 , see step 3, v is not on ρ a2 . In particular, v ̸ = d a2 and thus v ∈ S \ {d a2 , d a3 }. Moreover, since v ̸ = d a3 and g a1,a3 ̸ = d a2 by the respective definitions of and g a2,a3 , the vertex v is a non end-point vertex on πa2,a3 c . Since πa2,a3 c is colliderfree by means of being canonical with respect to πa2,a3 and since v ∈ S \ {d a2 , d a3 }, we arrive at a contradiction to the assumption that πa2,a3 c is active given S \ {d a2 , d a3 }. 3. Third, assume v is on κ a2 . Then, since every element of S is observable, v is h a1,a2 or g a2,a3 and thus on πa1,a2 c (d a1 , h a1,a2 ) or πa2,a3 c (g a2,a3 , d a3 ). These cases have already been covered by the previous two points.</p><p>We have thus shown that πa1,a3 c is active given S \ {d a1 , d a3 }, which completes the proof.</p><p>In case at least one of πa1,a2  are, respectively, canonical with respect to πa1,a2 and πa2,a3 as well as , respectively, active given S \ {d a1 , d a2 } and S \ {d a2 , d a3 }, this procedure can be repeated in case, for example, πa1,a3 c or πa3,a4 ci is out of d a3 , and so on. LEMMA D.30. Let (i, t i ) and (j, t j ) with tp ≤ t i , t j ≤ t be distinct observable vertices in D and let S ⊆ O(tp, t) \ {(i, t i ), (j, t j )}. Then: If (i, t i ) and (j, t j ) are d-connected given S in D, then (i, t i ) and (j, t j ) are d-connected given S in D c (M p (D)).</p><p>PROOF OF LEMMA D.30. Let (i, t i ) and (j, t j ) be d-connected given S ⊆ O(tp, t) \ {(i, t i ), (j, t j )}. Then, in D there is path π between (i, t i ) and (j, t j ) that is active given S. The proof is by induction over n c , the number of colliders on π.</p><p>Induction base case: n c = 0 In this case, (i, t i ) and (j, t j ) are d-connected given S in D c (M p (D)) according to Lemma D.23.</p><p>Induction step: n c → n c + 1 In this case, π has n c + 1 ≥ 1 colliders and according to the assumption of induction we have already proven the statement for paths that have at most n c colliders. PROOF OF LEMMA D.32. The time series structure of D c (G) with T = Z is apparent from the first point of Def. 4.13, the repeating edges property is enforced explicitly in the second point of Def. 4.13, and time order is enforced explicitly in the second point of Def. 4.13 by only considering edges in E stat → of the form ((i, tτ ), (j, t)) in the first and second point of Def. 4.13. Assume there is a directed cycle in D c (G). Because as apparent from the second point of Def. 4.13 there are no edges into unobservable vertices, all vertices on the directed cycle are observable. Moreover, due to time order all vertices on the directed cycle must be at a single time step. Due to repeating edges there thus is a directed cycle at time t in D c (G). Since the second point of Def. 4.13 further shows that all edges between observable vertices at time t in D c (G) are also in stat(G), which is a subgraph of G, we get a contradiction to the acyclicity of G. PROOF OF LEMMA E.1. Note that A D is stronger than A ta and that A ta is stronger than A to . Thus, every graph consistent with A has repeating orientations. The statement then follows from the definition of m.i. DPAGs because every element in [M] A has repeating orientations.</p><p>PROOF OF LEMMA 5.5. From Lemma E.1 we know that P(M p (D), A) has repeating orientations. Moreover, P(M p (D), A) has past-repeating adjacencies because its by definition equals the skeleton of M p (D).</p><p>Let (i, t i ) and (j, t j ) with tp ≤ t i ≤ t j ≤ t be distinct observable vertices. According to Lemma D.1, these vertices are adjacent in stat(P(M p (D), A)) if and only if (i, t -(t jt i )) and (j, t) are adjacent in P(M p (D), A). Because the skeletons of P(M p (D), A) and M p (D) are the same, Lemma 4.7 gives that (i, t -(t jt i )) and (j, t) are adjacent in P(M p (D), A) if and only if (i, t i ) and (j, t j ) are adjacent in M p st (D). Consequently, stat(P(M p (D), A)) and M p st (D) have the same skeleton. Moreover, consider an unambiguous edge mark in stat(P(M p (D), A)). This edge mark is also in P(M p (D), A) and therefore corresponds to an ancestral relationship in M p (D). Because according to Lemma 4.10 the graphs M p (D) and M p st (D) have the same ancestral relationships, the same unambiguous edge mark is then also in M p st (D).</p><p>LEMMA E.2. Let M be a DMAG with time series structure that has repeating orientations and past-repeating adjacencies and for part 2 in addition is time ordered. Then:</p><p>is oriented as a collider in M if and only if (i, t i ) * - * (j, t j ) * - * (k, t k ) is oriented as a collider in stat(M). 2. Let π = (l, t l ) . . . * →(i, t i )← * (j, t j ) * →(k, t k ) with t j ≤ max(t l , t k ) be a discriminating path for (j, t j ) in stat(M) and let ∆t = t jmax(t l , t k ). Then: a) π ∆t , the copy of π shifted forward in time by ∆t time steps, is a discriminating path for (j, t j + ∆t) in M. b) (i, t i + ∆t) * - * (j, t j + ∆t) * - * (k, t k + ∆t) is oriented as a collider in M if and only if (i, t i ) * - * (j, t j ) * - * (k, t k ) is oriented as a collider in stat(M).</p><p>combining these observations we arrive at a contradiction since there are only finitely many edges between the finitely many vertices of M p (D).</p><p>2. Assume the opposite. Then, means there is a strictly monotonically increasing sequence a n of positive integers such that P p+an,[t-p,t] (D) ̸ = P p+an+1, [t-p,t] (D) for all n ∈ N. Let m be such that M p+am, [t-p,t] (D) = M p lim (D), which exists as a result of part 1 of Lemma B.16. Then, for all n ≥ m the skeletons of P p+an, [t-p,t] (D) and P p+an+1, [t-p,t] (D) are equal. Using part 1 of Lemma B.13 we thus learn that for all n ≥ m there is a non-circle mark in P p+an+1,[t-p,t] (D) that is not in P p+an, [t-p,t]  Maximality: Assume the opposite, i.e., assume in M[O] there are non-adjacent vertices i and j between which there is an inducing path π. Since M[O] is a subgraph of M, the inducing path π between i and j is also in M. Maximality of M thus implies that i and j are adjacent in M. By the definition of induced subgraphs the nodes i and j would then also be adjacent in M[O], a contradiction. PROOF OF LEMMA B.18. 1. According to part 1 of Lemma B.16, there is ∆p such that M p lim (D) = M p+∆p ′ ,[t-p,t] (D) = M p+∆p,[t-p,t] (D) for all ∆p ′ ≥ ∆p. Thus, since the ts-DMAG M p+∆p (D) has repeating orientations and past-repeating adjacencies, also M p lim (D) = M p+∆p,[t-p,t] (D) has both these properties.</p><p>To complete the proof, we need to show that M p lim (D) has repeating adjacencies. To this end, assume the opposite. Since M p lim (D) has past-repeating adjacencies, this assumption means in M p+∆p (D) there is an edge (i, t i -∆t) * - * (j, t j -∆t) with tp ≤ t i , t j ≤ t and ∆t &gt; 0 such that (i, t i ) and (j, t j ) are non-adjacent in M p+∆p (D). That (i, t i ) and (j, t j ) are non-adjacent in M p+∆p (D) shows the existence of S ⊆ O(tp -∆p, t) \ {(i, t i ), (j, t j )} with (i, t i ) ⊥ ⊥ (j, t j ) | S in D. Due to the repeating separating sets property of D, then (i, t i -∆t) ⊥ ⊥ (j, t j -∆t) | S -∆t where S -∆t is obtained by shifting all vertices in S backward in time by ∆t steps. The vertices (i, t i -∆t) and (j, t j -∆t) are thus non-adjacent in M p+∆p+∆t (D) and hence also non-adjacent in M p lim (D). This observation is in contradiction to the equality M p lim (D) = M p+∆p,[t-p,t] (D). 2. Let (i, t i ) and (j, t j ) with τ = t jt i ≥ 0 be distinct non-adjacent vertices in M p st (D). Then, the vertices (i, tτ ) and (j, t) are non-adjacent in M p st (D) due to the repeating edges property of M p st (D) and thus, using Lemma 4.7, also non-adjacent in M p (D). Hence, there is S ⊆ O(tp, t) \ {(i, tτ ), (j, t)} such that (i, tτ ) ⊥ ⊥ (j, t) | S in D. Due to the repeating separating sets property of D, we thus get that (i, t i ) and (j, t j ) are non-adjacent in M p+(t-tj) (D) and hence also non-adjacent in M p lim (D). Consequently, the skeleton of M p lim (D) is a subgraph of the skeleton of M p st (D). Next, let (i, t i ) * - * (j, t j ) be an edge in M p lim (D). Then, since the skeleton of M p lim (D) is a subgraph of the skeleton of M p st (D), the vertices (i, t i ) and (j, t j ) are also adjacent in M p st (D). Note that, since M p lim (D) = M p,[t-p,t] (D) for some p &gt; p according to part 1 of Lemma B.16, the orientation of (i, t i ) * - * (j, t j ) in M p lim (D) conveys an ancestral relationships according to D. Since also in M p st (D) the orientations of edges convey ancestral relationships according to D, we finally get that the edge (i, t i ) * - * (j, t j ) in M p st (D) has the same orientation as in M p lim (D). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Markov Equivalence for Ancestral Graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2808" to="2837" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Completeness of Causal Discovery in the Presence of Latent Confounding with Tiered Background Knowledge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="4002" to="4011" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discovery of extended summary graphs in time series</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="96" to="106" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<title level="m">Structural Equations with Latent Variables</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08784</idno>
		<title level="m">Causal modeling of dynamical systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Search for Additive Nonlinear Time Series Causal Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="967" to="991" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphical models for marked point processes based on local independence</title>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphical Gaussian modelling of multivariate time series with latent variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics 193-200. JMLR Workshop and Conference Proceedings</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics 193-200. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal Reasoning in Graphical Time Series Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Van Der Gaag</surname></persName>
		</editor>
		<meeting>the Twenty-Third Conference on Uncertainty in Artificial Intelligence<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
	<note>UAI&apos;07</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On Granger causality and the effect of interventions in time series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lifetime data analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3" to="32" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On Causal Discovery from Time Series Data using FCI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Workshop on Probabilistic Graphical Models</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</editor>
		<meeting>the 5th European Workshop on Probabilistic Graphical Models<address><addrLine>Helsinki, FI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
		<respStmt>
			<orgName>Helsinki Institute for Information Technology HIIT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent ancestral graph of structure vector autoregressive models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Engineering and Electronics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="233" to="238" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying independence in Bayesian networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="507" to="534" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Supplement to &quot;Characterization of causal ancestral graphs for time series with latent confounders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gerhardus</surname></persName>
		</author>
		<ptr target="https://doi.org/xx.xxx/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>provided by typesetter</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-recall causal discovery for autocorrelated time series with latent confounders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12615" to="12625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Projecting infinite time series graphs to finite marginal graphs using number theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Faltenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ninad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In preparation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Causal factors of anxiety and depression in college students: longitudinal ecological momentary assessment and causal analysis using Peter and Clark momentary conditional independence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Huckins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Dasilva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Obuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Holtzheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>JMIR mental health 7 e16684</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1709" to="1731" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using Causal Effect Networks to Analyze Different Arctic Drivers of Midlatitude Winter Circulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coumou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Donges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Climate</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4069" to="4081" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A generalized back-door criterion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1060" to="1088" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating Causal Effects with Ancestral Graph Markov Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Probabilistic Graphical Models</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Antonucci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Corani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Campos</surname></persName>
		</editor>
		<meeting>the Eighth International Conference on Probabilistic Graphical Models<address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="299" to="309" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal Structure Learning from Multivariate Time Series in Settings with Unmeasured Confounding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 ACM SIGKDD Workshop on Causal Disocvery</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>2018 ACM SIGKDD Workshop on Causal Disocvery<address><addrLine>London, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="23" to="47" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Markov equivalence of marginalized local independence graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mogensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="539" to="559" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constraint-Based Causal Discovery using Partial Ancestral Graphs in the presence of Cycles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</editor>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence (UAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DYNOTEARS: Structure Learning from Time-Series Data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pamfil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sriwattanaworachai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pilgerstorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Georgatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Calan-Dra</surname></persName>
		</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">;</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perkovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Textor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>New York, NY, USA. PEARL, J; Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000. 2009. 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="62" />
		</imprint>
	</monogr>
	<note>a)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perkovi Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Textor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="62" />
			<date type="published" when="2018">2018b)</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal Inference on Time Series using Restricted Structural Equation Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ancestral Graph Markov Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</editor>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence (UAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Escaping the Curse of Dimensionality in Estimating Multivariate Transfer Entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petoukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">258701</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Constructing brain connectivity model using causal network reconstruction approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saetia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">619557</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Statistics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Causal Inference in the Presence of Latent Variables and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Besnard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</editor>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
	<note>UAI&apos;95</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Causal Networks: Semantics and Expressiveness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Levitt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lemmer</surname></persName>
		</editor>
		<meeting><address><addrLine>North-Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
	<note>Uncertainty in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Causal inference and reasoning in causally insufficient systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Philosophy, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal Reasoning with Ancestral Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1437" to="1474" />
			<date type="published" when="2008">2008a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">) and (k 3 , t 3 ) are distinct observable vertices on π ci and (k 2 , t 2 ) is on π ci ((k 1 , t 1 )</title>
		<imprint/>
	</monogr>
	<note>k 3 , t 3 )), then (k 2 , t 2 ) is on π((k 1 , t 1 ), (k 3 , t 3</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">If (k 1 , t 1 ) and (k 2 , t 2 ) are distinct observable vertices on π ci , then π((k 1 , t 1 ), (k 2 , t 2 )) is an inducing path relative to O(max(t 1 , t 2 )p, t)</title>
		<imprint/>
	</monogr>
	<note>π ci ((k 1 , t 1 ), (k 2 , t 2 ))</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PROOF OF LEMMA D</title>
	</analytic>
	<monogr>
		<title level="m">If π is active given S in D, then π ci is active given S in D c (M p (D))</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note>be as in Lemma D.19 applied to π</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The definition of canonically induced paths is such that ((l 1 , s 1 ), . . . , (l m , s m )) is the sequence of all observable vertices on π ci . Moreover, all of (l 1 , s 1 ), . . . , (l m , s m ) are on π by construction</title>
		<imprint/>
	</monogr>
	<note>see Lemma D.19</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">t 1 ) is closer to (i, t i ) on π ci than (k 3 , t 3 ) is to (i, t i ) on π ci . Hence, there are 1 ≤ α 1 &lt; α 2 &lt; α 3 ≤ m such that (k 1 , t 1 ) = (l α1 , s α1 ), (k 2 , t 2 ) = (l α2 , s α2 ) and (k 3 , t 3 ) = (l α3 , s α3 ). Moreover, by definition of ((l 1 , s 1 ), . . . , (l m , s m )) and canonically induced paths, (l α , s α ) is closer to (i, t i ) on π than (l q , s q ) with α &lt; q is to (i, t i ) on π. 3. Assume there is a collider on π ci . Since in D c (M p (D)) there are no edges into unobservable vertices, all colliders on π ci are observable. Hence, there must be 1 &lt; α &lt; m such that (l α , s α ) is a collider on π ci , i.e., such that both π ci ((l α-1 , s α-1 ), (l α , s α )) and π ci ((l α , s α ), (l α+1 , s α+1 )) are into (l α , s α )</title>
	</analytic>
	<monogr>
		<title level="m">The repeating edges property of stat(M) together with t j ≤ max(t i , t k ) implies that (i, t i + ∆t) * - * (j, t j + ∆t) * - * (k, t k + ∆t) is an unshielded triple in stat(M). Lemma D.1 then guarantees that</title>
		<imprint/>
	</monogr>
	<note>Assume π((l α-1 , s α-1 ), (l α , s α )) is out of (l α , s α ). Since π is collider-free, it then follows that π((l α-1 , s α-1 ), (l α , s α )) is directed from (l α , s α ) to (l α-1 , s α-1 ) and hence (l α , s α ) ∈ an((l α-1 , s α-1 ), D). According to Def. D.21 this ancestral relationships requires PROOF OF LEMMA E.2. 1(a. i, t i + ∆t) = (i, t -(max(t i , t k )t i )) and (k, t k + ∆t) = (k, t -(max(t i , t k )t i )) are non-adjacent in M, because else they would be adjacent in stat(M) too and hence (i, t i + ∆t) * - * (j, t j + ∆t) * - * (k, t k + ∆t. would not be unshielded in stat(M). Since stat(M) is a subgraph of M, we thus get that (i, t i +</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">By the definition of discriminating paths, all vertices on π other than, perhaps, (j, t j ) and/or (l, t l ) are ancestors of (k, t k )</title>
		<author>
			<persName><forename type="first">*</forename><surname>- * (j, T J + ∆t) * - * (k, T K + ∆t</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">combination with the repeating edges property of stat(M) we thus see that π ∆t is a discriminating path for (j, t j + ∆t) in stat(M). Lemma D.1 then guarantees that</title>
		<imprint/>
	</monogr>
	<note>Time order of M together with t j ≤ max(t l , t k ) thus guarantees that all vertices on π are within. t-p, max(t l , t k ). l, t l + ∆t) = (l, t -(max(t l , t k. and (k, t k + ∆t) = (k, t -(max(t l , t k )t k )) are non-adjacent in M because else they would be adjacent in stat(M) too and hence π ∆t would not be a discriminating path in stat(M)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><surname>Consequently</surname></persName>
		</author>
		<title level="m">M because stat(M) is a subgraph of M. 1(b) and 2(b) Because stat(M) has repeating edges, the triple (i, t i ) * - * (j, t j ) * - * (k, t k ) is oriented as a collider in stat(M) if and only if (i, t i + ∆t) * - * (j, t j + ∆t) * - * (k, t k + ∆t) is oriented as a collider in stat(M)</title>
		<imprint/>
	</monogr>
	<note>Moreover, since stat(M) is a subgraph of M and (i, t i +</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">* - * (j, t j + ∆j) * - * (k, t k + ∆t) is oriented as collider in stat(M) if and only if (i, t i + ∆t) * - * (j, t j + ∆j) * - * (k, t k + ∆t) is oriented as a collider in M. LEMMA E.3. Let M 1 and M 2 be Markov equivalent DMAGs with time series structure that are time ordered and have repeating orientations and past-repeating adjacencies. Then, stat(M 1 ) and stat(M 2 ) are Markov equivalent DMAGs. PROOF OF LEMMA E.3. Both stat(M 1 ) and stat(M 2 ) are DMAGs according to Lemma D.2. Next, we show that stat(M 1 ) and stat(M 2 ) are Markov equivalent</title>
		<author>
			<persName><forename type="first">*</forename><surname>- * (j, T J + ∆j) * - * (k, T K + ∆t</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>is in M, the triple (i, t i + ∆t. For this purpose, assume the opposite. Then, according to the characterizing of Markov equivalence of MAGs in Spirtes and Richardson (1997), at least one of the following statements is true</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m">The skeletons of stat(M 1 ) and stat</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">* - * (j, t j ) * - * (k, t k ) in both stat(M 1 ) and stat(M 1 ) that is oriented as a collider in stat(M a ) with a ∈ {1, 2} and oriented as a non-collider in stat</title>
		<imprint/>
	</monogr>
	<note>There is an unshielded triple (i, t i. with ā = 3a</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">There is a path π that is in both stat(M 1 ) and stat(M 1 ) a discriminating path for (j, t j ) such that (j, t j ) is a collider on π in stat(M a ) with a ∈ {1, 2} and a non-collider in stat</title>
		<imprint/>
	</monogr>
	<note>with ā = 3a</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">We now show that stat(M 1 ) and stat(M 2 ) do have the same skeleton and that both the second and third statement contradict Markov equivalence of M 1 and M 2 . Case 1: Skeleton. According to Lemma D.1, the skeletons of stat(M 1 ) and stat(M 2 ) are determined uniquely by, respectively, the skeletons of M 1 and M 2 . Thus, since M 1 and M 2 have the same skeleton due to being Markov equivalent, also stat(M 1 ) and stat(M 2 ) have the same skeleton. Case 2: Unshielded colliders</title>
	</analytic>
	<monogr>
		<title level="m">Since (i, t i ) * - * (j, t j ) * - * (k, t k ) is oriented as a non-collider in stat(M ā), the vertex (j, t j ) is in stat(M ā) an ancestor (parent, in fact) of (i, t i ) or (k, t k )</title>
		<imprint/>
	</monogr>
	<note>Time order of stat(M ā) thus implies t j ≤ max. t i , t k ). Hence, we can apply part 1 of Lemma E.2 to both stat(M ā) and stat(M a. which gives that (i, t i + ∆t) * - * (j, t j +</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">+ ∆t) with ∆t j = tmax(t i , t k ) is an unshielded collider in M a and an unshielded non-collider in M ā. This observation contradicts the assumptiont that M 1 and M 2 are Markov equivalent</title>
		<author>
			<persName><forename type="first">* - * (</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Case 3: Discriminating paths. By definition of discriminating paths, π takes the form . .</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">In combination with the fact that (j, t j ) is in stat(M ā) an ancestor (parent, in fact) of (i, t i ) or (k, t k ) by means of (j, t j ) being a non-collider on π in stat(M ā), we thus find that (j, t j ) is in stat(M ā) an ancestor (parent, in fact) of (k, t k )</title>
		<author>
			<persName><forename type="first">)</forename><surname>* →(i, T I )← * (j, T J ) * →(k, T K</surname></persName>
		</author>
		<author>
			<persName><surname>Moreover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time order of stat(M ā) thus implies t j ≤ t k ≤ max(t l , t k ). Hence, we can apply part 2 of Lemma E.2 to both stat(M ā) and stat(M a ), which gives that π ∆t , the copy of π that is shifted forward in time by ∆t = tmax(t l , t k ) time steps, is a discriminating path for (j, t j + ∆t) in both M a and M ā and that (j, t j + ∆t) is a collider on π ∆t in M a whereas (j, t j + ∆t) is a non-collider on π ∆t in M ā. This observation contradicts Markov equivalence of M 1 and M 2 . PROOF OF THEOREM 3. 1. Note that stat(P(M p (D), A)) and P(M p st</title>
		<editor>i, t i )↔(j, t j ) if (j, t j )↔(k,</editor>
		<imprint/>
	</monogr>
	<note>A stat ) have the same skeleton because both of them are DPAGs for M p st. as follows from Lemma 5.5. We prove the statement by showing that (i, t i )•- * (j, t j ) in stat(P(M p (D), A)) implies (i, t i )•- * (j, t j ) in P(M p st (D), A stat</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>P(m P (d)</surname></persName>
		</author>
		<title level="m">Then, (i, t i )•- * (j, t j ) is also in P(M p (D), A) because P(M p (D), A) is a supergraph of stat(P(M p (D), A)). By definition of m.i. DPAGs, there thus are DMAGs M 1 and M 2 in [M p (D)] A such that (i, t i )→(j, t j ) in M 1 and (i, t i )← * (j, t j ) in M 2 . Without loss of generality we may assume that M 1 or M 2 is M p (D). Since i) according to Lemma E.3 stat(M 1 ) and stat(M 2 ) are Markov equivalent DMAGs and since ii) either M 1 or M 2 is M p (D) and hence either stat(M 1 ) = M p st (D) or stat(M 2 ) = M p st</title>
		<imprint/>
	</monogr>
	<note>Let the edge (i, t i )•- * (j, t j ) be in stat. we thus get that both stat(M 1 ) and stat(M 2 ) are in [M p st (D)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">that stat(M) is time ordered if M is time ordered, and that stat(M) is a stationarified ts-DMAG if M is a ts-DMAG. Hence, both stat(M 1 ) and stat(M 2 ) are in the Markov equivalence class [M p st (D)] A stat</title>
	</analytic>
	<monogr>
		<title level="m">A stat and since ii) (i, t i ) and (j, t j ) are adjacent in M p st (D), the vertices (i, t i ) and (j, t j ) are also adjacent in both stat(M 1 ) and stat</title>
		<imprint/>
	</monogr>
	<note>Since stat(M i ) is a subgraph of M i for i = 1, 2, we conclude that (i, t i )→(j, t j ) in stat(M 1 ) and (i, t i )← * (j, t j ) in stat. Hence, (i, t i )•- * (j, t j ) in P(M p st (D), A stat</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A)</title>
	</analytic>
	<monogr>
		<title level="m">This claim immediately follows from part 1 of Theorem 3 because stat(P(M p (D), A)) is a subgraph of P(M p</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A) have the same skeleton. E.2</title>
		<author>
			<persName><forename type="first">A</forename><surname>P(m P (d)</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>) ; P(m P (d)</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>(m P St (d)</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proofs for Sec. 5.4. PROOF OF LEMMA 5.9. The premise that (i, t i )•- * (j, t j ) is in the ts-DPAG P p (D) by definition of m.i. DPAGs and the background knowledge A D means: There are ts-DAGs D 1 and D 2 such that both M p (D 1 ) and M p (D 2 ) are Markov equivalent to M p (D) and (i, t i )→(j, t j ) in M p (D 1 ) and (i, t i )← * (j, t j ) in M p (D 2 ). Consequently, (i, t i ) ∈ an((j, t j ), M p (D 1 )) and (i, t i ) / ∈ an</title>
		<imprint/>
	</monogr>
	<note>is a subgraph of P(M p (D), A) and because stat. and thus, using Lemma C.1, (i, t i ) ∈ an((j, t j ), D 1 ) and (i, t i ) / ∈ an((j, t j</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m">t j ) with tp ≤ t i , t j ≤ t be distinct observable vertices in a ts-DAG D and let ∆t &gt; 0. Then: There is S ⊆ O(tp, t) \ {(i, t i ), (j, t j )} such that (i, t i ) ⊥ ⊥ (j, t j ) | S if and only if there is S ′ ⊆ O(tp, t + ∆t) \ {(i, t i )</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">1</forename><surname>Lemma</surname></persName>
		</editor>
		<editor>
			<persName><surname>Let</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>j, t j )} such that (i, t</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Example for proving part 2 of Lemma F.2. a) The canonical ts-DAG Dc(M 1 (D)) of the ts-DMAG M 1 (D) in part b) of Fig. B . b) The ts-DMAG M 2 (Dc(M 1 (D))) implied by the canonical ts-DAG in part a). PROOF OF LEMMA B.11. 1. This claim follows from the commutativity of the marginal</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>ization process as stated by Theorem 4.20 in Richardson and Spirtes</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m">According to part 2 of Lemma F.2, there is a ts-DAG D and p &gt; p ≥ 0 such M p(D) ̸ = M p(D c (M p (D))). 2 Moreover, Lemma 4.14 implies the equality M p (D) = M p (D c (M p (D))). Take D 1 = D and D 2 = D c (M p (D))</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Without loss of generality we may assume this edge to be of the form (i, t i )•- * (j, t j )</title>
		<author>
			<persName><forename type="first">Proof</forename><surname>Of Lemma B ; I, T I ) * - * (j, T J ) In P P(d)</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thus, by Lemma 5.9, there are ts-DAGs D 1 and D 2 -one of which without loss of generality is D-such that the ts-DMAGs M p(D 1 ) and M p(D 2 ) are Markov equivalent and that (i, t i ) ∈ an((j, t j ), D 1 ) and (i, t i ) / ∈ an((j, t j ), D 2 ). According to commutativity of the marginalization process as stated in Theorem 4.20 in Richardson and Spirtes (2002), the ts-DMAGs M p (D 1 ) and M p (D 2 ) are respectively obtained by marginalizing M p(D 1 ) and M p(D 2 ) over the vertices within</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Hence, given that M p(D 1 ) and M p(D 2 ) are Markov equivalent, so are M p (D 1 ) and M p. These considerations show that (i, t i )•- * (j, t j ) in P p (D)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Because P p(D) is a supergraph of P p</title>
		<author>
			<orgName type="collaboration">D</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">), the edge (i, t i )•- * (j, t j ) is then also in P p(D). Moreover, according to part 1 of Lemma B.10 (for ∆t = 0) in combination with the definition of DPAGs, (i, t i ) and (j, t j ) are adjacent in P p (D)</title>
		<imprint/>
	</monogr>
	<note>Let (i, t i )•- * (j, t j ) be an edge in P p. We conclude that the edge (i, t i )•- * (j, t j ) is P p (D) too because the opposite would contradict part 1 of Lemma B.13</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Using part 1 of Lemma B.10 with (p, p, ∆) → (p + a n</title>
		<author>
			<persName><forename type="first">Proof</forename><surname>Of Lemma B ; D) ̸ = M P+an+1</surname></persName>
			<affiliation>
				<orgName type="collaboration">D</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Moreover, using part 1 of Lemma B.10 for (p, p, ∆) → (p, p+a n , 0) and for (p, p, ∆) → (p, p+a n+1 , 0), we learn that both M p+an</title>
		<imprint/>
	</monogr>
	<note>Then, there is a strictly monotonically increasing sequence a n of positive integers such that M p+an. t-p,t] (D) and M p+an+1. t-p,t] (D) are subgraphs of M p (D). By</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">High-recall causal discovery for autocorrelated time series with latent confounders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12615" to="12625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Causal Structure Learning from Multivariate Time Series in Settings with Unmeasured Confounding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 ACM SIGKDD Workshop on Causal Disocvery</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>2018 ACM SIGKDD Workshop on Causal Disocvery<address><addrLine>London, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="23" to="47" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Ancestral Graph Markov Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">;</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2009. 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="962" to="1030" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A Polynomial Time Algorithm for Determining DAG Equivalence in the Presence of Latent Variables and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno>PMLR on 30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting>the Sixth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="1997-03">1997. March 2021</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research R1</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Equivalence and Synthesis of Causal Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence. UAI &apos;90</title>
		<meeting>the Sixth Annual Conference on Uncertainty in Artificial Intelligence. UAI &apos;90<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science Inc</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Causal Reasoning with Ancestral Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1437" to="1474" />
			<date type="published" when="2008">2008a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
