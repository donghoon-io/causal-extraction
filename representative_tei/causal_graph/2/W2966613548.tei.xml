<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Achieving Causal Fairness through Generative Adversarial Networks</title>
				<funder ref="#_maENC9X #_a2XmBjC #_rzkfsEN">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Depeng</forename><surname>Xu</surname></persName>
							<email>depengxu@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongkai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
							<email>xintaowu@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Achieving Causal Fairness through Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Achieving fairness in learning models is currently an imperative task in machine learning. Meanwhile, recent research showed that fairness should be studied from the causal perspective, and proposed a number of fairness criteria based on Pearl's causal modeling framework. In this paper, we investigate the problem of building causal fairnessaware generative adversarial networks (CFGAN), which can learn a close distribution from a given dataset, while also ensuring various causal fairness criteria based on a given causal graph. CFGAN adopts two generators, whose structures are purposefully designed to reflect the structures of causal graph and interventional graph. Therefore, the two generators can respectively simulate the underlying causal model that generates the real data, as well as the causal model after the intervention. On the other hand, two discriminators are used for producing a close-to-real distribution, as well as for achieving various fairness criteria based on causal quantities simulated by generators. Experiments on a real-world dataset show that CFGAN can generate high quality fair data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fairness-aware learning is receiving an increasing attention in machine learning fields. How to obtain the training data that satisfy fairness is an important research problem, as machine learning models learned from biased training data may also have biased performance against sensitive attributes, such as gender, race, age <ref type="bibr" target="#b5">[Pedreshi et al., 2008;</ref><ref type="bibr">Zliobaite et al., 2011;</ref><ref type="bibr" target="#b3">Hardt et al., 2016;</ref><ref type="bibr" target="#b7">Zhang et al., 2017;</ref><ref type="bibr">Zhang et al., 2018b]</ref>. In the literature, many methods have been proposed to modify the training data for mitigating biases and achieving fairness. These methods include: Massaging [Kamiran and <ref type="bibr" target="#b0">Calders, 2009]</ref>, Reweighting <ref type="bibr" target="#b0">[Calders et al., 2009]</ref>, Sampling <ref type="bibr">[Kamiran and Calders, 2012]</ref>, Disparate Impact Removal <ref type="bibr">[Feldman et al., 2015]</ref>, Causal-based Removal <ref type="bibr" target="#b7">[Zhang et al., 2017;</ref><ref type="bibr">Zhang et al., 2018c]</ref> and Fair Representation Learning <ref type="bibr" target="#b2">[Edwards and Storkey, 2016;</ref><ref type="bibr" target="#b6">Xie et al., 2017;</ref><ref type="bibr" target="#b4">Madras et al., 2018;</ref><ref type="bibr">Zhang et al., 2018a]</ref>.</p><p>As the general requirement of modifying datasets is to preserve the data utility as much as possible, a recent study leverages Generative Adversarial Networks (GAN) for generating high quality fair data <ref type="bibr">[Xu et al., 2018]</ref>. GAN is a generative model that has demonstrated impressive performance on generating synthetic data that are indistinguishable from real data <ref type="bibr" target="#b3">[Goodfellow et al., 2014]</ref>. The idea of GAN is to let a generator and a discriminator play the adversarial game with each other. In <ref type="bibr">[Xu et al., 2018]</ref>, the authors modify the architecture of GAN to consist of one generator and two discriminators, where one discriminator aims to ensure closeto-real generation and the other discriminator aims to ensure fairness. Their method, entitled FairGAN, can meet both requirements of high data utility and fairness.</p><p>The critical limitation of FairGAN is that it can only achieve fairness in terms of a simple statistical-based fairness criterion called demographic parity. However, as paid increasing attentions recently by researchers, fairness is a causal notion that concerns the causal connection between the sensitive attributes and the challenged decisions or outputs <ref type="bibr" target="#b7">[Zhang et al., 2017;</ref><ref type="bibr" target="#b7">Zhang and Bareinboim, 2018;</ref><ref type="bibr">Nabi and Shpitser, 2018;</ref><ref type="bibr" target="#b4">Kusner et al., 2017;</ref><ref type="bibr" target="#b1">Chiappa, 2019;</ref><ref type="bibr" target="#b6">Wu et al., 2019;</ref><ref type="bibr" target="#b6">Salimi et al., 2019]</ref>. Based on Pearl's causal modeling framework <ref type="bibr">[Pearl, 2009]</ref>, a number of causal-based fairness notions and criteria have been proposed, including total effect <ref type="bibr" target="#b7">[Zhang and Bareinboim, 2018]</ref>, direct discrimination <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref>, indirect discrimination <ref type="bibr" target="#b6">[Zhang et al., 2017]</ref>, and counterfactual fairness <ref type="bibr" target="#b4">[Kusner et al., 2017]</ref>. Each notion captures fairness in one particular situation from the causal perspective. Total effect treats all causal effects from the sensitive attribute to the decision as unfair. Direct and indirect discrimination, on the other hand, consider the situation where discrimination is transmitted through certain paths in the causal graph. Counterfactual fairness, again considers a different situation where we focus on the fairness with respect to a particular individual or a subgroup of individuals instead of the whole population. The causal blindness of FairGAN makes it unable to handle some causal-based notions such as counterfactual fairness, and may also affect its utility as it may remove both causal and spurious effects.</p><p>In this paper, we propose a causal fairness-aware generative adversarial network (CFGAN) for generating data that achieve various causal-based fairness criteria. Motivated by <ref type="bibr">CausalGAN [Kocaoglu et al., 2018]</ref>, we preserve the causal structure in the generator by arranging the neural network structure of the generator following a given causal graph. As a result, the generator can be considered as to simulate the underlying causal model of generating the observational data.<ref type="foot" target="#foot_1">foot_1</ref> Then, in order to handle different fairness criteria, we adopt two generators for explicitly modeling the real world and the world after we perform some hypothetical interventions. The two generators differ in some aspects to reflect the effect of interventions, but are also synchronized in terms of sharing parameters to reflect the connections between the two worlds. Then we adopt two discriminators for achieving both the high data utility and causal-based fairness. Experiments using the real world dataset show that CFGAN can generate high quality fair data based on different criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Model and Intervention</head><p>Definition 1. A causal model <ref type="bibr">[Pearl, 2009]</ref> is a triple M = {U, V, F} where 1) U is a set of hidden random variables that are determined by factors outside the model. A joint probability distribution P (U) is defined over the variables in U.</p><p>2) V is a set of observed random variables that are determined by variables in U ∪ V.</p><p>3) F is a set of deterministic functions; for each</p><formula xml:id="formula_0">V i ∈ V, a corresponding function f Vi is a mapping from U∪(V \{V i }) to V i , i.e., V i = f Vi (P a Vi , U Vi ), where P a Vi ⊆ V\{V i } is called the parents of V i , and U Vi ⊆ U.</formula><p>A causal model is often illustrated by a causal graph G <ref type="bibr">[Pearl, 2009]</ref>, where each observed variable is represented by a node, and the causal relationships are represented by directed edges →. In this graphical representation, the definition of parents is consistent with that in the causal model. In addition, each node V i is associated with a conditional distribution given all its parents, i.e., P (V i |P a Vi ).</p><p>Inferring causal effects in the causal model is facilitated by do-operator <ref type="bibr">[Pearl, 2009]</ref>, which simulates the physical intervention that forces some variable X ∈ V to take certain value x. For a causal model M, intervention do(X = x) is performed by replacing original function X = f X (P a X , U X ) with X = x. After replacing, the distributions of all variables that are the descendants of X may be changed. We call the causal model after the intervention the interventional model, denoted by M x . Correspondingly, M x can be illustrated by the interventional graph G x where all incoming edges to X are deleted and node X is replaced with constant x. The interventional distribution for any Y ⊆ V \ {X} is denoted by P (Y|do(X = x)) or P (Y x ). Symbolically, P (Y x ) can be expressed as a truncated factorization formula <ref type="bibr">[Pearl, 2009]</ref> and computed from the observed distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Effects</head><p>With the help of do-operator, we can infer the causal effect of X on Y by comparing the difference in interventional distributions under different interventions. Based on how the intervention is transferred in the causal model (graph), there are mainly three types of causal effects: total effect, path-specific effect, counterfactual effect <ref type="bibr">[Pearl, 2009]</ref>.</p><p>The total effect measures the causal effect of X on Y where the intervention is transferred along all causal paths (i.e., directed paths) from X to Y . Definition 2. The total effect of the value change of X from</p><formula xml:id="formula_1">x 1 to x 2 on Y is given by T E(x 2 , x 1 ) = P (Y x2 ) -P (Y x1 ).</formula><p>The path-specific effect measures the causal effect of X on Y where the intervention is transferred only along a subset of causal paths from X to Y , which is also referred to as the π-specific effect denoting the subset of causal paths as π.</p><p>Definition 3. Given a path set π, the π-specific effect of the value change of X from x 1 to x 2 on Y (with reference x 1 ) is given by SE π (x 2 , x 1 ) = P (Y x2|π ) -P (Y x1|π ), where P (Y x|π ) represents the interventional distribution where the intervention is transferred only along π.</p><p>In the total effect and path-specific effect, the intervention is performed on the whole population. The counterfactual effect measures the causal effect while the intervention is performed conditioning on only certain individuals or groups specified by a subset of observed variables</p><formula xml:id="formula_2">O = o. Definition 4. Given a context O = o, the counterfactual effect of the value change of X from x 1 to x 2 on Y is given by CE(x 2 , x 1 |o) = P (Y x2 |o) -P (Y x1 |o).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generative Adversarial Network</head><p>Generative Adversarial Networks (GAN) <ref type="bibr" target="#b3">[Goodfellow et al., 2014]</ref> are generative models that consist of two components: a generator and a discriminator. Typically, both the generator and discriminator are multilayer neural networks. Generator G(Z) takes random noises Z as input and attempts to learn a generative distribution P G to match the real data distribution P data . On the contrary, the discriminative model D is a binary classifier that predicts whether an input is a real data x or a generated fake data from G(Z). By playing the adversarial game, GAN is formalized as a minimax problem min</p><formula xml:id="formula_3">G max D V (G, D) with: V (G, D) = E x∼P data [log D(x)] + E z∼P (Z) [log(1 -D(G(z)))],</formula><p>where D(•) outputs the probability that • is from real data rather than generated fake data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CausalGAN</head><p>Research in <ref type="bibr">[Kocaoglu et al., 2018]</ref> shows that GAN can be modified to generate both observational and interventional distributions while preserving the causal structure among all attributes, referred to as the CausalGAN. Given a causal graph, generator G(Z) attempts to play the role of a causal model that agrees with this causal graph in terms of both the graph structure and conditional distributions. To this end, noises Z are partitioned into |V| subsets {Z V1 , Z V2 , . . .}, each of which Z Vi plays the role of hidden variables U Vi . Similarly, generator G(Z) is partitioned into |V| sub-neural networks {G V1 , G V2 , . . .}, each of which G Vi plays the role of function f Vi for generating the values of V i . Then, if node V j is a parent of V i in the causal graph, the output of G Vj is designed as an input of G Vi to reflect this connection. Meanwhile, the adversarial game is played to ensure P G (G(Z) = v) = P (V = v), ∀v. The authors have proved that G(Z) is consistent with any causal model that agrees with the same causal graph in terms of any identifiable interventional distributions, if: (1) P (V) is strictly positive; (2) the connections of sub-neural networks G Vi are arranged to reflect the causal graph structure; and (3) the generated observational distribution matches the real observational distribution, i.e., P G (G(Z) = v) = P (V = v), ∀v. Therefore, CausalGAN can be used to simulate the real causal model that agrees with the causal graph in identifiable situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CFGAN</head><p>To discuss the design of CFGAN, we first formulate our problem (Section 3.1), and then discuss the overall framework (Section 3.2). The CFGAN based on different fairness criteria will be discussed in Sections 3.3, 3.4 and 3.5. For all types of causal effects, we simply assume they are identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>In this paper, we follow the conventional notations in fairness-aware learning. We consider V = {X, Y, S}, where S denotes the sensitive variable, Y denotes the decision variable, and X denotes the set of all other variables (profile attributes). Given a causal graph G = (V, E) and a dataset with m samples (x, y, s) ∼ P data = P (V), the goal of CF-GAN is to (1) generate new data (x, ŷ, ŝ) which preserves the distribution of all attributes in the real data and (2) ensure that in the generated data Ŝ has no discriminatory effect on Ŷ in terms of various causal-based criteria. Note that we use the hatted variables to denote the fake data generated by the generator. For ease of discussion, we consider both S and Y as binary variables, where s + denotes S = 1 and s -denotes S = 0. It's straightforward to extend this to the multicategorical or numerical cases. In this paper we mostly discuss the causal effect of a single variable S on another single variable Y . However, the model is capable to handle causal effects between multiple variables as well.</p><p>We consider causal fairness criteria based on total effect <ref type="bibr" target="#b7">[Zhang and Bareinboim, 2018]</ref>, direct discrimination <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref>, indirect discrimination <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref>, and counterfactual fairness <ref type="bibr" target="#b4">[Kusner et al., 2017]</ref>, defined below. Definition 5. There is no total effect in the data if T E(s + , s -) = 0. Definition 6. There is no direct discrimination in the data if SE π d (s + , s -) = 0, where π d is the path set that only contains the direct edge from S to Y , i.e., S → Y . Definition 7. Given a subset of attributes R ⊆ X that cannot be objectively justified in decision making, there is no indirect discrimination in the data if SE πi (s + , s -) = 0, where π i is the set of causal paths from S to Y that pass through R. Definition 8. Given a subset of attributes O ⊆ X, counterfactual fairness is achieved in the data if CE(s + , s -|o) = 0 under any context O = o. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Framework</head><p>We propose the CFGAN model which consists of two generators (G 1 , G 2 ) and two discriminators (D 1 , D 2 ). Figure <ref type="figure" target="#fig_0">1</ref> shows the framework of CFGAN.</p><p>As shown in Sections 2.2 and 3.1, in general, causal-based fairness criteria compare the intervention distributions of Y under two different interventions do(S = s + ) and do(S = s -). To implement these criteria, CFGAN adopts two generators. One generator G 1 plays the role of original causal model M similar to CausalGAN, while the other generator G 2 explicitly plays the roles of different interventional models M s based on the type of causal effects. Generator G 1 aims to generate observational data whose distribution is close to the real observational distribution, and generator G 2 aims to generate interventional data that satisfy the criterion defined in Section 3.1. The two generators share the input noises and parameters to reflect the connections between the two causal models, and differ in connections of sub-neural networks to reflect the intervention. Then, CFGAN adopts two discriminators, where one discriminator D 1 tries to distinguish the generated data from the real data, and the other discriminator D 2 tries to distinguish the two intervention distributions under do(S = s + ) and do(S = s -). Finally, generators and discriminators play the adversarial game to produce high quality fair data.</p><p>Next, we give the details in designing the generators and discriminators for different fairness criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CFGAN based on Total Effect</head><p>We first show the CFGAN with no total effect (Definition 5). Generators. Generator G 1 is designed to agree with the causal graph G = (V, E). It consists of |V| sub-neural networks, where each of them corresponds to a node in V. All sub-neural networks are connected following the connections in G. To be specific, each sub-neural network G 1</p><p>Vi takes as input an independent noise vector Z Vi as well as the output of any other sub-neural network</p><formula xml:id="formula_4">G 1 Vj if V j is a parent of V i in G.</formula><p>Then, it outputs sample values of V i , i.e., vi .</p><p>The other generator G 2 is designed to agree with the interventional graph G s = (V, E \ {V j → S} Vj ∈Pa S ), where all incoming edges to S are deleted under intervention do(S = s). The structure of G 2 is similar to that of G 1 , except for   <ref type="table"></ref>and<ref type="table" target="#tab_1">G 2</ref> S ≡ 0 if s = s -. The two generators G 1 and G 2 are synchronized by sharing the same set of parameters for each pair of corresponding sub-neural networks, i.e., G 1</p><formula xml:id="formula_5">P G 2 (A s + , B s + , Y s + ) (red) and P G 2 (A s -, B s -, Y s -) (green) respectively. that sub-neural network G 2 S is set as G 2 S ≡ 1 if s = s + ,</formula><p>Vi and G 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vi</head><p>for each V i except for S, as well as the same noise vectors Z = z. As a result, G 1 can generate samples from the observational distribution, and G 2 can generate samples from two interventional distributions, i.e., (x, ŷ, ŝ</p><formula xml:id="formula_6">) ∼ P G 1 (X, Y, S), (x s + , ŷs + ) ∼ P G 2 (X s + , Y s + ), if s = s + , (x s -, ŷs -) ∼ P G 2 (X s -, Y s -), if s = s -.</formula><p>Consider an example in Figure <ref type="figure" target="#fig_2">2</ref> which involves 4 variables {A, S, B, Y }. Figure <ref type="figure" target="#fig_2">2a</ref> shows the causal graph G and the interventional graph G s under do(S = s), where the double headed arrows indicate the pair of nodes that share the same hidden variables and the function. Figure <ref type="figure" target="#fig_2">2b</ref> shows the structures of the generators where G 1 agrees with G and G 2 agrees with G s . The double headed arrows indicate the sharing of noises and parameters of sub-neural networks. As shown, the edge from A to S is deleted in G s , which is also reflected in G 2 . In addition, for each pair of nodes in the graphs, e.g., B in G and B in G s , the corresponding sub-neural networks are also synchronized, e.g., G 1 B and G 2 B . Discriminators. Discriminator D 1 is designed to distinguish between the real observational data (x, y, s) ∼ P data (X, Y, S) and the generated fake observational data (x, ŷ, ŝ) ∼ P G 1 (X, Y, S). The other discriminator D 2 is designed to distinguish between the two interventional distributions ŷs</p><formula xml:id="formula_7">+ ∼ P G 2 (Y s + ) and ŷs -∼ P G 2 (Y s -).</formula><p>Putting the generators and discriminators together, generator G 1 plays the adversarial game with the discriminator D 1 , and generator G 2 plays the adversarial game with the discriminator D 2 . The overall minimax game is described as:</p><formula xml:id="formula_8">min G 1 ,G 2 max D 1 ,D 2 J(G 1 ,G 2 ,D 1 ,D 2 ) = J 1 (G 1 ,D 1 ) + λJ 2 (G 2 ,D 2 ), where J 1 (G 1 , D 1 ) = E (x,y,s)∼P data (X,Y,S) [log D 1 (x, y, s)] + E (x,ŷ,ŝ)∼P G 1 (X,Y,S) [1 -log D 1 (x, ŷ, ŝ)], J 2 (G 2 , D 2 ) = E ŷs + ∼P G 2 (Y s + ) [log D 2 (ŷ s + )] + E ŷs -∼P G 2 (Y s -) [1 -log D 2 (ŷ s -)],</formula><p>and λ is a hyperparameter which controls a trade-off between utility and fairness of data generation. The first value function J 1 aims to achieve P G 1 (X, Y, S) = P data (X, Y, S), i.e., to make the generated observational data indistinguishable from the real data. The second value function J 2 aims to achieve</p><formula xml:id="formula_9">P G 2 (Y s + ) = P G 2 (Y s -).</formula><p>Since Definition 5 requires T E(s + , s -) = 0, or equivalently P (Y s + ) = P (Y s -), J 2 actually makes the generated interventional data satisfy the fairness criterion. As G 1 and G 2 share the same sets of parameters, the observational data generated by G 1 can be considered as being generated by a causal model which is close to the real causal model and also satisfies the fairness criterion.</p><p>Finally, the generated fair data can be released to public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CFGAN based on Direct and Indirect Discrimination</head><p>Both direct and indirect discrimination are based on pathspecific effects. In this section, we focus on the indirect discrimination criterion, and direct discrimination criterion can be achieved similarly. Given a path set π i that contains the paths pass through unjustified attributes, Definition 7 requires that SE πi (s + , s -) = 0, or equivalently P (Y s</p><formula xml:id="formula_10">+ |π i ) = P (Y s -|π i ) with reference s -.</formula><p>The design of generator G 1 is similar to that in Section 3.3, but G 2 is different in that it needs to simulate the situation where the intervention is transferred along π i only. To this end, we first similarly design the structure of G 2 to agree with the interventional graph G s = (V, E \ {V j → S} Vj ∈Pa S ). Then, we consider two types of value settings for sub-neural network G 2 S : the reference setting and the interventional setting. For the reference setting, G 2 S is always set as G 2 S ≡ 0. For the interventional setting, G 2 S is set as G 2 S ≡ 1 if s = s + and G 2 S ≡ 0 if s = s -. On the other hand, each of other sub-neural networks may output two types of sample values according to the value setting of G 2 S , referred to as the reference value and interventional value respectively. For a subneural network, if its corresponding node is not on any path in π i , it always takes reference values as input and outputs reference values. However, for any other sub-neural network G 2 Vj that is on at least one path in π i , it may take both types of val- generates fair decisions using a classifier that is built on nondescendants of S. A3 is similar to A1 but presupposes an additive noise model for estimating noise terms, which are then used for building the classifier. For both A1 and A3, we use SVM as the classifier for generating fair decisions.</p><p>For data utility, we compute the χ 2 distance, where a smaller χ 2 indicates better utility. We also use the generated data to train classifiers and measure the accuracy. We evaluate 4 classifiers: support vector machine (SVM), decision tree (DT), logistic regression (LR) and random forest (RF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Total Effect</head><p>We calculate the total effect for the original dataset and different generated datasets. The results are shown in Table <ref type="table" target="#tab_0">1</ref>. As can be seen, the original data has a total effect of 0.1936, and CausalGAN preserves similar total effect. FairGAN produces no total effect, but with the worst utility in terms of χ 2 . This may be because FairGAN removes too much information due to its causal blindness. The generated data by CFGAN based on total effect (CFGAN (TE), λ = 1) produces no total effect, and also preserves good data utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Indirect Discrimination</head><p>For indirect discrimination, we consider all the paths passing through marital status as π i . The results are also shown in Table <ref type="table" target="#tab_0">1</ref>. Similar to total effect, CausalGAN preserves indirect discrimination close to the original data, and FairGAN removes indirect discrimination but causes the largest utility loss. On the other hand, PSE-DR and our method (CFGAN (SE), λ = 1) can remove indirect discrimination and also have good data utility. We see that the two methods achieve comparable performance based on different techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Counterfactual Fairness</head><p>For counterfactual fairness, we consider the observation of two attributes, i.e., O = {race, native country}, which has 4 value combinations. Table <ref type="table" target="#tab_1">2</ref> shows the results for all 4 subgroups. As can be seen, the original data and CausalGAN contain biases in terms of counterfactual fairness in all subgroups. A1 is counterfactual fair as expected since it is proved to be so in <ref type="bibr" target="#b4">[Kusner et al., 2017]</ref>. However, the data utility is bad especially in terms of classifier accuracy, since it only uses non-descendants of sex in labeling decisions. A3 cannot achieve counterfactual fairness, probably because its linear assumption does not fit the original data well. Finally, our method (CFGAN (CE), λ = 1) achieves both counterfactual fairness and good data utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Sensitivity</head><p>We evaluate the trade-off between utility and fairness when changing λ in the overall minimax game. A larger λ indicates a stronger enforcement on the fairness and compromise on utility. Figure <ref type="figure">5</ref> shows the results for total effect, where we get a fairly good trade-off between utility and fairness at λ = 1. We observe similar results for other fairness types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed the causal fairness-aware generative adversarial networks (CFGAN) for generating high quality fair data. We considered various causal-based fairness criteria, including total effect, direct discrimination, indirect discrimination, and counterfactual fairness. CFGAN consists of two generators and two discriminators. The two generators aim to simulate the original causal model and the interventional model. This is achieved by arranging the neural network structure of the generators following the original causal graph and the interventional graph. Then, two discriminators are adopted for achieving both the high data utility and causal fairness. Experiments using the Adult dataset showed that CFGAN can achieve all types of fairness with relatively small utility loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of CFGAN</figDesc><graphic coords="3,327.15,54.00,218.70,159.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Causal graph G and interventional graph Gs (b) Generators G 1 and G 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the generators G 1 and G 2 for CFGAN based on total effect. S is set to 1 or 0 to sample from the interventional distributions P G 2 (A s + , B s + , Y s + ) (red) and P G 2 (A s -, B s -, Y s -) (green) respectively.</figDesc><graphic coords="4,99.19,90.47,133.06,102.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The total effect and indirect discrimination of real and generated datasets</figDesc><table><row><cell></cell><cell cols="4">Total effect Indirect discrimination</cell><cell>χ 2</cell><cell>SVM</cell><cell>Classifier accuracy DT LR</cell><cell>RF</cell></row><row><cell>Real data</cell><cell cols="2">0.1936</cell><cell>0.1754</cell><cell></cell><cell>0</cell><cell cols="2">0.8178 0.8177 0.8170 0.8178</cell></row><row><cell>CausalGAN</cell><cell cols="2">0.1721</cell><cell>0.1508</cell><cell></cell><cell cols="3">14482 0.8143 0.8136 0.8160 0.8137</cell></row><row><cell>FairGAN</cell><cell cols="2">0.0021</cell><cell>0.0133</cell><cell></cell><cell cols="3">41931 0.8088 0.8081 0.8136 0.8082</cell></row><row><cell>PSE-DR</cell><cell>NA</cell><cell></cell><cell>0.0243</cell><cell></cell><cell cols="3">12468 0.8073 0.8073 0.8128 0.8075</cell></row><row><cell>CFGAN (TE)</cell><cell cols="2">0.0102</cell><cell>NA</cell><cell></cell><cell cols="3">14566 0.8134 0.8126 0.8120 0.8127</cell></row><row><cell>CFGAN (SE)</cell><cell>NA</cell><cell></cell><cell>0.0030</cell><cell></cell><cell cols="3">19724 0.8037 0.8030 0.8103 0.8024</cell></row><row><cell></cell><cell>o1</cell><cell cols="2">Counterfactual effect o2 o3</cell><cell>o4</cell><cell>χ 2</cell><cell>SVM</cell><cell>Classifier accuracy DT LR</cell><cell>RF</cell></row><row><cell>Real data</cell><cell cols="2">0.2023 0.1293</cell><cell>0.1266</cell><cell>0.1785</cell><cell>0</cell><cell cols="2">0.8178 0.8177 0.8170 0.8178</cell></row><row><cell>CausalGAN</cell><cell cols="2">0.1824 0.1155</cell><cell>0.1466</cell><cell>0.0959</cell><cell cols="3">14482 0.8143 0.8136 0.8160 0.8137</cell></row><row><cell>A1</cell><cell cols="2">0.0000 0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell cols="3">17757 0.7615 0.7615 0.7615 0.7615</cell></row><row><cell>A3</cell><cell cols="2">0.2159 0.1127</cell><cell>0.1056</cell><cell>0.1860</cell><cell cols="3">12313 0.8159 0.8159 0.8159 0.8159</cell></row><row><cell cols="5">CFGAN (CE) 0.0209 0.0034 -0.0030 -0.0482</cell><cell cols="3">13904 0.8130 0.8123 0.8130 0.8115</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The counterfactual effect of real and generated datasets (O = {race, native country})</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>It is worth noting that causal effects may not be estimated from observational data in certain situations, referred to as unidentifiable situations. The generator can be treated as simulating the true causal model only in identifiable situations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">1646654</rs>, <rs type="grantNumber">1564250</rs>,  and <rs type="grantNumber">1841119</rs>.   </p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_maENC9X">
					<idno type="grant-number">1646654</idno>
				</org>
				<org type="funding" xml:id="_a2XmBjC">
					<idno type="grant-number">1564250</idno>
				</org>
				<org type="funding" xml:id="_rzkfsEN">
					<idno type="grant-number">1841119</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ues as input and output both. Specifically, for any sub-neural network G 2</p><p>Vi where V i is a child of V j , if edge V j → V i does not belong to any path in π i , then G 2 Vj will feed the reference output values to G 2</p><p>Vi . Otherwise, the interventional output values will be fed. As a result, the interventional distribution generated by G 2 simulates the situation of the path-specific effect, which we denote as P G 2 (X s|π , Y s|π ).</p><p>Consider an example (Figure <ref type="figure">3</ref>) with the same causal graph in B and reference values from G 2 S ≡ 0 as input. To achieve no indirect discrimination, discriminator D 2 is designed to distinguish between two interventional distributions ŷs</p><p>By playing the adversarial game with G 2 , the corresponding value function J 2 aims to achieve</p><p>Similarly, since G 1 and G 2 share the parameters, the observational data generated by G 1 can also be considered as satisfying the no indirect discrimination criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CFGAN for Counterfactual Fairness</head><p>In counterfactual fairness, the intervention is performed conditioning on a subset of variables O = o. Thus, different from previous fairness criteria that concern the interventional model only, counterfactual fairness concerns the connection between the original causal model and the interventional model. We reflect this connection in CFGAN by building a direct dependency between the samples generated by G 1 and the samples generated by G 2 . Specifically, the structures of G 1 and G 2 are similar to those in Section 3.3. However, for each noise vector z, we first generate the observational sample by using G 1 , and observe whether in the sample we have O = o. Only for those noise vectors with O = o in the generated samples, we use them for gen-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>The dataset we use for evaluation is the UCI Adult income dataset <ref type="bibr">[Dheeru and Karra Taniskidou, 2017]</ref>. It contains 65,123 samples with 11 variables. Following the setting in <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref>, we binarize each attribute to reduce the complexity for causal graph discovery. We treat sex as the sensitive variable S, income as the decision variable Y . The estimated causal graph is shown in Figure <ref type="figure">4</ref>.</p><p>We evaluate the performance of CFGAN in generating fair data for different types of causal fairness. The fairness threshold is 0.05, i.e., the effect should be in [-0.05, 0.05] to be fair. We compare CFGAN with other data generating approaches for different fairness respectively as other approaches may only be able to achieve one or two types of fairness.</p><p>Specifically, we consider two baselines: (1) the original dataset; and (2) <ref type="bibr">CausalGAN [Kocaoglu et al., 2018]</ref>, which preserves the causal structure of the original data but is unaware of the fairness constraint. For total effect, we compare with <ref type="bibr">FairGAN [Xu et al., 2018]</ref>, which removes all information correlated to S in other attributes. For indirect discrimination (we skip the results for direct discrimination as the original dataset contains no direct discrimination), we further compare with PSE-DR <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref>, which is a direct/indirect discrimination removing algorithm by modifying the causal graph and generating new fair data based on the modified causal graph. For counterfactual fairness, we instead compare with A1 and A3 <ref type="bibr" target="#b4">[Kusner et al., 2017]</ref>. A1</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building Classifiers with Independency Constraints</title>
		<author>
			<persName><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dheeru and Karra Taniskidou, 2017] Dua Dheeru and Efi Karra Taniskidou</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Path-Specific Counterfactual Fairness. The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2017</date>
		</imprint>
		<respStmt>
			<orgName>UCI Machine Learning Repository. University of California, Irvine, School of Information and Computer Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Censoring representations with an adversary</title>
		<author>
			<persName><forename type="first">Storkey</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Feldman et al., 2015] Michael Feldman, Sorelle A</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">John</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath</publisher>
			<date type="published" when="2009-02">2015. 2014. 2014. 2014. 2016. 2016. 2009. February 2009. 2012. October 2012. 2018</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
	<note>6th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning adversarially fair and transferable representations</title>
		<author>
			<persName><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Razieh Nabi and Ilya Shpitser. Fair Inference On Outcomes. The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting><address><addrLine>Stockholmsmässan, Stockholm, Sweden; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2017. 2017. 2018. 2018. 2018. 2018. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Nabi and Shpitser</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the 35th International Conference on Machine Learning. Pearl, 2009. 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName><surname>Pedreshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD 08</title>
		<meeting>eeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD 08<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counterfactual fairness: Unidentification, bound and algorithm</title>
		<author>
			<persName><surname>Salimi</surname></persName>
		</author>
		<idno>CoRR, abs/1902.08283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Zihang</forename><surname>Xie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulun</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eduard</forename><surname>Du</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Graham</forename><surname>Hovy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Neubig</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China; Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2019. 2019. August 10-16, 2019, 2019. 2017. 2017. 2017. 2018. December 10-13, 2018. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="570" to="575" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Big Data, Big Data</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A causal framework for discovering and removing direct and indirect discrimination</title>
		<author>
			<persName><forename type="first">Bareinboim</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2018. 2018. 2017. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno>AIES 2018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal modeling-based discrimination discovery and removal: Criteria, bounds, and algorithms</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2018. July 13-19, 2018. 2018. 2018. 2018. 2011. 2011</date>
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
