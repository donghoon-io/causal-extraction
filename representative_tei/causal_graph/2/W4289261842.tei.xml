<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Structure Learning: A Combinatorial Perspective</title>
				<funder ref="#_b4tKbyv">
					<orgName type="full">Eric and Wendy Schmidt Center at the Broad Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">Health</orgName>
				</funder>
				<funder ref="#_r24rhe7 #_aWhpHDY">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_7hVGH79 #_nJuvHtc">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-08-01">1 August 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
							<email>csquires@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
							<email>cuhler@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Broad Institute and Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teresa</forename><surname>Krick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hans</forename><surname>Munthe-Kaas</surname></persName>
						</author>
						<title level="a" type="main">Causal Structure Learning: A Combinatorial Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-01">1 August 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s10208-022-09581-9</idno>
					<note type="submission">Received: 28 May 2022 / Accepted: 8 June 2022 / Invited paper based on the FoCM 2021 Online Seminar lecture Causal Inference and Overparameterized Autoencoders in the Light of Drug Repurposing for COVID-19 presented by Caroline Uhler in January 2021.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal inference</term>
					<term>Causal structure discovery</term>
					<term>Markov equivalence Mathematics Subject Classification 62H22</term>
					<term>68R05</term>
					<term>05C75</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this review, we discuss approaches for learning causal structure from data, also called causal discovery. In particular, we focus on approaches for learning directed acyclic graphs and various generalizations which allow for some variables to be unobserved in the available data. We devote special attention to two fundamental combinatorial aspects of causal structure learning. First, we discuss the structure of the search space over causal graphs. Second, we discuss the structure of equivalence classes over causal graphs, i.e., sets of graphs which represent what can be learned from observational data alone, and how these equivalence classes can be refined by adding interventional data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many important scientific, sociological, and technological questions are fundamentally causal: "which genes regulate one another?," "how would raising minimum wage affect unemployment rate?," "which treatment most effectively prolongs the lifespan of breast cancer patients?." In each case, answering the question requires predicting how a system, e.g., a cell, economy, or human body, will react to external manipulation. Structural causal models can be used to formalize such questions, to create algorithms that determine whether such questions can be answered from available data sources, and to develop general-purpose methods for learning the answers to such questions. In the framework of structural causal models, a directed graph is used to reflect how the variables in these models depend causally on one another. Each node i of the directed graph is associated with a variable X i , and an edge i → j indicates that the variable X i is a direct cause of the variable X j . In some special, well-studied settings, background knowledge and human reasoning can be used to propose plausible directed graph models. However, in large systems such as gene regulatory networks, the directed graph is not known a priori, making it necessary to develop methods for learning the graph from data. Once this graph is learned, it can be used to predict the effects of interventions or distributional shifts, in contrast to traditional machine learning methods which can only make predictions on inputs that come from the same distribution as the training data.</p><p>The problem of learning such a causal graph from data, known as causal structure learning (or causal discovery), has been the focus of much recent work in computer science, statistics, and bioinformatics , covered in a number of recent reviews <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b112">113]</ref>. Compared to these reviews, we here emphasize the combinatorial aspects of causal structure learning, including characterizations of equivalence classes of graphs, computing the size and number of these equivalence classes, and how the characterization and properties are influenced by the presence of latent variables or interventional data. After discussing these topics, we will cover methods for causal structure learning which are based heavily on the combinatorial structure over the space of directed graphs. Focusing on this combinatorial structure has three significant advantages:</p><p>1. Causal structure learning can be dramatically simplified when fixing some combinatorial aspect of the problem, such as the ordering of the variables. 2. Understanding the combinatorial aspects of structure learning allows a number of different methods to be synthesized into a single framework and eases future methodological development. 3. Insights into the combinatorial aspects of structure learning are also useful for other tasks, such as experimental design.</p><p>The framework provided by the combinatorial viewpoint encompasses methods for learning causal models with unobserved variables, as well as methods for learning from a combination of observational and interventional data. The second point is especially important, since interventional data are often crucial for identifying the true causal model and subsequently using the causal model for predicting the effects of interventions or distributional shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structural Causal Models</head><p>A structural causal model defines causal relationships over a set of random variables {X i } p i=1 . These relationships are summarized by a directed acyclic graph (DAG) G over nodes i = 1, . . . , p, where the node i in G is associated with the variable X i . Given a DAG G, we let pa G (i) denote the parents of the node i, i.e., pa G (i) = { j | j → i in G}. Then, a (Markovian) structural causal model (SCM) <ref type="bibr" target="#b123">[124]</ref> with causal graph G consists of a set of endogenous variables {X i } p i=1 , a set of exogenous variables { i } p i=1 , a product distribution P over the exogenous variables, and a set of structural assignments { f i } p i=1 . In particular, the structural assignment f i asserts the relation X i = f i (X pa G (i) , i ). Via these structural assignments, the distribution P over the exogenous variables induces a distribution P X over the endogenous variables, called the entailed distribution <ref type="bibr" target="#b123">[124]</ref>. In particular, we have</p><formula xml:id="formula_0">P X (X i | X pa G (i) ) = E i [1 X i = f i (X pa G (i) , i ) | X pa G (i) ] and P X (X ) = p i=1 P X (X i | X pa G (i) ).</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>Example 1 (A simple structural causal model of genetic inheritance) As a running example, we will consider a simplified model of genetic inheritance of weight among a family of mice. Let X 2 and X 3 represent the weights, in grams, of an unrelated male and female mouse, respectively. Let X 4 represent the weight of their offspring, and X 5 represent the weight of the offspring's offspring. Finally, let X 1 be a binary variable representing whether the two parent mice are genetically modified for increased weight. Assume that these variables are related via the following set of assignments:</p><formula xml:id="formula_2">X 1 = 1 1 ∼ Ber(0.5) X 2 = 2 + 2X 1 2 ∼ N (25, 1) X 3 = 3 + 2X 1 2 ∼ N (20, 1) X 4 = 1 2 (X 2 + X 3 ) + 4 4 ∼ N (0, 1) X 5 = X 5 + 5 4 ∼ N (0, 2)</formula><p>where the set of are mutually independent. The parent sets are pa G (1) = ∅, pa G (2) = {1} pa G (3) = {1}, pa G (4) = {2, 3}, and pa G (5) = {4}. The causal graph is given in Fig. <ref type="figure" target="#fig_1">1</ref>, and P X (X ) = Ber(X 1 ; 0.5) × N (X 2 ; 25 + 2X 1 , 1) × N (X 3 ; 20 + 2X 1 , 1)</p><formula xml:id="formula_3">× N (X 4 ; 1 2 (X 2 + X 3 ), 1) × N (X 5 ; X 4 , 1)</formula><p>is the entailed distribution.</p><p>The above definition of structural causal models can be generalized in at least two ways. First, one may remove the assumption that the distribution over the exogenous variables is a product distribution, i.e., one may allow dependence between i and j for i = j. Such SCMs are called semi-Markovian and are taken as the basic definition of SCMs by some authors <ref type="bibr" target="#b121">[122]</ref>. Instead of allowing for dependencies between exogenous variables, we use Markovian SCMs as the basic definition and assume that any unmodeled dependence between endogenous variables is due to some other unobserved endogenous variables, which we will cover in 2.3. Second, one may remove the assumption that G is acyclic. The assumption of acyclicity is natural when considering endogenous variables which are defined at certain time points, since the intuitive notion of causality dictates that a cause precedes any of its effects. However, if the endogenous variables are not well defined in time, e.g., if they represent the average state of a system in equilibrium, then feedback loops may occur. We will briefly discuss recent progress on causal structure learning for cyclic causal models in 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Markov Properties and Markov Equivalence in DAGs</head><p>Given a DAG G, the set of distributions P X that factorize according to 1 are said to follow the Markov factorization property with respect to G. Depending on assumptions on the structural equations { f i } p i=1 and the exogenous variables { i } p i=1 , the Markov factorization property implies many other testable properties of the distribution P X . For instance, the entire set of conditional independence statements entailed by the Markov factorization property can be characterized simply in terms of a graphical criterion, known as d-separation, that can be read off from the DAG G. The definition of d-separation relies on the notion of a collider along a path from i to j. Given a path γ = γ 1 = i, γ 2 , . . . , γ M = j from i to j, the node γ m is a collider if γ m-1 → γ m ← γ m+1 , i.e., two arrowheads "collide" at γ m . Then, a path γ d-connects i and j given the set C ⊆ [p]\{i, j} if:</p><p>1. All non-colliders on the path do not belong to C. 2. All colliders on the path either belong to C or have a descendant which belongs to C.</p><p>Finally, i and j are d-connecting given C if there exists any d-connecting path given C; otherwise, they are d-separated. We denote that i and j are d-separated in G given  Given a distribution P X , we call X i and X j conditionally independent given</p><formula xml:id="formula_4">C via i ⊥ ⊥ G j | C. We denote the complete set of d-separation statements in a DAG G as I ⊥ ⊥ (G); i.e., I ⊥ ⊥ (G) = {(i, j, C) | i, j ∈ [p], C ⊆ [p]\{i, j}, i ⊥ ⊥ G j | C}.</formula><formula xml:id="formula_5">X C if P X (X i , X j | X C ) = P X (X i | X C )P X (X j | X C ).</formula><p>This is denoted by i ⊥ ⊥ P X j | C. We denote the set of all conditional independence statements in P X as</p><formula xml:id="formula_6">I ⊥ ⊥ (P X ) = {(i, j, C) | i, j ∈ [p], C ⊆ [p]\{i, j}, i ⊥ ⊥ P X j | C}.</formula><p>If all d-separation statements in the DAG G hold as conditional independence statements in P X , i.e., I ⊥ ⊥ (G) ⊆ I ⊥ ⊥ (P X ), then P X is said to satisfy the global Markov property with respect to G. Suppose that P X has a density with respect to some product measure. Then, without any additional assumptions on the structural equations or the distributions of exogenous variables, the Markov factorization property and the global Markov property are equivalent <ref type="bibr" target="#b108">[109]</ref>. Conversely, a given distribution P X may satisfy the global Markov property with respect to many different DAGs. These DAGs are called independence maps (I-MAPs) of the distribution P X . As an extreme example, the complete graph implies no conditional independencies in P X , so it is an I-MAP of all distributions. However, the complete graph does not capture any of the independence structure in P X . For a variety of purposes, including computational and statistical efficiency in inference and estimation, it is preferable to find a DAG G that captures as many of the independences of P X as possible. This intuition is captured in the definition of a minimal I-MAP for P X , which is an I-MAP G of P X , such that the deletion of any edge will result in a new DAG G which is no longer an I-MAP for P X . The following example shows that a distribution P X can have several minimal I-MAPs.</p><p>Example 3 (A distribution P X can have multiple minimal I-MAPs) Let P X be the distribution in 1. Then the DAG G * in Fig. <ref type="figure" target="#fig_1">1a</ref> is a minimal I-MAP for P X . To see this, we consider the deletion of each edge. Deleting 1 → 2 or 1 → 3 implies that X 1 ⊥ ⊥ X 2 , or X 1 ⊥ ⊥ X 3 , respectively, both of which are false. Similarly, deleting 2 → 4 or 3 → 4 implies that X 2 ⊥ ⊥ X 4 , or X 3 ⊥ ⊥ X 4 , respectively, but both are false. Finally, deleting 4 → 5 implies that X 4 ⊥ ⊥ X 5 , which is again false. </p><formula xml:id="formula_7">→ 1 implies X 2 ⊥ ⊥ X 1 | X 4 , X 3 and X 3 ⊥ ⊥ X 1 | X 2 , respectively, both of which are false. Deleting 2 → 3 implies that X 2 ⊥ ⊥ X 3 | X 4 , deleting 4 → 2 implies X 4 ⊥ ⊥ X 2 , deleting 4 → 3 implies X 4 ⊥ ⊥ X 3 | X 2 , and deleting 4 → 5 implies X 4 ⊥ ⊥ X 5 , showing that G 2 is indeed minimal.</formula><p>Suppose P X is entailed by an SCM with causal graph G * . Since P X may have multiple minimal I-MAPs, it is natural to ask, under some set of assumptions, whether G * can be distinguished from the other minimal I-MAPs, and if not, whether a small subset of the minimal I-MAPs can be distinguished as candidates for G * . As we will discuss in 4, without assumptions on the functional forms of the structural assignments f i , one cannot in general distinguish G * from all other graphs using only P X . In particular, two DAGs G and G with the same set of d-separation statements (i.e., Example 4 (Markov equivalence) The three DAGs in 2a-c are all Markov equivalent to one another, since for all three graphs, the only d-separation statement is that 1 and 3 are d-separated given 2. However, the DAG in 2d is not a member of the same MEC, since in G 4 , 1 and 3 are (unconditionally) d-separated, but are d-connected given 2.</p><formula xml:id="formula_8">I ⊥ ⊥ (G) = I ⊥ ⊥ (G ))</formula><p>However, under certain assumptions, it is possible to distinguish the set M(G * ) from all other minimal I-MAPs of P X . This is the case under the sparsest Markov representation (SMR) assumption <ref type="bibr" target="#b130">[131]</ref>, which states that, for any minimal I-MAP G of P X such that G / ∈ M(G * ), we have |G | &gt; |G * |, where |G| denotes the number of edges in G. Under this assumption, M(G * ) can be identified by enumerating over minimal I-MAPs of P X and picking the sparsest minimal I-MAP.</p><p>More generally, to identify M(G * ), structure learning algorithms require some form of faithfulness assumption. The strongest such assumption, referred to simply as the faithfulness assumption, is exactly the converse to the global Markov property: all conditional independence statements in P X must hold as d-separation statements in G * , i.e., I ⊥ ⊥ (P X ) = I ⊥ ⊥ (G * ). The faithfulness assumption is a "genericity" assumption in the sense that for parametric models, such as linear Gaussian models, the set of parameters which violate the faithfulness assumption is of Lebesgue measure zero <ref type="bibr" target="#b148">[149]</ref>. This is demonstrated by the following example.</p><p>Example 5 Consider the distribution P X entailed by the following SCM:</p><formula xml:id="formula_9">X 1 = 1 1 ∼ N (0, 1) X 2 = 2 + β 12 X 1 2 ∼ N (0, 1) X 3 = 3 + β 13 X 1 2 ∼ N (0, 1) X 4 = β 24 X 2 + β 34 X 3 + 4 4 ∼ N (0, 1)</formula><p>Denoting the corresponding causal graph by G, then the d-separation statements are given by In this example, the effect of X 1 on X 4 along the paths 1 → 2 → 4 and 1 → 3 → 4 perfectly "cancels out." While perfect cancelation may only occur for very specific parameters, structure learning algorithms do not have direct access to P X , and must test for conditional independence using samples from P X . Thus, near cancelations, e.g., if β 12 β 24 +β 13 β 34 = 0.0015, may be indistinguishable from cancelations at small sample sizes. To overcome noise and provide finite sample or high-dimensional guarantees for structure learning algorithms, it is necessary to make stronger assumption, such as strong faithfulness <ref type="bibr" target="#b180">[181]</ref>, which assumes that the (conditional) mutual information between d-connected variables is bounded away from zero. However, the set of parameters which violate the strong faithfulness assumption can have large Lebesgue measure <ref type="bibr" target="#b167">[168]</ref>. This has motivated the development of structure learning algorithms under assumptions that only require some subset of the missing d-separation statements in I ⊥ ⊥ (G) to hold "strongly" in P X , thus reducing the size of the set of violating parameters. Such assumptions, including a strong version of the SMR assumption, are reviewed and compared in <ref type="bibr" target="#b130">[131,</ref><ref type="bibr" target="#b182">183]</ref>.</p><formula xml:id="formula_10">I ⊥ ⊥ (G) = {(1,</formula><p>Since in general G * can only be identified up to its MEC, the natural search space for causal structure learning algorithms is over MECs, rather than DAGs. Consequently, characterizing the structure within and between MECs has been an important problem for developing structure learning algorithms. We will discuss useful characterizations of the MEC in 3. One way to overcome the limitations on learning from observational data is by using data from interventions, which we now formalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interventions and Interventional Markov Equivalence</head><p>To formalize the effect of an intervention I in an SCM, we consider a new interventional SCM where we modify some subset of the structural assignments and/or the distributions of exogenous noise variables, without introducing new nodes into any of the parent sets. If a node i has either its structural assignment f i or the distribution 123 of its exogenous noise i modified by intervention I , it is called a target of the intervention, and we write i ∈ I . The new SCM induces a different distribution P I X on X , called the interventional distribution, which takes the form</p><formula xml:id="formula_11">P I X (X ) = i / ∈I P X (X i | X pa G (i) ) i∈I P I X (X i | X pa G (i) ).<label>(2)</label></formula><p>In general, an intervention consists of any modification of the structural assignment or exogenous noise. To distinguish this most general form of intervention from more stringent definitions of intervention, we will follow <ref type="bibr" target="#b123">[124]</ref> and call these soft interventions (also referred to as mechanism changes in <ref type="bibr" target="#b162">[163]</ref>). Particular subclasses of interventions have generated special interest. Most significantly, a hard intervention, also called a perfect, surgical <ref type="bibr" target="#b25">[26]</ref>, or structural <ref type="bibr" target="#b43">[44]</ref> intervention, is one which completely removes the dependence of a target X i on its parents. However, perfect interventions allow for the target to depend on i , so that the target's value may still be random, i.e., the interventional distribution is</p><formula xml:id="formula_12">P I X (X ) = i / ∈I P X (X i | X pa G (i) ) i∈I P I X (X i ). (<label>3</label></formula><formula xml:id="formula_13">)</formula><p>More extremely, if the structural assignment of X i is changed to a constant a i , then there is no randomness left in X i . Such a perfect intervention is called a do-intervention <ref type="bibr" target="#b112">[113]</ref>. In this case, the interventional distribution is</p><formula xml:id="formula_14">P I X (X ) = i / ∈I P X (X i | X pa G (i) ) i∈I 1 X i =a i . (<label>4</label></formula><formula xml:id="formula_15">)</formula><p>Example 6 (The interventional SCM for mouse genetic modification) Suppose we implement an intervention on the model in 1, where we edit the genome of the offspring mouse to reduce its weight. In particular, the effect of this intervention is to change the distribution of 4 to N (-10, 0.1). The interventional distribution is</p><formula xml:id="formula_16">P X (X ) = Ber(X 1 ; .5) × N (X 2 ; 25 + 2X 1 , 1) × N (X 3 ; 20 + 2X 1 , 1) × N (X 4 ; 1 2 (X 2 + X 3 ) -10, 1) × N (X 5 ; X 4 , 1).</formula><p>This intervention is not a perfect intervention, since X 4 still depends on its parent X 2 and X 3 . If instead the genetic modification perfectly ensures that the offspring weights 15 grams, i.e. X 4 = 15 always, then the intervention would be a perfect interventionin particular, a do-intervention. In this case, the interventional distribution becomes</p><formula xml:id="formula_17">P X (X ) = Ber(X 1 ; .5) × N (X 2 ; 25 + 2X 1 , 1) × N (X 3 ; 20 + 2X 1 , 1) × 1 X 4 =15 × N (X 5 ; X 4 , 1),</formula><p>where X 4 does not depend on its parents anymore. The causal DAG also implies relationships between the observational and interventional distributions. The simplest approach to deriving these relationships is to extend the DAG to include variables which represent different interventions, as proposed in <ref type="bibr" target="#b176">[177]</ref> and used by <ref type="bibr" target="#b152">[153]</ref>. This approach can be seen as an important special case of the Joint Causal Inference (JCI) framework <ref type="bibr" target="#b114">[115]</ref>. For a single intervention I with targets T , this can be achieved by adding a node ζ with children T . ζ represents a binary variable, where ζ = 1 denotes that a sample comes from the intervention I , and ζ = 0 denotes otherwise.</p><p>Example 7 (Binary encoding of an intervention) Consider the intervention I 1 in 6, where the intervention is applied with probability 0.5. Then the joint distribution over X and ζ is</p><formula xml:id="formula_18">P X ,ζ (X , ζ ) = Ber(ζ ; .5) × Ber(X 1 ; .5) × N (X 2 ; 25 + 2X 1 , 1) × N (X 3 ; 20 + 2X 1 , 1) × N (X 4 ; 1 2 (X 2 + X 3 ) -10ζ, 1) × N (X 5 ; X 4 , 1). The causal DAG for ζ, X 1 , X 2 , X 3 , X 4 , X 5 is shown in 3a. The node 5 is d-separated from ζ given 4. Therefore, P(X 5 | X 4 , ζ = 1) = P(X 5 | X 4 , ζ = 0), i.e., P X (X 5 | X 4 ) = P I 1 X (X 5 | X 4 ).</formula><p>To generalize to multiple interventions, we add a node for each intervention. In particular, consider a set of interventions </p><formula xml:id="formula_19">I = {I 1 , . . . , I M }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>Suppose each intervention has a 40% chance of being applied. In the remaining 20% of the time, no intervention takes place. Then the joint distribution over X , ζ 1 , ζ 2 , and</p><formula xml:id="formula_20">ζ * is P X ,ζ (X , ζ ) =Cat(ζ * ; (0, 1, 2), (0.2, 0.4, 0.4)) × 1 -1 ζ 1 =1,ζ * =1 × 1 -1 ζ 2 =1,ζ * =2 ×Ber(X 1 ; .5) × N (X 2 ; 25 + 2X 1 , 1) × N (X 3 ; 20 + 2X 1 , 1) ×N (X 4 ; 1 2 (X 2 + X 3 ) -10ζ 1 -5ζ 2 , 1) × N (X 5 ; X 4 , 1). The causal DAG for ζ 1 , ζ 2 , ζ * , X 1 , X 2 , X 3 , X 4 , X 5 is shown in 3b.</formula><p>Following <ref type="bibr" target="#b176">[177]</ref>, we define a conditional invariance statement to be a conditional independence statement where the conditioning set includes intervention variables, e.g., P</p><formula xml:id="formula_21">X ,ξ (X i | X C , ξ * = m) = P X ,ξ (X i | X C , ξ * = 0)</formula><p>. This statements posits that a conditional distribution in the mth interventional setting is the same as it is in the observational setting, i.e., the conditional distribution is invariant under the intervention. A set of observational and interventional distributions satisfies the I-Markov property with respect to a DAG G and a set of interventions I if it satisfies the global Markov property with respect to G, and satisfies all conditional invariance statements entailed by the I-DAG. Similarly to the observational case, given a set I of interventions, if two DAGs G and G entail the same set of conditional independence and conditional invariance statements, we call them I-Markov equivalent, denoted G ≈ M I G . The resulting I-Markov equivalence class (I-MEC) is thus a (not necessarily strict) subset of the MEC, as demonstrated by the following example.</p><p>Example 9 (Interventional Markov equivalence) Given the intervention set I = {I 1 } for I 1 with target 1, the graphs G 2 and G 3 in 2 are I-Markov equivalent, since they both entail the invariance statements P I 1 (X 2 ) = P(X 2 ) and P I 1 (X 3 ) = P(X 3 ). However, G 1 does not entail these invariance statements, so it is not I-Markov equivalent to G 2 and G 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graphical Representations for Latent Confounding</head><p>Thus far, we have discussed how a structural causal model defines a data generating process for a particular system and interventions on that system. In the simplest case, called the causally sufficient setting, one directly observes the generated data. However, it is often the case that observations are subject to additional processing, in which case we call the setting causally insufficient. Two forms of causal insufficiency are commonly considered. First, under latent confounding, some of the endogenous variables are simply unobserved, and we call these variables latent confounders. Thus, instead of observing samples from the distribution P X , one observes samples from a marginal distribution P X for X ⊂ X . For instance, suppose that in 1, the experimentalist does not record the variable X 1 indicating whether the mice were genetically modified. Then, an observer looking at their data would see samples from the distribution P X 2 ,X 3 ,X 4 ,X 5 . Second, under selection bias, the probability that a sample is observed may depend on the values of some of the variables in the sample. Thus, if we introduce a binary variable S to indicate whether a sample is observed, and we have P(S = 1 | X ) describe the selection process, then one observes samples from the conditional distribution P(X | S = 1). For instance, suppose that in 1, the experimentalist only records those experiments for which the mouse in the final generation weighs more than 20 grams. Then, someone looking at their data would see samples from the distribution P X (• | X 5 ≥ 20).</p><p>In this section, we will focus on the first type of causal insufficiency, latent confounding. We postpone discussion of selection bias to 5. Without causal sufficiency, one must somehow account for latent confounders to perform accurate causal structure learning. When the latent confounders have special structure, it may be possible to explicitly recover the relationship of the latent confounders and the observed variables. One such case is when each latent confounder is a parent of a large portion of the observed variables, which is termed pervasive confounding. In such settings, the observed data may be "deconfounded" by removing its top principal components <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b140">141]</ref>, even when the causal relations are nonlinear <ref type="bibr" target="#b2">[3]</ref>. A large range of assumptions on the structure between the unobserved and observed variables may be suitable for different applications. A thorough summary of methods using such assumptions is outside of the scope of the current review. Instead, we focus on a different approach for accounting for latent confounders, which acknowledges their presence but does not attempt to explicitly recover their relationships with the observed variables.</p><p>Structural assumptions on latent confounders can leave a wide range of signatures on the distribution of the observed variables. These signatures include not only conditional independence constraints, which can be expressed in the form</p><formula xml:id="formula_22">P X (X i , X j | X C ) = P X (X i | X C )P X (X j | X C</formula><p>), but also more complex constraints. This includes both equality constraints on the distribution P X , commonly called Verma constraints, as well as inequality constraints. The full set of constraints is referred to as a marginal DAG model <ref type="bibr" target="#b45">[46]</ref>, and can be graphically modeled using a hypergraph. Indeed, <ref type="bibr" target="#b45">[46]</ref> show that ordinary mixed graphs are incapable of representing marginal DAG models. Nevertheless, ordinary mixed graphs are capable of encoding a rich subset of the constraints implied by a marginal DAG model. For example, an acyclic directed mixed graph (ADMG) encodes a subset of the equality constraints of the marginal DAG model via the associated nested Markov model <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b144">145]</ref>; in fact, the nested Markov model is known to encode all equality constraints in the case of discrete variables <ref type="bibr" target="#b46">[47]</ref>. It is outside the scope of this review to provide a full overview of the different types of graphs used to capture the constraints of marginal DAG models, instead see <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b102">[103]</ref> for more thorough overviews.</p><p>In our review, we focus on (directed) ancestral graphs, which encode only conditional independencies, are closed under marginalization, and have at most one edge between each pair of vertices. Directed ancestral graphs are mixed graphs, consisting of both directed and bidirected edges. A bidirected edge between two nodes indicates the possibility that they are both children of the same unobserved variable(s). Similarly to directed graphs in the causally sufficient setting, the mixed graphs in the causally insufficient case are required to obey a form of acyclicity condition. In particular, a mixed graph with directed and bidirected edges is called "ancestral" if there are no directed cycles, and if any two nodes that are connected by a bidirected edge (called spouses) are not ancestors of one another <ref type="bibr" target="#b131">[132]</ref>. Similarly to DAG models, there is a notion of separation in directed ancestral graphs called m-separation. The same definition works as for d-separation: two nodes are mconnected by a path γ given a set of nodes C if (1) every non-collider on the path is not in C, and (2) every collider on the path is either in C or has a descendant in C. Unfortunately, this notion of separation has the property that two non-adjacent nodes may have no m-separating set. Fortunately, adding a bidirected edge between two such nodes does not affect the set of m-separation statements which hold in the directed ancestral graph ( <ref type="bibr" target="#b131">[132]</ref>, Theorem 5.1). The operation of adding bidirected edges between all such nodes is called taking the maximal completion of a directed ancestral graph, and a directed ancestral graph is called maximal if it is its own maximal completion. It is natural in structure learning to restrict the search space to directed maximal ancestral graphs (DMAGs), so that each adjacency between nodes corresponds exactly to the lack of an m-separating set.</p><p>Example 10 (Maximal completion) 4 shows a graph (left) which is not maximal, since 1 and 4 are m-connected given any of the sets {∅, {2}, {3}, {2, 3}}, but they are not adjacent. The graph on the right is its maximal completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Identifiability</head><p>As alluded to in the previous section, two Markov equivalent DAGs cannot be distinguished from observational data alone. In particular, given a DAG G, consider the collection of distributions M(G) which factorize according to G, i.e., can be written in the form 1. This collection depends on the allowed set of conditional distributions P X (X i | X pa(i) ). If the set of conditional distributions is unrestricted, then we have that M(G) = M(G ) if and only if I ⊥ ⊥ (G) = I ⊥ ⊥ (G ), i.e., Markov equivalent DAGs give rise to the exact same set of distributions. If the conditional distributions are restricted to specific classes, such as Gaussians or discrete measures, then this equivalence remains <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b158">159]</ref>.</p><p>Broadly speaking, there are two approaches to distinguishing between Markov equivalent DAGs. The first approach, which we call the functional form approach, considers restricting the class of conditional distributions in such a way that identifiability is possible from only observational data. The second approach, which we call the equivalence class approach, does not restrict the class of conditional distributions, but instead uses interventional data to refine the level of identifiability from the MEC to the I-MEC. Given enough interventions, the equivalence class approach is sufficient for completely identifying a DAG or an ADMG <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Functional Form Approaches to Identifiability</head><p>Suppose the true causal graph is X 1 → X 2 . The core idea in this class of approaches is to find asymmetries between models learned in the "causal" (X 1 → X 2 ) and "anticausal" (X 2 → X 1 ) directions. The asymmetries in this bivariate case are often easy to subsequently extend to the multivariate case.</p><p>As a canonical example, assume that noise is additive, i.e.,</p><formula xml:id="formula_23">X 2 = f 2 (X 1 ) + 2 , with 2 ⊥ ⊥ X 1 .</formula><p>By making assumptions about the functional form of f 2 and the distribution of 2 , it is often possible to show that the induced distribution P X cannot be induced by a model of the form</p><formula xml:id="formula_24">X 1 = f 1 (X 2 ) + 1 , 1 ⊥ ⊥ X 2 ,</formula><p>under the same assumptions on f 1 and 1 . For example, <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143,</ref><ref type="bibr" target="#b143">144]</ref> assume that each function f i is linear, and each i is non-Gaussian. Indeed, <ref type="bibr" target="#b75">[76]</ref> shows that in linear models, symmetry is only possible in the Gaussian case, and gives more general results for the case where f i is nonlinear, which form the basis for structure learning methods such as the Causal Additive Model (CAM) algorithm <ref type="bibr" target="#b22">[23]</ref>. Even in the linear Gaussian case, it is possible to achieve identifiability by imposing additional assumptions, such as equal error variances for each i <ref type="bibr" target="#b122">[123]</ref>. It is also possible to move beyond the additive noise case, e.g., by allowing for further nonlinearities after the addition of noise <ref type="bibr" target="#b184">[185]</ref>.</p><p>Thus far, we have discussed identification strategies designed for continuous random variables. Similar results are achievable in the discrete case, e.g., by assuming that the exogenous noise terms have low entropy <ref type="bibr" target="#b92">[93]</ref> or by assuming the existence of a (hidden) low cardinality representation of the cause variable that mediates its effects <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Equivalence Class Approach to Identifiability</head><p>When no assumptions are made on the functional form, and only observational data is available, the true graph G * can only be identified up to the MEC, i.e., the set of DAGs G such that G ≈ M G * . Thus, for the purposes of algorithm design, it becomes interesting to characterize when two DAGs are Markov equivalent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Characterizations of Markov Equivalence Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characterizations of Markov equivalence in DAGs</head><formula xml:id="formula_25">(G) = {(i, j) | i → j or j → i in G}.</formula><p>The v-structures (also called immoralities) are defined as vstruct</p><formula xml:id="formula_26">(G) = {(i, j, k) | i → j ← k in G, (i, k) / ∈ skel(G)}.</formula><p>Verma and Pearl <ref type="bibr" target="#b169">[170]</ref> show that two DAGs G and G are Markov equivalent if and only if they have the same skeleton and v-structures, i.e., G ≈ M G if and only if skel(G) = skel(G ) and vstruct(G) = vstruct(G ). Given this graphical notion, it is natural to represent an MEC via an essential graph, which is a mixed graph with the same adjacencies as all DAGs in the equivalence class, and with the edge i → j directed only if i → j in all DAGs in the equivalence class. Meanwhile, the transformational characterization is based on a single notion: a covered edge is an edge i → j in G such that pa G (i) = pa G ( j)\{i}. G and G are related by a covered edge flip if G has all of the same edges as G, except that the covered edge i → j in G is oriented as j → i in G . From the graphical characterization, one can deduce that if G and G are related by a series of covered edge flips, then G ≈ M G . The transformational characterization states that the converse is also true: If G ≈ M G , then G can be transformed into G by a series of covered edge flips <ref type="bibr" target="#b28">[29]</ref>. This transformation is illustrated in 5. Finally, the geometric characterization encodes each graph as an integer-valued vector in the space Z 2 [ p] . First, we introduce a set of basis vectors δ A for all subsets A ⊂ [p]. Then, the standard imset for a DAG G is given by u</p><formula xml:id="formula_27">G = δ [ p] -δ ∅ + p i=1 δ pa G (i) -δ {i}∪pa G (i) . Alternatively, [160] introduces the characteristic imset c G , with c G (A) = 1 if and only if there exists some i ∈ [p] such that A\{i} ⊆ pa G (i). Two DAGs G and G are Markov equivalent if and only if u G = u G , or equivalently, c G = c G .</formula><p>Characterizations of interventional Markov equivalence in DAGs As discussed in 2.2, the effect of an intervention can be formalized by introducing new binary variables to represent each intervention <ref type="bibr" target="#b114">[115]</ref>. Therefore, the same characterizations of Markov equivalence that apply in the observational case just discussed also apply in the interventional case. However, it is still instructive to directly characterize the interventional Markov equivalence class. Consider a set of interventions I such that ∅ ∈ I (i.e., observational data is available). Extending a result for perfect interventions <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b176">177]</ref> shows that two DAGs G and G are I-Markov equivalent if and only if they (1) have the same skeleton and v-structures, as in the case of a DAG and (2) for all Characterizations of Markov equivalence in DMAGs As in the case of DAGs, equivalence between DMAG models can be characterized in multiple ways, and we will cover the graphical and transformational characterizations. For both characterizations, we must define the notion of a discriminating path for a vertex k. A path γ = i, . . . , k, j is a discriminating path for k if (i) there is at least one node on the path between i and k, (ii) every node between i and k is a collider on the path, and (iii) every node between i and k is a parent of j. We denote the set of discriminating paths for node k in G as discr k (G). A fundamental result <ref type="bibr" target="#b150">[151]</ref> states that two DMAGs G and G are Markov equivalent if and only if (i) they have the same skeleton and v-structures, and (ii) for all k, for all γ ∈ discr k (G) ∩ discr k (G ), k is a collider on γ in G if and only if k is a collider on γ in G . Checking this graphical condition for Markov equivalence can be computationally expensive, motivating recent work <ref type="bibr" target="#b76">[77]</ref> which provides a new graphical characterization of Markov equivalence in DMAGs that can be checked more efficiently. We next describe the transformational characterization of Markov equivalence in DMAGs. As in the case of DAGs, the transformational characterization requires us to define a local structural modification. In particular, the modification of the edge i → j in G to the edge i ↔ j in G , or vice versa, is called a legitimate mark change <ref type="bibr" target="#b181">[182]</ref> </p><formula xml:id="formula_28">if (i) pa G (i) ⊆ pa G ( j), (ii) sp G (i)\{ j} ⊆ sp G ( j) ∪ pa G ( j)</formula><p>, and (iii) there is no γ ∈ discr i (G) for which j is the endpoint adjacent to i. The authors in <ref type="bibr" target="#b181">[182]</ref> show that G ≈ M G if and only if G and G are connected by a series of legitimate mark changes. This transformation is illustrated in 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Combinatorial Aspects of Markov Equivalence</head><p>Since DAGs in general can only be identified up to (I-)Markov equivalence, it has been of significant interest to study the size of a given MEC, the number of MECs The first problem-computing the number of DAGs within a given MEC, or computationally equivalently, sampling uniformly from the MEC-is important for a number of experimental design algorithms <ref type="bibr" target="#b55">[56]</ref>, which use Monte Carlo approximations to compute expectations over the MEC and pick interventions with good average-case behavior. A recent advance <ref type="bibr" target="#b174">[175]</ref> provides a polynomial-time algorithm for this task based on a representation of the equivalence class via clique trees, improving over previous algorithms with exponential worst-case runtime <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b161">162]</ref>.</p><p>To address the second problem, <ref type="bibr" target="#b60">[61]</ref> develops a program for enumerating all MECs on graphs with a given number of nodes, and obtained results for graphs of up to 10 nodes, shown in 1. Further theoretical works <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b126">127]</ref> study the problem of enumerating all MECs for a fixed skeleton using the idea of generating functions from combinatorics. The computational results in <ref type="bibr" target="#b60">[61]</ref> suggest that, asymptotically, the average MEC contains approximately 4 DAGs, and that roughly one quarter of all MECs are comprised of only a single DAG, in which case no interventional data is needed to identify the causal DAG. However, proving these conjectured limits, as well as efficiently enumerating the number of MECs on a given number of nodes, remain open combinatorial and computational problems. Less work has been done to characterize the average number of interventions required to identify a DAG. For a given DAG, <ref type="bibr" target="#b151">[152]</ref> characterizes the minimum-size set of single-node interventions needed to identify the underlying causal DAG, using a representation based on clique trees. However, this work does not address the average of this quantity over all DAGs on a given number of nodes. Meanwhile, <ref type="bibr" target="#b87">[88]</ref> conducts a computational study of the average number of greedily selected interventions to identify a graph, where the average is with respect to a directed Erdös-Rényi graph model. In this model, the results suggest that the number of interventions necessary is typically less than 4, but further work is necessary to characterize the average with respect to the uniform distribution over graphs and to address the case where interventions are picked optimally. cuss a number of algorithms which are consistent, i.e., in the limit of infinite data, they provably learn all identifiable causal structures. We will also highlight some heuristic algorithms, which do not have consistency guarantees but often perform well in practice. We begin with a broad overview of the different paradigms for causal structure learning, before diving into methods which explicitly leverage the combinatorial structures already discussed. At the highest level, methods for estimating causal models from data fall into two broad categories: constraint-based methods and score-based methods. Constraint-based methods are natural when viewing causal structure learning as a constraint satisfaction problem, where conditional independences or other constraints that can be inferred from data are used to iteratively prune the space of possible graphs. In contrast, score-based methods arise from viewing causal structure learning as a combinatorial optimization problem. These methods assign a score to each graph (or equivalence class) which quantifies how well it fits the data, then search the space of graphs (or equivalence classes) to find a model which optimizes the score. To highlight the general principles of these two paradigms, we will first concentrate on the causally sufficient case with only observational data. Then, in 4.3, we discuss algorithms that can make use of interventional data, and in 4.4, we briefly discuss algorithms for learning in the presence of latent confounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraint-based approaches</head><p>The most prominent constraint-based approach to causal structure learning is the PC algorithm <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b148">149]</ref>. The PC algorithm begins with a complete undirected graph and iteratively deletes edges by testing conditional independences involving conditioning sets of increasing cardinality. Then, the second phase of the PC algorithm orients v-structures by reusing the conditional independences found in the first phase. Additional orientations can be inferred via the Meek orientation rules <ref type="bibr" target="#b110">[111]</ref>.</p><p>The method for testing conditional independence (CI) depends on modeling assumptions as well as practical considerations such as computational complexity. For example, in a multivariate Gaussian distribution, two variables X i and X j are conditionally independent given the variables X C if and only if the partial correlation ρ i j|C is zero. Since the distribution of sample partial correlation coefficients is well known (see, e.g., <ref type="bibr" target="#b85">[86]</ref>), hypothesis testing for CI in the Gaussian setting is straightforward and computationally efficient. On the other hand, in nonparametric settings, hypothesis tests for conditional independence can often be performed based on more complicated test statistics <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b157">158,</ref><ref type="bibr" target="#b185">186]</ref>. Unfortunately, impossibility results <ref type="bibr" target="#b141">[142]</ref> state that any uniformly valid conditional independence test (i.e., one whose false positive rate tends to at most the significance level α, over all possible distributions P where X ⊥ ⊥ P Y | Z ) will have no statistical power (i.e., the probability of a true positive will also be at most α). Thus, testing conditional independence requires additional assumptions on the set of possible distributions, such as complexity restrictions on the function space of</p><formula xml:id="formula_29">E P [X | Z ].</formula><p>Under such complexity assumptions, conditional independence tests allow constraintbased approaches to directly be applied to nonparametric settings, even permitting high-dimensional consistency bounds in these settings <ref type="bibr" target="#b68">[69]</ref>. Furthermore, because conditional independences also characterize DMAG models, constraint-based approaches can be easily extended to settings with latent variables <ref type="bibr" target="#b34">[35]</ref>. Pushing further, one may encode conditional independences as logical constraints, allowing them to be used 123 in answer set programming (ASP) solvers. These solvers can search over more general model classes and easily incorporate background knowledge <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b179">180]</ref>. However, ASP-based causal structure learning methods are widely viewed as being difficult to scale for many practical applications.</p><p>Score-based approaches Score-based methods for causal structure learning originated in parametric settings, such as in discrete or linear Gaussian models. In parametric settings, the score S(G) of a graph G is often based on the marginal likelihood P(X | G) of the data X given the graph G, with respect to some prior P(θ ) over the parameters θ . In some cases, e.g., when choosing a conjugate prior for the likelihood function, P(X | G) can be computed in closed form <ref type="bibr" target="#b54">[55]</ref>. Alternatively, it is common to use a consistent approximation of the marginal likelihood, in the form of the Bayesian information criterion (BIC) score <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Such likelihood-based scores can be extended to nonparametric settings, e.g., by using Gaussian process priors <ref type="bibr" target="#b49">[50]</ref> or non-paranormal distributions <ref type="bibr" target="#b116">[117]</ref>. The BIC score and related scores are also a natural starting point from which to develop more sophisticated scores with better statistical and computational properties, see, e.g., <ref type="bibr" target="#b20">[21]</ref>.</p><p>Finding the highest scoring DAG model is generally NP-hard <ref type="bibr" target="#b29">[30]</ref>, imposing a trade-off between computational efficiency and algorithmic consistency guarantees. Score-based methods can generally be subdivided into three categories based on how they address this trade-off. On the one end of the spectrum, exact score-based approaches find some G that exactly optimizes the score S. Exact approaches address computational issues using a variety of combinatorial optimization techniques and heuristics, e.g., dynamic programming <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b120">121]</ref>, A*-style state-space search <ref type="bibr" target="#b178">[179]</ref>, or methods from integer linear programming <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b81">82]</ref>. For example, the GOBNILP algorithm <ref type="bibr" target="#b38">[39]</ref> uses the geometric characterization of Markov equivalence classes to reduce structure learning to an integer linear programming problem. This reduction allows the use of techniques such as cutting planes and pricing to handle the exponential number of decision variables and constraints.</p><p>Greedy score-based approaches trade off to achieve better computational efficiency over exact approaches by relaxing the requirement that G optimizes S. Most prominently, greedy equivalence search (GES) <ref type="bibr" target="#b30">[31]</ref> and its variants <ref type="bibr" target="#b31">[32]</ref> perform a search over equivalence classes of graphs that greedily optimizes S. While greedy algorithms are not exact, they are still consistent, placing them in a middle ground on the computational-statistical trade-off. Notably, <ref type="bibr" target="#b105">[106]</ref> shows that GES and a number of other greedy approaches can also be viewed geometrically. In particular, these methods can be seen as edge walks between vertices of the characteristic imset polytope, i.e., the convex hull of all characteristic imsets on p variables. Finally, at the other extreme of this trade-off, gradient-based methods <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b177">178,</ref><ref type="bibr" target="#b186">187,</ref><ref type="bibr" target="#b189">190]</ref> relax the discrete search space over DAGs to a continuous search space, allowing gradient descent and other techniques from continuous optimization to be applied to causal structure learning. However, the search space of these problems is highly non-convex, so that the optimization procedure may become stuck in a local minima. Thus, consistency guarantees for these methods will depend on theoretical advances in global minimization of such non-convex optimization problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning DAGs Using Permutation-Based Algorithms</head><p>Beyond the constraint-based and score-based paradigms for causal structure learning already discussed, there are a variety of hybrid methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b137">138,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b165">166]</ref>, which generally use constraints to reduce the search space, and scores to optimize over this reduced search space. In this section, we discuss the greedy sparsest permutation (GSP) algorithm, a hybrid method that constrains the search space to the set of (estimated) minimal I-MAPs of P X . By focusing on this method, we highlight the combinatorial nature of the problem of causal structure learning. As discussed in 2, a distribution P X may permit several different minimal I-MAPs. Since the minimal I-MAPs of P X are the (locally) sparsest DAGs which can correctly model P X , they form a natural space over which to search for the true DAG G * . Furthermore, the space of minimal I-MAPs of P X can be described as the image of a P X -dependent map, with the P X -independent domain of S p of permutations of [ p]. We denote by i &lt; π j that i is earlier in the permutation π than j, and we call a graph G consistent with a permutation π if and only if i &lt; π j implies that j → i in G. The following result establishes the existence of a unique map from permutations to minimal I-MAPs.</p><p>Theorem 1 (from <ref type="bibr" target="#b168">[169]</ref>) Given a permutation π and a distribution P X , there exists a unique graph G P X (π ) that is consistent with π and is a minimal I-MAP for P X . This graph has edges</p><formula xml:id="formula_30">{i → j | X i ⊥ ⊥ X j | X pre π ( j)\{i} } where pre π ( j) = {k | k &lt; π j}.</formula><p>Given a graph G, let |G| be the number of edges in the graph. The sparsest I-MAP theorem <ref type="bibr" target="#b130">[131]</ref> establishes that, under a mild condition, the sparsest minimal I-MAPs of P X -i.e, those such that |G P X (π )| is minimized-are Markov equivalent to the underlying causal graph G * . In particular, the required condition for this result is strictly weaker than the restricted faithfulness assumption <ref type="bibr" target="#b128">[129]</ref>, which only requires that I ⊥ ⊥ (P X ) and I ⊥ ⊥ (G * ) agree on conditional independences/d-separations involving nodes connected by paths of lengths one or two. The sparsest I-MAP theorem directly suggests the sparsest permutation (SP) algorithm: enumerate over all permutations π ∈ S p , estimating the minimal I-MAP G P X (π ) for each of these permutations using conditional independence testing, and return the sparsest graphs.</p><p>However, the SP algorithm is clearly computationally prohibitive, since the size of S p is super-exponential in p. To address this issue, <ref type="bibr" target="#b147">[148]</ref> proposed the greedy sparsest permutation (GSP) algorithm. GSP searches greedily over the space of permutations, and hence, minimal I-MAPs. In particular, at each step i of the algorithm, GSP maintains a permutation π (i) and its corresponding minimal I-MAP G P X (π (i) ). At this step, GSP searches over the Markov equivalence class of G P X (π (i) ) for some DAG G which is not a minimal I-MAP of P X . This search can be executed by repeatedly flipping covered edges to generate new permutations. Upon finding G which is not a minimal I-MAP of P X , there must be some strict sub-DAG G of G which is a minimal I-MAP of P X . GSP then takes the topological ordering of this sub-DAG as the new permutation π (i+1) , with G as its corresponding minimal I-MAP G P X (π (i+1) ). One greedy step of GSP is demonstrated in 7.</p><p>As in the case for other greedy approaches, GSP has an interpretation as an edge walk over a convex polytope. In particular, starting from the permutahedron, i.e., the convex hull of all permutations on p nodes, we may define the DAG associahedron by contracting all edges π (i) -π ( j) of the permutahedron for which G P X (π (i) ) = G P X (π ( j) ). As shown in <ref type="bibr" target="#b147">[148]</ref>, this contraction results in a convex polytope, GSP is equivalent to an edge walk along this polytope, and, under conditions that are strictly weaker than the faithfulness assumption, this edge walk terminates in the Markov equivalence class of the causal graph G * underlying P X . The central technical ingredient in this proof is the existence of Chickering sequences. In particular, <ref type="bibr" target="#b30">[31]</ref> proves the Meek conjecture for DAGs <ref type="bibr" target="#b111">[112]</ref>: if G M is an I-MAP of G 0 = G * , then there exists a sequence (G 0 , G 1 , . . . , G M ) composed only of edge additions and covered edge reversals. This sequence is called a Chickering sequence <ref type="bibr" target="#b147">[148]</ref> and its existence guarantees the consistency of GSP.</p><p>In addition to the consistency of GSP and the algorithms discussed previously, which provides guarantees as the sample size goes to infinity, it is important to understand the performance of different algorithms for finite sample size. Simulation results suggest that score-based and hybrid approaches perform better for fixed sample sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b116">117]</ref>. However, a theoretical characterization of the trade-offs between these algorithms on finite samples is not well understood and is an important area for future research, as also briefly described in 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bayesian Methods for Causal Structure Learning</head><p>Thus far, we have only discussed causal structure learning methods which return a point estimate-i.e., a single DAG that (approximately or locally) maximizes a score, and/or satisfies inferred conditional independences. However, when the amount of data is small, there may be substantial uncertainty about the underlying graph (or equivalence class). A common framework for quantifying this uncertainty is Bayesian inference. Given some dataset D, instead of returning a point estimate, Bayesian methods return (an approximation to) the posterior P(G | D) over graphs. This posterior allows one to compute marginal probabilities of any feature of interest, such as the posterior probability of some edge i → j.</p><p>Bayesian methods for causal structure learning can be divided into three types of approaches: exact approaches (e.g., <ref type="bibr" target="#b41">[42]</ref>) and two types of approximate approaches: variational and sampling-based approaches. Similarly to the gradientbased approaches discussed before, variational approaches do not necessarily return a consistent estimate of the posterior; rather, they project the posterior onto a variational family {Q(• | θ)} θ∈ , which is more computationally convenient. However, traditional variational families, such as multivariate Gaussians, are continuous and thus do not apply to the discrete setting of DAGs. Thus, until recently, variational methods for Bayesian causal structure learning have not been widely studied. For a recent work in this space, see <ref type="bibr" target="#b106">[107]</ref>, which uses relaxations of DAGs to a continuous search space and neural networks to parameterize a flexible variational family.</p><p>On the other hand, sampling-based approaches to Bayesian causal structure learning have been much more widely studied. Markov chain Monte Carlo (MCMC) methods have been especially popular, beginning with the structure MCMC algorithm <ref type="bibr" target="#b109">[110]</ref>, which runs a Metropolis-Hastings algorithm over the space of DAG models, using edge additions and deletions to move in this space. However, this approach suffers from slow mixing times due to regions of high-probability DAG models being separated by large regions of low-probability DAG models, i.e., if structure MCMC finds some highprobability DAG G 0 , some other high-probability DAG G M may only be reachable from G 0 by a sequence of DAGs G 1 , . . . G M-1 which have very low probability. Thus, the probability that structure MCMC traverses this path becomes incredibly low, so that G M will not be sampled without running the algorithm for many steps.</p><p>This difficulty has motivated a search for "smoother" sampling spaces, either by adding moves to structure MCMC <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b66">67]</ref>, or by changing the search space, as was done in order MCMC <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>, partial order MCMC <ref type="bibr" target="#b117">[118]</ref>, and partition MCMC <ref type="bibr" target="#b98">[99]</ref>. These methods run a Markov chain over some "coarser" space (permutations, partial orders, or ordered partitions), then sample DAGs conditionally based on their consistency with the coarser structure. The minimal I-MAP MCMC algorithm <ref type="bibr" target="#b4">[5]</ref> also runs a Markov chain over the coarser space of permutations. However, instead of conditionally sampling a DAG based on each permutation, it estimates the minimal I-MAP associated to each sampled permutation.</p><p>Since the space of permutations is much smaller than the space of DAGs or MECs, the minimal I-MAP MCMC algorithm can mix more quickly than previous algorithms. But this comes at a price: Minimal I-MAP MCMC does not sample over the entire posterior distribution of DAG models, but only a restricted subset. Luckily, this price is small: intuitively, conditional on an order, the minimal I-MAP asymptotically has the highest posterior probability, so a point mass on the minimal I-MAP is a good approximation of the true conditional distribution. Indeed, <ref type="bibr" target="#b4">[5]</ref> shows that the posterior approximation error for any bounded function of the graph decreases exponentially with the number of samples. By highlighting this algorithm, we once again see the computational benefits that are possible when considering the combinatorial nature of the causal structure learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Causal Structure Learning Using Interventional Data</head><p>As discussed in 3, interventional data can significantly improve the identifiability of causal models. Several approaches have been proposed for learning from a combination of observational and experimental data, going back at least to the Bayesian approaches of <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b41">[42]</ref>. As in the case of learning from purely observational data, these approaches can be divided into constraint-based approaches, such as the COm-bINE <ref type="bibr" target="#b164">[165]</ref> algorithm, and score-based approaches. Score-based approaches include greedy algorithms, such as Greedy Interventional Equivalence Search (GIES) <ref type="bibr" target="#b69">[70]</ref>, and gradient-based algorithms, such as meta-learning approaches <ref type="bibr" target="#b88">[89]</ref> and DCDI <ref type="bibr" target="#b21">[22]</ref>. Note that, unlike in the case of GES for observational data, GIES is known to not be consistent for interventional data <ref type="bibr" target="#b170">[171]</ref>.</p><p>The Joint Causal Inference framework <ref type="bibr" target="#b114">[115]</ref> discussed in 2.2 suggests a natural way to extend causal structure learning algorithms for observational data to settings with interventional data. In particular, an algorithm for the observational setting can be used to learn the I-DAG by appending indicator variables to the dataset for each intervention I ∈ I, as long as the algorithm can incorporate appropriate forms of background knowledge. This background knowledge includes exogeneity-i.e., intervention variables are not caused by the original "system" variables, randomized context-i.e., lack of confounding between the intervention and system variables, and generic context-i.e., that the intervention variables are deterministically related to one another. As an example, <ref type="bibr" target="#b152">[153]</ref> shows that the GSP algorithm can be adapted to include these assumptions, along with any assumptions about known targets of each intervention, while maintaining consistency of the algorithm. They call the resulting algorithm the Unknown Target Intervention GSP (UT-IGSP) algorithm to emphasize its ability to handle interventions with unknown targets, extending previous works where targets were assumed to be known <ref type="bibr" target="#b170">[171,</ref><ref type="bibr" target="#b176">177]</ref>. Finally, it is also natural to develop Bayesian variants of causal structure learning algorithms for interventional data, e.g., <ref type="bibr" target="#b26">[27]</ref> shows how to compute posteriors over DAGs in the setting when the data is multivariate Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Causal Structure Learning in the Presence of Latent Confounding</head><p>The approaches to causal structure learning in the causally insufficient setting follow the same broad categorization as approaches in the causally sufficient setting. In particular, the Fast Causal Inference (FCI) algorithm <ref type="bibr" target="#b149">[150]</ref> is a constraint-based algorithm for learning DMAGs, similar in spirit to the PC algorithm. The FCI algorithm has inspired several variants, including Really Fast Causal Inference (RFCI) <ref type="bibr" target="#b34">[35]</ref>, and FCI+ <ref type="bibr" target="#b33">[34]</ref>. Score-based methods include both greedy search strategies, such as Greedy FCI (GFCI) <ref type="bibr" target="#b118">[119]</ref>, MAG Max-Min Hill Climbing (M 3 HC) <ref type="bibr" target="#b166">[167]</ref>, and Conservative rule and Causal effect Hill Climbing (CCHM) <ref type="bibr" target="#b32">[33]</ref>, exact score-based approaches, such as AGIP <ref type="bibr" target="#b27">[28]</ref>, and gradient-based approaches <ref type="bibr" target="#b15">[16]</ref>.</p><p>As in the case of learning DAGs, we will discuss a hybrid method for learning DMAGs, which combines elements of both score-based and constraint-based approaches, and elucidates the combinatorial aspects of learning DMAGs. This method, called the Greedy Sparsest Poset (GSPo) method, restricts the search space of DMAGs to minimal I-MAPs of the distribution P X . This space can be realized as the image of a map G P X from partially ordered sets (posets) to graphs. A partially ordered set π defines a relation π that captures the notion of an ordering via three requirements: reflexivity (i π i for all i), antisymmetry (i π j and j π i implies i = j), and transitivity (i π j and j k implies i π k). Because of the definition of the ancestrality condition, the set of complete DMAGs can be put in bijection to the set of posets, so that posets form a natural domain for the map G P X .</p><p>The authors in <ref type="bibr" target="#b12">[13]</ref> show that G P X (π ) can be constructed using a procedure similar to the procedure defined for DAGs in 1, although the construction requires two iterations of conditional independence testing between pairs of variables instead of one. They also provided a version of the sparsest I-MAP theorem for DMAGs, i.e., under a restricted faithfulness assumption, the sparsest minimal I-MAPs of P X are all Markov equivalent to the underlying DMAG G * . Motivated by the GSP algorithm for learning DAGs, <ref type="bibr" target="#b12">[13]</ref> introduce the greedy sparsest poset (GSPo) algorithm for learning DMAGs, which uses legitimate mark changes to search over posets and iteratively find sparser I-MAPs. Over 100,000 synthetic examples suggest that the GSPo algorithm is consistent, but proof of its consistency is an important open problem, and closely tied to the open problem of generalizing Meek's conjecture <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b111">112]</ref> to DMAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Open Problems</head><p>In this review article, we sought to cover both classical and recent approaches to causal structure learning, emphasizing the combinatorial nature of this problem. We end by discussing several related areas of work that were not covered in depth and remain under active development.</p><p>Learning with both interventions and latent confounding While we separately discussed learning with interventional data and learning under confounding, it is natural to combine these two settings. Recent work <ref type="bibr" target="#b82">[83]</ref> considers this combination for DMAGs, introducing the new notion of -Markov equivalence to capture pairs of graphs and interventions which induce the same set of conditional independencies and conditional invariances. This work allows for both soft and unknown-target interventions. Furthermore, <ref type="bibr" target="#b82">[83]</ref> provides a graphical characterization of -Markov equivalence, and introduces a constraint-based algorithm, called -FCI, for learning the -Markov equivalence class from data. As a next step it is natural to consider score-based algorithms, both exact and greedy, for learning DMAGs, ADMGs, and other subclasses of marginal DAG models, using a combination of observational and interventional data.</p><p>Learning with assumptions on the latent structure As indicated in 2.3, in some cases with unobserved confounding, it is desirable to recover the unobserved variables and their relationship to the observed variables. Naturally, recovery of these details requires assumptions on their structure. A common assumption, called the exogeneity or measurement assumption, is that all unobserved variables are upstream of the observed variables, i.e., none of the unobserved variables are caused by any of the observed variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>With the exogeneity assumption as a starting point, additional assumptions may be made to (approximately) recover the latent variables, and possibly, the structure between them. For example, several works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b140">141]</ref> consider recovering the unobserved variables in settings with pervasive confounding, i.e., when each unobserved variable has a direct effect on a large number of observed variables. As an important special case of this setting, some works have considered recovering a mixture of DAG models <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b156">157]</ref>, where there is a single unobserved variable that is a parent of all variables in the graph. Alternatively, many works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b183">184]</ref> consider recovering unobserved variables under the measurement assumption and a form of purity or anchor assumption, where each unobserved variable must have some number of observed variables which are only their children. Few works consider recovering unobserved variables without the assumption of exogeneity, with <ref type="bibr" target="#b153">[154]</ref> being a recent exception.</p><p>Learning in the presence of selection bias As suggested in 2.3, considerable effort has gone into characterizing the distributional constraints imposed by marginalization of DAG models. However, in many applications, the observed distribution is the result of both marginalization and conditioning of an underlying distribution. In particular, such observed distributions are induced by selection bias, where the probability that a sample is observed is dependent on the value of some of the variables in the sample. General maximal ancestral graphs (see 2.3), which allow for undirected edges in addition to directed and bidirected edges, are conditional independence models which are closed under marginalization and conditioning. As in the case of marginalization, several graphical representations, including MC graphs <ref type="bibr" target="#b95">[96]</ref> and summary graphs <ref type="bibr" target="#b173">[174]</ref>, have been introduced to capture constraints induced by such conditional models. However, to the best of our knowledge, there is no graphical representation which exactly captures all equality and inequality constraints induced by conditioning a DAG model, in contrast to the case for marginal models <ref type="bibr" target="#b45">[46]</ref>. Thus, important next steps include (1) developing a graphical representation which fully captures both marginalization and conditioning, (2) developing notions of Markov equivalence in this setting, including with interventional data, and (3) developing structure learning algorithms in this general setting.</p><p>Learning cyclic causal models As indicated in 2, a widespread assumption in causal modeling and causal structure learning is that the structural causal model (SCM) induces an acyclic graph. However, this may not be the case if the SCM models a system that involves feedback loops. While the underlying dynamics of the system are necessarily acyclic over time, feedback loops can arise when modeling the equilibrium states of such systems <ref type="bibr" target="#b18">[19]</ref>. For example, in gene regulatory networks, we may have that gene A regulates gene B, and gene B also regulates gene A, so that intervening on either gene will affect the value of the other gene. Recent work <ref type="bibr" target="#b19">[20]</ref> has investigated the semantics of cyclic causal models, showing that Markov properties and other desirable properties hold in the case of certain solvability conditions. Despite the technical difficulties associated with cyclic models, several approaches have been proposed for learning their structure from data. These approaches include many algorithms designed for the linear case, including LLC <ref type="bibr" target="#b77">[78]</ref>, score-based approaches <ref type="bibr" target="#b57">[58]</ref>, and BackShift <ref type="bibr" target="#b133">[134]</ref>. Algorithms for the general case include SAT-based approaches <ref type="bibr" target="#b80">[81]</ref>, exact score-based approaches <ref type="bibr" target="#b129">[130]</ref>, and constraint-based approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b154">155]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical and computational complexity of causal structure learning</head><p>In conjunction with methodological developments for settings with cycles, latent confounding, selection bias, and interventional data, it is important to understand the fundamental statistical and computational limits of causal structure learning, and any trade-offs between these. The analysis of existing causal structure learning algorithms gives upper bounds on what is statistically and computationally achievable. Recent work derives upper bounds for a wide range of settings, including the linear equal-variance setting <ref type="bibr" target="#b59">[60]</ref>, the linear non-Gaussian setting <ref type="bibr" target="#b172">[173]</ref>, other parametric settings <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b127">128]</ref>, as well as nonparametric settings <ref type="bibr" target="#b52">[53]</ref>. On the other hand, it is important to understand the fundamental lower bounds on the sample complexity needed by any causal structure learning algorithm. Such lower bounds have been established for the exponential family setting <ref type="bibr" target="#b58">[59]</ref> and the linear equal-variance setting <ref type="bibr" target="#b53">[54]</ref>, but the lower bounds for a wide range of settings and assumptions remain uncharacterized.</p><p>Furthermore, since consistency of causal structure learning algorithms always requires some form of "faithfulness" or genericity assumption (see 2), there are likely trade-offs between the strength of faithfulness assumption imposed and computational and statistical complexity. Indeed, an interesting open question is to characterize the weakest assumption needed for causal structure learning, with the sparsest Markov representation assumption <ref type="bibr" target="#b130">[131]</ref> being one candidate. Finally, the works discussed above are all in causally sufficient settings with only observational data. Incorporating interventional data into these analyses would open the possibility for a reduction in overall sample complexity, and may introduce a landscape of trade-offs between interventional and observational sample complexities. Indeed, interventional data has been considered in recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> on the statistical and computational complexity of causal inference tasks, where the causal graph is assumed to be known and the task is to estimate interventional distributions. An interesting future direction is to also explore the effect of interventional data on the complexity of causal structure learning.</p><p>Experimental design for causal structure learning In this review article, we have focused on causal structure learning in a passive setting, where we are given a dataset, or possibly several datasets from different interventions or contexts. However, in many scientific settings, such as biology, where interventions such as genetic or chemical perturbations can readily be performed, an important component of causal discovery is the choice of what data to gather <ref type="bibr" target="#b62">[63]</ref>. This leads us to consider experimental design approaches for causal structure learning, where an experimenter may pick interventions (and their values) in an effort to identify the underlying causal structure. Several approaches have been proposed for a variety of settings. In the non-adaptive setting, the experimenter picks all interventions at once. In <ref type="bibr" target="#b42">[43]</ref> it is shown that, in the absence of any preexisting observational data, p -1 interventions are sufficient and in the worst-case necessary for identifying the underlying causal structure over p variables. Other work in the non-adaptive setting considers the presence of background knowledge (e.g., from observational data) <ref type="bibr" target="#b78">[79]</ref>, differences in costs between interventions <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b104">105]</ref>, and a fixed-budget setting <ref type="bibr" target="#b55">[56]</ref>.</p><p>Alternatively, the adaptive setting allows the experimenter to observe the outcome of each intervention before picking the next intervention. He and Geng <ref type="bibr" target="#b72">[73]</ref> and Hauser and Bühlmann <ref type="bibr" target="#b70">[71]</ref> propose greedy approaches for the adaptive setting, picking new interventions based on some measure of either expected or worst-case information 123 gain. While these approaches are designed for the noiseless setting, in which an infinite amount of data is gathered from each intervention, more recent works <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b163">164]</ref> explore greedy approaches in the noisy setting. <ref type="bibr" target="#b65">[66]</ref> shows that strategies which maximize expected information gain can be exponentially suboptimal in the number of interventions that they use, and propose the Central Node algorithm for settings where the essential graph is a tree. They show that this algorithm is a 2-approximation to the optimal adaptive strategy. Follow-up work <ref type="bibr" target="#b151">[152]</ref> adapts this algorithm to a more general class of essential graphs, provides a characterization of the number of singlenode interventions needed by an oracle to identify a causal graph, and shows that their algorithm uses within a logarithmic factor of this number of interventions.</p><p>In between the non-adaptive and adaptive settings, <ref type="bibr" target="#b3">[4]</ref> considers the active batched setting, in which the experimenter observes the outcome of a batch of interventions before picking the next batch of interventions. Recent work <ref type="bibr" target="#b160">[161]</ref> establishes novel submodularity properties for greedy objectives in this settings, allowing for efficient optimization over the choice of interventions in each batch. Taken together, these recent advances suggest several future directions, including (1) characterizing the number of multi-target interventions needed by an oracle in the adaptive case <ref type="bibr" target="#b124">[125]</ref>, (2) approximation guarantees for experimental design, compared to either oracles or optimal strategies, and (3) experimental design in settings with latent confounding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b93">94]</ref>, selection bias, and cycles.</p><p>Targeted causal structure learning Thus far, we have focused on the problem of causal structure learning as an end in itself; i.e., in both the passive and active settings discussed, the desired output was a causal graph (or equivalence class). However, ultimately, a major motivation for causal structure learning is to use the causal model in downstream tasks. A task of considerable importance is policy evaluation, i.e., predicting the effect of an action. The overall goal of task can be phrased as estimating a specific functional of an interventional distribution defined by a structural causal model M. Then two principal subtasks are (1) determining whether this functional is identifiable by transforming it into a functional of the available distributions and (2) estimating the resulting functional from samples. When the only available distribution is the observational distribution defined by M, possibly with some variables unobserved, the first subtask is covered by the ID algorithm <ref type="bibr" target="#b145">[146]</ref> and its variants <ref type="bibr" target="#b146">[147]</ref>.</p><p>More generally, data might be available from some set of interventional distributions defined by M, or from observational and interventional distributions associated to some related structural causal model M 1 , . . . , M K . The relation between these structural causal models is encoded using a selection diagram, and the task of using the selection diagram to identify the functional is covered by a rich literature on transportability <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b103">104]</ref>. Once the target functional is transformed into a functional of the available distributions, it becomes essential to estimate the functional in a sample-efficient way. This has been extensively studied in the literature on semiparametric efficiency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b134">135]</ref>, double machine learning <ref type="bibr" target="#b84">[85]</ref>, and targeted machine learning <ref type="bibr" target="#b138">[139]</ref>, also covered in a recent review <ref type="bibr" target="#b89">[90]</ref>. Thus far, causal structure learning and policy evaluation have been studied as separate tasks: The output of causal structure learning is a causal graph, while the input to policy evaluation is a causal graph or selection diagram. Therefore, the current approach to using policy evaluation tasks when the graph is unknown would be to first perform causal structure learning, then to use the methods discussed for policy evaluation. It is likely that this approach is not optimally sampleefficient-the two steps should be "aware" of each other, i.e., causal structure learning should be performed in a way that is targeted toward the downstream task.</p><p>The problem of targeted causal structure learning remains mostly unexplored, with a few notable exceptions. In the adaptive experimental design setting, <ref type="bibr" target="#b3">[4]</ref> considers targeted learning of any property of the underlying graph, and <ref type="bibr" target="#b187">[188]</ref> considers targeted learning of a "matching" intervention, which affects the system in some desired way. In the batched data setting, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b188">189]</ref> consider targeted learning of the difference between two DAG models, instead of the DAG models themselves. All of these works demonstrate computational and statistical benefits to targeted learning over untargeted structure learning, indicating that this is an important and promising direction.</p><p>Causal structure in reinforcement learning Policy evaluation is also an important task in reinforcement learning, where the policy is a sequence of actions that can depend on the state of the environment. The overlap between reinforcement learning and causality has been recently explored in the simple setting of multi-armed bandits, where an agent's actions do not affect the state of the environment. By assuming that actions correspond to interventions in a known causal graph, the effects of different actions become related, allowing for better regret bounds <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b115">116]</ref>. If the causal graph is not assumed to be known, there is an additional exploration-exploitation trade-off that needs to be taken into account, which has been considered in recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b107">108]</ref>. Since certain parts of the causal graph might not be relevant to predicting the effect of an action on some reward, the reinforcement learning setting is another case in which targeted structure learning may be more efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 2 (</head><label>2</label><figDesc>d-connection and d-separation) In G * from Fig.1a, there are two paths between 2 and 3, the path γ 1 = 2 ← 1 → 3, and the path γ 2 = 2 → 4 ← 3. For C = ∅, γ 1 is a d-connecting path between 2 and 3, since 1 is a non-collider and does not belong to C, while γ 2 is not a d-connecting path, since 4 is a collider but neither 4 nor 5 is in C. Thus, 2 and 3 are d-connected given C = ∅. For C = {1}, neither γ 1 nor γ 2 are d-connecting paths, so 2 and 3 are d-separated given C = {1}. Finally, for any C containing 4 or 5, γ 2 is a d-connecting path between 2 and 3. Thus, 2 and 3 are d-connected given C = {4}, C = {5}, C = {1, 4}, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 a</head><label>1</label><figDesc>Fig. 1 a Causal graph G * for the structural causal model in 1. b A minimal I-MAP G 2 for G * , described in 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 a</head><label>2</label><figDesc>Fig. 2 a-c Three Markov equivalent graphs from 4. d A fourth graph that is not Markov equivalent to the other three</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>are called Markov equivalent, and we denote this by G ≈ M G . The set of all DAGs that are Markov equivalent to G * is called the Markov equivalence class (MEC) of G * , denoted M(G * ), and P X can in general only identify G * up to M(G * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 a</head><label>3</label><figDesc>Fig. 3 a I-DAG from 7. b I-DAG from 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For the intervention I m with targets T m , we introduce a node ζ m with children T m . Again, ζ m = 1 denotes that the sample comes from the intervention I m , and ζ m = 0 otherwise. However, each sample can only be generated from a single intervention, i.e., ζ m = 1 for at most one m. To reflect this constraint, we include a final node ζ * , which takes values in 0, 1, . . . , M, to indicate which intervention the sample comes from, i.e., ζ m = 1 if and only if ζ * = m. Thus, if ζ * = 0, the sample comes from the observational distribution. The resulting DAG is called the interventional DAG (I-DAG) [177]. Example 8 (Binary encoding of a set of interventions) Let I 1 be the intervention in 6, and let I 2 be an intervention which changes the distribution of 4 to N (-5, 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 a</head><label>4</label><figDesc>Fig. 4 a An ancestral graph that is not maximal; b its maximal completion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Transformational characterization of equivalence in a DAG. Starting with the DAG on the left, we proceed to the right by performing covered edge reversals on the red edges (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Transformational characterization of equivalence in a DMAG. Starting with the DMAG on the left, we proceed to the right by performing legitimate mark changes on the red edges (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 A</head><label>7</label><figDesc>Fig. 7 A greedy step over minimal I-MAPs performed by GSP. a True graph G * , to which the distribution P(X ) is faithful. b Minimal I-MAP associated with the permutation π (0) = [2, 4, 1, 3], with covered edges shown in red. c Minimal I-MAP associated with the permutation π (1) = [2, 4, 3, 1], obtained after flipping the covered edge 1 → 3 (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4, {2, 3}), (2, 3, {1})}. However, if β 12 β 24 + β 13 β 34 = 0, then Cov(X 1 , X 2 ) = 0, so by Gaussianity, we have that 1 ⊥ ⊥ P X 4, i.e.,<ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4</ref>, ∅) ∈ I ⊥ ⊥ (P X ) but (1, 4, ∅) / ∈ I ⊥ ⊥ (G * ).The set of parameters (β 12 , β 13 , β 24 , β 34 ) satisfying this equality is of Lebesgue measure zero.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Number of MECs (first row), the ratio of the number of MECs to the number of DAGs (second row), and the ratio of the number of MECs of size 1 compared to the total number of MECs (third row), up to 10 nodes</figDesc><table><row><cell>p</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1 0</cell></row><row><cell># MEC</cell><cell>11</cell><cell>185</cell><cell>8.78e4</cell><cell>1.06e6</cell><cell>3.13e8</cell><cell>2.12e11</cell><cell>3.26e14</cell><cell>1.12e18</cell></row><row><cell>#MEC #DAG</cell><cell>0.44</cell><cell>0.34</cell><cell>0.30</cell><cell>0.28</cell><cell>0.27</cell><cell>0.27</cell><cell>0.27</cell><cell>0.27</cell></row><row><cell>#MEC-1 #MEC</cell><cell>0.36</cell><cell>0.32</cell><cell>0.30</cell><cell>0.29</cell><cell>0.28</cell><cell>0.28</cell><cell>0.28</cell><cell>0.28</cell></row><row><cell cols="9">over a given number of variables, and the minimum number of interventions required</cell></row><row><cell cols="6">to identify a DAG (i.e., obtain a I-MEC of size 1).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Methods for Causal Structure LearningThus far, we have discussed what is in principle identifiable about the underlying causal DAG with observational and interventional data. Now, we present algorithms which carry these principles of identifiability into practice. In particular, we will dis-</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements Chandler Squires was partially supported by an <rs type="funder">NSF</rs> <rs type="grantName">Graduate Fellowship</rs>. Caroline</p></div>
			</div>
			<div type="funding">
<div><p>Uhler was partially supported by <rs type="funder">NSF</rs> (<rs type="grantNumber">DMS-1651995</rs>), <rs type="funder">ONR</rs> (<rs type="grantNumber">N00014-17-1-2147</rs> and <rs type="grantNumber">N00014-22-1-2116</rs>), the <rs type="institution">MIT-IBM Watson AI Lab, MIT J-Clinic</rs> for <rs type="person">Machine Learning</rs> and <rs type="funder">Health</rs>, the <rs type="funder">Eric and Wendy Schmidt Center at the Broad Institute</rs>, and a <rs type="grantName">Simons Investigator Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7hVGH79">
					<orgName type="grant-name">Graduate Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_nJuvHtc">
					<idno type="grant-number">DMS-1651995</idno>
				</org>
				<org type="funding" xml:id="_r24rhe7">
					<idno type="grant-number">N00014-17-1-2147</idno>
				</org>
				<org type="funding" xml:id="_aWhpHDY">
					<idno type="grant-number">N00014-22-1-2116</idno>
				</org>
				<org type="funding" xml:id="_b4tKbyv">
					<orgName type="grant-name">Simons Investigator Award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and testing causal models with interventions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient intervention design for causal discovery with latents</title>
		<author>
			<persName><forename type="first">R</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musco</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The DeCAMFounder: Non-linear causal discovery in the presence of hidden variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07921</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ABCD-strategy: Budgeted experimental design for targeted causal structure discovery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3400" to="3409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Minimal I-MAP MCMC for scalable structure discovery in causal DAG models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lazyiter: a fast algorithm for counting Markov equivalent DAGs and designing experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmaditeshnizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="125" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling up the greedy equivalence search algorithm by constraining the search space of equivalence classes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Alonso-Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gámez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Puerta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of approximate reasoning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="451" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with mixed data-types</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 2019 ACM SIGKDD Workshop on Causal Discovery</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transportability of causal effects: Completeness results</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transportability from multiple environments with limited experiments: Completeness results</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integer linear programming for the Bayesian network structure learning problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="258" to="271" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DCI: Learning causal differences between gene regulatory networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belyaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics btab</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ordering-based causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4098" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tetali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09717</idno>
		<title level="m">On sampling graphical Markov models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semiparametric inference for causal effects in graphical models with hidden variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12659</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2314" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vinodchandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11712</idno>
		<title level="m">Efficient inference of interventional distributions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05100</idno>
		<title level="m">Adaptively exploiting d-separators with causal bandits</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08784</idno>
		<title level="m">Causal modeling of dynamical systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Foundations of structural causal models with cycles and latent variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2885" to="2915" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SparsityBoost: a new scoring function for learning Bayesian network structure</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery from interventional data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21865" to="21877" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Causal discovery from discrete data using hidden compact representation. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">2666</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An interventionist approach to causation in psychology. Causal learning: Psychology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network structure learning under uncertain interventions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Castelletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peluso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>just-accepted</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integer programming for causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A transformational characterization of equivalent Bayesian network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks is NP-complete</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistically efficient greedy equivalence search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="241" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning with causal effects in the presence of latent variables</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chobtham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Constantinou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Probabilistic Graphical Models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning sparse causal models is not NP-hard</title>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal discovery from a mixture of experimental and observational data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">General transportability of soft interventions: Completeness results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10902" to="10912" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian network learning with cutting planes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</title>
		<meeting>the 27th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GOBNILP: Learning Bayesian network structure with integer programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v138/cussens20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Probabilistic Graphical Models, Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Jaeger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting>the 10th International Conference on Probabilistic Graphical Models, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="605" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: approaches and issues</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aitken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The knowledge engineering review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="157" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structure learning in graphical modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="365" to="393" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exact Bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence and statistics</title>
		<imprint>
			<biblScope unit="page" from="107" to="114" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="178" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interventions and causal inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning causal Bayesian network structures from experimental data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">482</biblScope>
			<biblScope unit="page" from="778" to="789" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graphs for margins of Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="625" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Margins of discrete Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="2623" to="2656" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Constraint-based causal discovery for non-linear structural causal models with cycles and latent confounders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03024</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gaussian process networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Sixteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="211" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Frot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01151</idno>
		<title level="m">Robust causal structure learning with some hidden variables</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An efficient algorithm for counting Markov equivalent DAGs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ganian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Talvitie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page">103648</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient Bayesian network structure learning via local Markov boundary search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10548</idno>
		<title level="m">Optimal estimation of Gaussian DAG models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Parameter priors for directed acyclic graphical models and the characterization of several probability distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1412" to="1440" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Budgeted experiment design for causal structure learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1724" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Counting and sampling from Markov equivalent DAGs using clique trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3664" to="3671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Characterizing distribution equivalence and structure learning for cyclic and acyclic directed graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3494" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Information-theoretic limits of Bayesian network structure learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="767" to="775" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning identifiable Gaussian Bayesian networks in polynomial time and sample complexity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Enumerating Markov Equivalence Classes of Acyclic Digraph Models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gillispie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lemieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence</title>
		<meeting>the 17th Conference in Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving Markov chain Monte Carlo model search for data mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Giudici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="158" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Causality in digital medicine</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musolesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11602</idno>
		<title level="m">Identifying mixtures of Bayesian network distributions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sample efficient active learning of causal trees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boix Adsera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving the structure MCMC sampler for Bayesian networks by introducing a new edge reversal move</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grzegorczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Husmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03299</idno>
		<title level="m">Anchored discrete factor analysis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PC algorithm for nonparanormal graphical models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Two optimal strategies for active learning of causal models from interventional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="926" to="939" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Counting and exploring sizes of Markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2589" to="2609" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Active learning of causal networks with intervention experiments and optimal designs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2523" to="2547" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Causal structure learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="371" to="391" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Invariant causal prediction for nonlinear models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="689" to="696" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Faster algorithms for Markov equivalence</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning linear cyclic causal models with latent variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3387" to="3439" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Experiment selection for causal discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3041" to="3071" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Constraint-based causal discovery: Conflict resolution with answer set programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>UAI</publisher>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Discovering cyclic causal models with latent variables: A general SAT-based procedure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jarvisalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6836</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning Bayesian network structure using LP relaxations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Causal discovery from soft interventions with unknown targets: Characterization and learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9551" to="9561" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Discovering hidden variables in noisy-or networks using quartet tests</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Estimating identifiable causal effects on Markov equivalence class through double machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5168" to="5179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PC-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Causal inference using nonnormality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on science of modeling, the 30th anniversary of the information criterion</title>
		<meeting>the international symposium on science of modeling, the 30th anniversary of the information criterion</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Size of interventional Markov equivalence classes in random DAG models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01075</idno>
		<title level="m">Learning neural causal models from unknown interventions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Kennedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06469</idno>
		<title level="m">Semiparametric doubly robust targeted double machine learning: a review</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning latent causal graphs via mixture oracles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Cost-optimal learning of causal graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1875" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Entropic causal inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Experimental design for learning causal graphs with latent variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Exact Bayesian structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="549" to="573" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Marginalizing and conditioning in graphical models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="page" from="817" to="840" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>De Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07916</idno>
		<title level="m">Causal discovery for causal bandits utilizing separating sets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Partition MCMC for inference on acyclic digraphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">517</biblScope>
			<biblScope unit="page" from="282" to="299" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Causal clustering for 1-factor measurement models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1655" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Gradient-Based Neural DAG Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Causal bandits: Learning good interventions via causal inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Unifying Markov properties for graphical models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2251" to="2278" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Generalized transportability: Synthesis of experiments from heterogeneous domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Experimental design for cost-aware learning of causal graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Greedy causal discovery is geometric</title>
		<author>
			<persName><forename type="first">S</forename><surname>Linusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restadh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03771</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">DiBS: Differentiable Bayesian Structure Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Causal bandits with unknown graph structure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meisami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Handbook of graphical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Bayesian graphical models for discrete data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Allard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review/Revue Internationale de Statistique</title>
		<imprint>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Graphical models: Selecting causal and statistical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Methods for causal inference from gene perturbation experiments and validation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7361" to="7368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery with partial ancestral graphs in the presence of cycles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">124</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<title level="m">Joint causal inference from multiple contexts</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Budgeted and non-budgeted causal bandits</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sinha</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">High-dimensional consistency in score-based and hybrid structure learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3151" to="3183" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Structure discovery in Bayesian networks by sampling partial orders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Niinimäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parviainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2002" to="2048" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A hybrid causal search algorithm for latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogarrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on probabilistic graphical models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Identifiability of generalized hypergeometric distribution (ghd) directed acyclic graphical models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="158" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Bayesian structure discovery in Bayesian networks with less space</title>
		<author>
			<persName><forename type="first">P</forename><surname>Parviainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="589" to="596" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Causality. Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Identifiability of Gaussian structural equation models with equal error variances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Almost Optimal Universal Lower Bound for Learning Causal DAGs with Atomic Interventions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Porwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sinha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05070</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Counting Markov equivalence classes by number of immoralities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>AUAI Press Corvallis</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Counting Markov equivalence classes for DAG models on trees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="170" to="185" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Structure learning in polynomial time: Greedy algorithms, Bregman information, and exponential families</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Adjacency-faithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Second Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Discovering causal graphs with cycles and latent confounders: an exact branch-and-bound approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="29" to="49" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Learning directed acyclic graph models based on sparsest permutations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Ancestral graph Markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Nested Markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06686</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Backshift: Learning causal cyclic graphs from unknown shift interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Efficient adjustment sets for population average treatment effect estimation in non-parametric causal graphical models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rotnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smucler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00306</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Anchored causal inference in the presence of measurement error</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belyaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Causal structure discovery from distributions arising from mixtures of DAGs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8336" to="8345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Learning graphical model structure using l1regularization paths</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1278" to="1283" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Targeted maximum likelihood estimation for causal inference in observational studies</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">The IMAP hybrid method for learning Gaussian Bayes nets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Right singular vector projection graphs: fast high dimensional covariance matrix estimation under latent confounding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Thanei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">The hardness of conditional independence testing and the generalised covariance measure</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1514" to="1538" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Introduction to nested Markov models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-Markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 21st national conference on Artificial intelligence</title>
		<meeting>the 21st national conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wood-Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Tchetgen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06818</idno>
		<title level="m">The proximal ID algorithm</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Consistency Guarantees for Greedy Permutation-Based Causal Inference Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asaa104</idno>
		<ptr target="https://doi.org/10.1093/biomet/asaa104" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Asaa</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A polynomial time algorithm for determining DAG equivalence in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the 6th International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Active structure learning of causal DAGs via directed clique trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21500" to="21511" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Permutation-based causal structure learning with unknown intervention targets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Causal structure discovery between clusters of nodes induced by latent factors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A constraint-based algorithm for causal discovery with cycles, latent variables and selection bias</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="56" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Improved causal discovery from longitudinal data using a mixture of DAGs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 2019 ACM SIGKDD Workshop on Causal Discovery</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="100" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05418</idno>
		<title level="m">The global Markov property for a mixture of DAGs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Probabilistic conditional independence structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studeny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Characteristic imset: a simple algebraic representative of a Bayesian network structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studenỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hemmecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European workshop on probabilistic graphical models</title>
		<meeting>the 5th European workshop on probabilistic graphical models</meeting>
		<imprint>
			<publisher>HIIT Publications</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Near-optimal multi-perturbation experimental design for causal structure learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Counting and sampling Markov equivalent directed acyclic graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7984" to="7991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Causal discovery from changes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02016</idno>
		<title level="m">Interventions, where and how? experimental design for causal models at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery from multiple interventions over overlapping variable sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2147" to="2205" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">On scoring maximal ancestral graphs with the max-min hill climbing algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsirlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Geometry of the faithfulness assumption in causal inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="436" to="463" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Causal networks: Semantics and expressiveness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine intelligence and pattern recognition</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Sixth Annual Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Permutation-based causal inference algorithms with interventions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st Annual Conference on Neural Information Processing Systems, NIPS 2017</title>
		<meeting><address><addrLine>Long Beach, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">4 December 2017 through 9 December 2017. 2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5823" to="5832" />
		</imprint>
	</monogr>
	<note>Neural information processing systems foundation</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Direct estimation of differences in causal graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belyaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">High-dimensional causal discovery under non-Gaussianity</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Probability distributions with summary graph structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="845" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Polynomial-time algorithms for counting and sampling Markov equivalent DAGs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wienöbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liskiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12198" to="12206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating latent variable causal graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14891" to="14902" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Characterizing and learning equivalence classes of causal DAGs under interventions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katcoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5541" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Learning optimal Bayesian networks: A shortest path perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Malone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="23" to="65" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">SAT-Based Causal Discovery under Weaker Assumptions</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhalama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Strong faithfulness and uniform consistency in causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Nineteenth conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="632" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A transformational characterization of Markov equivalence for directed acyclic graphs with latent variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="667" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">The three faces of faithfulness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1011" to="1027" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03768</idno>
		<title level="m">Causal discovery in the presence of measurement error: Identifiability conditions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Conference on Uncertainty in Artificial Intelligence (UAI 2009)</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">D-VAE: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>1904.11088</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Matching a desired causal state via shift interventions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Direct estimation of differential functional graphical models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: Continuous Optimization for Structure Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
