<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
							<email>lun.du@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
							<email>qifu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
							<email>shihan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Song</surname></persName>
							<email>songx@mail.sustech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an endto-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer decision boundary, and thus yields better performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data is widely used in real-world domains, such as social networks <ref type="bibr" target="#b31">[Zhang and Chen, 2018]</ref>, recommender systems <ref type="bibr">[Wu et al., 2022b]</ref>, and biological molecules <ref type="bibr" target="#b4">[Gilmer et al., 2017]</ref>. The non-euclidean nature of graphs has inspired a new type of machine learning model, Graph Neural Networks (GNNs) <ref type="bibr" target="#b9">[Kipf and Welling, 2016;</ref><ref type="bibr" target="#b3">Defferrard et al., 2016;</ref><ref type="bibr" target="#b3">Du et al., 2022]</ref>. Generally, GNN iteratively updates features of the center node by aggregating those of its neighbors and has achieved remarkable success across various graph analytical tasks. However, the aggregation of features between unrelated nodes has long been an obstacle for GNN, keeping it from further improvement.</p><p>Recently, Graph Attention Network (GAT) <ref type="bibr">[Veličković et al., 2017]</ref> pioneered the adoption of the attention mechanism, a well-established method with proven effectiveness in deep learning <ref type="bibr" target="#b24">[Vaswani et al., 2017]</ref>, into the neighborhood aggregation process of GNNs to alleviate the issue. The key concept behind GAT is to adaptively assign importance to each neighbor during the aggregation process. Its simplicity and effectiveness have made it the most widely used variant of GNN. Following this line, a myriad of attention-based GNNs have been proposed and have achieved state-of-the-art performance in various tasks <ref type="bibr" target="#b23">[Sun et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2022;</ref><ref type="bibr" target="#b30">Ying et al., 2021;</ref><ref type="bibr">Brody et al., 2021]</ref>.</p><p>Nevertheless, despite the widespread use and satisfying results, in the past several years, researchers began to rethink if the learned attention functions are truly effective <ref type="bibr" target="#b9">[Kim and Oh, 2022;</ref><ref type="bibr" target="#b24">Wang et al., 2019;</ref><ref type="bibr">Wu et al., 2022a;</ref><ref type="bibr" target="#b9">Knyazev et al., 2019;</ref><ref type="bibr" target="#b13">Liu et al., 2021;</ref><ref type="bibr">Wang et al., 2021]</ref>. As we know, most existing attention-based GNNs learn the attention function in a weakly-supervised manner, where the attention modules are simply supervised by the final loss function, without a powerful supervising signal to guide the training process. And the lack of direct supervision on attention might be a potential cause of a less robust and generalizable attention function against real-world noisy graphs <ref type="bibr" target="#b23">[Sui et al., 2022;</ref><ref type="bibr" target="#b9">Knyazev et al., 2019;</ref><ref type="bibr" target="#b12">Li et al., 2022;</ref><ref type="bibr" target="#b24">Wang et al., 2019;</ref><ref type="bibr" target="#b29">Ye and Ji, 2021;</ref><ref type="bibr" target="#b7">Huang et al., 2023]</ref>. To address this problem, existing work enhances the quality of attention through auxiliary regularization terms (supervision). However, concerns have been raised that these methods often rely heavily on human-specified prior assumptions about a specific task, which limits their generalizability <ref type="bibr">[You et al., 2020;</ref><ref type="bibr">Wu et al., 2022a]</ref>. Additionally, the auxiliary regularization is formulated independently of the primary prediction task, which may disrupt the original optimization target and cause the model to "switch" to a different objective function during training <ref type="bibr" target="#b9">[Kim and Oh, 2022;</ref><ref type="bibr">You et al., 2020]</ref>.</p><p>Recently, causal inference <ref type="bibr" target="#b14">[Pearl, 2009]</ref> has attracted many researchers in the field of GNNs by utilizing structural causal model (SCM) <ref type="bibr" target="#b3">[Cinelli et al., 2019]</ref> to handle distribution shift <ref type="bibr" target="#b34">[Zhao et al., 2022]</ref> and shortcut learning <ref type="bibr">[Feng et al., 2021]</ref>. In this paper, we argue that the tool of causal inference has also shed light on a promising avenue that could supervise and improve the quality of GNN's attention directly, while in the meantime we will not make any assumptions about specific tasks or models, and the supervision signal for attention implicitly aligns well with the primary task. Before going any deeper, we first provide a general schema for the SCM of attention-based GNNs in Figure <ref type="figure" target="#fig_0">1</ref>, which uses nodes to represent variables and edges to indicate causal relations between variables. As we can see, after a high-level abstraction, there are only three key factors in SCM, including the node features X, attention maps A, and the model's final prediction Y . Note that in causal language, X, A, and Y also denotes the context, treatment, and outcome respectively. For edges in SCM, the link X → A represents that the attention generation relies on the node's features (i.e., context decides treatment). And links (X, A) → Y indicate that the model's final prediction is based on both the node's features X and the attention A (i.e., the final outcome is jointly determined by both context and treatment).</p><p>In order to provide a direct supervision signal and further enhance the learning of attention functions, the first step would be finding a way to measure the quality of attention (i.e., quantifying what to improve). Since there are no unified criteria on the way of measurement, researchers usually propose their own solution according to the tasks they are facing, and this is very likely to introduce the unfavorable human-intervened prior assumptions <ref type="bibr">[You et al., 2020]</ref>. For example, CGAT <ref type="bibr" target="#b24">[Wang et al., 2019]</ref> believes that better attention should focus more on one-hop neighbors. While this assumption surely works on homophilous graphs, it will suffer from huge performance degradation in heterophilous scenarios. Our method differs a lot from existing work in that we introduce SCM to effectively decouple the direct causal effect of attention on the final prediction (i.e., link A → Y ), and use such causal effect as a measurement for the quality of attention. In this way, it is the model and data that decide if the attention works well during training instead of human-predefined rules. And this has been shown to be non-trivial in various machine learning fields because what might seem reasonable to a human might not be considered the same way by the model <ref type="bibr" target="#b10">[Kumar et al., 2010;</ref><ref type="bibr">Wang et al., 2022b]</ref>. Another drawback of existing attention regularization methods, as previously mentioned, is the deviation from primary tasks. SuperGAT <ref type="bibr" target="#b9">[Kim and Oh, 2022]</ref> uses link prediction to improve the attention quality for node classification, but as the author claims in the paper, there is an obvious trade-off between the two tasks. In this paper, we alleviate this problem by directly maximizing the causal effect of attention on the primary task (i.e., strengthening the causal relation A → Y ). Under mild conditions, we can deem the overall optimization is still towards the primary objective, except that we additionally provide a direct and powerful signal for the learning of attention in a fully-supervised manner.</p><p>In summary, this paper presents a Causal Supervision for Attention in graph neural networks (abbreviated as CSA in the following paragraphs). CSA has strong applicability because no human-intervened assumptions are made on the target models or training tasks. And the supervision of CSA can be easily and smoothly integrated into optimizing the primary task to performing end-to-end training. We list the main con-tributions in this paper as follows:</p><p>• We explore and provide a brand-new perspective to directly boost GNN's attention with the tool of causality. To the best of our knowledge, this is a promising direction that still remains unexplored.</p><p>• We propose CSA, a novel causal-based supervision framework for attention in GNNs, which can be formulated as a simple yet effective external plug-in for a wide range of models and tasks to improve their attention quality.</p><p>• We perform extensive experiments and analysis on CSA and the universal performance gain on standard benchmark datasets validates the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Attention-based Graph Neural Networks. Modeling pairwise importance between elements in graph-structured data dates back to interaction networks <ref type="bibr" target="#b0">[Battaglia et al., 2016;</ref><ref type="bibr" target="#b5">Hoshen, 2017]</ref> and relational networks <ref type="bibr" target="#b19">[Santoro et al., 2017]</ref>.</p><p>Recently <ref type="bibr">GAT [Veličković et al., 2017]</ref> rose as one of the representative work of attention-based GNNs using selfattention <ref type="bibr" target="#b24">[Vaswani et al., 2017]</ref>. The remarkable success of GAT in multiple tasks has motivated many works focusing on integrating attention into <ref type="bibr">GNN [Thekumparampil et al., 2018;</ref><ref type="bibr" target="#b31">Zhang et al., 2018;</ref><ref type="bibr">Wang et al., 2022a;</ref><ref type="bibr" target="#b33">Zhang et al., 2020;</ref><ref type="bibr">Gao and Ji, 2019;</ref><ref type="bibr" target="#b6">Hou et al., 2022]</ref>. Lee et al. have also conducted a comprehensive survey <ref type="bibr" target="#b11">[Lee et al., 2019]</ref> on various types of attention used in GNNs.</p><p>Causal Inference in Graph Neural Network. Causality <ref type="bibr" target="#b14">[Pearl, 2014]</ref> provides researchers new methodologies to design robust measurements, discover hidden causal structures and confront data biases. A myriad of studies has shown that incorporating causality is beneficial to graph neural network in various tasks. <ref type="bibr" target="#b34">[Zhao et al., 2022]</ref> makes use of counterfactual links to augment data for link prediction improvement. <ref type="bibr" target="#b23">[Sui et al., 2022]</ref> performs interventions on the representations of graph data to identify the causally attended subgraph for graph classification. <ref type="bibr">[Feng et al., 2021]</ref> on the other hand, applies causality to estimate the causal effect of node's local structure to assist node classification.</p><p>Improving Attention in GAT. There is a great number of work dedicated to improving attention learning in GAT. <ref type="bibr" target="#b8">[Kim and Oh, 2020]</ref> enhances attention by exploiting two attention forms compatible with a self-supervised task to predict edges.</p><p>[Brody et al., 2021] introduces a simple fix by modifying the order of operations in GAT. <ref type="bibr" target="#b24">[Wang et al., 2019]</ref> develops an approach using constraint on the attention weights according to the class boundary and feature aggregation pattern. In addition, causality also plays a role in boosting the attention of GATs recently. <ref type="bibr">[Wu et al., 2022a]</ref> estimates the causal effect of edges by intervention and regularizes edges' attention weights according to their causal effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We start by introducing the notations and formulations of graph neural networks and their attention variant. Let G = {V, E} represents a graph where V = {v i } n i=0 is the set of nodes and E ∈ V ×V is the set of edges. For each node v ∈ V, </p><formula xml:id="formula_0">(v) = {u ∈ V | (v, u) ∈ E}</formula><p>and its initial feature vector x 0 v ∈ R d 0 , where d 0 is the original feature dimension. Generally, GNN follows the messagepassing mechanism to perform feature updating, where each node's feature representation is updated by aggregating the representations of its neighbors and then combining the aggregated messages with its ego representation <ref type="bibr" target="#b28">[Xu et al., 2018]</ref>. Let m l v ∈ R d l and x l v ∈ R d l be the message vector and representation vector of node v at layer l, we formally define the updating process of GNN as:</p><formula xml:id="formula_1">m l v = AGGREGATE x (l-1) j , ∀j ∈ N (v) x l v = COMBINE x l-1 v , m l v ,</formula><p>where AGGREGATE and COMBINE are aggregation functions (e.g., mean, LSTM) and combination function (e.g., concatenation), respectively. The design of these two functions is what mostly distinguishes one type of GNN from the other. <ref type="bibr">GAT [Veličković et al., 2017]</ref> augments the normal aggregation with the introduction of self-attention. The core idea of self-attention in GAT is to learn a scoring function that computes an attention score for every node in N (v) to indicate their relational importance to node v. In layer l, such process is defined by the following equation:</p><formula xml:id="formula_2">e x l vi , x l vj = σ (a l ) ⊤ • W l x l vi ∥W l x l vj ,</formula><p>where (a l , W l ), σ are learnable matrices and activation function (e.g., LeakyReLU) respectively, and ∥ denotes vector concatenation. The attention scores are then normalized across all neighbors v j ∈ N (v i ) using softmax to ensure consistency:</p><formula xml:id="formula_3">α l ij = exp e x l vi , x l vj vj ∈N (vi) exp e x l vi , x l vj</formula><p>Finally, GAT computes a weighted average of the features of the neighboring nodes as the new feature of v i , which is demonstrated as follows:</p><formula xml:id="formula_4">x l+1 vi = σ vj ∈N (vi) α l ij W l x l vj .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Casual-based Supervision on GNN's Attention</head><p>In this section, we first introduce how the causal effect of attention can be derived from the structural causal model of attention-based GNNs. Specifically, this is done with the help of the widely used counterfactual analysis in causal reasoning. After that, with the obtained causal effects, we elaborate three candidate schemes to incorporate with the training of attention-based GNNs to improve their quality of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Causal Effects of Attention</head><p>As previously mentioned, the first step towards improving attention lies in measuring the quality of existing attention. However, since deep learning models usually exhibit as black boxes, it is generally infeasible to directly assess their attention qualities. Existing works mainly address this issue by introducing human priors to build pre-defined rules for some specific models and tasks. Yet, it has been a long debate on whether human-made rules share consensus with deep learning models during training <ref type="bibr" target="#b10">[Kumar et al., 2010;</ref><ref type="bibr" target="#b18">Ribeiro et al., 2016]</ref>. Fortunately, the recent rise of causal inference technology has offered effective tools to help us think beyond the black box and analyze causalities between model variables, which leads us to an alternative way to directly utilize the causal effect of attention to measure its quality. Since the obtained causal effects are mainly affected by the model itself, it is a more accurate and unbiased measurement of how well the attention actually learns. We first give a brief review of the formulation of attentionbased graph neural network in causal languages, as shown in Figure <ref type="figure" target="#fig_1">2</ref>(a). The generated attention map A is directly affected by node feature X. And the model prediction Y is jointly determined by both X and A. We denote the inferring process of the model as:</p><formula xml:id="formula_5">Y x,a = Y (X = x, A = a),<label>(1)</label></formula><p>which indicates that model will give value Y x,a if the value of X and A are set to x and a respectively. In order to pursue the attention's causal effect, we introduce the widely-used counterfactual analysis <ref type="bibr">[Pearl, 2022]</ref> in causal reasoning.</p><p>The core idea of counterfactual causality lies in asking: given a certain data context (node feature X), what the outcome (model prediction Y ) would have been if the treatment (attention map A) had not been the observed value? To answer the imaginary question, we have to manipulate the values of several variables to see the effect, and this is formally termed as intervention in causal inference literature, which can be denoted as do(•). In do(•) operation, we compulsively select a counterfactual value to replace the original factual value of the intervened variable. And once a variable is intervened, its all incoming links in the SCM will be cut off and its value is independently given, while other variables that are not affected still maintain the original value. In our case, for example, do(A = a * ) means we demand the attention A to take the non-real value a * (e.g., reverse/random attention) so that the link X → A is cut-off and A is no longer be affected by its causal parent X. This process is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>(b) and the mathematical formulation is given as:</p><formula xml:id="formula_6">Y x,a * = Y (X = x, do(A = a * )),<label>(2)</label></formula><p>which indicates that after do(•) operation which changes the value of attention to be a * , the output value of the model also changes to Y x,a * . Finally, let us consider a case where we assign a dummy value ã to the attention map so that for each ego node, all its neighbors share the same attention weights, the feature aggregation of the graph attention model will then degrade to an unweighted average. In this case, according to the theory of causal inferences <ref type="bibr" target="#b24">[VanderWeele, 2016]</ref>, the Total Direct Effect (TDE) of attention to model prediction can be obtained by computing the differences of model outcome Y x,a and Y x,ã , which is formulated as follows:</p><formula xml:id="formula_7">T DE = Y x,a -Y x,ã .<label>(3)</label></formula><p>It is worth noting that the induction of attention's causal effect does not exert any assumptions and constraints, which lays a solid foundation for our wide applicability on any graph attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervision on Attention with Causal Effects</head><p>We have already demonstrated the derivation of attention's causal effect in the previous section. In this part, we will discuss how to utilize the obtained causal effect for attention quality improvement. Previous works that make use of an auxiliary task to regularize attention usually suffered from performance trade-off between the primary task and auxiliary task. In this work, we alleviate this problem by directly maximizing the causal effect of attention on the primary task. The overall schema of this part is shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Consider a simple case where we conduct the node classification task with a standard L-layer GAT. For each layer l, we have the node representations X l-1 ∈ R n×d l-1 from the previous layer as input. Then, we perform feature aggregation and updating with factual attention map A l to obtain the factual output feature X l = f (X l-1 , A l ). Similarly, when we intervene the attention maps of layer l (e.g., assigning dummy values using do(•) operation), we can get a counterfactual output feature Xl . We further employ a learnable matrix W l ∈ R c×d l (c denotes the number of classes) to get the node's factual predicted label Y l pred and counterfactual predicted label Ŷ l pred using the corresponding features from layer l. Therefore, the causal effect of attention at layer l is obtained as: Y l pred -Ŷ l pred . To this end, we can use the causal effect as a supervision signal to explicitly guide the attention learning process. The new objective of the CSA-assisted GAT model can be formulated as:</p><formula xml:id="formula_8">L= l λ l L ce (Y l effect , y)+L others , (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where y is the ground-truth label, L ce is the cross-entropy loss, λ l is the coefficient to balance training, and L others represents the original objective such as standard classification loss. Note that Equation.( <ref type="formula" target="#formula_8">4</ref>) is a general form of CSA where we compute additional losses for each GAT layer to supervise attention directly. However in practice it is not necessary, and we found that simply selecting one or two layers is enough for CSA to bring satisfying performance improvement. Moreover, since our aim is to boost the quality of attention, it is not necessary to estimate the correct causal effect of attention using dummy values. Instead, a strong counterfactual baseline might even be helpful for the attention quality improvement. We hereby further propose three heuristic counterfactual schemes and test them in our experiments. We note that the exact form of how counterfactual is achieved is not limited, and our goal here is just to set the ball rolling. Scheme I: In the first scheme, we utilize the uniform distribution to generate the counterfactual attention map. Specifically, the counterfactual attention is produced by</p><formula xml:id="formula_10">â ∼ U (e, f ),<label>(5)</label></formula><p>where e and f are the lower and upper boundaries. In this case, the generated counterfactual could vary from very bad (i.e., mapping all unrelated neighbors) to very good (i.e., mapping all meaningful neighbors). This is a similar process to the Randomized Controlled Trial <ref type="bibr" target="#b22">[Stolberg et al., 2004]</ref> where all possible treatments are enumerated. We hope that maximizing causal effects computed over all possible treatments can lead to a robust improvement of attention. Scheme II: Scheme I is easy and straightforward to apply. However due to its randomness, a possible concern is that if most of the generated counterfactual attentions are inferior to the factual one, then we will only have very small gradient on attention improvements. Therefore we are actually motivated to find "better" counterfactuals to spur the factual one to evolve. Heuristically, given that MLP is a strong baseline one several datasets (e.g., Texas, Cornell, and Wisconsin), we employ an identical mapping to generate the counterfactual attention, which only attends to the ego-node instead of neighbors. Specifically, the counterfactual attention map is equal to the identity matrix I:</p><formula xml:id="formula_11">â ∼ I (6)</formula><p>Scheme III: Our last schema can be considered as an extension of Schema II. Since the fast development of GAT family has introduced to us some variants that already outperform MLP in many datasets, using counterfactuals derived from the behavior of MLP does not seem to be a wise choice for these GAT variants to improve their attentions. Inspired by the self-boosting concept <ref type="bibr" target="#b17">[Pi et al., 2016]</ref> widely used in machine learning, we leverage the historical attention map as the counterfactual to urge the factual one keep refining itself. The specific formulation is written as follows:</p><formula xml:id="formula_12">â ∼ A hist ,<label>(7)</label></formula><p>where A hist denotes the historical attention map (e.g., the attention map from the last update iteration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we conduct extensive node classification experiments to evaluate the performance of CSA. Specifically, we 1) validate the effectiveness of CSA on three popular GAT variants using a wide range of datasets, including both homophily and heterophily scenarios; 2) compare CSA with other attention improvement baselines of GAT to show the superiority of our method; 3) show that CSA can induce better attention that improve the robustness of GAT; 4) test CSA's sensitivity to hyper-parameters; 5) analyze the influences of CSA in feature space; and 6) examine the performances of some special cases of CSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For heterophily scenario, we select seven standard benchmark datasets: Wisconsin, Cornell, Texas, Actor, Squirrel, Chameleon, and Crocodile. Wisconsin, Cornell, and Texas collected by Carnegie Mellon University are published in We-bKB 1 . Actor <ref type="bibr" target="#b16">[Pei et al., 2020]</ref> is the actor-only subgraph 1 <ref type="url" target="http://www.cs.cmu.edu/">http://www.cs.cmu.edu/</ref> webkb/ sampling from a film-director-actor-writer network. Squirrel, Chameleon, and Crocodile are datasets collected from the English Wikipedia <ref type="bibr" target="#b19">[Rozemberczki et al., 2021]</ref>. We summarize the basic attributions for each dataset in Table <ref type="table" target="#tab_0">1</ref>. H(G) is the homophily ratio <ref type="bibr">[Zhu et al., 2020]</ref>, where H(G) → 1 represents extreme homophily and vice versa.</p><p>For homophily scenario, two large datasets released by Open Graph Benchmark (OGB)<ref type="foot" target="#foot_1">foot_1</ref> [Hu et al., 2020]: ogbnproducts and ogbn-arxiv, are included in our experiments, together with two small-scale homophily graph datasets: Cora and Citeseer <ref type="bibr">[McCallum et al., 2000]</ref>. Similarly, the attribution of the dataset is summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We employ popular node classification models in our experiments as the baselines: <ref type="bibr">GCN [Kipf and Welling, 2016]</ref>, <ref type="bibr">GAT [Veličković et al., 2017]</ref>, SGCN <ref type="bibr" target="#b26">[Wu et al., 2019]</ref>, <ref type="bibr">FAGCN [Bo et al., 2021]</ref>, GPR-GNN <ref type="bibr" target="#b2">[Chien et al., 2020]</ref>, <ref type="bibr">H2GCN [Zhu et al., 2020]</ref>, <ref type="bibr">WRGAT [Suresh et al., 2021]</ref>, APPNP <ref type="bibr">[Gasteiger et al., 2018]</ref> and UniMP <ref type="bibr" target="#b20">[Shi et al., 2020]</ref>. We also present the performance of MLPs, serving as a strong non-graph-based baseline. Due to page limit, we only select four models: GAT, FAGCN, WRGAT and UniMP to examine the effectiveness of CSA. These models ranges from the classic ones to the latest ones, and are considered as representatives for state-of-the-art node classification models. One thing to be noted here is that for all these models, we implement CSA only in their first layers to avoid excessive computational cost.</p><p>In our experiments, each GNN is run with the best hyperparameters if provided. We set the same random seed for each model and dataset for reproducibility. The reported results are in the form of mean and standard deviation, calculated from 10 random node splits (the ratio of train/validation/test is 48%/32%/20% from <ref type="bibr" target="#b16">[Pei et al., 2020]</ref>). Our experiments are conducted on a GPU server with eight NVIDIA DGX A100 graphics cards, and the codes are implemented using Cuda Toolkit 11.5, PyTorch 1.8.1 and torch-geometric 2.0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Analysis</head><p>Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref> provide the test accuracy of different GNNs in different variants of CSA in the supervised node classification task. A graph's homophily level is the average of its nodes' homophily levels. CSA achieves the best in terms of the vanilla one across all datasets. In particular, the highest improved datasets are Texas, Wisconsin, and Cornell. By observing the performance of MLPs, we can see that the common ground of those three datasets contains distinguishable features and a large part of non-homophilous edges. In the meanwhile, the performance of CSA is proportional to the modeling ability. The mechanism behind CSA is to extend the causal effect between nodes representation and final prediction. Therefore, CSA owns limited performance when the node's representations are chaotic. Our experiments highlight that I) The model, which is already better than MLP, does not improve much in CSA-II, while CSA-III improves it relatively more. This is because in those datasets, the graph structure can provide meaningful information, so that CSA-III have more advantages. II) The dataset, which has distinctive features indicated by the performance of MLPs, is more satisfied CSA-II. Similarly, in this scene, the features can be more informative. III) The random strategy (CSA-I) relatively inferior to others, since the distribution is hard to control and tend to generate worst attention map, whereby the regularization is vanished. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with Attention Promotion Baselines</head><p>We here apply multiple attention promotion baselines: CAL <ref type="bibr" target="#b23">[Sui et al., 2022]</ref>, CAR <ref type="bibr">[Wu et al., 2022a]</ref>, Super <ref type="bibr" target="#b9">[Kim and Oh, 2022]</ref>, Constraint interventions <ref type="bibr" target="#b24">[Wang et al., 2019]</ref>  and CAR employs an edge intervention strategy that enables causal effects to be computed scalable, while our method does not exert any assumptions and constraints on GATs, compared with CAL and CAR. Therefore, CSA tends to own good generalization ability. In terms of SuperGAT and Constraint method, since there is a trade-off between node classification and regularization. For example, SuperGAT implies that it is hard to learn the relational importance of edges by simply optimizing graph attention for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">CSA Provides Robustness</head><p>In this section, we systematically study the performance of CSA on robustness against the input perturbations including feature and edge perturbations. Following <ref type="bibr" target="#b21">[Stadler et al., 2021]</ref>, we conduct node-level feature perturbations by replac- ing them with the noise sampled from the Bernoulli distribution with p = 0.5 and edge perturbations by stochastically generating the edges. According to the performance shown in Figure <ref type="figure">5</ref>, CSA produces robust performance on input perturbations. Figure <ref type="figure">5</ref> demonstrate that CSA in higher noise situations achieves more robust results than in lower noise scenes on both node and edge perturbations with perturbation percentages ranging from 0% to 40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Hyper-parameter Analysis</head><p>We analyze the sensitivity of λ and plot node classification performance in Figure <ref type="figure">6</ref>. For λ, there is a specific range that maximizes test performance in all datasets. The performance in Texas is the highest when λ is 0.4, but the difference is relatively small compared to Cora. We observe that there is an optimal level of causal supervision for each dataset, and using too large λ degrades node classification performance. Since Cora owns friendly neighbors, its performance is less sensitive than Texas. Based on this, we can also see that Texas relatively needs a larger regularization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Pairwise Distance among Classes</head><p>To further evaluate whether the good performance of CSA can be contributed to the mitigation of lacking supervision, we visualize the pairwise distance of the node representations among classes learned by CSA and vanilla GAT. Following <ref type="bibr" target="#b21">[Stadler et al., 2021]</ref>, we calculate the Mean Average Distance (MAD) with cosine distance among node representations for the last layer. The larger the MAD is, the better the node representations are. Results are reported in Figure <ref type="figure" target="#fig_6">7</ref>. It can be observed that the node representations learned by CSA keep having a large distance throughout the optimization process, indicating relief of lacking supervision issues.</p><p>On the contrary, GAT suffers from severely indistinguishable representations of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Comparison with Heuristic Causal Strategies.</head><p>To validate the effectiveness of CSA, we compare it with two heuristic causal designs (Last and Pure) that 1) directly estimate the total causal effect by subtracting between the model's and the counterfactual's output in the final layer; 2) replace the attention with the static aggregate weights (i.e., each node allocates the same weight). The results are shown in Table <ref type="table" target="#tab_2">3</ref>. We observe that their performance outperforms vanilla one, but is still inferior to ours. In terms of Last, the major difference is whether to explicitly estimate causal effect or not. In our framework, we plug the MLPs into the hidden layers to precisely estimate the causal effect for each layer. Regarding Pure, our strategy can provide more strong baselines, leading to better regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced CSA, a counterfactual-based regularization scheme that can be applied to graph attention architectures. Unlike other causal approaches, we first built the causal graph of GATs in a general way and do not impose any assumptions and constraints on GATs. Subsequently, we introduce an efficient scheme to directly estimate the causal effect of attention in hidden layers. Applying it to both homophilic and heterophilic node-classification tasks, we found accuracy improvements and robustness in almost all circumstances. We performed three variants of counterfactual attention strategies and found that they can adapt to different situations, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structural causal model of attention-based GNNs</figDesc><graphic coords="2,66.15,54.00,218.70,61.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Deriving causal effects through counterfactualit has its own neighbor set N (v) = {u ∈ V | (v, u) ∈ E} and its initial feature vector x 0 v ∈ R d 0 , where d 0 is the original feature dimension. Generally, GNN follows the messagepassing mechanism to perform feature updating, where each node's feature representation is updated by aggregating the representations of its neighbors and then combining the aggregated messages with its ego representation<ref type="bibr" target="#b28">[Xu et al., 2018]</ref>. Let m l v ∈ R d l and x l v ∈ R d l be the message vector and representation vector of node v at layer l, we formally define the updating process of GNN as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The schematic of CSA is shown above as a plug-in to graph attention methods. The a and â indicate the factual and counterfactual attention values, respectively. We subtract the counterfactual classification results from the original classification to analyze the causal effects of learned attention (i.e., attention quality) and directly maximize them in the training process towards primary task.</figDesc><graphic coords="4,54.00,54.00,242.98,176.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison with different GATs promotion strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The robust performance on the node and edge perturbations.</figDesc><graphic coords="7,54.00,54.00,243.00,220.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Mean Average Distance among node representations of Last GAT layer.</figDesc><graphic coords="7,323.42,58.98,109.35,75.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>± 6.22 84.38 ± 5.34 36.09 ± 1.35 28.98 ± 1.32 46.21 ± 2.89 83.92 ± 5.88  54.35 ± 1.90 SGCN 56.41 ± 4.29 54.82 ± 3.63 30.50 ± 0.94 52.74 ± 1.58 60.89 ± 2.21 62.52 ± 5.10 51.80 ± 1.± 5.61 87.02 ± 3.55 36.47 ± 0.74 48.93 ± 0.89 65.33 ± 0.92 82.76 ± 3.67 54.97 ± 1.23 +CSA-III 84.88 ± 5.23 87.86 ± 3.77 36.89 ± 0.72 49.43 ± 0.88 66.02 ± 1.01 82.43 ± 4.00 55.33 ± 1.18 Classification accuracy on heterophily datasets. CSA-I, CSA-II, and CSA-III indicate our three counterfactual schemes respectively.</figDesc><table><row><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Chameleon</cell><cell>Cornell</cell><cell>Crocodile</cell></row><row><cell cols="7">H(G) #Nodes #Edges #Classes #Features MLP GCN H2GCN APPNP GPR-GNN 79.44 ± 5.17 84.46 ± 6.36 35.11 ± 0.82 32.33 ± 2.42 46.76 ± 2.10 79.91 ± 6.60 52.74 ± 1.88 0.11 0.21 0.22 0.22 0.23 0.3 0.26 183 251 7,600 5,201 2,277 183 11,631 295 466 191,506 198,493 31,421 280 899,756 5 5 5 5 5 5 6 1703 1703 932 2089 2325 1703 500 81.32 53 55.59 ± 5.96 53.48 ± 4.75 28.40 ± 0.88 53.98 ± 1.53 61.54 ± 2.59 60.01 ± 5.67 52.24 ± 2.54 84.81 ± 6.94 86.64 ± 4.63 35.83 ± 0.96 37.95 ± 1.89 58.27 ± 2.63 82.08 ± 4.71 53.10 ± 1.23 81.93 ± 5.77 85.48 ± 4.58 35.90 ± 0.96 39.08 ± 1.76 57.80 ± 2.47 81.92 ± 6.12 53.06 ± 1.90 GAT 55.21 ± 5.70 52.80 ± 6.11 29.04 ± 0.66 40.00 ± 0.99 59.32 ± 1.54 61.89 ± 6.08 51.28 ± 1.79 +CSA-I 56.17 ± 5.32 53.23 ± 6.28 29.03 ± 0.79 40.51 ± 0.98 60.73 ± 1.35 62.75 ± 6.32 51.67 ± 1.62 +CSA-II 58.21 ± 4.79 54.35 ± 6.54 29.71 ± 0.74 41.02 ± 1.23 61.31 ± 1.13 64.26 ± 5.21 52.20 ± 1.74 +CSA-III 58.04 ± 5.27 53.98 ± 6.30 29.72 ± 0.86 41.38 ± 1.19 61.20 ± 1.37 63.58 ± 6.03 52.13 ± 1.83 FAGCN 82.54 ± 6.89 82.84 ± 7.95 34.85 ± 1.24 42.55 ± 0.86 61.21 ± 3.13 79.24 ± 9.92 54.35 ± 1.11 +CSA-I 82.65 ± 7.11 83.37 ± 7.79 34.77 ± 0.95 42.55 ± 0.74 61.86 ± 2.98 80.01 ± 9.72 54.44 ± 1.18 +CSA-II 83.29 ± 6.80 83.11 ± 8.26 34.88 ± 0.86 42.58 ± 0.93 61.74 ± 3.39 81.35 ± 9.68 54.45 ± 1.23 +CSA-III 84.72 ± 6.71 84.23 ± 7.21 35.12 ± 0.98 43.38 ± 1.02 62.52 ± 3.20 80.94 ± 9.77 55.16 ± 0.97 WRGAT 83.62 ± 5.50 86.98 ± 3.78 36.53 ± 0.77 48.85 ± 0.78 65.24 ± 0.87 81.62 ± 3.90 54.76 ± 1.12 +CSA-I 83.69 ± 5.63 87.23 ± 3.94 36.55 ± 0.93 49.46 ± 0.74 65.36 ± 1.05 81.88 ± 3.93 54.86 ± 1.31 +CSA-II 83.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on homophily datasets.</figDesc><table><row><cell>Models H(G) #Nodes #Edges #Classes #Features GAT +CSA-I +CSA-II +CSA-III UniMP +CSA-I +CSA-II +CSA-III</cell><cell>Cora 0.81 2,708 5,278 7 1433 86.21 ± 0.78 75.73 ± 1.23 CiteSeer 0.74 3,327 4,467 7 3703 86.16 ± 0.95 76.81 ± 1.29 86.89 ± 0.64 76.53 ± 1.18 87.86 ± 0.87 77.72 ± 1.25 86.89 ± 0.90 75.14 ± 0.68 87.47 ± 0.87 75.89 ± 0.73 85.62 ± 0.73 75.87 ± 0.72 88.64 ± 1.28 77.61 ± 0.82</cell><cell>ogbn-products 0.81 2,449,029 61,859,140 47 100 77.02 ± 0.63 77.28 ± 0.69 77.44 ± 0.63 78.36 ± 0.72 81.37 ± 0.47 81.55 ± 0.62 81.39 ± 0.47 82.24 ± 0.63</cell><cell>ogbn-arxiv 0.66 169,343 1,166,243 40 128 70.96 ± 0.14 71.08 ± 0.14 71.05 ± 0.14 71.20 ± 0.16 72.92 ± 0.10 72.94 ± 0.10 72.96 ± 0.10 73.08 ± 0.11</cell></row><row><cell cols="4">Texas 55.21 ± 5.70 61.89 ± 6.08 70.96 ± 0.14 Cornell ogbn-arxiv +CSA (Last) 57.83 ± 4.65 63.52 ± 5.34 71.08 ± 0.15 Models GAT +CSA (Pure) 55.79 ± 5.05 61.97 ± 5.18 70.96 ± 0.14 +CSA (Ours) 58.21 ± 4.79 64.26 ± 5.21 71.20 ± 0.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the heuristic causal strategies.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://ogb.stanford.edu/ Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution Statement</head><p>Hongjun Wang and Jiyuan Chen contributed equally to the work. Lun Du and Xuan Song are the corresponding authors. This work is done during Hongjun Wang's internship at Microsoft Research Asia.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><surname>Brody</surname></persName>
		</editor>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
	<note>How attentive are graph attention networks? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><surname>Chien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feng et al., 2021] Fuli Feng, Weiran Huang, Xiangnan He, Xin Xin, Qifan Wang, and Tat-Seng Chua. Should graph convolution trust neighbors? a simple causal inference method</title>
		<author>
			<persName><surname>Cinelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</editor>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2019. 2019. 2016. 2016. 2022. 2022. 2021. 2019. 2018</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="741" to="749" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph neural networks meet personalized pagerank</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13170</idno>
		<imprint>
			<date type="published" when="2020">2022. 2022. 2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust mid-pass filtering graph convolutional networks</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<meeting>the ACM Web Conference 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with selfsupervision</title>
		<author>
			<persName><forename type="first">Oh</forename><forename type="middle">;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How to find your friendly neighborhood: Graph attention design with selfsupervision</title>
		<author>
			<persName><forename type="first">Oh</forename><forename type="middle">;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Knyazev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04879</idno>
		<idno>arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2022. 2022. 2016. 2016. 2019. 2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Understanding attention and generalization in graph neural networks. Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07308</idno>
		<title level="m">Finding global homophily in graph neural networks when meeting heterophily</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<editor>
			<persName><surname>Mccallum</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2000">2021. 2021. 2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
		</imprint>
	</monogr>
	<note>Non-local graph neural networks</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pearl, 2014] Judea Pearl. Interpretation and identification of causal mediation</title>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2009">2009. 2009. 2014</date>
			<publisher>Judea Pearl. Causality. Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Judea Pearl. Direct and indirect effects</title>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Causal Inference: The Works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="373" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Pei</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-paced boost learning for classification</title>
		<author>
			<persName><surname>Pi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1932" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><surname>Rozemberczki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2021. 2021. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Multi-scale attributed node embedding</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph posterior network: Bayesian predictive uncertainty for node classification</title>
		<author>
			<persName><surname>Stadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18033" to="18048" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randomized controlled trials</title>
		<author>
			<persName><surname>Stolberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AJR Am J Roentgenol</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1539" to="1544" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Breaking the limit of graph neural networks by improving the assortativity of graphs with local mixing patterns</title>
		<author>
			<persName><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<idno>arXiv:1803.03735</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2022. 2022. 2021. 2021. 2018</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention-based graph neural network for semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Veličković et al., 2017] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><forename type="first">;</forename><surname>Vanderweele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Vanderweele</surname></persName>
		</author>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<idno>arXiv:1910.11945</idno>
	</analytic>
	<monogr>
		<title level="m">Explanation in causal inference: developments in mediation and interaction</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016. 2016. 2017. 2017. 2017. 2019. 2019. 2021. 2021</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">St-expertnet: A deep expert framework for traffic prediction</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15182</idno>
	</analytic>
	<monogr>
		<title level="m">Easy begun is half done: Spatialtemporal graph modeling with st-curriculum dropout</title>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Wang et al., 2022b</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Causally-guided regularization of graph attention improves generalizability</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10946</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
	</analytic>
	<monogr>
		<title level="m">Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2022. 2022. 2018</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
	<note>Xu et al., 2018] Keyulu Xu</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse graph attention networks</title>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">;</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">Ying</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<editor>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2021. 2021. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
	<note>When does self-supervision help graph convolutional networks?</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5165" to="5175" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive structural fingerprints for graph attention networks</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04355</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graph attention multi-layer perceptron</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2022. 2022. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
