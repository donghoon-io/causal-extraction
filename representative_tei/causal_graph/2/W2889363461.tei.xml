<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Extraction of Causal Relations from Text using Linguistically Informed Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tirthankar</forename><surname>Dasgupta</surname></persName>
							<email>dasgupta.tirthankar@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">TCS Innovation Lab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rupsa</forename><surname>Saha</surname></persName>
							<email>rupsa.s@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">TCS Innovation Lab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lipika</forename><surname>Dey</surname></persName>
							<email>lipika.dey@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">TCS Innovation Lab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abir</forename><surname>Naskar</surname></persName>
							<email>abir.naskar@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">TCS Innovation Lab</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Extraction of Causal Relations from Text using Linguistically Informed Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we have proposed a linguistically informed recursive neural network architecture for automatic extraction of cause-effect relations from text. These relations can be expressed in arbitrarily complex ways. The architecture uses word level embeddings and other linguistic features to detect causal events and their effects mentioned within a sentence. The extracted events and their relations are used to build a causal-graph after clustering and appropriate generalization, which is then used for predictive purposes. We have evaluated the performance of the proposed extraction model with respect to two baseline systems,one a rule-based classifier, and the other a conditional random field (CRF) based supervised model. We have also compared our results with related work reported in the past by other authors on SEMEVAL data set, and found that the proposed bidirectional LSTM model enhanced with an additional linguistic layer performs better. We have also worked extensively on creating new annotated datasets from publicly available data, which we are willing to share with the community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The concept of causality can be informally introduced as a relationship between two events e 1 and e 2 such that occurrence of e 1 results in the occurrence of e 2 . Curating causal relations from text documents help in automatically building causal networks which can be used for predictive tasks. Expression of causality can be expressed within text documents in arbitrarily complex ways. For example, in the sentence "Aircel files for bankruptcy over mounting financial troubles", the event "mounting financial troubles" is causing the event "Aircel filed for bankruptcy." In a more complicated scenario, "Company recalled some vehicles to fix loose bolts that could lead to engine stall" we can observe nested cause-effect pairs. Here, the effect "company recalled vehicle" is caused by the event "to fix loose bolts is not easy to extract. That the cause "loose bolts" could lead to engine stall", is even more difficult to detect.</p><p>While there has been a considerable body of researchers working in the area whose work has been reviewed in section 2, there are many challenges that are still not properly addressed. Most of the earlier approaches have considered rule based or traditional machine learning algorithms which heavily depend on careful feature engineering. Though one sees adoption of deep learning techniques for causality extraction, it is still considerably low compared to other text mining tasks. This is largely due to the unavailability of adequate annotated data: the only available dataset for evaluation is the SEMEVAL-10 Task 8 which is woefully inadequate to train such deep models. There are challenges with annotations of this data also <ref type="bibr" target="#b22">(Rehbein and Ruppenhofer, 2017)</ref>.</p><p>Most of the existing extraction mechanisms look for single word representation of events within a sentence, thereby yielding wrong results. For example, in the sentence "The AIDS pandemic caused by the spread of HIV infection" the cause and effect are both multi-word phrases i.e. "spread of HIV infection" and 'AIDS pandemic'. However, SEMEVAL 2010 annotated dataset for this task mentions the cause and effect as "infection" and "pandemic" only. In another example, "Infectious diseases or communicable diseases are caused by bacteria, viruses, and parasites.", the need to extract multiple causal as well as effect events is obvious. The example sentence in the first paragraph not only demonstrates the need to extract phrases as events, but also highlights how complex such statements can be, often without the use of known causal connectives like "causes, because of, leads to, after, due to" etc. which have been traditionally exploited by the community.</p><p>In this work, we explore the use of bidirectional LSTMs that can learn to detect causal instances from sentences. To address the paucity of training data, we propose the use of additional linguistic feature embeddings, over and above the regular word embeddings. With the use of such linguistically-informed deep architecture, we avoid the task of complex feature engineering.</p><p>A major contribution of this work is in developing annotated datasets with information curated from multiple sources spanning across different domains. To do this, we have collected news articles and generate annotations. Beside SE-MEVAL dataset we have also used another available dataset that has annotated data about drugs and their adverse effect extracted from Medline <ref type="bibr" target="#b11">(Gurulingappa et al., 2012)</ref>. We have done intensive experimentations with parts of the dataset for training and testing which will be discussed in the following sections.</p><p>Detection of causal relation from text has many analytical and predictive applications. Few of these are: detecting cause-effect relations in medical documents, learning about after effects of natural disasters, learning causes for safety related incidents etc.. However to build a meaningful application that can detect an event from texts and predict its possible effects, there is a need to curate large volume of cause-effect event pairs. Further, similar events need to be grouped and generalized to super classes, over which the predictive framework can be built <ref type="bibr" target="#b28">(Zhao et al., 2017)</ref>. In this paper, we have proposed a k-means clustering of causal and effect events detected from text, using word vector representations.</p><p>The rest of the paper is organized as follows. Section 2 summarizes challenges and related works on causality detection. Section 3 presents the resource creation and the architecture of the proposed causality extraction framework. Experiments and evaluation are detailed in Section 4. Finally, in section 5 we conclude the paper.  <ref type="bibr" target="#b0">(Blanco et al., 2008)</ref> <ref type="bibr" target="#b12">(Hendrickx et al., 2009</ref>) <ref type="bibr" target="#b25">(Sorgente et al., 2013)</ref>. Marked Causality is where there is a linguistic signal of causation present. For example, "I attended the event because I was invited". Here, causality is marked by because. On the other hand in "Drive slowly.</p><p>There are potholes", causality is unmarked. Explicit Causality is where both cause and effect are stated. For example, "The burst has been caused by water hammer pressure" has both cause and effect stated explicitly. However, "The car ran over his leg" does not have the effect of the accident explicitly stated.</p><p>Automatic extraction of cause-effect relations are primarily based on three different approaches namely, Linguistic rule based, supervised and unsupervised machine learning approaches. Both SemEval-2007 <ref type="bibr" target="#b7">(Girju et al., 2007</ref><ref type="bibr">) &amp; 2010</ref><ref type="bibr" target="#b12">(Hendrickx et al., 2009)</ref> had tasks aimed at identifying different relations from text, including Cause-Effect relations. Both tasks offered a corpus of annotated gold standard data to researchers. However, the task has primarily focused on extracting single word cause-effect pairs. Early work in this area relied totally on hand-coded patterns. These were heavily dependent on both domain and linguistic knowledge, due to the nature of the patterns, and were hard to scale up. PROTEUS <ref type="bibr" target="#b10">(Grishman, 1988)</ref> and COATIS <ref type="bibr" target="#b4">(Garcia, 1997)</ref> were two early systems that used such non-statistical techniques. C.G Khoo carried out extensive development of this train of thought in a series of works <ref type="bibr" target="#b16">(Khoo et al., 1998</ref><ref type="bibr" target="#b17">) (Khoo et al., 2001)</ref>, and eliminated a lot of the need for domain knowledge.</p><p>A method of automatically identifying linguistic patterns that indicate causal relations and a semi-supervised method of validation of patterns obtained was proposed by <ref type="bibr" target="#b6">(Girju et al., 2002)</ref>. In particular, this work introduced the usage of WordNet hierarchal classes, namely, human action, phenomenon, state, psychological feature and event, as a distinguishing feature. <ref type="bibr">Radinsky et al.</ref> in their work uses statistical inferencing combined with hierarchical clustering technique to predict future events from news <ref type="bibr" target="#b21">(Radinsky et al., 2012)</ref>. Logistic regression was employed <ref type="bibr" target="#b1">(Bui et al., 2010)</ref> to extract drugs (cause) and virus mutation (effect) occurrences from medical literature. The relatively untouched task of extracting implicit cause-effect from sentences was tackled by Ittoo et.al <ref type="bibr" target="#b15">(Ittoo and Bouma, 2011)</ref>. More recently, Zhao et al. <ref type="bibr" target="#b28">(Zhao et al., 2017)</ref> have proposed novel causality network embeddings for the abstract representation of causal events from News headlines. Here, the authors have primarily used four common causal connectives namely, "because", "after", "because of" and "lead to" to extract causal mentions in news headlines and constructed a network of causal relations. The authors have proposed a novel generalization technique to represent "specific events" into more abstract form. Finally, they proposed a dual cause-effect model that uses the causal network embeddings and optimize the margin based loss function to predict effect of a given cause. Although the work is commendable, there are various factors that need to be addressed further. For example, construction of the causal network itself is a non trivial task. Some of the linguistic challenges have already mentioned earlier in this section. Further, Zhao et al. worked with only unambiguous causal connectives. On the contrary causal connectives can be ambiguous also <ref type="bibr" target="#b25">(Sorgente et al., 2013)</ref>  <ref type="bibr" target="#b12">(Hendrickx et al., 2009)</ref> For example, from in "Profits from the sale were given to charity" implies causation of profits due to the sale, while from in "Sales profits increased from 1.2% to 2%" does not have any causality involved in it. Analysis of such complex constructs are yet to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>The overall architecture of our proposed approach is composed of three modules: a)Resource Creation b) Linguistic preprocessor and feature extractor, c) Classification model builder, and d) Prediction framework for cause/effect, built on the output of the classifier module. Each of the individual modules are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Resource Creation</head><p>Data Description: In this section we will discuss about the following dataset used to develop and test our proposed models. 1) Part of the SemEval 2010 Task 8 data set dealing with"Cause-Effect"  <ref type="bibr">2004</ref><ref type="bibr">-2005</ref><ref type="bibr" target="#b9">(Greene and Cunningham, 2006)</ref>. We have considered 140 business news articles, containing approximately 1950 sentences. Out of this, around 500 sentences were found to contain causation. 4)Around 4500 analyst reports of a specific organization over a period of seven months is the fourth dataset that we have considered. We have manually extracted all the sentences that contained causation. 5) The Recall dataset<ref type="foot" target="#foot_0">foot_0</ref> is a collection of 1050 recall news of different products.</p><p>The first two datasets, that is, SemEval and ADE datasets, are already publicly available. However, for the SemEval dataset we have extended the annotation to phrase-level causal relationships. Hence the fresh annotations of these existing data sets, as well as parts of the annotated Recall news and BBC news datasets, will be publicly shared with this paper. We could not share the analyst report dataset due to copyright and IPR issues.</p><p>Preprocessing: We perform a number of preprocessing over the collected dataset. The first stage of preprocessing involves identifying which sentences are probably candidates for cause-effect identification out of a body of text. This involves looking for the presence of at least one causal connective in the sentence under consideration. Xuelan (Xuelan and Kennedy, 1992) reported a list of 130 causal connectives in English. To extend the list we follow methods similar to Girju <ref type="bibr" target="#b5">(Girju, 2003)</ref> and Blanco <ref type="bibr" target="#b0">(Blanco et al., 2008)</ref>. We use Wordnet (University, 2010) as our lexical database. An entry of WordNet, whose gloss definition contains any of the terms in the exist- ing causal list, is included in the list as a possible causal connectives. Once we have a list of words, we further expand the list by adding common phrases with contain one or more of these words. For example, the seed word causes is extended to include phrases like "one of the main causes of", "a leading cause of" etc. This gives us an extended connective list of 310 words/phrases. Table <ref type="table" target="#tab_4">3</ref> shows a few examples of seed words and new terms added to the list. After preprocessing, we finally obtained a dataset of 8K sentences for annotation in terms of their cause, effect and causal connectives.</p><p>The Annotation Process: The above sentences are presented to three expert annotators. The experts were asked to complete the following two tasks. a) Identify whether a given sentence contains a causal event (either cause/effect) and b) Annotate each word in a sentence in terms of the four labels cause (C), effect(E), causal connectives(CC) and None. An illustration of the annotated dataset is depicted in Table <ref type="table" target="#tab_2">2</ref>.</p><p>In some of the candidate sentences, it is observed that a single sentence contains multiple cause-effect pairs, some of which are even chained together. In order to handle multiple instances of causality present in the same sentence, sentences are split into sub-sentences. e.g. "In developing countries four-fifths of all the illnesses are caused by water-borne diseases with diarrhoea being the leading cause of childhood death" <ref type="bibr" target="#b12">(Hendrickx et al., 2009)</ref>. This sentence has two distinct causes and their corresponding effects : four-fifths of all the illnesses are caused by water-borne diseases and diarrhoea being the leading cause of childhood death.</p><p>We have also observed a number of cases where a single sentence contains a chain of causal events where a cause event e 1 results the effect of another event e 2 which in turn causes event e 3 . In such cases e 2 will be marked as both effect for e 1 and cause for e 3 . For example, in "The reactor meltdown caused a chain reaction that destroyed all the towers in the network" <ref type="bibr" target="#b12">(Hendrickx et al., 2009)</ref>, there are two different causalities, chained  together: (1)The reactor meltdown caused a chain reaction and (2)a chain reaction that destroyed all the towers in the network. The effect in the first case and the cause in the second is "A chained reaction". Similar example illustrated with an annotation is depicted in example (2) of Table <ref type="table" target="#tab_2">2</ref>. In order to extract all instances of causality present in a sentence, the sentence is divided into subsentences. We use openIE <ref type="bibr" target="#b24">(Schmitz et al., 2012)</ref> to extract multiple relationships from the sentence, and then treat each relationship as a separate sentence.</p><p>Based on the given annotation scheme, each of the annotator received around 2500 sentences. Out of these, 2000 sentences are unique and rest 500 are overlapping. Using these 500 common sentences, we measure the inter annotator agreement of the annotation using the Fleiss Kappa <ref type="bibr" target="#b3">(Fleiss and Paik, 1981)</ref> measure (κ). This is computed as κ = P -Pe 1-Pe . The factor 1 -Pe gives the degree of agreement that is attainable above chance, and P -Pe gives the degree of agreement actually achieved above chance. We have achieved the inter annotator agreement to be around 0.63. This implies that the expert annotated dataset is reliable to be used for further processing. Some more examples of annotated sentences are elaborated in the appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The linguistically informed Bi-directional LSTM model</head><p>There is a recent surge of interest in deep neural network based models that are based on continuous-space representation of the input and non-linear functions. Thus, such models are capable of modeling complex patterns in data and since they do not depend on manual engineering of features, they can be applied to solve problems in an end-to-end fashion. On the other hand, such neural network models fails to consider the latent linguistic characteristics of a text that can play an important role in extraction of the relevant information. Therefore, we have proposed a deep neural network model based on the bidirectional long-short term memory (LSTM) model <ref type="bibr" target="#b13">(Hochreiter and</ref><ref type="bibr">Schmidhuber, 1997) (Schmidhuber et al., 2006)</ref> that along with the word embeddings, utilizes different linguistic features within a text for the automatic classification of cause-effect relations.</p><p>In identification of causal relationships from text, the surrounding context is of paramount information. While typical LSTMs allow the preceding elements to be considered as context for an element under scrutiny, we prefer to use bidirectional LSTMs (Bi-LSTM) networks <ref type="bibr" target="#b8">(Graves et al., 2012)</ref> that are connected so that both future and past sequence context can be examined, i.e. both preceding and succeeding elements can be considered.</p><p>The overview of the proposed model is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Corresponding to each input text, we determine the word embedding representation of each words of the text and the different linguistic feature embeddings. The input to the Bi-LSTM unit is an embedding vector (E)which is the composition of the word embedding representation (W e ) and the linguistic feature embeddings (W l ). This is represented as</p><formula xml:id="formula_0">- → E = -→ W e -→</formula><p>W l Generating Word Embeddings: Pre-trained GloVe word vector representations of dimension 300 have been used for this work <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>. GloVe is a relatively recent method of obtaining vector representations of words and has been proven to be effective. Along with the GloVe vector, the embedding vector of each word is appended with the vector formed from the linguistic features that has been described in the earlier section.</p><p>Generating linguistic feature embeddings: Apart from the presence of causal connectives mentioned earlier, other features added to make our model linguistically informed are relevant lexical and syntactic features : Part of Speech(POS) tags <ref type="bibr" target="#b19">(Manning et al., 2014)</ref>, Universal Dependency relations <ref type="bibr" target="#b2">(De Marneffe et al., 2006)</ref> and position in Verb/ Noun/ Prepositional Phrase structure. We have also used the semantic features as identified by Girju <ref type="bibr" target="#b5">(Girju, 2003)</ref> -the nine Noun hierarchies (H(1) to H( <ref type="formula">9</ref>)) in WordNet namely, entity, psychological feature, abstraction, state, event, act, group, possession, and phenomenon. First, a single feature Primary Causal Class (PCC) is defined for a word w i . If w i ∈ H i where H i is any of the nine WordNet hierarchies, P CC = H i , else P CC = null. Another feature, Secondary Causal Class(SCC) is also defined. This takes value H(i) if any WordNet synonym of the word belongs to H(i), and is N ull otherwise. Further, we consider the dependency structure of the sentence, which gives us that w i is dependent on word p i . In addition to the five features described above for w i , we also consider the same five features of p i as part of w i s feature set. If w i is not dependent on any other word in the sentence, then the parent features are the same as the word features. An example of the linguistic feature selection can be found in appendix A.</p><p>Network Architecture: We use a k-layer Bi-RNN, composed of k Bi-RNNs stacked, where the output of each such unit is the input to the next unit <ref type="bibr" target="#b14">(Irsoy and Cardie, 2014)</ref>. A two-layer stack of Bi-LSTMs is employed for the purpose of experiments. The model is trained with Adam optimizer <ref type="bibr" target="#b18">(Kingma and Ba, 2014</ref>) and dropout layer with the dropout value of 0.5 for each Bi-RNN. The dropout layer reduces the problem of overfitting often seen in trained models by dropping unit with connections to the neural network at random during the training process <ref type="bibr" target="#b26">(Srivastava et al., 2014)</ref>. The model is fit over runs of 2000 epochs, with batch size of 128. The loss is calculated as a function of the mean cross entropy generated. Each Bi-LSTM has 256 hidden layers and 1 final dense layer with softmax activation as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal Embeddings for Representing Similar Events</head><p>We have applied the proposed causal extraction technique over a large set of data from four different domains namely, Analyst Reports, Adverse Drug Effects, Business News and Product Recall News. We observe that a number of extracted causal events shows high degree of semantic similarity. For example, "Engine breakdown" and "Engine failure" represents the same semantic sense. Therefore, we intend to group these events into clusters. Accordingly, we device a novel algorithm to determine similar causal events. The algorithm follows the following steps: a) first identify the word embeddings of each constituent word of a causal event. The word embeddings are identified using the standard GloVe representations <ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>. Apart from the word embeddings, we have also created phrase embeddings by computing a tensor product between the individual word embeddings. For example, given two causal events C 1 = w 1 , w 2 ..., w i and C 2 = w 1 , w 2 , ...w j , where w 1 , w 2 , ...w k and w 1 , w 2 ...w k are the constituent word embeddings of the causal events C 1 , and C 2 such that i = j, the phrase embedding P ( w 1 , w 2 ) is created by computing the tensor product of each adjacent word embedding pairs. This is represented as P (w 1 , w 2 ) = w 1 w 2 . Similar word and phrase embeddings are constructed for causal event C 2 . Consequently, we define A and B as the number of word embeddings in C 1 andC 2 respectively. Similarly, A and B are the number of phrase embeddings in C 1 and C 2 respectively. Therefore, the similarity</p><formula xml:id="formula_1">S(C 1 , C 2 ) = (S + S ) N 1 + N 2</formula><p>The expressions N 1 and N 2 implies A∪B and A ∪ B respectively. S and S are computed as: S = ∀w i ∈C 1 S w i and S = ∀p i ∈C 1 S p i Where,</p><formula xml:id="formula_2">S w i = max ∀w j ∈C 2</formula><p>(Sim(w i , w j ))</p><formula xml:id="formula_3">S p i = max ∀p j ∈C 2 (Sim(p i , p j ))</formula><p>Again, p and p are the individual phrase embeddings in sentence C 1 and C 2 respectively. Sim(x, y) is the cosine similarity between the two word vector w x and w y . Based on the similarity score, we perform a k-means clustering to form clusters of similar causal events. We have used the Average silhouette method to identify number of clusters k. For the present work we obtained the value of k as 21. A partial network of a few representative clusters, as obtained from the vehicle Recall database, is shown in Figure <ref type="figure" target="#fig_1">2</ref>. For each cluster, the size is given as number of phrases that constitute the cluster, and a few representative phrases of each cluster is also shown as reference. The name of the cluster is chosen from the most common noun chunks present in the cluster.</p><p>The network itself is shown as a directed graph, with edges directed from Cause to Effect, as edge weights being computed as the fraction of total occurrences of the cause that lead to the effect. Following the method each cluster can be further represented by a verb-noun pair as proposed in <ref type="bibr" target="#b28">(Zhao et al., 2017)</ref>. For noisy clusters where no such generalization is possible are left out for the time being.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We perform a number of different experiments to evaluate and compare the performance of our proposed system with the baseline systems. In general we classify the experiments into three different groups. Each group uses different techniques to identify causality in text. Group-1 uses rule based method, group-2 uses a CRF based classification model, group-3 uses Bi-LSTM model and group-4 uses our proposed linguistically informed Bi-LSTM model. The outputs of the experiments are evaluated in terms of the five given datasets that are explained earlier. Again, corresponding to each group, we define three different evaluation tasks. The tasks are distinguished in terms of the way each datasets are divided for training, development and testing purposes.</p><p>In Task-I, we took the five datasets separately and each dataset is divided into 80%, 10% and 10% for training, testing and development respectively. The F1 scores obtained by each system on the datasets by this model are reported in Table <ref type="table">4</ref> for identified Cause, Effect and Causal Connec-  tives.</p><p>In Task-II, we combine all the five datasets together and divide the training set, development set and test sets into 80%, 10% and 10% respectively. The division in dataset follows a five-fold manner. Therefore, the 10% testing data in fold-1 is different from the 10% testing data in fold-2 or fold-3. We compute the individual results and report the average of them.</p><p>Finally, in Task-III, we train the model using one dataset and test it to other four models. We conducted the experiments using the designated training portions of each dataset of BBC news, Recall News, Analyst Reports and SemEval individually to train the model and then tested all the sets on each resultant model. Of these, the best results were seen to be from the model trained on the BBC dataset.</p><p>From Table <ref type="table">4</ref> we observe that in most of the The presence of descriptive clause along with valid cause/effect phrases made it difficult for the system to correctly identify and localize the valid phrases. In fact, the system suffered when working with such sentences, even when there was just a single instances of causality present. In the SemEval dataset, openIE usage led to identification of multiple causality in around 1/4th of the cases where multiple causality was indeed present. However, in the BBC News dataset, this amount was barely 8% of all the sentences that contained multiple instances of causation. On an average, around 7% cases the system incorrectly predicted a cause/effect relation as valid which is actually not, whereas only 4% of the sentences were incorrectly identified as "Not an cause/effect" despite being marked as "cause/effect" by the experts. The primary reason behind this is due to fact that most of the collected texts are noisy, as a result of which the dependency parser fails to parse the texts properly and thus returning incorrect linguistic feature values. For ADE dataset, we observed that a large number of descriptions are written in languages other than English, as a result of which the classifier failed to predict correctly. Another source of error is the occurrence of incomplete sentences that restricts the classification engine to correctly label the descriptions. Apart from labeling the cause and effect events, the proposed classifier also aims to label the explicit causal connectives. Table <ref type="table">4</ref> reports the results of the connective classification. We have observed that the proposed classification model is able to identify novel causal connectives that were previously not enlisted in the original causal connective list. We previously mentioned that existing schemes of having a single word represent cause and effect leads to a loss of information. Just in the SemEval dataset, just 33% of the total corpus is such that their given single-word annotation effectively captures all the information about the causal event present in the sentence. Using our proposed methodology and extending the scheme to phrases give us the complete causal information in almost 60% of the sentences that were only partially covered previously. However, we are able to somewhat quantify this observation only for the SemEval dataset, since the other datasets do not have a single-word gold standard annotation. As discussed in section 2, ambiguous causatives are a big contributor to causality being identified when it is not actually present in the sentence. Examples of some common ambiguous causal connectives, as well some of the novel connectives identified by the system (which were not present in our original list), are given in Appendix A. In addition to the above results, Figures <ref type="figure">3, 4</ref> and<ref type="figure">5</ref> show the relative performances of models trained with the individual datasets and then tested on all the test sets (Task-III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a linguistically informed deep neural network architecture for the automatic extraction of cause-effect relations from text documents. Our proposed architecture uses word level embeddings and other linguistic features to detect causal events and their effects. We evaluate the performance of the proposed model with respect to a rule based classifier and a conditional random field (CRF) based supervised classifier. We find that the bi-directional LSTM model along with an additional linguistic layer performs much better than existing baseline systems. Along with the extraction task another important contribution of this work is the development of new dataset annotated in terms of the cause-effect relations, which will be publicly shared with this paper for further research in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>We use this section to elaborate on certain aspects of our work with the help of some more examples.</p><p>Table <ref type="table">6</ref> shows the list of linguistic features constructed for each word of an example sentence. W1-W6 are similarly features of the original word, which are, in order,part of speech tag, universal dependency tag, parent word id, phrase structure, primary causal class and secondary causal class. Feature P is the parent word, and P1-P6 are the features of the parent word, similar to those described as W1-W6. Finally, the last column is the label associated with the word. C implies Cause, CN implies Causal Connective, E implies Effect, and N implies None.</p><p>Table <ref type="table">7</ref> shows some more typical cases of causal sentences encountered and their respective annotations. As explained, the four annotation labels are cause (C), effect(E), causal connectives(CC) and None(N). The second sentence contains an example of a phrase irrelevant to the actual causality that is present in the target sentence. In the current work, preciseness of the solution is dependent on it correctly disregarding the irrelevant portion and identifying causality only in the rest of the sentence. The third sentence, on the other hand, shows an example of one of the more challenging scenarios of causality identification, i.e. in the absence of any explicit causal connective. While the causality in the given sentence looks obvious to an observer, the challenge lies in the fact that there are possible grammatically and structurally similar sentences that do not contain causality.</p><p>Table <ref type="table">8</ref> shows some common ambiguous causal connectives that identify sentences as causal even in the cases where they are not being used to identify causality. To further emphasize on their ambiguity, we show, in parallel, examples where the same connectives imply causality. He suffers from seizures stemming from a childhood injury punishment for They claim the downfall was punishment for the political ambitions of their leader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>having</head><p>Having dealt with their internal problems, the two companies were ripe for consolidation.</p><p>Table <ref type="table" target="#tab_7">5</ref> depicts a sample set of novel causal connectives identified by our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the bidirectional LSTM architecture for Cause-Effect relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A projection of the network of cause-effect clusters</figDesc><graphic coords="7,72.00,62.81,204.09,132.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2</head><label></label><figDesc>Challenges in Causality Detection and the State of the Art Identification of causality is not a trivial problem. Causation can occur in various forms. Two common differentiations are made on: a) Marked and Unmarked causality and b) Implicit and Explicit causality</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data Statistics</figDesc><table><row><cell>Source</cell><cell cols="2">Sentence countAvg. sent. length</cell></row><row><cell>Analyst Report (AR)</cell><cell>4500</cell><cell>23.7</cell></row><row><cell>SEMEVAL (SEM)</cell><cell>1331</cell><cell>18.7</cell></row><row><cell>BBC News(BBC)</cell><cell>503</cell><cell>22.5</cell></row><row><cell>ADE</cell><cell>3000</cell><cell>20.5</cell></row><row><cell>Recall News (RN)</cell><cell>1052</cell><cell>23.1</cell></row><row><cell cols="3">relation, which consists of 1331 sentences. 2) The</cell></row><row><cell cols="3">adverse drug effect (ADE) dataset (Gurulingappa</cell></row><row><cell cols="3">et al., 2012) composed of 1000 sentences consist-</cell></row><row><cell cols="3">ing of information about consumption of differ-</cell></row><row><cell cols="3">ent drugs and their associated side effects. 3)The</cell></row><row><cell cols="3">BBC News Article dataset, created by the Trin-</cell></row><row><cell cols="3">ity College Computer Science Department, con-</cell></row><row><cell cols="3">taining news articles in five topical areas : busi-</cell></row><row><cell cols="3">ness, sports, tech, entertainment and politics from</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Annotation Examples Honda/E1 Motor/E1 Co./E1 is/E1 recalling/E1 Acura/E1 ILX/E1 and/E1 ILX/E1 Hybrid/E1 vehicles/E1 because/CC1 excessive/C1 headlight/C1 temperatures/C1 pose/C1 a/C1 fire/C1 risk/C1.</figDesc><table><row><cell>Attrition/C1 of/C1 associates/C1 will/CC1 effect/CC1 scheduled/E1/C2 release/E1/C2 of/E1/C2 product/E1/C2</cell></row><row><cell>causing/CC2 high/E2 business/E2 impact/E2.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Examples of seed and learnt terms from WordNet for lexical patterns</figDesc><table><row><cell>Seed</cell><cell cols="2">New Term Wordnet Gloss of Term</cell><cell>Example</cell></row><row><cell>due to</cell><cell cols="2">corrode break down collapse due to agent cause to deteriorate due to agent</cell><cell>The acid corroded the metal. Stomach juices break down proteins.</cell></row><row><cell>cause to</cell><cell>choke confuse</cell><cell cols="2">become or cause to become obstructed He choked on a fishbone. cause to be unable to think clearly The sudden onslaught confused the enemy.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Examples of some unusual learnt connectives account for Direct payments by the patient account for a large proportion of funding derive from The name of Portugal derives from the Romano-Celtic name Portus Cale dictate by A spin label's motions aredictated by its local environment based on the fact His conclusion is based on the fact the objects contain more than 1% Arsenic on account of The amount covers expenses on account of his staff and transportation stem from</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.edmunds.com/recalls/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The companys sales rose to $18.6bn from last year's $12.3bn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>followed by</head><p>The tornado caused destruction followed by widespread disease.</p><p>The leader was followed by his supporters in the march.</p><p>since</p><p>The company has cut jobs since demands were low.</p><p>The company has cut 5% jobs since September 2002.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal relation extraction</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Castell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting causal relations on hiv drug resistance from literature</title>
		<author>
			<persName><forename type="first">Quoc-Chinh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Breanndán Ó Nualláin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Ma</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><surname>Sloot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Levin</forename><forename type="middle">B</forename><surname>Fleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Paik</surname></persName>
		</author>
		<title level="m">The measurement of interrater agreement. Statistical methods for rates and proportions</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="212" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Coatis, an nlp system to locate expressions of actions connected by causality links. Knowledge acquisition, modeling and management</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic detection of causal relations for question answering</title>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering</title>
		<meeting>the ACL 2003 workshop on Multilingual summarization and question answering</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text mining for causal relations</title>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="360" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 04: Classification of semantic relations between nominals</title>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pádraig</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Domain modeling for language analysis</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting explicit and implicit causal relations from sparse, domain-specific texts</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Ittoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Application of Natural Language to Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing</title>
		<author>
			<persName><forename type="first">Jaklin</forename><surname>Christopher Sg Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">N</forename><surname>Kornfilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><surname>Oddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myaeng</forename><surname>Hyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using cause-effect relations in text to improve information retrieval precision</title>
		<author>
			<persName><forename type="first">Sung</forename><surname>Christopher Sg Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">N</forename><surname>Hyon Myaeng</surname></persName>
		</author>
		<author>
			<persName><surname>Oddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing &amp; management</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="145" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bauer</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finkel</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethard</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict from textual data</title>
		<author>
			<persName><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Davidovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="641" to="684" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Catching the common cause: extraction and annotation of causal relations and their participants</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Linguistic Annotation Workshop</title>
		<meeting>the 11th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning nonregular languages: A comparison of simple recurrent networks and lstm</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open language learning for information extraction</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic extraction of cause-effect relations in natural language text</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Sorgente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Vettigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Mele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DART@ AI* IA</title>
		<imprint>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wordnet. About WordNet. Fang Xuelan and Graeme Kennedy. 1992. Expressing causation in written english</title>
	</analytic>
	<monogr>
		<title level="j">RELC Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constructing and embedding abstract event causality networks from text snippets</title>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
