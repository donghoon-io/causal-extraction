<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Human-Centered Approach for Bootstrapping Causal Graph Creation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-03">3 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minh</forename><forename type="middle">Q</forename><surname>Tram</surname></persName>
							<email>minh.tram@mavs.uta.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Arlington Arlington</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nolan</forename><forename type="middle">B</forename><surname>Gutierrez</surname></persName>
							<email>nolan.gutierrez@mavs.uta.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Arlington Arlington</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Beksi</surname></persName>
							<email>william.beksi@uta.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Texas at Arlington Arlington</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Human-Centered Approach for Bootstrapping Causal Graph Creation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-03">3 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.01622v1[cs.RO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-Robot Interaction</term>
					<term>Augmented Reality</term>
					<term>Causal Graphs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal inference, a cornerstone in disciplines such as economics, genomics, and medicine, is increasingly being recognized as fundamental to advancing the field of robotics. In particular, the ability to reason about cause and effect from observational data is crucial for robust generalization in robotic systems. However, the construction of a causal graphical model, a mechanism for representing causal relations, presents an immense challenge. Currently, a nuanced grasp of causal inference, coupled with an understanding of causal relationships, must be manually programmed into a causal graphical model. To address this difficulty, we present initial results towards a human-centered augmented reality framework for creating causal graphical models. Concretely, our system bootstraps the causal discovery process by involving humans in selecting variables, establishing relationships, performing interventions, generating counterfactual explanations, and evaluating the resulting causal graph at every step. We highlight the potential of our framework via a physical robot manipulator on a pick-and-place task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causality allows for deciphering complex relationships and drawing informed conclusions. This foundational understanding is beginning to make significant contributions in the field of robotics, reshaping the path of its advancement <ref type="bibr" target="#b9">[10]</ref>. However, constructing accurate causal models, including causal inference and graph representations, is a daunting task. This stems from the inherent intricacy of the interactions, often compounded by a myriad of variables, which often remain elusive or hidden in the environment.</p><p>A causal graphical model, aka causal graph, serves as a visual mathematical representation of the relationships between variables Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. HRI '24 Causal-HRI Workshop, March 2024, Boulder, CO. Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. in a system. Delineating the direction and structure of these relationships allows for identifying potential causes and effects, and disentangling confounding variables <ref type="bibr" target="#b13">[14]</ref>. By employing causal graphs, robots can predict the outcomes of their actions more accurately, make informed decisions in dynamic environments, and adapt more fluidly to new situations with a deeper comprehension of the underlying mechanisms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Parallel to these advancements, the use of simulations and virtual interfaces offers a more economical means of gathering diverse and valuable data, which is especially conducive to improving machine learning algorithms <ref type="bibr" target="#b10">[11]</ref>. For example, the integration of virtual, augmented, and mixed reality (VAMR) has shown promising results in fostering a more intuitive and enhanced interaction between human operators and robots (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>). VAMR interfaces provide a way for operators to have a more immersive control experience, bridging the gap between the digital and physical realms.</p><p>In this work we present preliminary results on a new framework that leverages VAMR technologies to address the challenge of causal graph construction, Fig. <ref type="figure" target="#fig_0">1</ref>. To summarize, our contributions are the following.</p><p>â€¢ We create an augmented reality (AR) interface that allows an operator to naturally construct a causal graph on the fly. â€¢ We provide a human-centered approach to highlight critical information when establishing a causal graph of a scene. â€¢ Our system ensures that a robot prioritizes human insights during the initial stages of scene comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V B T</head><p>Fig. <ref type="figure">2:</ref> A causal graph representing the relationship between a robot's battery level ğµ, the terrain roughness ğ‘‡ , and the robot's velocity ğ‘‰ during navigation. The solid edge represents a well-established cause and effect, while the dashed edge represents an indirect or latent confounding variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION 2.1 Causal Graphs and Causal Inference</head><p>To establish a foundation for causal inference, Pearl introduced a number of mathematical formulations, notably structural causal models (SCMs), intervention and do-calculus, and counterfactuals, to rigorously define the relationships between variables. For example, consider a mobile robot navigation scenario. The causal graph that represents the relationship between the robot's velocity, the battery level, and the roughness or difficulty of the terrain can be represented as a directed acyclic graph (DAG), Fig. <ref type="figure">2</ref>. In this scenario, we can represent the relationships between the variables using an SCM ğ‘“ as</p><formula xml:id="formula_0">ğ‘‰ = ğ‘“ (ğµ,ğ‘‡ , ğ‘ˆ ğ‘‰ ),<label>(1)</label></formula><p>where ğ‘‰ is the velocity, ğµ is the battery level, ğ‘‡ is the terrain roughness, and ğ‘ˆ ğ‘‰ is an internal variable that is unobserved, but has an effect on the velocity of the robot. An intervention in this situation would be to charge the robot's battery regardless of any other factor hence ensuring it is always full. This can be represented using do-calculus, i.e.,</p><formula xml:id="formula_1">ğ‘ƒ (ğ‘‰ | ğ‘‘ğ‘œ (ğµ = full)) = âˆ‘ï¸ ğ‘‡ ğ‘ƒ (ğ‘‰ | ğµ = full,ğ‘‡ = ğ‘¡)ğ‘ƒ (ğ‘‡ = ğ‘¡).<label>(2)</label></formula><p>In <ref type="bibr" target="#b1">(2)</ref>, estimates of the speed of the robot are given through the intervention by averaging over all possible values of ğ‘‡ . Assuming that there are no confounding relationships between the variables, we have effectively broken the natural causal relationship between ğµ and ğ‘‰ , while leaving the causal relationship between ğ‘‡ and ğ‘‰ intact. This is where counterfactuals can help infer the causal relationship between ğµ and ğ‘‰ during an intervention, which can aid in reinforcing or disproving causal relationships between variables. This is important for both pre-planning, on-the-fly decision making, and insights toward post-incident analysis. By using this mathematical framework to understand the causal relationships between variables, we can make informed predictions about a system. For instance, in the robot navigation scenario we can use these relationships to pinpoint the root cause of performance issues, pre-charge the robot's battery if it is expected to go through rough terrain that will drain the battery, or adjust the robot's behavior to achieve a desired outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation</head><p>The use of causal inference is not new in robotics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Nonetheless, automating the creation of a viable causal graph remains an underexplored area of research. Constructing a causal graph is hard due to the following reasons. Fig. <ref type="figure">3</ref>: The visualization and interaction pipeline of our proposed framework. The operator can interact with the robot using an overlay interface via various modes and provide context hinting directly onto the workspace to aid the robot in its understanding of the scene.</p><p>â€¢ Complexity of the environment: Variables and their causal relationships must be identified in dynamic environments while accounting for confounding variables. â€¢ Causation vs. correlation: The distinction between causation and correlation from observational data is not always clear, and definitive proof of causation often requires more data.</p><p>â€¢ High dimensionality: The number of variables in an environment can be large, which can lead to complex and/or multiple causal graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HUMAN-CENTERED CAUSAL GRAPHICAL MODELS</head><p>In this section, we delineate the specific methodologies and procedures employed to actualize human-centered causal graphs. First, we present a brief overview of our framework's components. Then, we provide a detailed description of each component and the communication pipeline between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Component Overview and Consideration</head><p>Our framework to facilitate robotic understanding through humancentered causal graph construction is composed of three primary components: (i) an AR-capable headset, (ii) a software interface to visualize the robot's perception and allow the operator to interact with the robot, and (iii) a control interface to realize the operator's commands and interventions. Each of these components plays a pivotal role in the seamless execution of the system, providing both unique and complementary functionalities. Fig. <ref type="figure">3</ref> details the overall system architecture and the transmission pipeline between the components. For the AR headset, we opted for the Microsoft HoloLens 2 due to its unparalleled capabilities. Its commercial availability ensures ease of access and its extensive API support, via Microsoft's Mixed Reality Toolkit (MRTK) <ref type="bibr" target="#b0">[1]</ref>, offers a versatile platform for development and integration. The HoloLens 2 is a self-contained head-mounted display (HMD) that is capable of spatial mapping and tracking, while also providing a projected holographic display. It permits the operator to operate in a hands-free, untethered manner. This feature is crucial for our work for two primary reasons. First, being able to superimpose additional information directly onto the workspace enhances the operator's understanding of the robot's perception of the environment. This is vital when visualizing the implications of the causal graph on the robot's physical actions in real-time. Second, the ability to operate in a hands-free manner through different modalities (i.e., direct-hand input, eye-tracking, and voice commands) allows the operator to swiftly and naturally interact with both the robot's actions and the underlying causal structure. This ensures that the operator can manipulate the causal graph, adjust parameters, and interact with robot perception and environment understanding without the constraints of traditional input methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interaction and Visualization Modalities</head><p>Relying on the HoloLens 2's hand-tracking capabilities, our system allows the operator to interact with the robot and environment through a number of mechanisms. This includes direct manipulation of the robot's joint positions via virtual sliders and command buttons, providing a planning context such as desired end-effector positions and approach vectors, and the ability to query forward and inverse kinematics solutions. Furthermore, compared to traditional VR devices where operator interactions are bound to handheld controllers and the operator's vision is obstructed by a display, the HoloLens 2 offers greater interaction and visualization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal Graph Construction and Intervention</head><p>In our human-centered framework, the focus is primarily on the dynamic construction and intervention of causal graphs. To this end, we leveraged Unity <ref type="bibr" target="#b2">[3]</ref> to render causal structures as 3D objects in the AR space allowing the operator to interact with the causal graph directly. These graphical constructs, composed of nodes and edges, offer a visual guide that aids the operator in deciphering the complex interplay between robot actions and environmental cues. Nodes, visualized as tangible entities within the AR landscape, symbolize specific events or actions that the robot can detect or perform. Their design incorporates distinct visual features, such as shades, opacities, and pulsations, communicating their present state or relevance. On the other hand, edges provide visual insights into the direction, strength, or likelihood of causal linkages. These are often characterized by variations in their thickness, texture, or color gradient, allowing users to quickly comprehend causal dependencies.</p><p>Within the immersive world facilitated by the HMD, operators can actively modify and intervene in the causal relationships by adding, removing, or modifying the nodes and edges of the current causal structure. The device's advanced hand-tracking feature supports intuitive gestures, such as pinching, swiping, or rotating, enabling users to adjust causality strength, node prominence, or even the direction of a causal link. Furthermore, the mobility of the HMD lets an operator freely move around the environment and provide context hints directly onto the robot workspace, accommodating seamless adjustments and experiments with the causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Physical Robot Actuation and Simulation</head><p>The physical robot actuation and simulation components of our system, driven by ROS 2 Humble <ref type="bibr" target="#b1">[2]</ref>, facilitate communication between the robot and Unity software layers. This allows the operator to interact with the robot and environment through the AR interface. Concretely, it enables the translation of intricate directives from a causal graph into precise physical movements of the robot, ensuring adaptability and dexterity in diverse real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION 4.1 Causal Graph Creation</head><p>Using a UR5e robot arm provisioned with a Robotiq 2F-85 gripper and tasked with picking up a finite set of known objects and placing them at a desired location, we construct a causal graph to represent the relationship between the robot's actions and the environment. To achieve this, the potential variables related to our task are represented as follows.</p><p>(1) ğ‘…ğ‘œğ‘ğ‘œğ‘¡: The robot state.</p><p>(2) ğºğ‘Ÿğ‘–ğ‘ğ‘ğ‘’ğ‘Ÿ : The gripper state.</p><p>(3) ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡: The robot's motion planning to achieve the goal position. (4) ğºğ‘œğ‘ğ‘™: The desired object placement location.</p><p>(5) ğ‘‡ğ‘¦ğ‘ğ‘’: The object's type. ( <ref type="formula">6</ref>) ğ‘Š ğ‘’ğ‘–ğ‘”â„ğ‘¡: The object's weight (light -heavy). ( <ref type="formula">7</ref>) ğ‘†ğ‘–ğ‘§ğ‘’: The object's size (small -large). ( <ref type="formula">8</ref>) ğ‘‡ ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘Ÿğ‘’: The object's texture (smooth -rough). ( <ref type="formula">9</ref>) ğ‘…ğ‘–ğ‘”ğ‘–ğ‘‘ğ‘–ğ‘¡ğ‘¦: The object's rigidness (soft -hard). ( <ref type="formula">10</ref>) ğ‘†ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ : The success rate of a pick-and-place sequence. For the purpose of demonstration, we construct this causal graph under the assumption that external factors (e.g., robot power loss, environmental disruptions, etc.) do not occur and are insulated from the causal representation. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the relationships between the variables can be visually represented as a graph for an easier understanding of potential causality and the interplay between nodes. Within the graph, each edge represents a direct causal relationship between the robot and the operating environment. For example, if the weight of the object changes then it could influence the behavior of the gripper (e.g., the gripper force or grasp vector).</p><p>Our framework can directly create the causal graph in the AR space, Fig. <ref type="figure" target="#fig_4">5</ref>. The operator interacts via the AR interface to label the nodes and edges, set the content of each node, and adjust the strength of each edge. The operator can also communicate with the robot and the environment through the AR interface to obtain additional information and context to aid in the construction of the causal graph. The causal graph can then be parsed and interpreted by the motion planning backend during the planning step to ensure that the robot's actions are consistent with the state of the graph. Algorithm 1 demonstrates how the robot consumes a causal graph generated by the operator during each motion planning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Graph Intervention</head><p>For a pick-and-place task, we use our constructed causal graph to evaluate whether the relationships between the defined variables are valid, which in turn can be inferred by the robot's performance. For instance, if the robot consistently fails to pick up a cordless drill if operator is modifying causal graph then Execute with guidance from causal graph 6:</p><p>Check execution status end if 14: end while but it has no issues with other objects, then we can be relatively certain that several known properties or relationships (e.g., weight, texture, etc.) may be detrimentally affecting the robot. With this human-in-the-loop approach, we can quickly devise an intervention procedure to decide if certain variables are causing degraded performance. For example, we can intervene by setting the object's weight to a specific value and then benchmark the robot's performance under the assumption that the robot did not account for the object's weight properly.</p><p>If the robot's performance is influenced by the weight of the object, then we can make adjustments to the robot's behavior to ensure optimal performance (e.g., adjusting the pick points, etc.). Alternatively, should the intervention be inconclusive, then we can intervene by fixing other object properties to a specific value and then benchmark the robot's performance. However, further investigation may be required to understand the exact reason. For example, the gripper might not be suited for the cordless drill's shape, or the perception system may not be identifying the drill's correct pick points. Since our framework has the capability to adjust causal graph structure on the fly, we can quickly update the graph to reflect the new structure thus allowing the robot to make more informed decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Our framework provides advantages over traditional methods for constructing causal graphs. The AR interface allows an operator to interact with the robot and the environment directly and more intuitively, making it easier for humans to understand the system's dynamics. It also presents a visual understanding of causal relationships, which allows operators to make informed decisions on how to optimize the system. For example, the operator can visualize in real time where interventions might be needed, or where potential failures could occur.</p><p>Constructing causal graphs this way introduces a level of modularity and scalability. For instance, causal elements can be easily adjusted to fit the needs of the system or the entire graph can be replaced by constructing a replacement from scratch. Yet, this methodology is not without its limitations. A criticism of causal graphs is that they can be an oversimplification of the environment and interplay between variables. Real-world robotic operations may involve numerous subtle interactions, not all of which can be accurately captured in a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper provided preliminary results on a human-centered approach for automating the construction of causal graphical models. Our framework combines the strengths of VAMR and simulation technologies to address the dilemma of bootstrapping the creation of a causal graph. Furthermore, we highlighted the ability to visualize, intervene, and update causal graphs on a physical robot for a real-world pick-and-place task. In the future we will work on addressing the challenges of oversimplification and iteratively updating causal graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The point of view of the operator from the AR overlaid physical workspace. The operator can interact with the robot and provide additional context to aid the robot's understanding via on-the-fly construction of causal graphs.</figDesc><graphic coords="1,317.96,205.09,240.24,135.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: An example of the relationships between the operatoridentified variables for a pick-and-place task. Each edge corresponds to a direct causal relationship between the connected variables. Changes in the source of an edge have a direct consequence on the target of an edge. Algorithm 1 Human-in-the-loop Causal Graph Iteration Require: Human has initialized the causal graph Ensure: Causal graph is DAG, all nodes have properties 1: while true do 2:</figDesc><graphic coords="4,317.96,86.11,240.23,175.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The operator interacting with the robot and environment through the AR interface to construct a causal graph. Nodes and edges are visualized and can be manipulated directly in the AR space.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/microsoft/MixedRealityToolkit-Unity" />
		<title level="m">Microsoft Mixed Reality Toolkit</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robot Operating System</title>
		<ptr target="https://www.ros.org" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unity 3D Game Engine</title>
		<ptr target="https://unity.com" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Causal Approach to Tool Affordance Learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Brawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8394" to="8399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reasoning Operational Decisions for Robots via Time Series Causal Inference</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristides</forename><surname>Kiprakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6124" to="6131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Causal-Based Approach to Explain, Predict and Prevent Failures in Robotic Tasks</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karinne</forename><surname>Ramirez-Amaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">104376</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning</title>
		<meeting>the Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="812" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actional-Perceptual Causality: Concepts and Inductive Learning for AI and Robotics</title>
		<author>
			<persName><forename type="first">Seng-Beng</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium Series on Computational Intelligence</title>
		<meeting>the IEEE Symposium Series on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="442" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</title>
		<author>
			<persName><forename type="first">Tabitha</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrita</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Kroemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4776" to="4782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Causal Learning for Robotic Intelligence</title>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments</title>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinxi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hoeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Lin Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritvik</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunrong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hammad</forename><surname>Mazhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buck</forename><surname>Babich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavriel</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of Robot Cells via Head Mounted Displays and Augmented Reality</title>
		<author>
			<persName><forename type="first">David</forename><surname>Puljiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Bosing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>eeding of the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11445" to="11451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">General Hand Guidance Framework Using Microsoft HoloLens</title>
		<author>
			<persName><forename type="first">David</forename><surname>Puljiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>StÃ¶hr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><forename type="middle">S</forename><surname>Riesterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BjÃ¶rn</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>KrÃ¶ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>eeding of the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5185" to="5190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anirudh Goyal, and Yoshua Bengio. 2021. Toward Causal Representation Learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intuitive Robot Integration via Virtual Reality Workspaces</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Minh Q Tram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Cloud</surname></persName>
		</author>
		<author>
			<persName><surname>Beksi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11654" to="11660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robot Learning with a Spatial, Temporal, and Causal and-or Graph</title>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
