<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Full DAG Score-Based Algorithm for Learning Causal Bayesian Networks with Latent Confounders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Gonzales</surname></persName>
							<email>christophe.gonzales@lis-lab.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<region>LIS</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir-Hosein</forename><surname>Valizadeh</surname></persName>
							<email>amir.valizadeh@lis-lab.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<region>LIS</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Full DAG Score-Based Algorithm for Learning Causal Bayesian Networks with Latent Confounders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal Bayesian networks (CBN) are popular graphical probabilistic models that encode causal relations among variables. Learning their graphical structure from observational data has received a lot of attention in the literature. When there exists no latent (unobserved) confounder, i.e., no unobserved direct common cause of some observed variables, learning algorithms can be divided essentially into two classes: constraint-based and score-based approaches. The latter are often thought to be more robust than the former and to produce better results. However, to the best of our knowledge, when variables are discrete, no score-based algorithm is capable of dealing with latent confounders. This paper introduces the first fully score-based structure learning algorithm searching the space of DAGs (directed acyclic graphs) that is capable of identifying the presence of some latent confounders. It is justified mathematically and experiments highlight its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal networks, a.k.a. causal Bayesian networks (CBN) <ref type="bibr" target="#b14">[15]</ref>, are graphical probabilistic models that encode cause-and-effect relationships. Like Bayesian networks (BN), they are constituted by i) a directed acyclic graph (DAG) whose nodes represent random variables and whose edges encode their relationships; and ii) a set of conditional probability distributions of the nodes/random variables given their parents in the graph. However, unlike BNs, the semantics of the edges is not merely correlation but rather a causal relationship, that is, an arc from A to B states that A is a direct cause of B. CBNs are important for Artificial Intelligence because they enable to perform the same kind of reasoning as humans do, in particular counterfactual reasoning (if I had done this, what would have happened?).</p><p>Although it is well-known that learning the structure of CBNs from only (observational) data is theoretically not always possible <ref type="bibr" target="#b14">[15]</ref>, many algorithms have been proposed in the literature for this purpose. When there exists no unmeasured confounder, i.e., no unobserved direct common cause of some measured variables, they can be essentially divided into two classes: constraint-based and scorebased approaches. The former <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref> rely on statistical conditional independence tests to uncover the independence properties underlying the probability distribution that generated the data, thereby learning the graphical structure of the causal model. These methods are often not able to uncover the whole DAG of the CBN, so they provide weaker information in the form of a Completed Partially Directed Acyclic Graph (CPDAG). In such a graph, only the directed edges represent "true" causal relations, the undirected ones representing correlations, their causal direction remaining unknown. On the other hand, score-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref> identify the DAG of the CBN as the one maximizing some fitness criterion on the data. They rely on either approximate or exact optimization techniques to uncover the searched DAG. However the orientations of its arcs may not always have a causal meaning. So this DAG is mapped into the CPDAG of its Markov equivalence class, which is the best that can be extracted in terms of causality from the data. Score-based approaches are usually considered more robust than constraint-based approaches, notably because, in the latter, errors in statistical tests can chain and decrease significantly the quality of the resulting CPDAGs.</p><p>However, in most practical situations, some variables play an important role in the causal mechanism and, yet, for different reasons, they are not or cannot be observed in the data. For instance, their measuring may be too expensive or it would require unethical processes. Constraint-based methods have been successfully extended to cope with such latent (unobserved) variables <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. For score-based algorithms, it is somewhat different: it is commonly admitted that they are unable to cope with latent variables because they rely on searching for DAGs and DAGs are inadequate in the presence of latent variables. An extension of DAGs called Maximal Ancestral Graphs (MAG) <ref type="bibr" target="#b17">[18]</ref> has been introduced precisely to fix this issue and score-based approaches have been adapted to learn MAGs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. Unfortunately, currently, they can only cope with scoring MAGs over continuous variables. Yet, this is restrictive because there exist situations in which variables are discrete by nature and cannot be meaningfully extended as continuous ones, e.g., nonordinal variables such as colors, locations, types of devices, etc.</p><p>In this paper, we address problems in which all the random variables are discrete. In <ref type="bibr" target="#b21">[22]</ref>,it was shown that causal models with arbitrary latent variables can always be converted into semi-Markovian causal models (SMCM), i.e., models in which latent variables have no parent and only two children, while preserving the same independence relations between the observed variables. So, to deal with latent confounders, we focus on learning SMCMs. More precisely, we show that, without any prior knowledge about the latent confounders or their number, DAGs learnt from observational data by latent confounders-unaware score-based approaches encode sufficient information to recover many latent confounders and their locations. Exploiting this property, we provide and justify a structure learning algorithm that i) only relies on scores; ii) uses only DAGs; and iii) is capable of identifying some latent confounders and their locations.</p><p>The rest of the paper is organized as follows. Section 2 presents formally causal models and some algorithms for learning BN and/or CBN structures from observational data that can cope with latent confounders. Then, in Section 3, we introduce our new algorithm and justify why it is capable of identifying latent confounders. Its effectiveness is highlighted through experiments in Section 4. Finally a conclusion and some future works are provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Causal Models and Structure Learning</head><p>In the paper, bold letters represent sets. X denotes a set discrete random variables. For a directed graph G, ChG(X) and PaG(X) denote the set of children and parents of node X respectively, i.e., the set of nodes Y such that there exists an arc from X to Y and from Y to X respectively. A causal model over X is defined as follows <ref type="bibr" target="#b14">[15]</ref>: Definition 1. A causal model is a pair (G, Θ) where G = (X, E) is a DAG<ref type="foot" target="#foot_0">foot_0</ref> and E is a set of arcs. To each Xi ∈ X is assigned a random disturbance ξi. Θ = {fi(PaG(Xi), ξi)} X i ∈X ∪ {P (ξi)} X i ∈X , where fi's are functions assigned to Xi's and P are probability distributions over disturbances ξi.</p><p>It is easy to see<ref type="foot" target="#foot_1">foot_1</ref> that a causal model can be represented equivalently by a Bayesian network -BN <ref type="bibr" target="#b13">[14]</ref>: Definition 2. A BN is a pair (G, Θ) where G = (X, E) is a directed acyclic graph (DAG), X represents a set of random variables, E is a set of arcs, and Θ = {P (X|PaG(X))} X∈X is the set of the conditional probability distributions (CPD) of the nodes / random variables X in G given their parents PaG(X) in G. The BN encodes the joint probability over X as P (X) = X∈X P (X|PaG(X)).</p><p>Causal models impose that the arcs are oriented in the direction of causality, that is, an arc X → Y means that X is a direct cause of Y . In this case, the BN is called a CBN. In general, BNs do not impose this restriction since they only model probabilistic dependences. Hence a BN containing only Arc X → Y is equivalent to one containing Arc Y → X. More precisely, the independence model of a BN is specified by the d-separation criterion <ref type="bibr" target="#b13">[14]</ref>: Definition 3 (Trails and d-separation). Let G be a DAG. A trail C between nodes X and Y is a sequence of nodes ⟨X1 = X, . . . , X k = Y ⟩ such that, for every i ∈ {1, . . . , k -1}, G contains either Arc Xi → Xi+1 or Arc Xi ← Xi+1.</p><p>Let Z be a set of nodes disjoint from {X, Y }. X and Y are said to be d-separated by Z, which is denoted by ⟨X ⊥G Y |Z⟩, if, for every trail C between X and Y , there exists a node Xi ∈ C, i ̸ ∈ {1, k}, such that one of the following two conditions holds:</p><p>1. ⟨Xi-1, Xi, Xi+1⟩ is a collider, i.e., G contains Arcs Xi-1 → Xi and Xi ← Xi+1. In addition, neither Xi nor its descendants in G belong to Z. The descendants of a node are defined recursively as the union of its children and the descendants of these children. 2. ⟨Xi-1, Xi, Xi+1⟩ is not a collider and Xi belongs to Z.</p><p>Such trails are called blocked, else they are active. Let U, V, Z be disjoint sets of nodes. Then U and V are d-separated by Z if and only if X and Y are d-separated by Z for all X ∈ U and all Y ∈ V.</p><p>Let U, V, Z ⊆ X be disjoint sets and let P be a probability distribution over X. We denote by U⊥ ⊥P V|Z the probabilistic conditional independence of U and V given Z. The independence model of BNs is the following:</p><formula xml:id="formula_0">⟨U ⊥G V|Z⟩ =⇒ U⊥ ⊥P V|Z.<label>(1)</label></formula><p>Two BNs with the same set of d-separation properties therefore represent the same independence model. Any graphical model satisfying Eq. ( <ref type="formula" target="#formula_0">1</ref>) is called an I-map (independence map). The d-separation criterion implies that two BNs represent the same independence model if and only if they have the same skeleton and the same set of vstructures <ref type="bibr" target="#b28">[29]</ref>: the skeleton of a directed graph G is the undirected graph obtained by removing all the orientations from the arcs of G; vstructures are colliders ⟨Xi-1, Xi, Xi+1⟩ such that G does not contain any arc between Xi-1 and Xi+1. The directions of the arcs in v-structures are therefore identical for all BNs that represent the same sets of independences. When Eq. ( <ref type="formula" target="#formula_0">1</ref>) is substituted by:</p><formula xml:id="formula_1">⟨U ⊥G V|Z⟩ ⇐⇒ U⊥ ⊥P V|Z,<label>(2)</label></formula><p>then the graphical model is called a P-map (perfect map).</p><p>To learn the structure of a BN from observational data, it is usually assumed that the distribution P that generated the data has a P-map (although there exist some papers that relax this assumption <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>). Then, it is sufficient to learn the independence model of the data (the set of conditional independences): each independence X⊥ ⊥P Y |Z necessarily implies the lack of an arc between X and Y in the BN. The (undirected) edges of the skeleton therefore correspond to all the pairs of nodes (X, Y ) for which no conditional independence was found. The set of v-structures ⟨X, Z, Y ⟩ is the set of triples (X, Z, Y ) such that i) edges X -Z and Z -Y belong to the skeleton and ii) there exist sets Z ̸ ⊇ {Z} such that X⊥ ⊥P Y |Z. The edges of the skeleton corresponding to v-structures can be oriented accordingly. Now, to avoid creating additional spurious v-structures or directed cycles (which are forbidden in DAGs), some edges need necessarily be oriented in a given direction. These are identified using Meek's rules <ref type="bibr" target="#b11">[12]</ref> and can be computed in polynomial time <ref type="bibr" target="#b1">[2]</ref>. The graph resulting from all these orientations is called a CPDAG (Completed Partially Directed Acyclic Graph). To complete the learning of the BN's structure, there just remains to orient the remaining undirected edges. To do so, it is sufficient to sequentially apply the following two operations until there remains no undirected edge: i) orient in any direction one (arbitrary) edge; and ii) apply Meek's rules. This is precisely what constraint-based algorithms like PC <ref type="bibr" target="#b19">[20]</ref>, PC-stable <ref type="bibr" target="#b3">[4]</ref>, CPC <ref type="bibr" target="#b15">[16]</ref>, IC <ref type="bibr" target="#b28">[29]</ref> or FCI <ref type="bibr" target="#b20">[21]</ref> do, relying on statistical independence tests. MIIC <ref type="bibr" target="#b29">[30]</ref> essentially performs the same operations but exploiting multivariate information instead.</p><p>There exist many other algorithms, based notably on learning Markov blankets <ref type="bibr" target="#b24">[25]</ref> or on scoring DAGs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b25">26]</ref>. The key idea of score-based approaches is to assign to each DAG a score representing its fitness on the data. Under some assumptions, the one maximizing this criterion is the one that generated the data. In the rest of the paper, we exploit the BIC score <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_2">S(X|Z) = x∈Ω X z∈Ω Z Nxz log Nxz Nz - 1 2 log(|D|)dim(X|Z),</formula><p>where S(X|Z) denotes the score of node X given parent set Z; ΩX and Ω Z represent the domains of X and Z repectively; Nxz is the number of records in Database D such that X = x and Z = z; Nz = x∈Ω X Nxz; and, finally, dim(X|Z) is the number of free parameters in the conditional probability distribution P (X|Z), i.e.,</p><formula xml:id="formula_3">dim(X|Z) = (|ΩX | -1) × |Ω Z |. The term 1 2 log(|D|)dim(X|Z) is called the penalty of the score.</formula><p>In the literature, v-structures are considered to represent causal relations, that is, the directions of their arcs have a causal meaning. This implies that, when there exist no latent confounder, CPDAGs are precisely the best that can be extracted in terms of causality from the data. So, all the aforementioned algorithms do actually learn CBNs when they return CPDAGs instead of full BNs.</p><p>In practice, for different reasons (ethics, price, immeasurability, etc.), it is often the case that some confounders cannot be observed. Constraint-based approaches, notably FCI, have been extended to deal with such situations. Instead of returning a CPDAG, they provide more informative graphs like Partial Ancestral Graphs (PAG) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. PAGs are constituted by usual arcs (→) but also labeled edges (•-•, •→) and bidirected arcs (↔). The latter indicate the presence of a confounder, that is, A ↔ B in a PAG means that there exists A ← L → B in the generating DAG, with L the confounder. The • labels indicate that the learning algorithm is uncertain whether there should be an arrow head or not, i.e., •is equivalent to either ← or -. For continuous random variables, score-based approaches have been extended to cope with latent confounders <ref type="bibr" target="#b16">[17]</ref>. But, to our knowledge, when variables are discrete, there exists no score-based approach capable of dealing with latent confounders. One reason is that it is believed that detecting confounders is impossible when searching with scores the space of DAGs. We show in the next section that it is not the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A New Full Score-based Causal Learning Algorithm</head><p>In the rest of the paper, X is divided into XO and XH , which represent sets of observed and hidden (latent) variables respectively. We assume that there exists an underlying probability distribution P * over X = XO ∪ XH that generated a dataset D * . But, as the variables in XH are unobserved (they are the latent confounders), only the projection D of D * over XO, i.e., the dataset resulting from the removal from D * of all the values of the variables of XH , is available for learning. In addition, we assume that P * is decomposable according to some DAG G * . The goal is to recover G * .</p><p>The key idea of our algorithm is summarized on Figure <ref type="figure" target="#fig_1">1</ref>: the left side displays part of Graph G * , which contains a latent confounder L ∈ XH and represents the structure of the causal network that generated the data; Graph G on the right should be the one learnt by a score-based algorithm. Indeed, in G * , there exists no set Z ⊆ XO that d-separates A and B (because Trail ⟨A, L, B⟩ is active). Hence, provided G * is a P-map, A and B should be dependent and a structure with an arc between A and B should have a higher score than one without. Assume that this arc is A → B. For the same reason, a structure with an arc between A and C (resp. B and D) should have a higher score that one without. Now, given any Z ⊇ {A}, node B is not d-separated from C in G * because Trail ⟨C, A, L, B⟩ is active. But it would be on Fig. <ref type="figure" target="#fig_1">1</ref>.b if there were no arc between B and C. This is the reason why score-based algorithms tend to produce structures with such an arc and analyzing such triangles (A, B, C) in the learnt graph should provide some insight on the location of the latent confounders. This intuition is confirmed by the next proposition.  Proposition 1. Assume that there exists a perfect map G * = (X, E) for Distribution P * and that XH is the set of latent confounders, i.e., all the nodes of XH have no parent and exactly two children in G * , which both belong to XO.</p><p>Let L ∈ XH be any variable such that both of its children A, B in G * have at least one parent in XO. Then, if G is a DAG maximizing the BIC score over D, as |D| → ∞, G contains an arc between A and B. Without loss of generality, assume this is Arc A → B. Then, for every C ∈ PaG * (A) ∩ XO, (A, B, C) is a clique in G.</p><p>Proof: All the proofs are provided in the supplementary material, Appendix A, at the end of the paper. ■ Note that, in Proposition 1, both A and B have other parents than L. This is important because, as shown in Proposition 2 below, whenever node L is the only parent of A (resp. B), no learning algorithm can distinguish between a graph G * with a latent confounder L whose children are A and B (Fig. <ref type="figure" target="#fig_2">2</ref> Proposition 2. Let B * = (G * , Θ) be a Bayesian network, with G * = (X, E). Let L ∈ XH be such that ChG * (L) = {A, B} ⊆ XO and PaG * (L) = ∅. In addition, assume that PaG * (A) = {L}. Finally, let G be the graph resulting from the removal of L (and its outgoing arcs) from G * and the addition of arc A → B (if G * does not already contain it). Then, for any triple of disjoint subsets of variables U, V, W of XO, we have that:</p><formula xml:id="formula_4">⟨U ⊥G V|W⟩ ⇐⇒ ⟨U ⊥G * V|W⟩.<label>(3)</label></formula><p>Proposition 3. Let B * = (G * , Θ) be a Bayesian network such that G * = (X, E) and Arc A → B belongs to E. Let L ∈ XH be such that ChG * (L) = {A, B} ⊆ XO. Finally, let G be the graph resulting from the removal of L (and its outgoing arcs) from G * and the addition the set of arcs {X → B : X ∈ PaG * (A)\{L}} (if G * does not already contain them). Then, for any triple of disjoint subsets of variables U, V, W of XO, we have that:</p><formula xml:id="formula_5">⟨U ⊥G V|W⟩ ⇐⇒ ⟨U ⊥G * V|W⟩.<label>(4)</label></formula><p>In the two situations mentioned above, algorithms like FCI deal with indistinguishability by not choosing a single structure to return but rather by labelling arcs with • to highlight the uncertainty about the locations of the arrow heads and by asking the user to personally select which structure seems the most appropriate. In a sense, this corresponds to completing the structure learning with some user's expert knowledge. Other algorithms like MIIC or our algorithm prefer to choose which structure seems the best, hence relieving the user of such a burden. The rule followed by our algorithm is to discard latent variables when they are not absolutely necessary (i.e., in Fig. <ref type="figure" target="#fig_2">2</ref>, it selects only Graphs 2.c &amp; 2.e). This criterion can be viewed as a simple Occam razor. So, without loss of generality, in the rest of the paper, we assume that i) there exists no arc between A and B in G * ; and ii) both A and B have other parents than L in G * .</p><p>The rationale of our algorithm relies on first learning a structure using any score-based algorithm and, second, examining the triangles to discover the latent confounders. The learnt DAG is then updated to take into account these confounders, mapping Fig. <ref type="figure" target="#fig_1">1</ref>.b into Fig. <ref type="figure" target="#fig_1">1</ref>.a. Unfortunately, the original network G * may itself contain some triangles, which we will call genuine triangles. So, we need to discriminate them from the ones induced by the latent confounders, which we call latent triangles. For this purpose, remark that, for every pair (X, Y ) of variables of the genuine triangles, there exists no set Z ⊆ XO\{X, Y } such that ⟨X ⊥G * Y |Z⟩ because, G * containing an arc between X and Y , Trail ⟨X, Y ⟩ is active. So, if G * is a perfect map and Dataset D is sufficiently large, we should not find any Z ⊆ XO\{X, Y } such that X⊥ ⊥P * Y |Z. Here, note that Dataset D being the projection of D * on XO, it is generated by a distribution P over XO defined as the marginal of P * over XO, i.e., P = X H P * . But since XO contains {X}, {Y } and Z, joint probability P (X, Y, Z) = P * (X, Y, Z), so that X⊥ ⊥P Y |Z is equivalent to X⊥ ⊥P * Y |Z. Overall, in Dataset D, for genuine triangles, it should not be possible to find any pair of variables (X, Y ) in the triangle such that there exists a set Z ⊆ XO\{X, Y } such that X⊥ ⊥P Y |Z.</p><p>In latent triangles, for the same reason, this property holds for pair (A, C). For Pair (A, B), it also holds because Trail ⟨A, L, B⟩ is active for all Z ⊆ XO\{A, B} (see Fig. <ref type="figure" target="#fig_1">1</ref>.a). On the contrary, for pair (B, C) of Fig. <ref type="figure" target="#fig_1">1</ref>.b, there exist d-separating sets Z ⊆ XO\{B, C} in G * because G * does not contain any arc between B and C. Note that A ̸ ∈ Z otherwise Trail ⟨B, L, A, C⟩ of Fig. <ref type="figure" target="#fig_1">1</ref>.a would be active and, so, ⟨B ̸ ⊥G * C|Z⟩. So, in terms of independences observable in Dataset D, there exist Z ⊆ XO\{A, B, C} such B⊥ ⊥P C|Z and, moreover, B̸ ⊥ ⊥ P C|Z ∪ {A}. This suggests the following property to discriminate between latent and genuine triangles: Rule 1. Only latent triangles contain exactly one pair of nodes that are independent given some set Z ⊆ XO but are dependent given Z union the third node of the triangle.</p><p>For the case of Fig. <ref type="figure" target="#fig_1">1</ref>.a, any of the latent triangles of Fig. <ref type="figure">3</ref> can be learnt by score-based approaches because they encode exactly the same d-separation properties. In Fig. <ref type="figure">3</ref>, the independent pair of nodes mentioned in Rule 1 is (B, C) for the three types. Triangles of types 2 and 3 share the fact that G * 's arc A → C has been reversed. If C had parents in G * , then these have become its children in G to avoid creating new v-structures. So, when triangles of types 2 or 3 are learnt, C should not have many parents other than A, which may not be the case for B. So, to minimize the BIC score's penalty induced by the arc between B and C, the latter will most probably be oriented as B → C. This intuition was confirmed in experiments we conducted: Type 2 triangles only allowed us to determine less than 0.5% of the latent confounders 3 . So, to speed-up our algorithm without decreasing significantly its effectiveness, we chose to exploit only Types 1 and 3 latent triangles. Note that this makes our algorithm only approximate but, as shown in the experiments, it still remains very effective. Rule 2 summarizes the features of the above triangles:</p><p>3 Note that, from an observational point of view, it is possible to discriminate between Type 2 and Type 3 triangles: in the former, C is a parent of B (as in the original graph G * ). So, for parents D of B in G * , ⟨C, B, D⟩ is a v-structure in G, which fits the independences observable in the dataset. On the contrary, in Type 3 triangles, the connection ⟨C, B, D⟩ is not a vstructure, which contradicts the observable independences. So, as shown in Figure <ref type="figure">4</ref>, the learning algorithm should add an arc between D and C. If score-based algorithms never made any mistake in their learning, we could simply use Rule 1 to identify latent triangles and the location of the latent confounders. However, in practice, similarly to constraint-based methods, they do make mistakes, which may induce our algorithm to incorrectly identify spurious latent triangles. Fortunately, it is possible to increase the robustness of the latent triangles identification by not only considering the dependences between nodes A, B and C but also by taking into account those involving node D, the parent of B in G * (See Fig. <ref type="figure" target="#fig_1">1</ref>.a).</p><p>The dotted arcs in Fig. <ref type="figure">4</ref> are those involving D and nodes A, B, C that should be learnt by the score-based algorithm. In Type 1 triangles, there should therefore exist an arc D → B, which can be translated as "node B must have at least 3 parents in G". Triangles of Type 1 are therefore considered as latent only if they satisfy both this property and Rule 2. For Type 3 triangles, the situation is more complex because, in addition to Arc D → B, there should also exist an arc between D and C to account for ⟨D ̸ ⊥G * C|{A, B}⟩. Note that Triangle (D, B, C) cannot be misinterpreted as latent because both pairs (B, C) and (C, D) can be made independent given some Z ⊆ XO, hence ruling out Rule 1. Indeed, according to Fig. <ref type="figure" target="#fig_1">1</ref>.a, ⟨C ⊥G * D|Z⟩ for any Z ⊆ XO\{C, D} equal to ∅, {A} or {B}. Unfortunately, for the same reason that Types 1, 2 and 3 triangles encode the same d-separation properties, the orientations of the arcs of Triangle (D, B, C) can be reversed (provided they do not induce directed cycles). As a consequence, in the learnt graph G, node D may be a child of B rather than its parent. At first sight, it is difficult to discriminate between the true children of B in G * and its true parents in which the arcs of Triangle (D, B, C) have been reversed. However, note that if D were a true child of B in G * , ⟨C ⊥G * D|Z⟩ would hold for Z only equal to ∅ or {B} (see Fig. <ref type="figure" target="#fig_1">1</ref>.a). As a consequence, only the true parents D of B in G are such that ⟨C ⊥G * D|Z⟩ given some set Z ⊆ XO\{C, D} containing node A. So Type 3 triangles are considered as latent only if they satisfy both this property and Rule 2. Overall, this results in Algorithm 1.</p><p>Remark that, as mentioned previously, on Line 12, Algorithm 1 is allowed to remove Arc A → B because, when this arc truly exists in G * (see Fig. <ref type="figure" target="#fig_2">2</ref>.d), based on Dataset D, it is impossible for any learning algorithm to distinguish between G * and Fig. <ref type="figure" target="#fig_2">2</ref>.e. In such a case, our algorithm deliberately chooses to return the structure of Fig. <ref type="figure" target="#fig_2">2</ref>.e in order to enforce some Occam razor and to avoid requiring some expert knowledge to select the right structure.  Rule 2 and Algorithm 1 require the determination of some set Z ⊆ XO such that some pairs of nodes are conditionally independent given Z. Below, we suggest two algorithms for this purpose. The first algorithm to determine Set Z consists of exploiting the fact that, in I-maps, d-separation implies conditional independences. Hence, applying a d-separation analysis on G using, e.g., van der Zander and Liśkiewicz <ref type="bibr" target="#b27">[28]</ref>'s algorithm, it is possible to get some d-separating set Z and, consequently, a set inducing a conditional independence. The following proposition justifies that this approach can be used<ref type="foot" target="#foot_2">foot_2</ref> : Proposition 4. Let D * be a dataset generated by some distribution P * over X for which there exists a perfect map G * = (X, E), and let D be the projection of D * over XO. Then, as |D| → ∞, every DAG G maximizing the BIC score over D is a minimal I-map.</p><p>However, when the database is not "too" large and Graph G is not highly trustworthy, another option is to exploit Algorithm 2 which adds iteratively to Z the variable X that allows to reduce the most the dependence between the pair of nodes U, V until an independence between U and V is inferred or no independence can be proven. In essence, this is the approach followed by MIIC <ref type="bibr" target="#b29">[30]</ref>, except that MIIC estimates dependences through an information theoretic criterion whereas we exploit the BIC score: Definition 4. For every pair of variables U, V and every set of variables Z, let fBIC (U, V |Z) be defined as: </p><formula xml:id="formula_6">fBIC (U, V |Z) = 2× S(U |V, Z) -S(U |Z) + 1 2 log(|D|)δ (5) where δ = (|ΩU | -1) × (|ΩV | -1) × |Ω Z |</formula><formula xml:id="formula_7">∩ F = ∅ Output: A set Z s.t. U ⊥ ⊥V |Z if such set is found, else False 1 Z ← C; F ← F ∪ {U, V } 2 δ ← (ΩU | -1) × (|ΩV | -1) X∈C |ΩX | 3 if |Z| &gt; h then return False; 4 if fBIC (U, V |Z) &lt; χ 2 δ (α) then return Z; 5 while |Z| &lt; h do 6 X = argmin{fBIC (U, V |Z ∪ {Y }) : Y ∈ XO\(Z ∪ F)} 7 Z ← Z ∪ {X} 8 δ ← δ × |ΩX | 9 if fBIC (U, V |Z) &lt; χ 2</formula><p>δ (α) then return Z; 10 return False Proposition 5. If U and V are independent given a set Z, the formula of Eq. ( <ref type="formula">5</ref>) follows a χ 2 distribution of δ degrees of freedom. So, given a risk level α, U and V are judged independent if the value of Eq. ( <ref type="formula">5</ref>) is lower than the critical value χ 2 δ (α) of the χ 2 distribution.</p><p>To conclude this section, we provide below the time complexity of Algorithm 1, assuming (as we did in our experiments) that the algorithm used for determining the conditioning sets Z required in Rule 2 and Algorithm 1 is Algorithm 2: Proposition 6. Assume that the existence of d-separating sets Z is checked with Algorithm 2 with h the maximal size allowed for Z and fBIC defined as Eq. ( <ref type="formula">5</ref>). Let n and m denote the number of nodes and arcs of G as defined on Line 1 respectively. Let k be the maximum number of parents and children of the nodes in G. Then the time complexity of Algorithm 1 over dataset D is O(n 2 k 3 h|D|+m log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, some experiments on classical benchmark CBNs (child (|XO| = 20), water (|XO| = 32), insurance (|XO| = 27), alarm (|XO| = 37), barley (|XO| = 48)) from the BNLearn Bayes net repository <ref type="foot" target="#foot_3">5</ref> are performed to highlight the effectiveness of our algorithm to find latent confounders and to recover CBN structures.</p><p>For each experiment, a CBN is selected, and some new (latent) variables Li are added to it. To make them confounders of a semi-Markovian causal model, for each Li, two nodes of XO are randomly chosen to become Li's children. To fit the propositions of the paper, Line 1 of Algorithm 1 is performed using the CPBayes exact score-based learning algorithm <ref type="bibr" target="#b25">[26]</ref>. For this purpose, Datasets D need to be converted into so-called instances that are passed as input to CPBayes. These contain all the possible nodes' sets that CPBayes will consider as potential parent sets. So, to control the combinatorial explosion it has to face, CPBayes requires limiting the number of possible parents of the nodes. In the experiments, we set this limit to 4 because i) no node of XO had more than 4 parents in the original CBN (the one without confounders); and ii) this enabled to control the amount of computations performed by CPBayes (see <ref type="bibr" target="#b25">[26]</ref> for more details). As a consequence, since the score-based algorithm may add 2 additional parents (A and C) to some Li's children (B), as shown in Fig. <ref type="figure" target="#fig_1">1</ref>.b, all the Li's children are selected randomly but with the constraint that they have 1 or 2 parents in XO. This upper bound To discover that some nodes A and B are children of some confounder Li, all the learning algorithms rely in some way or another on the fact that A and B are conditionnally dependent given any set Z. This imposes some restrictions on the way the conditional probability distributions (CPD) of Li and its children should be generated. Actually, assume that they are uniformly randomly generated. Then the cells of the joint distribution P of A, B and their parents is a sample generated from a uniform distribution. Testing the conditional (in)dependence of A and B given some sets Z strictly included in A and B's parents amounts to marginalize out some variables from P or, equivalently, to sum some values of P . The sum of 2 independent variables uniformly distributed follows a triangular distribution and, by the central limit theorem, the sum of more than 2 variables tends to a variable normally distributed. As such, the values of the marginals of P have much more chances to be located on the mode of the distribution than on the tails. In other words, the cells of the marginals of P tend to have more or less the same values, which makes A and B appear to be independent, and no learning algorithm can determine that they are the children of a confounder. This is the reason why the CPDs of A, B and Li need to be generated differently.</p><p>In our experiments, the CPD of Li is set to a uniform distribution in order to maximize the chances of A and B to be dependent. For A (resp. B), for each value of its parents, the CPD of A (resp. B) is set to a mixture of a Dirichlet distribution whose hyperparameters αi are all set to 4 and a Dirac distribution. The weight of the latter is selected randomly between 2/3 and 5/6. Such mixtures tend experimentally to limit the effect of the central limit theorem but, of course, do not discard it completely. So, to check whether A and B have some chance to be identified as dependent, we test whether the values of their mutual information and their conditional mutual information given their parents are higher than some thresholds (these are some extreme cases for the Z sets mentioned in the preceding paragraph). If this is the case, the CPDs of A and B are judged admissible for the experiments. To make them realistic, the thresholds are defined as the averages of the mutual information and condition mutual infor-mation respectively of all the pairs of nodes with a common parent in the original CBN. The use of these information-theoretic criteria has been made to favor algorithms like MIIC over our algorithm.</p><p>For each CBN with confounders created as defined above, a dataset D * is randomly generated using the pyAgrum 1.13.0 library <ref type="bibr" target="#b6">[7]</ref> and Dataset D which is given as input of the learning algorithms is the projection of D * over XO. Algorithm 2 is exploited for independence testing and its risk level α and Size h are set to 0.05 and 7 respectively <ref type="foot" target="#foot_4">6</ref> . In the experiments, we compare 3 learning algorithms: Algorithm 1, MIIC <ref type="bibr" target="#b29">[30]</ref> and FCI <ref type="bibr" target="#b20">[21]</ref>. For MIIC, we use the pyAgrum's implementation with the NML correction to be more accurate. For FCI, we use the python causal-learn 0.1.3.8 package <ref type="bibr" target="#b31">[32]</ref>. All the tables below display averages of the results over 50 CBN/datasets. All the experiments are executed on an Intel Xeon Gold 5218 CPU with 128GB of RAM.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, to every original CBN, 2 confounders have been added whose domain sizes are equal to 2. The table compares the performance of Algorithm 1, MIIC and FCI for different CBNs with different dataset sizes. Columns ok and ¬ok contain the number of correctly and wrongly identified confounders respectively. Column "prec." displays the precision metrics, i.e., it is equal to ok / (ok + ¬ok). Column "recall" is the usual recall metrics, i.e., it is equal to ok / the number of confounders Li added to the original CBN. The F1 score is defined as 2 × (precision × recall)/(precision + recall). Column "time" reports the average computation times in seconds of Lines 2 to 16 of Algorithm 1 (finding the confounders). These lines increase only marginally the structure learning computation times.</p><p>FCI finds most of the latent confounders. This is the reason why, in Table <ref type="table" target="#tab_1">1</ref>, it is the best in terms of recall. Unfortunately, it also misidentifies numerous variables as confounder's children. This is the reason why its precision and F1 score are, by far, never the best. Here, we should emphasize that we identify confounders with ↔ connections, that is, we do not take into account • labels since those express an uncertainty about the existence of arrow heads. Converting every -• into → results in adding numerous false positive, which would de-  crease significantly the quality of the solutions found by FCI. MIIC identifies fewer confounders but it also makes much fewer mistakes, which makes it better in terms of F1 score. Algo. 1 makes even fewer mistakes and this is the reason why its precision is almost always the best of all the algorithms. Yet, it is somewhat cautious and tends to miss some confounders, which explains why its recall is not the best. However, the larger the size of the dataset, the higher the number of confounders correctly identified by Algorithm 1. This can also be observed in terms of F1 score: for small datasets, MIIC outperforms Algorithm 1 but this is the converse when |D| increases. This phenomenon illustrates empirically Proposition 1.</p><p>Table <ref type="table" target="#tab_3">2</ref> reports how the CPDAGs learnt by Algorithm 1, MIIC and FCI compare with those of the CBNs (with their confounders) that were used to generate the datasets. Columns "ok" (resp. "miss") indicate the number of arcs and edges that were learnt correctly (resp. that existed in the generating CBN but for which no arc nor edge was learnt). As can be observed, Algorithm 1 and MIIC are the best for these metrics and their results are quite comparable. Of course, for these metrics, the larger the dataset, the better the quality of the learnt CPDAGs. Column "rev." displays the number of arcs in the CPDAG of the generating CBN that were learnt in the opposite direction, i.e., the direction of the causality is incorrectly learnt. For this metrics, FCI always outperforms the other algorithms. Note, however, that the number of reversed arcs is always very small for all the CBNs and all dataset sizes. Column "type" refers to the number of arcs (resp. edges) that were learnt as edges (resp. arcs), i.e., their types (directed, undirected) are incorrect. For this metrics, Algorithm 1 and MIIC usually outperform FCI and, in general, when Algorithm 1 outperforms MIIC, the difference is bigger than when this is the converse. Finally, Column "xs" reports the number of arcs or edges that belong to the learnt CPDAG but whose extremal nodes are linked neither by an arc nor by an edge in the generating CBN. Here, Algorithm 1 significantly outperforms both MIIC and FCI. This means that Algorithm 1 is less prone to learn spurious direct causes. Overall, empirically, Algorithm 1 is very competitive and often produces CPDAGs closer to those of the original CBNs than the other two methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Perspectives</head><p>In this paper, we have introduced the first score-based CBN structure learning algorithm for discrete variables that searches only the space of DAGs, exploits only observational data and, yet, is capable of identifying some latent confounders. It has been justified mathematically, notably through Proposition 1. In addition, theoretically, it is asymptotically guaranteed to produce an I-map. Experiments highlighted it effectiveness, especially for large datasets. Notably, both in terms of the CPDAGs and the latent confounders found, the results of this algorithm may be judged as very competitive compared to those of its constraint-based competitors like MIIC or FCI.</p><p>For future works, to be more scalable, we plan to substitute the use of the CPBayes on Line 1 of Algorithm 1 by a faster approximate algorithm like greedy hill climbing (GHC). Usually, GHC-like methods make more mistakes in the directions of the causal arcs learned than CPBayes. So, to compensate for this issue, the rules used in Lines 6 and 8 may certainly have to be improved. Notably, in Algorithm 1, we did not take into account Type 2 triangles (see Figure <ref type="figure">3</ref>.b) because, empirically, they were very seldom encountered in the experiments. However, with GHC-like algorithms, this may not be the case anymore and they should be taken into account.</p><p>Perhaps a more immediate improvement could be made by observing that, whenever Type 3 triangles are found, their A → C connection is in the wrong direction (see Figure <ref type="figure">3</ref>.b). Therefore, to produce better CPDAGs, Algorithm 1 should reverse this arc and relearn the neighborhood of node C. In addition, as shown in Propositions 2 and 3, some structures are indistinguishable from the observational data point of view and, among all the structures of Fig. <ref type="figure" target="#fig_2">2</ref>, Algorithm 1 makes the decision to select only those of Fig. <ref type="figure" target="#fig_2">2</ref>.c and 2.e. So, in the same spirit as PAGs, the output of Algorithm 1 could be improved to express the uncertainty about which of these structures should be selected. Note that, in this case, PAG's • labels are not sufficient since the uncertainty not only concerns the location of arrow heads but also the very existence of some arcs (like C → B of Fig. <ref type="figure" target="#fig_2">2</ref>.e) for which no independence test can detect that they can be dispensed with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>Below are the proofs of all the propositions of the paper. Lemma 1. Let XO and XH be two disjoint sets of random variables and let X = XO ∪ XH . Let D * be a dataset generated by some distribution P * over X for which there exists a perfect map G * = (X, E), and let D be the projection of D * over XO, i.e., the dataset resulting from the removal from D * of all the values of the variables of XH . Let G be a DAG maximizing the BIC score over D. Then, as |D| → ∞, if, in Graph G, there is no arc between a pair of nodes (A, B) and no directed path<ref type="foot" target="#foot_5">foot_5</ref> from B to A, then A⊥ ⊥P * B|PaG(B).</p><p>Proof of Lemma 1. As there is no directed path from B to A in G, adding A → B cannot create a directed cycle, hence this would produce a new DAG. If G does not contain this arc, this therefore means that the BIC score S(B|PaG(B) ∪ {A}) is lower than or equal to S(B|PaG(B)). D is the projection of D * over XO, so the distribution P which generated D is the projection of P * over XO, hence, for any Z, W ⊆ XO, P (Z|W) = P * (Z|W). As a consequence, as |D| → ∞, the BIC score can be expressed in terms of mutual information (IP * ) and entropy (HP * ) over P * [9, p792]:</p><formula xml:id="formula_8">S(B|PaG(B) ∪ {A}) → |D| × [IP * (B; PaG(B) ∪ {A}) -HP * (B)] - log |D| 2 dim(B|PaG(B) ∪ {A}),</formula><p>where, for all Z, W:    <ref type="figure">(A,</ref><ref type="figure">B,</ref><ref type="figure">C</ref>).</p><formula xml:id="formula_9">dim(B|PaG(B) ∪ {A}) = (|ΩB| -1) × |ΩA| × X∈Pa G (B)</formula><formula xml:id="formula_10">■ Proof of Proposition 2. ⟨U ⊥G V|W⟩ is equivalent to ⟨U ⊥G V |W⟩ for all U ∈ U and V ∈ V. So, we just need to prove that, for any U, V ∈ XO, U ̸ = V , ⟨U ⊥G V |W⟩ ⇐⇒ ⟨U ⊥G * V |W⟩.</formula><p>First, note that G * does not contain Arc B → A, otherwise PaG * (A) would contain B. Adding Arc A → B cannot create a directed cycle in G * because A has only one parent, L, which cannot be involved in any cycle since it has no parent. Let DescG(A) denote the set of descendants of A in G, i.e., DescG(A) = {X : there exists a directed path from A to X}. Note first that DescG(A) = DescG * (A) ∪ DescG * (B) and, for all X ∈ X\{A, L}, we have that DescG(X) = DescG * (X). Now, consider any simple<ref type="foot" target="#foot_6">foot_6</ref> trail C * = ⟨X1 = U, . . . , X k = V ⟩ between U and V in G * . Assume it is blocked in G * . This is the case if and only if i) it contains is a convergent connection at some node T such that neither T nor its descendants are in W; or ii) it contains a non-convergent connection at some node T ∈ W.</p><p>Assume Trail C * does not contain L, then it also exists in G. Case i) cannot occur at T = A because A has only one parent in G * or at T = L because C * does not contain L by hypothesis. Since all the other nodes in G * have the same set of descendants as their counterparts in G, the convergent connections in C * have therefore exactly the same status (blocked, active) in G and G * . As for case ii), any non-convergent connection in C * is the same in G and G * and cannot involve L (by hypothesis). Hence, they have the same status in G and G * . So C * is blocked or active in both G and G * . Now, if C * contains L, it includes the non-convergent connection</p><formula xml:id="formula_11">A ← L → B. Let C be the trail of G obtained from C * by substi- tuting A ← L → B by A → B.</formula><p>For the same reasons as above, all the nodes different from A, L have the same connections in C and C * and the same set of descendants. Hence their status (blocked, active) are the same in C and C * . Node A cannot have a convergent connection in C * because PaG * (A) = {L}. In C, its connection is also non-convergent since its child B in G is its neighbor in C. So, the connection at A is non-convergent and has the same status in both C and C * . Node L has a non-convergent connection in C * but, as it is unobserved, it cannot belong to W and cannot block the trail. So, removing it from C * cannot change the status of the trail. Hence, overall, all the trails in G * can be mapped into a trail in G with exactly the same status.</p><p>Conversely, let C be a simple trail in G. If this trail does not include Arc A → B, then it also belongs to G * and the same reasoning as above shows that this trail has the same status in G and G * .</p><p>If, on the other hand, C contains Arc A → B, then substituting it by A ← L → B results in a new trail C * of G * . As above, all the nodes except A, B, L have the same status. In addition, L cannot block trail C * and the connection at A is non-convergent in both C and C * (because it has at most one parent, L). If the connection at B is convergent in C, it is also convergent in C * , with its parent A substituted by L. If the connection at B in C is non-convergent, this means that it is of the form A → B → X, and in C * , its connection is L → B → X, also a non-convergent connection. Hence, overall, every trail of G can be mapped into a trail of G * with the same status.</p><p>So, for all U ∈ U and V ∈ V, we have that ⟨U ⊥G V |W⟩ ⇐⇒ ⟨U ⊥G * V |W⟩. ■ Proof of Proposition 3. As for Proposition 2, we just need to prove that, for any</p><formula xml:id="formula_12">U, V ∈ XO, U ̸ = V , ⟨U ⊥G V |W⟩ ⇐⇒ ⟨U ⊥G * V |W⟩. A simple trail C * = ⟨X1 = U, . . . , X k = V ⟩ between U and V is active in G * if</formula><p>and only if i) for all the nodes T with a convergent connection, either T or some of its descendants are in W; and ii) nodes T with non-convergent connections do not belong to W. Note that, by definition of G, if DescG(X) denotes the set of descendants of Node X in G, then DescG(X) = DescG * (X) for any X ∈ X\{L}. Let us first show that, if there exists an active trail in G * , then there also exists an active trail in G.</p><p>Let C * be an active trail of G * that does not contain L, then it also exists in G and, since DescG(X) = DescG * (X) for all X ∈ X\{L}, it is also an active trail of G.</p><p>Assume now that Trail C * contains L. Node L is equal neither to X1 nor to X k since U and V belong to XO. So, let i ̸ = 1, k be the index such that, in Trail C * , Xi = L. By definition of G * , it holds that {Xi-1, Xi+1} = {A, B}. Without loss of generality, assume below that Xi-1 = A and Xi+1 = B (if this is the converse, consider the reversed trail ⟨Y1 = V, . . . , Y k = U ⟩, which has the same dseparation status as C * , i.e., it is an active trail).</p><p>If X1 = A, then A is the first node of Trail C * . Let C = ⟨X1 = A, X3 = B, X4, . . . , X k ⟩. Then C belongs to both G * and G. In addition, the types of connection (convergent/non-convergent) of all the nodes X3, . . . , X k-1 are the same in C * and C, and the sets of descendants of these nodes are the same in G * and G. Finally, A and X k cannot belong to W since they belong to U and V respectively. Hence, C is an active trail in G.</p><p>Suppose now that X1 ̸ = A. Then, Trail C * contains Node Xi-2, i.e., it contains Subsequence ⟨Xi-2, A, L, B⟩. If Xi-2 is a child of A, then Trail C = ⟨X1, . . . , Xi-2, Xi-1 = A, Xi+1 = B, . . . , X k ⟩ belongs to both G * and G. In addition, all the nodes in C have the same type of connection (convergent/non-convergent) as in C * . Therefore, since DescG(X) = DescG * (X) for all the nodes X ∈ C, if C * is an active trail of G * , then C is an active trail of G.</p><p>Assume now that Xi-2 is a parent of A. Let C = ⟨X1, . . . , Xi-2, Xi+1 = B, . . . , X k ⟩. C is a trail of G. In addition, all of its nodes have the same type of connection (convergent/nonconvergent) in G as their corresponding node of C * in G * . Therefore, since DescG(X) = DescG * (X) for all the nodes X ∈ C, C is an active trail of G.</p><p>To complete the proof, let us now show that, if there exists an active trail C = ⟨X1 = U, . . . , X k = V ⟩ in G, then there also exists an active trail C * in G * .</p><p>If B ̸ ∈ C, then C also belong to G * because the only arcs that belong to G but not to G * are Arcs Y → B with Y ∈ PaG(A). As DescG(X) = DescG * (X) for all the nodes X ∈ C, C is also an active trail in G * . Assume now that B ∈ C. For the same reason as above, if none of the neighbors of B in C belong to PaG(A), then C is a trail of G * and is also active.</p><p>Let i be the index such that Xi = B. If some neighbors of B belong to PaG(A), two cases can obtain: case 1) exactly one neighbor of B belongs to PaG(A); and case 2) exactly two neighbors of B belongs to PaG(A). As for case 1), without loss of generality, assume that Xi-1 ∈ PaG(A) (else, reverse Trail C). If A ̸ ∈ W, then Trail C * = ⟨X1, . . . , Xi-1, A, Xi = B, . . . , X k ⟩ belongs to G * . In addition, the type of connection (convergent/non-convergent) of all the nodes in C * except A are the same as their counterpart in C. The connection at A is non-convergent and A ̸ ∈ W, so that A does not block</p><formula xml:id="formula_13">C * in G * . So, overall C * is an active trail of G * . If, now, A ∈ W, then let C * = ⟨X1, . . . , Xi-1, A, L, Xi = B, . . . , X k ⟩.</formula><p>This is a trail of G * for which the type of connection (convergent/non-convergent) of all the nodes in C * except A and L are the same as their counterpart in C. The connection at A is convergent and</p><formula xml:id="formula_14">A ∈ W, so A does not block C * in G * . The connection at L is non-convergent and L ̸ ∈ W since L ̸ ∈ XO. So L does not block C * . Consequently, C * is active in G * .</formula><p>Consider now Case 2), i.e., the two neighbors of B in C belong to PaG(A). This means that Xi-1, B, Xi+1 form a convergent connection. Since C is active, either B or some of its descendants belong to W. But B is a child of A in both G and G * . Therefore, either A or some of its descendants belong to W. As a consequence, if C * is the trail obtained from C by substituting B by A, then the connection at A in C * is convergent and A does not block C * . For all the other nodes, the connections are similar in C and C * . Hence, C * is active in G * . This completes the proof. ■</p><p>Proof of Proposition 4. Lemma 1 considers pairs of nodes (A, B) such that there exists no directed path from B to A. So, A is not a descendant of B. In addition, in this lemma, there exists no arc between A and B, so A is not a parent of B. So Lemma 1 states that, in G, every node is independent of its non-descendants given its parents.</p><p>Let G be a DAG maximizing the BIC score. Then it contains no arc X → Y such that the graph G ′ resulting from the removal of X → Y from G can maximize the BIC score. Indeed, as</p><formula xml:id="formula_15">|D| → ∞, Score S(Y |PaG(Y )) → |D| × [IP * (Y ; Pa G ′ (Y ) ∪ {X}) -HP * (Y )] - log |D| 2 dim(B|Pa G ′ (Y ) ∪ {X}) (see the proof of Lemma 1). It can- not be the case that IP * (Y ; Pa G ′ (Y ) ∪ {X}) &lt; IP * (Y ; Pa G ′ (Y ))</formula><p>else G would not maximize the BIC score (since the term in log |D| is infinitely smaller than |D|). But, for every probability distribution Q and every U, V, Z, it always holds that IQ(U</p><formula xml:id="formula_16">; Z∪{V }) ≥ IQ(U ; Z), with equality only if U ⊥ ⊥QV |Z. So, necessarily, IP * (Y ; Pa G ′ (Y ) ∪ {X}) = IP * (Y ; Pa G ′ (Y )).</formula><p>As a consequence, we have that:</p><formula xml:id="formula_17">S(Y |PaG(Y )) -S(Y |Pa G ′ (Y )) ≈ log |D| 2 [dim(B|Pa G ′ (Y ) ∪ {X}) -dim(B|Pa G ′ (Y ))] ≈ log |D| 2 (|ΩX | -1) × (|ΩY | -1) × (|Ω Pa G ′ (Y ) |) &gt; 0.</formula><p>Hence G ′ cannot maximize the BIC score. So G is minimal. Overall, by Corollary 4, p. 120, of <ref type="bibr" target="#b13">[14]</ref>, G is a minimal I-map. ■ </p><formula xml:id="formula_18">∆ dim = 1 2 log(|D|)[dim(U |V, W) -dim(U |W)] = 1 2 log(|D|)δ, with δ = (|ΩU | -1) × (|ΩV | -1) × |Ω W |.</formula><p>As a consequence, we have that:</p><formula xml:id="formula_19">S(U |V, W) -S(U |W) = u,v,w Nuvw log Nuvw Nvw -log Nuw Nw -∆ dim = u,v,w Nuvw log NuvwNw NuwNvw -∆ dim</formula><p>So we have that:</p><formula xml:id="formula_20">2 × (S(U |V, W) -S(U |W) + ∆ dim ) = 2 u,v,w Nuvw log NuvwNw NuwNvw = G2(U, V |W),</formula><p>where G2() is the formula used in the classical G-test. It is wellknown that, when U and V are independent given a set Z, the G2 formula follows a χ 2 distribution of δ degrees of freedom. So, given a risk level α, U and V are judged independent if the value of fBIC (U, V |Z) is lower than the critical value χ 2 δ (α) of the χ 2 distribution. ■</p><p>Proof of Proposition 6. Let n and m denote the number of nodes and arcs of G respectively. Let k be the maximum number of parents and children of the nodes. Assume that, for conditional independence tests, Algorithm 2 is used and that we limit it to sets Z such that |Z| ≤ h. Then Algorithm 2 completes in O(nh|D|) time. Indeed, each call to Function fBIC requires parsing the database once, which is performed in O(|D|) time. As Step 6 examines all the possible Y nodes, its time complexity is O(n|D|). Finally, the while loop of Lines 5-9 is executed at most h times. So, overall, the time complexity of Algorithm 2 is O(nh|D|). Now, let us determine the complexity of Algorithm 1. In Graph G, there exist at most nk 2 triangles (for each node, triangles can be created by selecting two parents). To check whether one is latent (Rule 2), each of its 3 arcs must be examined by Algorithm 2. So Line 2 is completed in O(n 2 k 2 h|D|) time. On Line 2, the information about which pair of nodes is independent is cached. So, determining their types on Lines 6 and 8 is performed in O(1) time. Determining whether |PaG(X3)| ≥ 3 can also be done in O(1). As for Line 8, there are at most 2k nodes D to examine (at most k parents and k children), each using Algorithm 2. So, the time complexity of Line 8 is O(nkh|D|) and, therefore, that of Loop 5-9 is O(n 2 k 3 h|D|). There are at most nk 2 triangles in G, hence O(nk 2 ) iterations of the for loop of Lines 10-14. Each instruction on lines 11 to 14 can be performed in O(1) times, hence the loop of Lines 10-14 can be performed in O(n 2 k 3 h|D|) time. Finally, on Line 15, G is transformed into a CPDAG, which can be done in time O(m log n), see <ref type="bibr" target="#b1">[2]</ref>.</p><p>Overall, the time complexity of Algorithm 1 is therefore O(n 2 k 3 h|D| + m log n). ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional experiments</head><p>In this section, additional experiments generated similarly to those of Section 4 are performed, highlighting the robustness of our results. In the first two subsections, we vary the number of latent confounders as well as their domain size. The experiments highlight the fact that the results presented in the paper (both recovering the structure and detecting latent confounders) are not sensitive to these features. In the third subsection, we vary the limit on the number of parents required by CPBayes from 4 to 6. The results show that increasing these numbers lead to significant improvements neither in the learning of structures nor in that of latent confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Detecting multiple/non-Boolean latent confounders</head><p>Table <ref type="table" target="#tab_5">3</ref> reports the learning of the confounders in datasets generated by the Insurance CBN to which we added 2 to 4 latent Boolean confounders. As for the results presented in the paper, whatever the number of latent confounders to be found, Algorithm 1 outperforms MIIC and FCI w.r.t. the wrongly identified confounders and the precision metrics. FCI outperforms the other algorithms w.r.t. the number of correctly identified latent confounders and the recall metrics.</p><p>As for the F1 metrics, Algorithm 1 and MIIC outperform the other algorithms on large and small datasets respectively. Table <ref type="table" target="#tab_6">4</ref> reports the learning of the confounders in datasets generated by the Insurance CBN to which we added two latent confounders with different domain sizes. Here again, the results are similar to those provided in the paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 The quality of the CPDAGs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 The impact of the parents number's limit of CPBayes</head><p>In the experiments section of the paper, the first step of Algorithm 1 is performed by CPBayes <ref type="bibr" target="#b25">[26]</ref>. As such, CPBayes takes as input socalled instances that are computed from Datasets D. These contain all the possible nodes' sets that CPBayes will consider as potential parent sets and, to control the combinatorial explosion, this requires limiting the number of possible parents of the nodes. In the experiments of Section 4, we set this limit to 4. In the literature, people also fix it to 5 or 6, see, e.g., <ref type="bibr" target="#b26">[27]</ref> or <ref type="bibr" target="#b23">[24]</ref>. This makes sense because, in classical Bayesian networks, nodes seldom have more than 6 parents. In addition, in practical situations, the number of parents that can be possibly learnt is limited by the size of the dataset (e.g., to be meaningful, independence tests require contingency tables much smaller than the dataset size). Tables <ref type="table" target="#tab_10">7</ref> and<ref type="table" target="#tab_11">8</ref> show the impact of these limits on the determination of the latent confounder and on the learnt structure respectively. In these tables columns "N:#" report the results obtained by limiting CPBayes instances to have at most # parents. As can be observed, there is no noticeable difference increasing the limit from 4 to 6. This is probably due to the fact that the benchmark Bayesian networks (child, water, insurance) that generated the datasets have at most 4 parents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Converting a causal model into a Bayesian network</head><p>In this section, we show on an example how to convert a causal model, as defined in Definition 1 of the paper, into a Bayesian network. For this purpose, consider the causal model of Figure <ref type="figure" target="#fig_8">5</ref>, where X = {A, B, C, D, E, F }. The domain sizes of all the random disturbances ξi are equal to {1, 2, 3, 4}. Those of the variables of X will be clearly identified from their assigned parameters. Deterministic functions like those above can be equivalently represented as conditional probability tables (CPT) such that: i) the variables on the right side of the conditioning bar are those over which the function is defined; ii) the variable on the left side of the conditioning bar is the one corresponding to the codomain of the function; iii) the values in the CPT are either 1 or 0, depending on whether the value of the variable on the left side of the conditioning bar corresponds or not to the value of the function given those of the variables at the right side of the conditioning bar. Therefore, the above deterministic functions can be represented as: e1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 e2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 e3 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1</p><formula xml:id="formula_21">P (F |E, ξF ) = e1 e2 e3</formula><p>F \ξF 1 2 3 4 1 2 3 4 1 2 3 4 f1 1 0 0 0 1 0 0 0 1 0 0 0 f2 0 1 0 0 0 0 0 0 0 1 0 0 f3 0 0 1 1 0 1 1 1 0 0 1 1</p><p>All the above probability distributions, together with the structure of Figure <ref type="figure" target="#fig_12">6</ref>.a, form the Bayesian network corresponding to the causal model of Figure <ref type="figure" target="#fig_8">5</ref>. Unfortunately, this Bayesian network still includes the disturbance nodes whereas the one that we are looking for is that of Figure <ref type="figure" target="#fig_12">6</ref>.b. Fortunately, it is easy to remove these disturbance variables: it is sufficient to marginalize them out from the joint distribution of the Bayesian network of Figure <ref type="figure" target="#fig_12">6</ref>.a, as shown below.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Triangles induced by latent confounders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>.a &amp; 2.b), and a graph G * without latent confounder L but with an arc A → B (resp. B → A) (Fig. 2.c). Similarly, as shown in Proposition 3, it is impossible to distinguish between a graph G * containing both latent confounder L and Arc A → B (Fig. 2.d), and a graph G * without latent confounder L but with both arcs A → B and C → B (Fig. 2.e). Some indistingushable structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Rule 2 .</head><label>32</label><figDesc>Possibly learnt latent triangles. For the three types, the independent pair of nodes mentioned in Rule 1 is (B, C). Triangles of Type 1 and Type 3 are such that C A B and A B C respectively. For both types, there exists some set Z ⊆ XO\{A, B, C} such that B⊥ ⊥P C|Z and B̸ ⊥ ⊥ P C|Z ∪ {A}; and there exists no Z ⊆ XO\{A, B, C} such that A⊥ ⊥P B|Z or A⊥ ⊥P C|Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Figure 4 :</head><label>34</label><figDesc>Arcs learnt in the vicinity of latent triangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 7 T1 ← T1 ∪ T 8 else</head><label>178</label><figDesc>Learning with confounders. Input: Dataset D Output: The CPDAG of the learnt CBN 1 G ← DAG learnt from D by a score-based algorithm 2 T ← the triangles of G satisfying Rule 2 // Get the latent triangles 3 T1 ← ∅ // latent triangles of type 1 4 T3 ← ∅ // latent triangles of type 3 5 foreach triangle T = (A, B, C) in T do 6 if T is of Type 1 and |PaG(B)| ≥ 3 then if T is of Type 3 and there exists D ∈ (PaG(B)\{A}) ∪ (ChG(B)\{C}) such that there exists Z ⊆ XO\{C, D} such that A ∈ Z and D⊥ ⊥P C|Z then 9 T3 ← T3 ∪ T // Recreate the latent variables foreach triangle T = (A, B, C) in T1 ∪ T3 do Add a new node L (confounder) to G Remove from G Arc A → B Remove from G the arc between B and C Add arcs L → A and L → B to G M ← the CPDAG of G return CPDAG M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Corollary 1 .</head><label>1</label><figDesc>IP * (Z; W) = z∈Ω Z w∈Ω W P * (z, w) log P * (z, w) P * (z)P * (w) , HP * (Z) = z∈Ω Z P * (z) log(P * (z)).So, if α denotes the difference between the two scores, we have that:α = S(B|PaG(B) ∪ {A}) -S(B|PaG(B)) = |D| × [IP * (B; PaG(B) ∪ {A}) -IP * (B; PaG(B))] -δ log |D|/2, with δ = dim(B|PaG(B) ∪ {A}) -dim(B|PaG(B)). When |D| → ∞,log |D| is infinitely smaller than |D|. Hence α ≤ 0 if and only if IP * (B; PaG(B) ∪ {A}) ≤ IP * (B; PaG(B)). But, for every probability distribution Q and every X, Y, Z, it always holds that IQ(X; Z ∪ {Y }) ≥ IQ(X; Z), with equality only if X⊥ ⊥QY |Z. Hence α ≤ 0 if and only if IP * (B; PaG(B) ∪ {A}) = IP * (B; PaG(B)). This implies that A⊥ ⊥P * B|PaG(B).■ Let XO and XH be two disjoint sets of random variables and let X = XO ∪ XH . Let D * be a dataset generated by some distribution P * over X for which there exists a perfect map G * = (X, E), and let D be the projection of D * over XO. Let G be a DAG maximizing the BIC score over D. Then, as |D| → ∞, for any A, B ∈ XO, if G * contains Arc A → B, then Graph G contains either Arc A → B or B → A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Such a C exists by the hypotheses at the beginning of the Proposition. By Corollary 1, G contains an arc between A and C. As for an arc between B and C, assume that there exists none in G. Note that Trail CCB = ⟨C, A, L, B⟩ is always active in G * given any Z ⊆ XO\{B, C} that contains A. So B̸ ⊥ ⊥ P * C|Z since G * is a perfect map. Now, as shown at the beginning of the proof, there exists either i) no directed path from B to C in G; or ii) no directed path from C to B in G. If Case i) obtains, by Lemma 1, C⊥ ⊥P * B|PaG(B), which is impossible since A ∈ PaG(B) (according to the preceding paragraph), so that Trail CCB is active in G * given Z = PaG(B), a contradiction (since it is equivalent to C̸ ⊥ ⊥ P * B|PaG(B)). If Case ii) obtains (but not Case i)), then the arc added previously between A and C could not be C → A because, since Case i) does not obtain, there exists a directed path from B to C, hence also a directed path from A to C (since G contains Arc A → B). So Arc C → A would create a directed cycle. Hence, A ∈ PaG(C). Now, by Lemma 1, B⊥ ⊥P * C|PaG(C), which is impossible since Z = PaG(C) ⊇ {A} would make Trail CCB active in G * , a contradiction. As a consequence, there must necessarily exist an arc between B and C in G. So, overall, G contains clique</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Proof of Proposition 5 .</head><label>5</label><figDesc>Let Nuvw denote the number of records in D such that U = u, V = v and W = w and let Nuw = v∈Ω V Nuvw, Nvw = u∈Ω U Nuvw and Nw = u∈Ω U v∈Ω V Nuvw. Let dim(U |V, W) and dim(U |W) denote the number of free parameters in the conditional probability tables P (U |V, W) and P (U |W) respectively, i.e., dim(U |V, W) = (|ΩU | -1) × |ΩV | × |Ω W | and dim(U |W) = (|ΩU | -1) × |Ω W |. So, we have that:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 : 4 P 1 P 2 P 1 P 4 P</head><label>541214</label><figDesc>Figure 5: The structure of a causal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The Bayesian networks corresponding to the causal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Confounders found for different dataset sizes and CBNs with 2 Boolean confounders.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Algorithm 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MIIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCI</cell></row><row><cell cols="2">CBN |D|</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>time</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell></row><row><cell></cell><cell>5000</cell><cell>0.50</cell><cell>0.06</cell><cell>0.89</cell><cell>0.25</cell><cell cols="4">0.39 0.014 1.40 0.74</cell><cell>0.65</cell><cell>0.70</cell><cell cols="2">0.68 1.76</cell><cell>3.96</cell><cell>0.31</cell><cell>0.88</cell><cell>0.46</cell></row><row><cell></cell><cell>10000</cell><cell>0.76</cell><cell>0.18</cell><cell>0.81</cell><cell>0.38</cell><cell cols="4">0.52 0.028 1.58 0.62</cell><cell>0.72</cell><cell>0.79</cell><cell cols="2">0.75 1.86</cell><cell>2.92</cell><cell>0.39</cell><cell>0.93</cell><cell>0.55</cell></row><row><cell>child</cell><cell>20000</cell><cell>1.10</cell><cell>0.12</cell><cell>0.90</cell><cell>0.55</cell><cell>0.68</cell><cell cols="3">0.050 1.34 0.80</cell><cell>0.63</cell><cell>0.67</cell><cell cols="2">0.65 1.74</cell><cell>2.12</cell><cell>0.45</cell><cell>0.87</cell><cell>0.59</cell></row><row><cell></cell><cell>50000</cell><cell>1.42</cell><cell>0.14</cell><cell>0.91</cell><cell>0.71</cell><cell>0.80</cell><cell cols="3">0.103 1.40 0.68</cell><cell>0.67</cell><cell>0.70</cell><cell cols="2">0.69 1.74</cell><cell>1.08</cell><cell>0.62</cell><cell>0.87</cell><cell>0.72</cell></row><row><cell></cell><cell cols="2">100000 1.44</cell><cell>0.14</cell><cell>0.91</cell><cell>0.72</cell><cell>0.80</cell><cell cols="3">0.190 1.46 0.56</cell><cell>0.72</cell><cell>0.73</cell><cell cols="2">0.73 1.72</cell><cell>0.68</cell><cell>0.72</cell><cell>0.86</cell><cell>0.78</cell></row><row><cell></cell><cell>5000</cell><cell>0.16</cell><cell>1.56</cell><cell>0.09</cell><cell>0.08</cell><cell cols="2">0.09 0.017</cell><cell>0.34</cell><cell>2.76</cell><cell>0.11</cell><cell>0.17</cell><cell cols="2">0.13 0.34</cell><cell>8.26</cell><cell>0.04</cell><cell>0.17</cell><cell>0.06</cell></row><row><cell></cell><cell>10000</cell><cell>0.44</cell><cell>0.04</cell><cell>0.92</cell><cell>0.22</cell><cell cols="4">0.35 0.043 1.24 1.68</cell><cell>0.42</cell><cell>0.62</cell><cell cols="2">0.50 1.54</cell><cell>5.40</cell><cell>0.22</cell><cell>0.77</cell><cell>0.34</cell></row><row><cell>water</cell><cell>20000</cell><cell>0.80</cell><cell>0.22</cell><cell>0.78</cell><cell>0.40</cell><cell>0.53</cell><cell cols="3">0.104 1.40 1.88</cell><cell>0.43</cell><cell>0.70</cell><cell cols="2">0.53 1.60</cell><cell>6.04</cell><cell>0.21</cell><cell>0.80</cell><cell>0.33</cell></row><row><cell></cell><cell>50000</cell><cell>1.16</cell><cell>0.60</cell><cell>0.66</cell><cell>0.58</cell><cell>0.62</cell><cell cols="3">0.132 1.40 1.92</cell><cell>0.42</cell><cell>0.70</cell><cell cols="2">0.53 1.56</cell><cell>7.86</cell><cell>0.17</cell><cell>0.78</cell><cell>0.27</cell></row><row><cell></cell><cell cols="2">100000 1.22</cell><cell>1.04</cell><cell>0.54</cell><cell>0.61</cell><cell>0.57</cell><cell cols="3">0.248 1.44 2.78</cell><cell>0.34</cell><cell>0.72</cell><cell cols="2">0.46 1.52</cell><cell>8.32</cell><cell>0.15</cell><cell>0.76</cell><cell>0.26</cell></row><row><cell></cell><cell>5000</cell><cell>0.20</cell><cell>0.20</cell><cell>0.50</cell><cell>0.10</cell><cell cols="4">0.17 0.042 1.62 4.04</cell><cell>0.29</cell><cell>0.81</cell><cell cols="2">0.42 1.88</cell><cell>8.86</cell><cell>0.18</cell><cell>0.94</cell><cell>0.30</cell></row><row><cell>insu-rance</cell><cell>10000 20000 50000</cell><cell>0.30 0.62 1.16</cell><cell>0.28 0.30 0.54</cell><cell>0.52 0.67 0.68</cell><cell>0.15 0.31 0.58</cell><cell cols="4">0.23 0.079 1.48 3.32 0.42 0.172 1.66 3.88 0.63 0.373 1.78 3.48</cell><cell>0.31 0.30 0.34</cell><cell>0.74 0.83 0.89</cell><cell cols="2">0.44 1.96 0.44 1.94 0.49 1.90</cell><cell>8.78 8.44 8.66</cell><cell>0.18 0.19 0.18</cell><cell>0.98 0.97 0.95</cell><cell>0.31 0.31 0.30</cell></row><row><cell></cell><cell cols="2">100000 1.32</cell><cell>0.72</cell><cell>0.65</cell><cell>0.66</cell><cell>0.65</cell><cell>0.791</cell><cell>1.76</cell><cell>3.00</cell><cell>0.37</cell><cell>0.88</cell><cell>0.52</cell><cell>1.76</cell><cell>8.20</cell><cell>0.18</cell><cell>0.88</cell><cell>0.29</cell></row><row><cell></cell><cell>5000</cell><cell>0.36</cell><cell>0.40</cell><cell>0.47</cell><cell>0.18</cell><cell cols="4">0.26 0.058 1.24 1.48</cell><cell>0.46</cell><cell>0.62</cell><cell cols="2">0.53 1.48</cell><cell>4.74</cell><cell>0.24</cell><cell>0.74</cell><cell>0.36</cell></row><row><cell></cell><cell>10000</cell><cell>0.86</cell><cell>0.58</cell><cell>0.60</cell><cell>0.43</cell><cell cols="4">0.50 0.119 1.50 0.98</cell><cell>0.60</cell><cell>0.75</cell><cell cols="2">0.67 1.56</cell><cell>4.14</cell><cell>0.27</cell><cell>0.78</cell><cell>0.41</cell></row><row><cell>alarm</cell><cell>20000</cell><cell cols="2">1.12 0.66</cell><cell>0.63</cell><cell>0.56</cell><cell cols="4">0.59 0.180 1.50 0.52</cell><cell>0.74</cell><cell>0.75</cell><cell cols="2">0.75 1.72</cell><cell>3.42</cell><cell>0.33</cell><cell>0.86</cell><cell>0.48</cell></row><row><cell></cell><cell>50000</cell><cell cols="2">1.36 1.16</cell><cell>0.54</cell><cell>0.68</cell><cell cols="4">0.60 0.385 1.64 1.04</cell><cell>0.61</cell><cell>0.82</cell><cell cols="2">0.70 1.68</cell><cell>2.92</cell><cell>0.37</cell><cell>0.84</cell><cell>0.51</cell></row><row><cell></cell><cell cols="3">100000 1.44 1.18</cell><cell>0.55</cell><cell>0.72</cell><cell cols="3">0.62 0.744 1.62</cell><cell>0.76</cell><cell>0.68</cell><cell>0.81</cell><cell cols="2">0.74 1.70</cell><cell>2.42</cell><cell>0.41</cell><cell>0.85</cell><cell>0.56</cell></row><row><cell></cell><cell>5000</cell><cell>0.02</cell><cell>0.00</cell><cell>1.00</cell><cell>0.01</cell><cell cols="4">0.02 0.013 0.60 3.14</cell><cell>0.16</cell><cell>0.30</cell><cell cols="2">0.21 1.58</cell><cell>20.70</cell><cell>0.07</cell><cell>0.79</cell><cell>0.13</cell></row><row><cell></cell><cell>10000</cell><cell>0.08</cell><cell>0.00</cell><cell>1.00</cell><cell>0.04</cell><cell cols="4">0.08 0.032 0.74 2.30</cell><cell>0.24</cell><cell>0.37</cell><cell cols="2">0.29 1.78</cell><cell>19.72</cell><cell>0.08</cell><cell>0.89</cell><cell>0.15</cell></row><row><cell>barley</cell><cell>20000</cell><cell>0.36</cell><cell>0.00</cell><cell>1.00</cell><cell>0.18</cell><cell cols="4">0.31 0.076 1.02 1.84</cell><cell>0.36</cell><cell>0.51</cell><cell cols="2">0.42 1.86</cell><cell>16.56</cell><cell>0.10</cell><cell>0.93</cell><cell>0.18</cell></row><row><cell></cell><cell>50000</cell><cell>0.74</cell><cell>0.00</cell><cell>1.00</cell><cell>0.37</cell><cell>0.54</cell><cell cols="3">0.229 1.34 3.72</cell><cell>0.26</cell><cell>0.67</cell><cell cols="2">0.38 1.86</cell><cell>14.86</cell><cell>0.11</cell><cell>0.93</cell><cell>0.20</cell></row><row><cell></cell><cell cols="2">100000 0.96</cell><cell>0.08</cell><cell>0.92</cell><cell>0.48</cell><cell>0.63</cell><cell cols="3">0.511 1.36 2.96</cell><cell>0.31</cell><cell>0.68</cell><cell cols="2">0.43 1.84</cell><cell>13.70</cell><cell>0.12</cell><cell>0.92</cell><cell>0.21</cell></row></table><note><p>constraint is only due to our use of CPBayes, not to Algorithm 1. But the lower bound is due to Proposition 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.02 2.44 2.54 19.46 2.00 3.18 4.36 3.42 15.00 4.96 0.22 8.82 8.00 10000 23.10 2.52 1.52 1.86 3.12 20.72 1.18 3.28 3.82 3.42 15.64 3.40 0.06 9.90 6.00 20000 22.88 1.84 2.22 2.06 3.62 19.94 1.44 2.84 4.78 3.82 16.70 2.60 0.00 9.70 4.44 50000 22.98 1.22 2.84 1.96 4.34 20.36 1.30 2.98 4.36 3.74 20.80 1.54 0.00 6.66 2.54 100000 23.20 1.14 2.88 1.78 4.50 21.28 1.14 3.04 3.54 3.72 22.38 1.22 0.00 5.40</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Algorithm 1</cell><cell></cell><cell>MIIC</cell><cell></cell><cell></cell><cell></cell><cell>FCI</cell><cell></cell><cell></cell></row><row><cell>CBN</cell><cell>|D|</cell><cell>ok</cell><cell>miss rev. type xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell></row><row><cell>child</cell><cell>5000</cell><cell>22.46</cell><cell cols="9">3.08 11.88</cell></row><row><cell></cell><cell>5000</cell><cell cols="5">16.72 40.18 4.72 8.38 4.60 15.48 40.20 7.26 7.06</cell><cell cols="3">9.38 13.46 45.54 1.38</cell><cell cols="2">9.62 11.50</cell></row><row><cell></cell><cell>10000</cell><cell cols="5">19.00 36.98 7.58 6.44 5.96 17.82 37.78 8.04 6.36</cell><cell cols="3">9.40 17.90 43.02 1.02</cell><cell cols="2">8.06 12.82</cell></row><row><cell>water</cell><cell>20000</cell><cell cols="4">21.12 36.64 6.82 5.42 8.72 21.84 38.10 4.78</cell><cell cols="4">5.28 11.36 17.76 43.76 0.52</cell><cell cols="2">7.96 16.88</cell></row><row><cell></cell><cell>50000</cell><cell cols="8">26.22 32.48 7.20 4.10 7.02 23.76 34.70 6.12 5.42 10.58 25.12 38.66 0.76</cell><cell cols="2">5.46 16.42</cell></row><row><cell></cell><cell cols="9">100000 29.94 29.42 7.10 3.54 7.06 27.44 32.32 5.34 4.90 12.54 26.80 37.68 0.52</cell><cell cols="2">5.00 17.26</cell></row><row><cell></cell><cell>5000</cell><cell cols="10">32.38 14.44 2.46 6.72 3.82 29.20 12.70 3.60 10.50 12.44 23.90 21.60 0.46 10.04 18.06</cell></row><row><cell></cell><cell>10000</cell><cell cols="10">34.68 12.22 2.38 6.72 4.06 29.86 11.70 3.52 10.92 11.46 25.76 18.64 0.30 11.30 17.72</cell></row><row><cell>insurance</cell><cell>20000</cell><cell cols="10">38.76 9.20 3.34 4.70 5.04 30.42 10.00 3.74 11.84 13.12 27.98 16.18 0.14 11.70 17.08</cell></row><row><cell></cell><cell>50000</cell><cell cols="3">41.08 7.58 4.42 2.92 6.42 31.32</cell><cell cols="7">8.34 4.00 12.34 12.94 29.42 13.92 0.10 12.56 17.70</cell></row><row><cell></cell><cell cols="4">100000 40.34 7.08 4.94 3.64 7.48 31.46</cell><cell cols="7">7.88 3.88 12.78 13.40 31.36 12.64 0.12 11.88 16.96</cell></row><row><cell></cell><cell>5000</cell><cell cols="4">38.10 6.42 3.90 1.58 5.08 40.04 5.54 3.10</cell><cell>1.32</cell><cell cols="3">7.72 37.34 10.36 0.58</cell><cell cols="2">1.72 10.40</cell></row><row><cell></cell><cell>10000</cell><cell cols="10">39.08 4.52 4.96 1.44 5.88 40.92 4.46 3.30 1.32 6.72 39.48 8.88 0.46 1.18 9.22</cell></row><row><cell>alarm</cell><cell>20000</cell><cell>40.64</cell><cell cols="4">3.08 5.02 1.26 5.82 42.48 3.54 3.32 0.66</cell><cell cols="2">5.92 42.90</cell><cell>6.06 0.18</cell><cell cols="2">0.86 7.60</cell></row><row><cell></cell><cell>50000</cell><cell>40.28</cell><cell cols="6">2.62 5.86 1.24 7.46 42.46 3.02 3.60 0.92 8.16 43.44</cell><cell cols="3">5.48 0.18 0.90 6.58</cell></row><row><cell></cell><cell cols="5">100000 40.72 1.84 6.02 1.42 7.72 43.04 2.44 3.66</cell><cell>0.86</cell><cell cols="2">7.76 44.66</cell><cell>4.18 0.10</cell><cell cols="2">1.06 5.60</cell></row><row><cell></cell><cell>5000</cell><cell cols="10">41.46 34.66 3.02 8.86 6.32 51.12 25.14 4.32 7.42 13.74 42.48 38.30 2.08 5.14 42.46</cell></row><row><cell></cell><cell>10000</cell><cell cols="10">46.30 29.10 2.82 9.78 5.40 56.10 21.52 4.44 5.94 13.34 47.48 33.70 1.38 5.44 40.72</cell></row><row><cell>barley</cell><cell>20000</cell><cell cols="4">52.08 22.70 3.74 9.48 4.64 60.26 17.90 4.80</cell><cell cols="4">5.04 13.44 53.72 27.32 1.68</cell><cell cols="2">5.28 34.70</cell></row><row><cell></cell><cell>50000</cell><cell cols="4">57.74 17.96 4.28 8.02 4.54 67.24 12.96 4.42</cell><cell cols="4">3.38 20.72 59.74 21.26 1.34</cell><cell cols="2">5.66 31.02</cell></row><row><cell></cell><cell cols="5">100000 62.52 14.80 5.00 5.68 4.48 69.72 12.16 3.96</cell><cell cols="4">2.16 21.76 62.14 19.16 1.02</cell><cell cols="2">5.68 29.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of the learnt CPDAGs with those of the CBNs (with confounders) that generated the datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Proof of Corollary 1. Assume that, for some pair A, B ∈ XO such that there exists an arc between A and B in G * , Graph G contains neither Arc A → B nor Arc B → A. It is impossible that G contains both a directed path from A to B and another one from B to A because the concatenation of these paths would be a directed cycle. Without loss of generality, assume that there exists no directed path from B to A in G, then, byLemma  1, A⊥ ⊥P * B|PaG(B). As G * is a perfect map, this implies that ⟨A ⊥G * B|PaG(B)⟩, a contradiction since G * contains an arc between A and B. ■ Proof of Proposition 1. Let L be a variable in XH and let A, B be its children. Let G be a graph maximizing the BIC score over D. First, let us prove that there must exist an arc between A and B in G. Assume the contrary. It is impossible that there exists both a directed path CAB from A to B in G and a directed path CBA from B to A in G because their concatenation would be a directed cycle, a contradiction since G is a DAG. Without loss of generality, assume that there exists no directed path from B to A in G. Then, as, by hypothesis, there exists no arc between A and B, by Lemma 1, A⊥ ⊥P * B|PaG(B). But this is impossible because, whatever the set Z ⊆ XO\{A, B}, Trail ⟨A, L, B⟩ is active given Z in G</figDesc><table /><note><p><p>* , so that ⟨A ̸ ⊥G * B|PaG(B)⟩, which contradicts A⊥ ⊥P * B|PaG(B) since G * is a perfect map. So, there must exist an arc between A and B in G. In the rest of the proof, without loss of generality, assume that it contains Arc A → B.</p>Let C ∈ PaG * (A) ∩ XO.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Confounders learnt for Insurance with different numbers of Boolean latent confounders.</figDesc><table><row><cell>|X H | |D|</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>time</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell></row><row><cell>5000</cell><cell cols="2">0.20 0.20</cell><cell>0.50</cell><cell>0.10</cell><cell cols="4">0.17 0.042 1.62 4.04</cell><cell>0.29</cell><cell>0.81</cell><cell cols="2">0.42 1.88</cell><cell>8.86</cell><cell>0.18</cell><cell>0.94</cell><cell>0.30</cell></row><row><cell></cell><cell>0.30</cell><cell>0.28</cell><cell>0.52</cell><cell>0.15</cell><cell cols="4">0.23 0.079 1.48 3.32</cell><cell>0.31</cell><cell>0.74</cell><cell cols="2">0.44 1.96</cell><cell>8.78</cell><cell>0.18</cell><cell>0.98</cell><cell>0.31</cell></row><row><cell>2</cell><cell>0.62</cell><cell>0.30</cell><cell>0.67</cell><cell>0.31</cell><cell cols="4">0.42 0.172 1.66 3.88</cell><cell>0.30</cell><cell>0.83</cell><cell cols="2">0.44 1.94</cell><cell>8.44</cell><cell>0.19</cell><cell>0.97</cell><cell>0.31</cell></row><row><cell></cell><cell>1.16</cell><cell>0.54</cell><cell>0.68</cell><cell>0.58</cell><cell>0.63</cell><cell cols="3">0.373 1.78 3.48</cell><cell>0.34</cell><cell>0.89</cell><cell cols="2">0.49 1.90</cell><cell>8.66</cell><cell>0.18</cell><cell>0.95</cell><cell>0.30</cell></row><row><cell cols="3">100000 1.32 0.72</cell><cell>0.65</cell><cell>0.66</cell><cell>0.65</cell><cell>0.791</cell><cell cols="2">1.76 3.00</cell><cell>0.37</cell><cell>0.88</cell><cell>0.52</cell><cell>1.76</cell><cell>8.20</cell><cell>0.18</cell><cell>0.88</cell><cell>0.29</cell></row><row><cell>5000</cell><cell cols="2">0.18 0.16</cell><cell>0.53</cell><cell>0.06</cell><cell cols="4">0.11 0.047 2.04 5.20</cell><cell>0.28</cell><cell>0.68</cell><cell cols="2">0.40 2.62</cell><cell>9.84</cell><cell>0.21</cell><cell>0.87</cell><cell>0.34</cell></row><row><cell></cell><cell>0.48</cell><cell>0.20</cell><cell>0.71</cell><cell>0.16</cell><cell cols="4">0.26 0.088 2.38 5.00</cell><cell>0.32</cell><cell>0.79</cell><cell cols="2">0.46 2.86</cell><cell>10.10</cell><cell>0.22</cell><cell>0.95</cell><cell>0.36</cell></row><row><cell>3</cell><cell>0.96</cell><cell>0.48</cell><cell>0.67</cell><cell>0.32</cell><cell>0.43</cell><cell cols="3">0.207 2.08 6.22</cell><cell>0.25</cell><cell>0.69</cell><cell cols="2">0.37 2.84</cell><cell>9.90</cell><cell>0.22</cell><cell>0.95</cell><cell>0.36</cell></row><row><cell></cell><cell>1.50</cell><cell>0.52</cell><cell>0.74</cell><cell>0.50</cell><cell>0.60</cell><cell cols="3">0.469 2.26 5.96</cell><cell>0.27</cell><cell>0.75</cell><cell cols="2">0.40 2.78</cell><cell>8.78</cell><cell>0.24</cell><cell>0.93</cell><cell>0.38</cell></row><row><cell cols="3">100000 1.74 0.76</cell><cell>0.70</cell><cell>0.58</cell><cell>0.63</cell><cell cols="3">1.383 2.28 5.24</cell><cell>0.30</cell><cell>0.76</cell><cell cols="2">0.43 2.60</cell><cell>8.30</cell><cell>0.24</cell><cell>0.87</cell><cell>0.37</cell></row><row><cell>5000</cell><cell cols="2">0.20 0.24</cell><cell>0.45</cell><cell>0.05</cell><cell cols="4">0.09 0.057 2.62 6.66</cell><cell>0.28</cell><cell>0.66</cell><cell cols="2">0.39 3.56</cell><cell>12.60</cell><cell>0.22</cell><cell>0.89</cell><cell>0.35</cell></row><row><cell></cell><cell>0.40</cell><cell>0.32</cell><cell>0.56</cell><cell>0.10</cell><cell cols="4">0.17 0.104 2.70 6.08</cell><cell>0.31</cell><cell>0.68</cell><cell cols="2">0.42 3.66</cell><cell>11.24</cell><cell>0.25</cell><cell>0.92</cell><cell>0.39</cell></row><row><cell>4</cell><cell>1.08</cell><cell>0.34</cell><cell>0.76</cell><cell>0.27</cell><cell cols="4">0.40 0.221 3.04 6.04</cell><cell>0.33</cell><cell>0.76</cell><cell cols="2">0.46 3.70</cell><cell>11.04</cell><cell>0.25</cell><cell>0.93</cell><cell>0.39</cell></row><row><cell></cell><cell>1.96</cell><cell>0.96</cell><cell>0.67</cell><cell>0.49</cell><cell>0.57</cell><cell cols="3">0.588 2.94 6.72</cell><cell>0.30</cell><cell>0.74</cell><cell cols="2">0.43 3.68</cell><cell>10.20</cell><cell>0.27</cell><cell>0.92</cell><cell>0.41</cell></row><row><cell cols="3">100000 2.16 1.20</cell><cell>0.64</cell><cell>0.54</cell><cell>0.59</cell><cell cols="3">1.748 2.90 6.72</cell><cell>0.30</cell><cell>0.73</cell><cell cols="2">0.43 3.38</cell><cell>8.74</cell><cell>0.28</cell><cell>0.85</cell><cell>0.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Algorithm 1</cell><cell></cell><cell></cell><cell></cell><cell>MIIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCI</cell><cell></cell><cell></cell></row><row><cell>|Ω L i | |D|</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>time</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell><cell>ok</cell><cell>¬ok</cell><cell cols="2">prec. recall</cell><cell>F1</cell></row><row><cell>5000</cell><cell>0.20</cell><cell>0.20</cell><cell>0.50</cell><cell>0.10</cell><cell cols="4">0.17 0.042 1.62 4.04</cell><cell>0.29</cell><cell>0.81</cell><cell cols="2">0.42 1.88</cell><cell>8.86</cell><cell>0.18</cell><cell>0.94</cell><cell>0.30</cell></row><row><cell></cell><cell>0.30</cell><cell>0.28</cell><cell>0.52</cell><cell>0.15</cell><cell cols="4">0.23 0.079 1.48 3.32</cell><cell>0.31</cell><cell>0.74</cell><cell cols="2">0.44 1.96</cell><cell>8.78</cell><cell>0.18</cell><cell>0.98</cell><cell>0.31</cell></row><row><cell>2</cell><cell>0.62</cell><cell>0.30</cell><cell>0.67</cell><cell>0.31</cell><cell cols="4">0.42 0.172 1.66 3.88</cell><cell>0.30</cell><cell>0.83</cell><cell cols="2">0.44 1.94</cell><cell>8.44</cell><cell>0.19</cell><cell>0.97</cell><cell>0.31</cell></row><row><cell></cell><cell>1.16</cell><cell>0.54</cell><cell>0.68</cell><cell>0.58</cell><cell>0.63</cell><cell cols="3">0.373 1.78 3.48</cell><cell>0.34</cell><cell>0.89</cell><cell cols="2">0.49 1.90</cell><cell>8.66</cell><cell>0.18</cell><cell>0.95</cell><cell>0.30</cell></row><row><cell cols="2">100000 1.32</cell><cell>0.72</cell><cell>0.65</cell><cell>0.66</cell><cell>0.65</cell><cell>0.791</cell><cell>1.76</cell><cell>3.00</cell><cell>0.37</cell><cell>0.88</cell><cell>0.52</cell><cell>1.76</cell><cell>8.20</cell><cell>0.18</cell><cell>0.88</cell><cell>0.29</cell></row><row><cell>5000</cell><cell>0.10</cell><cell>0.20</cell><cell>0.33</cell><cell>0.05</cell><cell cols="4">0.09 0.048 1.18 5.16</cell><cell>0.19</cell><cell>0.59</cell><cell cols="2">0.28 1.86</cell><cell>10.38</cell><cell>0.15</cell><cell>0.93</cell><cell>0.26</cell></row><row><cell></cell><cell>0.26</cell><cell>0.28</cell><cell>0.48</cell><cell>0.13</cell><cell cols="4">0.20 0.091 1.14 3.48</cell><cell>0.25</cell><cell>0.57</cell><cell cols="2">0.34 1.74</cell><cell>9.22</cell><cell>0.16</cell><cell>0.87</cell><cell>0.27</cell></row><row><cell>3</cell><cell>0.62</cell><cell>0.48</cell><cell>0.56</cell><cell>0.31</cell><cell>0.40</cell><cell cols="3">0.181 1.50 4.46</cell><cell>0.25</cell><cell>0.75</cell><cell cols="2">0.38 1.68</cell><cell>8.86</cell><cell>0.16</cell><cell>0.84</cell><cell>0.27</cell></row><row><cell></cell><cell>0.88</cell><cell>0.62</cell><cell>0.59</cell><cell>0.44</cell><cell>0.50</cell><cell cols="3">0.420 1.48 4.16</cell><cell>0.26</cell><cell>0.74</cell><cell cols="2">0.39 1.68</cell><cell>8.82</cell><cell>0.16</cell><cell>0.84</cell><cell>0.27</cell></row><row><cell cols="2">100000 1.10</cell><cell>0.78</cell><cell>0.59</cell><cell>0.55</cell><cell>0.57</cell><cell cols="3">1.108 1.32 4.46</cell><cell>0.23</cell><cell>0.66</cell><cell cols="2">0.34 1.54</cell><cell>7.18</cell><cell>0.18</cell><cell>0.77</cell><cell>0.29</cell></row><row><cell>5000</cell><cell>0.20</cell><cell>0.24</cell><cell>0.45</cell><cell>0.10</cell><cell cols="4">0.16 0.051 1.34 5.46</cell><cell>0.20</cell><cell>0.67</cell><cell cols="2">0.30 1.86</cell><cell>11.32</cell><cell>0.14</cell><cell>0.93</cell><cell>0.25</cell></row><row><cell></cell><cell>0.28</cell><cell>0.26</cell><cell>0.52</cell><cell>0.14</cell><cell cols="4">0.22 0.097 1.42 4.80</cell><cell>0.23</cell><cell>0.71</cell><cell cols="2">0.35 1.82</cell><cell>10.10</cell><cell>0.15</cell><cell>0.91</cell><cell>0.26</cell></row><row><cell>4</cell><cell>0.50</cell><cell>0.36</cell><cell>0.58</cell><cell>0.25</cell><cell cols="4">0.35 0.197 1.42 4.58</cell><cell>0.24</cell><cell>0.71</cell><cell cols="2">0.36 1.66</cell><cell>9.74</cell><cell>0.15</cell><cell>0.83</cell><cell>0.25</cell></row><row><cell></cell><cell>0.84</cell><cell>0.64</cell><cell>0.57</cell><cell>0.42</cell><cell>0.48</cell><cell cols="3">0.442 1.32 5.50</cell><cell>0.19</cell><cell>0.66</cell><cell cols="2">0.30 1.64</cell><cell>8.82</cell><cell>0.16</cell><cell>0.82</cell><cell>0.26</cell></row><row><cell cols="2">100000 1.08</cell><cell>0.60</cell><cell>0.64</cell><cell>0.54</cell><cell>0.59</cell><cell cols="3">1.121 1.52 6.50</cell><cell>0.19</cell><cell>0.76</cell><cell cols="2">0.30 1.62</cell><cell>8.44</cell><cell>0.16</cell><cell>0.81</cell><cell>0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Confounders learnt for Insurance with latent confounders with different domain sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>compares the CPDAGs learnt by Algorithm 1, MIIC and FCI with those of the CBNs that generated the datasets. These CBNs correspond to Insurance to which we added 2 to 4 Boolean latent confounders. As for the results presented in the paper, Algorithm 1 outperforms MIIC and FCI in terms of the number of arcs/edges learnt correctly as well as in terms of the incorrect types of edges/arcs (undirected edges (resp. arcs) of the generating CBN learnt as arcs (resp. edges)) and in terms of the edges/arcs learnt in excess (the original CBN contains neither an edge nor an arc for the pairs of nodes concerned). FCI outperforms the other algorithms in terms of arcs reversed. MIIC is the best in terms of missed arcs/edges.Table reports the results of experiments in which the datasets were generated from Insurance with two latent confounders. Here, the domain sizes of these confounders vary from 2 to 4.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>.44 2.46 6.72 3.82 29.20 12.70 3.60 10.50 12.44 23.90 21.60 0.46 10.04 18.06 10000 34.68 12.22 2.38 6.72 4.06 29.86 11.70 3.52 10.92 11.46 25.76 18.64 0.30 11.30 17.72 20000 38.76 9.20 3.34 4.70 5.04 30.42 10.00 3.74 11.84 13.12 27.98 16.18 0.14 11.70 17.08 50000 41.08 7.58 4.42 2.92 6.42 31.32 8.34 4.00 12.34 12.94 29.42 13.92 0.10 12.56 17.70 100000 40.34 7.08 4.94 3.64 7.48 31.46 7.88 3.88 12.78 13.40 31.36 12.64 0.12 11.88 16.96 Comparisons of the learnt CPDAGs with those of the CBNs (with confounders) that generated the datasets in function of the number of the CBN's latent confounders. .44 2.46 6.72 3.82 29.20 12.70 3.60 10.50 12.44 23.90 21.60 0.46 10.04 18.06 10000 34.68 12.22 2.38 6.72 4.06 29.86 11.70 3.52 10.92 11.46 25.76 18.64 0.30 11.30 17.72 20000 38.76 9.20 3.34 4.70 5.04 30.42 10.00 3.74 11.84 13.12 27.98 16.18 0.14 11.70 17.08 50000 41.08 7.58 4.42 2.92 6.42 31.32 8.34 4.00 12.34 12.94 29.42 13.92 0.10 12.56 17.70 100000 40.34 7.08 4.94 3.64 7.48 31.46 7.88 3.88 12.78 13.40 31.36 12.64 0.12 11.88 16.96 3 5000 34.74 13.78 1.76 5.72 2.90 28.76 11.58 3.30 12.36 15.88 23.02 21.40 0.64 10.94 21.04 10000 38.00 11.86 1.96 4.18 3.58 29.22 11.04 3.04 12.70 12.42 25.84 18.54 0.22 11.40 18.90 20000 40.16 9.26 3.16 3.42 5.24 30.20 9.20 3.62 12.98 15.58 27.86 16.16 0.28 11.70 18.36 50000 41.76 7.72 3.92 2.60 6.54 30.48 8.12 3.66 13.74 16.40 29.42 14.24 0.22 12.12 18.24 100000 40.84 7.32 4.42 3.42 8.04 31.68 7.42 3.36 13.54 18.24 31.98 11.40 0.12 12.50 15.52</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Algorithm 1</cell><cell></cell><cell></cell><cell>MIIC</cell><cell></cell><cell></cell><cell></cell><cell>FCI</cell><cell></cell><cell></cell></row><row><cell>|X H |</cell><cell>|D|</cell><cell>ok</cell><cell>miss rev. type</cell><cell>xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell></row><row><cell cols="11">2 32.38 143 5000 5000 33.52 15.46 1.70 7.32 3.70 31.94 12.08 5.24 8.74 17.16 27.34 21.44 1.60 10000 34.72 13.38 2.54 7.36 4.86 33.80 10.00 5.74 8.46 17.38 29.62 19.08 1.20 20000 37.84 10.08 3.36 6.72 6.88 32.64 9.14 5.58 10.64 21.94 31.94 16.08 0.84</cell><cell cols="2">7.62 20.08 8.10 20.62 9.14 20.12</cell></row><row><cell></cell><cell>50000</cell><cell>40.42</cell><cell cols="3">8.54 4.28 4.76 8.36 35.26</cell><cell cols="7">7.80 4.92 10.02 23.24 34.00 13.18 0.72 10.10 18.20</cell></row><row><cell></cell><cell>100000</cell><cell cols="4">41.76 7.66 4.54 4.04 10.10 35.70</cell><cell cols="5">7.66 4.12 10.52 21.32 35.84 11.82 0.42</cell><cell cols="2">9.92 17.74</cell></row><row><cell></cell><cell>5000</cell><cell cols="2">33.86 16.08 1.84 8.22</cell><cell cols="9">4.34 33.66 11.28 6.56 8.50 20.42 29.10 23.06 1.80 6.04 25.46</cell></row><row><cell></cell><cell>10000</cell><cell cols="11">34.44 14.52 2.36 8.68 5.72 35.54 9.96 6.36 8.14 21.82 32.80 19.30 1.54 6.36 22.80</cell></row><row><cell>4</cell><cell>20000</cell><cell cols="4">38.54 11.78 3.40 6.28 7.74 36.70</cell><cell cols="5">8.40 5.70 9.20 22.66 33.66 17.00 0.64</cell><cell cols="2">8.70 22.62</cell></row><row><cell></cell><cell>50000</cell><cell>40.34</cell><cell cols="3">9.50 4.68 5.48 11.74 36.04</cell><cell cols="5">7.98 5.98 10.00 26.80 36.60 14.04 0.48</cell><cell cols="2">8.88 21.30</cell></row><row><cell></cell><cell cols="5">100000 37.84 8.94 5.38 7.84 13.60 37.94</cell><cell cols="7">7.34 6.96 7.76 29.02 39.92 12.28 1.04 6.76 19.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Algorithm 1</cell><cell></cell><cell></cell><cell>MIIC</cell><cell></cell><cell></cell><cell></cell><cell>FCI</cell><cell></cell><cell></cell></row><row><cell>|Ω L i |</cell><cell>|D|</cell><cell>ok</cell><cell cols="2">miss rev. type xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell><cell>ok</cell><cell>miss rev.</cell><cell>type</cell><cell>xs</cell></row><row><cell cols="13">2 32.38 144 5000 5000 34.22 13.28 1.88 6.62 3.00 27.28 11.42 3.80 13.50 16.38 22.18 22.06 0.68 11.08 22.90 10000 39.72 10.86 2.04 3.38 3.44 30.38 9.70 3.64 12.28 15.70 25.20 18.12 0.38 12.30 20.52 20000 39.92 9.06 2.94 4.08 5.08 29.80 8.44 4.00 13.76 17.18 26.92 16.22 0.24 12.62 20.18</cell></row><row><cell></cell><cell>50000</cell><cell>42.38</cell><cell cols="3">7.86 3.88 1.88 7.08 30.58</cell><cell cols="7">7.78 3.70 13.94 20.40 29.52 13.28 0.16 13.04 18.46</cell></row><row><cell></cell><cell cols="2">100000 42.24</cell><cell cols="3">6.74 4.26 2.76 7.92 30.76</cell><cell cols="7">6.52 4.64 14.08 25.40 31.16 11.96 0.14 12.74 17.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of the learnt CPDAGs with those of the CBNs (with 2 latent confounders) that generated the datasets, in function of the domain sizes of the CBN's latent confounders.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ok</cell><cell></cell><cell></cell><cell>¬ok</cell><cell></cell><cell></cell><cell>precision</cell><cell></cell><cell></cell><cell>recall</cell><cell></cell><cell></cell><cell>F1</cell><cell></cell><cell></cell><cell>time</cell></row><row><cell cols="2">CBN |D|</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell></row><row><cell></cell><cell>5000</cell><cell cols="17">0.50 0.50 0.50 0.06 0.06 0.06 0.89 0.89 0.89 0.25 0.25 0.25 0.39 0.39 0.39 0.014 0.013</cell><cell>0.016</cell></row><row><cell></cell><cell>10000</cell><cell cols="17">0.76 0.76 0.76 0.18 0.18 0.18 0.81 0.81 0.81 0.38 0.38 0.38 0.52 0.52 0.52 0.028 0.029</cell><cell>0.027</cell></row><row><cell>child</cell><cell>20000</cell><cell cols="17">1.10 1.10 1.10 0.12 0.12 0.12 0.90 0.90 0.90 0.55 0.55 0.55 0.68 0.68 0.68 0.050 0.068</cell><cell>0.059</cell></row><row><cell></cell><cell>50000</cell><cell cols="17">1.42 1.44 1.44 0.14 0.12 0.12 0.91 0.92 0.92 0.71 0.72 0.72 0.80 0.81 0.81 0.103 0.119</cell><cell>0.130</cell></row><row><cell></cell><cell cols="18">100000 1.44 1.44 1.44 0.14 0.14 0.14 0.91 0.91 0.91 0.72 0.72 0.72 0.80 0.80 0.80 0.190 0.223</cell><cell>0.225</cell></row><row><cell></cell><cell>5000</cell><cell cols="17">0.16 0.16 0.16 1.56 1.56 1.56 0.09 0.09 0.09 0.08 0.08 0.08 0.09 0.09 0.09 0.017 0.022</cell><cell>0.016</cell></row><row><cell></cell><cell>10000</cell><cell cols="17">0.44 0.44 0.44 0.04 0.04 0.04 0.92 0.92 0.92 0.22 0.22 0.22 0.35 0.35 0.35 0.043 0.050</cell><cell>0.050</cell></row><row><cell>water</cell><cell>20000</cell><cell cols="17">0.80 0.80 0.80 0.22 0.22 0.22 0.78 0.78 0.78 0.40 0.40 0.40 0.53 0.53 0.53 0.104 0.131</cell><cell>0.120</cell></row><row><cell></cell><cell>50000</cell><cell cols="17">1.16 1.16 1.16 0.60 0.60 0.60 0.66 0.66 0.66 0.58 0.58 0.58 0.62 0.62 0.62 0.132 0.157</cell><cell>0.159</cell></row><row><cell></cell><cell cols="18">100000 1.22 1.22 1.22 1.04 1.04 1.04 0.54 0.54 0.54 0.61 0.61 0.61 0.57 0.57 0.57 0.248 0.333</cell><cell>0.318</cell></row><row><cell></cell><cell>5000</cell><cell cols="17">0.20 0.20 0.20 0.20 0.20 0.20 0.50 0.50 0.50 0.10 0.10 0.10 0.17 0.17 0.17 0.042 0.044</cell><cell>0.042</cell></row><row><cell>insu-rance</cell><cell>10000 20000 50000</cell><cell cols="17">0.30 0.30 0.30 0.28 0.28 0.28 0.52 0.52 0.52 0.15 0.15 0.15 0.23 0.23 0.23 0.079 0.086 0.62 0.60 0.60 0.30 0.34 0.34 0.67 0.64 0.64 0.31 0.30 0.30 0.42 0.41 0.41 0.172 0.192 1.16 1.14 1.14 0.54 0.42 0.42 0.68 0.73 0.73 0.58 0.57 0.57 0.63 0.64 0.64 0.373 0.429</cell><cell>0.090 0.198 0.419</cell></row><row><cell></cell><cell cols="18">100000 1.32 1.32 1.32 0.72 0.54 0.54 0.65 0.71 0.71 0.66 0.66 0.66 0.65 0.68 0.68 0.791 1.009</cell><cell>0.994</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Confounders found for different limits on the number of parents used by CPBayes. .88 2.88 1.96 1.96 1.96 4.34 4.36 4.36 100000 23.20 23.20 23.20 1.14 1.14 1.14 2.88 2.88 2.88 1.78 1.78 1.78 4.50 4.56 4.56 water 5000 16.72 16.72 16.72 40.18 40.18 40.18 4.72 4.72 4.72 8.38 8.38 8.38 4.60 4.60 4.60 10000 19.00 19.00 19.00 36.98 36.98 36.98 7.58 7.58 7.58 6.44 6.44 6.44 5.96 5.96 5.96 20000 21.12 21.12 21.12 36.64 36.64 36.64 6.82 6.82 6.82 5.42 5.42 5.42 8.72 8.72 8.72 50000 26.22 26.22 26.22 32.48 32.48 32.48 7.20 7.20 7.20 4.10 4.10 4.10 7.02 7.02 7.02 100000 29.94 29.94 29.94 29.42 29.42 29.42 7.10 7.10 7.10 3.54 3.54 3.54 7.06 7.06 7.06 insurance 5000 32.38 32.38 32.38 14.44 14.44 14.44 2.46 2.46 2.46 6.72 6.72 6.72 3.82 3.82 3.82 10000 34.68 34.68 34.68 12.22 12.22 12.22 2.38 2.38 2.38 6.72 6.72 6.72 4.06 4.06 4.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ok</cell><cell></cell><cell></cell><cell>miss</cell><cell></cell><cell></cell><cell>reversed</cell><cell></cell><cell></cell><cell>type</cell><cell></cell><cell></cell><cell>excess</cell><cell></cell></row><row><cell>CBN</cell><cell>|D|</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell><cell>N:4</cell><cell>N:5</cell><cell>N:6</cell></row><row><cell></cell><cell>5000</cell><cell cols="3">22.46 22.46 22.46</cell><cell>3.08</cell><cell>3.08</cell><cell>3.08</cell><cell cols="9">1.02 1.02 1.02 2.44 2.44 2.44 2.54 2.54 2.54</cell></row><row><cell></cell><cell>10000</cell><cell cols="3">23.10 23.10 23.10</cell><cell>2.52</cell><cell>2.52</cell><cell>2.52</cell><cell cols="9">1.52 1.52 1.52 1.86 1.86 1.86 3.12 3.12 3.12</cell></row><row><cell>child</cell><cell>20000</cell><cell cols="3">22.88 22.88 22.88</cell><cell>1.84</cell><cell>1.84</cell><cell>1.84</cell><cell cols="9">2.22 2.22 2.22 2.06 2.06 2.06 3.62 3.64 3.64</cell></row><row><cell></cell><cell>50000</cell><cell cols="3">22.98 22.98 22.98</cell><cell>1.22</cell><cell>1.18</cell><cell>1.18</cell><cell cols="9">2.84 206</cell></row><row><cell></cell><cell>20000</cell><cell cols="3">38.76 38.76 38.76</cell><cell>9.20</cell><cell>9.24</cell><cell>9.24</cell><cell cols="9">3.34 3.30 3.30 4.70 4.70 4.70 5.04 5.04 5.04</cell></row><row><cell></cell><cell>50000</cell><cell cols="3">41.08 41.56 41.56</cell><cell>7.58</cell><cell>7.60</cell><cell>7.60</cell><cell cols="9">4.42 4.26 4.26 2.92 2.58 2.58 6.42 6.06 6.06</cell></row><row><cell></cell><cell cols="4">100000 40.34 41.06 41.06</cell><cell>7.08</cell><cell>7.08</cell><cell>7.08</cell><cell cols="9">4.94 4.76 4.76 3.64 3.10 3.10 7.48 6.90 6.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of the learnt CPDAGs with those of the CBNs (with confounders) that generated the datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>By abuse of notation, we use interchangeably X ∈ X to denote a node in the model and its corresponding random variable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See the supplementary material, Appendix C, at the end of the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The asymptotic consistency of the BIC score was already known in other cases<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.bnlearn.com/bnrepository</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Sets Z may include other nodes than just the sets of parents of U or V . Hence, for robustness, we allowed to almost double their maximal size h.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>A directed path C = ⟨X 1 , . . . , X k ⟩ is a trail C such that, for all i ∈ {1, . . . , k -1}, the graph contains Arc X i → X i+1 .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>A simple trail is a trail in which no node appears more than once. It is well known that, for any active (resp. blocked) trail between a pair of nodes (X, Y ), there also exists a simple active (resp. blocked) trail between X and Y .</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Let G and Ξ denote the graph of Figure <ref type="figure">6</ref>.b and Set {ξA, ξB, ξC , ξD, ξE, ξF } respectively. Then the joint distribution of the Bayesian network of Figure <ref type="figure">6</ref>.a is equal to: P (X, Ξ) = X∈X P (ξX ) × P (X|PaG(X), ξX ), and marginalizing out the disturbance variables corresponds to computing:</p><p>which corresponds to the decomposition of Figure <ref type="figure">6</ref>.b. Therefore, for each variable X in X, we just need to multiply the CPT of X by the probability distribution of ξX and, then, marginalize out ξX . This results in the following computations, which can also be found in the jupyter notebook at <ref type="url" target="https://pageperso.lis-lab.fr/christophe.gonzales/research/notebooks/ecai2024.ipynb">https://pageperso.lis-lab.fr/christophe.gonzales/ research/notebooks/ecai2024.ipynb</ref>:</p><p>Hence, marginalizing out ξA, we have that:</p><p>Similarly, we have that: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic network construction using the minimum description length principle</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bouckaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European conference on symbolic and quantitative approaches to reasoning and uncertainty (ECSQARU&apos;93)</title>
		<meeting>of the European conference on symbolic and quantitative approaches to reasoning and uncertainty (ECSQARU&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transformational characterization of equivalent Bayesian network structures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3921" to="3962" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks form data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">aGrUM/pyAgrum: a toolbox to build models and algorithms for probabilistic graphical models in python</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ducamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Wuillemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Probabilistic Graphical Models (PGM&apos;20)</title>
		<meeting>of the International Conference on Probabilistic Graphical Models (PGM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conservative independence-based causal structure learning in absence of adjacency faithfulness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lemeire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meganck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cartella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1325" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient Bayesian network structure learning algorithm in the presence of deterministic relations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mabrouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jabet-Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chojnaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Artificial Intelligence (ECAI&apos;14)</title>
		<meeting>of the European Conference on Artificial Intelligence (ECAI&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Uncertainty in Artifical Intelligence (UAI&apos;95)</title>
		<meeting>of the Conference on Uncertainty in Artifical Intelligence (UAI&apos;95)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hybrid causal search algorithm for latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ogarrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Probabilistic Graphical Models (PGM&apos;16)</title>
		<meeting>of International Conference on Probabilistic Graphical Models (PGM&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adjacency-faithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI&apos;06)</title>
		<meeting>of the Conference on Uncertainty in Artificial Intelligence (UAI&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scoring ancestral graph models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>CMU-PHIL-98</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Carnegie Mellon</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ancestral graph markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the identification of causal effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno>R-290-L</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>UCLA C.S. Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Score based vs constraint based causal learning in the presence of confounders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the &quot;Causation: Foundation to Application&quot; Workshop, Uncertainty in Artificial Intelligence</title>
		<meeting>of the &quot;Causation: Foundation to Application&quot; Workshop, Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">structured set variable domains in Bayesian network structure learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Trösser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Givry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Katsirelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Principles and Practice of Constraint Programming (CP&apos;22)</title>
		<meeting>of Principles and Practice of Constraint Programming (CP&apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for large scale Markov blanket discovery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the international FLAIRS conference (FLAIRS&apos;03)</title>
		<meeting>of the international FLAIRS conference (FLAIRS&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="376" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning of Bayesian networks using constraint programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Beek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Principles and Practice of Constraint Programming (CP&apos;15)</title>
		<meeting>of Principles and Practice of Constraint Programming (CP&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An experimental analysis of anytime algorithms for Bayesian network structure learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Beek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding minimal d-separators in linear time and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Der Zander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liśkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Uncertainty in Artificial Intelligence Conference (UAI&apos;20)</title>
		<meeting>of the Conference on Uncertainty in Artificial Intelligence Conference (UAI&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Uncertainty in Artifical Intelligence (UAI&apos;90)</title>
		<meeting>of the Conference on Uncertainty in Artifical Intelligence (UAI&apos;90)</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16405</idno>
		<title level="m">Causal-learn: causal discovery in python</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
