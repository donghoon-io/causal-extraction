<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An efficient search-and-score algorithm for ancestral graphs using multivariate information scores</title>
				<funder ref="#_Uk8ZveQ #_j3Dgeu3">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS-Imperial</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-23">23 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikita</forename><surname>Lagrange</surname></persName>
							<email>nikita.lagrange@curie.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">PSL University</orgName>
								<orgName type="institution" key="instit3">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Isambert</surname></persName>
							<email>herve.isambert@curie.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">PSL University</orgName>
								<orgName type="institution" key="instit3">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An efficient search-and-score algorithm for ancestral graphs using multivariate information scores</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-23">23 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.17508v1[cs.LG]</idno>
					<note type="submission">Revised opted-in paper submitted to NeurIPS 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a greedy search-and-score algorithm for ancestral graphs, which include directed as well as bidirected edges, originating from unobserved latent variables. The normalized likelihood score of ancestral graphs is estimated in terms of multivariate information over relevant "ac-connected subsets" of vertices, C, that are connected through collider paths confined to the ancestor set of C. For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational strategy, although restricted to information contributions from ac-connected subsets containing up to two-collider paths, is shown to outperform state-of-the-art causal discovery methods on challenging benchmark datasets.</p><p>(2)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The likelihood function plays a central role in the selection of a graphical model G based on observational data D. Given N independent samples from D, the likelihood L D|G that they might have been generated by the graphical model G is given by <ref type="bibr" target="#b0">[1]</ref>,</p><formula xml:id="formula_0">L D|G = 1 Z D,G exp -N H(p, q)<label>(1)</label></formula><p>where H(p, q) =x p(x) log q(x) is the cross-entropy between the empirical probability distribution p(x) of the observed data D and the theoretical probability distribution q(x) of the model G and Z D,G a data-and model-dependent factor ensuring proper normalization condition for finite dataset. In short, Eq.1 results from the asymptotic probability that the N independent samples, x (1) , • • • , x (N ) , are drawn from the model distribution, q(x), i.e. L D|G ≡ q(x (1) , • • • , x (N ) ) = i q(x (i) ), rather than the empirical distribution, p(x). This leads to, log L D|G = i log q(x (i) ), which converges towards N x p(x) log q(x) = -N H(p, q) in the large sample size limit, N → ∞, with log Z D,G = O(log N ).</p><p>The structural constraints of the model G translate into the factorization form of the theoretical probability distribution, q(x) <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. In particular, the probability distribution of Bayesian networks (BN) factorizes in terms of conditional probabilities of each variable given its parents, as q BN (x) = i q(x i |pa Xi ), where pa Xi denote the values of the parents of node X i in G, Pa Xi . For Bayesian networks, the factors of the model distribution, q(x i |pa Xi ), can be directly estimated with the empirical conditional probabilities of each node given its parents as, q(x i |pa Xi ) ≡ p(x i |pa Xi ), leading to the well known estimation of the likelihood function in terms of conditional entropies H(X i |Pa Xi ) =x p(x i , pa Xi ) log p(x i |pa Xi ), This paper concerns the experimental setting for which some variables of the underlying Bayesian model are not observed. This frequently occurs in practice for many applications. We derive an explicit likelihood function for the class of ancestral graphs, which include directed as well as bidirected edges, arising from the presence of unobserved latent variables. Tian and Pearl 2002 <ref type="bibr" target="#b6">[7]</ref> showed that the probability distribution of such graphs factorizes into c-components including subsets of variables connected through bidirected paths (i.e. containing only bidirected edges). <ref type="bibr">Richardson 2009 [6]</ref> later proposed a refined factorization of the model distribution of the broader class of acyclic directed mixed graphs in terms of conditional probabilities over "head" and "tail" subsets of variables within each ancestrally closed subsets of vertices. However, unlike with Bayesian networks, the contributions of c-components or head-and-tail factors to the likelihood function cannot simply be estimated in terms of empirical distribution p(x), as shown below. This leaves the likelihood function of ancestral graphs difficult to estimate from empirical data, in general, although iterative methods have been developped when the data is normally distributed <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>The present paper provides an explicit decomposition of the likelihood function of ancestral graphs in terms of multivariate cross-information over relevant 'ac-connected' subsets of variables, Figs. 1., which do not rely on the head-and-tail factorization but coincide with the parametrizing sets <ref type="bibr" target="#b13">[14]</ref> derived from the head-and-tail factorization. It suggests a natural estimation of these revelant contributions to the likelihood function in terms of empirical distribution p(x). This result extends the likelihood expression of Bayesian Networks (Eq. 2) to include the effect of unobserved latent variables and enables the implementation of a greedy search-and-score algorithm for ancestral graphs. For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational strategy is shown to outperform state-of-the-art causal discovery methods on challenging benchmark datasets.</p><p>2 Theoretical results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multivariate cross-entropy and cross-information</head><p>The theoretical result of the paper (Theorem 1) is expressed in terms of multivariate cross-information derived from multivariate cross-entropies through the Inclusion-Exclusion Principle. The same expressions can be written between multivariate information and multivariate entropies by simply substituting q({x i }) with p({x i }) in the equations below and will be used to estimate the likelihood function of ancestral graphs (Proposition 3).</p><p>As recalled above, the cross-entropy between m variables, V = {X 1 , • • • , X m }, is defined as,</p><formula xml:id="formula_1">H(V ) = - {xi} p(x 1 , • • • , x m ) log q(x 1 , • • • , x m )<label>(3)</label></formula><p>where p({x i }) is the empirical joint probability distribution of the variables {X i } and q({x i }) the joint probability distribution of the model. Bayes formula, q({x i }, {y j }) = q({x i }|{y j }) q({y j }), directly translates into the definition of conditional cross-entropy through the decomposition,</p><formula xml:id="formula_2">H({X i }, {Y j }) = H({X i }|{Y j }) + H({Y j })<label>(4)</label></formula><p>Multivariate (cross) information, I(V ) ≡ I(X 1 ; • • • ; X m ), are defined from multivariate (cross) entropies through Inclusion-Exclusion formulas over all subsets of variables <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> as,</p><formula xml:id="formula_3">I(X) = H(X) I(X; Y ) = H(X) + H(Y ) -H(X, Y ) I(X; Y ; Z) = H(X) + H(Y ) + H(Z) -H(X, Y ) -H(X, Z) -H(Y, Z) +H(X, Y, Z) I(V ) = - S⊆V (-1) |S| H(S)<label>(5)</label></formula><p>where the semicolon separators are needed to distinguish multipoint (cross) information from joint variables as {X, Z} in I({X, Z}; Y ) = I(X; Y )+I(Z; Y )-I(X; Y ; Z). Below, implicit separators between non-conditioning variables in multivariate (cross) information will always correspond to semicolons, e.g. as in I(V ) in Eq. 5. Unlike multivariate (cross) entropies, which are always positive,</p><formula xml:id="formula_4">H(X 1 , • • • , X k ) ⩾ 0, multivariate (cross) information, I(X 1 ; • • • ; X k )</formula><p>, can be positive or negative for k ⩾ 3, while they remain always positive for k &lt; 3, i.e. I(X; Y ) ⩾ 0 and I(X) ⩾ 0.</p><p>In turn, multivariate (cross) entropies can be expressed through the Principle of Inclusion-Exclusion into the same expression form but in terms of multivariate (cross) information,</p><formula xml:id="formula_5">H(V ) = - S⊆V (-1) |S| I(S),<label>(6)</label></formula><p>Conditional multivariate (cross) information I(V |Z) are defined similarly as multivariate (cross) information I(V ) but in terms of conditional (cross) entropies as,</p><formula xml:id="formula_6">I(V |Z) = - S⊆V (-1) |S| H(S|Z)<label>(7)</label></formula><p>Eqs. 5 &amp; 7 lead to a decomposition rule relative to a variable Z, Eq. 8, which can be conditioned on a set of joint variables, A = {A 1 , • • • , A m }, with implicit comma separators for conditioning variables in Eq. 9,</p><formula xml:id="formula_7">I(V ) = I(V |Z) + I(V ; Z) (8) I(V |A) = I(V |Z, A) + I(V ; Z|A)<label>(9)</label></formula><p>Alternatively, conditional (cross) information, such as I(X; Y |A), can be expressed in terms of non-conditional (cross) entropies using Eq. 4,</p><formula xml:id="formula_8">I(X; Y |A) = H(X|A) + H(Y |A) -H(X, Y |A) = H(X, A) + H(Y, A) -H(X, Y, A) -H(A)<label>(10</label></formula><p>) which can in turn be expressed in terms of non-conditional (cross) information as,</p><formula xml:id="formula_9">I(X; Y |A) = I(X; Y ) -• • • (-1) k i1&lt;•••&lt;i k I(X; Y ; A i1 ; • • •; A i k ) + • • • (-1) m I(X; Y ; A 1 ; • • •; A m ) = X,Y ∈S ′ S ′ ⊆S (-1) |S ′ | I(S ′ ),<label>(11)</label></formula><p>where S = {X, Y } ∪ A. This corresponds, up to an opposite sign, to all (cross) information terms including both X and Y in the expression of the multivariate (cross) entropy, H(X, Y, A), Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graphs and connection criteria</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Directed mixed graphs and ancestral graphs</head><p>Two vertices are said to be adjacent if there is an edge (of any type) between them, X * * Y , where * stands for any (head or tail) end mark. X and Y are said to be neighbors if X Y , parent and</p><formula xml:id="formula_10">child if X → Y and spouses if X ←→ Y in G. A path in G is a sequence of distinct vertices V 1 , . . . , V n consecutively adjacent in G, as, V 1 * * V 2 * * • • • * * V n-1 * * V n . In particular, a collider path between V 1 and V n has the form V 1 * → V 2 ←→ • • • ←→ V n-1 ← * V n and a directed path corresponds to V 1 → V 2 → • • • → V n . X is called an ancestor of Y and Y a descendant of X if X = Y or there is a directed path from X to Y , X → • • • → Y . An G (Y ) denotes the set of ancestors of Y in G. By extension, for any subset of vertices, C ⊆ V , An G (C) denotes the set of ancestors for all Y ∈ C in G.</formula><p>A directed mixed graph is a vertex-edge graph G = (V , E) that can contain two types of edges: directed (→) and bidirected (←→) edges.</p><p>A directed cycle occurs in G when X ∈ An G (Y ) and X ← Y . An almost directed cycle occurs when X ∈ An G (Y ) and X ←→ Y .</p><p>Definition 1. An ancestral graph is a directed mixed graph: i) without directed cycles; ii) without almost directed cycles.</p><p>An ancestral graph is said to be maximal if every missing edge corresponds to a structural independence. If an ancestral graph G is not maximal, there exists a unique maximal ancestral graph Ḡ by adding bidirected edges to G <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">ac-connecting paths and ac-connected subsets</head><p>Let us now define ancestor collider connecting paths or ac-connecting paths, which entail simpler path connecting criterion than the traditional m-connecting criterion, discussed in the Appendix A. Yet, ac-connecting paths and ac-connected subsets will turn out to be directly relevant to characterize the likelihood decomposition and Markov equivalent classes of ancestral graphs.</p><p>Definition 2. [ac-connecting path] An ac-connecting path between X and Y given a subset of variables C (possibly including X and Y ) is a collider path,</p><formula xml:id="formula_11">X * → Z 1 ←→ • • • ←→ Z K ← * Y , with all Z i ∈ An G ({X, Y } ∪ C), that is, with Z i in C or connected to {X, Y } ∪ C by an ancestor path, i.e. Z i → • • • → T with T ∈ {X, Y } ∪ C.</formula><p>Definition 3. [ac-connected subset] A subset C is said to be ac-connected if ∀X, Y ∈ C, X and Y are connected (through any type of edge) or there is an ac-connecting path between X and Y given C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Likelihood decomposition of ancestral graphs</head><p>Theorem 1. [likelihood of ancestral graphs] The cross-entropy H(p, q) and likelihood L D|G of an ancestral graph G is decomposable in terms of multivariate cross-information, I(C), summed over all ac-connected subsets of variables, C (Definition 3),</p><formula xml:id="formula_12">H(p, q) = - ac-connected C⊆V (-1) |C| I(C) L D|G = 1 Z D,G exp N ac-connected C⊆V (-1) |C| I(C) (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where N is the number of iid samples in the dataset D and Z D,G a data-and model-dependent normalization constant.</p><p>The proof of Theorem 1 is left to Appendix B. It is based on a partition of the cross-entropy (Eq. 6) into cross-information contributions from ac-connected and non-ac-connected subsets of variables, which does not rely on head-and-tail factorizations nor on imset formalism <ref type="foot" target="#foot_0">1</ref> Hu and Evans <ref type="bibr" target="#b13">[14]</ref> proposed an equivalent result (Proposition 3.3 in <ref type="bibr" target="#b13">[14]</ref>) with a proof using head-and-tail decomposition to define parametrizing sets, which happen to coincide with the ac-connected sets defined here (Definition 3). Theorem 1 characterizes in particular the Markov equivalence class of ancestral graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> as, Corollary 2. Two ancestral graphs are Markov equivalent if and only if they have the same acconnected subsets of vertices.</p><p>Note, in particular, that Eq. 12 holds for maximal ancestral graphs (MAG), for which all pairs of ac-connected variables are connected by an edge, and their Markov equivalent representatives, the partial ancestral graphs (PAG) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>.</p><p>Proposition 3. The likelihood decomposition of ancestral graphs (Eq. 12, Theorem 1) can be estimated by replacing the model distribution q by the empirical distribution p in the retained multivariate cross-information terms I(C) corresponding to all ac-connected subsets of variables, C.</p><p>Hence, Proposition 3 amounts to estimating all relevant cross-information terms in the likelihood function with the corresponding multivariate information terms computed from the available data, while assuming by construction that the model distribution obeys all local and global conditional independences entailed by the ancestral graph. The corresponding factorization of the model distribution can be expressed in terms of empirical distribution, assuming positive distributions, see Appendix C.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the cross-entropy decomposition for a few graphical models in terms of crossinformation contributions from their ac-connected subsets of vertices. In particular, an unshielded non-collider (e.g. X → Z → W , Fig. <ref type="figure" target="#fig_3">1A</ref>), is less likely (i.e. higher cross-entropy) than an unshielded collider or 'v-structure' (e.g. X → Z ← W , Fig. <ref type="figure" target="#fig_3">1B</ref>), if the corresponding three-point information term is negative, I(X; Z; W ) &lt; 0, in agreement with earlier results <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. However, this early Simple Bayesian graph not Markov equivalent to an ancestral graph (G) sharing the same edges and unshielded collider <ref type="bibr" target="#b23">[24]</ref>. Solid black edges correspond to direct connections or collider paths confined to the corresponding ac-connected subset C, while wiggly edges indicate collider paths extending beyond C yet indirectly connected to C by an ancestor path, marked with dashed edges, see Definition 2. By contrast, graphs H and I illustrate the fact that collider paths may not be unique nor conserved between two Markov equivalent graphs (i.e. sharing the same cross-information terms) <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_14">X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X Z Y T X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X Z Y T X Z X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices Y T X Y T Z Y T X Z X T + X Z Y T - - - Z Y T X Z T X Z T Z Y T Z Y T Z Y T X i X j X k X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices X i X j X k X l X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices B C A X i X i X j -Σ Edges I( ; ) = Σ I( ) Vertices G H I F E - - W Z X W Z X T Y Z X T Y Z X T Y Z X T Y Z X + + + T X + Σ I( ; ; ) H(p,q) ac-connected Triples + X Z W Y X Z W Y X Z W Y I( ; ; ; ) 4-uples ac-connected D X Z Y W + + X Z Y W - W X Z Y Y Z W + Z Y W X Z Y X Y Z W X Z Y Z Y W W X Y X Z W + ... Z Y W -Σ X X Z Y</formula><p>approach, exploiting the sign and magnitude of three-point information to orient v-structures, does not include higher order terms involving multiple v-structures, which can lead to orientation conflicts between unshielded triples, in practice. Resolving such orientation conflicts requires to include information contributions from higher-order ac-connected subgraphs, such as star-like ac-connected subsets including three or more parents, Fig. <ref type="figure" target="#fig_3">1C</ref>. Similarly, the cross-entropies of collider paths involving several colliders also include higher-order terms, as with the simple example of a twocollider path, Fig. <ref type="figure" target="#fig_3">1D</ref>. By contrast, the cross-entropy based on the head-and-tail factorization of the same two-collider path, i.e. q(x, z, y, w) = q(z, y|x, w)q(x)q(w) <ref type="bibr" target="#b5">[6]</ref>, is found to be equivalent to the cross-entropy of a Bayesian graph without bidirected edge, Fig. <ref type="figure" target="#fig_3">1E</ref>, when estimated with the empirical distribution p(.), see Appendix C. This observation illustrates the difficulty to estimate the likelihood functions of ancestral graphs using head-and-tail factorization.</p><p>Further examples of graphical models, Figs. 1F-I, show the relative simplicity of the decomposition with only few (non-trivial) ac-connected contributing subsets C with |C| ⩾ 3, as compared to the much larger number of non-ac-connected non-contributing subsets, that cancel each other by construction due to conditional independence constraints of the underlying model. Note, in particular, that most contributing multivariate information I(C) only concern direct connections or collider paths within a single component subgraph induced by C (solid line edges in Fig. <ref type="figure" target="#fig_0">1</ref>). However, occasionally, collider paths extending beyond C into An G (C) \ C (marked with wiggly edges) with corresponding ancestor path(s) (marked with dashed edges) do occur, as shown in Fig. <ref type="figure" target="#fig_3">1G</ref>.</p><p>In addition, the present information-theoretic decomposition of the likelihood of ancestral graphs can readily distinguish their Markov equivalence classes according to Corollary 2. For instance, the ancestral graphs of Fig. <ref type="figure" target="#fig_3">1F</ref> and Fig. <ref type="figure" target="#fig_3">1G</ref>, despite sharing the same edges and the same unshielded collider (X → Z ← T ), turn out not to be Markov equivalent, as discussed in <ref type="bibr" target="#b23">[24]</ref>. Indeed, their cross-entropy decompositions differ by two ac-connected contributing terms: a three-point cross information I(X; Y ; T ) with a collider path not confined in C (i.e. X ⇝ Z ↭ T ←→ Y and corresponding ancestor path Z Y ) and a four-point information term I(X; Y ; Z; T ) due to the two-collider path (X → Z ←→ T ←→ Y ). More quantitatively, it shows that the graph of Fig. <ref type="figure" target="#fig_3">1G</ref> with a two-collider path is more likely than the graph of Fig. <ref type="figure" target="#fig_3">1F whenever I</ref></p><formula xml:id="formula_15">(X; Y ; T ) - I(X; Y ; Z; T ) = I(X; Y ; T |Z) = I(X; Y |Z) -I(X; Y |Z, T ) &lt; 0.</formula><p>Finally, the Markov equivalent graphs of Fig. <ref type="figure" target="#fig_3">1H</ref> and Fig. <ref type="figure" target="#fig_3">1I</ref>, also due to <ref type="bibr" target="#b23">[24]</ref>, illustrate the fact that the actual ancestor collider path between unconnected pairs does not need to be unique nor conserved between Markov equivalent graphs (as long as their cross-entropies share the same multivariate cross-information decomposition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient search-and-score causal discovery using local information scores</head><p>The likelihood estimation of ancestral graphs (Theorem 1 and Proposition 3) enables the implementation of a search-and-score algorithm for this broad class of graphs, which has attracted a number of contributions recently <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Our specific objective is not to develop an exact method limited to simple graphical models with a few nodes and small datasets but to implement an efficient and reliable heuristic method applicable to more challenging graphical models and large datasets. Indeed, search-and-score structure learning methods need to rely on heuristic rather than exhaustive search, in general, given that the number of ancestral graphs grows super-exponentially as the number of vertices increases. This can be implemented for instance with a Monte Carlo algorithmic scheme with random restarts, which efficiently probes relevant graphical models. Here, we opt, instead, to use the prediction of an efficient hybrid causal discovery method, MIIC <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, as starting point for a subsequent search-and-score approach based on the proposed likelihood estimation of ancestral graphs (Eq. 12 and Proposition 3). Moreover, while the likelihood decomposition of ancestral graphs may involve extended ac-connected subsets of variables, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, we aim to implement a computationally efficient searchand-score causal discovery method based on approximate local scores limited to the close surrounding vertices of each node and edge. Yet, while MIIC only relies on unshielded triple scores, the novel search-and-score extension, MIIC_search&amp;score, uses also higher-order local information scores to compare alternative subgraphs, as detailed below.</p><p>The proposed method is shown to outperform MIIC and other state-of-the-art causal discovery methods on challenging datasets including latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIIC, an hybrid causal discovery method based on unshielded triple scores</head><p>MIIC is an hybrid causal discovery method combining constraint-based and information-theoretic frameworks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. Unlike traditional constraint-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, MIIC does not directly attempt to uncover conditional independences but, instead, iteratively substracts the most significant three-point (conditional) information contributions of successive contributors, A 1 , A 2 , ..., A n , from the mutual information between each pair of variables, I(X; Y ), as,</p><formula xml:id="formula_16">I(X; Y )-I(X; Y ; A 1 )-I(X; Y ; A 2 |A 1 )-• • •-I(X; Y ; A n |{A i } n-1 ) = I(X; Y |{A i } n ) (13)</formula><p>where I(X; Y ; A k |{A i } k-1 ) &gt; 0 is the positive information contribution from A k to I(X; Y ) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>. Conditional independence is eventually established when the residual conditional mutual information on the right hand side of Eq. 13, I(X; Y |{A i } n ), becomes smaller than a complexity term, i.e. k X;Y |{Ai} (N ) ⩾ I(X; Y |{A i } n ) ⩾ 0, which dependents on the considered variables and sample size N . This leads to an undirected skeleton, which MIIC then (partially) orients based on the sign and amplitude of the regularized conditional 3-point information terms <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. In particular, negative conditional 3-point information terms, I(X; Y ; Z|{A i }) &lt; 0, correspond to the signature of causality in observational data <ref type="bibr" target="#b27">[28]</ref> and lead to the prediction of a v-structure, X → Z ← Y , if X and Y are not connected in the skeleton. By contrast, a positive conditional 3-point information term, I(X; Y ; Z|{A i }) &gt; 0, implies the absence of a v-structure and suggests to propagate the orientation of a previously directed edge X → Z Y as X → Z → Y .</p><p>In practice, MIIC's strategy to circumvent spurious conditional independences significantly improves recall, that is, the fraction of correctly recovered edges, compared to traditional constraint-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Yet, MIIC only relies on unshielded triple scores to reliably uncover significant contributors and orient v-structures, as outlined above. MIIC has been recently improved to ensure the consistency of the separating set in terms of indirect paths in the final skeleton or (partially) oriented graphs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref> and to improve the reliably of predicted orientations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>The predictions of this recent version of MIIC, which include three type of edges (directed, bidirected and undirected), have been used as starting point for the subsequent local search-and-score method implemented in the present paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">New search-and-score method based on higher-order local information scores</head><p>Starting from the structure predicted by MIIC, as detailed above, MIIC_search&amp;score proceeds in two steps, first to remove likely false positive edges (Step 1) and then to (re)orient the remaining edges based on their estimated contributions to the global likelihood decomposition, Eq. 12 (Step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Step 1: Node scores for edge orientation priming and edge removal</head><p>The first step consists in minimizing a node score corresponding to the local normalized log likelihood of each node w.r.t. its possible parents or spouses amongst the connected nodes predicted by MIIC. To this end, the node score assesses the conditional entropy of each node w.r.t. a selection of parents, spouses or neighbors, Pa ′</p><formula xml:id="formula_17">X i ⊆ Pa X i ∪ Sp X i</formula><p>∪ Ne X i , and a factorized Normalized Maximum Likelihood (fNML) regularization <ref type="bibr" target="#b27">[28]</ref>, see Appendix D for details,</p><formula xml:id="formula_18">Score n (X i ) = H(X i |Pa ′ X i ) + 1 N qx i j log C rx i nj<label>(14)</label></formula><p>where q xi corresponds to the combination of levels of Pa ′ X i</p><p>, while r xi is the number of levels of X i , and n j the number of samples corresponding to a particular combination of levels j in each summand, with j n j = N , the total number of samples. log C rx i nj is the fNML regulatization cost summed over all combinations of levels, q xi , <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, see Appendix D. This first algorithm is looped over each node, priming the orientations of their surrounding edges (as directed, bidirected or undirected), until convergence. Edges without orientation priming at either extremity are assumed to be false positive edges and removed at the end of Step 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Step 2: Edge orientation scores, as local contributions to the global likelihood score</head><p>The second step consists in orienting the edges retained after Step 1, based on the optimization of their local contributions to the global likelihood score, Eq. 12, restricted to ac-connected subsets containing up to two-collider paths. This amounts to minimizing each edge orientation score w.r.t. its nodes' parents and spouses, corresponding to minus the conditional information plus a fNML complexity cost, Table <ref type="table" target="#tab_0">1</ref>, given three sets of parents and spouses of X and Y , i.e. Pa</p><formula xml:id="formula_19">′ X\Y = Pa X ∪ Sp X \Y , Pa ′ Y\X = Pa Y ∪ Sp Y \X and Pa ′ XY = Pa ′ X\Y ∪ Pa ′</formula><p>Y\X with their corresponding combinations of levels, q y\x , q x\y and q xy . These orientation scores, Table <ref type="table" target="#tab_0">1</ref>, include symmetrized fNML complexity terms to enforce Markov equivalence, if X and Y share the same parents or spouses (excluding X and Y ), see Appendix D. Indeed, all three scores become equals if Pa ′ Y\X = Pa ′ X\Y = Pa ′ XY implying also the same combinations of parent and spouse levels, q y\x = q x\y = q xy . </p><formula xml:id="formula_20">X → Y -I(X; Y |Pa ′ Y\X ) 1 2N q x\y ry j log C rx n j - q x\y j log C rx n j + q y\x rx j log C ry n j - q y\x j log C ry n j X ← Y -I(X; Y |Pa ′ X\Y ) 1 2N q x\y ry j log C rx n j - q x\y j log C rx n j + q y\x rx j log C ry n j - q y\x j log C ry n j X ↔ Y -I(X; Y |Pa ′ XY ) 1 2N q xy ry j log C rx n j - q xy j log C rx n j + q yx rx j log C ry n j - q yx j log C ry n j</formula><p>While orientation scores cannot be summed over individual edges due to multiple countings of acconnected contributions, score differences between alternative orientations provide an estimate of the global score change. Hence, step 2 algorithm is looped over each edge to compute an orientation score decrement, given its current orientation and the orientations of surrounding edges. The orientation change corresponding to the largest global score decrement, without forming new directed or almost directed triangular cycles, is then chosen at each iteration until convergence or until a limit cycle is reached. Limit cycles may originate from the local two-collider approximation of the global score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>We first tested whether MIIC_search&amp;score orientation scores (Table <ref type="table" target="#tab_0">1</ref>) effectively predicts bidirected orientations on three simple ancestral models, Fig. <ref type="figure" target="#fig_4">4</ref> in Appendix E, when the end nodes do not share the same parents (Fig. <ref type="figure" target="#fig_4">4</ref>, Model 1), share some parents (Fig. <ref type="figure" target="#fig_4">4</ref>, Model 2) or when the bidirected edge is part of a longer than two-collider paths (Fig. <ref type="figure" target="#fig_4">4</ref>, Model 3). The prediction of the edge orientation scores are summarized in Table <ref type="table" target="#tab_6">3</ref>, Appendix E, and show good predictions for large enough datasets.</p><p>Beyond these simple examples, focussing on the discovery of bidirected edges in small toy models of ancestral graphs, we also analyzed more challenging benchmarks from the bnlearn repository <ref type="bibr" target="#b39">[40]</ref>, Figs. We then assessed causal discovery performance in terms of Precision, T P/(T P + F P ), and Recall, T P/(T P + F N ), relative to the theoretical PAGs, while counting as false positive (F P ), all correctly predicted edges but with a different orientation as the directed or bidirected edges of the PAG.</p><p>Figs. 2-3 compare MIIC_search&amp;score performance to MIIC results used as starting point for MIIC_search&amp;score and to FCI <ref type="bibr" target="#b40">[41]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> results are obtained from independent datasets for each ancestral graph and sample size, while Fig. <ref type="figure" target="#fig_2">3</ref> results provide a bootstrap sensitivity analysis to sampling noise for each method based on independent resamplings with replacement of single datasets of increasing size. MIIC and MIIC_search&amp;score settings were set as described in section 3 above. The open-source MIIC R package (v1.5.2, GPL-3.0 license) was obtained at <ref type="url" target="https://github.com/miicTeam/miic_R_package">https: //github.com/miicTeam/miic_R_package</ref>. FCI from the python causal-learn package (v0.1.3.8, MIT license) <ref type="bibr" target="#b40">[41]</ref> was obtained at <ref type="url" target="https://github.com/py-why/causal-learn">https://github.com/py-why/causal-learn</ref> and run with G 2 -conditional independence test and default parameter α = 0.05.</p><p>Overall, MIIC_search&amp;score is found to outperform MIIC in terms of edge precision with little to no decrease in edge recall, Figs. <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref>, demonstrating the benefit of MIIC_search&amp;score's rationale to improve MIIC predictions by extending MIIC information scores from unshielded triples to higherorder information contributions. These originate from ac-connected subsets including nodes with more than two parents or spouses, or ac-connected subsets including two-collider paths.</p><p>MIIC_search&amp;score is also found to outperform FCI on both precision and recall on small datasets (e.g. N ⩽ 10, 000 samples) of complex graphical models (i.e. Insurance, Barley and Mildew), while reaching similar performance at larger sample sizes or for simpler graphical models (i.e. similar precision on Alarm model), as expected from the asymptotic consistency of FCI for very large datasets, Fig. <ref type="figure" target="#fig_1">2</ref>. We also observed that FCI had a hard time to converge on bootstrapped datasets, explaining the lack of FCI comparison with MIIC and MIIC_search&amp;score in Fig. <ref type="figure" target="#fig_2">3</ref>, except for Alarm on all sample sizes tested (N ⩽ 20, 000) and for Insurance at small sample sizes (N ⩽ 1, 000).  Importantly, the benchmark PAGs used to score the causal discovery results with increasing proportions of latent variables, Figs. <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref>, include not only bidirected edges originating from hidden common causes but also additional directed or undirected edges arising, in particular, from indirect effects of hidden variables with observed parents. Irrespective of their orientations, all these additional edges originating from indirect effects of hidden variables generally correspond to weaker effects (i.e. lower mutual information of indirect effects due to the Data Processing Inequality) and are more difficult to uncover than the edges of the original graphical model without hidden variables. This explains the steady decrease in recall for complex ancestral models (i.e. Insurance, Barley, Mildew) with higher proportions of hidden variables, while precision remains essentially unaffected, Figs. <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref>.</p><p>All in all, these results highlight MIIC_search&amp;score capacity to efficiently and robustly learn complex graphical models from limited available data, which is a frequent setting for many real-world applications, in practice. In addition, MIIC_search&amp;score, which has been implemented to analyze challenging categorical datasets, is quite unique in this regard, as all other search-and-score methods for ancestral graphs <ref type="bibr">[11-13, 30, 32]</ref> have only been demonstrated on continuous datasets from linear Gaussian models and could not be included in the present benchmarks, Figs.  Benchmark results on bootstrap sensitivity analysis to sampling noise based on 30 independent resamplings with replacement of single datasets of increasing sizes. Ancestral graphs are obtained by hiding 0%, 5%, 10% or 20% of variables in Discrete Bayesian Networks of increasing complexity (see main text): Alarm, Insurance, Barley and Mildew. MIIC_search&amp;score results are compared to MIIC results used as starting point for MIIC_search&amp;score and FCI <ref type="bibr" target="#b40">[41]</ref>. The lack of FCI results, except for Alarm on all sample sizes tested (N ⩽ 20, 000) and for Insurance at small sample sizes (N ⩽ 1, 000), stems from FCI difficulty to converge on bootstrapped datasets. Causal discovery performance is assessed in terms of Precision and Recall relative to the theoretical PAGs, while counting as false positive all correctly predicted edges but with a different orientation as the directed or bidirected edges of the PAG. Error bars: 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>The main limitation of the paper concerns the local scores used in the search-and-score algorithm, which are limited to ac-connected subsets of vertices with a maximum of two-collider paths.</p><p>While this approach could be extended to higher-order information contributions including threeor-more-collider paths, it allows for a simple two-step search-and-score scheme at the level of individual nodes (step 1) and edges (step 2), as detailed in section 3. This already shows a significant improvement in causal discovery performance (i.e. combining good precision and good recall on challenging benchmarks) as compared to existing state-of-the-art methods.</p><p>Appendix / supplemental material A Preliminaries: connection and separation criteria The probabilistic interpretation of ancestral graph is given by its global and pairwise Markov properties (which are equivalent <ref type="bibr" target="#b7">[8]</ref>): if A and B are m-separated by C, then A and B are conditionally independent given C and ∀X ∈ A and ∀Y ∈ B, there is a probability distribution P faithful to G such that their conditional mutual information vanishes, i.e. I P (X; Y |C) = 0, also noted X ⊥ ⊥ P Y |C.</p><p>However, as discussed above, the proof of Theorem 1 will require to introduce a weaker m ′ -connection criterion defined below.</p><formula xml:id="formula_21">Definition 6. [m ′ -connecting path] A path π between X and Y is m ′ -connecting given a subset C ⊆ V (with X, Y possibly in C) if: i) its non-collider(s) are not in C, and ii) its collider(s) are in An G ({X, Y } ∪ C).</formula><p>Note, in particular, that an m-connecting path is necessary an m ′ -connecting path but that the converse is not always true. For example, the path X → Z ←→ T ←→ Y in Fig. <ref type="figure" target="#fig_3">1G</ref> (with Z → Y ) is an m ′ -connecting path given T (as Z ∈ An G ({X, Y } ∪ T )) but not an m-connecting path given T (as Z / ∈ An G (T )).</p><p>However, <ref type="bibr">Richardson and Spirtes 2002 [8]</ref> have shown the following lemma, The probabilistic interpretation of an ancestral graph is given by its (global) Markov property: if A and B are m-separated (or m ′ -separated) by C, then A and B are conditionally independent given C, noted as, A ⊥ m B|C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ac-connecting paths and ac-connected subsets</head><p>Let us now recall the definition of ancestor collider connecting paths or ac-connecting paths, which is directly relevant to characterize the likelihood decomposition and Markov equivalent classes of ancestral graphs (Theorem 1). We give here a different yet equivalent definition of ac-connecting paths as defined in the main text (Definition 2) in order to underline the similarities and differencies with the notion of m ′ -connecting path (Definition 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 8. [ac-connecting path]</head><p>A path π between X and Y is an ac-connecting path given a subset C ⊆ V (with X and Y possibly in C) if: i) π does not have any noncollider, and ii) its collider(s) are in An G ({X, Y } ∪ C).</p><p>Hence, more simply (following Definition 2 in the main text), an ac-connecting path given C is a collider path,</p><formula xml:id="formula_22">X * → Z 1 ↔ • • • ↔ Z K ← * Y , with all Z i ∈ An G ({X, Y } ∪ C), i.e. with Z i in C or connected to {X, Y } ∪ C by an ancestor path, Z i → • • • → T with T ∈ {X, Y } ∪ C. Definition 9.</formula><p>[ac-separation criterion] The subsets A and B are said to be ac-separated by C if there is no ac-connecting path between any vertex in A and any vertex in B given C.</p><p>Previous definitions and Lemma 4 readily lead to the following corollary between the different connection and separation criteria:</p><formula xml:id="formula_23">Corollary 5. i) m-connecting path π =⇒ m ′ -connecting path π ii) ac-connecting path π =⇒ m ′ -connecting path π iii) m-separation ⇐⇒ m ′ -separation iv) m/m ′ -separation =⇒ ac-separation</formula><p>Finally, we recall the notion of ac-connected subset (Definition 3 in the main text), which is central for the decomposition of the likelihood of ancestral graphs (Theorem 1): A subset C is said to be ac-connected if ∀X, Y ∈ C, there is an ac-connecting path between X and Y w.r.t. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1.</head><p>In order to prove that the likelihood function of an ancestral graph, Eq. 12, contains all and only the ac-connected subsets of vertices in G (Definition 3), we will first show (i) that all non-acconnected subsets S ′ are included in a cancelling combination of multivariate cross-information terms, I(X; Y |A) = 0, with X, Y ∈ S ′ and S ′ ⊆ S = {X, Y } ∪ A, Eq. 11. Conversely, we will then show (ii) that cancelling combinations of multivariate cross-information terms associated to pairwise conditional independence, I(X; Y |A) = X,Y ∈S ′ S ′ ⊆S (-1) |S ′ | I(S ′ ) = 0, do not contain any ac-connected subset S ′ . Finally, we will prove (iii) that the information terms which appear in multiple cancelling combinations from different pairwise independence constraints do not modify the multivariate information decomposition of the likelihood function of ancestral graphs, Eq. 12, as these shared/overlapping terms in fact all cancel through more global Markov independence relationships involving higher order (three or more points) vanishing multivariate information terms, such as I(X; Y ; Z|A) = 0. i) Let's first prove that all non-ac-connected subsets S ′ are included in at least one cancelling combination of multivariate cross-information, I(X; Y |A) = 0, with X, Y ∈ S ′ and S ′ ⊆ {X, Y}∪A.</p><p>If S ′ is a non-ac-connected subset, there is at least one disconnected pair X and Y for which each path π j between X and Y contains either some collider(s) not in An G (S ′ ) or, if all colliders along π j are in An G (S ′ ), there must be some non-collider(s) at node(s) Z j but not necessarily in S ′ . Let's define S = S ′ ∪ j Z j . X and Y can be shown to be m-separated given S \ {X, Y }, as for each path π j between X and Y , its non-collider(s) are in S at node(s) Z j (when all collider(s) along π j are in S ′ ) or there is some collider(s) not in An G (S ′ ), which are not in An G (S) either. The latter statement is proven by contradiction assuming that there is a collider at Z / ∈ An G (S ′ ) such that Z ∈ An G (S). There is therefore a directed path Z → • • • → W with W ∈ S. Hence, W ∈ S ′ or there is a noncollider at W ∈ Z j which is on a path π j between X and Y along which all colliders are in An G (S ′ ) by construction of S. This leads by induction to</p><formula xml:id="formula_24">Z → • • • → W → • • • → T where T ∈ S ′ and thus Z ∈ An G (S ′</formula><p>), which is a contradiction. Hence, all non-ac-connected subsets S ′ are included in a cancelling combination of multivariate cross-information terms, I(X; Y |A) = 0, with X, Y ∈ S ′ and S ′ ⊆ S = {X, Y } ∪ A.</p><p>ii) Conversely, we will now show that cancelling combinations of multivariate cross-information terms associated to pairwise conditional independence,</p><formula xml:id="formula_25">I(X; Y |A) = X,Y ∈S ′ S ′ ⊆S (-1) |S ′ | I(S ′ ) = 0, do not contain any ac-connected subset S ′ , where S = {X, Y } ∪ A.</formula><p>We will prove it by contradiction assuming that there exists a subset W ⊆ A, such that S ′ = {X, Y } ∪ W is ac-connected. In particular, there should be an ac-connecting path between X and Y confined to An G (S ′ ) and thus to An G (S) ⊇ An G (S ′ ), which is an m ′ -connecting path between X and Y given A, contradicting the above hypothesis of m ′ -separation given A, i.e. I(X; Y |A) = 0. The use of m ′ -separation, i.e. the absence of m ′ -connecting paths with colliders in An G (S) rather than m-connecting paths with colliders in An G (A), is necessary here, see Definitions 4 and 6. Hence, no ac-connected subset S ′ is included in cancelling combinations of multivariate cross-information terms associated to pairwise conditional independence,</p><formula xml:id="formula_26">I(X; Y |A) = X,Y ∈S ′ S ′ ⊆S (-1) |S ′ | I(S ′ ) = 0.</formula><p>iii) Finally, we will show that the information terms which appear in multiple cancelling combinations from different pairwise independence constraints do not modify the multivariate cross-information decomposition of the likelihood function of ancestral graphs, Eq. 12, as these shared/overlapping terms in fact all cancel through more global Markov independence relationships involving higher order (three or more points) vanishing multivariate cross-information terms, such as I(X; Y ; Z|A) = 0. This result requires to use an ordering of the nodes, X k ≻ X j ≻ X i , that is compatible with the directed edges of the ancestral graph assumed to have no undirected edges, i.e. X j / ∈ An(X i ) if X j ≻ X i . Under this ordering, higher order nodes X k ≻ X j ≻ X i can be a priori excluded from all separating sets A ij of pairs of lower order nodes, i.e. if I(X i ;</p><formula xml:id="formula_27">X j |A ij ) = 0 then X k / ∈ A ij .</formula><p>In particular, the two pairwise conditional independence relations I(X k ; X ℓ |A kℓ ) = 0, with X ℓ ≻ X k , and I(X i ; X j |A ij ) = 0, with X j ≻ X i , do not share any multivariate cross-information terms, if X ℓ ̸ = X j . Indeed, as I(X i ; X j |A ij ) contains all cross-information terms including both X i and X j as well as every subset (possibly empty) of A ij , none of them includes X ℓ if X ℓ ≻ X j . Therefore I(X i ; X j |A ij ) does not contain any cross-information term of I(X k ; X ℓ |A kℓ ) which contains both X k and X ℓ as well as every subset (possibly empty) of A kℓ . This property eliminates all multiple countings of multivariate cross-information terms if X ℓ ̸ = X j . Note that this result does not hold in general for ancestral graphs including undirected edges.</p><p>Hence, the issue of redundant multivariate cross-information terms in the likelihood decomposition, Eq. 12, is related to the conditional independences of two or more pairs, {X i , X r }, {X j , X r }, ..., {X ℓ , X r }, sharing the same higher order node, X r , i.e., I(X k ; X r |A kr ) = 0 for k = i, j, • • • , ℓ. However, this situation also entails a more global Markov independence constraint between X r and {X i , X j , • • • , X ℓ }, given a separating set A, with</p><formula xml:id="formula_28">A kr ⊆ A ∪ {X i , • • • , X ℓ } for k = i, j, • • • , ℓ.</formula><p>Such a global Markov independence constraint can be decomposed into more local independence constraints using the chain rule (in any order of the variables X i , X j , • • • , X ℓ ) and the decomposition rules of multivariate (cross) information (Eq. 9),</p><formula xml:id="formula_29">0 = I({X i , X j , • • • , X ℓ }; X r |A) = I(X i ; X r |A) + I(X j ; X r |A, X i ) + I(X k ; X r |A, X i , X j ) + • • • + I(X ℓ ; X r |A, • • • ) = I(X i ; X r |A) + I(X j ; X r |A) -I(X i ; X j ; X r |A) + I(X k ; X r |A, X i ) -I(X j ; X k ; X r |A, X i ) + • • • + I(X ℓ ; X r |A, • • • ) = I(X i ; X r |A) + I(X j ; X r |A) -I(X i ; X j ; X r |A) + I(X k ; X r |A) -I(X j ; X k ; X r |A) -I(X i ; X k ; X r |A) + I(X i ; X j ; X k ; X r |A) + • • •</formula><p>where all the conditional multivariate cross-information terms vanish by induction due to the nonnegativity of (conditional) mutual (cross) information. In particular, the conditional multivariate cross-information terms in the last expression, i.e. between X r and each subset of {X i , X j , • • • , X ℓ } given the separating set A, all vanish. This result can be readily extended to any subsets</p><formula xml:id="formula_30">{X r , X s , • • • , X z } (conditionally) independent of {X i , X j , • • • , X ℓ } given a separating set A, i.e. I({X i , X j , • • • , X ℓ }; {X r , X s , • • • , X z }|A) = 0.</formula><p>Hence, as the final conditional multivariate cross-information terms of the decomposition all vanish while not sharing any subsets of variables, it proves the absence of redundancy and a global cancellation of non-ac-connected subsets (from pairwise and higher order conditional independence relations) in the likelihood function of ancestral graphs without undirected edges, Eq. 12.</p><p>Hence, only ac-connected subsets effectively contribute to the cross-entropy of an ancestral graph with only directed and bidirected edges, Eq. 12. □ C Factorization of the probability distribution of ancestral graphs C.1 Factorization resulting from Theorem 1 and Proposition 3</p><p>Before presenting the factorization of the model distribution of ancestral graphs resulting from Theorem 1 and Proposition 3, it is instructive to obtain an equivalent factorization for Bayesian graphs, assuming a positive empirical distributions, p(x</p><formula xml:id="formula_31">1 , • • • , x m ) = m i=1 p(x i |x i-1 , • • • , x 1 ) &gt; 0, q(x 1 , • • • , x m ) = m i=1 q(x i |pa xi ) = m i=1 p(x i |pa xi ) = p(x 1 , • • • , x m ) m i=1 p(x i |pa xi ) p(x i |x i-1 , • • • , x 1 ) = p(x 1 , • • • , x m ) m i=1 p(x i |pa xi )p(x i-1 \pa xi |pa xi ) p(x i , x i-1 \pa xi |pa xi )<label>(15)</label></formula><p>This leads to the following alternative expressions for the cross-entropy H(p, q) = x p(x) log q(x) in terms of multivariate entropy and information, which only depend on the empirical joint distribution p(x),</p><formula xml:id="formula_32">H(p, q) = m i=1 H(x i |Pa Xi ) = H(X 1 , • • • , X m ) + m i=1 I(X i ; X i-1 \Pa Xi |Pa Xi )<label>(16)</label></formula><p>where m i=1 I(X i ; X i-1 \Pa Xi |Pa Xi ) can be decomposed, using the chain rule and Eq. 11, into unconditional multivariate information terms, which exactly cancel all the multivariate information of the non-ac-connected subsets of variables in the multivariate entropy decomposition, Eq. 6.</p><p>Note, however, that this result obtained for Bayesian networks requires an explicit factorization of the global model distribution, q(x), in terms of the empirical distribution, p(x), which is not known and presumably does not exist, in general, for ancestral graphs.</p><p>Alternatively, assuming that the empirical and model distributions are positive (∀x, p(x) &gt; 0, q(x) &gt; 0), it is always possible to factorize them into factors associated to each (cross) information term in the (cross) entropy decomposition, Eq. 6, as,</p><formula xml:id="formula_33">q(x) = m i=1 q(x i ) × m i&lt;j q(x i , x j ) q(x i )q(x j ) × m i&lt;j&lt;k q(x i , x j , x k )q(x i )q(x j )q(x k ) q(x i , x j )q(x i , x k )q(x j , x k ) × • • •<label>(17)</label></formula><p>where all the marginal distributions over a subset of variables, e.g. q(x i , x j , x k ) = ℓ̸ =i,j,k q(x) or p(x i , x j , x k ) = ℓ̸ =i,j,k p(x), cancel two-by-two by construction.</p><p>This can be illustrated on a simple example of a two-collider path including one bidirected edge, X → Z ←→ Y ← W (Fig. <ref type="figure" target="#fig_3">1D</ref>), valid for q(.) and p(.) alike, q(x, z, y, w) = q(x) q(z) q(y) q(w) × q(x, z) q(x) q(z) q(z, y) q(z) q(y) q(y, w) q(y) q(w) q(x, y) q(x) q(y) q(x, w) q(x) q(w) q(z, w) q(z) q(w) × q(x) q(z) q(y) q(x, z, y) q(x, z) q(x, y) q(z, y) q(z) q(y) q(w) q(z, y, w) q(z, y) q(z, w) q(y, w) × q(x) q(z) q(w) q(x, z, w) q(x, z) q(x, w) q(z, w) q(x) q(y) q(w) q(x, y, w) q(x, y) q(x, w) q(y, w) × q(x, z) q(z, y) q(y, w) q(x, y) q(x, w) q(z, w) q(x, z, y, w) q(x, z, y) q(x, z, w) q(x, y, w) q(z, y, w) q(x) q(y) q(z) q(w) <ref type="bibr" target="#b17">(18)</ref> where all individual distribution marginals on subsets of variables, e.g. q(x), q(x, z), q(x, z, y) (or p(x), p(x, z), p(x, z, y)), cancel two-by-two by construction, except q(x, z, y, w) (or p(x, z, y, w)).</p><p>In addition and only for the model distribution q(.), all ratios in gray in Eq. 18 also cancel due to Markov independence relations across non-ac-connected subsets (see proof of Theorem 1). This leaves a truncated factorization retaining all and only the ac-connected subsets of variables in the graph, which we propose to estimate on empirical data by substituting the remaining q(.) terms by their empirical counterparts p(.), see Proposition 3.</p><p>This leads to the following global factorization for q(.) in terms of p(.), q(x, z, y, w) ≡ p(x) p(z) p(y) p(w) p(x, z) p( <ref type="formula">x</ref> </p><p>where the terms in gray have been passed to the lhs of Eq. 18 applied to p(.). This ultimately leads to the analog of the Bayesian Network factorization in Eq. 15 but for the two-collider path, X → Z ←→ Y ← W (Fig. <ref type="figure" target="#fig_3">1D</ref>), q(x, z, y, w) ≡ p(x, z, y </p><p>where the last three factors "correct" the expression of p(x, z, y, w) for the three (conditional) independences entailed by the underlying graph, that is, X ⊥ W , Z ⊥ W |X, and X ⊥ Y |W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Relation to the head-and-tail factorizations</head><p>The head-and-tail factorizations of the model distribution of an acyclic directed mixed graph, introduced by <ref type="bibr">Richardson 2009 [6]</ref>, do not correspond to a single factorized equation (as with Bayesian graphs, Eq. 15) but to multiple factorized equations, which enable the parametrization of the joint probability distribution with independent parameters for ancestrally closed subsets of vertices.</p><p>For instance, the head-and-tail factorizations of the simple two-collider path including one bidirected edge, X → Z ←→ Y ← W , introduced above, Fig. <ref type="figure" target="#fig_3">1D</ref>, correspond to the following equations <ref type="bibr" target="#b5">[6]</ref>,</p><p>q(x, w) = q(x) q(w) q(x, z) = q(z|x) q(x) q(y, w) = q(y|w) q(w) q(x, z, w) = q(z|x) q(x) q(w) q(x, y, w) = q(y|w) q(w) q(x) q(x, z, y, w) = q(z, y|x, w) q(x) q(w)</p><p>Importantly, these head-and-tail factorizations imply additional relations such as q(y|w) = q(y|x, w) (i.e. X ⊥ Y |W ) obtained by comparing the last two relations in Eqs. 21 after marginalizing q(x, z, y, w) over z. However, such implicit conditional independence relations are not verified by the empirical distribution p(.) in general and prevent the estimation of the head-and-tail factorizations by substituting the rhs q(.) terms in Eqs. 21 with their empirical counterparts p(.), as in the case of Bayesian networks, Eq. 15.</p><p>Indeed, while the head-and-tail factorization relations, Eqs. 21, obey the local and global Markov independence relations entailed by the graphical model, Fig. <ref type="figure" target="#fig_3">1D</ref>, leading to the cancellation of all factors associated to non-ac-connected subsets in gray in Eq. 18, the remaining head-and-tail factors cannot be readily estimated with the empirical distribution p(.).</p><p>In particular, the cross-entropy of the two-collider path of interest, Fig. <ref type="figure" target="#fig_3">1D</ref>, obtained with the headand-tail factorizations corresponds to<ref type="foot" target="#foot_1">foot_1</ref> H(p, q) = -p(x, z, y, w) log q(z, y|x, w) q(x) q(w). Then, estimating the q(.) terms with their p(.) counterparts leads to the cross-entropy of a Bayesian graph, Fig. <ref type="figure" target="#fig_3">1E</ref>, with a different Markov equivalent class than the ancestral graph of interest, Fig. <ref type="figure" target="#fig_3">1D</ref>. A similar discrepancy is obtained with a c-component factorization which leads to the cross-entropy of the Bayesian graph of Fig. <ref type="figure" target="#fig_3">1E</ref> without edge X → Y , corresponding to a different Markov equivalence class than the previous two graphs, Figs. <ref type="figure" target="#fig_3">1D</ref> &amp;<ref type="figure">E</ref>.</p><p>These examples illustrate the difficulty to exploit the c-component or head-and-tail factorizations to estimate the likelihood of ancestral graphs including bidirected edge(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Node and edge scores based on Normalized Maximum Likelihood criteria</head><p>Search-and-score methods based on likelihood estimates need to properly account for finite sample size, as cross-entropy minimization leads to ever more complex models, resulting in model overfitting for finite datasets. BIC regularization is valid in the asymptotic limit of very large datasets and leads to the following finite size corrections of the cross-information terms in the likelihood decomposition Eq. 12,</p><formula xml:id="formula_38">I(C) → I ′ (C) = I(C) - 1 2 |C| k=1 (1 -r k ) log N N<label>(22)</label></formula><p>for categorical datasets, where r k is the number of categories or levels of the kth variable of C.</p><p>However, BIC regularization tends to overestimate finite size corrections, leading to lower recall, in general. In order to better take into account finite sample size, we used instead the (universal) Normalized Maximum Likelihood (NML) criterion <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, which amounts to normalizing the likelihood function over all possible datasets with the same number N of samples.</p><p>Moreover, as search-and-score structure learning methods need to rely on heuristic rather than exhaustive search, we have implemented a computationally efficient search-and-score method based on the likelihood decomposition of ancestral graphs (Eq. 12) limited to the close surrounding vertices of each node and edge. These node and edge scores, detailed below, extend MIIC's unshielded triple scores to higher-order local information scores including ac-connected subsets of vertices with a maximum of two-collider paths.</p><p>Node score. We first used the factorized Normalized Maximum Likelihood (fNML) complexity <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> to define a local score for each node X i , which extends the decomposable likelihood of Bayesian graphs given each node's parents, Eq. 2, to all non-descendant neighbors, Pa ′ X i ,</p><formula xml:id="formula_39">L D|GX i = e -N. Scoren(Xi) = e -N H(Xi|Pa ′ X i ) |D ′ |=N e -N H(Xi|Pa ′ X i )<label>(23)</label></formula><p>= e</p><p>-N H(Xi|Pa ′ X i</p><p>)-</p><formula xml:id="formula_40">q i j log C r i n j<label>(24)</label></formula><p>= e where n jk corresponds to the number of data points for which X i is in its kth state and its nondescendant neighbors in their jth state, with n j = ri k n jk . The universal normalization constant C r n is then computed by summing the numerator over all possible partitions of the n data points into a maximum of r subsets,</p><formula xml:id="formula_41">ℓ 1 + ℓ 2 + • • • + ℓ r = n with ℓ k ⩾ 0, C r n = ℓ1+ℓ2+•••+ℓr=n n! ℓ 1 !ℓ 2 ! • • • ℓ r ! r k=1 ℓ k n ℓ k (27)</formula><p>function using Hartemink's pairwise mutual information method <ref type="bibr" target="#b39">[40]</ref>. For these toy models, the edge orientation scores are computed assuming the correct parents of each node.</p><p>The prediction of the edge orientation scores are summarized in Table <ref type="table" target="#tab_6">3</ref> in % of replicates displaying directed edges (wrong) or bidirected edge (correct) as a function of increasing dataset size N . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cross-entropy decomposition of ancestral graphs. Examples of cross-entropy decomposition ofancestral graphs (red edges, lhs) in terms of relevant multivariate cross-information contributions I(C) with C ⊆ V (red nodes, rhs). Simple graphs: (A) without unshielded colliders, (B) with a single or non-overlapping unshielded colliders, (C) with overlapping unshielded colliders through three or more (conditionally) independent parents or (D) through a two-(or more)-collider path. (E) Bayesian graph corresponding to the head-and-tail factorization of the two-collider path in (D) estimated using the empirical distribution p(.), see Appendix C. (F) Simple Bayesian graph not Markov equivalent to an ancestral graph (G) sharing the same edges and unshielded collider<ref type="bibr" target="#b23">[24]</ref>. Solid black edges correspond to direct connections or collider paths confined to the corresponding ac-connected subset C, while wiggly edges indicate collider paths extending beyond C yet indirectly connected to C by an ancestor path, marked with dashed edges, see Definition 2. By contrast, graphs H and I illustrate the fact that collider paths may not be unique nor conserved between two Markov equivalent graphs (i.e. sharing the same cross-information terms)<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Benchmark results on ancestral graphs of increasing complexity. Benchmark results are averaged over 50 independent categorical datasets from ancestral graphs obtained by hiding 0%, 5%, 10% or 20% of variables in Discrete Bayesian Networks of increasing complexity (see main text): Alarm, Insurance, Barley and Mildew. MIIC_search&amp;score results are compared to MIIC results used as starting point for MIIC_search&amp;score and FCI<ref type="bibr" target="#b40">[41]</ref>. Causal discovery performance is assessed in terms of Precision and Recall relative to the theoretical PAGs, while counting as false positive all correctly predicted edges but with a different orientation as the directed or bidirected edges of the PAG. Error bars: 95% confidence interval.</figDesc><graphic coords="10,112.92,377.87,94.23,65.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Benchmark results on bootstrap datasets from ancestral graphs of increasing complexity.</figDesc><graphic coords="11,112.92,329.90,94.23,65.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A. 1</head><label>1</label><figDesc>m-connection vs m'-connection criteria An ancestral graph can be interpreted as encoding a set of conditional indepencence relations by a graphical criterion, called m-separation, based on the concept of m-connecting paths, which generalizes the separation criteria of Markov and Bayesian networks to ancestral graphs. Definition 4. [m-connecting path] A path π between X and Y is m-connecting given a (possibly empty) subset C ⊆ V (with X, Y / ∈ C) if: i) its non-collider(s) are not in C, and ii) its collider(s) are in An G (C). Definition 5. [m-separation criterion] The subsets A and B are said to be m-separated by C, noted A ⊥ m B|C, if there is no m-connecting path between any vertex in A and any vertex in B given C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 4 .</head><label>4</label><figDesc>[Corollary 3.15  in<ref type="bibr" target="#b7">[8]</ref>] In an ancestral graph G, there is a m ′ -connecting path µ between X and Y given C if and only if there is a (possibly different) m-connecting path π between X and Y given C.Hence, Lemma 4 implies that m ′ -separation and m-separation criteria are in fact equivalent, as an absence of m ′ -connecting paths implies an absence of m-connecting paths and vice versa. This enables to reformulate the m-separation criterion above as, Definition 7. [m ′ -separation (and m-separation) criteria] The subsets A and B are said to be m ′separated (or m-separated) by C, if all paths from any X ∈ A to any Y ∈ B have either i) a non-collider in C, or ii) a collider not in An G ({X, Y } ∪ C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Local scores for the orientation of a single directed or bidirected edge, see Appendix D.</figDesc><table><row><cell>Edge</cell><cell>Information</cell><cell>Symmetrized fNML complexity (Markov equivalent)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2-3.    </figDesc><table><row><cell></cell><cell cols="5">ALARM (509 parameters)</cell><cell></cell><cell cols="5">INSURANCE (1,008 parameters)</cell><cell></cell><cell cols="5">BARLEY (114,005 parameters)</cell><cell></cell><cell cols="5">MILDEW (540,150 parameters)</cell></row><row><cell></cell><cell cols="2">Precision</cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell cols="2">Recall</cell></row><row><cell>0 % LV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 % LV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 % LV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Model 1, X 2 X 4 Model 2, X 2 X 4 Model 3, X 2 X 4 Model 3, X 4 X 6</figDesc><table><row><cell>N</cell><cell>←</cell><cell>→</cell><cell>↔</cell><cell cols="2">← →</cell><cell>↔</cell><cell cols="2">← →</cell><cell>↔</cell><cell cols="2">← →</cell><cell>↔</cell></row><row><cell>1000</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">50 42</cell><cell>8</cell><cell cols="2">2 98</cell><cell>0</cell><cell>94</cell><cell>6</cell><cell>0</cell></row><row><cell>5000</cell><cell>0</cell><cell>68</cell><cell>32</cell><cell>18</cell><cell>2</cell><cell>80</cell><cell cols="2">22 78</cell><cell>0</cell><cell cols="2">62 38</cell><cell>0</cell></row><row><cell>10000</cell><cell>0</cell><cell>10</cell><cell>90</cell><cell>0</cell><cell cols="2">0 100</cell><cell cols="2">12 70</cell><cell>18</cell><cell cols="2">40 16</cell><cell>44</cell></row><row><cell>20000</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>2</cell><cell>0</cell><cell>98</cell></row><row><cell>35000</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell>0 100</cell></row><row><cell>50000</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell cols="2">0 100</cell><cell>0</cell><cell>0 100</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The genesis of Theorem 1 and Proposition 3 is further discussed in https://openreview.net/forum? id=Z2f4Laqi8U&amp;noteId=8GLWeaAKc9.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Indeed, all terms in Eq. 18 actually cancel two-by-two by construction, whatever their factorized expression, except for the remaining joint-distribution over all variables, q(x, z, y, w) = q(z, y|x, w) q(x) q(w).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>HI acknowledges support from <rs type="funder">CNRS</rs>, <rs type="funder">Institut Curie</rs> and <rs type="institution">Prairie Institute</rs>, as well as funding from <rs type="grantNumber">ANR-22-PESN-0002</rs> "<rs type="projectName">AI4scMed</rs>" and <rs type="funder">ANR</rs> <rs type="grantNumber">23CE13001802</rs> "Patterning" grants. NL acknowledges a PhD fellowship from <rs type="funder">CNRS-Imperial</rs> College joint PhD programme.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Uk8ZveQ">
					<idno type="grant-number">ANR-22-PESN-0002</idno>
					<orgName type="project" subtype="full">AI4scMed</orgName>
				</org>
				<org type="funding" xml:id="_j3Dgeu3">
					<idno type="grant-number">23CE13001802</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>which can in fact be computed in linear-time using the following recursion <ref type="bibr" target="#b37">[38]</ref>,</p><p>with C 1 n = 1 for all n and applying Eq. 31 below for r = 2. However, for large n and r, C r n computation tends to be numerically unstable, which can be circumvented by implementing the recursion on parametric complexity ratios D r n = C r n /C r-1 n rather than parametric complexities themselves <ref type="bibr" target="#b34">[35]</ref> as,</p><p>for r ⩾ 3, with C 1 n = 1 and C 2 n = D 2 n , which can be computed directly with the general formula, Eq. 27, for r = 2,</p><p>or its Szpankowski approximation for large n (needed for n &gt; 1000 in practice) <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>,</p><p>This leads to the following local score for each node X i , which is minimized over alternative combinations of non-descendant neighbors, Pa ′</p><p>∪ Ne X i , in the first step of the local search-and-score algorithm (step 1) detailed in the main text,</p><p>Edge scores. We then defined several edge scores to optimize the orientation of each edge, X Y , given its close surrounding vertices.</p><p>To this end, we first introduced a local score for node pairs which simply sums the node scores, Eq. 34, for each node. The resulting pair scores are listed in Table <ref type="table">2</ref> for unconnected node pairs and for pairs of nodes connected by a directed edge, where Pa</p><p>with their corresponding combinations of levels, q y\x and q x\y . </p><p>Then, edge scores for directed edges, X → Y and Y → X, are defined w.r.t. to the edge removal score, X ̸ Y , by substracting the pair scores of unconnected pairs to the pair scores of directed 20 edges, leading to the following edge orientation scores,</p><p>However, if r x ̸ = r y , the fNML complexities of these orientation scores are not identical for Markov equivalent edge orientations between nodes sharing the same parents (or spouses) <ref type="bibr" target="#b46">[47]</ref>,</p><p>Pa ′ Y\X = Pa ′ X\Y = Pa ′ and q y\x = q x\y , despite sharing the same conditional mutual information,</p><p>This suggests to symmetrize the fNML complexities for edge orientation scores by averaging them over each directed orientation, as for the conditional information in Eq. 37, leading to the proposed fNML complexity for directed edges given in Table <ref type="table">1</ref> in the main text.</p><p>For bidirected edges, the proposed local orientation score accounts for all ac-connected subsets in close vicinity of the bidirected edge, which concerns all subsets including either X and any combination (possibly void) of parents or spouses different from Y (i.e. corresponding to the information contributions H(X|Pa ′ X\Y )) or Y and any combination of parents or spouses different from X (i.e. corresponding to the information contributions H(Y |Pa ′ Y\X )) or, else, including both nodes X and Y plus any combination of their parents or spouses, corresponding to the following information contribution, -I(X; Y |Pa ′ XY ), where Pa ′ XY = Pa ′ X\Y ∪Pa ′ Y\X . This last term, -I(X; Y |Pa ′ XY ), contains all the remaining information contributions once the bidirected orientation score is given relative to the edge removal score (Table <ref type="table">2</ref>) as for the two directed orientation scores, above. Finally, the symmetrized fNML complexity associated with a bidirected edge should be computed with the whole set of conditioning parents or spouses, Pa ′ XY , as indicated in Table <ref type="table">1</ref>. Note that this bidirected orientation score becomes also Markov equivalent to the two directed orientation scores, as required, when the nodes share the same parents and spouses, i.e. Pa ′ XY = Pa ′ Y\X = Pa ′ X\Y and q xy = q y\x = q x\y in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Toy models</head><p>Fig. <ref type="figure">4</ref> shows three simple ancestral models used to test MIIC_search&amp;score orientation scores (Table <ref type="table">1</ref>) to effectively predict bidirected orientations when the end nodes do not share the same parents (Model 1), share some parents (Model 2) or when the bidirected edge is part of a longer than two-collider paths (Model 3). The data is generated from the theoretical DAG using the rmvDAG function in the pcalg package <ref type="bibr" target="#b47">[48]</ref>. Each node follows a normal distribution, and the data is discretized using bnlearn's discretize</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graphoids: A graph-based logic for reasoning about relevance relations, or when would x tell you more about y if you already know z</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>UCLA Computer Science Department</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: models, reasoning and inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>second edn</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>second edn</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A factorization criterion for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09<address><addrLine>Arlington, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="462" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A general identification condition for causal effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ancestral graph markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computing maximum likelihood estimates in recursive linear models with correlated errors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2329" to="2348" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood fitting of acyclic directed mixed graphs to binary data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI&apos;10</title>
		<meeting>the 26th Conference on Uncertainty in Artificial Intelligence, UAI&apos;10<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Score-based vs constraint-based causal learning in the presence of confounders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CFA@UAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximal ancestral graph structure learning via exact search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, C. de</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Campos</surname></persName>
		</editor>
		<editor>
			<persName><surname>Maathuis</surname></persName>
		</editor>
		<meeting>the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, C. de</meeting>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="1237" to="1247" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Greedy equivalence search in the presence of latent confounders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Bucur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="443" to="452" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster algorithms for markov equivalence</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</editor>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multivariate information transmission</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of the IRE Professional Group on Information Theory (TIT)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="93" to="111" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the amount of information</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Probab. Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="439" to="447" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple mutual informations and multiple interactions in frequency data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="26" to="45" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new outlook on shannon&apos;s information measures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="466" to="474" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A polynomial time algorithm for determinint dag equivalence in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the 6th International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov equivalence classes for maximal ancestral graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;02</title>
		<meeting>the Eighteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards characterizing markov equivalence classes for directed acyclic graphs with latent variables</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;05</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;05<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating markov equivalent maximal ancestral graphs by single edge replacement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;05</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;05<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Markov equivalence for ancestral graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2808" to="2837" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scoring ancestral graph models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>CMU-PHIL 98</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A characterization of markov equivalence classes for directed acyclic graphs with latent variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;07</title>
		<meeting>the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">July 12-16, 2015. 2015</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1005662</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The m-connecting imset and factorization for admg models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>Arxiv 2207.08963</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards standard imsets for maximal ancestral graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A fast score-based search algorithm for maximal ancestral graphs using entropy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<idno>Arxiv 2402.04777</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reliable causal discovery based on mutual information supremum principle for finite datasets, WHY21</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Da Câmara Ribeiro-Dantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning interpretable causal networks from very large datasets, application to 400, 000 medical records of breast cancer patients</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D C</forename><surname>Ribeiro-Dantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hettal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Hamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">109736</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning clinical networks from medical records based on information estimates in mixed-type data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1007866</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">off2: A network reconstruction algorithm based on 2-point and 3-point information statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Constraint-based causal structure learning with consistent separating sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A linear-time algorithm for computing the multinomial stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning using factorized nml universal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2008 Information Theory and Applications Workshop (ITA-2008</title>
		<meeting>2008 Information Theory and Applications Workshop (ITA-2008</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Bayesian Networks with the bnlearn R Package</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Causal-learn: Causal discovery in python</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Universal sequential coding of single messages</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Shtarkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problems of Information Transmission</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3" to="17" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tabus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Min. Descrip. Length Theory Appl</title>
		<imprint>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="2005">2005</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Average case analysis of algorithms on sequences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient computation of stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</editor>
		<meeting>the Ninth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Computationally efficient methods for mdl-optimal density estimation and data clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<title level="m">A Transformational Characterization of Equivalent Bayesian Network Structures, UAI &apos;95: Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causal inference using graphical models with the r package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
