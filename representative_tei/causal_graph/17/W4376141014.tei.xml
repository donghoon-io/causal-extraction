<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video question answering via tra c knowledge base and question classi cation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-11">May 11th, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Systems on January 16th</orgName>
								<orgName type="institution" key="instit1">Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Zhejiang University of Science and Technology Yuchen Wang Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Zhejiang University of Science and Technology Xuefen Lin Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>2024</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Systems on January 16th</orgName>
								<orgName type="institution" key="instit1">Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Zhejiang University of Science and Technology Yuchen Wang Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Zhejiang University of Science and Technology Xuefen Lin Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>2024</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weifeng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Systems on January 16th</orgName>
								<orgName type="institution" key="instit1">Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Zhejiang University of Science and Technology Yuchen Wang Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Zhejiang University of Science and Technology Xuefen Lin Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>2024</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Systems on January 16th</orgName>
								<orgName type="institution" key="instit1">Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Zhejiang University of Science and Technology Yuchen Wang Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Zhejiang University of Science and Technology Xuefen Lin Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>2024</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuefen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Systems on January 16th</orgName>
								<orgName type="institution" key="instit1">Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Zhejiang University of Science and Technology Yuchen Wang Zhejiang University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Zhejiang University of Science and Technology Xuefen Lin Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>2024</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University of Science and Technology</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou Zhejiang</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video question answering via tra c knowledge base and question classi cation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-11">May 11th, 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.21203/rs.3.rs-2880996/v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video question answering, Knowledge, Transformer, Question classi cation Video question answering</term>
					<term>Knowledge</term>
					<term>Transformer</term>
					<term>Question classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Question Answering (VideoQA) is a task that involves answering questions related to videos. The main idea is to understand the content of the video and combine it with the relevant semantic context to answer different types of questions. Existing methods typically analyze the spatio-temporal correlations of the entire video to answer questions. However, for some simple questions, the answer is only related to a specific frame of the video, and analyzing the entire video undoubtedly increases the learning cost. For some complex questions, the information contained in the video is limited, and these methods are not sufficient to fully answer such questions. Therefore, this paper proposes a VideoQA model based on question classification and a traffic knowledge base.The model starts from the perspective of the question and classifies the questions into general scene questions and causal questions, using different methods to process these two types of questions. For general scene questions, we first extract the key frames of the video to convert it into a simpler image question-answering task, and then use top-down and bottom-up attention mechanisms to process it. For causal questions, we design a lightweight traffic knowledge base that provides relevant traffic knowledge not originally present in VideoQA datasets, to help the model reason. We then use a question and knowledge-guided aggregation graph attention network to process causal questions.Experiments show that our model performs better on the TrafficQA dataset than models using millions of external data for pre-training, while greatly reducing resource costs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there has been increasing attention on visual question answering (VQA). This task connects computer vision and natural language processing to return the correct answer from visual content based on user questions. VQA is mainly divided into image-based and video-based question answering, with the latter being particularly challenging as it requires not only extracting the semantic relationship between questions and images, but also understanding the temporal semantic relationships between questions and frames in the video.</p><p>With the continuous development of video question answering, many researchers have created datasets in various fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, most Fig. <ref type="figure">1</ref>: Example of a question and answer based on transportation knowledge existing methods still struggle to achieve good results in video question answering. Existing video question answering methods can be divided into four categories: encoder-decoder, attention mechanism, memory network model, and other methods. In video question answering, many frames in the video are redundant, and existing models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> analyze all frames in the video for all questions. For some simple questions, this method includes frames that are not related to the question, increasing the model burden and interfering with correct predictions. In addition, existing models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> perform poorly on datasets <ref type="bibr" target="#b1">[2]</ref> that require strong causal reasoning and prediction abilities. K-PSTANet <ref type="bibr" target="#b5">[6]</ref> introduced an external knowledge base to help the model make predictions, improving the model's understanding ability to some extent. However, most external knowledge bases <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> are too redundant and can only provide assistance for specific questions, with little effect on causal reasoning and prediction questions.</p><p>To address the aforementioned issues, in this paper we propose a video question answering model based on traffic knowledge base and question categorization (TK-QC). Specifically, we design a lightweight traffic knowledge base tailored to traffic video question-answering datasets. As shown in Fig. <ref type="figure">1</ref>, the text represents annotations for a traffic accident. From the traffic knowledge base, we discover that "running a red light" is "violation of traffic rules by vehicles." For humans, it is easy to know that the correct answer in figure is "violation of traffic rules by vehicles," but for machines, it is difficult to obtain this answer without additional knowledge. We design a lightweight traffic knowledge base based on existing traffic question-answering and traffic rules, which mainly includes some traffic rules, extreme weather conditions on roads, safety environment on roads, and some types of vehicles, to help the model better infer the causal relationship of the video. In addition, we design a classification method to handle different types of questions separately. For some simple questions that do not involve causal reasoning, we first extract keyframes from the video and convert them into image questionanswering problems to process. For questions involving causal reasoning, we first return external knowledge related to the video question from the lightweight traffic knowledge base and then use a question and knowledge-guided aggregation graph attention network to extract the spatiotemporal dynamic information of the video and combine it with the question and external traffic knowledge to answer the question. Our main contributions are as follows:</p><p>• We designed a lightweight traffic knowledge base for causal reasoning to help the model reason about the causal relationships in questions. By using additional information, our proposed model can answer causal relationship questions more accurately. • We proposed a classification model to process different types of questions, greatly reducing the computational cost and overhead of the model. • We proposed a question and knowledge-guided aggregation graph attention network that not only extracts spatiotemporal features of the video but also combines external knowledge, questions, and spatiotemporal video features through cross-modal attention.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Question Answering</head><p>The task of image question answering requires a model to generate accurate answers given an image and a natural language question related to the image. The earliest work on this problem was proposed by Malinowski et al. <ref type="bibr" target="#b8">[9]</ref>, who introduced a method that combines semantic analysis with image segmentation using Bayesian methods. Geman et al. <ref type="bibr" target="#b9">[10]</ref> proposed an automatic query generator that generates a series of binary questions by training on annotated images. These early methods shared a common characteristic of limiting the questions to predefined forms. With the development of deep learning and neural networks, the feasibility of the interaction between the two modalities, i.e., image and natural language, was gradually discovered, and image captioning tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> were first explored, followed by image question answering. Malinowski et al. <ref type="bibr" target="#b12">[13]</ref> proposed a method called "Neural Image QA" that applied long short-term memory (LSTM). Noh et al. <ref type="bibr" target="#b13">[14]</ref> proposed a CNN with dynamic parameter layers (DPPnet) to handle the task. Fukui et al. <ref type="bibr" target="#b14">[15]</ref> used multimodal pooling to handle the joint interaction between image and natural language. Similarly, Kim et al. <ref type="bibr" target="#b15">[16]</ref> proposed a multimodal residual learning framework to learn the modal interaction between image and natural language.</p><p>With the widespread use of attention mechanisms in natural language processing and computer vision, attention mechanisms have gradually been applied in the multimodal field. Early applications in the visual field <ref type="bibr" target="#b16">[17]</ref> still came from captioning tasks. Lin et al. <ref type="bibr" target="#b17">[18]</ref> extended the image captioning ranking method to image question answering tasks. Zhu et al. <ref type="bibr" target="#b18">[19]</ref> added spatial attention to the standard LSTM model. Yang et al. <ref type="bibr" target="#b19">[20]</ref> proposed an attention layer, which is specified by a single layer weight that calculates the attention distribution between image positions using the question and CNN feature maps with softmax activation functions. Anderson et al. <ref type="bibr" target="#b20">[21]</ref> proposed a top-down and bottom-up attention mechanism to handle image question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Question Answering</head><p>As a natural extension of image question answering, video question answering is a more challenging task that requires models to generate or select the correct answer based on a video and a natural language question. Compared to image question answering, video question answering has greater exploratory potential. To address this problem, in recent years, an increasing number of researchers have proposed different approaches.Jang et al. <ref type="bibr" target="#b0">[1]</ref> proposed a dual LSTM-based approach with both spatial and temporal attention. Ye et al. <ref type="bibr" target="#b21">[22]</ref> developed an attribute-enhanced attention network that unifies frame-level attributes. Xu et al. <ref type="bibr" target="#b4">[5]</ref> proposed a progressively refined attention model that fuses appearance and motion flow features to represent video information. Na et al. <ref type="bibr" target="#b22">[23]</ref> developed a read-write memory network that stores temporal information of videos and integrates multimodal features. Zhao et al. <ref type="bibr" target="#b23">[24]</ref> proposed an adaptive hierarchical encoder to handle long videos. Gao et al. <ref type="bibr" target="#b24">[25]</ref> designed a co-memory network that fuses motion and appearance features of videos. Liang et al. <ref type="bibr" target="#b25">[26]</ref> proposed the Focal Visual-Text Attention Network (FVTA) for visual question answering reasoning. ClipBERT <ref type="bibr" target="#b26">[27]</ref> utilized image captioning data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> for pre-training, but its performance improvement on temporal reasoning tasks is limited due to the difficulty of learning temporal relationships from static images. Xiao et al. <ref type="bibr" target="#b3">[4]</ref> proposed a Video Graph Transformer model that extracts spatiotemporal features of videos using graph neural networks and multiple multihead self-attention mechanisms.However, most of these models approach the problem from the video perspective, thus ignoring the question itself. Therefore, in this paper, we take a question-centric approach and adopt different types of processing methods for different types of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Bases and Question Answering</head><p>When answering questions about video content, additional knowledge outside of the video is often required. Structured knowledge bases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> provide large-scale standardized representations, which can be constructed either by manual annotation (e.g., DBpedia <ref type="bibr" target="#b6">[7]</ref>, Freebase <ref type="bibr" target="#b7">[8]</ref>, and Wikidata <ref type="bibr" target="#b27">[28]</ref>) or by automatic conversion of unstructured and semi-structured data (e.g., YAGO <ref type="bibr" target="#b29">[30]</ref>, NEIL <ref type="bibr" target="#b30">[31]</ref>, OpenIE <ref type="bibr" target="#b31">[32]</ref>, and ConceptNet <ref type="bibr" target="#b32">[33]</ref>). In recent years, there has been growing interest in natural language question answering (QA) tasks based on structured knowledge bases (referred to as KB-QA). KB-QA methods are typically divided into two categories: information retrieval methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, which first retrieve a set of candidate answers and then select the final answer based on scores; and semantic analysis methods <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, which convert the question into a sequence representation and then map it to the knowledge base for querying. Recent research has shown that knowledge bases are also helpful for video QA. JIN et al. <ref type="bibr" target="#b5">[6]</ref> encoded text (knowledge) extracted from DBpedia as vector representations using the Word2Sec model. They combined the knowledge representation with visual features and used an LSTM model to predict the final answer. In this paper, we design a lightweight transportation knowledge base and integrate it into our model for answer inference.</p><p>3 Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We approached the problem from the perspective of the question and used different methods to handle different types of questions. We roughly divided all questions into two categories. The first type of question requires video analysis, such as "What factors may have caused this accident." These types of questions typically require combining video analysis to deduce the answer, which we refer to as causal questions. The second type of question can be answered directly by analyzing a single frame of the video, such as "How many cars were involved in the accident." We refer to these types of questions as general scene questions.We denote the causal inference question by q a ∈ Q ,general scene questions by q b ∈ Q ,the videos by v ∈ V ,the answer by a ∈ A ,where Q , v and A are the sets of questions, video, and answers, respectively. Given a video v and a question q , we need to infer the answer through v and q . To help the model better reason about causal inference questions, we designed a lightweight traffic knowledge base K . We optimize the following objective to infer the answer to causal questions:</p><formula xml:id="formula_0">a * = arg max a∈A H W (a|q a , v, A, k)<label>(1)</label></formula><p>We infer the answer to general scenario questions by optimizing the following objective:</p><formula xml:id="formula_1">a * = arg max a∈A Γ W (a|q b , v, A)<label>(2)</label></formula><p>Here, k ∈ K represents the data returned by the traffic knowledge base, H W and Γ W are mapping functions for learnable parameters W . We designed the traffic knowledge-based video graph transformer model(TK-VGT) to implement the function H W in Eqn. (1) and used BUTD <ref type="bibr" target="#b20">[21]</ref> to implement the function Γ W in Eqn. (2) The overall framework is shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>For a given question, we first classifies it into either a causality question or a general scenario question. For causality questions, we preprocess the question-answer pairs and the video to obtain objects to query a transportation knowledge base. Then, we input the video, question-answer pairs, and knowledge base data into the TK-VGT model to derive the answer. For general scenario questions, we preprocess the video and questionanswer pairs and use the BUTD model to derive the answer. Our model processes questions differently based on their type and leverages video, question-answer pairs, and external knowledge bases to help it derive better answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification and Processing of General Scenario Questions</head><p>In this section, we have introduced an overview of the models used in classification methods, as well as their application to general scenario problems. As shown in Fig. <ref type="figure">3</ref>,For causal inference problems, we first use video frames as one of the inputs. After a series of feature extractions, they are incorporated into the Question and Knowledge-Guided Aggregated Graph Attention Network (QK-AGA), we extract key objects from the video using Faster R-CNN The Top-Down model then uses an attention mechanism to match the key words in the question with the object feature vectors extracted by the Bottom-Up part, obtaining a fused question vector.Finally, the BUTD model fuses the fused question vector with the sequence of object feature vectors extracted by the Bottom-Up part to generate a comprehensive image-question feature vector. This vector represents all the information of the image and the question and is passed to the output layer to predict the answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relating to the Knowledge Base</head><p>In this section, we will introduce methods related to knowledge bases. According to the experiments in <ref type="bibr" target="#b5">[6]</ref>, knowledge bases have a certain degree of assistance for video question answering models.</p><p>Although existing knowledge bases have extensive coverage, most of them are based on fundamental knowledge, such as DBpedia, which is extracted from Wikipedia and includes a large amount of Wikipedia information such as people, places, organizations, books, and movies. However, due to the fact that knowledge bases like DBpedia mainly contain definitions of fundamental knowledge, their assistance for video question answering models is limited. For example, the trafficQA dataset contains a large number of causal reasoning questions, which cannot be effectively answered by knowledge bases like DBpedia. Therefore, we designed a traffic knowledge base specifically for helping the model reason about traffic causal questions, and integrated it with our model. We wrote basic traffic information, traffic rules, and some knowledge about traffic tools into a lightweight traffic knowledge base by searching on Google, Baidu, and Wikipedia. The knowledge base is stored in the form of binary tuples, with each tuple containing an object and its attribute. For example: ["Running a red light", "Violation of traffic rules by vehicles"], where "running a red light" is the event object and "Violation of traffic rules by vehicles" is its attribute. Humans can easily understand that running a red light violates traffic rules, but for machines, they can only recognize that a vehicle is running a red light. Therefore, additional knowledge bases can help the model make better inferences. Our lightweight traffic knowledge base contains a total of 312 binary tuples, including events that violate traffic rules, extreme weather conditions on roads, negligent conditions for road safety, types of motor vehicles, subtypes of motor vehicles, and other traffic information. Next, we will introduce methods related to knowledge bases.</p><p>First, we use BERT <ref type="bibr" target="#b38">[39]</ref> to extract text features X a = (X a i ) n i=1 of objects in the traffic knowledge base, where n is the number of binary tuples in the lightweight traffic knowledge base. We attempt to obtain the main objects in the video frames and use them as keywords to search external knowledge bases. As existing object detection technology has gradually matured, we select the high-precision and highspeed Faster R-CNN as the target detector. For each frame, we select the top five predicted objects with the highest scores and obtain the object set</p><formula xml:id="formula_2">O i = (O i1 , O i2 , O i3 , O i4 , O i5</formula><p>) for the i-th frame. For a video, we select the top k objects with the highest frequency in the object set as the main objects in the video. We then use BERT to extract semantic features X o = (X o j ) k j=1 of the objects and calculate the cosine similarity between X o and X a to return the attribute of the binary tuple with the highest score. Semantic extraction is performed using BERT. Thus, we obtain the knowledge feature set F k = (f 1 , f 2 , ..., f k ) of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Causal problem solving</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Video feature extraction</head><p>This section focuses on the processing methods of causal problems, aiming to explore how to effectively analyze causal relationships in research and provide more in-depth theoretical and practical guidance for related academic studies. Given a video, we divide it into k frames on average. For each frame, we use pre-trained object detectors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref> to extract appearance features F r = {f ri } n i=1 and spatial positions B r = {b ri } n i=1 , where F r consists of n RoI-aligned features f ri and r i is the i-th object region in the frame. Meanwhile, we use a pre-trained ResNet model <ref type="bibr" target="#b40">[41]</ref> to extract image-level features F I = {f It } k t=1 for all frames, where f It is the global image feature for each frame. We identify the same objects across different frames by matching their appearance and spatial location.</p><formula xml:id="formula_3">S ij = φ(f t ri , f t+1 rj ) + λ * IoU (b t i , b t+1 j ), t ∈ {1, 2, ..., k}<label>(3)</label></formula><p>Where φ represents the cosine similarity between the sum of two objects in adjacent frames, and IoU is the intersection over union ratio used to detect the overlap similarity between the positions of objects i and j . The parameter λ is set to 1, t is the current frame.We use the n objects in the first frame as the initial objects and iteratively maximize S across consecutive frames to link the objects in each frame to the initial objects. This allows us to align objects across frames to ensure consistency of nodes and edges in the constructed graph.</p><p>And then, the appearance feature f r and position feature f loc are combined using the ELU layer:</p><formula xml:id="formula_4">f o = ELU (ΦW o ([f r ; f loc ]))<label>(4)</label></formula><p>Where [; ] denotes feature concatenation and f loc is obtained by applying a 1 × 1 convolution over the relative coordinates as in <ref type="bibr" target="#b41">[42]</ref>. The function ΦW o represents a linear transformation of the learnable parameter W o .Thus, we can obtain the combined feature f o of the position and appearance of the video frame. For the t-th frame, we combine the feature representations F o = {f oi } n i=1 of all objects to obtain their pairwise similarity: R t = σ(φW ak (F ot )φW av (F ot ) T ), t ∈ {1, 2, ..., k} (5)</p><p>Where φW ak and φW av are linear transformations with parameters W ak and W av respectively. σ represents the softmax operation for each row. The graph for the t-th frame is represented by G t = (F ot , R t ) , where F o represents the nodes and R represents the edges of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Question and Knowledge-Guided Aggregated Graph Attention Network</head><p>Compared to static images, videos contain more rich spatio-temporal dynamic information. In order to better capture the spatio-temporal information of videos, this section introduces a Question and Knowledge-Guided Aggregated Graph Attention Network (QK-AGA). This module first captures the dynamic interactions between different frame objects, condensing complex video spatio-temporal dynamics into video feature nodes. Then, through a cross-modal attention mechanism, it combines video spatio-temporal features, question-answering features, and external knowledge features to obtain multimodal information and improve model performance. The overall architecture is shown in Fig. <ref type="figure" target="#fig_3">5</ref>. This module first employs the multi-head selfattention(MHSA) to aggregate other node information of the same object in adjacent frames and enhance node representations, effectively integrating relevant content in the video into nodes with different attention weights and thereby extracting dynamic temporal interaction relationships in the video. Then, the U-layer graph attention convolution <ref type="bibr" target="#b28">[29]</ref> and global image feature f I are utilized to enhance the model's spatial interaction reasoning ability and global feature representation. Finally, an attention mechanism is used to fuse cross-modal text features, knowledge features, and video features, as described below.</p><p>To better capture the temporal motion features of video frames, we enhance the node representation by aggregating information from other nodes of the same object in adjacent frames to obtain the temporal context graph G out = (F ′ o , R ′ ) . Where F ′ o is obtained from F o through H selfattention blocks of, and then the relationship matrix R in Eqn. ( <ref type="formula">5</ref>) is updated based on the new nodes F ′ o . R ′ is obtained from R through H self-attention blocks.</p><formula xml:id="formula_5">F ′ oi = M HSA (H) (F oi ) (6) R ′ = M HSA (H) (R)<label>(7)</label></formula><p>To better reason about spatial interactions, we use the U-layer graph attention convolution[61] on all graphs:</p><formula xml:id="formula_6">F ′ (u) o = RELU ((R ′ + I)F ′ (u-1) o W (u) ) (8)</formula><p>Where W (u) represents the graph parameters for the u-th layer, I represents the identity matrix in the skip connection, and</p><formula xml:id="formula_7">F Oout = F ′ o + F ′ (U )</formula><p>o denotes the last skip connection. After obtaining the spatio-temporal features of the objects, we first aggregate the graph nodes of each frame through simple attention, and then use imagelevel features f I to supplement the video features,obtaining featureF G :</p><formula xml:id="formula_8">F G = ELU (φW m ([φW f (f I ); Σ n i=1 α i F O (out) i ])) (9)</formula><p>Where</p><formula xml:id="formula_9">α = σ(φW G (F Oout )) , φW G represents the linear transformation of parameter W G ∈ R d×1</formula><p>, and φW m and φW f represent the linear transformations of parameters W m ∈ R 2d×d andW f ∈ R 2048×d , Thus, we obtain the set representation F G = {f Gt } k t=1 of k frames. After obtaining the feature representation of the entire video, we use BERT to extract semantic features from the question-answer pairs:</p><formula xml:id="formula_10">X q = φW Q (BERT (Q)) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>Where W Q ∈ R 768×d , Q denotes the questionanswer pair. Given a set of video nodes f G , we combine them with text features X q = {x q m } M m=1 and knowledge features F k = {f k n } N n=1 through cross-modal attention mechanisms:</p><formula xml:id="formula_12">f kgf = f G + Σ M m=1 β m x q m + Σ N n=1 δ n f k n (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>Where</p><formula xml:id="formula_14">β = σ(f G (X q ) T ) , δ = σ(f G (F k ) T ) ,</formula><p>M is the number of text tokens, and N is the number of object sets in the video. We combine the extracted knowledge features, video features, and QA features using attention mechanism, which not only integrates textual semantics into visual semantics, but also cleverly adds additional knowledge features. Thus, we update the visual nodes to obtain</p><formula xml:id="formula_15">F kgf = {f kgf t } k t=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Global Transformer</head><p>After combining text features, knowledge features, and video features, to capture the temporal dynamics between clips, we use H layers of Transformer on the cross-modal feature (F kgf ) and add learnable sine time positional embeddings <ref type="bibr" target="#b38">[39]</ref>.Finally, we average and concatenate the output of the Transformer to obtain the global representation of the entire video f qvk , defined as follows:</p><formula xml:id="formula_16">f qvk = M P ool(M HSA (H) (F kgf ))<label>(12)</label></formula><p>The global transformer has two main advantages: 1) it preserves the overall hierarchical structure that progressively drives video elements at different granularities; 2) it enhances the compatibility of visual and textual features, facilitating crossmodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Answer prediction</head><p>For multiple-choice questions, we first extract the feature representation X A of each answer using BERT, and then obtain the final global representation f A = M pool(X A ) of the answer through an average pooling layer. We compute the similarity between f A and the video representation f qvk using a dot-product, and return the answer with the highest similarity score as the final prediction:</p><formula xml:id="formula_17">s = (f qvk ) T F A , a * = argmax(s)<label>(13)</label></formula><p>Where</p><formula xml:id="formula_18">F A = {f A a } |A| a=1 ∈ R |A|×d , |A|</formula><p>is the number of candidate answers. During training, we maximize the similarity of the correct answer for a given sample by optimizing the softmax cross-entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment 4.1 Dataset</head><p>To address the main and related issues in developing intelligent transportation solutions, <ref type="bibr" target="#b1">[2]</ref> et al. designed a video question-answering dataset called TrafficQA that is related to the field of transportation. The dataset includes 62,535 QA pairs and 10,080 videos, which were collected using a combination of online and offline methods to ensure diversity in weather conditions, time, road conditions, traffic events, and perspectives. The TrafficQA dataset consists of training and testing sets, with multiple-choice questions for video QA. Additionally, the dataset includes six challenging inference tasks that have stronger causal relationships compared to other datasets. These six inference tasks include basic video content analysis, event prediction, reverse reasoning, counterfactual </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Configuration</head><p>We designed a classification model to deal with different types of data, so we adopted two different processing methods when handling videos. For causal inference problems, we first sampled 20 frames evenly from each video, and then used the ResNet network to extract appearance features of each video frame, with the dimension of this feature set to 2048. Next, we used the method described in Section 3.4.1 to extract object-level features and set their dimension to 2048 as well. For the question features, we used a pre-trained BERT model to extract the question features, with the dimension of this feature set to 300. Then, we used the method described in Section 3.3 to extract the knowledge base features and set their dimension to 300. The hidden layer dimension of the model was set to d=512, the default number of layers in the transformer and the number of self-attention heads were set to H=1 and e=8, respectively. Additionally, the number of layers in the graph was set to U=2. In the training phase, we used the Adam optimizer with a learning rate of 0.00001 and a batch size of 8. For general scenario questions, we first used ffmpeg to extract video keyframes, converted them into image-based questions, and then trained them using the BUTD model <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">baseline</head><p>We compared our network with the following networks. VIS+LSTM <ref type="bibr" target="#b42">[43]</ref> used LSTM to encode image representations and text features. Since the original method takes a single image as input, we adopted the approach used in <ref type="bibr" target="#b1">[2]</ref> to adjust this method. CNN+LSTM used two LSTMs to encode video sequences and QA text, respectively. The two final hidden states were concatenated to predict the correct answer. I3D+LSTM used the I3D network <ref type="bibr" target="#b43">[44]</ref> to extract video motion features, which were then fused with QA text features encoded by LSTM to calculate model predictions. TVQA <ref type="bibr" target="#b44">[45]</ref> is a multi-stream network that combines input features from different modalities to answer questions. HCRN <ref type="bibr" target="#b45">[46]</ref> used a hierarchical conditional relation network to model the complex structure of video reasoning. BERT-VQA <ref type="bibr" target="#b46">[47]</ref> used BERT <ref type="bibr" target="#b38">[39]</ref> to jointly encode visual and language information to predict answers. Eclipse <ref type="bibr" target="#b1">[2]</ref> is a model proposed by the creators of the trafficQA dataset, which uses bidirectional long short-term memory networks (Bi-LSTMs) and convolutional neural networks (CNNs) to encode input video and question information, and uses gated attention mechanisms to compute the relationship between questions and videos. VGT <ref type="bibr" target="#b3">[4]</ref> is based on the idea of graph neural networks, which represent videos as spatiotemporal graphs and use Transformer models to model video features and predict question answers. CLIPBERT <ref type="bibr" target="#b26">[27]</ref> used a sparse sampling strategy to select only a small number of frames from the video for encoding, achieving efficient video feature extraction and processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparisons</head><p>In Table <ref type="table" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Comparison with the Addition of Traffic Knowledge Base</head><p>To validate the effectiveness of the lightweight traffic knowledge base in causal reasoning questions, we conducted comparative experiments on the entire TrafficQA test set, specifically on causal questions, and evaluated the method without using the traffic knowledge base (i.e., QC). As shown in Table <ref type="table" target="#tab_1">2</ref>, for all questions, the accuracy of the model without using the lightweight traffic knowledge base was 46.76%. However, after using the lightweight traffic knowledge base, the model's performance improved by 2.46%. In addition, the experimental results show that after using the traffic knowledge base, the accuracy of causal questions increased by 4.79%. This indicates that our designed lightweight traffic knowledge base is helpful for the model's inference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Comparison among classification models</head><p>To validate the effectiveness of the classification model, we evaluated the time cost and accuracy of using the classification model and various models within the classification model on the TrafficQA dataset. The model for handling causal questions is called TK-VGT, and BUTD is the top-down and bottom-up attention model for handling general scene questions.As shown in Table <ref type="table" target="#tab_2">3</ref>, when we tested the entire TrafficQA dataset using the BUTD model, we found that although BUTD only took 258 seconds to answer all test questions, its accuracy was only 39.76%. In contrast, our model had an accuracy that was 9.46% higher. When we fully trained and tested the TK-VGT model, the model had an accuracy of 49.52%. Compared to our classification model, the model only increased the accuracy by 0.3%. However, it took 924 seconds for the model to answer all the questions in the test set, while our classification model only took 504 seconds, reducing the time cost by 420 seconds. Therefore, it can be concluded that our model has achieved a balance between time efficiency and accuracy by reducing the time cost significantly while only sacrificing a small amount of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Comparison of classification models in different question scenarios</head><p>According to Table <ref type="table" target="#tab_3">4</ref>(a), to validate the effectiveness of the BUTD model, we divided the test set into general scene questions and causal questions, using the same method as in Section 3.1. We conducted experiments on TK-VGT and BUTD separately for general scene questions. The results show that for general scene questions, the accuracy of using the TK-VGT model on the test set is 68.82%, while the accuracy of BUTD on the test set is 69.32%. We found that the simple BUTD model is more effective for some simple questions than the more complex TK-VGT model, indicating that the BUTD model is more effective for some simple questions.</p><p>(a) general questions  <ref type="table" target="#tab_3">4</ref>(b), we conducted a comparative experiment between TK-VGT and BUTD on causal questions. The results showed that the accuracy of TK-VGT model on causal questions was 37.18%, while the accuracy of BUTD model on the test set was only 20.92%. This indicates that traditional image questionanswering models are not suitable for more complex video question-answering problems, especially for causal questions that require complex reasoning, as a single image cannot capture the dynamic relationships between objects in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>In Fig. <ref type="figure" target="#fig_5">7</ref>, we conducted a qualitative analysis of the advantages of the traffic knowledge base and the classification model, and reached the following conclusions: the example (a) shows that when deducing causal questions, the model can better infer the cause of the accident and other related information using the traffic knowledge base; the example (b) shows that for general scene questions, both complex video QA models and simple image QA models can successfully infer the answer to the question; (c) as for the failure cases of model predictions, the model still incorrectly identifies objects that did not have traffic accidents as those that did, and our classification model cannot solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this article, we investigated video question answering and proposed a lightweight traffic knowledge base that we effectively integrated into our model. Through experimentation, we demonstrated that combining this knowledge base can effectively help the model deduce causal traffic problems. Additionally, we proposed a classification strategy that uses different models to infer different types of problems based on the question, saving a significant amount of time while maintaining accuracy. Finally, we developed the Question and Knowledge-Guided Aggregated Graph Attention Network to better integrate the traffic knowledge base for causal relationship inference. While our classification model achieved good results, the classification method was straightforward and relied on certain keywords in the question. Therefore, future research will focus on more effective ways to classify questions. Additionally, we aim to improve the knowledge-based video question answering reasoning capabilities. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of video question answering model based on Traffic Knowledge base and Question Categorization(TK-QC)</figDesc><graphic coords="6,66.90,53.70,514.80,284.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 3: Overview of Causal Inference Problems Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Overview of the Model for General Scenario Problems</figDesc><graphic coords="8,66.90,73.70,231.30,284.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Overview of the Question and Knowledge-Guided Aggregated Graph Attention Network</figDesc><graphic coords="9,312.95,73.70,216.00,255.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The distribution of questions in SUTD-TrafficQA</figDesc><graphic coords="11,94.19,73.70,208.50,125.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Regarding the visualization results of trafficQA, the green color indicates the correct answer.</figDesc><graphic coords="14,66.90,73.69,458.10,397.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,74.84,73.70,460.80,309.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,82.95,38.70,504.00,298.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, we compared the performance of TK-QC with other models on the TrafficQA dataset. As shown in the table, our TK-QC model outperforms other models in terms of accuracy on the TrafficQA dataset. Compared to traditional CNN models, our model improves accuracy by 18.77%. Compared to the Eclipse model, our model improves accuracy by 12.17%. Compared to the VGT model, our model improves accuracy by 1.87%. Even without using a pre-trained model, our model performs better than Clipbert, which uses a pre-trained model. This indicates that our model exhibits excellent performance. Experimental Results on SUTD-TrafficQA Dataset</figDesc><table><row><cell>Models</cell><cell>CM-Pretrain</cell><cell>Accuracy</cell></row><row><cell>VIS+LSTM[43]</cell><cell>-</cell><cell>29.91</cell></row><row><cell>I3D+LSTM[44]</cell><cell>-</cell><cell>30.78</cell></row><row><cell>CNN+LSTM[45]</cell><cell>-</cell><cell>30.45</cell></row><row><cell>BERT-VQA[47]</cell><cell>-</cell><cell>33.68</cell></row><row><cell>HCRN[46]</cell><cell>-</cell><cell>36.49</cell></row><row><cell>Eclipse[2]</cell><cell>-</cell><cell>37.05</cell></row><row><cell>VGT[4]</cell><cell>-</cell><cell>47.35</cell></row><row><cell>Clipbert[27]</cell><cell>VG+COCO Caption</cell><cell>48.30</cell></row><row><cell>TK-QC</cell><cell>-</cell><cell>49.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of using traffic knowledge base on SUTD-TrafficQA Dataset</figDesc><table><row><cell>Models</cell><cell>Total Accuracy</cell><cell>Causal questions Accuracy</cell></row><row><cell>Eclipse[2]</cell><cell>37.05</cell><cell>-</cell></row><row><cell>QC 3.3</cell><cell>46.76</cell><cell>32.39</cell></row><row><cell>TK-QC</cell><cell>49.22</cell><cell>37.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Classification Models on the TrafficQA Dataset</figDesc><table><row><cell>Models</cell><cell>Accuracy</cell><cell>Time</cell></row><row><cell>BUTD 3.2</cell><cell>39.76</cell><cell>258s</cell></row><row><cell>TK-VGT 3.4</cell><cell>49.52</cell><cell>924s</cell></row><row><cell>TK-QC</cell><cell>49.22</cell><cell>504s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of BUTD and TK-VGT for different questions on the TrafficQA dataset According to Table</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(b) causal questions</cell></row><row><cell>Models</cell><cell>Accuracy</cell><cell>Models</cell><cell>Accuracy</cell></row><row><cell>TK-VGT</cell><cell>68.82</cell><cell>TK-VGT</cell><cell>37.18</cell></row><row><cell>BUTD</cell><cell>69.32</cell><cell>BUTD</cell><cell>20.92</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9878" to="9888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video graph transformer for video question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="39" to="58" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXXVI</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video question answering via knowledge-based progressive spatialtemporal attention network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">November 11-15, 2007. 2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multiworld approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="261" to="277" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Stacked attention networks for</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video question answering via attribute-augmented attention network learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="829" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A readwrite memory network for movie story understanding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Openended long-form video question answering via adaptive hierarchical reinforced networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6576" to="6585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6135" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th biennial conference on innovative data systems research, CIDR Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conceptnet-a practical commonsense reasoning tool-kit</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014</title>
		<meeting><address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 15-19, 2014. 2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video as conditional graph hierarchy for multi-granular question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2804" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9972" to="9981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
