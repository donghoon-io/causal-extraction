<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior (Extended Version)</title>
				<funder ref="#_R4Qt2Du">
					<orgName type="full">RGC CRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-15">15 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><roleName>SAR</roleName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
							<email>juding@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
							<email>shihan@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
							<email>qifu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiaru</forename><surname>Zhang</surname></persName>
							<email>jiaruzhang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
							<email>shuaiw@cse.ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<email>dongmeiz@microsoft.com</email>
						</author>
						<author>
							<persName><surname>Scalable Differentiable</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Qiang Fu</orgName>
								<orgName type="institution">Microsoft Research Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Jiaru Zhang</orgName>
								<orgName type="institution">Microsoft Research Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Shuai Wang</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dongmei Zhang</orgName>
								<orgName type="institution" key="instit1">HKUST Hong Kong SAR</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Beijing</orgName>
								<address>
									<addrLine>KDD &apos;24 August 25-29</addrLine>
									<postCode>2024</postCode>
									<settlement>Barcelona</settlement>
									<country>China Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior (Extended Version)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-15">15 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3637528.3672031</idno>
					<idno type="arXiv">arXiv:2406.10537v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables).</p><p>The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery.</p><p>To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a "point-estimation", SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs.</p><p>Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also * Corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causal discovery in the presence of latent confounders is a longstanding problem <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b40">42]</ref>. Under this setting, the causal relations are typically represented by a maximal ancestral graph (MAG) <ref type="bibr" target="#b32">[34]</ref>, a special class of acyclic directed mixed graphs (ADMGs). MAG learning has historically been conducted by either constraint-based methods, such as FCI <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b40">42]</ref>, RFCI <ref type="bibr" target="#b9">[10]</ref> and ICD <ref type="bibr" target="#b29">[31]</ref>, or by scorebased methods, such as M3HC <ref type="bibr" target="#b34">[36]</ref>, AGIP <ref type="bibr" target="#b6">[7]</ref> and GPS <ref type="bibr" target="#b8">[9]</ref>. In recent years, differentiable causal discovery has emerged as a promising approach to enhance the accuracy and efficiency of existing methods <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b42">44]</ref>. By recasting the combinatorial constraint of graph structure into a differentiable form, continuous optimization techniques can be applied in an "out-of-the-box" manner.</p><p>The goal of differentiable methods is to identify the ancestral ADMG, and apply maximal ancestral projection, a standard process, to generate the corresponding MAG. Despite the encouraging results achieved thus far, they struggle with large-scale causal graphs, particularly those containing more than 20 variables. One widelyused method, ABIC <ref type="bibr" target="#b4">[5]</ref>, acknowledges its limitations with datasets of only 10-15 variables (i.e., a causal graph with 10-15 nodes), which restricts its broader applicability. Likewise, N-ADMG <ref type="bibr" target="#b2">[3]</ref>, another differentiable method for ADMG learning, suffers from similar scalability issues. Upon closer examination, we find that these methods are both inefficient and inaccurate when working with large-scale causal graphs. For example, ABIC may take days to converge and sometimes produces cyclic graphs when dealing with 50 nodes. This is likely due to the inherent challenges of learning from causally insufficient data, as ABIC must manage a more complicated objective function, additional optimization variables, and complex constraints. Consequently, the search space of large datasets poses significant challenges for standard optimization techniques, such as L-BFGS <ref type="bibr" target="#b14">[15]</ref> or gradient descent.</p><p>On the other hand, it has been demonstrated that adjacency information (i.e., the skeleton) can be utilized as a pre/post-processing step to facilitate differentiable methods for learning DAGs (directed acyclic graphs). For instance, CDRL <ref type="bibr" target="#b43">[45]</ref> uses the CAM <ref type="bibr" target="#b5">[6]</ref> to rule out spurious edges while ML4S <ref type="bibr" target="#b25">[26]</ref> investigates using skeleton in a pre-processing step to preclude superfluous variables for NOTEARS <ref type="bibr" target="#b42">[44]</ref>. These approaches show the potential for boosting differentiable DAG learning by leveraging skeleton information. Key Observation. These promising results on DAG learning motivate us to investigate whether MAG learning can benefit from a high-quality estimation of the skeleton. To this end, we implement two ABIC variants, one using the ground-truth skeleton as prior knowledge and the other using the skeleton learnt by FCI, to validate our hypotheses. In both variants, we black out the edges that are not in the skeleton and then run ABIC to only optimize the remaining edges. In this way, the optimization procedure is only applied on a subset of variables rather than the entire adjacency matrix. Then, we evaluate them on ten synthetic datasets with 50-100 variables and 1000 samples (see Table <ref type="table" target="#tab_0">1</ref>). In particular, we find that ABIC is impressively accurate when the true skeleton is known (3rd row in Table <ref type="table" target="#tab_0">1</ref>). This result, to a considerable extent, validates our hypothesis that the skeleton can be used to reduce the search space of the optimization procedure and therefore boost the performance of differentiable MAG learning.</p><p>On the contrary, we also find that when the ground-truth skeleton is unknown and the used skeleton is learned by FCI (4th row in Table <ref type="table" target="#tab_0">1</ref>), the improvement becomes modest because many erroneous/missing edges induced by FCI propagate to the subsequent optimization procedure. This indicates the practical hurdle of using the skeleton to facilitate differentiable causal discovery.</p><p>Indeed, due to finite samples, learning a high-quality skeleton is challenging. Simply putting a "point estimation" of the skeleton as a hard constraint on the optimization procedure can result in considerable missing edges (i.e., false negatives) and finally impair performance.</p><p>The above preliminary results shed light on a possible solution that synergistically combines differentiable causal discovery with skeleton information, while alleviating error propagations. Recently, much research has promoted the value of posterior distribution of DAGs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Enlightened by these works, we advocate estimating a posterior distribution (i.e., ğ‘ (ğ‘† | D) where ğ‘† is the skeleton and D is the dataset) over all skeletons to replace the conventional "point estimation." (i.e., arg max ğ‘† ğ‘ (ğ‘† | D) ) The posterior effectively quantifies epistemic uncertainty and the degree of confidence to any skeletons w.r.t. the given dataset. Nonetheless, unlike DAGs, skeletons over causally insufficient variables do not have an explicit form of likelihood function (i.e., ğ‘ (D, ğ‘†)).</p><p>To address these challenges, we propose SPOT (Skeleton Posteriorguided OpTimization) as a two-phase framework for facilitating differentiable causal discovery in the presence of latent confounders. SPOT first performs amortized variational inference to estimate skeleton posterior, SPOT then employs a novel optimization procedure for boosting subsequent differentiable causal discovery. Specifically, SPOT leverages a recent advancement to amortize the variational inference into the joint distribution of data and corresponding skeleton ğ‘ (D, ğ‘†) and alleviates the need of an explicit likelihood function. Then, it concretizes the amortized inference with a supervised model to estimate the skeleton posterior. To effectively facilitate the optimization procedure, SPOT employs the skeleton posterior to stochastically update variables in each optimization step, instead of deterministically updating optimization variables with gradients.</p><p>As shown in the last row of Table <ref type="table" target="#tab_0">1</ref>, SPOT improves the performance of differentiable causal discovery methods by a notable margin. In Sec. 4, we conduct extensive experiments on various largescale datasets and show that SPOT delivers 8% improvement on skeleton F1 score and 13% and 16% improvement on arrowhead and tail F1 scores, respectively, which are representative metrics for evaluating the accuracy of the learned ADMGs. We also demonstrate that the skeleton posterior estimated by SPOT is highly accurate compared to other variational inference-based and non-parametric bootstrap-based solutions. Finally, we explore the versatile applications of SPOT in MAG learning for non-linear causally insufficient datasets and also in DAG learning methods. Our empirical results indicate the strong potential of SPOT in such scenarios.</p><p>In summary, we make the following contributions: (1) Conceptually, we advocate a novel focus of using the skeleton posterior to facilitate differentiable causal discovery in the presence of latent confounders. (2) Technically, we formulate the problem of skeleton posterior estimation under amortized inference framework and propose a supervised learning-based solution to estimate the skeleton posterior from observational data. (3) On the basis of the skeleton posterior, we propose SPOT, a novel stochastic optimization procedure, to facilitate differentiable causal discovery in the presence of latent confounders, which incorporates the skeleton posterior in a stochastic manner. (4) Empirically, SPOT demonstrates superior performance on nearly all evaluation metrics and various datasets, substantially improving upon its counterpart. We also explore the extension of SPOT to other causal discovery settings, including non-linear ADMG and DAG. Our results show that SPOT consistently improves state-of-the-art methods in these settings. Our code will be made publicly available at [30].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>As in many previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">36]</ref>, we focus on discovering MAG from a linear Gaussian structural causal model (SCM) in the presence of latent confounders and assume the absence of selection bias (no undirected edges in ADMGs). In this section, we introduce preliminary knowledge of linear Gaussian SCM with latent confounders, differentiable causal discovery and amortized inference.</p><p>In Appendix, we provide definitions to terminologies that are not explicitly defined in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linear Gaussian SCM with Latent Confounders</head><p>We first start with the definition of a linear Gaussian SCM without latent confounders. Consider a linear SCM with ğ‘‘ observable variables parameterized by a coefficient matrix ğ›¿ âˆˆ R ğ‘‘ Ã—ğ‘‘ . The SCM can be written as</p><formula xml:id="formula_0">ğ‘‰ ğ‘– â† âˆ‘ï¸ ğ‘‰ ğ‘— âˆˆPA ğ‘– ğ›¿ ğ‘—ğ‘– ğ‘‰ ğ‘— + ğœ– ğ‘–<label>(1)</label></formula><p>where PA ğ‘– are the parents of ğ‘‰ ğ‘– and ğœ– ğ‘– is a noise term that is mutually independent of all other noise terms.</p><p>The noise term ğœ– ğ‘– is mutually independent of others if and only if ğ‘‰ ğ‘– has no latent confounder. Otherwise, ğœ– ğ‘– is correlated with ğœ– ğ‘— if ğ‘‰ ğ‘— shares a latent confounder with ğ‘‰ ğ‘– . When the noise terms are Gaussian, the correlation can be expressed by a covariance matrix ğ›½ = E[ğœ–ğœ– ğ‘‡ ] and the joint distribution marginalized over observable variables ğ‘½ O forms a zero-mean multivariate Gaussian distribution with covariance matrix as Î£ = (ğ¼ -ğ›¿) -ğ‘‡ ğ›½ (ğ¼ -ğ›¿) -1 . The induced graph ğº is an ADMG and contains two types of edges, including directed edges (â†’) and bidirected edges (â†”), which implies two adjacency matrices, namely ğ· and ğµ. In the special case where there is no latent confounder, the ADMG is equivalent to a DAG and the adjacency matrix of bidirected edges ğµ is all zeros. ğ‘‰ ğ‘– â†’ ğ‘‰ ğ‘— exists in ğº and ğ· ğ‘– ğ‘— = 1 if and only if ğ›¿ ğ‘– ğ‘— â‰  0. ğ‘‰ ğ‘– â†” ğ‘‰ ğ‘— exists in ğº and ğµ ğ‘– ğ‘— = 1 if and only if ğ›½ ğ‘– ğ‘— â‰  0. It is commonly assumed that ğ›¿ ğ‘– ğ‘— = ğ›¿ ğ‘—ğ‘– = ğ›½ ğ‘– ğ‘— = ğ›½ ğ‘—ğ‘– = 0 if and only if ğ· ğ‘– ğ‘— = ğ· ğ‘—ğ‘– = ğµ ğ‘– ğ‘— = ğµ ğ‘—ğ‘– = 0.</p><p>Remark. In the context of ADMGs, the noise term ğœ– does not exclusively represent the exogenous variables. In particular, ğœ– ğ‘– in Eqn. 1 includes both the exogenous variable and the confounding effect from ğ‘‰ ğ‘¢ (the latent confounder). Similarly, the noise term on another variable ğ‘‰ ğ‘— , ğœ– ğ‘— , also includes its own exogenous variable and the confounding effect from ğ‘‰ ğ‘¢ . Therefore, ğœ– ğ‘– and ğœ– ğ‘— are jointly influenced by the confounding effect from ğ‘‰ ğ‘¢ , which makes them dependent. We provide a more detailed explanation below.</p><p>Consider the causal graph ğ‘‰ ğ‘¢ ğ‘– â†’ ğ‘‰ ğ‘– â† ğ‘‰ ğ‘¢ â†’ ğ‘‰ ğ‘— â† ğ‘‰ ğ‘¢ ğ‘— . Without loss of generality on the linear setting, we have ğ‘‰ ğ‘– = ğ‘‰ ğ‘¢ ğ‘– + ğ‘ğ‘‰ ğ‘¢ and ğ‘‰ ğ‘— = ğ‘‰ ğ‘¢ ğ‘— + ğ‘ğ‘‰ ğ‘¢ , where ğ‘, ğ‘ are non-zero coefficients. Given that only ğ‘‰ ğ‘– and ğ‘‰ ğ‘— are observed, the resulting ADMG should be ğ‘‰ ğ‘– â†” ğ‘‰ ğ‘— , based on Sec. 2.2 of <ref type="bibr" target="#b40">[42]</ref>. In Eqn. 1, ğ‘‰ ğ‘– = ğ‘‰ ğ‘˜ âˆˆğ‘ƒğ‘ (ğ‘‰ ğ‘– ) ğ›¿ ğ‘˜ğ‘– ğ‘‰ ğ‘˜ +ğœ– ğ‘– where ğ‘ƒğ‘(ğ‘‰ ğ‘– ) are the parents of ğ‘‰ ğ‘– in the ADMG. Since there is a bidirectional edge ğ‘‰ ğ‘– â†” ğ‘‰ ğ‘— , neither ğ‘‰ ğ‘– is the parent of ğ‘‰ ğ‘— nor ğ‘‰ ğ‘— the parent of ğ‘‰ ğ‘– . Hence, ğ‘ƒğ‘(ğ‘‰ ğ‘– ) = ğ‘ƒğ‘(ğ‘‰ ğ‘— ) = âˆ…. Thus, we have ğ‘‰ ğ‘– = ğœ– ğ‘– and ğ‘‰ ğ‘— = ğœ– ğ‘— . However, since ğ‘‰ ğ‘– and ğ‘‰ ğ‘— are dependent, it follows that ğœ– ğ‘– and ğœ– ğ‘— must also be dependent.</p><p>According to the causal Markov and faithfulness assumption, ancestral ADMG, MAG and skeleton can be defined as follows.</p><p>Definition 1 (Ancestral Acyclic Directed Mixed Graph). ğº is an ancestral ADMG if it is a mixed graph with directed edges (â†’) and bidirected edges (â†”) and contains no directed cycles or almost directed cycles <ref type="bibr" target="#b40">[42]</ref>.</p><p>Definition 2 (Maximal Ancestral Graph). An ancestral ADMG ğº is a Maximal Ancestral Graph if for each pair of non-adjacent nodes, there exists a set of nodes that make them m-separated <ref type="bibr" target="#b40">[42]</ref>.</p><p>Definition 3 (Skeleton). An undirected graph ğ‘† is a skeleton of an MAG ğº if ğ‘† is obtained from ğº by replacing all directed edges with undirected edges. Therefore, two nodes ğ‘‰ ğ‘– , ğ‘‰ ğ‘— are adjacent in the skeleton if and only if âˆ€ğ’ âŠ† ğ‘½ O \ {ğ‘‰ ğ‘– , ğ‘‰ ğ‘— }, ğ‘‰ ğ‘– Ì¸ âŠ¥ ğ‘‰ ğ‘— | ğ’ , where âŠ¥ and Ì¸ âŠ¥ denote conditional independence and dependence, respectively. Hence, it is clear that given a dataset whether two nodes are adjacent is not influenced by the adjacencies of other nodes.</p><p>Parameter Estimation of Linear Gaussian SCM. For DAGs, one easily can estimate the parameters of the SCM by solving a simple least squares regression problem. However, for ADMGs, the estimation of the parameters is more challenging due to the presence of latent confounders. Drton et al. <ref type="bibr" target="#b13">[14]</ref> proposed an iterative algorithm to estimate the parameters of the SCM called Residual Iterative Conditional Fitting (RICF). RICF generally works for bowfree ADMGs and ancestral ADMGs, which are our focus in this paper, are special cases of bow-free ADMGs <ref type="bibr" target="#b4">[5]</ref>. This algorithm iteratively fits the SCM to the data and updates the covariance matrix. The algorithm is guaranteed to converge to a local minimum when the corresponding ADMGs are ancestral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differentiable Causal Discovery</head><p>Score-based methods aim to maximize the score (e.g., log-likelihood) of the graph on the given data, which can be written in the following form:</p><formula xml:id="formula_1">arg max ğº ğ‘“ (ğº ) s.t. ğº is acyclic (2)</formula><p>where ğ‘“ (â€¢) is the score function. Given the acyclicity constraint, the optimization process is combinatorial. Recently, differentiable methods reformulate this combinatorial constraint into a constraint â„ DAG (ğ‘Š ) = tr(ğ‘’ ğ‘Š â€¢ğ‘Š ) -ğ‘‘ such that</p><formula xml:id="formula_2">â„ DAG (ğ‘Š ) = 0 â‡â‡’ ğº is acyclic<label>(3)</label></formula><p>where ğ‘‘ = |ğ‘½ O | and ğ‘Š is a weighted adjacency matrix of ğº. For linear SCM, an element in ğ‘Š represents the linear coefficient. As a continuous optimization with equality constraints, the augmented Lagrangian method (ALM) is commonly used to convert the constrained optimization problem into several unconstrained subproblems and use standard optimizers to solve them separately <ref type="bibr" target="#b42">[44]</ref>. Despite the success of differentiable methods for DAG learning, the algebraic characterization in Eqn. 3 cannot be directly applied to ADMGs (and MAGs). As aforementioned, ADMGs requires two adjacency matrices, ğ· and ğµ, to represent directed edges and bidirected edges, respectively. To extend the algebraic characterization to ADMGs, ABIC <ref type="bibr" target="#b4">[5]</ref> modified it as</p><formula xml:id="formula_3">â„ ADMG (ğ·, ğµ) = tr(ğ‘’ ğ· ) -ğ‘‘ + sum(ğ‘’ ğ· â€¢ ğµ)<label>(4)</label></formula><p>where â€¢ denotes the Hadamard product, i.e., element-wise multiplication, and sum(â€¢) is the sum of all elements in a matrix. It has Algorithm 1: ABIC <ref type="bibr" target="#b4">[5]</ref> Input: Dataset D Output: Adjacency Matrix ğ·, ğµ 1 Initialize ğ›¿ (1,1) , ğ›½ (1,1) ; 2 Define â„ (ğ›¿, ğ›½ ) according to Eqn. 4;</p><formula xml:id="formula_4">3 foreach ğ‘¡ 1 = 1, â€¢ â€¢ â€¢ ,ğ‘‡ ALM do 4 foreach ğ‘¡ 2 = 1, â€¢ â€¢ â€¢ ,ğ‘‡ RICF do 5 Update pseudovariables by ğ›¿ (ğ‘¡ 1 ,ğ‘¡ 2 ) , ğ›½ (ğ‘¡ 1 ,ğ‘¡ 2 ) ; 6 Constitute ğ‘“ (ğ›¿, ğ›½ ); 7 ğ›¿ (ğ‘¡ 1 ,ğ‘¡ 2 +1) , ğ›½ (ğ‘¡ 1 ,ğ‘¡ 2 +1) â† arg min ğ‘“ (ğ›¿, ğ›½ ); 8 end 9 Update ğ›¼ (ğ‘¡ 1 ) , ğœŒ (ğ‘¡ 1 ) , ğœ† (ğ‘¡ 1 ) ; 10 end 11 ğ· â† 1( |ğ›¿ (ğ‘‡ ALM ,ğ‘‡ RICF ) &gt; ğœ” | ); 12 ğµ â† 1( |ğ›½ (ğ‘‡ ALM ,ğ‘‡ RICF ) &gt; ğœ” | ); 13 return ğ·, ğµ;</formula><p>been proved that â„ ADMG (ğ·, ğµ) = 0 â‡â‡’ ğº is ancestral ADMG. In a nutshell, tr(ğ‘’ ğ· ) -ğ‘‘ implies the standard directed acyclicity constraint and the last term sum(ğ‘’ ğ· â€¢ ğµ) is a term to ensure that the bidirected edges does not introduce "almost directed cycles". In the following, we use â„ to denote â„ ADMG for simplicity. Now, we introduce ABIC <ref type="bibr" target="#b4">[5]</ref> in Alg. 1. To ease presentation, we omit technical details (e.g., calculations of ğ›¼, ğœŒ, ğœ†, pseudovariables, and stopping criteria) and refer readers to the original paper for the full version. The ABIC algorithm aims to maximize the likelihood of ADMG over the observational data D, ensuring â„ ADMG (ğ›¿, ğ›½) = 0. It employs two nested loops: the outer loop applies the augmented Lagrangian method (ALM) to convert the equality-constrained problem into ğ‘‡ ALM steps of unconstrained ones (lines 3-10 in Alg. 1). After ALM, ğ›¿ and ğ›½ represent the ADMG's linear coefficients and covariance, respectively. The adjacency matrix is derived by applying a threshold to remove negligible coefficients (lines 22-23), yielding the ancestral ADMG. If a MAG is required, the maximal ancestral projection, a standard post-processing procedure, is performed.</p><p>Inside the inner loop, Alg. 1 iteratively adjust the objective function (lines 5-6) and solve the unconstrained optimization problem accordingly (line 7). In particular, the objective function ğ‘“ (ğ›¿, ğ›½) (line 6) consists of the least square loss to fit the ADMG to the data, the constraint â„(ğ›¿, ğ›½), and a regularization term to enforce sparsity. The form of ğ‘“ (ğ›¿, ğ›½) is updated according to the current pseudovariables (line 5). Then, in the optimization phase (line 7), optimization algorithms are used in an "out-of-the-box" manner to find a minimum of ğ‘“ (ğ›¿, ğ›½). However, due to the complexity and non-convex nature of ğ‘“ (ğ›¿, ğ›½), it is challenging for standard optimization techniques to find a plausible solution, especially when the ADMG is large, as we will show in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Amortized Inference for Causal Discovery</head><p>In the conventional variational inference framework, the posterior distribution is approximated by a parametric distribution ğ‘ ğœ™ (ğº |D) with parameters ğœ™. Here, ğº is the causal graph and D is the observed data in the context of causal discovery. The parameters are learned by minimizing the KL divergence between the true posterior ğ‘ (ğº |D) and the parametric distribution ğ‘ ğœ™ (ğº |D). Typically, variational inference requires the likelihood function ğ‘ (D, ğº) to compute the KL divergence. Recently, amortized inference has been proposed to alleviate the need of an explicit likelihood function by introducing a simulator that can yield samples from the joint distribution ğ‘ (ğº, D) <ref type="bibr" target="#b1">[2]</ref>. Lorch et al. <ref type="bibr" target="#b24">[25]</ref> follows this line of work and introduces AVICI, which amortizes the inference process by introducing a sampler that can generate samples from the simulator and capture the domain-specific inductive biases that would have been difficult to characterize (e.g., gene regulatory networks or non-linear functions of random Fourier features).</p><p>Recently, SCL (supervised causal learning) has emerged as a promising paradigm for causal discovery <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Compared to standard machine learning models, SCL models features many advantages tailored for causal discovery (e.g., invariant to the permutation of variables). It is worth noting that the concept of "permutation invariance" is different from the one in machine learning setting. One dataset (a ğ‘€ Ã— ğ‘ matrix) is a sample to the SCL model. It implies that the model is permutation in-and equivariant with respect to the observation and variable dimensions of the provided dataset, respectively <ref type="bibr" target="#b21">[22]</ref>. SCL models can be trained on samples from a known simulator. In essence, SCL can be viewed as an instantiation of amortized inference when the simulator is in line with the underlying causal mechanism of the observational data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPOT</head><p>As aforementioned, skeleton information can be leveraged to enhance the optimization procedure of differentiable causal discovery in the presence of latent confounders. However, a "point estimation" of the skeleton is prone-to-error. In this section, we propose SPOT (Skeleton Posterior-guided OpTimization) to leverage a probabilistic skeleton posterior to guide the optimization of ADMGs.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, SPOT consists of four steps: â€ a static simulator first generates an initial set of data/skeleton pairs to obtain an initial amortized inference model (blue one in Fig. <ref type="figure" target="#fig_0">1</ref>). â while the initial amortized inference model already provides a good estimation of the skeleton posterior, SPOT can optionally use a dynamic simulator to generate more in-domain training instances with respect to the input dataset. â‚ with samples from the dynamic simulator, SPOT adapts the initial amortized inference model to obtain an updated model (yellow one in Fig. <ref type="figure" target="#fig_0">1</ref>) for skeleton posterior inference. âƒ SPOT uses the inferred skeleton posterior to enhance the optimization procedure of differentiable causal discovery.</p><p>Conceptual Complexity. While our solution provides an additional layer of complexity compared to standard differentiable causal discovery, we argue that the level of complexity is generally manageable and comparable to other causal discovery algorithms. First, the two stages in our pipeline are independent and do not involve joint training. Second, constraint-based causal discovery algorithms, such as FCI, also involve two similar steps: learning the skeleton in the first step and orienting the edges in the second step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skeleton Variational Objective</head><formula xml:id="formula_5">Let D = {ğ’™ 1 , â€¢ â€¢ â€¢ , ğ’™ ğ‘› } âˆ¼ ğ‘ (ğ‘‰</formula><p>) be the observational dataset, where ğ’™ ğ‘– is sampled from the joint distribution ğ‘ (ğ‘‰ ) and ğ‘› is sample size. We aim to approximate the posterior over skeletons ğ‘ (ğ‘† | D) with a variational distribution ğ‘(ğ‘†; ğ‘) where ğ‘† is the (symmetric) adjacency matrix of the skeleton and ğ‘ is the variational parameters. Thus, the skeleton estimation can be decomposed into a set of edge estimations (i.e., the probability of adjacency) independently. Then, the variational family of ğ‘(ğ‘†; ğ‘) is defined as</p><formula xml:id="formula_6">ğ‘ (ğ‘†; ğ‘) = ğ‘–&lt; ğ‘— ğ‘ (ğ‘† ğ‘– ğ‘— ; ğ‘ ğ‘– ğ‘— ) with ğ‘† ğ‘– ğ‘— âˆ¼ Bernoulli(ğ‘ ğ‘– ğ‘— ).<label>(5)</label></formula><p>In this regard, we aim to find an inference model ğ‘“ ğœ™ (D) that predicts ğ‘. This procedure can be attained by minimizing the expected forward KL divergence from ğ‘ (ğ‘† | D):</p><formula xml:id="formula_7">min ğœ™ E ğ‘ (D) KL ğ‘ (ğ‘† | D ) | | ğ‘ (ğ‘†; ğ‘“ ğœ™ ( D ) )<label>(6)</label></formula><p>Following the principle of amortized inference <ref type="bibr" target="#b1">[2]</ref>, we amortize the inference and rewrite the objective as Estimate ğ‘ (D, ğ‘†) via Static Simulator. Naturally, we can estimate ğ‘ (D, ğ‘†) by using a simulator (e.g., ErdÅ‘s-RÃ©nyi random graph model) then generate the corresponding dataset with pre-defined functional forms. The simulator serves a direct sampler of ğ‘ (D, ğ‘†) if the test data D (i.e., the input dataset on which we need to conduct causal discovery) is in the same distribution of the static simulator. In other words, the pair of D and the skeleton ğ‘† is known to be drawn from ğ‘ a priori.</p><p>Estimate ğ‘ (D, ğ‘†) via Dynamic Simulator. To further feed the model with more in-domain training instances, we propose to use nonparametric bootstrap method <ref type="bibr" target="#b15">[16]</ref> to estimate a MAG Äœ with a random subset of observational data. Then, we fit the parameters of the underlying SCM (i.e., ğ›¿, ğ›½; see Sec. 2.1) on the given test observational data using the RICF algorithm <ref type="bibr" target="#b13">[14]</ref>. Finally, we regenerate the new observational data D from the fitted SCM. By repeating the above procedure, we obtain a set of data/MAG pairs {( D1 , Äœ1 ), â€¢ â€¢ â€¢ } from which we can derive the samples of ğ‘ (D, ğ‘†) as {( D1 , Äœ1 + Äœğ‘‡ 1 ), â€¢ â€¢ â€¢ }. 1 We note that, while the way latent variables affect the observed variables is implicit, in the linear Gaussian 1 ğº and ğº ğ‘‡ are the adjacency matrix and its transpose, Äœ1 + Äœğ‘‡ 1 is the skeleton adjacency matrix. setting, we can characterize the marginalized distribution over observed variables as a zero-mean multivariate Gaussian distribution with a covariance matrix defined as Î£ = (ğ¼ -ğ›¿) -ğ‘‡ ğ›½ (ğ¼ -ğ›¿) -1 , where ğ›¿ and ğ›½ are parameters fitted by RICF. In this way, we presume this dynamic (dataset-dependent) simulator would provide more relevant training instances with respect to the input dataset D.</p><p>Remark. In summary, the static simulator features a robust model that generalizes well to the test data, even with potential domain shift, as validated in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. On the other hand, the dynamic simulator provides more in-domain training instances, which may lead to a better performance at a fairly lightweight cost on the runtime, as will be shown shortly in Sec. 3.2.  â€ Model Architecture. Existing SCL methods either use an endto-end model (e.g., Transformer) to directly predict the adjacency matrix <ref type="bibr" target="#b21">[22]</ref> or use a simple classifier (e.g., xgboost) to predict the local structure (e.g., the adjacency of two variables or the vstructure) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. While the design of SPOT is agnostic to the model architecture, we anticipate to use a simple classifier, as it is more effective and efficient for estimating ğ‘ ğ‘– ğ‘— in practice. Hence, in our implementation, we adopt a ML4S-like cascade model <ref type="bibr" target="#b25">[26]</ref> to predict the adjacency and constitute the skeleton posterior. We formulate the problem of efficient skeleton posterior inference as predicting a series of adjacencies and construct the skeleton posterior. Specifically, we aim to estimate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skeleton Posterior Inference</head><formula xml:id="formula_8">ğ‘“ ğœ™ ( D ) ğ‘– ğ‘— = E ğ‘ (D,ğ‘† ) ğ‘ (ğ‘‰ ğ‘– -ğ‘‰ ğ‘— âˆˆ ğ‘† | D ) ,<label>(8)</label></formula><p>We focus on the probability ğ‘ (ğ‘‰ ğ‘– -ğ‘‰ ğ‘— | D) for the adjacency of ğ‘‰ ğ‘– , ğ‘‰ ğ‘— given D, which helps constitute the skeleton posterior (we use ğ‘ ğ‘– ğ‘— as an abbreviation of ğ‘ (ğ‘‰ ğ‘– -ğ‘‰ ğ‘— | D)). Besides, we also reproduce Ke et al. <ref type="bibr" target="#b21">[22]</ref>'s Transformer-based model, and the results show that the cascade model is more effective and efficient. â Objective Refinement. Instead of standard causal discovery objectives (e.g., the higher F1 score), we prefer higher recall over precision to prevent missed edges. This is because the spurious edges will be offloaded to and eventually removed by differentiable causal discovery algorithms. Specifically, even though the SCL model may predict a spurious edge and corresponding parameters ğ›¿ ğ‘– ğ‘— , ğ›½ ğ‘– ğ‘— tend to be non-zero, the differentiable causal discovery algorithm will gradually move the coefficients to zero to minimize the objective function. In this regard, the spurious edges are eventually removed. However, the missed edges are hard to recover due to the sparse nature of the ADMG. To this end, we can either set a conservative threshold (subject to the specific design of the SCL model) or apply label smoothing trick to the training instances to prevent the model from predicting zero probability for the true edges.</p><p>â‚ Efficient Training. When using the dynamic simulator, the model has to be trained from scratch with the training instances sampled from non-parametric bootstrap. However, this would impose a heavy sampling and training overheads in the runtime. To alleviate this issue, we adopt a widely-used domain adaptation paradigm to reduce the training overheads by leveraging the static model. Specifically, given a dataset, we only sample a few training instances from the dynamic simulator. Then, we use the output and intermediate values of the static model to augment the original features of the samples from the dynamic simulator and train an adapted dynamic model efficiently. In this way, the dynamic model can rapidly adapt to the new dataset with few-shot training instances and yields a better performance with mild cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Posterior-guided Optimization</head><p>Non-adjacency between ğ‘‰ ğ‘– , ğ‘‰ ğ‘— implies ğ›¿ ğ‘– ğ‘— = ğ›¿ ğ‘—ğ‘– = ğ›½ ğ‘– ğ‘— = ğ›½ ğ‘—ğ‘– = 0. When ground-truth skeleton is available, we enforce extra equality constraints to optimize. However, the true skeleton is unattainable in practice, necessitating estimating a skeleton from observational data. We advocate using the skeleton posterior for optimization as it encodes epistemic uncertainty.</p><p>It is unclear how to incorporate a skeleton posterior in the optimization procedure. The estimated skeleton posterior from Sec. 3.2 is a continuous adjacency matrix ğ‘ ğ‘– ğ‘— âˆˆ [0, 1]. Thus, we cannot derive meaningful constraints for coefficients/covariances. Formally, ğ‘ (ğ‘‰ ğ‘– -ğ‘‰ ğ‘— | D) = ğ‘ ğ‘– ğ‘— implies the probability of the union of (disjoint) events ğ‘ƒ (ğ›¿ ğ‘– ğ‘— â‰  0 âˆª ğ›¿ ğ‘—ğ‘– â‰  0 âˆª ğ›½ ğ‘– ğ‘— â‰  0 | D) equals ğ‘ ğ‘– ğ‘— . Higher posterior probability indicates higher likelihood of non-zero coefficients. In this regard, ğ‘ ğ‘– ğ‘— cannot provide any additional information regarding the value of non-zero coefficients. For example, ğ›¿ ğ‘– ğ‘— = 0.1 and ğ›¿ ğ‘– ğ‘— = 0.9 both comply with arbitrary positive posterior ğ‘ ğ‘– ğ‘— &gt; 0 equally well. To account for the probabilistic nature of the skeleton posterior, we propose the posterior-guided optimizer for structure learning as a replacement of the standard optimizer in differentiable causal discovery (e.g., the gradient descent or the L-BFGS).</p><p>Alg. 2 presents the skeleton posterior-guided optimizer. In accordance with many standard differentiable causal discovery methods, we first optimize the objective function ğ‘“ (ğ›¿, ğ›½) with the standard optimizer (e.g., gradient descent) to obtain the optimal coefficients and covariances ğ›¿ * , ğ›½ * (line 1). the final result, we introduce a stochastic update scheme to update ğ›¿, ğ›½ based on the skeleton posterior ğ‘ (lines 2-8) as follows:</p><formula xml:id="formula_9">ğ‘ƒ (ğ›¿ ğ‘– ğ‘— â† ğ›¿ * ğ‘– ğ‘— ) = 1 ğ›¿ ğ‘– ğ‘— Ã— ğœ•ğ‘“ (ğ›¿,ğ›½ ) ğœ•ğ›¿ ğ‘– ğ‘— &gt; 0 (ğ‘ ğ‘– ğ‘— + ğ‘ ) 1 ğ‘¡ +1 otherwise (9) If ğ›¿ ğ‘– ğ‘— Ã— ğœ•ğ‘“ (ğ›¿,ğ›½ )</formula><p>ğœ•ğ›¿ ğ‘– ğ‘— &gt; 0, the update is accepted unconditionally (line 3-4); otherwise, it is accepted with a probability of (ğ‘ ğ‘– ğ‘— +ğ‘)</p><p>1 ğ‘¡ +1 (line 12-15; Fig. <ref type="figure" target="#fig_4">3</ref>). If rejected, we keep ğ›¿ ğ‘– ğ‘— unchanged (line 13). ğ›½ is also optimized in a similar way.</p><p>Conceptually, the overall procedure can be viewed as a modified simulated annealing process <ref type="bibr" target="#b22">[23]</ref>. ğ›¿ ğ‘– ğ‘— Ã—</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğœ•ğ‘“ (ğ›¿,ğ›½ )</head><p>ğœ•ğ›¿ ğ‘– ğ‘— &gt; 0 implies that the coefficient is "moving to zero" (i.e., a weakened edge) in this update. Given that real-world ADMGs are often sparse, "moving-to-zero" update is encouraged and therefore accepted unconditionally. This design consideration is thus an analogy of the unconditional update in simulated annealing. In contrast, if the coefficient is "moving to non-zero" (i.e., a strengthened edge), the update is accepted with a probability computed by ğ‘ ğ‘– ğ‘— and ğ‘¡ jointly. Recall that ğ‘ ğ‘– ğ‘— is the probability of the edge existence. The update is thus more encouraged with higher ğ‘ ğ‘– ğ‘— .</p><p>1 ğ‘¡ is a analogy of the temperature parameter in simulated annealing. As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, an update is more likely to be rejected with lower ğ‘¡ and higher ğ‘¡ yields (ğ‘ ğ‘– ğ‘— + ğ‘) 1 ğ‘¡ â‰ˆ 1. With the increase of ğ‘¡, the probability of accepting an update is gradually similar to what would be in the vanilla optimization procedure even with a low ğ‘ ğ‘– ğ‘— . In this way, the optimization procedure is gradually stabilized. Furthermore, it is worth noting that the scheme is robust to the accuracy of the skeleton posterior. For spurious edges, even if ğ‘ ğ‘– ğ‘— is high, the optimization procedure of differentiable causal discovery is still able to move the coefficients to zero given the unconditional update for "edge-weakening" updates.</p><p>Convergence. In the context of differentiable causal discovery, the concept of convergence is three-fold. First, in accordance with the ABIC <ref type="bibr" target="#b4">[5]</ref>, asymptotically, convergence to the global optimum of its objective function implies that the corresponding ADMG is within the Markov equivalence class of the true ADMG. Second, given the non-convexity of the objective function, it is not guaranteed that the optimization procedure converges to the global optimum; instead, it may result in a local minimum or a saddle point. Finally, we discuss the impact of the stochastic update scheme on convergence to the local minima in the following sense: Since the probability of an update reaches 1 (see Fig. <ref type="figure" target="#fig_4">3</ref>), the optimization procedure becomes equivalent to the vanilla optimizer when ğ‘¡ is sufficiently large. In this regard, the stochastic update scheme does not negatively affect the convergence to the local minima. In fact, our empirical results show that our stochastic update scheme yields much better results than ABIC and also surpasses other SOTA methods.</p><p>Probabilistic Update vs. Regularization. A natural question arises: why not use regularization (e.g., a penalty term) to incorporate the skeleton posterior? We argue that the penalty term may face several hurdles in practice. First, the objective function is often unstable due to the acyclic term (Eqn. 4). Its value can range from 1 Ã— 10 -5 to 1 Ã— 10 5 over different stages of the optimization. Second, as aforementioned, ğ›¿, ğ›½ are not directly comparable with posterior probability. This necessitates considerable efforts for implementation, thus we did not adopt it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In evaluation, we aim to evaluate the performance of SPOT from three perspectives: âŠ end-to-end causal discovery performance; â‹ standalone skeleton posterior inference performance; âŒ extension to other differentiable causal structure learning algorithms. We also compare SPOT with a wide range of baselines. Data Generation. We follow the similar setting from previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to generate synthetic datasets. We generates ADMGs from ErdÅ‘s-RÃ©nyi (ER) model with various nodes whose average indegree is âˆˆ [1, 1.5]. Each ADMG contains 5 -15% bidirected edges. Then, it is parameterized by the same rules of <ref type="bibr" target="#b4">[5]</ref>: if ğ‘‰ ğ‘– â†’ ğ‘‰ ğ‘— , ğ›¿ ğ‘– ğ‘— is uniformly sampled from Â±[0.5, 2.0]; if ğ‘‰ ğ‘– â†” ğ‘‰ ğ‘— , ğ›½ ğ‘– ğ‘— = ğ›½ ğ‘—ğ‘– is uniformly sampled from Â±[0.4, 0.7]; and ğ›½ ğ‘–ğ‘– is uniformly sampled from Â±[0.7, 1.2] and add sum(|ğ›½ ğ‘–,-ğ‘– |) to ensure positive definiteness. For each ADMG, we generate 1000 samples.</p><p>Baselines. We compare our method with a wide range of baselines, including constraint-based methods (FCI <ref type="bibr" target="#b32">[34]</ref>, RFCI <ref type="bibr" target="#b9">[10]</ref> and ICD <ref type="bibr" target="#b29">[31]</ref>), score-based methods (M3HC <ref type="bibr" target="#b34">[36]</ref> and GPS <ref type="bibr" target="#b8">[9]</ref>) and also the differentiable causal discovery methods (ABIC <ref type="bibr" target="#b4">[5]</ref>). The comparison is made on synthetic datasets with 50-100 nodes. We observe that ABIC occasionally outputs cyclic graphs with the default threshold, which is unwarranted for maximal ancestral projection. In such cases, we use the minimal threshold that produces an acyclic graph. We also tentatively tried AGIP <ref type="bibr" target="#b6">[7]</ref> algorithm and excluded it from comparison, because its preprocessing step is considerably slow (i.e., taking several days) for large datasets. Additionally, we omit algorithms lacking open-source implementation (e.g., GreedySPo <ref type="bibr" target="#b3">[4]</ref>). Since we focus on linear Gaussian data, we omit the algorithms that do not comply with this assumption (e.g., <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b37">39]</ref>) in the main comparison. In Sec. 4.3, we explore the effectiveness of SPOT on other settings.</p><p>Running Time. Many algorithms can take a long time to run on large datasets and the convergence condition is not always satisfied. To make the comparison fair, we set a timeout of 24 CPU hours for each algorithm. For algorithms that take more than 24 CPU hours, we halt the algorithm and use the best-so-far graph as their output.</p><p>Metrics. Under linear Gaussian SCM <ref type="bibr" target="#b32">[34]</ref>, we can only up to identify a Markov equivalence class. To make all baselines comparable, for those algorithms that output an MAG, we convert it to a PAG (Partial Ancestral Graph) and compare it with the true PAG. Following <ref type="bibr" target="#b4">[5]</ref>, we report the F1 score, TPR (True Positive Rate), and FDR (False Discovery Rate) on the skeleton, arrowheads, and tails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">End-to-end Comparison</head><p>We report the results in Table <ref type="table" target="#tab_2">2</ref> on ten datasets with nodes sampled between [50, 100]. We observe that the large datasets pose a considerable challenge to all methods while SPOT accurately identifies the underlying causal structure. ICD fails on one dataset; M3HC and GPS frequently terminate with convergence warnings on these datasets. We also observe that constraint-based methods are usually more powerful at identifying the correct skeleton while all methods have notable difficulties to accurately identify the arrowheads and tails. In contrast, SPOT consistently manifests its superiority across nearly all criteria and yields more precise and stable estimation of skeletons, arrowheads, and tails. Over the ten datasets, it significantly improves the performance of its counterpart, ABIC (with p-value of 0.008). We also investigate the criterion (i.e., Skeleton's FDR) on which SPOT is sub-optimal while M3HC is the best. Having said that, we observe that M3HC also suffers from the lowest TPR and F1 scores on skeleton. Hence, we conclude that M3HC may be too strict on confirming an edge while SPOT is more robust and attains the best F1 scores.</p><p>It is worth noting that SPOT's strong performance can be attributed to its ability to effectively balance precision and recall in estimating the underlying causal relationships. Additionally, the more consistent performance of SPOT could potentially make it more suitable for various real-world applications where accuracy and robustness are crucial factors.</p><p>Comparison by Node Size. In addition to the aggregated results, we also report the edge-wise F1 scores under different node sizes in Fig. <ref type="figure" target="#fig_5">4</ref> where the data point of each node size is the average of three datasets and the error bar is the standard deviation. First, we observe that SPOT consistently outperforms all baselines across all node sizes except for 20-node graphs. For 20-node graphs, potentially due to randomness, though ABIC slightly outperforms SPOT, the difference is not statistically significant (ğ‘ = 0.51). In other settings, especially for large graphs, SPOT outperforms ABIC (the second-best method) by a large margin (e.g., ğ‘ = 0.001 on 90-node graphs). Second, the performance of ABIC significantly drops on  100-node graphs where it fails to learn a meaningful causal structure. In contrast, SPOT, by incorporating the skeleton posterior in the optimization, manifests stable and strong performance across all node sizes with a small standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Skeleton Posterior Inference</head><p>Through this experiment, our goal is to determine if SPOT is a reliable skeleton posterior estimator. We assess the posterior quality by calculating the KL-divergence between the estimated posterior and ground-truth skeleton. Following earlier research <ref type="bibr" target="#b24">[25]</ref>, we report AUPRC and AUROC for edge predictions to evaluate the quality of the estimated posterior.</p><p>Using the same datasets from Sec. 4.1, we compare SPOT to three baselines: AVICI <ref type="bibr" target="#b24">[25]</ref>, FCI* (Nonparametric Bootstrap FCI), and RFCI* (Nonparametric Bootstrap RFCI). We tried to include more baselines (e.g., DiBS <ref type="bibr" target="#b23">[24]</ref> and N-ADMG <ref type="bibr" target="#b2">[3]</ref>) but they either frequently crash or encounter out-of-memory issues when processing large graphs used in the experiment. These baselines are not designed for skeleton posterior inference; however, we sum the probabilities of all edge types, calibrate to a maximum of 1, and recast their output as a skeleton posterior for a meaningful comparison in Table <ref type="table" target="#tab_3">3</ref>. In addition, we also conduct an ablation study by replacing the ML4S-like model with an end-to-end Transformerbased model (denoted as SPOT (w/ Ke et al. <ref type="bibr" target="#b21">[22]</ref>)) to justify our design consideration in Sec. 3.2.</p><p>Comparison with Baselines. Overall, we observe that SPOT consistently outperforms the baselines across all criteria in Table <ref type="table" target="#tab_3">3</ref>. This finding is interpreted as encouraging and reasonable. First, SPOT employs a machine learning model to estimate the adjacency probability while FCI and RFCI apply hard thresholds over p-values to reject edges. Since the relationship between p-values and adjacency probabilities may be intricate, SPOT is more adaptable in capturing these dependencies. Second, despite having a similar focus on variational inference, AVICI is not intended for use with causally insufficient data, and the size of the graph may make it more challenging to perform whole-graph inference. In contrast, SPOT efficiently resolves this problem by edge decomposition.</p><p>Ablation Study. We also observe that the Transformer-based model (SPOT (w/ Ke et al. <ref type="bibr" target="#b21">[22]</ref>)) yields a lower AUROC (0.95) and AUPRC (0.77) as well as a higher KL (0.07) than the ML4S-like cascade model (i.e., worse performance on all criteria). This finding is consistent with our design consideration in Sec. 3.2 that the cascade model is more suitable for the task of skeleton posterior inference. We presume that the cascade model effectively decompose the whole graph into edge predictions and reduce the complexity of the task, which is crucial for large graphs.</p><p>Out-of-Distribution Setting. In addition, we also explore the effectiveness of the domain adaption procedure in SPOT for out-ofdistribution datasets. To do so, we generate ten ADMGs from the Scale-Free (SF) model as test datasets and evaluate the enhancement of our domain adaption strategy. On the SF graphs, the original model trained on ER graphs suffers from a downgrade on KL (from 0.03 to 0.06). When augmented with the lightweight domain adaption step, the KL is further reduced to 0.05 (16.7% enhancement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Extension to Other Settings</head><p>Though the primary focus of SPOT lies in linear Gaussian data with latent confounders, we also explore its effectiveness on other types of data. In particular, we integrate SPOT with neural variational ADMG learning <ref type="bibr" target="#b2">[3]</ref> (N-ADMG) which handles non-linear data and neural variational DAG learning <ref type="bibr" target="#b16">[17]</ref> (DECI) and GFlowNets <ref type="bibr" target="#b12">[13]</ref> on linear Gaussian data without latent confounders. Neural Variational ADMG Learning. Due to the scalability issue of N-ADMG, we are unable to apply it to datasets with similar sizes as in Sec. 4.1. Instead, we use the same dataset in its original paper <ref type="bibr" target="#b2">[3]</ref> with five nodes and 2000 samples under the non-linear Gaussian setting. Here, we use the kernel-based conditional independence test <ref type="bibr" target="#b41">[43]</ref> to compute the required statistics in Sec. 3.2. However, it is worth noting that SPOT itself is agnostic to the functional form of the SCM as long as the distribution is faithful and the required statistics can be appropriately computed. Since N-ADMG yields a distribution over ADMGs, we sample 100 ADMGs from the posterior and report the average performance in Table <ref type="table" target="#tab_4">4</ref>. Note that, in this setting, because the ADMG is identifiable, we compare the generated ADMG with the ground-truth ADMG. We observe that SPOT significantly improves the performance of N-ADMG on all criteria. In particular, it reduces the FDR by 13% and the SHD by 21%.</p><p>In the dataset, N-ADMG is capable of identifying all true edges (i.e., TPR=1.0). However, it also introduces many false positives. SPOT effectively reduces the number of false positives and improves the overall performance. In this regard, SPOT offers a principled way to discourage potentially superfluous edges and thus alleviates the overfitting issue. Neural Variational DAG Learning. Since MAG is a generalization of DAG, our framework is naturally applicable to infer the skeleton posterior of DAGs and thus can be integrated with neural variational DAG learning. To demonstrate this potential extension of SPOT, we integrate it with DECI <ref type="bibr" target="#b16">[17]</ref> and report the result in Table <ref type="table" target="#tab_5">5</ref>. Here, even though SPOT is originally designed for MAGs, we can observe a non-trivial performance boost on DAGs. In particular, SPOT improves the F1 score of skeleton and orientation by 5% and 9%, respectively, and reduces the SHD by 2%. This finding indicates a potential avenue for future research to further improve the performance of differentiable DAG learning algorithms with skeleton posterior. In addition to DECI, we also tried to integrate SPOT with GFlowNets <ref type="bibr" target="#b12">[13]</ref> which estimates the posterior of DAGs using generative flow networks. With the same setting, we observe that SPOT reduces the expected SHD of the posterior from 339.8 to 224.4, indicating a 34% improvement. We present the full results in the appendix due to the space limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Causal Discovery with Latent Confounders. Causal discovery with latent confounders involves constraint-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b40">42]</ref> and score-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">36]</ref>. Constraint-based methods establish causal graphs based on conditional independence <ref type="bibr" target="#b28">[29]</ref>, while score-based methods find maximal likelihood estimations. Differentiable causal discovery utilizes continuous optimization techniques <ref type="bibr" target="#b4">[5]</ref>. With the prosperity of its practical applications <ref type="bibr">[18-21, 28, 39]</ref>, SPOT advances scalable and accurate causal discovery with latent confounders for real-world use.</p><p>Posterior Inference of Causal Structure. Traditional causal discovery provides a maximum-likelihood point estimation, but reliability is often criticized for small sample sizes <ref type="bibr" target="#b10">[11]</ref>. Providing a posterior distribution is more desired using MCMC <ref type="bibr" target="#b35">[37]</ref>, variational inference <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, reinforcement learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">40]</ref>, generative flow network <ref type="bibr" target="#b12">[13]</ref>, or supervised learning <ref type="bibr" target="#b25">[26]</ref>. All methods estimate DAGs' posterior distributions and is infeasible for MAGs. Among these methods, AVICI <ref type="bibr" target="#b24">[25]</ref> is the most relevant to SPOT as it also uses amortized variational inference. In a nutshell, it trains an inference model over samples from a known (static) simulator and learn the DAGs from data. In the context of ADMG learning, N-ADMG <ref type="bibr" target="#b2">[3]</ref> presents a variational inference method to estimate the posterior of ADMGs on non-linear data. However, it is not scalable well to large datasets, as shown in Sec. 4. SPOT trains an inference model on these samples from static/dynamic simulators and alleviate the additional assumption of the data.</p><p>The Role of Skeleton in Causal Discovery. Skeleton learning is crucial for constraint-based methods <ref type="bibr" target="#b39">[41]</ref> and affects overall performance. Score-based DAG learning methods like MMHC use skeletons to reduce search space <ref type="bibr" target="#b33">[35]</ref>. Ma et al. improve the performance of NOTEARS via a more precise skeleton learned by ML4S <ref type="bibr" target="#b25">[26]</ref>. SPOT demonstrates the importance of skeleton learning in differentiable causal discovery. Taking these evidences into consideration, it may become clear that skeleton learning can serve as the backbone for general causal discovery tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we introduce a framework for differentiable causal discovery in the presence of latent confounders with skeleton posterior. To this end, we propose SPOT, which features a highly efficient variational inference algorithm to estimate the underlying skeleton distribution from given observational data. It also provides a stochastic optimization procedure to seamlessly incorporate the skeleton posterior with the differentiable causal discovery pipeline in an "out-of-the-box" manner. The results of our experiments are highly encouraging and show that SPOT outperforms all existing methods to a notable extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PRELIMINARY</head><p>To keep the paper self-contained, we provide additional definitions and notations used in the main text.</p><p>Definition 4 (Directed Cycle). Given an ADMG ğº with a set of nodes ğ‘‰ and a set of edges ğ¸, a directed cycle exists when there is a directed path from a node ğ‘‰ ğ‘– back to itself. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION OF SCL MODELS IN SPOT</head><p>In accordance with ML4S <ref type="bibr" target="#b25">[26]</ref>, we implement the supervised causal learning (SCL) model in SPOT as a series of cascade xgboost <ref type="bibr" target="#b7">[8]</ref> classifiers with default hyperparameters. For linear datasets, we use Fisher-z test to calculate the conditional independence and constitute the features for the classifier. For non-linear datasets, we use the kernel-based conditional independence test <ref type="bibr" target="#b41">[43]</ref> instead. We use the output of the last layer of the cascade xgboost classifiers as the posterior probability of the presence of an edge. Since the model used in Ke et al. <ref type="bibr" target="#b21">[22]</ref> is not publicly available, we made our best effort to replicate the model based on description in the paper. The experiments are conducted on a server with NVIDIA A6000 GPU and 256GB RAM. The model is trained with 128 epochs and a batch size of 1. We use the Adam optimizer with a learning rate of 0.0003. When trained with 128 epochs, the metrics is seen to converge, as shown in Fig. <ref type="figure" target="#fig_6">5</ref>. On the large-scale causal graphs with 50-100 nodes, each dataset contains 1,000 observational samples. However, the GPU (with 48GB memory which is the largest GPU memory available to us) can at most handle 600 samples at a time. Hence, we drop the last 400 samples in each dataset. This issue, to a certain extent, indicates the scalability limitation of end-to-end supervised causal learning models and promotes the necessity of ML4S-like methods, which are much more scalable and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ABLATION STUDY ON SPARSITY PRIOR</head><p>In our optimization procedure, we enforce a heuristic in which sparsifying proposals are always accepted. This design attempts to analogize a range of the regularization term in score-based causal discovery algorithms where sparse causal graphs are preferred over dense ones. To evaluate the impact of this design, we conduct an ablation study by removing the sparsity prior from the optimization procedure. The results are shown in Table <ref type="table" target="#tab_7">6</ref>. Overall, we observed that the strategy indeed provides a nontrivial improvement to performance and believe that such sparsity consideration plays an important role in "pruning" dense causal graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D COMPUTE TIME</head><p>We refrain from providing a table regarding compute time because different algorithms are computed on different architectures, making it difficult to draw meaningful conclusions. For instance, FCI can only use a single core, whereas ABIC/SPOT can utilize up to 32 cores, and N-ADMG primarily relies on GPU. Additionally, on some large datasets, ABIC may fail to terminate even after running for one week. These issues complicate direct comparisons. Following your suggestion and consistent with the main setup in our paper, we report the compute times for ABIC and SPOT on the same computing architecture below.</p><p>Overall, we believe SPOT enables lower compute time as well as better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E COMPARISON BY DIFFERENT NUMBER OF NODES</head><p>We report the full results of the comparison of different methods on different methods, including FCI, RFCI, ABIC and SPOT. Aligned   We also explore the integration of SPOT with GFlowNets <ref type="bibr" target="#b12">[13]</ref> to further improve the performance of differentiable causal discovery. GFlowNets is a generative flow network that learns the posterior distribution of DAGs. We use the same experimental setup as in Sec. 4.3, and the results are shown in Table <ref type="table" target="#tab_9">8</ref>. We observe that SPOT improves the performance of GFlowNets by 13.6% in F1 score and 28.6% in precision at the cost of a slight decrease in recall. This result demonstrates the synergistical effect of SPOT in differentiable causal discovery. We also explore the application of our proposed framework on Sachs <ref type="bibr" target="#b30">[32]</ref>, a real-world dataset with 853 entries on protein expressions involved in human immune system cells. Fig. <ref type="figure" target="#fig_8">7</ref> presents the promising results of SPOT applied to the Sachs dataset. Since every edge endpoint of Sachs is unidentifiable, its PAG corresponds exactly to the skeleton. Therefore, we evaluate the learned graph's quality at the skeleton level. The discovered skeleton in Fig. <ref type="figure" target="#fig_8">7</ref> achieves an F1 score of 0.63. Although there is a decrease in performance compared to the results in Sec. 4.1, it still significantly outperforms ABIC (with an F1 score of 0.48) and provides satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F INTEGRATION WITH GFLOWNETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G REAL-WORLD DATA</head><p>The decrease in performance can be partially attributed to the non-Gaussian nature of real-world data, which affects the fundamental premise of ABIC, the backbone of our framework. Since SPOT itself (i.e., skeleton posterior inference phase) is agnostic to Gaussianity, it delivers an impressive 31.2% improvement over vanilla ABIC. We believe that the performance of SPOT has the potential for even further improvement by incorporating differentiable methods that handle non-Gaussian data in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SPOT workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>E</head><label></label><figDesc>ğ‘ (D) KL ğ‘ (ğ‘† | D ) | | ğ‘ (ğ‘†; ğ‘“ ğœ™ ( D ) ) =E ğ‘ (D) E ğ‘ (ğ‘† |D) log ğ‘ (ğ‘† | D )log ğ‘ (ğ‘†; ğ‘“ ğœ™ ( D ) ) = -E ğ‘ (ğ‘† ) E ğ‘ (D |ğ‘† ) log ğ‘ (ğ‘†; ğ‘“ ğœ™ ( D ) ) + const. = -E ğ‘ (D,ğ‘† ) log ğ‘ (ğ‘†; ğ‘“ ğœ™ ( D ) ) + const. (7) Since the constant does not depend on ğœ™, we can merely minimize L (ğœ™) -E ğ‘ ( D,ğ‘† ) log ğ‘(ğ‘†; ğ‘“ ğœ™ (D)) to obtain ğœ™. In other words, the problem is recast to train a predictive model ğ‘“ ğœ™ (â€¢) : D â†¦ â†’ ğ‘† over the distribution ğ‘ (D, ğ‘†).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Skeleton Inference Procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The probability of accepting an update for different ğ‘ ğ‘– ğ‘— and ğ‘¡ with ğ‘ = 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of ABIC and SPOT on different node sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence of the model in Ke et al. [22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of methods on different node sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Application on Sachs dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Boosting differentiable causal discovery with skeleton information. (avg. on ten datasets with 50-100 variables.)</figDesc><table><row><cell>Method</cell><cell cols="3">Skeleton F1 Arrowhead F1 Tail F1</cell></row><row><cell>ABIC</cell><cell>0.84</cell><cell>0.76</cell><cell>0.67</cell></row><row><cell cols="2">w/ True Skeleton 1.00 +19%</cell><cell>0.96 +26%</cell><cell>0.96 +33%</cell></row><row><cell>w/ FCI</cell><cell>0.87 +4%</cell><cell>0.84 +11%</cell><cell>0.71 +6%</cell></row><row><cell>SPOT</cell><cell>0.91 +8%</cell><cell>0.86 +13%</cell><cell>0.78 +16%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Then, instead of directly taking ğ›¿ * , ğ›½ * as Algorithm 2: Skeleton Posterior-guided Optimizer Input: Skeleton Posterior ğ‘, Optimization Variables ğ›¿, ğ›½, Objective Function ğ‘“ , ALM Step ğ‘¡, Temperature Constant ğ‘ Output: Optimized Variables ğ›¿, ğ›½ 1 ğ›¿ * , ğ›½ * â† arg min ğ‘“ (ğ›¿, ğ›½); 2 forall ğ›¿ ğ‘– ğ‘— and ğ›½ ğ‘– ğ‘— do Update ğ›¿ ğ‘– ğ‘— â† ğ›¿ * ğ‘– ğ‘— with probability (ğ‘ ğ‘– ğ‘— + ğ‘)</figDesc><table><row><cell>3</cell><cell>if ğ›¿ ğ‘– ğ‘— Ã—</cell><cell>ğœ•ğ‘“ (ğ›¿,ğ›½ ) ğœ•ğ›¿ ğ‘– ğ‘—</cell><cell>&gt; 0 then</cell></row><row><cell>4</cell><cell cols="2">ğ›¿ ğ‘– ğ‘— â† ğ›¿  *  ğ‘– ğ‘—</cell><cell></cell></row><row><cell>5</cell><cell>else</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>1 ğ‘¡ +1 ;</cell></row><row><cell>7</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell cols="2">8 end</cell><cell></cell><cell></cell></row><row><cell cols="2">9 return ğ›¿, ğ›½;</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>End-to-end comparison (avg. on ten datasets with nodes âˆˆ [50, 100]).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Skeleton</cell><cell></cell><cell></cell><cell></cell><cell>Arrowhead</cell><cell>Tail</cell><cell>#Failed</cell></row><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>F1</cell><cell></cell><cell cols="2">TPR</cell><cell></cell><cell>FDR</cell><cell></cell><cell>F1</cell><cell>TPR</cell><cell>FDR</cell><cell>F1</cell><cell>TPR</cell><cell>FDR</cell><cell>Datasets</cell></row><row><cell>FCI</cell><cell></cell><cell cols="9">0.84 Â± 0.06 0.79 Â± 0.10 0.10 Â± 0.05 0.57 Â± 0.05 0.78 Â± 0.15 0.54 Â± 0.04 0.53 Â± 0.18 0.46 Â± 0.20 0.31 Â± 0.15</cell><cell>0/10</cell></row><row><cell>RFCI</cell><cell></cell><cell cols="9">0.85 Â± 0.07 0.77 Â± 0.09 0.05 Â± 0.03 0.62 Â± 0.09 0.76 Â± 0.12 0.45 Â± 0.12 0.51 Â± 0.15 0.42 Â± 0.17 0.27 Â± 0.14</cell><cell>0/10</cell></row><row><cell>ICD</cell><cell></cell><cell cols="9">0.82 Â± 0.09 0.81 Â± 0.07 0.17 Â± 0.11 0.56 Â± 0.10 0.82 Â± 0.06 0.57 Â± 0.12 0.48 Â± 0.15 0.41 Â± 0.15 0.39 Â± 0.18</cell><cell>1/10</cell></row><row><cell cols="2">M3HC</cell><cell cols="9">0.73 Â± 0.09 0.59 Â± 0.12 0.03Â± 0.03 0.56 Â± 0.15 0.46 Â± 0.16 0.27 Â± 0.11 0.32 Â± 0.11 0.24 Â± 0.10 0.48 Â± 0.13</cell><cell>0/10</cell></row><row><cell>GPS</cell><cell></cell><cell cols="9">0.73 Â± 0.10 0.81 Â± 0.11 0.33 Â± 0.11 0.62 Â± 0.09 0.85 Â± 0.11 0.50 Â± 0.10 0.43 Â± 0.12 0.62 Â± 0.19 0.66 Â± 0.10</cell><cell>0/10</cell></row><row><cell>ABIC</cell><cell></cell><cell cols="9">0.84 Â± 0.04 0.89 Â± 0.07 0.19 Â± 0.07 0.76 Â± 0.07 0.85 Â± 0.15 0.29 Â± 0.11 0.67 Â± 0.07 0.80 Â± 0.15 0.40 Â± 0.07</cell><cell>0/10</cell></row><row><cell>SPOT</cell><cell></cell><cell cols="9">0.91 Â± 0.03 0.94 Â± 0.04 0.11 Â± 0.04 0.86 Â± 0.05 0.89 Â± 0.10 0.16 Â± 0.06 0.78 Â± 0.09 0.86 Â± 0.13 0.27 Â± 0.13</cell><cell>0/10</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of nodes</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ABIC</cell><cell></cell><cell>SPOT</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on skeleton posterior inference (avg. on ten datasets). AUROC and AUPRC: higher is better; KL: lower is better.</figDesc><table><row><cell>Metric</cell><cell cols="4">AVICI FCI* RFCI* SPOT</cell></row><row><cell>AUROC</cell><cell>0.96</cell><cell>0.97</cell><cell>0.95</cell><cell>0.99</cell></row><row><cell>AUPRC</cell><cell>0.83</cell><cell>0.92</cell><cell>0.91</cell><cell>0.97</cell></row><row><cell>KL</cell><cell>0.05</cell><cell>0.06</cell><cell>0.10</cell><cell>0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Integration with neural variational ADMG learning (avg. on 100 sampled ADMGs).</figDesc><table><row><cell></cell><cell>SHD</cell><cell>FDR</cell><cell>TPR</cell></row><row><cell>N-ADMG [3]</cell><cell>2.61</cell><cell>0.39</cell><cell>1.0</cell></row><row><cell cols="4">N-ADMG + SPOT 2.07 -21% 0.34 -13% 1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Integration with neural variational DAG learning (avg. on 100 sampled DAGs).</figDesc><table><row><cell></cell><cell cols="2">Skeleton F1 Orientation F1</cell><cell>SHD</cell></row><row><cell>DECI [17]</cell><cell>0.55</cell><cell>0.44</cell><cell>121.2</cell></row><row><cell>DECI + SPOT</cell><cell>0.58 +5%</cell><cell>0.48 +9%</cell><cell>118.7 -2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Definition 5 (Almost Directed Cycle). Given an ADMG ğº with a set of nodes ğ‘‰ and a set of edges ğ¸, an almost directed cycle exists when there is a bidirected edge ğ‘‰ ğ‘– â†” ğ‘‰ ğ‘— such that ğ‘‰ ğ‘– âˆˆ ğ‘¨ğ’ ğº (ğ‘‰ ğ‘— ). ğ‘¨ğ’ ğº (ğ‘‰ ğ‘— ) denotes the set of ancestors (by directed paths) of ğ‘‰ ğ‘— in ğº.Definition 6 (Bow). Given an ADMG ğº with a set of nodes ğ‘‰ and a set of edges ğ¸, a bow exists when there are two edges ğ‘‰ ğ‘– â†’ ğ‘‰ ğ‘— and ğ‘‰ ğ‘— â†” ğ‘‰ ğ‘— .Definition 7 (Bow-free ADMG). An ADMG is bow-free if it does not contain any directed or a bow.A path (ğ‘‹,ğ‘Š 1 , â€¢ â€¢ â€¢ ,ğ‘Š ğ‘˜ , ğ‘Œ ) is said to be blocked by ğ‘ âŠ† ğ‘¿ \{ğ‘‹, ğ‘Œ } if there exists a node ğ‘Š ğ‘– âˆˆ {ğ‘Š 1 , â€¢ â€¢ â€¢ ,ğ‘Š ğ‘˜ } such that a) ğ‘Š ğ‘– is not a collider but a member of ğ‘ , or b) ğ‘Š ğ‘– is a collider but not an ancestor of any nodes of ğ‘ . We now introduce m-separation.Definition 8 (m-separation<ref type="bibr" target="#b40">[42]</ref>). ğ‘‹, ğ‘Œ are m-separated by ğ‘ (denoted by ğ‘‹ â«« G ğ‘Œ | ğ‘ ) if all paths between ğ‘‹, ğ‘Œ are blocked by ğ‘ .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on sparsity prior.</figDesc><table><row><cell></cell><cell cols="3">Skeleton F1 Head F1 Tail F1</cell></row><row><cell>ABIC</cell><cell>0.84</cell><cell>0.76</cell><cell>0.67</cell></row><row><cell>SPOT w/o Sparsity</cell><cell>0.90</cell><cell>0.81</cell><cell>0.71</cell></row><row><cell>SPOT</cell><cell>0.91</cell><cell>0.86</cell><cell>0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Average compute time (in seconds) for ABIC and SPOT.</figDesc><table><row><cell cols="2">Algorithm Avg. Time (sec.)</cell></row><row><cell>ABIC</cell><cell>3139</cell></row><row><cell>SPOT</cell><cell>2883</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Integration with GFlowNets<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell></cell><cell cols="3">F1 Precision Recall</cell></row><row><cell>GFlowNets [13]</cell><cell>0.22</cell><cell>0.14</cell><cell>0.45</cell></row><row><cell cols="2">GFlowNets + SPOT 0.25</cell><cell>0.18</cell><cell>0.44</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their valuable comments. The authors from HKUST were supported in part by a <rs type="funder">RGC CRF</rs> grant under the contract <rs type="grantNumber">C6015-23G</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_R4Qt2Du">
					<idno type="grant-number">C6015-23G</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimal I-MAP MCMC for scalable structure discovery in causal DAG models</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Forward amortized inference for likelihood-free variational marginalization</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ambrogioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><surname>GÃ¼Ã§lÃ¼</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Berezutskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Borne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">YaÇ§mur</forename><surname>GÃ¼Ã§lÃ¼tÃ¼rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Gerven</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrin</forename><surname>Hilmkil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ordering-based causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4098" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2314" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Stats</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integer programming for causal structure learning in the presence of latent variables</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeeb</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greedy equivalence search in the presence of latent confounders</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ioan</surname></persName>
		</author>
		<author>
			<persName><surname>Bucur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence. PMLR</title>
		<imprint>
			<biblScope unit="page" from="443" to="452" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Stats</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bcd nets: Scalable variational approaches for bayesian causal discovery</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7095" to="7110" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ml4c: Seeing causality through latent vicinity</title>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2023 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian structure learning with generative flow networks</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AntÃ³nio</forename><surname>GÃ³is</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Rankawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="518" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Practical methods of optimization</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987. 1987</date>
			<publisher>Wiley Interscience Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data analysis with bayesian networks: a bootstrap approach</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moises</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agrin Hilmkil, et al. 2022. Deep End-to-end Causal Inference</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Antoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kukla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Causality for Real-world Impact</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach</title>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06680</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perfce: Performance debugging on databases with chaos engineering-enhanced causality analysis</title>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1454" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causality-aided trade-off analysis for machine learning fairness</title>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="371" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cc: Causalityaware coverage criterion for deep neural networks</title>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1788" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Induce Causal Structure</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Rosemary Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorg</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Curtis Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">P</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dibs: Differentiable bayesian structure learning</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24111" to="24123" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12934</idno>
		<title level="m">Amortized Inference for Causal Structure Learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ml4s: Learning causal skeleton from vicinal graphs</title>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1213" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">XInsight: eXplainable Data Analysis Through The Lens of Causality</title>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Management of Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Noleaks: Differentially private causal discovery under functional causal model</title>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2324" to="2338" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enabling Runtime Verification of Causal Discovery Algorithms with Automated Conditional Independence Reasoning</title>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peisen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE/ACM International Conference on Software Engineering</title>
		<meeting>the 46th IEEE/ACM International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Iterative causal discovery in the possible presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Shami</forename><surname>Raanan Y Rohekar</surname></persName>
		</author>
		<author>
			<persName><surname>Nisimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2454" to="2465" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Yaniv Gurwicz, and Gal Novik</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning linear non-Gaussian causal models in the presence of latent variables</title>
		<author>
			<persName><forename type="first">Saber</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1436" to="1459" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Clark N Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><surname>Heckerman</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On scoring maximal ancestral graphs with the max-min hill climbing algorithm</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Tsirlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Lagani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Sofia Triantafillou, and Ioannis Tsamardinos</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards scalable bayesian learning of causal dags</title>
		<author>
			<persName><forename type="first">Jussi</forename><surname>Viinikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Pensar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikko</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6584" to="6594" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">D&apos;ya like dags? a survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necati</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causal discovery with unobserved confounding and non-Gaussian data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="61" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinforcement causal structure learning on order graph</title>
		<author>
			<persName><forename type="first">Dezhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengtian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maozu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10737" to="10744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A review on algorithms for constraint-based causal discovery</title>
		<author>
			<persName><forename type="first">Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03977</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kernelbased conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Causal Discovery with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
