<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding and evaluating computer vision models through the lens of counterfactuals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-08-28">28 Aug 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Walter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Chinchure</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Diana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Tolbert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kartik</forename><surname>Hosanagar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vineeth</forename><surname>Balasubramanian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">INSTITUTE AT CHICAGO Chicago</orgName>
								<orgName type="institution">TOYOTA TECHNOLOGICAL</orgName>
								<address>
									<postCode>2025</postCode>
									<settlement>Illinois August</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding and evaluating computer vision models through the lens of counterfactuals</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-08-28">28 Aug 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2508.20881v1[cs.CV]</idno>
					<note type="submission">A thesis submitted in partial fulfillment of the requirements for the degree of</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Good collaboration is often key to impactful research, and the work presented in this thesis has largely been the result of collaborative efforts. I would like to acknowledge and give due credit to my fellow collaborators. I also wish to emphasize that I personally value collaboration with researchers from diverse backgrounds, as it fosters deeper discussions and brings varied perspectives to the work.</p><p>The work in Chapter 3 was a collaboration between myself, Sushil Bharati, and Matthew Turk. Sushil Bharati contributed by running experiments on the CelebA dataset, while Matthew Turk provided valuable guidance through insightful discussions.</p><p>The work in Chapter 4 was a collaboration between myself, Dhruv Srikanth, Lee Cohen, and Matthew Turk. I led the conceptualization and prototyping of the approach, while Dhruv Srikanth played a key role in scaling and executing the experiments. Lee Cohen and Matthew Turk provided valuable feedback and were actively involved in project discussions.</p><p>The work in Chapter 5 was a collaboration between myself, Aditya Chinchure, Gaurav Bhatt, Kiri Salij, Kartik Hosanagar, Leonid Sigal, and Matthew Turk. I led the conceptualization of the project, while Aditya Chinchure led the experimental efforts, though both of us contributed across these aspects. Gaurav Bhatt conducted comparisons between our approach and ITI-GEN, and Kiri Salij contributed code for using image captions to evaluate TTI models. Leonid Sigal, Matthew Turk, and Kartik Hosanagar provided valuable feedback throughout, with Kartik playing an instrumental role in designing the user study and robustness analyses for VQA.</p><p>The work in Chapters 6 and 7 was a collaboration between myself,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>A PhD thesis is not just a collection of research aimed at advancing the frontiers of knowledge; it's also a multi-year endurance test disguised as academia-filled with growth, setbacks, far too many late nights, and the occasional existential crisis. Along the way, you realize it's not a solo adventure; many people, both academically and personally, play vital roles in helping you survive (and sometimes even enjoy) the journey. In my case, I have been fortunate to have many such people whose support, encouragement, and kindness helped me reach this point. To borrow a line from my advisor's own PhD acknowledgments: "Barring an unexpected Academy Award, this may be my only opportunity to publicly thank you." I would add: while I may not deserve the award for Best Actor, many of you certainly deserve recognition for Best Supporting Actor.</p><p>Thank you for walking alongside me during this journey. First and foremost, I would like to thank my advisor, Prof. Matthew Turk. Matthew has been a constant source of support-not just as my PhD advisor at TTIC but also during my time as a master's student at UC Santa Barbara. He has always been calm and composed, qualities I have learned from during both good and challenging times. I am immensely grateful for his unwavering support and for always finding time for me, even while serving as the President of TTIC. He has pushed me to strive for excellence in the gentlest of ways, and his mentorship has been one of the most important influences on my academic and personal journey. I would like to thank my committee members, Leonid Sigal and Matthew Walter-it was a pleasure to also serve as a TA for one of Matthew's courses. Leon, your suggestions during our discussions were highly beneficial and contributed meaningfully to my work.</p><p>I would also like to thank all members of the TTIC family-TTIC is truly a wonderful place to pursue a PhD. I am especially grateful to Madhur Tulsiani and Julia Chuzhoy for their initial support, particularly in helping me survive my Algorithms coursework. I am also thankful to Greg Shakhnarovich for his guidance during my Machine Learning class.</p><p>The administrative team at TTIC-Amy Minick, Chrissy Coleman, Brandie Jones, Mary Marre, Adam Bohlander, Jessica Jacobson, Erica Cocom, Rose Bradford, Erica Cocom, Celste Ki, Randall Landsberg, Alicia McClarin and Deree Kobets-deserves my heartfelt thanks for their patience and support in helping me navigate countless forms and deadlines, despite my chronic tardiness. Your reminders and willingness to assist were invaluable. A special thanks to Mary Marre for always keeping me informed about leftover food in the fridge waiting to be rescued -those timely up-dates truly brightened many of my days! I am also deeply grateful to Amy Minick for her endless patience and helpful suggestions, especially when it came to navigating the labyrinth of visa paperwork. I am grateful to Randy for giving me a chance to be a part of the TTIC outreach initiative and a taking the time out to give extremely valuable feedback on my defense presentation. Also, a note of thanks to Jerry and Ayesha for cheering me up with random conversations.</p><p>I am grateful to all my research collaborators who made this journey possible: Gaurav Bhatt, Kartik Hosanagar, Kiri Salij, Alexander Tolbert, Dhruv Srikanth, Lee Cohen, Emily Diana, Vineeth Balasubramanian, and Sushil Bharati-thank you for your insightful discussions and collaborations. Gaurav, our friendship has been long-standing, and it was great to finally work together on a project. I owe special thanks to Aditya Chinchure (Adi) for extensive collaborations-honestly, a large part of this thesis would not have been possible without your support.I thoroughly enjoyed those late-night conversations about research -where every idea sounded brilliant at 2 a.m. but much less so after coffee the next morning. Your visit to TTIC as an intern, including our trip to Yosemite and the West Coast, will always be memorable. This journey would not have been possible without the friendships I've been fortunate to form. Among my friends at TTIC, I am especially grateful to Shubham Toshniwal for his advice in my early years and for our exploratory trips to random restaurants in Chicago-even during COVID times, masked up. Shashank Srivastava deserves thanks for helping me through coursework and qualifying exams. Sudarshan Babu, living as roommates for two years was indeed an adventure -I'm just glad we both survived with our sanity (mostly) intact. David Yunis, thank you for those deep, thoughtful conversations over Diet Coke in Chicago bars-you are truly a gem of a person. I am also grateful to Max and Kavya for making me the honorary third wing of Dumbledore's Army. I am thankful to Ankita, Mrinal, Takuma, Gene, Han, Kezziah, Teddy, Kshitij and Nirmit for some great interactions during and outside TTIC.</p><p>My heartfelt thanks extend to my friends in Chicago beyond TTIC: Ansh, Neha, Peter, Shehjar, and Apeksha. Shehjar and Apeksha, it has been very special to be the godfather to your daughter. I am also grateful to my running group-thank you for giving me something to look forward to every Saturday during long runs. I am thankful to Aditya Raj for his unwavering support and for introducing me to the philosophical perspective of Sir Christian Cole. In the past three years, you have been my go-to person for every kind of help. I am grateful to Eleven Dew Drops for the positive impact during a low phase of my PhD. Although our interactions were brief, the impact was deep and much needed.</p><p>To my college friends-Lokesh, Hitesh, Jeetu, Avinash, Shailesh, Kapil, Parashar, Meghna, and Sharma-thank you for four wonderful years and for your continued encouragement afterward. Even after eleven years, our friendship remains strong. Lokesh, you were an incredible roommate-thank you for tolerating all my eccentricities in undergrad. To my childhood friends, Mishra, Anindya, and Archit, I am proud of how our bond has only grown stronger over time. Very few people have believed in me as much as you have. I am not someone who openly romanticizes friendship, but secretly, I cherish it the most.</p><p>Two practices that have stayed with me through the toughest phases of this journey are reading the Gita and practicing Vipassana. I would urge every PhD student to cultivate some form of spiritual practice. On days when reviews were harsh or experiments failed, it was reassuring to hear a quiet voice within say: "This too shall pass." Unfortunately, that voice did not write rebuttals or fix broken code. I would like to sincerely thank Ajay Ji, Vinika Didi, Vivek Ji, and Aditya Ji for their consistent encouragement and for sharing thoughtful perspectives on life. Their guidance, often drawing from the teachings of the Gita-particularly the emphasis on focusing on one's actions without attachment to outcomes-has been a valuable source of clarity and balance throughout this journey.</p><p>The last three years wouldn't have been nearly as fulfilling or productive without my meditation buddies, Gaurav and Kartik. I still can't believe I've been regular at something for this long-thank you for keeping me accountable. This practice has had a meaningful and lasting impact on both my personal and academic life. Beyond our the practice I really admire how the two of you push me to be a better person in different aspects of my life. I really look up to the two of you.</p><p>I am also deeply grateful to the teachers who shaped me. Archana Ma'am supported me in my early years and laid the foundation for my academic journey. Bilal Sir played a pivotal role in instilling in me a love for programming, a passion that has stayed with me ever since. During my undergraduate years, Kamal Kapoor Sir provided constant encouragement and support, helping me navigate both academic and personal challenges. Varun Pratap Singh Sir was the first to suggest that I consider pursuing a PhD abroad-he saw potential in me that I had not yet recognized in myself. Among all my teachers, the one to whom I am most indebted is Ankush Mittal Sir. His guidance shaped me not just as a student but as an individual, and I will always remain grateful for his belief in my potential and for showing me the path that brought me here.</p><p>Coming to family: I am grateful to all my grandparents for their love and affection -thanks to them, I've developed an above-average capacity for unsolicited advice, overfeeding others, and expressing heartfelt concern, all clear signs of deep affection. I am especially thankful to my grandfathers for all those wonderful stories and for patiently playing cricket with me all day long, and to my grandmothers for their tireless efforts to overfeed me as a child -a mission they took very seriously and executed with great success. Kamala Awasthi and Ravi-Shankar Shukla, I still miss you from the deepest corners of my heart. To my uncle and aunt, Sudhanshu and Kanchan, thank you for showing me how to treat others with kindness and care. Your excitement about me pursuing my studies in the US was truly unmatched -even if, at the time, the rising price of bhaat(rice) in India might have suggested there were more pressing things to worry about. To Mausa and Mausi, thank you for treating me like your own child. A note of thanks to Kanishka for inspiring me to write that paper in 2017, which set me on this journey. To my cousins -Shruti, Pavani, Ashutosh, Saarika, Supriya, Saurabh, and Shashank -thank you for your steady support. A special thanks to Shashank for being my personal tech support, life coach, and emergency backup plan whenever things went sideways. Shruti and Pavani, thank you for the constant reminder that Winter is coming, and for being my Arya and Sansa in life -because, as you know, I know nothing.</p><p>I am deeply grateful to my parents, whose unwavering support has made this long and winding journey possible. Honestly, this PhD feels like a joint family project at this point. My mother, who once aspired to study mathematics but couldn't, has been a quiet (and occasionally not-so-quiet) source of inspiration -I hope this PhD in an applied math field earns me at least some bragging rights on her behalf. My father's unique philosophy on failure -that as a scientist, I should be well-trained by now to expect failure in 95% of the things I attempt -has kept me grounded and helped me maintain perspective (and my sense of humor) throughout this process. I suspect he's just happy I've finally reached the 5% that worked.</p><p>Finally, I am grateful to God for providing me with the strength, clarity, and resilience to survive this journey. No amount of machine learning or scientific reasoning has calmed my anxiety as effectively as a quick prayer in moments of panic. Honestly, prayer has been my go-to debugging tool for life. Please continue to save me -and let's just hope I pass this final hurdle, because at this point, divine intervention feels like my best bet.  In a famous study conducted by Moss-Racusin et al. <ref type="bibr" target="#b0">[1]</ref>, identical resumes differing only in the applicant's name ("John" vs. "Jennifer") were submitted for a lab manager position. It was observed that faculty were significantly more likely to favor the male applicant. This style of reasoning-holding all factors constant while changing desired attributes-is referred to as counterfactual reasoning. This simple idea is at the heart of the methods and contributions in this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 This figure illustrates how counterfactual reasoning is employed throughout the thesis to probe and enhance the capabilities of computer vision models. In Chapter 3, we introduce CAVLI, a method for quantifying the influence of visual concepts on model decisions via counterfactuals. Chapter 4 leverages counterfactuals for bias mitigation using ASACs, showing their efficacy in reducing unfair model behavior. In Chapter 5, we present TIBET, a framework for evaluating bias in text-to-image models through systematic counterfactual interventions. We further explore interactions between multiple bias dimensions in TTI models using our tool BiasConnect in Chapter 6, and propose a strategy for intersectional bias mitigationin generative models called InterMit in Chapter 7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p><note type="other">v</note><p>2.1 A brief timeline tracing the development of counterfactual reasoning. While the term "counterfactual" was popularized by David Lewis in 1973 <ref type="bibr" target="#b1">[2]</ref>, the underlying idea of reasoning about alternative possibilities has roots in both ancient Western and Eastern philosophical traditions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1</head><p>This chapter begins with a central question: When a black-box model predicts a label (e.g., "Cow"), how can we determine which meaningful, human-interpretable concepts influenced that decision? For instance, did the model recognize the image as a cow because of meaningful features like its distinctive body structure, or was the decision influenced by spurious cues such as the presence of grasslands in the background? To address this, we introduce a method that quantifies the extent to which a model relies on human-defined concepts-offering a more interpretable alternative to conventional pixel-level explanations. . . . . . . . . . . . . . . . . . . . . . . . . . . xi 3.2 Overview of our proposed approach, CAVLI, to estimate the dependence of a concept (e.g., "grassland") on a decision (e.g., "cow detection"). After decomposing the input image into superpixels, in Step 1 we find the regions of the image that have the highest association with the concept, defined by a set of images. In Step 2 we identify image regions with the highest involvement in the classification decision. Finally, we measure the overlap between the two in order to quantify the dependence. . . . . . . . 3. <ref type="bibr" target="#b2">3</ref> We use concept and decision heatmaps to analyze a classifier's decisions and their dependence on a specific concept, such as identifying whether an image contains a cow and what parts are the most useful in decision-making. Here we focus on grasslands, and the concept heatmap displays the areas of the image that the model associates most strongly with this concept. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1</head><p>What happens when we ask a counterfactual generation engine to modify an image along specific attributes? When asked to alter attributes like smile, eye color, or hair, standard counterfactual engines often introduce unintended changes-like clothing or background-revealing entangled attributes and stereotypes. This chapter attempts to address these problems by proposing a novel counterfactual generation scheme, called Attribute Specific Adversarial Counterfactuals (ASACs), and demonstrates that ASACs can be utilized for bias mitigation in computer vision classifiers. . . . . . . . 4.2 An example of gender-based counterfactuals generated by StyleGAN2 and our method for a smile classification task. In the first row, we attempt to generate more femininelooking versions of a given face, and in the second row, more masculine-looking versions. The StyleGAN2 outputs (right) exhibit biased correlations-femininity is often expressed through darker lipstick and exaggerated smiles, while masculinity is associated with older or wrinkled facial features. In contrast, our method (left) produces Adversarial Semantic Attribute Counterfactuals (ASACs) that preserve the original face's overall visual identity while varying gender-relevant features in a more controlled and semantically faithful manner. . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Bias Mitigation Strategy: Our proposed solution for mitigating biases in a model (e.g., smile classifier) M (θ,ρ) involves training sensitive attribute classifier C (θ,ϕ) (shown in the network architecture). We then follow a three-stage pipeline. <ref type="bibr" target="#b0">(1)</ref> We generate ASACs that are capable of deceiving C (θ,ϕ) . ( <ref type="formula">2</ref>) We define a curriculum assignment strategy that organizes these ASACs based on the degree to which they deceive the original model M (θ,ρ) . ( <ref type="formula">3</ref> In generative modeling, the dimensions along which bias manifests are highly dependent on the input prompt. For example, the relevant axes of bias for "a photo of a philosopher" may involve gender, age, or cultural presentation, while a prompt like "capybaras getting married in Italy" may invoke romanticization, geographic, or animal stereotypes. Unlike prior methods that evaluate models using a fixed set of demographic axes (e.g., gender or race), our approach dynamically surfaces contextrelevant bias dimensions per prompt-enabling more nuanced bias analysis in text-toimage generation systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 TIBET. Given an input prompt, we query an LLM (GPT-3) to identify axes of biases (Step 1), and generate counterfactual prompts for each axis of bias (Step 2). Here, we show a sample of three counterfactual prompts for the physical appearance bias, and two for the ableism bias. Next, we use a black-box TTI model (Stable Diffusion <ref type="bibr" target="#b2">[3]</ref>) to generate images for the initial prompt as well as each counterfactual for all axes of bias (Step 3). In this example, we leverage VQA-based concept extraction to obtain a list of concepts and their frequencies for each set of images, and compare the concepts of the initial set with concepts of each counterfactual to obtain CAS scores (Step 4). Finally, we compute MAD, a measure of how strong the bias is in the images generated by the initial prompt ( In this example, we see that humans rank racial bias to be more significant compared to gender bias, which is also observable in the images. We compare our VQA-based method to our CLIP-based method, and observe that the VQA-based method better aligns with human ranking. This is because, in most cases, biases are attributed to specific characteristics or parts of an image (which VQA helps us obtain), and not the semantic information of the image as a whole (which CLIP embeddings provide). This is in line with what we observe in User Study 2. . . . . . . . . . . . . . . . . . . . . . . . xiii 5.9 Bias identification and mitigation. We compute difference in CAS scores for male and female counterfactuals for 11 occupation prompts. (a) and (b) show male and female leaning professions using Stable Diffusion 1.5 and 2.1 respectively. (c) shows how the difference in CAS scores after using ITI-GEN to mitigate gender bias. . . . . 5.10 Bias Identification and Mitigation using TIBET and ITI-GEN -Ground Truth.</p><p>Here, we show ground truth gender differences in the initial set of images before bias mitigation, and after bias mitigation. The reduction in gender bias is in line with what we observe using CAS scores. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.1</head><p>This figure demonstrates how a minimal prompt change-from "An old man at a church" to "An Asian old man at a church"-can lead to significant differences in generated outputs. While the former yields grim, seated Caucasian men, the latter produces smiling, upright Asian men in brighter scenes. This shift illustrates the intersectional nature of generative model behavior, where the addition of a single identity attribute (ethnicity) modifies not just appearance but also emotion, posture, and context-highlighting how social dimensions interact rather than operate independently. 6.2 Exploring Intersectionality of Biases: Analysing the Top-K concepts shows that pharmacists in Europe and Asia are depicted with different gender distributions. . . . 6.3 Intersectional Observations using TIBET. We observe that images generated for a chef in Africa may be depicted outdoors (tree) unlike chefs in other regions of the world.102 6. <ref type="bibr" target="#b3">4</ref> An example for which BiasConnect estimates a negative impact of bias mitigation along one axis on another axis. For this query, increasing the gender diversity (Gen) skews age distribution (Age) for images of musicians generated by Flux-dev. . . . . . 6.5 An overview of BiasConnect. We use a counterfactual-based approach to measure pairwise causality between bias axes. For dependent axes, we measure the causal effect, estimating how bias mitigation on one axis impacts another. . . . . . . . . . . .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>John Jennifer</head><p>Applying for a Lab Manager position to top US universities , identical resumes differing only in the applicant's name ("John" vs. "Jennifer") were submitted for a lab manager position. It was observed that faculty were significantly more likely to favor the male applicant. This style of reasoning-holding all factors constant while changing desired attributes-is referred to as counterfactual reasoning. This simple idea is at the heart of the methods and contributions in this thesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>As computer vision models become increasingly integrated into domains such as healthcare, finance, education, and creative industries, concerns around their fairness, transparency, and accountability have grown more prominent. Among these, bias in classification and generative models has emerged as particularly pernicious-manifesting in both overt and subtle ways that reflect and amplify societal stereotypes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Models trained on large-scale web data often encode patterns that correlate sensitive attributes (e.g., gender, race) with undesired labels or aesthetics. For example, women may be overrepresented in submissive or decorative roles, while older individuals may be underrepresented altogether <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Despite growing awareness of the risks associated with these models, systematic tools for understanding, evaluating, and improving their safety remain limited..</p><p>One powerful tool for understanding and improving machine learning models is counterfactuals.</p><p>As illustrated in the example of the hiring bias study (Figure <ref type="figure" target="#fig_47">1</ref>.1), counterfactuals can be thought of as variations in the input that are made in order to understand how the output changes. In this study, identical résumés were submitted with only one change-the applicant's name, either "John" or "Jennifer." It was observed that faculty were more likely to favor "John" over "Jennifer." This simple change, while keeping everything else fixed, helped reveal how the outcome (hiring decisions) was influenced by gender. It is through such structured variations that we can draw inferences about the workings of a black-box model-referring to models like deep neural networks, where the mapping from input to output is not explicitly transparent or easily interpretable.</p><p>Counterfactual reasoning builds on this idea by formalizing it: it refers to the process of systematically altering parts of an input to an AI model and observing how the output changes. For instance, if we change one feature (e.g., the age or gender in a dataset) while holding all others constant and the model's prediction changes, we can deduce that the altered feature had a causal influence on the outcome. This kind of reasoning allows researchers and practitioners to analyze model behavior, uncover biases, and make informed improvements to ensure fairness, robustness, and interpretability.</p><p>At its core, counterfactual reasoning asks: What would the model do if a particular attribute were different, while everything else remained the same? This concept of counterfactual reasoning can take slightly different forms depending on the domain and task.</p><p>In computer vision-particularly for classification tasks-it often involves altering specific visual concepts in an image, such as changing the apparent age or gender of a person, while holding other attributes constant. This allows us to examine how the model's predictions shift in response to targeted changes, revealing underlying dependencies and potential biases <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>The same principle applies to structured decision-making systems. For instance, in a bank loan approval model, one might ask: Would this applicant have been approved if their gender were different, assuming income, credit score, and employment status remained the same? Such counterfactual queries help diagnose whether sensitive attributes like gender or race are exerting unjustified influence over outcomes.</p><p>In multimodal generative systems like Text-to-Image (TTI) models, counterfactual reasoning typically involves modifying textual prompts-for example, changing "an old man at a church" to "an</p><p>Asian old man at a church"-and observing how the generated images differ. These subtle changes can reveal hidden correlations between identity terms and visual outputs, surfacing stereotypes encoded in the model.</p><p>Although counterfactuals have gained prominence in recent years for diagnosing and improving computer vision models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, existing approaches remain limited in several important ways. Many methods generate counterfactuals that lack realism or causal grounding, making them less actionable for practitioners. Others focus narrowly on local, instance-specific explanations without offering broader insights into systemic issues such as dataset biases, fairness concerns, or the robustness of generative models. Moreover, evaluation criteria for counterfactuals in vision remain underdeveloped, often relying on ad hoc notions of plausibility rather than principled measures. This thesis addresses these gaps by developing methods that apply counterfactual reasoning not only to explain individual predictions, but also to evaluate and mitigate biases in both classification and generative models. In doing so, it moves counterfactual reasoning from being primarily a diagnostic tool to serving as a core approach for building fair, reliable, and interpretable vision systems.</p><p>This thesis explores several themes, all connected by the central idea of counterfactual reasoning.</p><p>We begin with a basic question: How can we tell if a model's decision-such as classifying an animal as a cow -depends on a human-defined concept like the presence of grasslands in the background?</p><p>While such dependencies are easier to measure at the pixel level <ref type="bibr" target="#b12">[13]</ref>, we propose a method called CAVLI that uses counterfactual reasoning to quantify how much a decision relies on high-level, human-defined concepts.</p><p>Next, we question the reliability of the counterfactuals themselves. What if the model or engine used to generate them is biased? Relying on flawed counterfactuals for explanation or mitigation can reinforce existing biases and lead to misleading conclusions <ref type="bibr" target="#b5">[6]</ref>. To address this, we introduce in-situ counterfactual generation, where modifications are made directly within the model, avoiding external generative systems. Specifically, we propose a method based on adversarial examples-called ASAC-that produces targeted, model-aware counterfactuals. We demonstrate that these examples contribute not only to training fairer models but also to improving their overall performance.</p><p>We also show that counterfactual reasoning can be used to evaluate biases in Text-to-Image models. We highlight that bias in generative models is not static. Rather, it is highly prompt-sensitive and dynamic-meaning that model outputs can vary widely based on how inputs are phrased or structured. This motivates the development of TIBET <ref type="bibr" target="#b13">[14]</ref>, a scalable framework for prompt-level counterfactual analysis in TTI models. TIBET uncovers how biases change across prompt variations, enabling both large-scale audits and deeper insights into prompt-induced bias shifts.</p><p>Another core motivation of this thesis is the recognition that different biases in vision models are rarely independent. Drawing from the framework of intersectionality <ref type="bibr" target="#b14">[15]</ref>, we argue that biases in generative models often emerge through complex interactions between social identities-such as gender, race, and age-and must be studied in combination rather than isolation. To this end, we develop tools like BiasConnect <ref type="bibr" target="#b15">[16]</ref>, which constructs pairwise causal graphs using counterfactual prompt interventions to measure how mitigating one type of bias (e.g., gender) affects another (e.g., age). This approach yields a structured diagnostic view of bias entanglement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finally, we extend this diagnosis into actionable mitigation through our proposed algorithm</head><p>InterMit <ref type="bibr" target="#b16">[17]</ref>, which uses user-defined fairness priorities and intersectional sensitivity scores to guide efficient bias reduction in generative outputs. Unlike prior methods, InterMit does not require retraining and adapts mitigation steps based on both user goals and observed causal effects.</p><p>In summary, this thesis makes a case for using counterfactual reasoning not only as a diagnostic tool but as a core methodology to rethink how we evaluate and mitigate bias in modern AI systems.</p><p>By applying counterfactual logic to both vision classifiers and generative models, we develop tools and algorithms that are conceptually unified, practically useful, and grounded in causal principles. A roadmap of these contributions is provided in Figure <ref type="figure" target="#fig_47">1</ref>.2, and a chapter-wise breakdown follows in Section 1.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">What is a Counterfactual?</head><p>Counterfactuals. We define a counterfactual as an alternate version of an input to an AI model that differs from the original in a small set of semantically meaningful human defined concepts or data points, while holding all other attributes fixed. This concept-level definition generalizes across modalities. In computer vision, counterfactuals involve modifying high-level visual attributes-such as changing a person's age, gender, or clothing in an image-while preserving the overall scene <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. In natural language processing (NLP), counterfactuals can be constructed by altering specific words or phrases in a sentence that correspond to sensitive or influential concepts. For instance, changing "He is a nurse" to "She is a nurse" helps assess how language models associate gender with profession. Similarly, in structured decision-making tasks like credit scoring or loan approval, counterfactuals involve tweaking categorical inputs-e.g., changing an applicant's gender from "male" to "female" while keeping income, credit score, and employment status fixed-to identify whether sensitive features unduly influence predictions <ref type="bibr" target="#b18">[19]</ref>. Across all these domains, counterfactuals serve as targeted probes for uncovering hidden biases, testing model robustness, and generating interpretable explanations.</p><p>Counterfactual Reasoning. Counterfactual reasoning refers to the systematic use of counterfactuals to isolate, test, and understand the causal impact of specific concepts on model behavior.</p><p>It provides a powerful framework for evaluating fairness, generating explanations, and guiding mitigation strategies across both discriminative and generative tasks. By comparing model outputs before and after intervening on a target concept-while holding all else constant-researchers can attribute model decisions to specific concepts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and uncover implicit biases embedded in data or learned representations <ref type="bibr" target="#b13">[14]</ref>. In this thesis, counterfactual reasoning underpins multiple contributions: from attributing decisions to visual concepts (e.g., via CAVLI) to evaluating bias in TTI models (e.g., using TIBET), and uncovering interactions between intersecting attributes (e.g., through BiasConnect). This reasoning framework not only helps expose undesirable behaviors but also lays the foundation for systematic bias mitigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">A Brief History of Counterfactuals in AI and Causal Inference</head><p>The concept of counterfactuals has longstanding origins in philosophy, social science, and causal inference. In philosophical logic, David Lewis formalized counterfactual conditionals in his seminal work "Counterfactuals" <ref type="bibr" target="#b1">[2]</ref>. He proposed a possible word semantics to evaluate statements like "If A had happened, B would have followed," where the truth of such a counterfactual depends on its closeness to the actual world.</p><p>In statistics and the social sciences, the Potential Outcomes Framework introduced by Donald Rubin <ref type="bibr" target="#b19">[20]</ref> laid a probabilistic foundation for counterfactual inference, imagining that each unit (e.g.,</p><p>an individual) has multiple potential outcomes under different treatment conditions. This framework underpins modern causal inference methods such as randomized control trials and treatment effect estimation.</p><p>The formal introduction of counterfactual reasoning into computer science was pioneered by Judea Pearl, who developed Structural Causal Models (SCMs) <ref type="bibr" target="#b20">[21]</ref>. SCMs provide a graphical and algebraic language for causal reasoning. Pearl also introduced the Ladder of Causality, distinguishing three levels of reasoning:</p><p>1. Association: Observing statistical relationships (e.g., "People who take aspirin tend to recover").</p><p>2. Intervention: Modeling outcomes of deliberate actions (e.g., "What happens if we give aspirin?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Counterfactuals:</head><p>Hypothetical alternate outcomes (e.g., "Would the patient have recovered if they had not taken aspirin?").</p><p>The third level-counterfactuals-is the most expressive and enables rich forms of explanation and introspection.</p><p>The integration of counterfactual reasoning into machine learning gained traction in the late 2010s, particularly in the domains of explainability and algorithmic fairness. Wachter et al. <ref type="bibr" target="#b18">[19]</ref> and Mothilal et al. <ref type="bibr" target="#b21">[22]</ref> proposed counterfactual explanations as a method to interpret decisions of black-box models by identifying the smallest change in input that alters the output.</p><p>In the field of computer vision, early forms of counterfactuals appeared in the context of adversarial examples, but the concept evolved into more semantically grounded image edits that aim to test model behavior in interpretable ways. Denton et al. <ref type="bibr" target="#b5">[6]</ref> introduced image counterfactuals using attribute-guided editing, while Wu et al. <ref type="bibr" target="#b22">[23]</ref> and Abid et al. <ref type="bibr" target="#b9">[10]</ref> extended counterfactual techniques to language and vision-language models.</p><p>This historical progression underscores how counterfactual reasoning has grown from a philosophical abstraction to a central tool in modern AI research, enabling deeper understanding, fairness auditing, and robust model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">The Utility of Counterfactuals in Computer Vision</head><p>Counterfactuals have emerged as a versatile and powerful tool in computer vision, enabling a deeper understanding of how vision models make decisions and how they behave under changes to their input. Their utility spans multiple dimensions:</p><p>(a) Interpretability. Counterfactuals are intuitive and human-understandable, often answering the question: "What would need to change in the image for the model to make a different decision?" This is particularly valuable in the context of black-box models, where internal decision mechanisms are inaccessible <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. In vision tasks, counterfactuals can be generated by modifying image regions to identify semantically meaningful features responsible for a prediction.</p><p>(b) Fairness and Bias Auditing. Overall, counterfactual reasoning provides a powerful lens for probing vision systems, not only by enabling better explanations, but also by guiding robust model design and ethical AI development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Thesis Contributions</head><p>We make the following key contributions in this thesis, unified by the use of counterfactual reasoning as a foundational lens to understand, evaluate, and mitigate bias in vision and generative models:</p><p>• Concept Attribution in Vision Models (Chapter 3): We propose CAVLI, a method that leverages image counterfactuals to quantify how classifier decisions (e.g., labeling an image as a zebra) depend on specific visual concepts (e.g., stripes). By measuring the spatial overlap between pixels involved in decision-making and those tied to a concept, CAVLI provides interpretable insights into the role of semantic features in model predictions.</p><p>• Counterfactual Generation for Bias Mitigation (Chapter 4): We identify a key limitation of using generative models for counterfactual image synthesis in bias mitigation -namely, the introduction of spurious attribute correlations (e.g., associating gender with makeup). To address this, we propose a novel adversarial counterfactual strategy combined with a curriculum-based fine-tuning approach that mitigates bias while preserving semantic realism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction</head><p>This chapter provides a review of the existing literature on counterfactual reasoning, fairness, and explainability in machine learning, with a particular focus on computer vision and generative models.</p><p>Since a large part of this thesis focuses on fairness and explainability, this review aims to give the necessary background to understand how these topics have developed and where current challenges remain.</p><p>The chapter begins by outlining the history of counterfactual thinking, tracing its roots from early philosophical ideas to its formal role in modern causal inference. It then discusses how counterfactuals are used in machine learning to improve understanding of model behavior, evaluate fairness, generate recourse, test robustness, and conduct causal analysis. The review highlights how these methods differ depending on the data type and how counterfactuals are constructed.</p><p>The chapter also surveys key definitions of fairness and examines how bias has been studied in both natural language processing and computer vision. Particular attention is given to work on intersectional biases, which considers how different social attributes like gender, race, and age combine in complex ways.</p><p>Finally, the chapter reviews explainability methods that aim to make model decisions more transparent, through techniques such as feature attribution, concept-based reasoning, and counterfactual examples. By reviewing this literature, the chapter sets the stage for the thesis's contributions. In this thesis, counterfactual reasoning has been explored as a tool to address questions of fairness and explainability. This review identifies where further work is needed, especially in addressing biases in generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A brief history of counterfactuals</head><p>Counterfactuals-and counterfactual thinking more broadly-have evolved over centuries across philosophy, logic, and psychology, long before being formally defined. Though the term itself is modern, the underlying mode of reasoning has deep roots in classical systems of thought. The early history of counterfactual thinking can be traced to ancient traditions such as Greek and Indian philosophy, where reasoning about alternatives played a central role in debates about knowledge, causality, and inference (see Figure <ref type="figure">2</ref>.1).</p><p>In Greek philosophy, thinkers like Aristotle and Plato examined subjunctive suppositions-claims about what could have happened but did not-to explore the logic and metaphysics of possibility <ref type="bibr" target="#b26">[27]</ref>. Their inquiries laid the groundwork for later developments in modal and conditional reasoning.</p><p>Philosophers from the Eleatic school, including Parmenides and Melissus, also used hypothetical premises in their deductive arguments to reflect on necessity and the nature of being <ref type="bibr" target="#b27">[28]</ref>. Similar forms of reasoning appear in Indian philosophy, where hypothetical and suppositional methods were central to classical logic <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. The Nyāya school developed tarka, a structured technique for testing propositions by considering imagined alternatives and their consequences. In parallel, Buddhist thinkers such as Nāgārjuna employed prasaṅga arguments-reductio-style reasoning that derived contradictions from assumed premises-to challenge views on reality and causation <ref type="bibr" target="#b30">[31]</ref>.</p><p>These frameworks evolved independently from Greek thought, yet reflected a similarly rigorous engagement with counterfactual logic.</p><p>In early modern philosophy, David Hume offered a critical turning point. Although known for his regularity theory of causation, Hume's claim that "if the cause had not been, the effect never had existed" <ref type="bibr" target="#b31">[32]</ref> prefigures counterfactual accounts of dependence. While rejecting metaphysical necessity, Hume relied on imagined alternatives to explain causal inference. His critics and successors extended this tradition. Thomas Reid emphasized causal necessity and direct perception, challenging Hume's skepticism <ref type="bibr" target="#b32">[33]</ref>, while Immanuel Kant, in the Critique of Pure Reason, proposed that causal structure is imposed by the mind, introducing a modal basis for reasoning about cause and effect <ref type="bibr" target="#b33">[34]</ref>.</p><p>In the 19th century, John Stuart Mill formalized a method resembling counterfactual testing-the Method of Difference-arguing that if removing a condition eliminates an effect, a causal connection is likely <ref type="bibr" target="#b34">[35]</ref>. Alexander Bain further examined the psychological foundations of such reasoning <ref type="bibr" target="#b35">[36]</ref>.</p><p>These philosophical developments were formalized in analytic philosophy by David Lewis, who is widely credited with defining the modern semantics of counterfactuals. In Counterfactuals <ref type="bibr" target="#b1">[2]</ref>,</p><p>Lewis introduced a possible-worlds framework in which a counterfactual is true if its consequent holds in the closest possible world(s) where the antecedent is true. This model gave counterfactuals a precise logical structure and made them central to debates in metaphysics, decision theory, and language. By embedding counterfactuals within modal logic, Lewis helped transition them from intuitive thought experiments to formal tools of analysis.</p><p>Building on this foundation, counterfactuals became integral to modern theories of causality in statistics and computer science. Donald Rubin's potential outcomes framework-known as the Rubin Causal Model (RCM)-defines causal effects as differences between outcomes under different treatment assignments for the same unit <ref type="bibr" target="#b19">[20]</ref>. Judea Pearl advanced causal inference by introducing</p><p>Structural Causal Models (SCMs), which unify counterfactual reasoning, graphical models, and algorithmic tools for causal analysis <ref type="bibr" target="#b36">[37]</ref>. SCMs use directed acyclic graphs (DAGs) to represent assumptions about causal relationships and are paired with structural equations that encode how variables influence one another. Within this framework, causal and counterfactual queries can be expressed formally and computed algorithmically, provided the causal structure is identified.</p><p>A key conceptual innovation in Pearl's framework is the Ladder of Causation, which organizes causal reasoning into three hierarchical levels:</p><p>1. Association (Level 1): This level describes patterns in data that we observe. For example, we might look at the probability of a person recovering (Y = y = recovered) given that they took a drug (X = x = tookdrug). This is written as</p><formula xml:id="formula_0">P (Y = y | X = x).</formula><p>• X: the observed input or condition, like "took the drug"</p><p>• Y : the observed outcome, like "recovered" This tells us whether two things are related (i.e., correlated), but not whether one caused the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Intervention (Level 2):</head><p>This level is about understanding causal effects-what happens when we actively intervene and change something. For instance, we might ask: "What's the chance of recovery if we make someone take the drug?" This is written as P (Y | do(X)).</p><p>• The do(X) notation means we are forcing the condition X to happen (e.g., giving the drug), rather than just observing it.</p><p>3. Counterfactuals (Level 3): This level goes one step further and asks: "What would have happened if things had been different?" For example, suppose a person took the drug (X = x)</p><p>and recovered (Y = y). A counterfactual question would be: "Would they still have recovered if they had not taken the drug?" This is written as:</p><formula xml:id="formula_1">P (Y |X = x ′ )</formula><p>• Y : the outcome we care about (e.g., recovery)</p><p>• X: the input condition (e.g., took the drug or not)</p><p>• x ′ : a different, hypothetical condition (e.g., did not take the drug) In machine learning and AI, counterfactual reasoning is increasingly used for tasks such as explaining model decisions, fairness, and causal modeling. For instance, counterfactual explanations can reveal which minimal changes to inputs would alter a model's decision <ref type="bibr" target="#b18">[19]</ref>, while fairness frameworks assess whether outcomes shift unjustifiably across protected attributes <ref type="bibr" target="#b23">[24]</ref>. These applications often build on formal foundations from causal inference <ref type="bibr" target="#b36">[37]</ref>, and their broader role will be discussed in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Counterfactuals: Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Counterfactuals in the Rubin Causal Model</head><p>In the Rubin Causal Model (RCM), counterfactuals are represented as potential outcomes for each unit under different treatment conditions. Let Y i (1) denote the potential outcome for unit i if it were assigned to the treatment condition, and Y i (0) denote the potential outcome if assigned to the control condition.</p><p>For any given unit i, only one of these outcomes can be observed in practice, depending on the treatment assignment:</p><formula xml:id="formula_2">Y obs i =        Y i (1), if T i = 1 Y i (0), if T i = 0</formula><p>The unobserved outcome is referred to as the counterfactual.</p><p>The individual causal effect for unit i is defined as the difference between the potential outcomes under treatment and control:</p><formula xml:id="formula_3">τ i = Y i (1) -Y i (0)</formula><p>However, because we cannot observe both Y i (1) and Y i (0) for the same unit, individual causal effects are generally not identifiable. Instead, the focus is often on estimating the average treatment effect (ATE):</p><formula xml:id="formula_4">ATE = E[Y (1)] -E[Y (0)]</formula><p>In this framework, a counterfactual is formally the potential outcome under a treatment condition that did not occur. </p><formula xml:id="formula_5">M = (U, V, F, P (U))</formula><p>where:</p><p>• U are exogenous (background) variables,</p><p>• V are endogenous (observed) variables,</p><p>• F is a set of structural equations {V i = f i (pa i , U i )}, where pa i ⊆ V ∪ U are the parents of V i ,</p><p>• P (U) is a probability distribution over U.</p><p>To evaluate a counterfactual expression such as "Y would be y if X were x, given that we observed X = x ′ and Y = y ′ ," Pearl proposes a three-step procedure:</p><p>1. Abduction: Given observations X = x ′ , Y = y ′ , update the model by inferring the posterior distribution P (U | X = x ′ , Y = y ′ ) over the exogenous variables.</p><p>2. Action: Modify the model by performing an intervention do(X = x), replacing the structural equation for X with the constant X = x.</p><p>3. Prediction: Use the modified model and the inferred U to compute the resulting distribution over Y . The counterfactual query is evaluated as:</p><formula xml:id="formula_6">P (Y x = y | X = x ′ , Y = y ′ ) = u P (Y x = y | u)P (u | X = x ′ , Y = y ′ )</formula><p>This formalization allows counterfactuals to be treated as computable objects under well-defined assumptions about the data-generating process, providing a foundation for retrospective causal analysis, policy evaluation, and algorithmic accountability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adavantages and Challenges of Counterfactual Reasoning</head><p>Counterfactual reasoning provides a rigorous way of asking "what if" questions, offering both conceptual advantages and practical limitations. This section outlines its major benefits and the challenges that arise when applying it to real-world domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Advantages</head><p>A central strength of counterfactual reasoning is its ability to move beyond statistical association and explicitly model causation. Whereas correlation describes patterns of co-occurrence, causation asks whether manipulating one variable would bring about a change in another. This distinction is particularly valuable in domains dominated by spurious associations, such as high-dimensional observational datasets.</p><p>Counterfactuals also enable reasoning at the level of individuals rather than populations. While average causal effects summarize treatment impacts across groups, counterfactuals ask what would have happened to a specific unit under an alternative condition. This unit-level perspective is crucial for generating recourse in machine learning <ref type="bibr" target="#b18">[19]</ref>, auditing fairness for individuals and subgroups <ref type="bibr" target="#b23">[24]</ref>, and designing personalized interventions in medicine and policy.</p><p>Finally, counterfactual reasoning is inherently action-oriented. By linking interventions to outcomes, it provides a framework for accountability and decision support. In applied machine learning, this translates into actionable recommendations, fairness interventions, and interpretability tools that highlight which features are causally relevant to a model's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Challenges</head><p>Despite these benefits, several fundamental challenges limit the practical use of counterfactual frameworks. Estimating direct and indirect (mediated) effects requires strong assumptions about the causal graph and the absence of hidden confounding . In real-world, high-dimensional domains, specifying all relevant mediators is rarely feasible, making such estimates fragile and highly sensitive to model misspecification.</p><p>Another limitation lies in the treatment of exogenous variables. Both potential outcomes and structural causal models assume that background factors can be enumerated or probabilistically modeled. In practice, however, many relevant influences remain unobserved. These hidden confounders bias both interventional and counterfactual estimates, constraining the reliability of inferences. Even when sensitivity analyses or partial identification techniques are employed, conclusions remain bounded by untestable assumptions.</p><p>Finally, counterfactual reasoning is highly model-dependent. Small errors in specifying structural equations or independence assumptions can lead to drastically different conclusions, raising concerns about robustness, reproducibility, and interpretability.</p><p>In summary, counterfactual reasoning combines unique strengths-clarifying causation, enabling individual-level reasoning, and supporting actionability-with significant challenges, particularly in complex, high-dimensional domains where hidden confounding and model misspecification remain persistent obstacles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">A Taxonomy of Counterfactuals in Machine Learning</head><p>This section presents a taxonomy of counterfactuals in machine learning based on three orthogonal dimensions. The first dimension, use-case, categorizes counterfactuals according to their functional role-such as explanation, fairness auditing, recourse generation, or causal inference. The second dimension, modality, distinguishes counterfactuals by the type of input data they operate on, including text, images, tabular data, or multimodal combinations. The third dimension, type of construction, captures how counterfactuals are generated-ranging from input-level perturbations to concept-level modifications, and from adversarial to causally grounded interventions. This structured classification clarifies the diverse forms counterfactuals can take and provides a foundation for analyzing their design and application across tasks and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">By Use-Case</head><p>Counterfactual methods in machine learning have been deployed for various use-cases ranging from interpretability and fairness to causal inference and robustness. This section categorizes the literature by primary application domains and highlights key works in each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.1">Interpretability</head><p>Counterfactual explanations are a widely used approach for understanding and interpreting the decisions of complex machine learning models. They aim to answer the question: "What minimal change to the input features would change the model's prediction?". By systematically changing features (whether input features or human defined concepts), counterfactual reasoning provides a framework for understanding the dependece of model output on these features. This use-case emphasizes transparency and user trust, especially in high-stakes domains like finance, healthcare, and law.</p><p>One of the earliest applications of counterfactuals for interpreting machine learning models was by Wachter et al. <ref type="bibr" target="#b18">[19]</ref>, who proposed an optimization-based method for generating counterfactual explanations without requiring access to model internals. Their work emphasized the legal and philosophical relevance of such explanations, particularly in the context of GDPR (European Union's data privacy law that gives individuals control over their personal data and enforces strict rules on how organizations collect, use, and protect it) and individual rights. Building on this foundation, subsequent works have extended counterfactual reasoning in various directions-seeking not only faithful explanations, but also those that are diverse <ref type="bibr" target="#b21">[22]</ref>, logically grounded <ref type="bibr" target="#b37">[38]</ref>, multi-objective <ref type="bibr" target="#b38">[39]</ref>, actionable <ref type="bibr" target="#b39">[40]</ref>, and causally consistent <ref type="bibr" target="#b40">[41]</ref>. Others have focused on enhancing human interpretability <ref type="bibr" target="#b41">[42]</ref>, addressing hidden confounding <ref type="bibr" target="#b42">[43]</ref>, offering diagnostic black-box tools <ref type="bibr" target="#b43">[44]</ref>,</p><p>or grounding explanation quality in cognitive principles <ref type="bibr" target="#b44">[45]</ref>. Together, these efforts reflect a growing recognition of counterfactuals as a versatile and strong lens for understanding machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.2">Fairness and Bias Auditing</head><p>Counterfactuals are a powerful tool for assessing algorithmic fairness. The core idea is to evaluate whether a model's prediction would remain consistent had a sensitive attribute (e.g., race, gender) been different, while holding all else equal. This supports definitions of fairness grounded in causal invariance or conditional independence.</p><p>A core central theme to this idea is the definition of counterfactual fairness, introduced by Kusner et al. <ref type="bibr" target="#b23">[24]</ref>. Counterfactual fairness states that a decision is fair towards an individual if the decision would have remained the same in a counterfactual world where the individual belonged to a different demographic group (e.g., race or gender), while keeping all other non-protected attributes constant. Since then, this idea has evolved through a variety of extensions. Scholars have introduced refinements that isolate unfair influence along specific causal pathways <ref type="bibr" target="#b45">[46]</ref>, integrated counterfactual reasoning into the training of fair representations through adversarial learning <ref type="bibr" target="#b46">[47]</ref>,</p><p>and applied the framework to domain-specific settings like text classification and NLP auditing <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. Other efforts have explored semantic perturbation techniques to probe visual model fairness <ref type="bibr" target="#b49">[50]</ref>, explicitly constructed image-based counterfactuals to reveal bias in generative models <ref type="bibr" target="#b50">[51]</ref>,</p><p>and used simulation-based evaluations to analyze multimodal systems <ref type="bibr" target="#b51">[52]</ref>. Alongside this, surveys and methodological innovations have broadened the use of counterfactuals for fairness auditing and data augmentation during model training <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Together, these contributions demonstrate how counterfactual reasoning has become a unifying thread across modalities and fairness paradigms, enabling both diagnostic and corrective interventions in machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.3">Algorithmic Recourse</head><p>Building on the foundation of counterfactual reasoning, recourse-oriented counterfactuals aim to provide individuals with actionable guidance for changing unfavorable model outcomes. Research in this area has explored several key directions. Some works focus on generating sparse and feasible interventions using optimization-based methods <ref type="bibr" target="#b54">[55]</ref>, while others emphasize causal consistency to ensure recommended changes are not only effective but also realistic within the constraints of the real world <ref type="bibr" target="#b40">[41]</ref>. A separate line of work frames recourse as a problem of identifying plausible transitions along data manifolds, capturing feasibility in high-dimensional settings <ref type="bibr" target="#b39">[40]</ref>. Together, these efforts treat counterfactuals not just as explanations, but as practical tools that can help users understand and potentially improve their outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.4">Robustness and Adversarial Testing</head><p>In robustness testing, counterfactuals are widely used to probe a model's sensitivity to small or structured input changes. These perturbations can expose vulnerabilities in how models generalize, even when the changes appear imperceptible to humans <ref type="bibr" target="#b55">[56]</ref>. Over time, this idea has been extended</p><p>to test robustness under broader distributional shifts, using stress tests and systematic evaluations across a range of inputs <ref type="bibr" target="#b56">[57]</ref>. In vision and language, counterfactual examples have been used to highlight misclassifications and reveal brittle behavior in models <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>. Increasingly, counterfactuals are also integrated with interpretability frameworks to offer more robust and transparent model explanations <ref type="bibr" target="#b59">[60]</ref>. These trends reflect a shift from using counterfactuals solely for adversarial attacks to employing them as diagnostic tools for improving model reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.5">Causal Inference and Treatment Effect Estimation</head><p>Counterfactuals are fundamental to causal inference, where they enable reasoning about hypothetical interventions and treatment effects. The potential outcomes framework defines causal effects as differences between actual and counterfactual outcomes <ref type="bibr" target="#b19">[20]</ref>, while structural causal models formalize this reasoning through graphical models and do-calculus <ref type="bibr" target="#b36">[37]</ref>. Recent advances leverage machine learning to improve counterfactual estimation under challenges like covariate shift. Approaches include representation learning for balanced comparisons <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, latent variable models for handling hidden confounders <ref type="bibr" target="#b62">[63]</ref>, and generative models for estimating individual treatment effects <ref type="bibr" target="#b63">[64]</ref>. These developments reflect a shift toward scalable, model-based causal inference rooted in counterfactual logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">By Input Modality</head><p>The form and challenges of counterfactual reasoning vary significantly across input modalities. In this section, we categorize counterfactual methods by the type of data they operate on: text, vision, and tabular. Each modality imposes different constraints on how counterfactuals can be generated, evaluated, and interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.1">Text</head><p>In language tasks, counterfactual generation is constrained by grammar, coherence, and semantic consitency. Even minor edits can drastically alter meaning, making fluency-preserving perturbations non-trivial. Methods have focused on identity-sensitive edits <ref type="bibr" target="#b47">[48]</ref>, morphological control <ref type="bibr" target="#b58">[59]</ref>,</p><p>and structured interventions for fairness and auditing <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b64">65]</ref>. More recent approaches aim to improve interpretability in sequence models <ref type="bibr" target="#b65">[66]</ref>, with surveys reviewing generation and evaluation frameworks tailored to text <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.2">Vision</head><p>In vision, counterfactuals require semantically meaningful changes that remain photorealistic and causally informative. This often involves disentangling latent features like gender or emotion from pixel-level noise. Approaches range from attribute editing in face images <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b67">68]</ref> to counterfactual scene composition for VQA <ref type="bibr" target="#b17">[18]</ref> and concept activation maps for interpretability <ref type="bibr" target="#b68">[69]</ref>. Additional methods probe bias in multimodal models <ref type="bibr" target="#b51">[52]</ref>, robustness to visual perturbations <ref type="bibr" target="#b57">[58]</ref>, and interventions in generative latent spaces <ref type="bibr" target="#b69">[70]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.3">Tabular</head><p>Tabular data offers the most direct setting for counterfactuals due to its structured and often lowdimensional nature. Optimization-based methods dominate this space, focusing on feasibility, diversity, and actionable recourse <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55]</ref>. Others integrate causal constraints <ref type="bibr" target="#b40">[41]</ref>, manifoldaware interpolation <ref type="bibr" target="#b39">[40]</ref>, or human-aligned filtering mechanisms <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">By Type of Construction</head><p>Counterfactuals differ not only in what they are used for or what data they operate on, but also in how they are constructed. This section distinguishes counterfactuals based on their construction strategy-ranging from low-level input perturbations to high-level semantic edits, and from causally grounded methods to adversarial variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.1">Input-Level Counterfactuals</head><p>Input-level counterfactuals involve direct changes to the input features, often constrained to minimal perturbations. These are among the earliest and most practical forms, particularly in tabular and text domains.</p><p>Wachter et al. <ref type="bibr" target="#b18">[19]</ref> proposed a gradient-based method to find minimally different inputs that flip model predictions. Dandl et al. <ref type="bibr" target="#b38">[39]</ref> formulated counterfactual generation as a multi-objective optimization problem over proximity and plausibility. Mothilal et al. <ref type="bibr" target="#b21">[22]</ref> emphasized diversity in generated counterfactuals for tabular classifiers. Russell <ref type="bibr" target="#b37">[38]</ref> used constraint solvers to generate logical and sparse counterfactuals. Poyiadzi et al. <ref type="bibr" target="#b39">[40]</ref> introduced geodesic counterfactuals that traverse realistic paths in the data manifold. Lucic et al. <ref type="bibr" target="#b41">[42]</ref> filtered input-level counterfactuals using human-aligned metrics. Dhamdhere et al. <ref type="bibr" target="#b70">[71]</ref> applied input perturbations for attribution analysis in saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.2">Concept-Level Counterfactuals</head><p>Concept-level counterfactuals manipulate high-level semantic factors (e.g., gender, pose, sentiment) that underlie the input. This often requires disentangled representations or causal structure.</p><p>Goyal et al. <ref type="bibr" target="#b17">[18]</ref> generated concept-level visual counterfactuals in VQA using scene editing.</p><p>Karimi et al. <ref type="bibr" target="#b40">[41]</ref> imposed causal constraints to ensure edits preserve structural dependencies.</p><p>Upadhyay et al. <ref type="bibr" target="#b71">[72]</ref> applied interventions to disentangled text representations. Balagopalan et al.</p><p>[51] developed image counterfactuals that modify specific concepts (e.g., skin tone). Mahajan et al. <ref type="bibr" target="#b72">[73]</ref> created counterfactuals that preserve task-specific concepts in vision tasks. Gauthier et al.</p><p>[74] applied concept-level edits for training robust language classifiers. Kim et al. <ref type="bibr" target="#b74">[75]</ref> proposed counterfactuals based on shifting latent attributes in multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.3">Adversarial vs. Causal Counterfactuals</head><p>Adversarial counterfactuals aim to test model robustness by finding near-indistinguishable inputs that cause misclassification. In contrast, causal counterfactuals focus on hypothetical scenarios rooted in structural causal models.</p><p>Goodfellow et al. <ref type="bibr" target="#b55">[56]</ref> introduced adversarial examples via gradient-based attacks. Ribeiro et al.</p><p>[76] proposed perturbation-based local explanations that test model stability. Hendricks et al. <ref type="bibr" target="#b76">[77]</ref> evaluated adversarial counterfactuals in image captioning. Pearl <ref type="bibr" target="#b36">[37]</ref> formalized causal counterfactuals using structural equations. Kusner et al. <ref type="bibr" target="#b23">[24]</ref> defined counterfactual fairness via causal graphs.</p><p>Chiappa <ref type="bibr" target="#b45">[46]</ref> developed path-specific counterfactuals. Karimi et al. <ref type="bibr" target="#b77">[78]</ref> compared adversarial and causal counterfactuals for recourse. Mahajan et al. <ref type="bibr" target="#b78">[79]</ref> applied structural interventions to generate interpretable counterfactuals across modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Fairness in Machine Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">What Is Fairness?</head><p>Fairness in machine learning broadly refers to the principle that algorithmic systems should not produce systematically discriminatory outcomes against individuals or groups based on socially salient attributes such as gender, race, caste, or socioeconomic status. The formalization of fairness has led to a proliferation of definitions, each capturing a different normative notion of equity. Three primary families of definitions dominate the literature:</p><p>• Group Fairness focuses on ensuring parity of outcomes across different groups defined by protected attributes. Common criteria include:</p><p>-Statistical Parity (Demographic Parity): The predicted outcome should be independent of the sensitive attribute <ref type="bibr" target="#b79">[80]</ref>.</p><p>-Equalized Odds: The model's true and false positive rates should be equal across groups <ref type="bibr" target="#b80">[81]</ref>.</p><p>-Equal Opportunity: A relaxation of equalized odds, requiring only equal true positive rates <ref type="bibr" target="#b80">[81]</ref>.</p><p>• Individual Fairness posits that similar individuals should receive similar outcomes, operationalized through a task-specific similarity metric <ref type="bibr" target="#b81">[82]</ref>.</p><p>• Counterfactual Fairness requires that a model's decision remain unchanged in a counterfactual world where the individual's protected attribute had been different <ref type="bibr" target="#b23">[24]</ref>.</p><p>These definitions are often mutually exclusive <ref type="bibr" target="#b82">[83]</ref>, necessitating careful trade-offs in deployment contexts. Recent work explores relaxations and hybrid notions to better reflect practical constraints <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b84">85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Fairness in Natural Language Processing</head><p>Natural Language Processing (NLP) models are known to inherit-and often exacerbate-social biases embedded in the large-scale corpora they are trained on. Multiple forms of such bias have been documented. For instance, coreference resolution systems tend to resolve gender-neutral roles like "the doctor" disproportionately as male <ref type="bibr" target="#b85">[86]</ref>. Word embeddings such as Word2Vec encode stereotypical analogies (e.g., "man:computer as woman:homemaker"), which can propagate harmful associations in downstream tasks <ref type="bibr" target="#b86">[87]</ref>. Language generation models have been shown to produce toxic or biased continuations when prompted with identity-related terms <ref type="bibr" target="#b87">[88]</ref>. Sentiment analysis systems, too, can demonstrate disparate performance across linguistic varieties like African-American Vernacular English (AAVE), reflecting socio-linguistic biases in training data <ref type="bibr" target="#b88">[89]</ref>.</p><p>To address these challenges, a range of intervention strategies have been proposed. One early line of work focused on debiasing word embeddings by removing gender-associated subspaces <ref type="bibr" target="#b86">[87]</ref>.</p><p>Other methods employ counterfactual data augmentation (CDA), generating paired sentences by swapping identity terms to ensure balanced model behavior across demographics <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b89">90]</ref>. More recently, causal probing techniques have been developed to measure and suppress the influence of sensitive attributes within model internals, either by adversarially minimizing their predictive power or by identifying and erasing specific latent features <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b91">92]</ref>. These strategies reflect a growing effort to build fairer and more socially aware NLP systems.</p><p>Recent trends in large language model fairness focus on aligning generation with human values, auditing RLHF training pipelines <ref type="bibr" target="#b92">[93]</ref>, and building explainable fairness probes <ref type="bibr" target="#b93">[94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Fairness in Computer Vision</head><p>Fairness in computer vision has become an increasingly important topic due to the deployment of vision systems in socially sensitive contexts such as surveillance, hiring, medical imaging, and content moderation. Unlike structured tabular or textual data, visual data is high-dimensional, opaque, and difficult to semantically disentangle, making bias detection and mitigation particularly challenging. Moreover, societal power structures are often reflected and amplified in visual data, especially when datasets and benchmarks are scraped from the internet without auditing or curation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b94">95]</ref>.</p><p>Facial Recognition Biases. The most cited work in this space is the "Gender Shades" study by Buolamwini and Gebru, which demonstrated stark disparities in commercial gender classification systems. Accuracy was as high as 99% for lighter-skinned males but dropped below 65% for darkerskinned females <ref type="bibr" target="#b95">[96]</ref>. This exposed the inadequacies of training data and systemic blind spots in commercial model pipelines. Subsequent work by Raji et al. <ref type="bibr" target="#b96">[97]</ref> further showed how public audits of model disparities led to measurable improvements, emphasizing the importance of transparency and third-party accountability.</p><p>Object Detection and Classification Biases. Wang et al. <ref type="bibr" target="#b6">[7]</ref> demonstrated that object recognition systems trained on datasets like COCO <ref type="bibr" target="#b97">[98]</ref> and Open Images <ref type="bibr" target="#b98">[99]</ref> tend to entrench gender stereotypes-for instance, associating women with kitchen scenes and men with sports or machinery. Even when gender is not explicitly labeled, these associations emerge implicitly through correlated visual context. Such biases propagate downstream in captioning, summarization, and decision-support tasks.</p><p>Biases in Generative Models. Fairness challenges are even more pronounced in generative systems.</p><p>Denton et al. <ref type="bibr" target="#b5">[6]</ref> and Birhane and Prabhu <ref type="bibr" target="#b94">[95]</ref> highlighted how training data scraped from uncurated internet sources reproduces misogynistic, racist, and class-based stereotypes in image generation.</p><p>For example, prompts like "CEO" might yield mostly white male images, while prompts like "nurse" might yield primarily women of color. Zhao et al. <ref type="bibr" target="#b99">[100]</ref> analyzed such systems to show how generative models reflect societal stereotypes in a feedback loop, raising complex ethical and epistemic questions about representation. Existing evaluation frameworks such as T2IAT <ref type="bibr" target="#b100">[101]</ref>,</p><p>DALL-Eval <ref type="bibr" target="#b101">[102]</ref>, and other studies <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105]</ref> primarily assess biases along predefined axes, such as gender <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>, skin tone <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>, culture <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b105">106]</ref>, and geographical location <ref type="bibr" target="#b105">[106]</ref>. While these works offer key insights into single-axis bias detection and mitigation, they lack a systematic examination of how biases on one axis influence another-a core aspect of intersectionality.</p><p>Intervention Strategies. To improve fairness and transparency in model predictions, various intervention strategies have emerged that modify datasets, model architectures, or interpretability tools to better account for sensitive attributes and human-aligned reasoning.</p><p>One class of approaches focuses on improving data quality and transparency. Structured documentation practices aim to make dataset collection and labeling processes more transparent and auditable <ref type="bibr" target="#b106">[107]</ref>, while data resampling and relabeling techniques have been used to reduce biased correlations, such as gender misclassification in image captions <ref type="bibr" target="#b107">[108]</ref>.</p><p>Another strategy involves generating controlled counterfactuals by manipulating specific input attributes. These interventions alter sensitive features like skin tone or hairstyle while keeping the overall content unchanged, enabling targeted fairness audits and behavior testing under attribute shifts <ref type="bibr" target="#b108">[109]</ref>.</p><p>A third direction probes models at the conceptual level. Concept activation techniques allow researchers to quantify how much certain human-defined concepts influence model predictions <ref type="bibr" target="#b109">[110]</ref>.</p><p>Building on this, more recent approaches introduce concept bottlenecks that constrain the model to reason explicitly through interpretable features, leading to more faithful and fairness-preserving predictions <ref type="bibr" target="#b110">[111]</ref>.</p><p>Ongoing Challenges and Directions. Despite this progress, vision fairness remains a moving target.</p><p>There is no universal definition of fairness that suits all visual tasks. Moreover, techniques like concept bottlenecks and dataset balancing require human input, which reintroduces subjectivity.</p><p>Another emerging area is fairness-aware diffusion models, where causal interventions are embedded directly into the generative process. Community-wide initiatives are needed to design benchmarks, metrics, and co-designed datasets that reflect global and intersectional notions of fairness.</p><p>Vision fairness remains an evolving area, with calls for more robust evaluation metrics, audit toolkits, and participatory approaches to dataset governance <ref type="bibr" target="#b111">[112]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">Intersectionality of Biases</head><p>Intersectionality, introduced by Crenshaw <ref type="bibr" target="#b14">[15]</ref>, describes how multiple forms of oppression-such as racism, sexism, and classism-intersect to shape unique experiences of discrimination. Two key models define this concept: the additive model, where oppression accumulates across marginalized identities, and the interactive model, where these identities interact synergistically, creating effects beyond simple accumulation <ref type="bibr" target="#b112">[113]</ref>. In the context of AI, most existing work <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b116">117]</ref> aligns more closely with the additive model, focusing on quantifying and mitigating biases in intersectional subgroups. This perspective has influenced fairness metrics <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b119">120]</ref> designed to assess subgroup-level performance, extending across various domains, including natural language processing (NLP) <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b123">124]</ref> and recent large language models <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b125">126,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b127">128]</ref>,</p><p>multimodal research <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b129">130]</ref>, and computer vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b130">131]</ref>. These approaches typically measure disparities across predefined demographic intersections and propose mitigation strategies accordingly.</p><p>Our work aligns with the interactive model of intersectionality, using counterfactual-driven causal analysis in TTI models. Beyond subgroup analysis, we intervene on a single bias axis to assess its ripple effects on others, revealing independences and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Explainability in Machine Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1">Motivations and Challenges</head><p>As machine learning models become increasingly complex and opaque-especially deep neural networks-explainability has emerged as a central requirement for trustworthy AI systems. Explainability enables stakeholders to understand, trust, and audit model decisions. It is crucial not only for scientific insight and debugging but also for meeting regulatory and ethical standards, such as the "right to explanation" in the GDPR. However, creating faithful, interpretable, and user-aligned explanations remains a major technical and philosophical challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.2">Approaches to Explainability</head><p>Explainability techniques in machine learning can be broadly categorized into four major families:</p><p>post-hoc attribution methods, concept-based explanations, visual counterfactual explanations, and natural language justifications. Each of these families offers a different lens into the model's decision-making process and is suited to different audiences and use-cases.</p><p>Post-hoc attribution methods attempt to explain a model's decision by assigning importance scores to individual input features. One of the earliest and most widely used methods is LIME <ref type="bibr" target="#b131">[132]</ref>,</p><p>which fits an interpretable linear model locally around the input to approximate the decision boundary.</p><p>SHAP <ref type="bibr" target="#b132">[133]</ref> builds on game-theoretic principles and uses Shapley values to ensure additive and consistent feature attributions. Integrated Gradients <ref type="bibr" target="#b133">[134]</ref> offers an axiomatic approach by computing the integral of gradients along a straight-line path from a baseline to the input. DeepLIFT <ref type="bibr" target="#b134">[135]</ref> compares activations relative to a reference point to attribute differences in output scores. Layer-wise Relevance Propagation (LRP) <ref type="bibr" target="#b135">[136]</ref> decomposes the prediction by propagating relevance scores backward through the network layers. SmoothGrad <ref type="bibr" target="#b136">[137]</ref> further improves the robustness of gradientbased saliency maps by averaging gradients over multiple noisy samples. While these methods are model-agnostic or gradient-based, they often lack stability and may fail to align with human intuitions.</p><p>Concept-based explanation methods move beyond raw features to attribute decisions to humaninterpretable concepts. Testing with Concept Activation Vectors (TCAV) <ref type="bibr" target="#b109">[110]</ref> computes directional derivatives of network predictions with respect to high-level concept vectors defined by curated example sets. Automated Concept Extraction (ACE) <ref type="bibr" target="#b137">[138]</ref> extends this by clustering activations to discover coherent concepts in a completely unsupervised manner. Concept Whitening <ref type="bibr" target="#b138">[139]</ref> reorients latent spaces to make concept directions orthogonal, enabling direct control and interpretation. Self-Explaining Neural Networks (SENN) <ref type="bibr" target="#b139">[140]</ref> builds interpretable models that decompose predictions into combinations of learned concepts with associated relevance scores. ProtoPNet <ref type="bibr" target="#b140">[141]</ref> learns prototypical patches from training data and uses similarity to these prototypes for classification, making the reasoning process visually intuitive. Another important direction is concept bottleneck models <ref type="bibr" target="#b141">[142]</ref>, which require models to make predictions through a set of intermediate, user-defined concepts, thereby enforcing semantic transparency.</p><p>Visual counterfactual explanations aim to identify features that causally influence model decisions by generating alternative inputs that lead to different predictions. This often involves modifying images in a way that preserves semantic meaning while altering the outcome. Early methods achieved this by editing specific regions of an image to induce label changes <ref type="bibr" target="#b17">[18]</ref>, while more advanced approaches learn disentangled latent representations to produce controlled interventional edits in videos <ref type="bibr" target="#b142">[143]</ref> or high-dimensional image spaces <ref type="bibr" target="#b143">[144]</ref>. Generative models such as GANs have also been employed to synthesize realistic counterfactuals for fairness assessment <ref type="bibr" target="#b144">[145]</ref>, and prompt-based interventions have been used to audit text-to-image systems for bias <ref type="bibr" target="#b108">[109]</ref>. More recent work takes a perturbation-based approach, directly optimizing over image regions to uncover minimal deletions that cause prediction shifts <ref type="bibr" target="#b145">[146]</ref>.</p><p>Natural language explanations provide accessible, human-readable insights into model behavior, especially in NLP and multimodal tasks. Several efforts have paired model predictions with commonsense-driven justifications to improve transparency and trust, often via datasets and models designed to output textual rationales <ref type="bibr" target="#b146">[147]</ref>. In visual settings, explanation frameworks combine language with visual grounding, linking answers with natural justifications and attention maps <ref type="bibr" target="#b147">[148,</ref><ref type="bibr" target="#b148">149]</ref>. Recent developments leverage large language models to generate chain-of-thought "self-talk" explanations that enhance both interpretability and generalization <ref type="bibr" target="#b149">[150]</ref>. Despite their accessibility, concerns remain about the faithfulness of these explanations; empirical studies suggest that many textual justifications do not accurately reflect the model's internal decision process <ref type="bibr" target="#b150">[151]</ref>.</p><p>Together, these techniques form the backbone of modern explainable AI research. However, each method has limitations in terms of stability, completeness, or faithfulness, and ongoing research seeks to unify causal reasoning, human alignment, and transparency within a coherent framework. Cow"), how can we determine which meaningful, human-interpretable concepts influenced that decision? For instance, did the model recognize the image as a cow because of meaningful features like its distinctive body structure, or was the decision influenced by spurious cues such as the presence of grasslands in the background? To address this, we introduce a method that quantifies the extent to which a model relies on human-defined concepts-offering a more interpretable alternative to conventional pixel-level explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Despite their widespread use, deep neural networks remain difficult to interpret. These models often behave as black boxes, making it hard to explain how they arrive at specific outputs. This lack of interpretability poses significant risks, especially when models are deployed in high-stakes areas such as healthcare, law enforcement, or financial decision-making. In such cases, the cost of a wrong prediction can be substantial, making transparency and accountability essential. Clear explanations not only help identify errors but also support human oversight and allow timely intervention when needed.</p><p>As discussed in earlier chapters, counterfactual reasoning provides a practical lens to examine In computer vision, this task is particularly challenging because the inputs are pixel arrays that lack an obvious mapping to high-level human concepts such as age, gender, or clothing type.</p><p>Consider, for example, a binary classifier that detects whether a person is smiling. The input is an image, and the output is a yes-or-no answer. While the decision may appear simple, the factors driving it might include complex and implicit cues such as facial structure, hairstyle, or even background context. We may want to ask: Is the classifier making the decision based on genuine smile features, or is it influenced by confounding cues like gender or lighting?</p><p>To address such questions, we introduce a method called CAVLI that combines two popular approaches called TCAV and LIME. This framework aims to quantify how much a neural network's decision relies on a given human-defined concept. The key idea is to measure the overlap between image regions that contribute to the model's decision and regions that are representative of the concept. For instance, we may want to test if a model identifying a cow relies heavily on the presence of grass in the background. If the model often uses grass as a signal to detect buffalo, it reveals a potential shortcut or bias in decision-making.</p><p>As depicted in Figure <ref type="figure">3</ref>.2 the high-level architecture of our approach consists of three main steps.</p><p>In Step 1, we identify the regions of an image that have the highest association with a given human concept. We use a heatmap to highlight the regions that are most relevant to the concept of interest.</p><p>In Step 2, we identify the image regions that have the highest involvement in the decision-making process of the neural network for a given input image. To achieve this, we use attribution methods to compute the contribution of each image region to the final decision. Finally, in Step 3, we measure the overlap between the image regions identified in Steps 1 and 2. This overlap gives us a measure of the degree to which the decision of the neural network is dependent on the human concept of interest.</p><p>Our framework is based on the insight that if a model's decision depends on a concept, then there should be a high overlap in regions that are used by the model for decision-making and regions of the image that is used for concept modelling. For example (Figure <ref type="figure">3</ref>.2), if parts of the image that are used for classifying whether an animal is a cow or not are also associated with the concept of grassland, then the decision cow depends on the concept grassland. Our approach introduces the Concept Dependency Score (CDS), which quantifies the reliance of decisions on visual concepts.</p><p>Apart from CDS we also present visual heatmaps that illustrate the overlap between decisions and concept regions. These heatmaps provide a visual understanding, enabling humans to see the extent to which a decision depends on a particular concept. The proposed approach, which combines Conditional Dependency Sampling (CDS) with heatmaps, offers multiple advantages. Firstly, the heatmap enables a computer vision practitioner to understand the overlap between regions used for decision-making and the regions associated with a given concept. This overlap is quantified using  Overview of our proposed approach, CAVLI, to estimate the dependence of a concept (e.g., "grassland") on a decision (e.g., "cow detection"). After decomposing the input image into superpixels, in Step 1 we find the regions of the image that have the highest association with the concept, defined by a set of images. In Step 2 we identify image regions with the highest involvement in the classification decision. Finally, we measure the overlap between the two in order to quantify the dependence. </p><formula xml:id="formula_7">i ∈ {0, 1} K with K classes. F k (x i ) := h l (f l (x i )),</formula><p>where f l (x i ) are the output logits of the l th layer and h l is the activation function of the l th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">TCAV and LIME</head><p>TCAV <ref type="bibr" target="#b109">[110]</ref> uses human-defined concepts (e.g., "gender" or "stripes") instead of input features to provide explanations for a machine learning model. To express a concept it finds a Concept Activation Vector (CAV) a ∈ R d (a layer with dimension d) in the network's activation space <ref type="bibr" target="#b151">[152]</ref> that points in the direction of the concepts. This is achieved by training a classifier that distinguishes concept activations ("striped" or "dotted") from activations of negative samples and taking a unit norm vector v c orthogonal to its decision boundary. The inner product in Equation <ref type="formula">1</ref>. denotes the similarity of the activstion to the required concept and v c denotes the direction of the concept vector. This is defined as the Conceptual Sensitivity CS of a given layer l for the network's output class k and the concept C:</p><formula xml:id="formula_8">CS k C,l (F, x i ) = ∇h l (f l (x i )) T v c (3.1)</formula><p>The TCAV score is given as the ratio of the number of inputs with positive conceptual sensitivity to the number of inputs for a class.</p><p>LIME <ref type="bibr" target="#b152">[153]</ref> is a black box method for understanding local explanations of a machine learning model. In order to explain the prediction of a model F on an image x i it:</p><p>1. Decomposes x i in r homogeneous image patches or superpixels.</p><p>2. Creates a set of new images x i 1 , ....., x in by selecting n subsets of the superpixels 3. Queries the model for each of these images y i j = F (x i j ) ∀j ∈ {1, 2..., n} 4. Builds a local weighted surrogate model βi fitting the y i j to the presence or absence of superpixels.</p><p>Each coefficient of βi is associated with a superpixel of the original image x i . Intuitively, the more positive the value of the coefficient, the more important the superpixel is for the prediction of the model. Generally, the user visualizes βi by highlighting the superpixels associated with the top positive coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Superpixels</head><p>Superpixels are perceptually coherent regions of an image obtained by grouping pixels with similar color, texture, or spatial proximity <ref type="bibr" target="#b153">[154]</ref>. Instead of analyzing individual pixels, which are both high-dimensional and visually unintuitive, superpixels provide a mid-level representation that aligns more closely with human perception. By reducing the number of primitives from thousands of pixels to a few hundred superpixels, they enable more efficient processing while preserving important object boundaries.</p><p>Several algorithms have been proposed for generating superpixels, among which the Simple Linear Iterative Clustering (SLIC) method is widely used due to its efficiency and effectiveness <ref type="bibr" target="#b153">[154]</ref>. SLIC adapts the k-means clustering algorithm in a five-dimensional space (Lab color values and pixel coordinates), ensuring that generated superpixels are spatially compact and adhere well to object boundaries. The main steps of SLIC involve:</p><p>1. Initialize k cluster centers uniformly across the image grid.</p><p>2. Assign each pixel to the nearest cluster center in a joint color-spatial distance metric.</p><p>3. Update cluster centers as the mean of assigned pixels.</p><p>4. Iterate until convergence, producing approximately equally sized, homogeneous regions.</p><p>For our purposes, superpixels serve two key roles. First, they act as the interpretable building blocks over which perturbations are applied in LIME, creating counterfactual-like variants of the original image. Second, they provide localized regions where concept-level sensitivities can be measured using TCAV. Together, these properties make superpixels a natural choice for bridging concept attribution and decision attribution in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Proposed Approach</head><p>Algorithm 1 CAVLI 1: Train a TCAV model for a given concept C, a model F , and a layer l, resulting in the CAV vector v c .</p><p>2: Decompose the input image x i ∈ X into r homogeneous superpixels {S}.</p><p>3: Create new images {x i 1 , . . . , x in } by masking x i over n random subsets of {S}.</p><p>4: Calculate conceptual sensitivities z i j = CS k C,l (F, x i j ) for each x i j .</p><p>5: Fit a local surrogate αi using z i j values and superpixel presence.</p><p>6: Query the model for each image patch: y i j = F (x i j ).</p><p>7: Fit a second surrogate βi using y i j values and superpixel presence.</p><p>8: Compute Pearson correlation γ i between coefficients of αi and βi . 9: Calculate Concept Dependency Score:</p><formula xml:id="formula_9">CDS i = γ i • CS k C,l (F, x i ).</formula><p>We propose a hybrid TCAV-LIME-based approach as a solution to the problem. Algorithm 1 describes our proposed framework. Finally, we measure the overlap between the two parts. The intuition behind the approach is that if there is a clear overlap between image patches that have a high dependency on a concept and image patches that have the highest weight in decision-making, then the decision made by the network is heavily dependent on the concept (Steps 8 and 9).</p><p>The coefficients of αi indicate the level of association between different superpixels and a particular concept. A higher coefficient value suggests that the model considers the concept to be more closely related to that region, and vice versa. Similarly, coefficients of βi corresponds to a superpixel in the original image x i . The higher the weight of the superpixel, the more significant its contribution to the model's decision-making process. We are interested in measuring whether the superpixels associated with the the given concepts are also associated with decisions made by the algorithm. We calculate the Pearson correlation γ i correlation of βi and αi to measure the overlap between the two decisions. A larger value of γ i indicates that there is a high overlap between the regions of the image that the model associates with the concept and those it uses for the decision.</p><p>The Concept Dependency Score CDS i , is calculated as the product of γ i and CS k C,l (F, x i ), ensuring that relevant concepts are given higher values. For a qualitative understanding, the coefficients of αi associated with the superpixels can be represented as a concept heatmap. This heatmap gives us a visualization of what parts of the image are more likely to be associated with a concept.</p><p>Randomly turning off subsets of superpixels during the analysis creates a counterfactual-like effect by simulating alternative versions of the input image in which certain visual information is masked. Each such perturbation can be interpreted as a counterfactual example: "What would the model predict if this region were absent?" This allows us to estimate the influence of specific regions on the model's decision without retraining or accessing model internals. By analyzing how the model's prediction changes in response to the removal of particular superpixels, we can identify which parts of the image are most critical for its output. This perturbation-based approach is central to LIME and complements TCAV's concept attribution, enabling a localized and interpretable view of the concept-decision relationship. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">ImageNet Dataset</head><p>In our initial experiments, we assess the effectiveness of CDS in explaining model decisions in terms of concepts. To establish a baseline, we compare the performance of TCAV with the average CDS scores across different samples. We propose a hypothesis that if there exists a correlation between the mean CDS scores and global concept methods like TCAV, it indicates that our metric is capable of accurately capturing the dependence between the model decisions and underlying concepts.We conducted experiments on the ImageNet dataset, using similar settings to Kim et al. <ref type="bibr" target="#b154">[155]</ref> and Schrouff et al. <ref type="bibr" target="#b151">[152]</ref> to validate our model. Specifically, we focused on the Zebra and Basketball classes, using three different models (GoogleNet, ResNet-50, and InceptionNet) for each class. Our goal was to measure the average statistics for each class using 100 images per set and calculating the mean correlation across all CDS scores. The experiments were conducted on the penultimate layer of all models.</p><p>Zebra. We ran experiments similar to Schrouff et al. <ref type="bibr" target="#b151">[152]</ref> that focus on four different concepts:</p><p>"stripes," "zigzagged," "dotted," "horse," and "grasslands." The results of the experiments are pre- Basketball. We examined four human concepts ("ball," "jersey," "gender," and "race") in a manner similar to the Zebra class. The results in Table <ref type="table" target="#tab_2">3</ref>.2 show higher mean CDS scores for "jersey" and "ball," and a lower score for "female." We trained a race concept classifier with positive class images of African American faces. Our results further confirm the previous findings of a correlation between decision made on the basketball class and concept race. <ref type="bibr" target="#b151">[152]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">CelebA dataset</head><p>We are interested in exploring whether our approach can detect biases in model decisions caused by unbalanced data. Through our experiments we are interested in measuring whether these confounds can be detected by our metric. The CelebA dataset <ref type="bibr" target="#b155">[156]</ref> is known to have naturally occurring confounds. We train a smile classifier in a biased setting, where the training set is subsampled to create a higher positive correlation between the female-smiling and male-non-smiling attributes. We analyze the average CDS scores for different subgroups on the test data, as shown in Table <ref type="table" target="#tab_2">3</ref>.3. We observe that the highest average CDS scores were for the "female smiling" group, while the lowest were for the "male smiling" group. These experimental findings align with the existing biases present in the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Qualitative Analysis</head><p>Our method generates concept heatmaps that illustrate the image regions and their dependence on human-defined concepts. These heatmaps aid in visually interpreting a model's image dependency, as shown in Figure <ref type="figure">3</ref>.3 for the cow-grasslands class-concept pair. The qualitative analysis reveals not only the parts of the image used for decision-making, but also whether the model links these parts to a concept. Additionally, this visual representation can identify spurious correlations where a classification decision (cow) is based on a concept (grassland) that is not directly related to the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>This work introduces a novel framework for generating local, concept-based explanations in computer vision models by combining TCAV and LIME in a hybrid approach. At its core, the method uses counterfactual reasoning to understand the relationship between human-understandable concepts and model predictions. By selectively turning off superpixels to simulate counterfactual inputs, and measuring their alignment with concept representations, we create localized explanations that reflect how changes in visual concepts affect model behavior. This approach culminates in the Concept Dependency Score (CDS), which quantifies the overlap between concept relevance and decision salience.</p><p>The method aligns closely with the broader goals of this thesis, which leverages counterfactual reasoning to make black-box models more transparent and trustworthy. By operationalizing counterfactuals through perturbations, our method not only enhances interpretability but also provides actionable insights for auditing model decisions in sensitive applications. While the framework has limitations-including reliance on segmentation quality and the subjective nature of concept definitions-it provides a principled way to connect input-level interventions with higher-level semantic reasoning. Future work could extend this approach through richer concept libraries, human-in-theloop validation, and deployment in real-world auditing settings. Overall, this chapter demonstrates how counterfactual reasoning can serve as a powerful foundation for concept-based interpretability in vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Limitations</head><p>While CAVLI provides a general framework to assess the dependence of model decisions on humandefined concepts, it relies on the assumption that a greater spatial overlap between regions important for decision-making and regions associated with a concept implies stronger dependence on that concept. This reasoning aligns only with the first rung of Pearl's ladder of causation-association-and does not account for deeper causal relationships. For example, if the regions associated with both skin color and smiling overlap significantly in an image, the resulting high CDS score may misleadingly suggest a strong dependence on both, even though only one may be causally relevant. Association, however, does not imply causation.</p><p>Moreover, CAVLI requires pre-defined concepts and lacks mechanisms for discovering or prioritizing concepts automatically. It does not determine which concepts are most relevant for a given image or task. While we address this limitation in the following chapter by introducing a method to automatically identify relevant concepts, the dependence on manually specified concepts remains a key constraint of the current approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 4</head><p>ASACs: Mitigating bias in computer vision models through adversarial counterfactuals  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>Computer vision systems trained on large amounts of data have been at the heart of recent technological innovation and growth. Such systems continue to find increasing use-cases in different areas such as healthcare, security, autonomous driving, remote sensing and education. However, despite the tremendous promise of large vision models, several studies have demonstrated the presence of unwanted societal biases in these models <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b107">108]</ref>. Therefore, understanding and mitigating these biases is crucial toward deploying such systems in real-world applications.</p><p>Several approaches have been proposed to mitigate <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b160">161,</ref><ref type="bibr" target="#b161">162]</ref>,</p><p>measure <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, and explain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> biases in computer vision models. Among these approaches, the use of counterfactuals has emerged as a promising and prominent line of research. This chapter takes a step further in that direction by proposing a new approach to mitigate biases in image classifiers.</p><p>This chapter builds on that line of work by proposing a new counterfactual-based strategy to mitigate bias in image classifiers. However, before embracing counterfactuals as a tool for fairness, we pause to ask a fundamental question: Are the counterfactuals we currently use valid and trustworthy? Could they, instead of helping, introduce new artifacts, reduce model robustness, or even amplify the very biases they are meant to mitigate? Indeed, existing counterfactual generation methods suffer from several critical limitations. Most rely on generative models to produce image variants along protected attributes such as race or gender.</p><p>These generators, often trained on biased datasets, may produce counterfactuals that embed spurious correlations or stereotypical visual features. For example, Figure <ref type="figure" target="#fig_14">4</ref>.2 shows biased outputs from StyleGAN2 <ref type="bibr" target="#b162">[163]</ref> for a smile classification task: when asked to generate more feminine faces, the model exaggerates smiles and adds heavy makeup; when asked to masculinize a face, it adds wrinkles or harsh facial structure. Such distortions can mislead both human and algorithmic decision-making, especially if used to train or fine-tune a supposedly fairer classifier.</p><p>A similar issue arises in more targeted edits (see Figure <ref type="figure" target="#fig_14">4</ref>.1). What happens when we ask a counterfactual generation engine to modify an image along specific attributes such as smile, eye Therefore, to ensure fair and responsible use of counterfactuals, we advocate for approaches that preserve semantic integrity, minimize spurious correlations, and explicitly guard against potential misuse. This chapter introduces such a method designed to generate precise, ethically sound counterfactuals that challenge biased model decisions without reinforcing harmful stereotypes. Instead of relying on generative models, we propose using adversarial images as an alternative for producing counterfactuals in the context of bias mitigation. While adversarial examples are traditionally used to expose model vulnerabilities, we repurpose them to construct minimal, targeted perturbations that reveal and reduce unfair model dependencies and also improve the overall performance of the model.</p><p>Our approach demonstrates that adversarial counterfactuals can not only improve fairness metrics but also enhance overall model accuracy.</p><p>Our goal is to improve the performance of a classifier as well as debias it against protected attributes without using images that may introduce or exacerbate such biases in the model. We attempt to do so in a way that also address the ethical concerns around training on images created based on societal stereotypes. We use existing adversarial techniques to create counterfactuals that challenge vision models based on protected attributes. We refer to these samples as "Attribute-Specific Adversarial Counterfactuals" or ASACs. Our method ensures that ASACs retain the visual appearance of the original image, effectively addressing ethical concerns regarding the quality of image counterfactuals. By keeping the essence of the image unaltered, the likelihood of introducing spurious correlations (propagated by the generative model during the image generation process) into the fairness mitigation pipeline is markedly reduced.</p><p>In addition, our work introduces a novel approach to improve both fairness and accuracy in these models by utilizing ASACs in a curriculum learning-based framework. Our approach utilizes ASACs generated at different noise magnitudes and assigns a learning curriculum to these samples based on their ability to deceive the model. We then fine-tune the model using the assigned curriculum. We validate our approach both qualitatively and quantitatively using experiments on various classifiers trained on multiple datasets and several target attributes. We show that our method generalizes to biased models of varying scales (between 8M and 24M parameters). Our contributions can be summarized as follows: (1) We introduce a bias-averse method for generating image counterfactuals using adversarial images to create Attribute-Specific Adversarial Counterfactuals (ASACs) for protected attributes.</p><p>(2) We also present a novel curriculum learning-based fine-tuning approach that leverages ASACs to mitigate pre-existing biases in vision models. Finally, (3) we provide a comprehensive evaluation across various datasets, architectures and metrics, demonstrating that our method enhances fairness metrics without compromising the model's overall performance and in many cases, improving the models performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Related Works</head><p>Adversarial examples have emerged as a notable challenge in the realm of computer vision <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b164">165,</ref><ref type="bibr" target="#b165">166]</ref>. These examples are crafted with the specific goal of deceiving machine learning models by making subtle alterations to input, resulting in model misclassification. Adversarial attacks are specialized algorithms designed to generate such examples and have been the subject of extensive research. These attacks are categorized into black-box <ref type="bibr" target="#b166">[167,</ref><ref type="bibr" target="#b167">168,</ref><ref type="bibr" target="#b168">169]</ref> and white-box attacks <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b165">166]</ref>, depending on the attacker's access to model parameters. Our approach uses two whitebox methods, FGSM <ref type="bibr" target="#b169">[170]</ref> and PGD <ref type="bibr" target="#b170">[171]</ref>.</p><p>The Relationship Between Counterfactual and Adversarial Examples While the connection between counterfactuals and adversarial images has been extensively explored in theoretical machine learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b172">173,</ref><ref type="bibr" target="#b173">174]</ref>, limited attention has been given to this relationship in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b174">175,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b176">177]</ref>. Among these approaches, work by Wang et al. <ref type="bibr" target="#b6">[7]</ref> and Zhang et al. previous methods that often rely on a single adversarial example, our approach systematically assesses the impact of each adversarial example on the original classifier, providing a more directed training regime. Compared to previous works our paper highlights how adversarial counterfactuals can not only prevent ethical issues compared to image generator-based counterfactuals but also prevent spurious correlations from being added in the de-biasing process. In addition, we conduct a more rigorous empirical analysis to show that the approach can not only improve on fairness metrics but also overall performance.</p><p>Curriculum Learning Curriculum learning is an effective training paradigm that gradually exposes models to progressively more complex examples, aiding in better generalization <ref type="bibr" target="#b177">[178,</ref><ref type="bibr" target="#b178">179,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b180">181,</ref><ref type="bibr" target="#b181">182]</ref>. This approach strategically organizes training data, facilitating smoother convergence and improved performance. Several approaches have since been proposed to organize data that include sorting the data <ref type="bibr" target="#b177">[178]</ref>, adaptive organization <ref type="bibr" target="#b179">[180]</ref>, self-paced curriculum assignment <ref type="bibr" target="#b179">[180]</ref>, and teacher-based curriculum learning <ref type="bibr" target="#b179">[180]</ref>. To the best of our knowledge, we believe that we are the first to use a curriculum based framework with adversarial images to improve both fairness as well as performance of biased classifiers in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Notations</head><p>We define the data distribution as D, from which a set of images X are sampled. Along with the images, we sample the corresponding ground truth labels for a target attribute and a protected attribute defined by Y target and Y protected respectively. A single sample drawn from the set is defined as (x, y target , y protected ) where x represents the input image and (y target , y protected ) represent the ground truth label of the target and protected attributes respectively.</p><p>Let M (θ,ρ) : X → Ŷtarget be a target attribute classifier that maps the input data to the target class.</p><p>After training M (θ,ρ) , we train a protected attribute classifier C (θ,ϕ) : X → Ŷprotected that maps the same input data to a protected attribute class. Here θ denotes the parameters of the common backbone network shared by the two classifiers, whereas ϕ and ρ are the linear layers for C and M respectively.</p><p>This means that for a given an input image x, ŷtarget = M (θ,ρ) (x) and ŷprotected = C (θ,ϕ) (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation Metrics</head><p>To evaluate the fairness of our model, we employ three key metrics: the Difference in Demographic Parity (DDP), the Difference in Equalized Odds (DEO), and the Difference in Equalized Opportunity (DEOp) <ref type="bibr" target="#b80">[81]</ref>. Alongside these, we also measure the model's accuracy (ACC). Lower values for fairness metrics indicate a less biased model whereas a high value in accuracy indicates a more performant model. The metrics are based on definitions of fairness and were chosen to compare with previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The DEO focuses on the difference in the rates of false negatives and false positives between genders. A substantial DEO indicates a bias in which one group is either less likely to be incorrectly dismissed or more likely to be inaccurately favored.</p><p>Definition 1 (Equalized Odds). A predictor Ŷ satisfies equalized odds with respect to protected attribute A ∈ {a, a ′ } and outcome Y , if Ŷ and A are independent conditional on Y . The difference in Equalized Opportunity is given as follows.</p><formula xml:id="formula_10">P ( Ŷ = 1|Y = y, A = a) = P ( Ŷ = 1|Y = y, A = a ′ ) . (<label>4</label></formula><p>Definition 3 (Equalized Opportunity). A predictor Ŷ satisfies equalized opportunity with respect to protected attribute A ∈ {a, a ′ } and outcome Y , if the following condition holds for a specific outcome value y:</p><formula xml:id="formula_11">P ( Ŷ = 1|Y = 1, A = a) = P ( Ŷ = 1|Y = 1, A = a ′ ).</formula><p>To measure how far a predictor Ŷ from having demographic parity, we estimate the Difference in Equalized Opportunity (DEOp), i.e., we estimate</p><formula xml:id="formula_12">|P ( Ŷ = 1|Y = 1, A = a) -P ( Ŷ = 1|Y = 1, A = a ′ )|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Method</head><p>Our method for creating attribute-specific adversarial counterfactuals (ASACs) and utilizing them  procedure can be seen in Algorithm 2. Finally, the concluding stage utilizes the curriculum and ASACs produced by taking gradient step for every mini-batch to fine-tune the original model M (θ,ρ) .</p><p>This aims to reduce biases within the model and enhance its discriminative performance. for j = 1 to l do 5:</p><formula xml:id="formula_13">x a ij ← f attack (x i , ϵ j ) 6:</formula><p>score ← DS(x a ij ; y target , M (θ,ρ) )</p><p>7:</p><formula xml:id="formula_14">ASACs ← ASACs ∪ {(x a ij , score)} 8:</formula><p>end for 9: end for 10: ASACs ← Sort(ASACs, by score in order)</p><p>11: minibatches ← Partition(ASACs, l)</p><p>12: return minibatches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Generating attribute-specific adversarial examples</head><p>This section details our approach to constructing ASACs-adversarial images designed to mislead a model concerning a specific protected attribute, denoted as a. We start by employing a target classifier M (θ,ρ) and develop an attribute classifier, C (θ,ϕ) , to predict the protected attribute a within the dataset D. Both models share a similar structure, with only the last layer differing, as depicted in x a = x + δ x (4.3)</p><p>In our work, we experiment with two commonly used adversarial methods, Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b169">[170]</ref> and Projected Gradient Descent (PGD) <ref type="bibr" target="#b170">[171]</ref>, to generate images capable of deceiving the attribute classifier. FGSM is a single-step attack approach that perturbs input images based on the gradient sign of the loss function, as shown in Equation <ref type="formula" target="#formula_10">4</ref>.4, with ϵ ∈ [0, 1] denoting a scaling factor which we refer to as the noise magnitude. PGD iteratively perturbs input data similar to FGSM to maximize the loss function, aiming to find the smallest perturbation that causes misclassification.</p><formula xml:id="formula_15">δ x = ε × sign (∇ x J(θ, ϕ, x, y)) (4.4)</formula><p>Here J is any differentiable loss function on which the model is trained. It should be noted that ASAC's differ from a normal attacks because the images generated by ASAC's intend to fool the protected attribute classifier C (θ,ϕ) . Hence, ASAC's are designed such that</p><formula xml:id="formula_16">C (θ,ϕ) (x) ̸ = C (θ,ϕ) (x a ).</formula><p>Since both the target classifier and the protected attribute classifier have the same backbone θ , the generated ASAC's can influence the target classsifer M (θ,ρ) 's understanding of the protected attribute.</p><p>If ŷtarget = M (θ,ρ) (x) is dependent on the protected attribute i.e., the target classifier is biased, then the generated ASAC may also be able to fool the target classifier and M (θ,ρ) (x) ̸ = M (θ,ρ) (x a ) even though the ASACs were generated to fool the protected attribute classifier C (θ,ϕ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Curriculum Learning</head><p>We propose a curriculum learning-based strategy for training our model. A curriculum learning approach trains the model from easy to hard examples or vice versa, thereby making the learning process efficient and faster. Our approach comprises two main stages. First, we evaluate the influence of each ASAC on the classification model M (θ,ρ) by assigning each ASAC a difficulty score. Subsequently, the model implements a curriculum for training, guided by the difficulty scores over the ASACs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.1">Computing Difficulty Scores</head><p>The difficulty score is crucial in curriculum learning, as it measures the utility of each example and determines the order in which ASACs are presented to the model during adversarial training <ref type="bibr" target="#b180">[181]</ref>.</p><p>However, obtaining the optimal difficulty score a priori is not feasible in our setting. To address this, we define the difficulty score for any ASAC image target label pair (x a , y target ) as the complement of the softmax value that the model M (θ,ρ) returns for x a corresponding to the ground truth label y target .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DS(x</head><formula xml:id="formula_17">a ; y target , M θ,ρ ) = 1 -Softmax(M θ,ρ (x a )) ytarget (4.5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.2">Curriculum Assignment</head><p>For each batch of k images {x 1 , . . . , x k }, we compute the ASACs as the Cartesian product of k images with the set of l noise magnitudes we use in the curriculum (including ϵ = 0 i.e., the original images). For each of the samples in the resulting batch of size k × l, we compute a difficulty score, DS(x a ), detailed in Equation 4.5, that scores an ASAC based on the difficulty of the example for the target classifier to classify correctly. We then sort this batch based on the difficulty of each sample and the curriculum strategy chosen. Finally, we partition the sorted batch into l mini-batches allowing us to take a gradient step through each mini-batch, thereby utilizing the ASACs in a curriculum.</p><p>See Table <ref type="table" target="#tab_14">4</ref>.6 for an ablation on the effect of a randomized, increasing, and decreasing order (w.r.t.</p><p>difficulty score) for samples in the curriculum. This procedure is illustrated in detail in Algorithm 2</p><p>and enables what defines each in-batch curriculum in the fine-tuning process of the target classifier</p><formula xml:id="formula_18">M (θ,ρ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment Setup</head><p>This section describes our experimental setup. We outline the datasets used, followed by the evaluation metrics that are employed to evaluate the model and finally describe the training details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Experiments</head><p>In this section, we present and discuss various experiments to evaluate and interpret our proposed approach. We evaluate our approach through quantitative performance metrics and fairness evaluations, followed by qualitative analyses using visualization methods for model understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.1">Comparing the proposed method across different target attributes on different datasets</head><p>As shown in Table <ref type="table" target="#tab_14">4</ref>.1, we quantitatively evaluate the performance and fairness of our approach on the CelebA <ref type="bibr" target="#b155">[156]</ref> dataset, focusing on target attributes such as Smile, Big Nose, and Wavy Hair, with Gender as the protected attribute. Using ResNet-18 <ref type="bibr" target="#b184">[185]</ref> as the backbone, we compare models trained with and without our proposed approach and evaluate against the original baseline model and two different methods: an adversarial training approach <ref type="bibr" target="#b3">[4]</ref> and a debiasing approach by Ramaswamy et al. <ref type="bibr" target="#b4">[5]</ref> that generates counterfactual images using GANs. Our approach, utilizing FGSM <ref type="bibr" target="#b169">[170]</ref> and PGD <ref type="bibr" target="#b170">[171]</ref> adversarial methods (with ϵ = {0, 0.01, 0.001}), outperforms these previous methods in terms of both accuracy and fairness metrics. The GAN-based counterfactual approach <ref type="bibr" target="#b4">[5]</ref> reduces the overall accuracy of the system post training, however, our method not only improves accuracy for all three target attributes, but also improves fairness metrics (for all target attributes except big nose).</p><p>Most of our experiments focus on Gender as the protected attribute due to its availability in relevant datasets and clear binary classes. However, we also test our approach on a different protected attribute class using a different dataset. Specifically, we use the UTK dataset <ref type="bibr" target="#b183">[184]</ref> to train a gender classifier with Age as the protected attribute, categorizing individuals as "older" (40+) or "younger" (&lt; 40), given the dataset's roughly equal split around this age. Using a ResNet-18 architecture, our binary age classifier demonstrates that the proposed approach improves both fairness and accuracy metrics, consistent with previous settings. 4.6.1.2 Comparing the proposed approach across different backbones Table <ref type="table" target="#tab_14">4</ref>.3 presents a quantitative comparison of our approach using different backbones (ResNet-18 <ref type="bibr" target="#b184">[185]</ref>, ResNet-50 <ref type="bibr" target="#b184">[185]</ref>, DenseNet-121 <ref type="bibr" target="#b186">[187]</ref>, and DenseNet-169 <ref type="bibr" target="#b186">[187]</ref>) on the CelebA dataset <ref type="bibr" target="#b155">[156]</ref>. The overall size of the models vary from 8M to 24M parameters. Similar to our evaluation on the CelebA and UTK datasets, we use the FGSM attack with ϵ = {0, 0.01, 0.001} to construct ASAC mini-batches across each batch sampled. Our method enhances both accuracy and fairness, demonstrating its scalability and generalization across model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.3">Evaluation on a wider set of features</head><p>We also perform evaluations on a wider range of target labels and aggregate the results, similar to the evaluation protocol in Ramaswamy et al <ref type="bibr" target="#b4">[5]</ref>. We show the results before and after averaging fairness and performance metrics in Table <ref type="table" target="#tab_14">4</ref>.4 and Table <ref type="table" target="#tab_14">4</ref>.5 respectively. Our setting was similar to the setting explained in Table <ref type="table" target="#tab_14">4</ref>.6 of their paper. We considered 4 attributes: Young, Pointy Nose, Attractive, and Arched Eyebrows. We use Gender as the protected attribute. Te experiments were conducted on the CelebA dataset. The mean metrics aggregated across these attributes are given below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.1">Robustness evaluation</head><p>To assess the impact of adversarial training using ASACs, we examine how adversarial images designed to attack a gender classifier affect the decision of the smile classifier, pre and post-training with our proposed approach. Given an image, we add varying magnitudes of adversarial noise and provide the corrupted images to the smile classifier. We observe that before using our proposed method, the adversarial noise intended to mislead the gender classifier also misleads the smile classifier, indicating a possible correlation between gender and smile attributes in these examples.</p><p>Such an outcome suggests that manipulating the protected attribute gender can influence predictions on smile. We perform an identical analysis on the models after our proposed fine-tuning approach.</p><p>We observe that after fine-tuning, the target classifier improves its discriminative ability while the protected attribute classifier used to construct ASACs continues to be deceived. This indicates that the fine-tuning procedure helps to decouple spurious correlations between the target and protected attribute, enabling the target classifier to make more accurate predictions independent of the protected attribute. Examples of these results have been shown in Figure <ref type="figure" target="#fig_14">4</ref>.4. For each image we look at how the target classifer (smile) changes at different levels of noise. The blue curve shows the sensitivity to adversarial noise before training, while the red curve shows the sensitivity after training. After training, the images were found to be more robust to adversarial noise, with predictions remaining In the initial set of ablation analyses, we explore different training strategies. We implement a curriculum learning approach, wherein the model is trained in ascending order of difficulty. We contrast this strategy with an alternative approach where the model encounters the most challenging examples at the outset, followed by progressively easier ones. Table <ref type="table" target="#tab_14">4</ref>.6 presents a comparison of these strategies across various performance metrics. An intriguing pattern emerges: while exposing the model to difficult examples towards the end of training yields superior performance on fairness metrics, the reverse strategy-starting with challenging examples-results in the highest accuracy attained by the system thus far. This observation sheds light on the impact of curriculum learning on the model. One strategy enhances the accuracy of the model, while the other demonstrates greater improvement in fairness metrics. This highlights the crucial role of curriculum design in shaping the trade-off between accuracy and fairness in machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3.2">Comparison across different noise magnitudes</head><p>In our second ablation study (Table <ref type="table" target="#tab_14">4</ref>.7), we investigate the effect of varying noise magnitudes on our smile classifier model's performance. Using FGSM as our chosen adversarial image generation that ϵ = 0.03 and ϵ = 0.05 yield optimal performance. We then incorporate examples generated at these magnitudes, along with original images, into our curriculum learning setup. We perform these experiments on a smile classifier with gender as the protected attribute that is trained on the CelebA dataset. This experimentation offers valuable insights into selecting noise magnitudes that enhance the model's robustness and effectiveness in adversarial scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3.3">Experiments over multiple seeds</head><p>To ensure that our results are not due to overfitting we conduct experiments on a small scale over five runs on the ResNet18 architecture with different seeds. As shown below, across five trials, our method averaged improved accuracy as well as across our fairness metrics. The results are presented in Table <ref type="table" target="#tab_14">4</ref>.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Conclusion</head><p>We propose a novel method for generating attribute-specific adversarial counterfactuals to improve fairness and performance of computer vision models. These counterfactuals are constructed via adversarial perturbations along protected attributes and are used to fine-tune existing classifiers.</p><p>Unlike traditional generative counterfactuals, our approach preserves the semantic content of the original image, reducing the risk of introducing spurious correlations or stereotypes. We further introduce a curriculum learning-based fine-tuning regime, which assigns adversarial examples of increasing difficulty to progressively train the model.</p><p>Our experiments across multiple datasets demonstrate that fine-tuning with these adversarial counterfactuals significantly improves fairness metrics without sacrificing accuracy-in fact, in many cases, overall accuracy is enhanced. Qualitative analysis also suggests that the model is better able to disentangle predictions from protected attributes. This positions our framework as a dual-purpose tool for both performance improvement and fairness enhancement. Looking forward, we aim to extend our approach to handle multiple protected attributes and explore how adversarial counterfactuals can be leveraged for interpretable model diagnostics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work directly addresses ethical concerns surrounding biased decision-making in vision</head><p>systems by mitigating discrimination in already trained models. However, the approach comes with important limitations. First, it assumes access to model weights, making it inapplicable to black-box systems. Second, it requires explicit knowledge of protected attributes, which may not always be available or appropriate to collect. While our method offers a promising step toward post hoc fairness correction, its deployment in real-world systems must be coupled with careful consideration of privacy, consent, and downstream social implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Limitations</head><p>While our work introduces an in-situ method for generating counterfactuals-thereby preventing biases from external or third-party models from influencing the system-a key limitation remains: it does not address the biases that may already exist within the model itself. These internal biases can significantly affect the fairness and reliability of the system. Additionally, as with CAVLI, our approach does not automate the selection of relevant concepts for bias mitigation; this process remains manual and dependent on human input. Furthermore, the datasets used to fine-tune attribute classifiers may themselves carry underlying biases, which can propagate into the final model behavior. These  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robertson Davies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An old man at a church A philosopher using a laptop on Mars</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Capybaras getting married in Italy</head><formula xml:id="formula_19">✅ Ethnicity ✅ Skin Color ✅ Gender ❔ Sexual Orientation ✅ Ethnic ✅ Religious ✅ Gender Stereotypes ✅ Appearance ✅ Cultural ✅ Ableism ✅ Ethnicity ✅ Skin Color ❔ Gender ❔ Sexual Orientation ❌ Ethnicity ❌ Skin Color ❌ Gender ❌ Sexual Orientation ✅ Gender ✅ Clothing ✅ Environmental ✅ Philosophical ✅ Age ✅ Technological ✅ Cultural ✅ Gender Norms ✅ Animal Stereotypes ✅ Economic Bias ✅ Romanticization</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TIBET generates dynamic prompt-specific bias axes</head><p>Other methods that use pre-determined bias axes</p><formula xml:id="formula_20">( ✅ Yes ❌ No ❔ Maybe )</formula><p>Figure <ref type="figure" target="#fig_21">5</ref>.1: In generative modeling, the dimensions along which bias manifests are highly dependent on the input prompt. For example, the relevant axes of bias for "a photo of a philosopher" may involve gender, age, or cultural presentation, while a prompt like "capybaras getting married in Italy" may invoke romanticization, geographic, or animal stereotypes. Unlike prior methods that evaluate models using a fixed set of demographic axes (e.g., gender or race), our approach dynamically surfaces context-relevant bias dimensions per prompt-enabling more nuanced bias analysis in text-to-image generation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introduction</head><p>In prior chapters, we explored counterfactual reasoning as a powerful methodology for explaining model decisions (via CAVLI) and for mitigating bias in vision models through carefully constructed counterfactuals (via ASACs). These chapters demonstrated how changing one concept-while holding others fixed-can reveal the dependencies and assumptions baked into model behavior.</p><p>We now extend this methodology to generative multimodal systems, specifically text-to-image (TTI) models. Unlike classification systems, where a label must be predicted, TTI models generate complex outputs conditioned on natural language prompts making them particularly susceptible to subtle, prompt-sensitive biases. A small change in prompt wording can drastically alter generated content (e.g., "an old man at a church" and "an Asian old man at a church"). This chapter introduces a framework for dynamically identifying and evaluating such biases in TTI systems through promptlevel counterfactuals.</p><p>Generative text-to-image (TTI) models have emerged as a prominent research area in computer vision over the past few years. These models are capable of producing high-quality images based on natural language descriptions and have found applications in various fields, including online content creation and image editing. However, despite their promise, TTI models have demonstrated various kinds of biases in the images they generate, as shown in prior research <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106]</ref>.</p><p>Therefore, the identification and mitigation of biases is crucial in order to fully harness the capabilities of these models.</p><p>Existing approaches for measuring biases in TTI models typically employ a predefined set of bias axes (gender, age, and skin color) along which biases are assessed, aggregating over a fixed domain of prompts (e.g., occupation prompts <ref type="bibr" target="#b187">[188]</ref>). This line of work is useful in measuring the relative bias of TTI models. However, biases evaluated in one prompt domain may vary from another, and may even vary from prompt to prompt. In such cases, averaging across prompts may even mask certain biases. For instance, Figure <ref type="figure" target="#fig_21">5</ref>.1 illustrates three distinct input prompts, each associated with a different axis of bias. Here, measuring a predefined bias (e.g., gender) across all prompts is less meaningful (and may underestimate or mask bias due to prompt irrelevance). Ultimately, having the ability to asses biases for individual prompts is as important as doing so in aggregate over a domain of prompts, and the former is lacking from most existing approaches <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>.</p><p>Additionally, images for a user-provided prompt, or set of prompts, can exhibit different types of biases. These biases may be societally harmful in nature (societal biases), or simply be a result of common co-occurrences in the real world or in data that the TTI model was trained on (incidental correlations). For example, a computer programmer is often depicted as male (societal bias) wearing glasses (incidental correlation). While societal biases are most important and are generally analyzed in related works <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b187">188]</ref>, the existence of incidental correlations <ref type="bibr" target="#b188">[189]</ref> may also lead to reduced diversity in the generated images, and therefore must also be identified. Henceforth, for simplicity, we use the term "bias" to represent both societal biases and incidental correlations. Finally, a good measurement method should not only be capable of quantifying biases, but also of providing interpretable insights.</p><p>In this chapter, we introduce a novel framework called TIBET (Text to Image Bias Evaluation Tool) for examining, quantifying, and explaining a wide range of biases in images generated by TTI models. Our approach is designed to be compatible with any TTI model, and versatile across any user-provided prompts. In contrast to prior work that rely on a predefined set of biases, we dynamically identify potential biases relevant to the given prompt by leveraging an LLM like GPT-3.5.</p><p>Next, we generate counterfactual prompts for the identified bias axes, and images sets for the input prompt and all counterfactual prompts using the TTI model we want to evaluate. Finally, we compare the images from the initial prompt and the counterfactual prompts, using a new metric, the Concept Association Score (CAS), and further quantify biases using Mean Absolute Deviation (MAD). Our model has the ability to provide post-hoc explanations to gain qualitative insights about biases in images generated by TTI models. Furthermore, we can aggregate our metrics over a domain of prompts with the same biases and counterfactuals, in line with previous work.</p><p>Our experiments demonstrate that TIBET not only excels in scenarios where previous approaches <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102]</ref> have been employed, such as detecting gender stereotypes in occupational prompts, but it can also be effectively combined with bias mitigation techniques like ITI-GEN <ref type="bibr" target="#b189">[190]</ref>. ITI-GEN is a bias mitigation technique that focuses on editing prompts in text-to-image generation to reduce the presence of stereotypes. It works by automatically generating counterfactual prompts along underrepresented or marginalized attributes and rebalancing the output distributions of generative models. By doing so, it helps mitigate biases by ensuring that generated images better reflect diversity across attributes like gender, race, and age. This combination offers a more comprehensive and automated approach to bias mitigation in TTI models. Moreover, we conduct user studies to validate our approach with human judgement.</p><p>Our contributions can be summarized as follows. First, we propose an automated approach for identifying and measuring biases in images generated by TTI models, accommodating the dynamic nature of biases across different input prompts. Unlike prior works <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b187">188]</ref>, our framework evaluates images on a diverse set of bias axes encapsulating both societal and incidental biases.</p><p>Second, we propose novel quantitative metrics, CAS and MAD, that can be used to quantify these biases and also offer post-hoc explanations along different dimensions of biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Related Works</head><p>Measuring biases in TTI models. Much research has been conducted on evaluating and mitigating common social biases in image-only models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b160">161,</ref><ref type="bibr" target="#b161">162</ref>] and textonly models <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b190">191,</ref><ref type="bibr" target="#b191">192,</ref><ref type="bibr" target="#b192">193,</ref><ref type="bibr" target="#b193">194]</ref>. However, recent research is extending these studies to include multimodal models and datasets, exploring various aspects of language and vision. These investigations encompass biases found in embeddings <ref type="bibr" target="#b194">[195]</ref>, text-to-image <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b189">190,</ref><ref type="bibr" target="#b195">196]</ref>, retrieval <ref type="bibr" target="#b182">[183]</ref>, image captioning <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b196">197]</ref>, and visual question-answering models <ref type="bibr" target="#b197">[198,</ref><ref type="bibr" target="#b198">199,</ref><ref type="bibr" target="#b199">200]</ref>. Nonetheless, limited attention has been given to understanding biases in text-to-image (TTI) models. Existing approaches such as T2IAT <ref type="bibr" target="#b100">[101]</ref>, DALL-Eval <ref type="bibr" target="#b101">[102]</ref>, and other works <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106]</ref> for evaluating and mitigating biases in TTI models differ from our work in several key ways. They mainly focus on predefined bias axes like gender <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>, skin  <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b105">106]</ref>, or location <ref type="bibr" target="#b105">[106]</ref>, whereas our approach is dynamic, allowing for more flexible bias measurement. Additionally, many of these existing methods <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b105">106]</ref> require specific prompt structures, whereas our approach can assess bias for any input prompt. Moreover, our approach goes a step further by offering post-hoc concept-level explanations.</p><p>This helps users analyze the presence or absence of different semantic concepts in the images, enhancing their understanding of these biases, and providing insight into our metrics.Current research on bias identification is compactly summarized in Table <ref type="table">5</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Method</head><p>Given an input prompt P , we first dynamically generate bias axes relevant to P , and then generate counterfactuals along each bias axes (Steps 1-2 in Figure <ref type="figure" target="#fig_21">5</ref>.2). This process is detailed in Section An old man at a church </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ableism Bias</head><p>CF1: An old man using a wheelchair...  </p><formula xml:id="formula_21">CF2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Dynamic Bias Axes and Counterfactuals</head><p>We propose a prompt-dependent, dynamic bias-axes and counterfactual generation scheme that, given an input prompt P , generates relevant bias axes, followed by the generation of counterfactual prompts along those axes. Counterfactuals for an input prompt are generated in two steps, using chain-of-thought reasoning in LLMs <ref type="bibr" target="#b200">[201,</ref><ref type="bibr" target="#b201">202]</ref>. Firstly, the input prompt is used to dynamically create a list of bias axes representing dimensions of biases that are potentially present in the model (Step 1 in Figure . 5.2). The creation of these bias axes is facilitated by Large Language Models (GPT-3 <ref type="bibr" target="#b202">[203]</ref>), leveraging their ability to comprehend complex relationships. These axes then serve as the foundation for generating counterfactual prompts within their respective dimensions (Step 2 in Figure <ref type="figure" target="#fig_21">5</ref>.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.1">Counterfactual prompt generation using Chain of Thought Prompting</head><p>We use GPT-3 (specifically, gpt-3.5-turbo) for bias axis and counterfactual generation, through a series of well-defined queries, 1. For the image generation prompt, &lt;initial prompt&gt;, what are some of the axes where the prompt may lead to biases in the image?</p><p>2. Generate many counterfactuals for each axis. Create counterfactuals for all diverse alternatives for an axis. Each counterfactual should look exactly like the original prompt, with only one concept changed at a time.</p><p>3. Convert these to a JSON dictionary where the axes are the keys and the counterfactuals are list for each key. Only return json.</p><p>where &lt;initial prompt&gt; is replaced with the user-provided initial prompt to the model. All GPT-3 generations were done using the OpenAI API, in October and November 2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Text-to-Image Generation</head><p>The initial prompt and the counterfactual prompts are fed into a black-box Text-to-Image (TTI) model (that is to be evaluated), which generates a set of images for the input prompt, I P , and counterfactual prompts, I P cf (Step 3 in Figure . 5.2). Our approach works for any black-box TTI model, and we experiment with Stable Diffusion 1.5 and 2.1. For each input prompt and counterfactual prompt, we generate 48 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Image Comparison</head><p>The primary motivation for employing counterfactuals is to discern the proximity or differences between images generated for a given prompt P and those produced for prompts altered along an axis of bias. This comparison enables us to gauge whether images generated for a specific prompt exhibits bias towards a particular counterfactual. Hence, we propose an image comparison module to compare two sets of images. This module can utilize any existing framework or model for comparing image sets. In our study, we investigate two distinct methods inspired by previous works <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b187">188]</ref>.</p><p>Expanding upon these approaches, we introduce a novel metric termed Concept Association Score (CAS) to quantify the similarity between image sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.1">Method 1: VQA-based concept extraction</head><p>In this approach, we use MiniGPT-v2 <ref type="bibr" target="#b203">[204]</ref>, a recent vision-language model with competitive performance in various VL tasks, in a question-answer format to extract information from generated</p><p>images. An example of this process is illustrated in Figure <ref type="figure" target="#fig_21">5</ref>.3. Given an initial prompt P , we generate a set of questions that are aligned with the axes of bias B that may be present in the images. For commonly occurring axes of bias, such as gender, age, or ethnicity, we hand-design VQA questions that can be used to query each image. For example, if "gender bias" is an axis of bias for a prompt, then we add "What is the gender of the person?" to the set of VQA questions. For other axes of biases, we implement a template question "What is {bias-name} in the image?" where {bias-name} is simply replaced with the type of bias (see Table <ref type="table">5</ref>.2).</p><p>The questions asked for a prompt P and its counterfactuals P cf remain the same.</p><p>We use the VQA setting of MiniGPT-v2 to obtain answers to axis-specific questions, as well as a caption for the image. For frequently occurring axes of bias, we write well-defined questions. These questions are in Table <ref type="table">5</ref>.2.</p><p>All VQA answers for all set of images of P and P cf are combined and pre-processed, to obtain a list of entities that describe the set of images. We measure the occurrence of each entity by calculating its frequency over the answers and captions. The final list of entities, and their frequencies, are considered as the concepts set C that are extracted for a set of images generated by one prompt.</p><p>Ultimately, we have C init = {(c i 1 , w i 1 ) . . .} for the initial prompt, and C cf = {(c cf 1 , w cf 1 ) . . .} for a counterfactual prompt, where c is a concept described in natural language, and w is the frequency of that concept in the VQA answers across the given set of images.  factual prompt in terms of relevant concepts. CAS VQA uses a concept-level matching algorithm to compare concepts generated for the two sets, as defined in Algorithm 3. This concept-level matching algorithm merges synonym words in C init and C cf , and reduces the concept lists to two histograms of word frequencies for the initial and counterfactual concept sets. Now, we define CAS as the histogram Intersection-over-Union between the frequency (W) of the two sets of concepts:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAS measures the similarity between generated images for the initial prompt and each counter-</head><formula xml:id="formula_22">W ∩ = min(w i , w cf ); ∀ w i ,w cf ∈C init ,C cf (5.1) W ∪ = max(w i , w cf ); ∀ w i ,w cf ∈C init ,C cf (5.2) CAS = i W ∩ i j W ∪ j (5.3)</formula><p>where w i and w cf are the concept frequencies for the same concept in the initial and counterfactual concept sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.2">Method 2: Vision-Language Embedding models</head><p>In this approach, we directly embed all images using CLIP <ref type="bibr" target="#b204">[205]</ref>, and compare each image in the initial prompt set, to every image in the counterfactual prompt set using the cosine similarity metric.</p><p>We then compute CAS CLIP as the mean of the cosine similarity scores, as follows:</p><formula xml:id="formula_23">CAS CLIP = mean [cosine(CLIP (I i ), CLIP (I cf ))] ∀I i ,I cf ∈I P ,I P cf .</formula><p>(5.4)</p><p>In both methods, CAS values range between [0, 1], where 0 indicates no association and 1</p><p>indicates complete matching of the two concept sets. Unlike CAS, which is derived from VQA concepts, CAS CLIP scores are limited in terms of post-hoc explainability as they only rely on image embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Metrics for Bias Evaluation</head><p>CAS scores measure the similarity between the input prompt and each counterfactual for a given axis of bias. If we have K counterfactuals generated for a bias axis b, then we obtain a distribution of K CAS scores as follows:</p><formula xml:id="formula_24">CAS b K = [CAS 1 ; . . . ; CAS k ] b (5.5)</formula><p>If this distribution of CAS scores is uniform, i.e., each counterfactual image set is equally similar to the initial set, it indicates that there high diversity in the initial set and low bias along that axis.</p><p>Conversely, if this distribution is skewed towards one counterfactual, it indicates that the initial set is heavily biased towards that set. Therefore, a measure of variability in a distribution of CAS scores can allow us to quantify the amount of bias along an axis, and compare the degree of bias along one axis against another. To that end, employ a statistical measure, Mean Absolute Deviation (MAD), for bias evaluation.</p><p>Moreover, we propose two qualitative metrics, Top-K concepts, and Axis-aligned Top-K concepts, attempt to provide post-hoc explanations about commonly occurring concepts in images generated for a prompt, when VQA-based concept extraction is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.1">Quantitative Metric: Mean Absolute Deviation (MAD)</head><p>MAD is used to measure the degree of bias with respect to a bias-axis. MAD is given as</p><formula xml:id="formula_25">MAD = 1 K K i=1 |CAS i -CAS K | (5.6)</formula><p>where K is the number of counterfactuals for a bias axis b ∈ B, CAS k represents the weight represented for corresponding to the k th counterfactual, and CAS K is the mean of the scores. We normalize MAD against the MAD score of the most skewed distribution with length K (where all scores are 0, except a single 1) in order to be able to compare bias axes even when K varies between them. Additional details regarding this are in A.2</p><p>Once normalized, MAD scores are between [0,1], where a low score suggests that the images generated for the initial prompt exhibits relatively low bias along a specific axis, and a high score indicates a strong association between the initial prompt and one counterfactual, indicating a higher likelihood of bias in the images. We illustrate the behaviour of MAD in Figure <ref type="figure" target="#fig_21">5</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.2">Qualitative Measures</head><p>In addition to our quantitative metrics, we also provide qualitative concept-based explanations (for the VQA-based method) to reason about the measured change in concepts between the initial set and the counterfactual set using two simple qualitative metrics defined below. Our qualitative metrics are:</p><p>• Top-K concepts. For a given input prompt, the Top-K concepts show the most commonly occurring concepts in images generated for a prompt.</p><p>• Axis-aligned Top-K concepts. The Axis-aligned Top-K concepts show the most frequently occurring concepts in a given image for a specific bias axis. To calculate this measure we extract concepts from questions specific to a bias axis and sort them in order of frequency over the image set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dataset</head><p>Predefined prompts for gender stereotypes in occupations. In order to evaluate our method against existing methods like T2IAT <ref type="bibr" target="#b100">[101]</ref>, DALL-Eval <ref type="bibr" target="#b101">[102]</ref>, and Stable Bias <ref type="bibr" target="#b187">[188]</ref> that studied gender stereotypes in occupational images generated by TTI models, we use pre-defined prompts for 11 occupations, including "computer programmer", "elementary school teacher", "architect" and others.</p><p>These prompts follow the format ''A photo of a &lt;occupation&gt;'', with &lt;occupation&gt; representing one of the 11 occupations. We also create male and female counterfactuals mirroring the ones used by T2IAT, and generate 48 images for each set using Stable Diffusion 1.5 and Stable Diffusion 2.1.</p><p>Varied Text Prompts for Evaluation. As our method is capable of using any input prompt, we create a set of 100 prompts to comprehensively assess our method's performance in bias evaluation, including: (1) Creative Prompts: This subset includes diverse and imaginative prompts meticulously written to evaluate our method thoroughly. Some examples are "astronauts cooking dinner on the moon" and "a boy at a museum". (2) Prompts from DiffusionDB: We also sample prompts from DiffusionDB <ref type="bibr" target="#b205">[206]</ref>, a database of 2.3M distinct human-generated TTI prompts across two sets, 2M and Large. These prompts undergo pre-processing where we extract the most descriptive substrings.</p><p>Let's say you are designing an advertisement for a university. You would like to include an AI generated photo of "A girl at a university". Here are some of the images you generate:</p><p>You can immediately observe some biases in these images: the ethnicity, background content, and the western-style clothing.</p><p>Now, let's apply TIBET to this prompt. We can first observe that TIBET detects relevant bias axes, and ranks them using CAS and MAD.</p><p>TIBET also identifies the same biases as humans, in addition to some new ones! When using TIBET with VQA-based scores, we can dive deeper into the concepts associated with each axis of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let's analyze the Clothing Bias axis.</head><p>We observe that our TII model failed to include culturally relevant dresses. We can confirm how Top K Axis-Aligned concepts help explain these differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Setting Clothing Bias</head><p>Finally, let's also consider Background Setting, which is an incidental correlation (not a societal bias).</p><p>In the initial set, the background setting is often a traditional university background, whereas images for the counterfactual show more diverse possibilities for what a university could be represented as. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiments</head><p>The section can be broadly divided into three parts. Firstly, in Section 5.5.1 and Section 5.5.2, we utilize our approach to examine biases in various prompts. Secondly, in Section 5.5.3, we analyze how biases in VQA models impact our approach. Thirdly, in Section 5.5.4, we present human evaluations aimed at assessing the alignment between human judgments and our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Qualitative Results</head><p>We show an example of analysing biases in images generated by the prompt "a philosopher" using TIBET with Stable Diffusion 2.1, in Figure <ref type="figure" target="#fig_21">5</ref>.6. In (a), we show images generated for the initial prompt, as well as Top-K concepts and Axis-Aligned Top-K concepts for Cultural Bias. These concepts are ordered by their frequency. In (b), we show CAS (in green) and MAD (in orange) scores of all counterfactuals across all axes. The MAD score tells us which bias may be stronger in the initial set of images. This plot provides a birds-eye view of the biases that are most prominent in the initial set (here, we notice that cultural, gender and facial expression biases are most prominent, as they have higher MAD scores), and study the CAS scores of each counterfactual for every bias axis. Finally, in (c), we show an example of one counterfactual that has a low CAS score, and show that the images and concept frequencies can be compared to those in (a). By observing concepts in the Top-K concepts list, we can validate what the metrics tell us. Here, "man" and "serious" are the only two concepts that remain in the top five, indicating the large difference between the two image sets, explaining low CAS score. Furthermore, comparing Axis-aligned Top-K Concepts helps us understand the significant differences in cultural depictions by the TTI model for the initial prompt, compared to the counterfactual, where the initial prompt has mostly "Greek" philosophers, whereas the counterfactual has "Native American" philosophers.</p><p>Another example is shown in Figure <ref type="figure" target="#fig_21">5</ref>.5, where we illustrate how TIBET helps users identify and understand biases in images generated by a Text-to-Image model. For instance, given the prompt "A girl at a university," our metric highlights potential biases across different axes. The user can then explore these axes in detail-for example, examining the "clothing" dimension and observing that the model consistently depicts girls in sweaters or dresses, thereby revealing a stereotypical bias embedded in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Measuring Gender Stereotypes in Occupations</head><p>Previous research <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b103">104]</ref> has brought attention to the issue of gender bias in generated images for profession-related text prompts. Building upon these findings, we embarked on a similar investigation to explore gender stereotypes in images generated by TTI models when provided with occupational prompts (as detailed in Section 5.4). In our study, we assess the disparity between the CAS values for male and female counterfactuals. This assessment allows us to determine whether the images for that profession lean male or female, shedding light on potential gender-related biases in the generated images.</p><p>Our analysis of the generated images (in Figure <ref type="figure" target="#fig_21">5</ref>.9 (a) and (b)) indicates that when using Stable Diffusion 1.5, images generated for "elementary school teachers" and "librarians" are female dominated, whereas "announcer," "chef," and most other occupations are male dominated. Stable Furthermore, we can use TIBET across a set of prompts with the same bias axes, to compute the an aggregate measure of bias. We observe an average CAS score of 0.56 for the male counterfactual and 0.44 for the female counterfactual across all 11 occupations, for images from Stable Diffusion 1.5. Whereas with Stable Diffusion 2.1, we get 0.52 and 0.48 for male and female counterfactuals, respectively, indicating lower gender bias in the newer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Sensitivity of Metrics to Errors in the VQA Model</head><p>Our goal with TIBET is to provide an accurate analysis of potential biases in the images generated by a TTI model for a given input prompt, inexpensively and efficiently. We are required to use models such as MiniGPT-v2 to conduct automatic analysis without expensive human annotations. These VL models carry their own biases, and these biases may be propagated to our metrics. Therefore, it is essential to conduct a sensitivity analysis of CAS and MAD scores. For VQA, we do this by simulating errors in answers. We assume IID errors at the image level, and average our rate of change of CAS and MAD across 10 simulation runs for 30 random prompts in our dataset, across all bias axes.</p><p>In Figure <ref type="figure" target="#fig_21">5</ref>.7, we show the results of our sensitivity analysis. We recognize that CAS and MAD do propagate error in VQA into our scores, but do so at a rate lower than the original rate of error.</p><p>As we use a large set of images in each set that we compare, the top concepts from VQA remain less affected by the per-VQA errors. For an error rate of 18% (established in User Study 3) in VQA, we observe that CAS only changes by 4.73% and MAD by 13.11%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Human Evaluation</head><p>User Study 1: Evaluating Dynamically Generated Bias Axes. We conduct a user study to evaluate the concurrence of axes of bias chosen by human participants and those generated by LLMs across 100 input prompts. A high level of agreement serves as an indicator of the effectiveness of LLMs in generating bias axes that are both contextually relevant and aligned with human perspectives.</p><p>Participants are tasked to identify if, for a given prompt, an axis of bias (e.g., gender) may potentially cause societal or incidental biases in the generated images. For each prompt, we present 10 axes of bias, including the ones that the LLM generated, and the rest from a random sample of biases.</p><p>Each question is answered by five MTurk workers. Further details about the setup of this study are provided in Appendix B.1.</p><p>We perform two experiments, measuring precision and recall across all axes (overall) and specifically for commonly occurring societal biases (societal). The results in Table <ref type="table">5</ref>.3 reveal a high precision of 0.90 in both experiments, showing human agreement with the LLM on generated biases.</p><p>In the overall case, a recall of 0.54 suggests that LLMs capture only a subset of human-indicated biases. However, in the societal case, a recall of 0.87 demonstrates GPT-3's strong ability to identify harmful societal biases in prompts. This user study examines the alignment between human bias ratings and MAD scores for a set of images generated from the initial prompt. Due to the subjectivity of identifying biases, we focus solely on a predefined set of societal biases (refer to Appendix B.2). We present participants with 10 randomly sampled images for each input prompt from our dataset of 100 prompts that may contain two or more societal biases. They rate bias presence on a 1-5 scale. With 10 participants per question, we assess the Spearman correlation between the median value of human bias ratings and our MAD scores. We also calculate Top-1 and Top-2 accuracy for prompts with three or more societal biases.</p><p>Our results in Table <ref type="table">5</ref>.4 indicate that there is a positive correlation of +0.51 between human ratings and our MAD score, indicating that our model is aligned with humans in ranking societal biases. We also compare both our image comparison methods against Bipartite Matching of concepts with cosine similarity of CLIP text embeddings. The poor ranking correlation and lower Top-K accuracy of CLIP also demonstrates the benefits of using CAS over CAS CLIP . This is in line with recent works <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b206">207]</ref>. We also observe a similar dip when we use CLIP scores in bipartite matching, likely due to incorrect matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4.2">User Study 3: Evaluating MiniGPT-v2</head><p>We conduct a third user study to evaluate the quality of the MiniGPT-v2 model for detecting concepts.</p><p>While each set of images can have several different concepts, we are primarily interested in the Top-K Overall concepts across all images, as those are most influential in calculating CAS scores.</p><p>Accordingly, we set up an MTurk task, where we ask 3 participants to select all concepts, from a list of 10 concepts, that are relevant to the given set of 10 randomly sampled images. We create 80 such tasks, where we first sample an initial prompt or any counterfactual prompt from our dataset, then sample 10 images for the prompt, and obtain the top 5 concepts that are present in the set of images (from C init or C cf ). We then also add 5 other random concepts that are not predicted by  to make the list of 10 concepts for that set. Each HIT is $0.10, and all workers are from the US. A HIT (Human Intelligence Task) refers to a small unit of work assigned to human annotators, typically through a crowdsourcing platform like Amazon Mechanical Turk.</p><p>We calculate the accuracy of our model based on the concepts that humans select. Of all the concepts that a human said yes to, our model selected 82.8% of those concepts (higher is better). This indicates that MiniGPT-v2 is fairly accurate at detecting useful concepts from the images, assuming we observe over a large set of images (48 images in our case). Moreover, for all the concepts that were randomly selected for this task (that our model did not produce for a set of images), humans only select 7.4% of concepts (lower is better).</p><p>For our sensitivity analysis, we roughly estimate a VQA error of 18%, assuming that our model missed about 17-18% of the concepts that humans had also said yes to. Because this is only a rough estimate, we report numbers for higher and lower error rates in our sensitivty analysis graph in Figure <ref type="figure" target="#fig_21">5</ref>.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.5">Comparing CLIP and VQA for Image-Based Bias Measurement</head><p>A key component of our framework involves comparing generated images across counterfactual prompts to measure semantic differences that may reflect bias. While prior works have often used CLIP embeddings to compare image sets holistically, we investigate whether incorporating more localized, attribute-specific cues through Visual Question Answering (VQA) provides better alignment with human judgment.</p><p>To evaluate this, we compare the results of our CLIP-based and VQA-based image comparison methods on a set of prompts exhibiting known biases. In particular, we examine cases where both racial and gender biases may be present. As shown in Figure <ref type="figure" target="#fig_21">5</ref>.8, human annotators consistently rank racial bias as more prominent than gender bias in the given image sets. Our VQA-based method better captures this distinction, whereas CLIP-based similarity scores often blur such attribute-specific differences due to their global, embedding-level nature.</p><p>This observation aligns with our findings from User Study 2, where participants noted that perceived biases in image generations often stemmed from distinct visual cues-such as skin tone, clothing, or hairstyle-rather than overall scene semantics. Since VQA enables targeted queries on specific visual elements (e.g., "What is the skin color of the person?"), it allows for more finegrained bias assessments that mirror human reasoning. This underscores the value of incorporating VQA-based methods for bias detection in TTI models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Applications</head><p>Having the capability to detect biases and provide concept-level explainability for any input prompt enables several downstream use-cases. Two downstream applications of TIBET are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Mitigating Biases in TTI models</head><p>While approaches like ITI-GEN <ref type="bibr" target="#b189">[190]</ref> have shown to decrease bias along a given axis, they are incapable of identifying what bias axes and what images to train on. Our approach can complement these approaches by automatically identifying relevant bias axes and producing counterfactual images for these axes. Further, our proposed metrics also measure the degree of bias changes along these axes. We conduct one such experiment where we use our method along with ITI-GEN to mitigate gender biases in occupational prompts. For each occupational prompt, we already generate male and female counterfactual images along the gender axis. These counterfactual images are used as input reference image sets in ITI-GEN. Post-training ITI-GEN, we generate 48 new images for each occupational prompt. As illustrated in Figure <ref type="figure" target="#fig_21">5</ref>.9(c), the difference in CAS values for the majority of  what we observed based on our CAS scores in Figure <ref type="figure" target="#fig_21">5</ref>.9(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Discussion and Conclusion</head><p>We propose TIBET, an approach to automatically detect and evaluate biases present in images generated by TTI models in an explainable manner. Our approach has the potential to address previously unexplored issues related to bias in TTI models, including reasoning about intersectionality of different bias axes and comprehensive and automated bias mitigation. Our hope is that TIBET can serve as the foundation for future research in the these directions. While our approach holds significant promise, it is essential to acknowledge the limitations of our model. Despite designing the tool with the ultimate aim of reducing biases, we have identified several flaws within our model.</p><p>This section is dedicated to a thorough exploration of these limitations.</p><p>It is important to note that, while these limitations exist, we view this work as an initial step towards conducting comprehensive bias evaluations for any prompt for TTI models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.1">Biases in Language Models (LLMs)</head><p>Our approach rests on the assumption that Large Language Models (LLMs) excel at detecting biases in prompts for Text-to-Image (TTI) models. While our user studies validate that humans agree with potential bias axes, there is always the possibility that some of the generated axes may not be meaningful, or may be a result of hallucination. Even though solutions like human intervention</p><p>and Automated External Sources (AES) filtering can mitigate these issues, the approach cannot be foolproof. Further research and development are necessary. While LLMs may have their own issues, they are the fastest and most capable way to identify biases in any prompt, and the task that TIBET does would take large amounts of time and money to conduct manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.2">Interpretation of Bias Axes</head><p>Another interesting challenge is that LLMs may generate a completely valid yet orthogonal set of bias axes compared to humans. While LLMs offer diversity in generating bias axes, we advocate for human intervention to validate these axes of bias. We also make clear that the interpretation of results is ultimately up to humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.3">Biases in Vision Language Models (VLLMs)</head><p>Utilizing Vision-Language Models (VLLMs) for image comparision introduces an additional dimension of bias. We have observed that current models do not perform perfectly, and require significant improvement in their image concept understanding capabilities. Additionally, VLLM models need more comprehensive training or fine-tuning on concept detection tasks to generate more relevant concepts. Automation of predefined questions for VLLMs is also a crucial step for a more comprehensive approach. Ultimately, we have tried to design our metrics to be robust to small errors in VQA, and our sensitivity analysis shows a weaker-than-linear correlation between changes caused by VQA errors on the values of our CAS and MAD scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.4">Challenges in Metric Evaluation</head><p>We propose the use of the Concept Association Score (CAS) and Mean Absolute Deviation (MAD) as metrics and suggest user studies to measure them. However, we lacked a comprehensive dataset with ground truth values to evaluate our metrics. In the future, we aim to conduct a more extensive evaluation on a larger, well-labeled dataset. Finally, our metrics rely on diverse counterfactuals, and make the assumption that the TTI model being evaluated is sensitive to the changes (made by the LLM) from the initial input prompt to the counterfactual prompts. TIBET may fail in the rare case when TTI models fail to incorporate the changes made in counterfactual prompts and the image generation may not always be faithful to the counterfactual prompts <ref type="bibr" target="#b207">[208]</ref>. We empirically observe in our qualitative examples that this is not an issue with the Stable Diffusion models we use in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.5">Challenges with Bias-related user studies</head><p>Conducting bias-related user studies is challenging in several ways, including:</p><p>1. As all individuals observe bias differently, teaching participants what bias is, what each type of bias means, and how it is relevant to our task is a challenge. Participants may misunderstand, or ignore the definitions of bias we provide, and rely on their subjective understanding of biases when answering questions. We provide extensive training and qualification tests to reduce subjectivity as much as possible.</p><p>2. Participants may consider societal biases more important (and therefore more likely, as with User Study 2) compared to incidental biases, as incidental biases are not frequently talked about in society and may not be inherently harmful in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER 6</head><p>Using counterfactuals to understand Intersectional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias Dynamics in Text-to-Image Models</head><p>"There is no such thing as a single-issue struggle because we do not live single-issue lives"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audre Lorde</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gen AI Engine</head><p>Generate an image of an old man at a church Generate an image of an asian old man at a church Grimmer Expressions, sitting, Caucasian, faded clothing Looks healthy, smiling, standing, new and more stylish clothing to "An Asian old man at a church"-can lead to significant differences in generated outputs. While the former yields grim, seated Caucasian men, the latter produces smiling, upright Asian men in brighter scenes. This shift illustrates the intersectional nature of generative model behavior, where the addition of a single identity attribute (ethnicity) modifies not just appearance but also emotion, posture, and context-highlighting how social dimensions interact rather than operate independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Introduction</head><p>In the previous chapter, we introduced TIBET, a diagnostic framework that applies counterfactual reasoning to dynamically evaluate bias in text-to-image (TTI) generative models. By leveraging context-specific prompt variations, TIBET enables fine-grained auditing of bias along individual social dimensions such as gender, age, or ethnicity. However, this framework raises a deeper question central to the fairness of generative systems: what happens to other bias dimensions when we intervene on one? In other words, if we mitigate bias along a particular axis (e.g., gender), do other forms of bias-such as age, race, or clothing-remain unchanged, improve, or worsen?</p><p>Consider a simple example: the prompt "an old man at a church." The generated images consistently depict seated, somber white men in shabby clothing. When the prompt is modified to "an Asian old man at a church," the outputs change significantly: the subjects appear happier, more vibrant, better dressed, and are standing. This qualitative shift suggests that altering one axis of identity (ethnicity) influences multiple other attributes, including emotion, body posture, and perceived social status. Such phenomena reveal that biases in TTI models are not merely independent, but often interconnected.</p><p>Answering these questions is crucial for understanding the inner workings of TTI models and for designing mitigation strategies that respect the intersectional nature of social identity. To this end, this chapter focuses on how counterfactual reasoning can be extended beyond axis-isolated evaluation to quantify intersectionality in generative systems.</p><p>We begin by revisiting the concept of intersectionality and its relevance to AI fairness. We then briefly review how TIBET can be adapted to qualitatively surface intersectional effects through prompt-based variations. However, we also highlight a core limitation of TIBET: it lacks a quantitative mechanism for modeling or measuring how different bias axes influence one another.</p><p>To address this gap, we introduce two complementary tools: BiasConnect and BiasGraph. Built upon a novel metric called Intersectional Sensitivity (IS), these tools enable us to quantify and structure the relationships between bias axes. BiasConnect uses counterfactual interventions to populate an Intersectionality Matrix, revealing whether intervening on one dimension improves or harms another. BiasGraph builds on this by transforming the matrix into a causal graph, where directional dependencies between bias axes can be analyzed at scale. Together, these tools offer a principled, interpretable framework for diagnosing intersecting and entangled biases in TTI models-laying the groundwork for future mitigation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Understanding Intersectionality and Its Role in Bias Diagnosis for Generative Models</head><p>Intersectionality is a theoretical framework developed to understand how multiple systems of oppression-such as racism, sexism, classism, and ageism-interact to shape the lived experiences of individuals with multiple marginalized identities. First introduced by legal scholar Kimberlé Crenshaw <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b208">209]</ref>, intersectionality emphasizes that the discrimination faced by, for example, a</p><p>Black woman, cannot be fully understood by analyzing race and gender in isolation. Instead, these identities intersect to produce qualitatively distinct experiences of marginalization, which are more than just the additive effects of each category. This concept has since become foundational in feminist theory, critical race studies, and increasingly in algorithmic fairness and bias analysis in machine learning.</p><p>Within this framework, two major models of interaction have emerged in scholarly discourse <ref type="bibr" target="#b112">[113]</ref>:</p><p>• The additive model, which assumes that disadvantage accumulates linearly across marginalized dimensions (e.g., racism + sexism = double discrimination).</p><p>• The interactive model, which suggests that dimensions of identity interact synergistically, producing novel or amplified effects that cannot be explained by additive logic alone.</p><p>While intersectionality originated in legal and sociological theory, it has increasingly informed the field of algorithmic fairness. Machine learning researchers have adapted its core principles to evaluate fairness in AI systems by measuring performance or representation disparities across intersectional subgroups-e.g., Black women vs. white men-in domains such as classification <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>, natural language processing <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b122">123]</ref>, computer vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b130">131]</ref>, and large language models <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b125">126]</ref>.</p><p>However, most of these approaches align with the additive paradigm, treating subgroup disparities as separable and largely static.</p><p>This chapter departs from that assumption. We adopt the interactive model of intersectionality, using counterfactual interventions to examine how changing one dimension of identity-such as gender-can alter model behavior along another, like age or emotion. This shift is especially critical in Text-to-Image (TTI) generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Generative models and the need for understanding intersectional biases</head><p>TTI models such as DALL-E <ref type="bibr" target="#b209">[210]</ref>, IMAGEN <ref type="bibr" target="#b210">[211]</ref>, and STABLE DIFFUSION <ref type="bibr" target="#b2">[3]</ref> have become powerful tools for synthesizing visual content from textual prompts. Yet these systems often reproduce harmful social stereotypes embedded in their training data <ref type="bibr" target="#b211">[212]</ref>, manifesting biases across dimensions such as gender, race, age, clothing, and setting. Although a growing body of work has sought to evaluate or mitigate such biases <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b105">106]</ref>, most approaches treat each dimension in isolation. For example, gender and race are often audited separately, and mitigation efforts typically focus on a single axis at a time.</p><p>This assumption of axis-independence fails to capture a key behavior of generative models: biases often interact. For instance, attempting to mitigate gender bias in a prompt like "a doctor treating patients" may inadvertently reduce diversity in age or ethnicity, leading to outputs that are gender-balanced but skewed toward young or racially homogenous representations. These shifts suggest that intervening on one dimension can distort others, resulting in unanticipated trade-offs.</p><p>Such entangled effects highlight the need for intersectional bias assessment in TTI models.</p><p>Without tools that account for the interactive nature of identity, we risk designing evaluations or mitigations that improve fairness along one axis while silently worsening it along others. To address this, we explore in this chapter how counterfactual reasoning can be extended to diagnose inter-axis dependencies-that is, how interventions on one attribute (e.g., race) affect distributions over other attributes (e.g., emotion, clothing, posture).</p><p>Through this lens, we introduce two diagnostic tools-BiasConnect and BiasGraph -built on a novel metric called Intersectional Sensitivity . These tools move beyond axis-isolated evaluation, enabling structured, interpretable, and quantitative analyses of how social biases in generative models interact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Using TIBET to Identify Prompt-Level Intersectional Bias</head><p>TIBET is a diagnostic framework, introduced in Chapter 5, for measuring social biases in text-toimage (TTI) generative models using a counterfactual reasoning approach. Unlike prior work that evaluates bias along static, pre-defined axes, TIBET dynamically identifies the bias dimensions most relevant to a given input prompt and uses them to guide a systematic evaluation.</p><p>Given an input prompt P , TIBET first detects and constructs a set of relevant social axes (e.g., gender, age, race, emotion) based on the semantics of P . It then generates a series of counterfactual prompts by varying one axis at a time while holding the rest of the prompt fixed. For each of these prompts-including the original-TIBET uses a black-box TTI model (e.g., Stable Diffusion) to synthesize a corresponding set of images.</p><p>To evaluate the effect of each counterfactual change, TIBET compares the images generated from the original prompt with those generated from each counterfactual. This comparison is done using a combination of vision-language tools: a VQA pipeline to extract interpretable concepts (e.g., clothing, emotion, background), and CLIP embeddings to capture high-level semantic differences.</p><p>These comparisons are quantified using the Concept Association Score (CAS), which measures the directional association between concept distributions across different prompts.</p><p>TIBET supports both quantitative metrics-such as the Mean Attribute Difference (MAD)-and qualitative visualization tools to provide interpretable explanations of bias. The framework enables users to identify which social attributes are disproportionately associated with a given prompt and how these associations shift under controlled counterfactual interventions.</p><p>Intersectionality in "a photo of a pharmacist": When we study counterfactuals for "geographical bias", how does the depicted gender change?  We observe that images generated for a chef in Africa may be depicted outdoors (tree) unlike chefs in other regions of the world.</p><p>Using TIBET, we can study intersectionality by observing counterfactuals along one bias axis, and comparing changes in concepts along another bias axis. This can uncover the interconnectedness between bias axes, showing that modifying one bias may unintentionally amplify biases in other dimensions.</p><p>An illustrative example of these interactions is shown in Figure <ref type="figure">6</ref>.2. The Axis-Aligned Top-K Concepts corresponding to secondary bias dimensions reveal noteworthy insights into the behavior of the TTI model. For instance, the male-to-female ratio is observed to be higher in images generated for prompts like "pharmacist in Asia" and "pharmacist in Africa", whereas the ratio is lower for "pharmacist in Europe"-relative to a neutral prompt baseline. These patterns indicate that gender representations are not fixed but vary significantly depending on geographic modifiers in the prompt.</p><p>A similar trend is visible in Figure <ref type="figure">6</ref>.3, where regional context influences background composition.</p><p>Specifically, prompts like "a chef in Africa" are more likely to be rendered in outdoor settings-e.g., with trees or natural elements-unlike depictions of chefs in other regions, which tend to be shown indoors. These observations highlight how secondary attributes such as location can conditionally shape other visual dimensions, reinforcing the need for intersectional bias evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Moving beyond TIBET for deeper intersectional analysis</head><p>While TIBET provides a powerful and flexible framework for dynamically diagnosing social bias in text-to-image models, it has important limitations when it comes to evaluating and reasoning about interactions between multiple bias dimensions.</p><p>First, TIBET is fundamentally a visual diagnostic tool. It allows users to observe how varying one social attribute (e.g., race or gender) affects the visual content generated by a model, and to qualitatively detect cases where changes in one axis appear to influence others. For example, as discussed earlier, modifying ethnicity in a prompt may unintentionally alter associated attributes such as emotion or clothing. However, these interactions are primarily surfaced through visual comparisons or concept ranking, rather than being explicitly measured or formalized.</p><p>Second, TIBET lacks a quantitative mechanism to assess whether and how biases across different dimensions are causally entangled. That is, while TIBET can reveal that two attributes change together, it does not offer a structured way to determine the direction, magnitude, or consistency of these interactions across prompts or datasets. There is no formal metric to evaluate whether mitigating one bias (e.g., gender) improves, worsens, or leaves unchanged the fairness of another (e.g., age).</p><p>Third, TIBET does not provide a scalable or generalizable structure for modeling bias dependencies across multiple axes. Each evaluation is prompt-specific and axis-specific, making it difficult to reason globally about the underlying entanglement structure of the model's latent space.</p><p>Finally, TIBET is limited to qualititative evaluation and offers no guidance for how to use observed interactions in downstream tasks such as bias mitigation.</p><p>To address the challenges outlined above and better understand intersectionality in generative models, we introduce two frameworks in this chapter: BiasConnect and BiasGraph. These methods aim to: (1) quantify how different bias dimensions interact, using a new metric called Intersectional Sensitivity, and (2) represent these interactions as graphs that support model auditing and guide mitigation strategies. While TIBET visualizes interactions between bias axes, these approaches take a step further by directly quantifying these interactions, enabling a more precise analysis of model behavior and providing concrete tools for understanding intersectional biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">BiasConnect: Quantifying Bias Relationships</head><p>To understand how biases in text-to-image (TTI) models influence one another, we propose BiasConnect, a framework that evaluates societal biases in TTI models while accounting for intersectional relationships, rather than treating bias dimensions independently. BiasConnect identifies and quantifies how interventions on one bias axis affect others, revealing cases where mitigating one bias may inadvertently worsen another. Unlike prior methods, our approach supports prompt-level intersectional analysis and enables aggregate evaluations across a set of prompts. This allows us to examine how model architecture, training data, or objectives contribute to bias entanglement. Given an input prompt, BiasConnect uses counterfactual interventions to measure how changes along one bias axis influence others in the generated outputs. These interactions are quantified using a metric called the Intersectional Sensitivity Score (Intersectional Sensitivity ), which captures the direction and magnitude of such effects. This section describes the BiasConnect framework in detail and discusses the utility of this approach for intersectional bias analysis in TTI models.</p><p>Figure <ref type="figure">6</ref>.4 illustrates a representative example where BiasConnect detects a negative interaction between two bias axes. In this case, we examine a prompt related to musicians generated by the Flux-dev <ref type="bibr" target="#b212">[213]</ref> model. The figure shows that increasing gender diversity leads to a skewed age distribution-specifically, a reduction in the representation of older individuals. This reflects a common failure mode in TTI models, where mitigating bias along one axis (e.g., gender) unintentionally exacerbates bias along another (e.g., age). Such cases underscore the need for intersectional analysis tools like BiasConnect, which make these trade-offs visible and quantifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Approach</head><p>The objective of BiasConnect is to identify and quantify the intersectional effects of intervening on one bias axis (B x ) to mitigate that bias, on any other bias axis (B y ). BiasConnect works by We use a counterfactual-based approach to measure pairwise causality between bias axes. For dependent axes, we measure the causal effect, estimating how bias mitigation on one axis impacts another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.1">Counterfactual Prompts &amp; Image Generation</head><p>Given an input prompt P and bias axes B = {B 1 , B 2 , . . . , B n }, we generate counterfactual prompts {CF 1 i , . . . , CF j i } for each bias B i ∈ B. The original prompt P and its counterfactuals are then used to generate images with the TTI model to measure intersectional effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.2">VQA-based Attribute Extraction</head><p>To facilitate the process of extracting bias-related attributes from the generated images, we use VQA.</p><p>This is inspired by previous approaches on bias evaluation, like TIBET and OpenBias <ref type="bibr" target="#b213">[214]</ref>, where a VQA-based method was used to extract concepts from generated images. Following TIBET, we use MiniGPT-v2 <ref type="bibr" target="#b203">[204]</ref> in a question-answer format to extract attributes from generated images.</p><p>For the societal biases we analyze, we have a list of predefined questions similar to TIBET corresponding to each bias axis in B, and each question has a choice of attributes to choose from. For example, for the gender bias axis, we ask the question " <ref type="bibr">[vqa]</ref> What is the gender (male, female) of the person?". Note that every question is multiple choice (in this example, male and female are the two attributes for gender). For datasets where counterfactuals are dynamically generated (e.g. TIBET dataset), an LLM-generated set of questions is used instead. The questions asked for all images of prompt P and its counterfactuals CF j i remain the same. With the completion of this process, we have attributes for all images, where each image has one attribute for each bias axis in B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.3">Computing Intersectional Sensitivity</head><p>Our objective is to understand how the impact of interventions on B x affects B y in a positive or negative direction concerning an ideal distribution. To address this, we propose a metric that quantifies the impact of bias mitigation on dependent biases with respect to an ideal distribution.</p><p>Defining an Ideal Distribution. We first define a desired (ideal) distribution D * , which represents the unbiased state we want bias axes to achieve. This can be a real-world distribution of a particular bias axis, a uniform distribution (which we use in our experiments), or anything that suits the demographic of a given sub-population.</p><p>Measuring Initial Bias Deviation. Given the images of initial prompt P , we compute the empirical distribution of attributes associated with bias axis B y , denoted as D init By . We then compute the Wasserstein distance between this empirical distribution and the ideal distribution:</p><formula xml:id="formula_26">w init By = W 1 (D init By , D * ) (6.1)</formula><p>where W 1 (•, •) represents the Wasserstein-1 distance. The Wasserstein-1 distance (also known as the Earth Mover's Distance) between two probability distributions D 1 and D 2 is defined as:  . Figure <ref type="figure">6</ref>.7: Sensitivity analysis on BiasConnect. We evaluate the robustness of our approach by analyzing the impact of VQA errors and the effect of the number of images on Intersectional Sensitivity .</p><formula xml:id="formula_27">W 1 (D 1 , D 2 ) = inf γ∈Π(D 1 ,D 2 ) E (x,y)∼γ [|x -y|] (6.</formula><p>These high correlations demonstrate that our framework effectively predicts the impact of counterfactual-based interventions on secondary bias dimensions without requiring actual mitigation. This capability allows practitioners to anticipate fairness trade-offs and make informed decisions about which bias axes to mitigate, how those interventions may affect others, and whether the expected benefits are worth the cost. Our approach provides a practical and interpretable means of assessing intersectional bias entanglement in TTI models, offering a valuable pre-mitigation diagnostic tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.4">Robustness of BiasConnect</head><p>We analyze the robustness of our method by evaluating the impact of number of images (Figure. drop, an removing 32 images (66.6%) yields a 31.3% drop. This sub-linear impact suggests that TTI models often generate similar bias distributions (e.g., always depicting nurses as females), preserving overall trends despite fewer images. Therefore, our approach is robust to moderate reductions in image count, but very small sets of images will significantly affect Intersectional Sensitivity values.</p><p>To test the robustness over VQA errors, we randomly change the VQA answers to a different answer (simulating an incorrect answer), from 5% to 40% of the time. We observe that even with low error rates of 5% and 10%, Intersectional Sensitivity values change by 10% and 17.3% respectively.</p><p>Here, the impact is compounded twice, because an error can skew the distribution away from one counterfactual towards another, and that a 5% error causes 13,478 answers out of a total of 269,568 answers to be changed, which is substantial. Nonetheless, we note that this impact remains linear.</p><p>As VQA models improve, achieving low error rates for robustness becomes practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">BiasGraph: From Matrices to Causal Graphs</head><p>While the Intersectional Sensitivity provides a useful measure of how one bias dimension influences another, the resulting intersectional matrices do not convey the underlying causal structure between bias axes. In other words, while these matrices quantify the strength of association or interaction, they do not reveal whether one axis is causally dependent on another. To address this, we extend our framework beyond matrix-based analysis and propose a graph-based approach that captures directional dependencies among bias dimensions.</p><p>As before, we begin by generating prompt-level counterfactuals and synthesizing corresponding images using a TTI model (Section 7.4.2). We then apply a visual question answering (VQA) pipeline to extract interpretable bias-related attributes from the generated images (Section 7.4.2). Since the Intersectional Sensitivity can be interpreted analogously to a causal treatment effect-measuring how an intervention on one axis affects another-we use it to guide a causal discovery process.</p><p>Specifically, we perform pairwise conditional independence testing between bias axes to identify statistically significant dependencies (Section 6.6).</p><p>This graph-based extension offers several advantages. First, it moves beyond symmetric correlation to uncover asymmetric, directional relationships, helping us identify which bias axes act as "sources" (i.e., interventions here propagate effects) and which act as "sinks" (i.e., axes affected by others). Second, we visualize this information using a directed graph structure, rather than a flat matrix, improving interpretability in high-dimensional settings. Third, we apply this approach to measure intersectional dependence only among strongly connected bias axes, ensuring that the structure reflects meaningful entanglement rather than noise.</p><p>Finally, we highlight several potential uses of this graph-based analysis. It allows for comparative audits across multiple TTI models, helps identify optimal mitigation paths by targeting upstream bias sources, and reveals how bias interactions encoded in training data or real-world prompts may be altered-or even amplified-in generative model outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Pairwise Causal Discovery</head><p>Given an initial set of bias axes B, we define an intersectional relationship between a pair of biases Next, we refine these relationships by extracting only statistically significant ones. This ensures that only strong dependencies between different bias pairs are retained. We apply conditional independence testing using the Chi-square (χ 2 ) test, pruning bias pairs with respect to B x if their p-value exceeds a predefined threshold (p-value&gt; 0.0001). Bias pairs with a p-value below this threshold are considered strongly dependent, indicating that intervening on B x results in a significant change in the other bias axis. This process is applied iteratively for all bias axes. This step is referred to as Pairwise Causal Discovery, and it returns a set of bias pair relationships where mitigating along one bias axis has led to a strong change in another bias dimension. Intersectional Sensitivity is used as weights for these edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.1">Visualization</head><p>Following the process above, we have a set of pairwise causal relationships for all significant intersectional bias pairs B x → B y . Furthermore, each pair B x → B y has an Intersectional Sensitivity score to quantify the intersectional effects. There are many ways to represent these pairwise relationships, including building an n × n matrix, or a graph with n nodes and directed edges that represent the relationships between these nodes.</p><p>A usermay want to understand all important intersectional effects together. In order to do a comparative analysis of intersectionality across models over a dataset of prompts, we perform an aggregation step. For the 26 occupation prompts, we first start by using counterfactuals and VQA to identify attributes over all bias axes in B. Now, in the Causal Discovery step, we build contingency tables that aggregate attributes over all CF prompts across all the occupations. For example, when considering the intersectional relationship Gender → Age, we consider all images for male occupation and female occupation for all occupations for the rows of the contingency matrix, and count over the Age attributes young, middle-aged, old to find the overall global distribution. This gives us the global contingency table for any bias pair. We follow the steps in Sec. 6.6 to obtain this list of bias intersectionality relationships that are significant. Next, in order  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.2">Studying Real-World Biases</head><p>BiasGraph can also be used to compare the distribution of images generated by TTI models against real-world data, offering insights into how generative models may distort or amplify real-world bias structures. Instead of assuming a uniform target distribution for computing bias sensitivity, we extend our framework by using empirical distributions derived from real-world image datasets as the reference. This allows us to evaluate whether a TTI model's internal bias dynamics align with those observed in the real world.</p><p>Given a prompt P (e.g., "A computer programmer"), let B = [B 1 , B 2 , ..., B n ] denote the set of relevant bias axes (e.g., gender, age, race). For each axis B y , we define:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• D real</head><p>By : the real-world distribution of B y , obtained from a curated image dataset or known statistics.</p><p>• D TTI By : the distribution of B y attributes in the TTI-generated images.</p><p>We first measure the initial bias deviation by computing the Wasserstein-1 distance between these two distributions: where a higher I signifies greater intersectional bias amplification and entanglement, while a lower value suggests closer alignment with real-world distributions.</p><formula xml:id="formula_28">w init By = W 1 (D TTI By , D real By ) (6.</formula><p>To illustrate this analysis, we curated a small dataset comprising 48 real-world images of computer programmers, as well as 48 male and 48 female programmer images sourced from the internet. We then compared the intersectional dependencies in this real-world dataset with those in images generated by Stable Diffusion 3.5 for the same prompt. As shown in Figure <ref type="figure">6</ref>.10, in real-world data, gender diversification positively influences body type diversity. However, in Stable Diffusion 3.5, gender not only impacts body type negatively-reducing its diversity-but also influences emotion, revealing a divergence from real-world relationships. Such discrepancies highlight how TTI models may amplify or alter real-world bias interactions, reinforcing the importance of intersectional auditing with tools like BiasGraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Conclusion</head><p>Our study proposes a tool to investigate intersectional biases in TTI models. While prior research has explored bias detection and mitigation in generative models, to the best of our knowledge, no previous work has focused on understanding how biases influence one another. We believe our work makes a significant contribution by enabling a more nuanced analysis of bias interactions. Beyond academic research, BiasConnect and BiasGraph have practical applications, including comparing biases dependencies learned across different models, establishing empirical guarantees for mitigation, and determining optimal mitigation approaches that account for intersectionality. We hope that this tool will facilitate more informed decision-making for AI practitioners, policymakers, and developers, ultimately leading to more equitable and transparent generative models.</p><p>While BiasConnect and BiasGraph provides a valuable framework, it represents only an initial step toward a more comprehensive causal approach to understanding intersectionality. Our current setup does not allow us to reason about indirect causal effects, or develop an optimal bias mitigation strategy that utilizes our tool to mitigate multiple biases simultaneously. Addressing these challenges presents an important avenue for future research. We also need to develop mitigation strategies that consider these interdependencies.</p><p>Ethical Considerations. We acknowledge that the presence of biases in generative AI models can lead to real-world harms, reinforcing stereotypes and disproportionately affecting marginalized groups. Our tool is intended to provide researchers and practitioners with a means to better understand and mitigate these biases, rather than to justify or amplify them. Additionally, we recognize that bias analysis can be sensitive to the choice of datasets, evaluation methods, and experimental assumptions, and we encourage future work to refine and expand upon our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Introduction</head><p>In show young women, we might end up reinforcing age bias or narrow ideas about gender roles, even while trying to be fair.</p><p>First, these models are inherently context-sensitive: a term like "man" or "doctor" cannot be mitigated in isolation, as its meaning and visual portrayal depend on surrounding concepts and scene structure (e.g., "a confident man leading a boardroom meeting" versus "an old man praying alone in a church"). Fairness in TTI is difficult to measure directly-the outputs are not discrete predictions but high-dimensional images, where bias must be inferred through proxy classifiers or semantic cues. These models are highly underconstrained and susceptible to non-linear prompt perturbations:  demographic concepts and ensure that fairness is maintained across a range of plausible phrasings, genres, and prompt templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Robust to Bias Intersections</head><p>Biases in text-to-image models often don't appear alone. A prompt that seems to reflect gender bias may also involve race, age, or profession in ways that are deeply connected. To truly reduce bias, we can't just look at one category at a time. We need methods that examine how different biases interact and influence each other. This means going beyond simple checks that look at only one type of bias. Instead, we need tools like causal graphs, which show how one bias might affect another, and sensitivity matrices, which help identify which combinations of features lead to biased results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Prioritizes user needs Fairness is not a universal constant-it varies by context, audience, and intention. A TTI model deployed in education may require different fairness constraints than one used in entertainment or advertising. As such, any mitigation strategy must be configurable.</p><p>Users should be able to encode their ethical goals and fairness priorities through an explicit, interpretable interface-such as a priority vector that governs which axes of bias are most important, and how trade-offs should be managed when fairness improvements along one dimension risk degrading another. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Proposed Solution</head><p>Taking into account the challenges outlined above, we propose InterMit a modular and training-free framework that addresses intersectional biases during the mitigation process. InterMit is designed to meet all the key requirements discussed in the previous section. First, it is context-aware and prompt-sensitive mitigation operates at the prompt level using counterfactual perturbations and causal sensitivity analysis, allowing it to adapt to the nuanced semantics of diverse generation tasks. Second, it is intersectionally robust, leveraging causal graphs from BiasConnect to diagnose entangled biases and prioritize interventions that avoid collateral harm across social dimensions. Third, InterMit is user-aligned and configurable through its use of a priority vector, which enables users to select specific bias axes to focus on. Fourth, it is fully training-free and modular, requiring no access to the TTI model's internals and can be integrated on top of any bias mitigation approach. Fifth, it is trade-off aware and transparent, quantifying and visualizing how mitigation along one axis influences others-thus alerting users when fairness improvements come at the cost of unintended harms. Sixth, it is flexible to different definitions of fairness and allows users to choose ideal distribution for each dimesnion of bias making and explicitly encode trade-offs according to their own fairness goals.</p><p>In our evaluation, InterMit outperforms existing methods by mitigating biases more effectively, producing higher-quality images, and requiring fewer mitigation steps. Moreover, unlike other methods, it can handle a larger number (&gt; 3) of bias axes and alerts users when mitigation along one axis adversely affects others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Methodology</head><p>This section introduces InterMit, an iterative strategy for mitigating intersectional biases. We begin by outlining the general input framework presented to the user (Section 7.4.1). Next, we briefly describe how our mitigation algorithm builds on ideas from previous chapters, specifically leveraging BiasConnect and the Intersectional Sensitivity metric as its foundation (Section 7.4.2. Finally, we explain the full mitigation process, which is detailed in Algorithm 4 (Section 7.4.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">User Interface</head><p>Given an input prompt P and a Text-to-Image (TTI) model M , our approach allows users to control how bias is mitigated based on their own fairness goals. The process begins by selecting a subset of relevant bias dimensions B * ⊆ B, where B = {b 1 , b 2 , . . . } is a larger set of possible bias axes.</p><p>This full set of dimensions can either be predefined or generated dynamically using a method like TIBET. For each selected bias dimension b i ∈ B * , the user specifies an ideal distribution D * i . This distribution represents how the user believes the model should behave if the bias were fully mitigated along that axis. In addition, the user assigns a priority score to each selected bias dimension through a priority vector p, where each value indicates the relative importance of that dimension. The vector is normalized such that |p| 1 = 1, ensuring the weights are interpretable and comparable.</p><p>Example : Suppose the input prompt is: P : "A CEO giving a presentation".</p><p>• The system offers a set of possible bias dimensions: B = {gender, race, age}.</p><p>• The user selects B * = {gender, age} as the relevant biases to address.</p><p>• For gender, the ideal distribution is defined as D * gender = [50% male, 50% female].</p><p>• For age, the ideal distribution is defined as D * age = [30% young, 40% middle-aged, 30% older adults].</p><p>• The user then defines a priority vector p = [0.7, 0.3], indicating that gender bias is more important to mitigate than age bias in this context.</p><p>This setup allows the user to specify which biases to address, define what fairness should look like along each axis, and determine acceptable trade-offs based on their specific needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Understanding Bias Interactions using BiasConnect</head><p>Once the user has defined their bias priorities, we use BiasConnect to analyze how different bias dimensions interact. This step allows the system to assess whether mitigating one type of bias (e.g., gender) will impact other types (e.g., age or race), either positively or negatively. It helps uncover dependencies that can inform more effective mitigation strategies. Notably, if the user selects only a single bias axis, this step is skipped, as intersectional effects are not relevant in that case.</p><p>BiasConnect is designed to identify and quantify how mitigating one bias axis, denoted B x , affects another axis B y . This enables the evaluation of intersectional dependencies between biases. The approach works by systematically modifying prompts along specific bias dimensions and analyzing how these interventions change the distributions of other attributes in the generated images.</p><p>Counterfactual Prompts &amp; Image Generation. Given an input prompt P and a set of bias axes Quantifying Intersectional Sensitivity. To measure how intervening along one bias affects another, we had defined a metric called Intersectional Sensitivity in the previous chapter, which captures the directional effect of mitigating B x on B y . This involves the following steps:</p><formula xml:id="formula_29">B = {B 1 ,</formula><p>1. Measuring Initial Bias. For the original prompt P , we compute the empirical distribution of attributes for axis B y , denoted D init By , and measure its deviation from the ideal using the Wasserstein-1 distance:</p><formula xml:id="formula_30">w init By = W 1 (D init By , D * ) (7.1)</formula><p>We normalize this value to obtain Eq. 6.3. This is denoted by τ = ⟨w init B * , p⟩ and measures the overall bias of the model on B * at any timestep. We proceed to mitigation if τ is greater than a threshold ϵ.</p><p>To choose which bias axis to mitigate on, we extract the submatrix S ′ ∈ R n×|B * | consisting of the relevant columns from S obtained using BiasConnect. For each row s ′ i of S ′ , we compute a similarity score γ i = ⟨s ′ i , p⟩, which quantifies the alignment between the i-th intersectional bias and the desired direction of mitigation. The bias axis i * = arg max i γ i with the highest alignment score is selected for targeted mitigation in the current iteration. The model is then updated to reduce bias along the direction corresponding to i * , using a mitigation method, giving M (1) . Compute bias score τ (t+1) = ⟨w init B * , p⟩ 11:</p><p>t ← t + 1 12: until τ (t) &lt; ϵ 13: return M (t) 7.5 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1">Analyzing InterMit for Mitigation</head><p>To evaluate the effectiveness of our intersectional bias mitigation strategy, we compare InterMit against ITI-GEN <ref type="bibr" target="#b189">[190]</ref>, a widely used method designed specifically for Stable Diffusion 1.4 (SD1.4). ITI-GEN supports up to three simultaneous bias mitigations but requires retraining and lacks scalability. In contrast, our method combines a simple, modular prompting technique that uses hardprompts with a powerful intersectional optimization algorithm InterMit.</p><p>Prompt Modification (PM ) Our method uses sequential prompt modification to incorporate bias counterfactuals. Consider mitigating 'environment' and 'clothing' bias for the prompt "a nurse":</p><p>• First, we replace the base prompt with "a nurse working indoors" and "a nurse working outdoors", ensuring a 50/50 split.</p><p>• Next, we extend to clothing: formal/informal variants of both environment prompts. This leads to 4 total prompts, evenly sampled. This modular setup allows compound mitigation, efficient image generation, and compatibility with hardware optimizations.</p><p>Mitigation. InterMit can use any sequential mitigation method, but we consider a simple training-free mitigation method using only prompt modifications. At each mitigation step, we modify the initial prompt to introduce counterfactual concepts associated with the mitigated bias axis. Over multiple steps, we create collections of counterfactual prompts that include all permutations of all mitigated axes (see 7.5.1). We emperically set ϵ = 0.35 for all our experiments. To compare our method to a traditional mitigation approach, we select ITI-GEN <ref type="bibr" target="#b189">[190]</ref>, as it uses a similar FairToken-based permutation approach.</p><p>Experimental Setup. We evaluate our mitigation framework using two different TTI models: Stable Diffusion 1.4 (SD 1.4) and Stable Diffusion 3.5 (SD 3.5). For SD1.4, we select random subsets of occupation-based biases and assign equal weights. For SD 3.5, we evaluate on 15 occupation prompts with user-defined priority vectors. We quantify mitigation using (1) MitAmt: the average bias score τ T after mitigation; and (2) MitSteps: the ratio of mitigated bias axes to the total biases in p. Visual quality is measured using CLIP-IQA <ref type="bibr" target="#b217">[218]</ref> and a VQA prompt: "[vqa] Is there a person in the image?".</p><p>Main Results. As shown in Table <ref type="table">7</ref>.1, InterMit-PM achieves a lower post-mitigation bias (0.33 Table <ref type="table">7</ref>.1: Comparing our Mitigation Algorithm to ITI-GEN. We mitigate a randomly chosen subset of 2-5 biases for prompts in the occupation set, and compute visual quality metrics and mitigation outcomes. We find that our algorithm uses 22% fewer mitigation steps, while still yielding higher mitigation amount and quality. * Indicates we use a different prompt set and priority on SD 3.5, so these should not be compared to SD1.4 results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Uncovering optimal bias mitigation strategies</head><p>InterMit is flexible and supports any set of user-specified bias axes. As shown in Figure <ref type="figure" target="#fig_37">7</ref>.3(a) &amp; (c), it often achieves effective mitigation in fewer steps than the user-defined threshold. By leveraging inter-axis relations, it identifies optimal strategies. In Figure <ref type="figure" target="#fig_37">7</ref>.3(a), when age and ethnicity are equally prioritized, mitigating ethnicity alone can reduce both due to demographic overlap, and a single intervention meets the threshold. In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Discussion</head><p>InterMit is a modular approach for mitigating biases in TTI models. In this section, we highlight key features of InterMit that enable it to meet the requirements of a dynamic bias mitigation framework for TTI models, as outlined in the earlier sections of this chapter.</p><p>Extension to Other Approaches. We propose a general framework for mitigating intersectional biases in TTI models. As shown in Alg. 4, our method can be layered on top of any sequential bias mitigation strategy. At each step, a single bias axis is mitigated, and the intersectionality matrix S is recomputed to capture updated dependencies between bias dimensions. This enables an iterative process where mitigation actions are informed not only by the user's priorities but also by how interventions on one bias axis may impact others. While we demonstrate our approach using a simple method like PM , it can be readily extended to more sophisticated strategies. For example, weightediting approaches such as UCE <ref type="bibr" target="#b218">[219]</ref> could leverage the embeddings of bias-relevant keywords to adjust attention weights dynamically. These adjustments can be made in accordance with the user's specified ideal distributions, allowing the system to fine-tune the weight of each keyword depending on whether a particular attribute needs to be amplified or suppressed to achieve a desired distributional outcome. Similarly, any LoRA -based method can incorporate the intersectionality matrix S directly into its low-rank decomposition setup by aligning the decomposition adjustments with the directional dependencies captured by S. This integration allows the model to modulate its representations in a targeted manner, mitigating intersectional biases more holistically without requiring retraining from scratch. In all these cases, our framework serves as a modular layer that complements existing mitigation techniques by explicitly accounting for the complex interactions between multiple bias dimensions. carefully selecting the ideal distribution, as it directly shapes which biases are prioritized and how intersectional dependencies are handled during mitigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role of Priority</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Conclusion</head><p>This chapter presented InterMit, a method for reducing intersectional bias in text-to-image models.</p><p>It is designed to meet the main needs of a good bias mitigation strategy. Below, we summarize how it addresses each of these needs:</p><p>• • Intersectionally robust: InterMit supports multiple bias axes through a priority vector. Users select which biases to focus on and assign weights to reflect trade-offs.</p><p>• Supports user-defined distributions: Users can define a target distribution for each bias axis.</p><p>These can be uniform, real-world-based, or context-specific, removing the need to assume a fixed "ideal" distribution.</p><p>• Modular and training-free: InterMit does not require model retraining or internal access. It can be used with different TTI models as a separate component.</p><p>• Trade-off aware and transparent: It reports how mitigation on one axis affects others. This helps users understand and manage cross-axis effects.</p><p>• Aligned with community and contextual norms: Users set their own fairness goals. This allows adaptation to different use cases and cultural settings.</p><p>• Empirical results: InterMit reduced bias more than ITI-GEN, needed fewer steps, and preserved image quality even when mitigating multiple axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.1">Ethical Consideration</head><p>While InterMit offers flexibility in how users define and control fairness goals, this same flexibility can also be misused. Users can arbitrarily modify the priority vector or define biased target distributions to push outputs toward harmful or exclusionary representations. The system does not enforce any default notion of fairness, and so users must take care to ensure that mitigation choices align with broader ethical standards and do not reinforce new forms of bias.</p><p>Additionally, in designing this system, we simplify complex social attributes such as gender and ethnicity into discrete, categorical variables. This design choice enables quantification, measurement, and intervention but deviates from how these concepts are experienced in real life-where identity is often fluid, non-binary, and shaped by culture and context. We acknowledge this limitation as a necessary trade-off to make the problem tractable and actionable within current generative AI systems.</p><p>We encourage future work to explore how these representations can be made more nuanced, participatory, and reflective of lived experiences, while maintaining technical feasibility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Key Contributions</head><p>A PhD thesis is, at its core, an attempt to push the boundaries of existing research. While the long-term impact of any such work can only be judged over time, the primary motivation behind this thesis has been to contribute meaningful advances to the growing body of research on computer vision safety and explainability. This thesis set out to challenge prevailing assumptions about fairness auditing in machine learning and to advocate for more context-sensitive, concept-aligned, and causally grounded methods-particularly within vision and generative models.</p><p>Across multiple chapters, we operationalized counterfactual reasoning at varying levels of abstraction-concept attribution (CAVLI), adversarial counterfactual training (ASACs), dynamic bias evaluation (TIBET), causal bias diagnosis (BiasConnect), and intersectional mitigation (InterMit).</p><p>Each of these contributions embodies the same core principle: isolate the influence of meaningful interventions while holding irrelevant factors constant.</p><p>Below, we summarize the key contributions of this thesis:</p><p>• Dynamic, Context-Dependent Bias Evaluation and Mitigation for Generative Models</p><p>A central theme of this thesis is the push for dynamic rather than static evaluations of bias.</p><p>Prior work in bias detection and mitigation often focuses on predefined dimensions such as gender or ethnicity, assuming these axes of bias are fixed and independent. However, in practice-especially in generative models like Text-to-Image (TTI) systems-the influence of these attributes is highly context-dependent. The same identity label can have vastly different semantic and visual implications depending on surrounding prompt cues.</p><p>To address this, the thesis introduces TIBET, a framework for dynamic, prompt-sensitive bias evaluation in TTI models. We also proposed InterMit, a context-aware bias mitigation algorithm that adapts its interventions based on observed interactions between bias axes and user-defined fairness priorities. These methods offer a more realistic and flexible evaluation and intervention pipeline that reflects the fluid semantics of generative tasks. This thesis moves in a different direction than previous approaches by emphasising on the need of dynamic bias evaluation and mitigation platforms.</p><p>• A Rigorous Framework for Studying Intersectional Biases in Generative Models. Intersectionality is a widely recognized concept in social sciences and phliosophy <ref type="bibr" target="#b112">[113,</ref><ref type="bibr" target="#b113">114]</ref>, few technical frameworks exist for quantifying and diagnosing how multiple biases interact within machine learning models-especially generative ones. In Chapter 6, this gap is addressed through BiasConnect, a causal, counterfactual-based diagnostic tool that measures the ripple effect of mitigating one bias axis (e.g., gender) on others (e.g., age or race). Unlike observational fairness metrics, our method uncovers deeper, causal relationships between protected attributes-offering a principled way to understand bias entanglement. Further InterMit was proposed not only to mitigate intersectional biases simulataneously but also alert the user about the trade-off about these biases. As per my knowledge this is the first concrete work to tackle intersectionality of biases in TTI models in such a comprehensive manner.</p><p>• A Flexible, Modular Bias Mitigation Algorithm for TTI Models. Chapter 7 argues that bias mitigation in TTI models is uniquely challenging due to the open-ended nature of generation, the lack of explicit labels, and the entanglement of social attributes in latent representations. We proposed InterMit, a novel mitigation algorithm that is modular, trainingfree, and sensitive to user-defined fairness trade-offs. InterMit leverages causal sensitivity scores to guide intervention, ensuring that debiasing in one dimension does not inadvertently worsen outcomes along others. This approach represents a step toward practically deployable, intersectionally aware mitigation tools for generative systems. One significant contribution of this thesis is outlaying the challenges associated with bias mitigation in TTI models and presenting a tool that takes a step in that direction.</p><p>• Using Adversarial Examples as Tools for Fairness. In Chapter 4 adversarial examples are presented not as threats to model robustness, but as tools for fairness enhancement. Through ASACs (Attribute-Specific Adversarial Counterfactuals), we demonstrate that adversarial perturbations targeting protected attributes can be repurposed to fine-tune biased classifiers in a curriculum-based manner. This method not only reduces fairness gaps but also improves overall model accuracy-offering an ethical and effective alternative to GAN-based counterfactuals, which often reinforce stereotypes.</p><p>• Concept-Level Interpretability for Individual Model Decisions. In Chapter 3, CAVLI, a novel hybrid method combining TCAV and LIME is introduced that produces localized, concept-based explanations. CAVLI quantifies the degree to which a classifier's decision on an individual image depends on human-defined concepts like "grassland" or "stripes."</p><p>By analyzing the spatial overlap between regions important for concept representation and those driving the model's decision, CAVLI provides a Concept Dependency Score (CDS).</p><p>This enables practitioners to go beyond pixel saliency and ask: Is the model predicting 'cow' because of the animal, or because of the grassy background?</p><p>Our results show that such fine-grained, interpretable diagnostics can uncover spurious correlations and dataset biases-thereby guiding more trustworthy model design. In sensitive applications like medical imaging or facial recognition, such explanations can help ensure decisions are grounded in appropriate semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Limitations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Counterfactual Approaches</head><p>Counterfactual methods help us understand how models behave by asking "what if" questions, but they come with important limitations. First, they rely on assumptions about what changes are small and realistic. In social settings, this is hard because traits like race, gender, and class are often linked in ways that make it unrealistic to change one without affecting others.</p><p>Many methods also assume that features can be changed one at a time or that effects add up in simple ways. These assumptions often break down, especially in complex models like text-to-image systems where features interact in messy and unpredictable ways.</p><p>There are also practical limits. It's not possible to test every possible counterfactual, so we use a small sample to keep things fast. But this means we might miss some important examples. There is often a higher computational costs that is associated with generating a large set of counterfactual images.</p><p>Finally, there is no single way to judge whether a counterfactual is good. We use model scores and concept measures, but these do not always reflect what people find fair, realistic, or helpful.</p><p>Without human feedback or ground truth, it is difficult to determine when a counterfactual truly makes sense.</p><p>These limitations do not mean that counterfactuals are not useful, but they remind us to be careful -especially when applying them in areas that directly impact people's lives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Dynamic Bias Mitigation</head><p>Dynamic bias mitigation methods adjust outputs based on the prompt, model state, or user-defined fairness goals. While this flexibility is useful, it can cause problems. These systems often behave differently for similar inputs, which makes it hard to reproduce results or audit fairness over time.</p><p>Since they make decisions on the fly, they also need more computation, which slows things down and makes large-scale deployment harder. It's also tough to judge what the "right" mitigation is, since fairness means different things to different people and contexts.</p><p>These methods can also run into deeper problems. They may fix one kind of bias but unintentionally make others worse, especially in intersectional cases. When users control fairness settings, the system may overfit to their short-term goals and miss long-term harms. It's also harder to explain how or why a certain output was generated, which can reduce trust. Finally, letting users or institutions decide what to fix raises questions about whose idea of fairness the system follows. These trade-offs show why dynamic approaches must be used carefully, especially in areas that affect people's lives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Adversarial Images for Fairness</head><p>Adversarial images can help us test and improve fairness in computer vision models. These are images that are slightly changed to see if the model gives a different output in ways that might show bias. For example, if a model changes its decision when skin tone or clothing is adjusted slightly, that might be a sign of unfairness. Researchers also use adversarial training to make models less sensitive to certain features by adding these images during training and making the model learn not to rely on them.</p><p>However, this approach has limits. Adversarial images may look strange or unrealistic, which can confuse both the model and the human trying to interpret the result. It's also hard to decide what counts as a fair change-sometimes a small tweak might flip the output even though it's not meaningful. Most adversarial fairness tests focus on one attribute at a time, and don't capture more complex combinations like race and gender together. Finally, if done carelessly, this method might make the model ignore useful signals or cause new types of bias to appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Ethical Considerations</head><p>This thesis touches on questions that sit at the boundary between technical modeling and social meaning. In building systems to detect and reduce bias, we work with sensitive social categories like race, gender, and ethnicity. These categories are shaped by history, culture, and power, and there is no single way to define or formalize them. As computer scientists, we may not always engage deeply with the full range of views on these topics, but we believe it is important to reflect on the assumptions behind our approach.</p><p>Our method takes a clear stance: making patterns of bias visible is a step toward accountability.</p><p>While our use of quantitative scores and fairness weights requires abstraction, we see this not as erasing social meaning, but as making it traceable. By letting users set fairness goals and inspect how outputs change, our method can also be prone to misuse if given in the wrong hands. We believe that making these decisions transparent-even when imperfect-is better than hiding them. This We do not claim that our way of modeling intersectionality is the only valid one. We work within a broader view that accepts both additive and interactive patterns of disadvantage. The categories we use are not fixed; they can overlap, shift, and take on different meanings in different contexts.</p><p>Rather than fix their meaning, we treat them as flexible tools to help surface how a model responds to real-world complexity.</p><p>There are also concerns about using visual cues to infer features like ethnicity. We understand that these features are socially constructed and may not always have clear boundaries. Still, in the absence of agreement on how to define them, we choose to stay neutral on the deeper debates. What matters for our work is not the ultimate nature of these categories, but how models respond to them in practice. By focusing on how models treat identity-related prompts and outputs, we provide a way to study patterns of harm-even if we cannot settle what these identities "really" are.</p><p>Finally, we acknowledge that no system can prevent misuse. But we can reduce the risk by making our assumptions clear, our settings open, and our results reproducible. We see transparency not only as a design choice, but as an ethical safeguard. In building tools that diagnose bias and allow users to control fairness goals, we aim to support open, responsible, and adaptive use-without claiming to offer a final answer to the deep social and philosophical questions at play.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Future Directions</head><p>There are several promising directions for extending this work. One key challenge is to better understand how intersectional identities interact in generative systems. Our current method models these identities through compositional prompts and measurable dimensions of bias, but deeper work is needed to capture how social meanings shift across contexts. Future research could explore richer prompt structures, broader cultural inputs, and participatory methods that reflect how affected groups view fairness and representation.</p><p>The work on CAVLI and ASAC can be extended to automated concept discovery approaches, removing the need to specify predefined concepts such as "grassland" or "gender." Instead, the system could automatically identify which concepts or prompts are most relevant and either mitigate biases along these dimensions (as in ASAC) or quantify the model's dependence on them (as in CAVLI). Additionally, the approach presented in Chapter 4 is currently designed to mitigate one attribute at a time. Extending it to support simultaneous mitigation of multiple attributes would make it more effective and better suited for real-world scenarios where multiple biases often coexist.</p><p>Our work in Chapter 6 introduces BiasGraph and BiasConnect as tools to diagnose the intersectionality of biases. However, these tools should be seen as initial steps toward this goal. Further work is needed to develop a fully causal framework for understanding intersectionality-one capable of evaluating higher-order dependencies and constructing comprehensive causal graphs. Similarly, the mitigation strategies presented in Chapter 7 could be extended to handle more than two bias dimensions simultaneously, leveraging higher-order methods that consider multiple axes of bias together rather than in isolation.</p><p>Another current limitation of the approach discussed in Chapter 7 is efficiency. At each step of our bias mitigation algorithm, we compute a full intersectionality matrix, denoted as S, which slows down the process. This points to the need for faster and more scalable approaches. Future research could also explore alternatives to the sensitivity matrix S and investigate better ways to incorporate such representations into mitigation frameworks. An important direction is to develop more robust, adaptive mitigation strategies. While our current approach allows users to set fairness goals dynamically, future work could incorporate feedback loops where users, auditors, or policy makers co-define mitigation objectives over time. This would help the system respond to evolving norms while maintaining transparency and accountability.</p><p>We also see potential in building connections between formal modeling and qualitative perspectives. Rather than treating abstraction and lived experience as opposites, future methods could combine structured diagnosis with narrative or case-based feedback. This would allow researchers to test how well formal scores align with how people experience bias in practice.</p><p>The approaches discussed in this thesis can also be extend to domains other than images. While scaling these tools to work with more complex generative pipelines-such as multi-turn conversations, video synthesis, or multi-agent systems-will require both technical and ethical advances.</p><p>These systems will likely raise new fairness concerns that go beyond individual prompts, involving interaction histories, social roles, or emergent behaviors. Understanding and addressing bias in these settings will require interdisciplinary collaboration and continued reflection on the limits of current methods. includes four straightforward questions resembling those in the primary study. Users who achieve a score above 90% on the qualification exam are eligible to participate in the main user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 User Study 2 B.2.1 Setup</head><p>Our approach to User Study 2 closely mirrors that of User Study 1. We administer the study through the Amazon Mechanical Turk platform. Before participants begin the study, we ensure they have access to detailed information about several key aspects of our research. This information encompasses thorough explanations about TTI models, their intended applications, the range of inputs and outputs these models handle, and a clear explanation of the concept of biases.</p><p>In this study, we present users with 10 randomly sampled images for a prompt and request them to rate the presence or absence of bias on a 5-point Likert scale (as depicted in Figure <ref type="figure">B</ref>.2). To compute our correlations, we only consider prompts with two or more societal biases, specifically, bias axes names containing 'gender', 'age', 'race', 'racial', 'geographic', 'ethnicity', 'cultural', as we find that humans are not very reliable at observing non-societal biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Training Details and Qualification Test</head><p>In alignment with our approach in User Study 1, our study participants are drawn from Canada, the United States, and the United Kingdom. We employ a qualification test designed to gauge their understanding of the training materials, consisting of four straightforward questions akin to those used in the primary study. Participants who attain a score exceeding 90% on the qualification exam qualify for participation in the main user study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 . 1</head><label>11</label><figDesc>FIGURE 1.1 Counterfactual reasoning reveals hiring bias. In a famous study conducted by Moss-Racusin et al.<ref type="bibr" target="#b0">[1]</ref>, identical resumes differing only in the applicant's name ("John" vs. "Jennifer") were submitted for a lab manager position. It was observed that faculty were significantly more likely to favor the male applicant. This style of reasoning-holding all factors constant while changing desired attributes-is referred to as counterfactual reasoning. This simple idea is at the heart of the methods and contributions in this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 This figure illustrates how counterfactual reasoning is employed throughout the thesis to probe and enhance the capabilities of computer vision models. In Chapter 3, we introduce CAVLI, a method for quantifying the influence of visual concepts on model decisions via counterfactuals. Chapter 4 leverages counterfactuals for bias mitigation using ASACs, showing their efficacy in reducing unfair model behavior. In Chapter 5, we present TIBET, a framework for evaluating bias in text-to-image models through systematic counterfactual interventions. We further explore interactions between multiple bias dimensions in TTI models using our tool BiasConnect in Chapter 6, and propose a strategy for intersectional bias mitigationin generative models called InterMit in Chapter 7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>We fine-tune the original model M (θ,ρ) using the organized ASACs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Qualitative results showing that our trained model becomes robust to ASACs after training. The blue curve represents the sensitivity to adversarial noise before training, and the red curve represents the sensitivity after training. Post-training, the images showed increased robustness to adversarial noise, with predictions remaining unchanged even at higher noise levels. . . . . . . . . . . . . . . . . . . . . . . . . . . xii 5.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8</head><label>8</label><figDesc>Step 5). . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 VQA-based Image Comparison. The VQA process in the Image Comparison Module. For each image in the initial set and the counterfactual set, we ask a list of questions to the VQA model, MiniGPT-v2, to extract concepts. The underlined words indicate the axis of bias that is associated with that question. . . . . . . . . . . . . . . 5.4 Metrics: (a) MAD is low when the CAS scores are uniform across all counterfactuals, and high when the CAS scores are skewed. (b) MAD is only dependent on variability in CAS, not on amount of CAS . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Usefulness of TIBET. In this example setting, we show how TIBET can be useful to a user concerned about biases in the images generated by a TTI model. We show how TIBET can analyse biases along human-observable axes of bias, with post-hoc explainablity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Analysis enabled by TIBET. Our approach calculates CAS and MAD scores to measure association with counterfactual prompts and bias degree in generated images. Qualitative metrics like Top-K Concepts and Axis-Aligned Top-K Concepts offer post-hoc model explanations. Additionally, our approach enables comparisons with counterfactual explanations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Sensitivity Analysis on CAS and MAD for errors in VQA. Per User Study 3 , we estimate an 18% error rate in VQA, leading to 4.73% and 13.11% error in CAS and MAD respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.Comparing our VQA and CLIP methods for Image Comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 . 1 :</head><label>11</label><figDesc>Figure 1.1: Counterfactual reasoning reveals hiring bias. In a famous study conducted by Moss-Racusin et al.<ref type="bibr" target="#b0">[1]</ref>, identical resumes differing only in the applicant's name ("John" vs. "Jennifer") were submitted for a lab manager position. It was observed that faculty were significantly more likely to favor the male applicant. This style of reasoning-holding all factors constant while changing desired attributes-is referred to as counterfactual reasoning. This simple idea is at the heart of the methods and contributions in this thesis.</figDesc><graphic coords="21,138.53,410.80,116.26,54.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 . 2 :</head><label>12</label><figDesc>Figure 1.2: This figure illustrates how counterfactual reasoning is employed throughout the thesis to probe and enhance the capabilities of computer vision models. In Chapter 3, we introduce CAVLI, a method for quantifying the influence of visual concepts on model decisions via counterfactuals. Chapter 4 leverages counterfactuals for bias mitigation using ASACs, showing their efficacy in reducing unfair model behavior. In Chapter 5, we present TIBET, a framework for evaluating bias in text-to-image models through systematic counterfactual interventions. We further explore interactions between multiple bias dimensions in TTI models using our tool BiasConnect in Chapter 6, and propose a strategy for intersectional bias mitigationin generative models called InterMit in Chapter 7.</figDesc><graphic coords="25,130.50,287.03,351.00,204.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: A brief timeline tracing the development of counterfactual reasoning. While the term "counterfactual" was popularized by David Lewis in 1973 [2], the underlying idea of reasoning about alternative possibilities has roots in both ancient Western and Eastern philosophical traditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>CHAPTER 3 CAVLI:Figure 3 . 1 :</head><label>331</label><figDesc>Figure 3.1: This chapter begins with a central question: When a black-box model predicts a label (e.g., "Cow"), how can we determine which meaningful, human-interpretable concepts influenced that decision? For instance, did the model recognize the image as a cow because of meaningful features like its distinctive body structure, or was the decision influenced by spurious cues such as the presence of grasslands in the background? To address this, we introduce a method that quantifies the extent to which a model relies on human-defined concepts-offering a more interpretable alternative to conventional pixel-level explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>model decisions. A counterfactual asks what would have happened if certain parts of the input were changed. Systematically changing the input and observing the changes in the output allows us to see what how removal or addition of certain features or concepts impact the model's outcome.Counterfactuals help bridge the gap between the model's internal logic and human intuition. They shift the focus from static attributions to dynamic, intervention-based insights. This chapter builds on that foundation to explore a specific application of counterfactuals-measuring the role of human concepts in the decisions made by vision models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>CDS. For example, a wildlife classifier should detect cows based on shape and texture, not just the background. If the CDS shows a high dependence on the background, it may indicate a dataset bias or shortcut learning. In other domains like medical imaging, such methods can reveal whether a diagnosis is based on relevant tissue features or artifacts in the scan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Figure3.2: Overview of our proposed approach, CAVLI, to estimate the dependence of a concept (e.g., "grassland") on a decision (e.g., "cow detection"). After decomposing the input image into superpixels, in Step 1 we find the regions of the image that have the highest association with the concept, defined by a set of images. In Step 2 we identify image regions with the highest involvement in the classification decision. Finally, we measure the overlap between the two in order to quantify the dependence.</figDesc><graphic coords="57,137.97,112.92,336.05,462.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>First, we try to understand how well the model captures a concept for an individual decision. We frame the question by trying to understand what parts of the image the model associates the most with a specific attribute. Techniques like TCAV cannot be used directly because of their global nature. We start by dividing the image into r homogeneous superpixels using the SLIC<ref type="bibr" target="#b153">[154]</ref> superpixel algorithm. The second part of our pipeline investigates the question, for a given decision, what parts of the image were the most influential in making the decision? We make use of LIME to generate these regions in the image (Steps 1, 2, 6, and 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 . 3 :</head><label>33</label><figDesc>Figure 3.3: We use concept and decision heatmaps to analyze a classifier's decisions and their dependence on a specific concept, such as identifying whether an image contains a cow and what parts are the most useful in decision-making. Here we focus on grasslands, and the concept heatmap displays the areas of the image that the model associates most strongly with this concept.</figDesc><graphic coords="65,85.03,72.00,441.94,227.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1:What happens when we ask a counterfactual generation engine to modify an image along specific attributes? When asked to alter attributes like smile, eye color, or hair, standard counterfactual engines often introduce unintended changes-like clothing or background-revealing entangled attributes and stereotypes. This chapter attempts to address these problems by proposing a novel counterfactual generation scheme, called Attribute Specific Adversarial Counterfactuals (ASACs), and demonstrates that ASACs can be utilized for bias mitigation in computer vision classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: An example of gender-based counterfactuals generated by StyleGAN2 and our method for a smile classification task.In the first row, we attempt to generate more feminine-looking versions of a given face, and in the second row, more masculine-looking versions. The StyleGAN2 outputs (right) exhibit biased correlations-femininity is often expressed through darker lipstick and exaggerated smiles, while masculinity is associated with older or wrinkled facial features. In contrast, our method (left) produces Adversarial Semantic Attribute Counterfactuals (ASACs) that preserve the original face's overall visual identity while varying gender-relevant features in a more controlled and semantically faithful manner.</figDesc><graphic coords="70,72.00,105.73,468.01,263.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>[ 4 ]</head><label>4</label><figDesc>is particularly noteworthy because of its proximity to our approach. Wang et al. proposed a post-processing approach to mitigate biases using adversarial examples, uniquely incorporating a GAN-based loss function. Conversely, Zhang et al. employ an architecture similar to ours, generating adversarial perturbations to counteract biases. Our work, however, is distinct in proposing a novel training setup that introduces a curriculum learning-based strategy with the use of ASACs. Unlike</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>. 1 ). 2 )</head><label>12</label><figDesc>where y ∈ {0, 1}. To estimate how far a predictor Ŷ from having equalized odds, we estimate the Difference in Equalized Odds (DEO), i.e., we estimatey |P ( Ŷ = 1|Y = y, A = a) -P ( Ŷ = 1|Y = y, A = a ′ )The second metric that we use to measure fairness is the Difference in Demographic Parity or DDP. DDP quantifies the absolute gap in approval rates (e.g., smile prediction) across different protected attributes. A large DDP value suggests a tendency for individuals in a specific group to receive more favorable outcomes than their counterparts in the other group.Definition 2 (Demographic Parity). A predictor Ŷ satisfies demographic parity with respect to protected attribute A ∈ {a, a ′ } and outcome Y , if the following condition holds:P ( Ŷ = 1|A = a) = P ( Ŷ = 1|A = a ′ ). (4To estimate how far a predictor Ŷ from having demographic parity, we estimate the Difference in Demographic Parity (DDP), i.e., we estimate|P ( Ŷ = 1|A = a) -P ( Ŷ = 1|A = a ′ )|.The Difference in Equalized Opportunity (DEOp) measures the disparity in True Positive Rates between different demographic groups in a model, indicating bias in favorable outcomes. A DEOp of zero signifies equal accuracy across groups, representing a fairness ideal in model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>to fine-tune a biased model can be broken down into three stages. For every batch during the finetuning process, we first generate ASACs for the batch X = {x 1 , x 2 , . . . , x k }, aiming to mislead the classification model M (θ,ρ) regarding a protected attribute a (detailed in Section 4.4.1). For example, when training a smile classifier, we produce adversarial images that prompt misclassification based on the protected attribute of gender. The next step (Section 4.4.2) evaluates the effectiveness of the generated ASACs. Continuing with our example, we measure the capacity of the ASACs to incorrectly influence the model's smile classification, despite the ASACs being generated to confuse the model with respect to the gender class. We assign a training curriculum by partitioning the batch of ASACs into mini-batches based on the ASAC's success in deceiving the model. This</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Bias Mitigation Strategy: Our proposed solution for mitigating biases in a model (e.g., smile classifier) M (θ,ρ) involves training sensitive attribute classifier C (θ,ϕ) (shown in the network architecture). We then follow a three-stage pipeline. (1) We generate ASACs that are capable of deceiving C (θ,ϕ) . (2) We define a curriculum assignment strategy that organizes these ASACs based on the degree to which they deceive the original model M (θ,ρ) . (3) We fine-tune the original model M (θ,ρ) using the organized ASACs.</figDesc><graphic coords="76,72.00,105.73,468.01,263.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 . 3 ,</head><label>43</label><figDesc>Figure 4.3, thereby utilizing nearly identical representations for classification. They are trained on identical data distributions D. To construct each ASAC x a , we augment the original image x with noise δ x using common methods to generate adversarial images as shown below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: Qualitative results showing that our trained model becomes robust to ASACs after training. The blue curve represents the sensitivity to adversarial noise before training, and the red curve represents the sensitivity after training. Post-training, the images showed increased robustness to adversarial noise, with predictions remaining unchanged even at higher noise levels.</figDesc><graphic coords="83,75.01,141.76,462.00,433.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>limitations highlight important directions for future work, aimed at automating concept selection, auditing internal model bias, and ensuring fairness in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>CHAPTER 5 TIBET:</head><label>5</label><figDesc>Using counterfactuals for identifying and evaluating biases in Text-to-Image generative models "The eye sees only what the mind is prepared to comprehend."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>5. 3 . 1 .</head><label>31</label><figDesc>We then generate images using a black box TTI model for the input prompt, and each of the counterfactual prompts across all bias axes (Step 3 in Figure 5.2; Section 5.3.2). Finally, we compare image sets of the input prompt with image sets of each counterfactual prompt, through the use of VQA-based concept decomposition and CLIP, and a novel metric, the Concept Association Score (CAS) (Step 4-5 in Figure 5.2; Section 5.3.3). Furthermore, we detail quantitative (MAD) and qualitative metrics for bias evaluation and post-hoc explainability in Section 5.3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>middle-aged looking old man at a church CF2: A fit and strong old man at... CF3: A stylishly dressed old man...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Figure 5.2: TIBET.Given an input prompt, we query an LLM (GPT-3) to identify axes of biases (Step 1), and generate counterfactual prompts for each axis of bias (Step 2). Here, we show a sample of three counterfactual prompts for the physical appearance bias, and two for the ableism bias. Next, we use a black-box TTI model (Stable Diffusion<ref type="bibr" target="#b2">[3]</ref>) to generate images for the initial prompt as well as each counterfactual for all axes of bias (Step 3). In this example, we leverage VQA-based concept extraction to obtain a list of concepts and their frequencies for each set of images, and compare the concepts of the initial set with concepts of each counterfactual to obtain CAS scores (Step 4). Finally, we compute MAD, a measure of how strong the bias is in the images generated by the initial prompt (Step 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>What is the gender (male, female, other) of the person? male What is the ethnicity of the person? white What is facial expression in this image?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 5 . 3 :</head><label>53</label><figDesc>Figure 5.3: VQA-based Image Comparison. The VQA process in the Image Comparison Module. For each image in the initial set and the counterfactual set, we ask a list of questions to the VQA model, MiniGPT-v2, to extract concepts. The underlined words indicate the axis of bias that is associated with that question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 5 . 4 :</head><label>54</label><figDesc>Figure 5.4: Metrics: (a) MAD is low when the CAS scores are uniform across all counterfactuals, and high when the CAS scores are skewed. (b) MAD is only dependent on variability in CAS, not on amount of CAS</figDesc><graphic coords="102,517.60,72.96,180.54,110.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 5 . 5 :Figure 5 . 6 :</head><label>5556</label><figDesc>Figure 5.5: Usefulness of TIBET. In this example setting, we show how TIBET can be useful to a user concerned about biases in the images generated by a TTI model. We show how TIBET can analyse biases along human-observable axes of bias, with post-hoc explainablity.</figDesc><graphic coords="104,142.59,547.86,315.11,78.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 5 . 7 :</head><label>57</label><figDesc>Figure 5.7: Sensitivity Analysis on CAS and MAD for errors in VQA. Per User Study 3 , we estimate an 18% error rate in VQA, leading to 4.73% and 13.11% error in CAS and MAD respectively.</figDesc><graphic coords="107,167.93,73.47,275.71,169.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 5 . 9 :</head><label>59</label><figDesc>Figure 5.9: Bias identification and mitigation. We compute difference in CAS scores for male and female counterfactuals for 11 occupation prompts. (a) and (b) show male and female leaning professions using Stable Diffusion 1.5 and 2.1 respectively. (c) shows how the difference in CAS scores after using ITI-GEN to mitigate gender bias.</figDesc><graphic coords="113,72.00,293.62,468.00,136.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 5 . 10 :</head><label>510</label><figDesc>Figure 5.10: Bias Identification and Mitigation using TIBET and ITI-GEN -Ground Truth.Here, we show ground truth gender differences in the initial set of images before bias mitigation, and after bias mitigation. The reduction in gender bias is in line with what we observe using CAS scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Figure 6.1: This figure demonstrates how a minimal prompt change-from "An old man at a church"to "An Asian old man at a church"-can lead to significant differences in generated outputs. While the former yields grim, seated Caucasian men, the latter produces smiling, upright Asian men in brighter scenes. This shift illustrates the intersectional nature of generative model behavior, where the addition of a single identity attribute (ethnicity) modifies not just appearance but also emotion, posture, and context-highlighting how social dimensions interact rather than operate independently.</figDesc><graphic coords="117,370.16,502.33,59.67,59.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 6 . 2 :</head><label>62</label><figDesc>Figure 6.2: Exploring Intersectionality of Biases: Analysing the Top-K concepts shows that pharmacists in Europe and Asia are depicted with different gender distributions.</figDesc><graphic coords="122,415.70,81.29,109.82,56.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>( a )Figure 6 . 3 :</head><label>a63</label><figDesc>Figure 6.3: Intersectional Observations using TIBET. We observe that images generated for a chef in Africa may be depicted outdoors (tree) unlike chefs in other regions of the world.</figDesc><graphic coords="122,421.46,203.12,115.60,79.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 6 . 4 :Figure 6 . 5 :</head><label>6465</label><figDesc>Figure 6.4: An example for which BiasConnect estimates a negative impact of bias mitigation along one axis on another axis. For this query, increasing the gender diversity (Gen) skews age distribution (Age) for images of musicians generated by Flux-dev.</figDesc><graphic coords="125,118.80,72.00,374.40,331.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>2 )where Π(D 1 , D 2 )</head><label>212</label><figDesc>is the set of all joint distributions γ(x, y) whose marginals are D 1 and D 2 , and |x -y| represents the transportation cost between points in the two distributions.We use w initBy to measure the amount of bias in the image set, where w By is computed by normalizing w By based on the number of counterfactuals in B y . w init B ∈ [0, 1] where 1 indicates that the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>6. 7</head><label>7</label><figDesc>(a)) and VQA error rate (Figure. 6.7(b)) on Intersectional Sensitivity values. Our method uses 48 images per prompt to study bias distributions. Removing 8 images (16.6%) results in a 10.5%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 6 . 10 :</head><label>610</label><figDesc>Figure 6.10: Comparison of real-world and Stable Diffusion 3.5 pairwise causal relationships for gender in computer programmer images. In the real world, gender diversification increases body type diversity, whereas in Stable Diffusion 1.4, it has a negative impact.</figDesc><graphic coords="139,327.16,93.85,143.76,71.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>10 )Stable Diffusion 3 . 5 :Figure 6 . 11 :</head><label>1035611</label><figDesc>Figure 6.11: The figure illustrates bias interpretations from Bias Connects, combining all pairwise graphs into one. (a) Shows how mitigating clothing bias also mitigates emotion bias. (b) Explores interactions between non-traditional bias axes in the TIBET dataset. (c) Reveals that generating ethnically diverse athletes reduces gender diversity. (d) Demonstrates that diversifying salesperson clothing is best achieved by increasing ethnic diversity rather than directly specifying clothing variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 7 . 2 :</head><label>72</label><figDesc>Figure 7.2: Overview of the InterMit pipeline. Prompts are first analyzed using BiasConnect and the Intersectional Sensitivity metric. Users then define their fairness preferences by specifying a priority vector and an ideal target distribution for each bias axis they wish to mitigate. InterMit uses this input to iteratively adjust the model's outputs, accounting for intersectional dependencies and mitigating biases along the specified directions. Throughout the process, users are informed about the trade-offs associated with each mitigation step (Section 7.4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 7 .</head><label>7</label><figDesc>3(c)  in two mitigation steps, the bias profile progressively aligns with the priority vector (dot product τ drops: 0.98 → 0.50 → 0.19). Notably, mitigating environment also reduces clothing bias due to strong intersectionality, showing how our method leverages inter-axis relationships for efficient mitigation. Moreover, if InterMit fails to reach the desired bias threshold or if mitigating one axis negatively impacts another, it can alert the user to these trade-offs, enabling informed decision-making.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 7 . 3 :</head><label>73</label><figDesc>Figure 7.3: Examples of InterMit mitigation. Blue columns are mitigated. (a) and (c) show efficient multi-bias mitigation. (b) highlights user-guided prioritization. Cross-axis effects like Ethnicity → Emotion are observable where mitigating ethnicity improves diversity in clothing but based on the user priority ignores emotion.</figDesc><graphic coords="156,72.00,214.64,468.00,288.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>Figure 7.4: Modifying D * (ideal distribution) in InterMit can have a significant effect on the Intersectional Sensitivity values. Intermit accoounts for this change during the mitigation process and allows user to specify the ideal distribution they want to have.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head></head><label></label><figDesc>Context-aware and prompt-sensitive: InterMit operates at the prompt level. It adjusts mitigation based on how prompts are written and what context they imply. The outcome may vary across different prompt sets. • Causally informed: It uses sensitivity scores from BiasConnect and Intersectional Sensitivity to understand how one bias axis affects another. This is based on counterfactual estimates, where Intersectional Sensitivity acts like a treatment effect showing the impact of changes on other axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>CHAPTER 8 Conclusion"</head><label>8</label><figDesc>In the process of conducting experiments, testing hypotheses, and pushing the boundaries of research during your PhD, you acquire a meta-skill -a skill that is hard to put into words but makes you wise."Reflection during my PhD journeyThis thesis presents my efforts to develop algorithms for evaluating and improving computer vision models. While counterfactual reasoning serves as a unifying theme, other ideas recur throughout the chapters. For instance, Chapters 3 and 5 focus on explainability, while Chapters 4, 6, and 7 emphasize fairness. Chapters 3, 5, and 6 propose methods for evaluating AI models, whereas Chapters 4 and 7 extend this by introducing techniques to improve model fairness. Additionally, Chapters 3 and 4 deal with classification models, while Chapters 5 through 7 focus on generative modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head></head><label></label><figDesc>helps reduce the risk of covert manipulation and supports open discussion about what fairness should look like in different settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure B. 1 :</head><label>1</label><figDesc>Figure B.1: User Study 1. This is the task that a Amazon Mechanical Turk participant sees.</figDesc><graphic coords="191,72.00,72.00,467.99,200.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="130,72.00,412.94,467.96,214.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="192,72.00,215.42,468.00,329.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>7.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7.1 Ethical Consideration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.1 Key Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.1 Counterfactual Approaches . . . . . . . . . . . . . . . . . . . . . . . . 8.2.2 Dynamic Bias Mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2.3 Adversarial Images for Fairness . . . . . . . . . . . . . . . . . . . . . . 8.3 Ethical Considerations .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Sensitivity analysis on BiasConnect. We evaluate the robustness of our approach by analyzing the impact of VQA errors and the effect of the number of images on Intersectional Sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.8 We compare aggregated causal graphs for four models: Stable Diffusion 1.4, Flux-dev, Kandinsky 2.2, and Playground 2.5. These graphs combine pairwise causal relationships across all bias axes, accumulated from occupation prompts in our dataset. . . . . 6.9 Additional examples on TIBET dataset (a-b) and Occupation prompt (c) on promptlevel analysis provided by BiasGraph. . . . . . . . . . . . . . . . . . . . . . . . . . 6.10 Comparison of real-world and Stable Diffusion 3.5 pairwise causal relationships for The figure illustrates bias interpretations from Bias Connects, combining all pairwise graphs into one. (a) Shows how mitigating clothing bias also mitigates emotion bias. (b) Explores interactions between non-traditional bias axes in the TIBET dataset. (c) Reveals that generating ethnically diverse athletes reduces gender diversity. (d) Demonstrates that diversifying salesperson clothing is best achieved by increasing ethnic diversity rather than directly specifying clothing variation. . . . . . . . . . . . 7.1 Key challenges in mitigating bias in TTI models. Effective methods must handle how bias changes with different prompts, capture how multiple biases interact, and let users choose which biases to address. They should support different fairness goals across cultures and help users understand the trade-offs involved in reducing specific types of bias. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Overview of the InterMit pipeline. Prompts are first analyzed using BiasConnect and the Intersectional Sensitivity metric. Users then define their fairness preferences by</figDesc><table /><note><p>6.6 Analyzing bias intersectionality matrices from BiasConnect. (a) Shows how mitigating clothing bias also mitigates emotion bias. (b) Explores interactions between non-traditional bias axes in the TIBET dataset. (c) Reveals that generating ethnically diverse athletes reduces gender diversity. TIBET can allow the user the user to understand whether interventions along one dimension impact other dimensions positively or negatively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.7 gender in computer programmer images. In the real world, gender diversification increases body type diversity, whereas in Stable Diffusion 1.4, it has a negative impact. xiv 6.11 specifying a priority vector and an ideal target distribution for each bias axis they wish to mitigate. InterMit uses this input to iteratively adjust the model's outputs, accounting for intersectional dependencies and mitigating biases along the specified directions. Throughout the process, users are informed about the trade-offs associated with each mitigation step (Section 7.4.3). . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Examples of InterMit mitigation. Blue columns are mitigated. (a) and (c) show efficient multi-bias mitigation. (b) highlights user-guided prioritization. Cross-axis effects like Ethnicity → Emotion are observable where mitigating ethnicity improves diversity in clothing but based on the user priority ignores emotion. . . . . . . . . . . 7.4 Modifying D * (ideal distribution) in InterMit can have a significant effect on the Intersectional Sensitivity values. Intermit accoounts for this change during the mitigation process and allows user to specify the ideal distribution they want to have. . . . . . . . B.1 User Study 1. This is the task that a Amazon Mechanical Turk participant sees. . . . . B.2 User Study 2. This is the task that a Amazon Mechanical Turk participant sees. . . . . xv LIST OF TABLES</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 .</head><label>3</label><figDesc>1 A comparison of mean CDS values and TCAV values of different concepts for the class Zebra in the ImageNet dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 A comparison of mean CDS values and TCAV values of different concepts for the class basketball in the ImageNet dataset. . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Average CDS scores for different subgroups in the CelebA dataset. . . . . . . . . . . 4.1 A comparison of our approach with other approaches [4, 5] on the CelebA dataset. . . 4.2 Performance across accuracy and fairness metrics on the UTK dataset with age being the protected attribute and gender being the target classifier. . . . . . . . . . . . . . . For commonly occurring axes of bias, we pre-define VQA questions. . . . . . . . . . 5.3 User Study 1: Can GPT-3 detect relevant biases? The high precision in both experiments indicate that Humans and GPT-3 agree on the biases that GPT-3 selected. The high recall in the societal case indicates that GPT-3 is better at capturing societal biases, compared to other types of biases. . . . . . . . . . . . . . . . . . . . . . . . . 5.4 User Study 2: Do humans see the same biases as our model?. We use prompts Correlation Between Estimates and Post-Mitigation Evaluation on ITI-GEN. The high correlation validates our mitigation estimates. For each prompt, we report one of the most influenced node (MaxInf) and the node with the greatest impact on others (MaxImp). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 7.1 Comparing our Mitigation Algorithm to ITI-GEN. We mitigate a randomly chosen subset of 2-5 biases for prompts in the occupation set, and compute visual quality metrics and mitigation outcomes. We find that our algorithm uses 22% fewer mitigation steps, while still yielding higher mitigation amount and quality. * Indicates we use a different prompt set and priority on SD 3.5, so these should not be compared to SD1.4 results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135</figDesc><table><row><cell>xvii</cell></row></table><note><p>4.3 Comparison of accuracy and fairness metrics between base classifier and fine-tuning using the proposed approach across different backbone architectures. . . . . . . . . . 4.4 Following Ramaswamy et al. [5], we show the mean fairness metrics and accuracy before and after fine-tuning, aggregated over target attributes -Young, Pointy Nose, Attractive and Arched Eyebrows with the protected attribute as Gender. . . . . . . . . 4.5 Fairness and accuracy results for individual attributes before and after fine-tuning . . . 4.6 Comparing different curriculum learning strategies. Here is CL stands for curriculum learning and ↑ denotes that examples are shown with increasing order of difficulty scores while ↓ denotes that examples are shown in decreasing order of difficulty scores. 4.7 Ablation on the effect of different values of noise magnitudes (ϵ) on the performance and fairness metrics of the target classifier. . . . . . . . . . . . . . . . . . . . . . . . 4.8 Comparison of classifiers before and after fine-tuning averaged over five different seeds. These experiments were performed on Smile classifier trained on CelebA dataset with a ResNet-18 backbone. . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Summary of select prior work. Three relevant characteristics are considered for each method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 with multiple societal biases ('gender', 'age', ...), and compute accuracy and ranking correlation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 The prompts for the occupation dataset constructed to evaluate BiasConnect . . . . . . xvi 6.2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Counterfactuals can be used to assess and improve model robustness by generating borderline or perturbed image examples. This helps evaluate whether a model's predictions are consistent under small semantic or visual changes<ref type="bibr" target="#b24">[25]</ref>. Additionally,</figDesc><table><row><cell>counterfactual images have been used to augment training data, improving generalization and</cell></row><row><cell>reducing overfitting in computer vision models [26].</cell></row><row><cell>(d) Causal Inference and Interventions. Counterfactual reasoning allows for causal analysis</cell></row><row><cell>of vision model behavior by posing questions like: "Would this image still be classified the same</cell></row><row><cell>way if we intervened on a particular feature (e.g., background, object texture)?" Such interventions</cell></row></table><note><p><p><p><p><p><p><p><p>Counterfactuals have become central to fairness analysis in vision</p>models by revealing disparities in model outputs across protected attributes such as race, gender, and age</p><ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref></p>. Vision-based counterfactuals can show whether a model would make a different decision if a person's apparent gender or ethnicity were changed.</p>(c) Robustness and Generalization.</p>help distinguish causal relationships from spurious correlations</p><ref type="bibr" target="#b20">[21]</ref></p>. This has applications in safety-critical systems, model debugging, and understanding unintended model behavior.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Indian: Nyaya Philosophy Buddhist: Nagarjuna Greek: Aristotle &amp; Plato Suppositions &amp; Hypotheticals Tarka Prasanga Regularity Theory of Causation David Hume, 1748 Causal Necessity, Direct Perception Thomas Reid, 1788 Critique of Pure Reason Immanuel Kant, 1781 Method of Difference John Stuart Mill, 1843 Physiological foundations of reasoning Alexander Bain, 1843 David Lewis, 1973 Donald Rubin, 1974 "Counterfactuals" Rubin's Causal Model Judea Pearl, 2000 Structural Causal Models, Ladder of Causation</head><label></label><figDesc></figDesc><table><row><cell cols="2">• Dynamic Evaluation of Biases in TTI Models (Chapter 5): We recognize that biases in Text-</cell></row><row><cell cols="2">to-Image (TTI) models are not static but dynamic and prompt-sensitive, with model behavior CHAPTER 2</cell></row><row><cell cols="2">varying substantially across different inputs. To address this, we propose TIBET, a novel</cell></row><row><cell cols="2">framework that uses prompt-level counterfactuals to dynamically evaluate and explain how Background</cell></row><row><cell cols="2">bias manifests across diverse prompts. By generating controlled variations of input prompts</cell></row><row><cell cols="2">(e.g., changing gender, ethnicity, or age terms) and analyzing resulting image attributes, TIBET</cell></row><row><cell cols="2">allows for both quantitative bias measurement and causal reasoning about where and how</cell></row><row><cell>Eleatic:</cell><cell></cell></row><row><cell cols="2">Parmenides different biases intersect. This enables large-scale, interpretable bias audits that account for the &amp; Melissus</cell></row><row><cell cols="2">contextual fluidity of generative model behavior-going beyond static benchmarks to uncover</cell></row><row><cell cols="2">Ancient Philosophy</cell></row><row><cell>more nuanced fairness failures.</cell><cell>Early Modern Philosophy</cell></row><row><cell cols="2">• Diagnosing Intersectional Bias in Generative Models (Chapter 6): Recognizing intersec-</cell></row><row><cell cols="2">tionality as a fundamental and understudied source of bias in generative AI, we introduce</cell></row><row><cell cols="2">BiasConnect, a diagnostic framework that uses prompt-level counterfactual interventions to Modern Philosophy</cell></row><row><cell cols="2">construct pairwise causal graphs and compute Intersectional Sensitivity scores. This enables Applications in</cell></row><row><cell cols="2">Computer Science structured, quantitative analysis of how mitigating bias along one axis (e.g., gender) may &amp; AI Research</cell></row><row><cell>improve or worsen biases along others (e.g., age, ethnicity).</cell><cell></cell></row><row><cell cols="2">• Interventional Mitigation of Intersectional Bias (Chapter 7) : Extending BiasConnect, we</cell></row><row><cell cols="2">propose InterMit, a training-free, modular algorithm that uses causal sensitivity estimates to</cell></row><row><cell cols="2">guide intersectional bias mitigation. InterMit incorporates user-defined priorities and target</cell></row><row><cell cols="2">distributions to adaptively select mitigation directions, achieving lower residual bias and higher</cell></row><row><cell cols="2">visual quality than prior approaches-while accounting for fairness trade-offs across multiple</cell></row><row><cell>social dimensions.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Once a practitioner uses CAVLI, they can inspect a model's decision alongside the Concept Dependency Score (CDS) and a visual heatmap to assess whether the decision aligns with the intended concept. The CDS quantifies reliance on the concept, while the heatmap shows where the model "looked" in the image-together offering a clear, interpretable summary of the model's reasoning. Together, heatmaps and CDS can inform a practitioner whether a given decision made by the classifier is dependent on a visual concept and, if so, what regions in the image used for decision-making were associated with the concept. This visualization using heatmaps, coupled with the quantification using CDS, can serve as a yardstick for practitioners who want to ensure that It supports both debugging and compliance in scenarios where accountability matters. This approach reflects the spirit of counterfactual reasoning: to ask not just what the model predicts, but why it predicts it-and how those reasons relate to human-understandable concepts.Consider a trained neural network F : X → {1, ...., K}, on a dataset X = {x 1 , x 2 , .., .x</figDesc><table><row><cell>3.2 Methodology</cell></row><row><cell>3.2.1 Notation</cell></row></table><note><p><p>decisions made by a computer vision model are based on the right concept. Higher CDS scores with desired or task-relevant concepts indicate that the model is relying on meaningful information to make its decision-such as focusing on facial features to detect a smile or using anatomical regions in a medical scan for diagnosis. This alignment suggests that the model has learned the intended patterns and is reasoning in a human-aligned way. In contrast, high CDS scores with irrelevant or sensitive concepts suggest potential problems. If the concept is social (e.g., gender, race), it may indicate bias, raising fairness concerns. If the concept is contextually irrelevant (e.g., background scenery in wildlife classification), it may signal that the model has picked up on spurious correlations-shortcuts that could fail under distribution shifts or real-world deployment. A classic example is ensuring that a model does not detect a buffalo simply because there is grass in the background.</p>Our method aligns with the broader goals of this thesis-leveraging counterfactual reasoning to improve the transparency and accountability of computer vision models. As part of our approach, we systematically blur different regions of an image to generate a diverse set of modified inputs. By observing how the model's output changes when specific parts of the image are masked, we can quantify the decision's dependence on those regions. This intervention-based process mirrors the counterfactual reasoning framework introduced in Chapter 1, where inputs are deliberately altered to assess their causal influence on the model's prediction. By connecting decisions to concepts using counterfactual reasoning, CAVLI allows for systematic inspection and correction of model behavior. t } and associated labels Y = {y 1 , y 2 , ..., y t }, where y</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>1: A comparison of mean CDS values and TCAV values of different concepts for the class Zebra in the ImageNet dataset.</figDesc><table><row><cell>Model</cell><cell>Stripes</cell><cell>Grassland</cell><cell>Indoor</cell><cell>Horse</cell></row><row><cell></cell><cell cols="4">CDS TCAV CDS TCAV CDS TCAV CDS TCAV</cell></row><row><cell>GoogleNet</cell><cell>0.17 0.78</cell><cell>0.26 0.62</cell><cell>0.13 0.12</cell><cell>0.02 0.41</cell></row><row><cell>ResNet</cell><cell>0.23 0.87</cell><cell>0.26 0.81</cell><cell>0.11 0.48</cell><cell>0.11 0.51</cell></row><row><cell cols="2">InceptionNet 0.45 0.84</cell><cell>0.21 0.71</cell><cell>-0.13 0.43</cell><cell>0.16 0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .</head><label>3</label><figDesc>2: A comparison of mean CDS values and TCAV values of different concepts for the class basketball in the ImageNet dataset.</figDesc><table><row><cell>Model</cell><cell>Ball</cell><cell>Jersey</cell><cell>Female</cell><cell>Race</cell></row><row><cell></cell><cell cols="4">CDS TCAV CDS TCAV CDS TCAV CDS TCAV</cell></row><row><cell>GoogleNet</cell><cell>0.29 0.56</cell><cell>0.38 0.93</cell><cell>-0.03 0.26</cell><cell>0.24 0.46</cell></row><row><cell>ResNet</cell><cell>0.27 0.68</cell><cell>0.21 0.46</cell><cell>-0.20 0.45</cell><cell>0.22 0.73</cell></row><row><cell cols="2">InceptionNet 0.41 0.87</cell><cell>0.05 0.31</cell><cell>0.09 0.31</cell><cell>0.18 0.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>3: Average CDS scores for different subgroups in the CelebA dataset.</figDesc><table><row><cell></cell><cell>Male Female</cell></row><row><cell>Smile</cell><cell>0.004 0.013</cell></row><row><cell cols="2">Non-Smile 0.005 0.007</cell></row><row><cell>sented in</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 .</head><label>3</label><figDesc><ref type="bibr" target="#b0">1</ref>, which shows that ResNet and GoogleNet both exhibited the highest mean CDS score for the concept stripes and the lowest mean CDS score for the concept indoor within the Zebra class. For InceptionNet, the concept grassland was more strongly associated with the Zebra class.The TCAV scores, which serve as global indicators of concept dependency, followed a similar trend.This pattern suggests that, on average, the CDS scores resemble the TCAV score. It is worth noting that higher CDS values indicate greater dependency on a concept, while lower values indicate lower dependency on the concept.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Algorithm 2 Construct Minibatches for Training Based on Curriculum. Note that a gradient step is taken over each minibatch returned. 1: Input: M (θ,ρ) (target classifier), {x 1 , . . . , x k } (input batch), {ϵ 1 , . . . , ϵ l } (noise magnitudes), f attack (adversarial attack), order (sorting order)</figDesc><table /><note><p>2: Output: minibatches: Partitioned minibatches sorted by difficulty score 3: for i = 1 to k do 4:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 4 .</head><label>4</label><figDesc>1: A comparison of our approach with other approaches<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> on the CelebA dataset.</figDesc><table><row><cell>Label</cell><cell>Method</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>Smile</cell><cell>Base Classifier</cell><cell>0.088</cell><cell>0.066</cell><cell>0.15</cell><cell>84.29</cell></row><row><cell></cell><cell>Adversarial Training [4]</cell><cell>0.077</cell><cell>0.051</cell><cell>0.17</cell><cell>91.74</cell></row><row><cell></cell><cell>Counterfactual GAN based-training [5]</cell><cell>0.065</cell><cell>0.050</cell><cell>0.17</cell><cell>86.50</cell></row><row><cell></cell><cell>ASAC (with FGSM)</cell><cell>0.050</cell><cell>0.043</cell><cell>0.15</cell><cell>91.91</cell></row><row><cell></cell><cell>ASAC (with PGD )</cell><cell>0.058</cell><cell>0.045</cell><cell>0.14</cell><cell>91.20</cell></row><row><cell>Big Nose</cell><cell>Base Classifier</cell><cell>0.332</cell><cell>0.257</cell><cell>0.28</cell><cell>80.77</cell></row><row><cell></cell><cell>Adversarial Training [4]</cell><cell>0.396</cell><cell>0.311</cell><cell>0.35</cell><cell>81.53</cell></row><row><cell></cell><cell>Counterfactual GAN based-training [5]</cell><cell>0.392</cell><cell>0.301</cell><cell>0.36</cell><cell>78.77</cell></row><row><cell></cell><cell>ASAC (with FGSM)</cell><cell>0.354</cell><cell>0.267</cell><cell>0.30</cell><cell>82.05</cell></row><row><cell></cell><cell>ASAC (with PGD)</cell><cell>0.374</cell><cell>0.281</cell><cell>0.31</cell><cell>81.03</cell></row><row><cell>Wavy Hair</cell><cell>Base Classifier</cell><cell>0.347</cell><cell>0.237</cell><cell>0.35</cell><cell>81.15</cell></row><row><cell></cell><cell>Adversarial Training [4]</cell><cell>0.359</cell><cell>0.264</cell><cell>0.40</cell><cell>82.07</cell></row><row><cell></cell><cell>Counterfactual GAN based-training [5]</cell><cell>0.344</cell><cell>0.230</cell><cell>0.34</cell><cell>79.21</cell></row><row><cell></cell><cell>ASAC (with FGSM)</cell><cell>0.34</cell><cell>0.29</cell><cell>0.342</cell><cell>82.01</cell></row><row><cell></cell><cell>ASAC (with PGD)</cell><cell>0.383</cell><cell>0.243</cell><cell>0.34</cell><cell>81.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 4 .</head><label>4</label><figDesc>2: Performance across accuracy and fairness metrics on the UTK dataset with age being the protected attribute and gender being the target classifier.</figDesc><table><row><cell>Method</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>Base</cell><cell>0.216</cell><cell>0.180</cell><cell>0.155</cell><cell>65.81</cell></row><row><cell>ASAC</cell><cell>0.195</cell><cell>0.175</cell><cell>0.184</cell><cell>71.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 4 .</head><label>4</label><figDesc>3: Comparison of accuracy and fairness metrics between base classifier and fine-tuning using the proposed approach across different backbone architectures.</figDesc><table><row><cell cols="2">Method Model</cell><cell>DEO DEOp DDP ACC</cell></row><row><cell>Base</cell><cell>ResNet-18</cell><cell>0.088 0.066 0.150 84.29</cell></row><row><cell>Base</cell><cell>ResNet-50</cell><cell>0.077 0.064 0.148 86.60</cell></row><row><cell>Base</cell><cell cols="2">DenseNet-121 0.090 0.075 0.146 80.36</cell></row><row><cell>Base</cell><cell cols="2">DenseNet-169 0.092 0.075 0.139 80.95</cell></row><row><cell cols="2">ASAC ResNet-18</cell><cell>0.050 0.043 0.150 91.91</cell></row><row><cell cols="2">ASAC ResNet-50</cell><cell>0.047 0.042 0.137 90.78</cell></row><row><cell cols="3">ASAC DenseNet-121 0.052 0.046 0.153 91.32</cell></row><row><cell cols="3">ASAC DenseNet-169 0.070 0.051 0.158 91.08</cell></row><row><cell cols="2">4.6.1 Quantitative Results</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 4 .</head><label>4</label><figDesc>4: Following Ramaswamy et al.<ref type="bibr" target="#b4">[5]</ref>, we show the mean fairness metrics and accuracy before and after fine-tuning, aggregated over target attributes -Young, Pointy Nose, Attractive and Arched Eyebrows with the protected attribute as Gender.</figDesc><table><row><cell>Method</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>Base</cell><cell cols="4">0.29±0.04 0.28±0.07 0.30±0.04 75.70±0.05</cell></row><row><cell cols="5">Proposed Approach 0.28±0.01 0.23±0.01 0.26±0.01 77.20±0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 4 .</head><label>4</label><figDesc>5: Fairness and accuracy results for individual attributes before and after fine-tuning</figDesc><table><row><cell>Attribute</cell><cell>DEO DEOp DDP ACC</cell></row><row><cell>Young</cell><cell>0.103 0.213 0.233 82.47</cell></row><row><cell>Young (ASAC training)</cell><cell>0.101 0.213 0.247 85.12</cell></row><row><cell>Pointy Nose</cell><cell>0.289 0.218 0.187 69.85</cell></row><row><cell>Pointy Nose (ASAC training)</cell><cell>0.282 0.206 0.180 72.97</cell></row><row><cell>Attractive</cell><cell>0.285 0.268 0.408 74.65</cell></row><row><cell>Attractive (ASAC training)</cell><cell>0.281 0.221 0.338 73.10</cell></row><row><cell>Arched Eyebrows</cell><cell>0.482 0.363 0.357 75.90</cell></row><row><cell cols="2">Arched Eyebrows(ASAC training) 0.438 0.301 0.281 77.84</cell></row><row><cell>4.6.2 Qualitative Results</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 4 .</head><label>4</label><figDesc>6: Comparing different curriculum learning strategies. Here is CL stands for curriculum learning and ↑ denotes that examples are shown with increasing order of difficulty scores while ↓ denotes that examples are shown in decreasing order of difficulty scores.</figDesc><table><row><cell>Method</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>Baseline</cell><cell>0.088</cell><cell>0.066</cell><cell>0.15</cell><cell>84.29</cell></row><row><cell>Randomized</cell><cell>0.055</cell><cell>0.044</cell><cell>0.14</cell><cell>91.23</cell></row><row><cell>CL in ↑</cell><cell>0.050</cell><cell>0.043</cell><cell>0.15</cell><cell>91.79</cell></row><row><cell>CL in ↓</cell><cell>0.058</cell><cell>0.048</cell><cell>0.17</cell><cell>92.08</cell></row><row><cell cols="2">stable even at higher noise magnitudes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.6.3 Ablation Studies</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">4.6.3.1 Evaluating the impact of Curriculum Learning</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 4 .</head><label>4</label><figDesc>7: Ablation on the effect of different values of noise magnitudes (ϵ) on the performance and fairness metrics of the target classifier.</figDesc><table><row><cell>Setting</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>{0.001, 0.01}</cell><cell>0.050</cell><cell>0.043</cell><cell>0.15</cell><cell>91.91</cell></row><row><cell>{0.001,0.03}</cell><cell>0.055</cell><cell>0.043</cell><cell>0.14</cell><cell>91.42</cell></row><row><cell>{0.001,0.05}</cell><cell>0.057</cell><cell>0.047</cell><cell>0.14</cell><cell>91.38</cell></row><row><cell cols="2">{0.001,0.03,0.05} 0.065</cell><cell>0.047</cell><cell>0.16</cell><cell>91.00</cell></row><row><cell>{0.001}</cell><cell>0.054</cell><cell>0.043</cell><cell>0.14</cell><cell>91.62</cell></row><row><cell>{0.05}</cell><cell>0.055</cell><cell>0.044</cell><cell>0.14</cell><cell>91.71</cell></row></table><note><p>method, we test different magnitudes of ϵ before implementing curriculum learning. Our results show</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 4 .</head><label>4</label><figDesc>8: Comparison of classifiers before and after fine-tuning averaged over five different seeds.</figDesc><table><row><cell cols="5">These experiments were performed on Smile classifier trained on CelebA dataset with a ResNet-18</cell></row><row><cell>backbone.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>DEO</cell><cell>DEOp</cell><cell>DDP</cell><cell>ACC</cell></row><row><cell>Base</cell><cell>0.14</cell><cell>0.12</cell><cell>0.13</cell><cell>84.31</cell></row><row><cell cols="5">Proposed Approach 0.12±0.04 0.10±0.03 0.17±0.08 90.50±0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 5 . 1 :</head><label>51</label><figDesc>Summary of select prior work. Three relevant characteristics are considered for each method.</figDesc><table><row><cell>Related Work</cell><cell cols="3">Bias-Axes Counterfactual Concept-level Explainability</cell></row><row><cell>T2IAT [101]</cell><cell>Predefined</cell><cell>✓</cell><cell>-</cell></row><row><cell>DALL-Eval [102]</cell><cell>Predefined</cell><cell>-</cell><cell>✓</cell></row><row><cell>Stable Bias [188]</cell><cell>Predefined</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">Esposito et al. [106] Predefined</cell><cell>✓</cell><cell>-</cell></row><row><cell>Ours</cell><cell>Dynamic</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">tone [101, 102, 103, 104, 106], culture</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 5 . 3 :</head><label>53</label><figDesc>User Study 1: Can GPT-3 detect relevant biases? The high precision in both experiments indicate that Humans and GPT-3 agree on the biases that GPT-3 selected. The high recall in the societal case indicates that GPT-3 is better at capturing societal biases, compared to other types of biases.</figDesc><table><row><cell>Experiment</cell><cell cols="2">Precision Recall</cell></row><row><cell>Human-vs-GPT (Overall)</cell><cell>0.90</cell><cell>0.54</cell></row><row><cell>Human-vs-GPT (Societal)</cell><cell>0.90</cell><cell>0.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 5 . 4 :</head><label>54</label><figDesc>User Study 2: Do humans see the same biases as our model?. We use prompts with multiple societal biases ('gender', 'age', ...), and compute accuracy and ranking correlation.</figDesc><table><row><cell></cell><cell cols="2">Accuracy</cell><cell>Ranking</cell></row><row><cell>Metric/Baseline</cell><cell cols="3">Top-1 Top-2 Correlation</cell></row><row><cell cols="4">Prompts with Societal Biases</cell></row><row><cell cols="2">Bipartite Matching 41%</cell><cell>76%</cell><cell>-0.08</cell></row><row><cell>CLIP (CAS CLIP )</cell><cell>50%</cell><cell>58%</cell><cell>+0.07</cell></row><row><cell>VQA (CAS)</cell><cell cols="2">75% 83%</cell><cell>+0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 6 .</head><label>6</label><figDesc>1: The prompts for the occupation dataset constructed to evaluate BiasConnect</figDesc><table><row><cell>Axis</cell><cell>Counterfactuals</cell></row><row><cell>gender bias</cell><cell>A photo of a male occupation A photo of a female occupation</cell></row><row><cell></cell><cell>A photo of an old occupation</cell></row><row><cell>age bias</cell><cell>A photo of a middle-aged occupation</cell></row><row><cell></cell><cell>A photo of a young occupation</cell></row><row><cell></cell><cell>A photo of a white occupation</cell></row><row><cell></cell><cell>A photo of a african american occupation</cell></row><row><cell>ethnicity bias</cell><cell>A photo of an asian occupation A photo of a south asian occupation</cell></row><row><cell></cell><cell>A photo of a middle eastern occupation</cell></row><row><cell></cell><cell>A photo of a hispanic occupation</cell></row><row><cell></cell><cell>A photo of a obese occupation</cell></row><row><cell>bodytype bias</cell><cell>A photo of a slim occupation</cell></row><row><cell></cell><cell>A photo of a muscular occupation</cell></row><row><cell>environment bias</cell><cell>A photo of a occupation working indoors A photo of a occupation working outdoors</cell></row><row><cell>clothing bias</cell><cell>A photo of a occupation in formal attire A photo of a occupation in informal attire</cell></row><row><cell></cell><cell>A photo of a occupation who is happy</cell></row><row><cell>emotion bias</cell><cell>A photo of a sad occupation who is sad A photo of a occupation who is serious</cell></row><row><cell></cell><cell>A photo of a occupation who is tired</cell></row><row><cell></cell><cell>A photo of a occupation who is fit</cell></row><row><cell>disability bias</cell><cell>A photo of a blind occupation A photo of a occupation with a hearing aid</cell></row><row><cell></cell><cell>A photo of a occupation on a wheelchair</cell></row><row><cell>6.4.2 Experiments</cell><cell></cell></row><row><cell>6.4.2.1 Experiment Setup</cell><cell></cell></row><row><cell cols="2">We conduct experiments on two prompt datasets, across six TTI models:</cell></row><row><cell cols="2">Occupation Prompts: To facilitate a structured evaluation, we develop a dataset with 26</cell></row><row><cell cols="2">occupational prompts, along eight distinct bias dimensions: gender, age, ethnicity, environment,</cell></row><row><cell cols="2">disability, emotion, body type, and clothing. We generate 48 images for all initial counterfactual</cell></row><row><cell cols="2">prompts using five TTI models: Stable Diffusion 1.4, Stable Diffusion 3.5, Flux [213], Playground</cell></row><row><cell cols="2">v2.5 [215] and Kandinsky 2.2 [216, 217]. We consider 26 occupations: computer programmer,</cell></row><row><cell cols="2">elementary school teacher, librarian, announcer, pharmacist, chef, chemist, police, accountant,</cell></row><row><cell cols="2">architect, lawyer, philosopher, scientist, doctor, nurse, engineer, musician, journalist, athlete, social</cell></row><row><cell cols="2">worker, sales person, politician, farmer, mechanic, firefighter, gardener. The different prompts and</cell></row><row><cell cols="2">axis used for each prompt have been presented in Table 6.1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>(B x , B y ) as B x → B y , indicating that a counterfactual intervention on B x to mitigate its bias also affects B y . As a first step, we intervene across all n × n bias relationships. Using attributes extracted by the VQA, we can count the attributes for a bias axis B y over any set of images. We construct a contingency table where rows represent the intervened bias axis B x , and columns capture the distribution on the target axis B y (e.g., age with old, middle-aged, and young categories). The values in the contingency tables are the counts of attributes of B y over the counterfactual image sets of B x .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>To that end, we adopt a graph representation for our output. This graph is referred to as a BiasGraph in the rest of the paper.Figures 6.11 and 6.8  show examples of such graphs. To interpret this graph, first pick a focal node where the intervention takes place. All outgoing edges from this node indicate intersectional relationships that are statistically significant. The weights of the edges show the Intersectional Sensitivity and can be interpreted as the impact of intervention on the bias axis for the focal node.</figDesc><table><row><cell>6.7 Applications</cell></row><row><cell>6.7.1 Applying BiasGraph to analyze TTI models</cell></row><row><cell>6.7.1.1 Global Aggregations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 6 . 2 :</head><label>62</label><figDesc>Correlation Between Estimates and Post-Mitigation Evaluation on ITI-GEN. The high correlation validates our mitigation estimates. For each prompt, we report one of the most influenced node (MaxInf) and the node with the greatest impact on others (MaxImp). This helps in model selection based on specific bias priorities.As an example, let's analyze how this information can help in selecting appropriate models using the global graphs in Figure6.8. If a user prioritizes robustness to age-related bias when selecting a model, Kandinsky 2.2 would be the best choice, as its Age node is the least influenced by other biases in the global analysis. This means that modifying other attributes (e.g., gender or clothing)</figDesc><table><row><cell>Prompt</cell><cell cols="2">Edges Corr.</cell><cell>MaxInf</cell><cell>MaxImp</cell></row><row><cell>Pharmacist</cell><cell>12</cell><cell>+0.399</cell><cell>Gender</cell><cell>Age</cell></row><row><cell>Scientist</cell><cell>9</cell><cell cols="3">+0.600 Clothing Ethnicity</cell></row><row><cell>Doctor</cell><cell>9</cell><cell>+0.638</cell><cell>Age</cell><cell>Disability</cell></row><row><cell>Librarian</cell><cell>14</cell><cell cols="2">+0.805 Emotion</cell><cell>Age</cell></row><row><cell>Nurse</cell><cell>5</cell><cell>+0.997</cell><cell>Age</cell><cell>Disability</cell></row><row><cell>Chef</cell><cell>8</cell><cell cols="3">+0.757 Bodytype Ethnicity</cell></row><row><cell>Politician</cell><cell>10</cell><cell cols="3">+0.782 Emotion Disability</cell></row><row><cell>Overall</cell><cell>-</cell><cell>+0.696</cell><cell>Gender</cell><cell>Age</cell></row><row><cell cols="5">edges (MaxInf). has minimal unintended effects on age representation, ensuring more stable and independent age</cell></row><row><cell>depictions across generated images.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Similarly, if the goal is to generate occupation-related images while minimizing unintended</cell></row><row><cell cols="5">bias propagation across other attributes, Playground 2.5 is the optimal choice. In this model,</cell></row><row><cell cols="5">variations in body type have the least impact on other biases, meaning changes in body shape do not</cell></row><row><cell cols="5">disproportionately affect other attributes like gender, ethnicity, or perceived professionalism. This</cell></row><row><cell cols="5">makes Playground 2.5 preferable in scenarios where maintaining fairness across multiple dimensions</cell></row><row><cell cols="5">while altering body type is critical. By analyzing bias influence and susceptibility, users can make</cell></row><row><cell cols="5">informed choices based on fairness priorities, whether aiming for stability in a bias axis or minimizing</cell></row><row><cell cols="2">unintended shifts in related attributes.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>the previous chapters, we introduced a suite of diagnostic tools, TIBET, BiasConnect, and BiasGraph, to systematically evaluate bias in Text-to-Image (TTI) generative models. TIBET enables dynamic, prompt-sensitive audits by using counterfactual perturbations to measure how attributes like gender, race, or age influence model outputs. BiasConnect and BiasGraph build on this by revealing how interventions on one attribute (e.g., gender) affect others (e.g., age or ethnicity).Yet while these diagnostic frameworks shed light upon the structure of the problem, they offer no guidance on how to act upon it. Should models always try to fix gender, race, and age bias at the same time? Or does it depend on the situation? For example, a healthcare tool that creates patient materials might focus more on racial diversity, while a fashion app might care more about showing</figDesc><table><row><cell>Together, these tools offer a powerful framework for uncovering where and how bias manifests in</cell></row><row><cell>TTI systems-highlighting not just individual failures, but structural patterns of representational</cell></row><row><cell>harm.</cell></row></table><note><p>different body types. Different users also have different goals: one person might want to remove gender stereotypes, while another might want to keep cultural details related to race. These choices are not just about style-they show deeper values. Also, fixing one type of bias but ignoring others can cause new problems. For example, if we improve gender balance in images of leaders but only</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>central, often unspoken dilemma in mitigating bias in TTI models is the question of what the ideal distribution of outputs should be. Should generative systems aim for demographic parity-ensuring equal representation of different identity groups across all prompts? Or should outputs reflect real-world statistics, such as actual gender ratios in specific professions or locations? But real-world statistics themselves often encode histories of exclusion and marginalization, making them a poor baseline for fairness. In other cases, fairness might require actively counterbalancing these historical biases-deliberately overrepresenting marginalized groups to correct for entrenched underrepresentation. Further complicating this question is the issue of context: a prompt like "a nurse in Tokyo" might demand a very different demographic distribution than "a global tech CEO." Thus, fairness in generative models is not reducible to a single distributional ideal. Most existing mitigation strategies either hard-code a static notion of fairness (often implicitly) or provide no mechanism for users to express their own. These preferences are not merely stylisticthey reflect deeper beliefs about what is fair or important. Also, fixing bias in one area while ignoring others can have unexpected and harmful effects. For example, improving gender balance in leadership roles but only showing younger women could still reinforce age bias or stereotypes about sexuality, even though it might appear fair at first glance.These challenges are further amplified when considering intersecting identities, where biases do not operate independently but interact in compounding and often unpredictable ways. A mitigation strategy that improves gender balance in the aggregate may still fail to generate equitable representations of older Black women or nonbinary South Asian professionals. In some cases, mitigating one bias dimension may actively worsen another-e.g., interventions that diversify gender portrayals may inadvertently homogenize race or age in the process. This raises a critical question: when such trade-offs are unavoidable, what should the mitigation system optimize for? Should it minimize the average bias, focus on the most underrepresented intersections, or follow explicit user directives?The answer depends heavily on context and on whose notion of fairness is prioritized. InterMit is designed to make these trade-offs visible and tunable, offering users diagnostic transparency and actionable control.Behind the technical challenges of building fair systems is a deeper question: who gets to decide what fairness means in a generative model? Is it the people who build the model, the companies that run the platform, the users, or the communities shown (or left out) in the results? Today, most systems are built with hidden ideas about fairness-often based on Western values-without ways for people to question or change them. But fairness isn't a fixed rule. It depends on culture, history, and politics.</figDesc><table><row><cell>Input Prompt</cell><cell>Bias Diagnosis</cell><cell></cell></row><row><cell>User Control</cell><cell>Mitigation Engine</cell><cell>Output Images</cell></row><row><cell cols="3">7.2 What Should a Bias Mitigation Strategy for TTI Models</cell></row><row><cell>Look Like?</cell><cell></cell><cell></cell></row><row><cell cols="3">Given the challenges of building fair text-to-image (TTI) models, we outline key requirements that</cell></row><row><cell cols="2">any effective bias mitigation strategy should meet.</cell><cell></cell></row><row><cell cols="3">1. Context-Aware and Prompt-Sensitive Bias in TTI models is not a fixed property of the</cell></row><row><cell cols="3">model, but a dynamic function of input prompts. The same identity attribute (e.g., gender)</cell></row></table><note><p><p><p><p>small textual edits can yield disproportionate and unpredictable visual changes, making mitigation strategies brittle and difficult to scale.</p>A</p>The question of which axes to mitigate further complicates the mitigation landscape. Should models always address gender, race, and age simultaneously? Or are there domains where one axis is more ethically or contextually salient than others? For example, a healthcare system generating patient education materials might prioritize racial inclusivity, while a fashion recommendation tool might focus on body-type diversity. Even among individual users, fairness goals may vary sharply: one might seek to remove only gender stereotypes, while another may wish to preserve race-specific cultural cues.</p>So, any real attempt to reduce bias must ask: Who sets the rules, who decides what trade-offs are okay, and who is responsible when things go wrong? If systems aren't open, user-focused, and built with community input, even the best models can end up repeating the same harms they were supposed to fix. may be rendered in radically different ways depending on scene elements, occupations, or aesthetics. Therefore, mitigation must occur at the level of prompt semantics, not token-level substitutions. An effective strategy must dynamically analyze how prompts interact with</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>4 .</head><label>4</label><figDesc>Modular Most deployed TTI models are large, closed-source, and difficult to retrain. Thus,</figDesc><table><row><cell>interventions risk being opaque, brittle, or misaligned with user goals.</cell></row><row><cell>6. Adaptive to different definitions of Fairness Finally, a mitigation strategy must acknowl-</cell></row><row><cell>edge that fairness is culturally situated and socially contested. What counts as appropriate</cell></row><row><cell>representation varies across communities, and no technical fix can substitute for participatory</cell></row><row><cell>engagement with those affected by generative outputs. Therefore, an ideal system must support</cell></row></table><note><p><p><p>any practical bias mitigation strategy must be training-free, operating as a lightweight postprocessing or pre-generation module. Such a strategy should be modular, capable of interfacing with a variety of TTI architectures (e.g., diffusion models, transformers) and diagnostic tools (e.g., prompt auditors, concept classifiers). This modularity also enables compatibility with evolving fairness frameworks and domain-specific applications.</p>5.</p>Transparent and Trade-off Aware Bias mitigation in generative systems inherently involves trade-offs-between performance and fairness, between different identity groups, and between competing stakeholder values. An ideal mitigation strategy must make these trade-offs auditable and visible. Users should be informed about the potential consequences of mitigation actions, including which intersections were improved or worsened, and what assumptions about the ideal distribution were made in the process. Without such transparency, fairness workflows where fairness preferences can be set, contested, and updated-whether by end users, institutions, or stakeholder communities.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head></head><label></label><figDesc>B 2 , . . . , B n }, we generate counterfactual prompts {CF 1 i , . . . , CF j i } for each B i ∈ B. These prompts are created by altering specific attributes (e.g., changing gender from male to female) and used with a TTI model to generate corresponding images. These counterfactual generations simulate interventions on a specific axis B x .</figDesc><table /><note><p><p><p>VQA-based Attribute Extraction. To extract semantic attributes relevant to bias analysis, we use a Visual Question Answering (VQA) model-MiniGPT-v2</p><ref type="bibr" target="#b203">[204]</ref></p>-to answer predefined, multiplechoice questions for each generated image. For example, for the gender axis, we ask: [vqa] What is the gender (male, female) of the person?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head></head><label></label><figDesc>After mitigation, we generate a new set of images, recompute τ , and continue the mitigation process if τ &gt; ϵ.</figDesc><table><row><cell cols="2">Algorithm 4 InterMit: Intersectional Mitigation</cell></row><row><cell>6:</cell><cell>Compute similarity score γ i ← ⟨s ′ i , p⟩</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell>8:</cell><cell>Identify target axis: i</cell></row></table><note><p><p><p><p>Require: Relevant bias axes B * ⊆ B, priority vector p ∈ R |B * | with ∥p∥ 1 = 1, sensitivity matrix S ∈ R n×|B * | , bias threshold ϵ, TTI model M Ensure: Final mitigated model M (t) with τ &lt; ϵ 1: Initialize model M (0) , set iteration counter t ← 0 2: repeat 3: Extract submatrix S ′ ∈ R n×|B * | from S 4: Extract priority vector p ∈ R |B * | 5:</p>for i = 1 to n do * ← arg max i γ i 9:</p>Mitigate axis i * to update model: M (t+1)</p>10:</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LIST OF FIGURES</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Datasets</head><p>Similar to Wang et al.'s approach <ref type="bibr" target="#b182">[183]</ref>, we use the CelebA <ref type="bibr" target="#b155">[156]</ref> and the UTK <ref type="bibr" target="#b183">[184]</ref> dataset. CelebA contains 202,599 images annotated with 40 attributes and we focus on Smiling, Big Nose, and Wavy Hair, with Gender as the protected attribute. Similarly, The UTK dataset comprises of over 20,000 images in the wild with only a single image per face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Training Details</head><p>While we experiment with various architectures to assess the generalizability of our approach, ResNet-18 <ref type="bibr" target="#b184">[185]</ref> serves as the default backbone architecture unless specified otherwise. The base model is pretrained for 50 epochs and subsequently fine-tuned using our proposed method for an additional 10 epochs, leveraging the same data distribution D. Training is conducted with a batch size of 128 using the Adam optimizer <ref type="bibr" target="#b185">[186]</ref>, with hyperparameters β 1 = 0.9, β 2 = 0.999, and ϵ = 1 × 10 -8 . Gradient clipping is applied with a maximum norm of 1.0.</p><p>To evaluate the effect of adversarial perturbations, we vary the noise magnitude ϵ within the range [0, <ref type="bibr" target="#b0">1]</ref>, where smaller values correspond to subtler perturbations and values closer to 1 indicate higher image corruption. Unless otherwise noted, we use noise magnitude values of 0.01, 0.001 and set the curriculum learning weight parameter to α = 0.5.</p><p>Experiments are run on multiple hardware configurations. Most training and fine-tuning experiments are performed on a compute cluster equipped with a Nvidia RTX A6000 GPU (48 GB VRAM), 384 GB RAM, and 20 CPU cores, with each run taking approximately 60 to 120 minutes. Additional runs are executed on an Apple M1 Pro system (8-core CPU, 14-core GPU, 16-core Neural Engine, and 16 GB RAM), and a Nvidia A100 GPU (40 GB VRAM) with 25 GB RAM for large-scale comparisons. Unless stated otherwise, all models are trained and fine-tuned using ResNet-18 with the aforementioned default hyperparameters. </p><p>remove</p><p>end if 10: end for 11: Repeat loop above for C cf 12:</p><p>Step 2: Add missing concepts 13: For any concept that is present in C init but not in C cf , add the concept into C c f with a frequency of 0, and vice versa. 14: Step 3: Compare Histograms 15: Re-order C init and C cf to the same order, as in the vocabulary, so that corresponding concept frequencies can be compared.  In this example, we see that humans rank racial bias to be more significant compared to gender bias, which is also observable in the images. We compare our VQA-based method to our CLIP-based method, and observe that the VQA-based method better aligns with human ranking. This is because, in most cases, biases are attributed to specific characteristics or parts of an image (which VQA helps us obtain), and not the semantic information of the image as a whole (which CLIP embeddings provide). This is in line with what we observe in User Study 2.</p><p>occupations exhibits a notable decrease, underscoring the successful mitigation of bias. Our proposed metric for bias identification effectively captures the reduction of bias achieved by a state-of-the-art method like ITI-GEN, reinforcing the credibility of our metrics.</p><p>In Figure <ref type="figure">5</ref>.10, we show the percentage difference (male%-female%) based on ground truth gender classification for the 48 images we generate for the initial prompt, and the 48 images generated after doing bias mitigation using ITI-GEN for the same prompt. This gender classification is conducted by a human participant who manually went through all 48 images before and after bias mitigation, and classified each image as "male", "female", or "other", where "other" usually implies that the image does not have a person in it, or only a part of the body is seen (e.g., hands) that is insufficient to classify gender. We can observe that the decrease in gender bias is consistent with distribution is completely biased and 0 indicates no bias.</p><p>Intervening on B x . Next, say we intervene on B x to simulate the mitigation of bias B x . This intervention ensures that all counterfactuals of B x are equally represented in the generated images.</p><p>For example, if B x is gender bias, we enforce equal proportions of male and female individuals in the dataset. This intervention is in line with most bias mitigation methods proposed for TTI models, like ITI-GEN <ref type="bibr" target="#b189">[190]</ref>. Using our counterfactuals along B x , we sum the distributions on B y across all counterfactuals of B x . This sum across the counterfactuals of B x yields a new empirical distribution of B y , denoted D Bx By , simulating the effect of mitigating B x (See Figure <ref type="figure">6</ref>.5). We compute its Wasserstein distance from the ideal distribution.</p><p>Computing Intersectional Sensitivity . To quantify the effect of mitigating B x on B y , we define the metric, Intersectional Sensitivity , as: IS xy = 0, mitigating B x has no effect on B y . This approach enables us to assess whether addressing one bias (e.g., gender) improves or worsens another (e.g., ethnicity) in generative models, providing a systematic way to evaluate trade-offs and unintended consequences in bias mitigation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.4">Visualization</head><p>To visualize IS scores comprehensively, we use a Bias Intersectionality Matrix S, where each entry IS ij quantifies the effect of intervening on row B i on column B j for mitigation. This matrix captures directional dependencies and enables a structured analysis of intersectional bias effects.</p><p>TIBET dataset: We use the same dataset introduced in the TIBET framework, which consists of 100 creative prompts paired with unique LLM-generated bias axes and corresponding counterfactuals.</p><p>This dataset enables evaluation across a diverse range of social and aesthetic biases. For each prompt and its counterfactuals, the dataset includes 48 images generated using Stable Diffusion 2.1, providing a rich basis for systematic bias analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.2">Studying prompt-level intersectionality</head><p>BiasConnect supports prompt-level analysis of intersectional biases (Figure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.3">Validating Intersectional Sensitivity</head><p>To evaluate the effectiveness of our framework, we examine how well our Intersectional Sensitivity Score (Intersectional Sensitivity ) predicts the downstream effects of counterfactual-based bias mitigation. Specifically, our goal is to estimate how intervening on one bias dimension (B x ) impacts another (B y ), without actually performing the full mitigation step. This predictive capability is crucial, as real-time mitigation across large prompt sets and multiple models is often computationally expensive.</p><p>To validate our estimates, we conduct an experiment using two mitigation strategies: ITI-GEN xy across all B x → B y pairs. This correlation quantifies how accurately our original estimates capture the real-world effect of mitigation.</p><note type="other">on</note><p>Our results show a strong average correlation of +0.65 across occupations using ITI-GEN, with particularly high correlations for specific prompts such as musician (+0.91), accountant (+0.81), and lawyer (+0.82). For hardprompt, the correlation is even stronger-averaging +0.95-which is expected since the mitigation prompts are closely aligned with the counterfactuals used in our estimation process.  Given the large number of images (as we aggregate over multiple sets), we choose to use a p-value threshold of 0.00005, and we further discard edges in the pairwise causal graph where the -0.03 &gt; IS global(xy) &gt; 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.1.2">Uncovering optimal Bias mitigation strategies</head><p>Identifying high-impact biases. Some biases act as primary sources, influencing multiple others, while some function as effects, shaped by upstream factors. A node's impact is measured by its outgoing edges (MaxImp, Table <ref type="table">6</ref>.2), while its susceptibility to influence is quantified by incoming  </p><p>where 0 indicates no bias and 1 indicates maximal deviation from the ideal.</p><p>2. Intervening on B x . We simulate a mitigation of B x by ensuring equal representation of its counterfactual values during image generation. This results in a new attribute distribution for B y , denoted D Bx By , and we compute its Wasserstein distance to the ideal:</p><p>We then compute the intersectional sensitivity score as:</p><p>A positive score (IS xy &gt; 0) indicates that mitigating B x improves fairness in B y , while a negative score (IS xy &lt; 0) suggests it worsens B y . A score of zero means there is no effect. This directional metric helps identify both positive outcomes and unintended consequences of single-axis interventions.</p><p>Visualization. To summarize and interpret intersectional effects, we build a bias intersectionality matrix S, where each entry IS ij represents the effect of mitigating bias axis B i (row) on bias axis B j (column). This matrix supports structured analysis of intersectional dependencies and can guide better prioritization and sequencing of bias mitigation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Multi Step Mitigation</head><p>Given the aforementioned information, we first calculate a bias score for initial model M (0) by taking the dot product of the priority vector p and the initial measures of biases B * (w init B * ) computed using APPENDIX A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definitions of Biases</head><p>Our aim is to quantify and establish a framework for analyzing biases in generative Text-to-Image (TTI) models. While these biases can take diverse forms, it's helpful to categorize them into two distinct groups: Societal Biases: These biases encompass the biases that are of societal concern. They are characterized by the presence of unfair or harmful associations between attributes within the generated images <ref type="bibr" target="#b219">[220]</ref>. These biases can stem from various sources, including the training data, and they have the potential to perpetuate and reinforce societal inequalities.</p><p>Incidental Correlations: This includes non-harmful correlations in the generated dataset, stemming from statistical training data correlations, incidental endogeneity, or spurious connections introduced by the TTI model <ref type="bibr" target="#b220">[221,</ref><ref type="bibr" target="#b221">222]</ref>. While not directly harmful, they can impact image generation diversity. As mentioned in Section 5.1, for simplicity, we use the word 'bias' to refer to either societal biases or incidental correlations, as the LLMs we use can detect both kinds of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 CAS scores</head><p>The Concept Association Score (CAS) tells us how similar the set of images for the initial prompt are to the set of images of a counterfactual prompt.</p><p>In the case of VQA, for each set of images:</p><p>1. First, we use VQA with MiniGPT-v2 to obtain answers for each image. All answers are combined into a single string.</p><p>2. This string undergoes processing to remove punctuation and stop words.</p><p>3. We use the FreqDict function in NLTK (<ref type="url" target="https://www.nltk.org/api/nltk.probability">https://www.nltk.org/api/nltk.probability</ref>.</p><p>FreqDist.html) to obtain a list of words and their corresponding word frequencies. We normalize this frequency by the number of images in the set (in our setting, 48). This represents a set of concepts, C = {(c 1 , w 1 )...}.</p><p>Once we complete this process for the initial set (to obtain C init ) and for a counterfactual set (to obtain C cf ), we apply Algorithm 3 to obtain the CAS score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MAD scores</head><p>We leverage the MAD metric to measure the amount of bias by computing the variability in CAS scores. We normalize MAD to make it comparable across bias axes with different number of counterfactuals. Specifically, we normalize such that our MAD score for the "most skewed" list of CAS scores for any length K is equal to 1. Therefore, we first create a vector of length K such that all numbers are 0, except one 1. The MAD score for this vector is the maximum MAD score we can obtain for a CAS score list of length K. For simplicity, let us call this M AD K , normalized as follows:</p><p>For simplicity, we use M AD to refer to M AD normalized in our results and figures. Alternatives to M AD. We considered different alternatives in place of MAD; however, we selected our (normalized) MAD as it achieved low error amplification on the sensitivity analysis compared to other metrics. At 18% VQA error rate, we observed a M AD change of 13.11% (lower is better). We attempted to use two other metrics: (1) Wasserstein distance between our scores, CAS b K from Eq. 5, to the uniform distribution of CAS scores of length K (which indicates no bias). With the same normalization strategy, this metric changes by 15.15%. ( <ref type="formula">2</ref> We conduct the user study using the Amazon Mechanical Turk platform. Prior to commencing the study, we provide workers with comprehensive information regarding various aspects of our research. This includes explanations about TTI models, their purposes, the types of inputs and outputs they handle, and an elucidation of biases.</p><p>Specifically, we categorize biases into two distinct types, which we previously referred to as "societal" and "incidental" biases. We offer a detailed definition of these biases and provide practical examples within the training materials to help users grasp these concepts.</p><p>In the course of the study, each user is tasked with evaluating input prompts for the presence of two types of bias:</p><p>1. Societal Bias Related to "Axes": Is there evidence of "Axes-related" societal bias in the prompt?</p><p>2. Incidental Bias Related to "Axes": Is there evidence of "Axes-related" incidental bias in the prompt?</p><p>where "Axes" is replaced with the actual bias axis name obtained using the LLM. This approach ensures that users are equipped to identify and assess these specific biases in the prompts they encounter during the study. Each user participating in our study is presented with five biases related to the displayed prompt in every HIT. In our study, we only display the prompts to the user, omitting the accompanying images. This is because GPT-3 is also coming up with bias axes just by looking at the prompt. Additionally, we include attention-check questions for all users to ensure they are actively engaged in the study. Users are compensated at a rate of $0.15 for each task they complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Training Details and Qualification Test</head><p>The participants in our user study are from Canada, the United States, and the United Kingdom. We administer a qualification test that assesses their comprehension of the training materials and </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Science faculty&apos;s subtle gender biases favor male students</title>
		<author>
			<persName><forename type="first">Corinne</forename><forename type="middle">A</forename><surname>Moss-Racusin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Dovidio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><forename type="middle">L</forename><surname>Brescoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Handelsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="16474" to="16479" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Counterfactuals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards accuracy-fairness paradox: Adversarial example-based data augmentation for visual debiasing</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitao</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fair attribute classification through latent space de-biasing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunnie</forename><forename type="middle">Sy</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9301" to="9310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image counterfactual sensitivity analysis for detecting unintended bias</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06439</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards fairness in visual recognition: Effective strategies for bias mitigation</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Karakozis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8919" to="8928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating and mitigating bias in image classifiers: A causal perspective using counterfactuals</title>
		<author>
			<persName><forename type="first">Saloni</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards causal benchmarking of biasin face analysis algorithms</title>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning-Based Face Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="327" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meaningfully debugging model mistakes using conceptual counterfactual explanations</title>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causalm: Causal model explanation through counterfactual language models</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="386" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cavli-using image associations to produce local concept-based explanations</title>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushil</forename><surname>Bharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3749" to="3754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tibet: Identifying and evaluating biases in text-to-image generative models</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chinchure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiri</forename><surname>Salij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Hosanagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory, and antiracist politics</title>
		<author>
			<persName><forename type="first">Kimberle</forename><surname>Crenshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="139" to="167" />
		</imprint>
		<respStmt>
			<orgName>University of Chicago Legal Forum</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Utilizing adversarial examples for bias mitigation and accuracy enhancement</title>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.11819</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mitigate one, skew another? tackling intersectional biases in text-to-image models</title>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chinchure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tolbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Hosanagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><surname>Turk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2505.17280</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counterfactual visual explanations</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box: Automated decisions and the gdpr</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Journal of Law &amp; Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramaravind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models</title>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6707" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Counterfactual sensitivity and robustness</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Connault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="298" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning the difference that makes a difference with counterfactually-augmented data</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12434</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Von Fintel</surname></persName>
		</author>
		<author>
			<persName><surname>Counterfactuals</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu/entries/counterfactuals/,2021" />
		<title level="m">Stanford Encyclopedia of Philosophy</title>
		<imprint>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Argumentation and counterfactual reasoning in parmenides and melissus. Archai: Revista de Estudos sobre as Origens do Pensamento Ocidental</title>
		<author>
			<persName><forename type="first">André</forename><surname>Laks</surname></persName>
		</author>
		<ptr target="https://www.scielo.br/j/archai/a/Cv7GBGFGwMZDTfQfmWK99Pr/?lang=en" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Philosophy in Classical India: The Proper Work of Reason</title>
		<author>
			<persName><forename type="first">Jonardon</forename><surname>Ganeri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Character of Logic in India</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Bimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kisor</forename><surname>Matilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chakrabarti</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>State University of New York Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recognizing Reality: Dharmakīrti&apos;s Philosophy and Its Tibetan Interpretations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><surname>Dreyfus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>SUNY Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An Enquiry Concerning Human Understanding. A. Millar, 1748</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hume</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1748">1748</date>
		</imprint>
	</monogr>
	<note>many modern editions available</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Reid</surname></persName>
		</author>
		<title level="m">Essays on the Active Powers of the Human Mind. Bell &amp; Bradfute</title>
		<imprint>
			<biblScope unit="page">1788</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Critique of Pure Reason. Hartknoch, 1781. English translations by Norman Kemp Smith and others available</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Kant</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mill</forename></persName>
		</author>
		<title level="m">A System of Logic, Ratiocinative and Inductive. Parker, 1843. See Book III for the Method of Difference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Emotions and the Will</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bain</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1859</biblScope>
			<pubPlace>Longman, Green, Longman, and Roberts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient search for diverse coherent explanations</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 FAT* Conference</title>
		<meeting>the 2020 FAT* Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-objective counterfactual explanations</title>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Dandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="961" to="989" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face: Feasible and actionable counterfactual explanations</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Poyiadzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kacper</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tijl</forename><surname>De Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model-agnostic counterfactual explanations for consequential decisions</title>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><surname>Valera</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Focus: Flexible optimizable counterfactual explanations for tree ensembles</title>
		<author>
			<persName><forename type="first">Ana-Marija</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinda</forename><surname>Haned</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2022 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hidden assumptions in counterfactual explanations and fairness</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Certifai: A common framework to provide explanations and recourse through counterfactuals</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07831</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1364" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Path-specific counterfactual fairness</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on Artificial Intelligence</title>
		<meeting>the AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7801" to="7808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fairness through causal awareness: Learning causal latent representations for fair prediction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 35th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Counterfactual fairness in text classification through robustness</title>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditional counterfactuals for fairness audits in nlp</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fliptest: Fairness testing via statistical and semantic counterfactual generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Constructing and measuring visual counterfactuals for grounded bias analysis</title>
		<author>
			<persName><forename type="first">Aparna</forename><surname>Balagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vajira</forename><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sigurdur</forename><surname>Olafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egil</forename><surname>Arnesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Responsible Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Evaluating bias and fairness in pretrained multimodal models</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11756</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Counterfactual fairness through semantic data augmentation</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munmun</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Actionable recourse in linear classification</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Spangher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Reassessing the robustness of visual counterfactual explanations</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Herlands</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08130</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguists</title>
		<meeting>Association of Computational Linguists</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond exploding and vanishing gradients: analysing rnn training using attractors and smoothness</title>
		<author>
			<persName><forename type="first">Koen</forename><surname>Antônio H Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">A</forename><surname>Tiels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Aguirre</surname></persName>
		</author>
		<author>
			<persName><surname>Schön</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2370" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fredrik D Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on machine Learning</title>
		<meeting>the International conference on machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on machine Learning</title>
		<meeting>the International conference on machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ganite: Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Der</forename><surname>Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learn: Controllable counterfactual generation via latent disentanglement. Findings North American Chapter</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandresh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A survey on counterfactual data generation for text</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04702</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Explanation by progressive exaggeration</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Explaining image classifiers using statistical fault localization</title>
		<author>
			<persName><forename type="first">Chun-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Explaining black-box classifiers using post-hoc explanations: A unified review</title>
		<author>
			<persName><forename type="first">Kedar</forename><surname>Dhamdhere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On robustness of causal claims in text classification</title>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Preserving semantic relations for zero-shot learning</title>
		<author>
			<persName><forename type="first">Dibya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><surname>Mishra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05071</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for robust nlp</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning in multimodal language models</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Anchors: High-precision modelagnostic explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Grounding visual explanations</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Algorithmic recourse: From counterfactual explanations to interventions</title>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Generating interpretable counterfactual explanations by structural interventions</title>
		<author>
			<persName><forename type="first">Dibya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</title>
		<meeting>the 3rd Innovations in Theoretical Computer Science Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05807</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Fairness metrics: A comparative study</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Trading off fairness and accuracy in algorithmic recourse</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Roessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harvineet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2022 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics : Human Language Technologies</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics : Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The risk of racial bias in hate speech detection</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Counterfactual probing for understanding gender bias in masked language models</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Prabhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter</title>
		<meeting>the Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Adversarial removal of demographic attributes from text data</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Investigating gender bias in language models using causal mediation analysis</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The red team blues: A survey of failures in rlhf models</title>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07858</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Zelikman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><surname>Prabhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01963</idno>
		<title level="m">Multimodal datasets: Misogyny, pornography, and malignant stereotypes</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Inioluwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Ethical dilemmas in generative vision systems</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">T2iat: Measuring valence and stereotypical biases in text-to-image generation</title>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Gabby Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Zala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3043" to="3054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">&apos;person&apos;== light-skinned, western man, and sexualization of women of color</title>
		<author>
			<persName><forename type="first">Sourojit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19981</idno>
	</analytic>
	<monogr>
		<title level="m">Stereotypes in stable diffusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Easily accessible text-to-image generation amplifies demographic stereotypes at large scale</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2023 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1493" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Fair diffusion: Instructing text-to-image generation models on fairness</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Struppek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Hintersdorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10893</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Mitigating stereotypical biases in text to image generative systems</title>
		<author>
			<persName><forename type="first">Piero</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parmida</forename><surname>Atighehchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Germanidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06904</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09010</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Datasheets for datasets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="771" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Counterfactual fairness in text-to-image generation</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">Mert</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maggie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15480</idno>
		<title level="m">Post-hoc concept bottleneck models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Datasheets for datasets help stakeholders understand social impacts</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Killing boogeymen: Phallicism and the misandric mischaracterizations of black males in theory</title>
		<author>
			<persName><forename type="first">Tommy</forename><forename type="middle">J</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res Philosophica</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Williams</forename><surname>Tolbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11112</idno>
		<title level="m">Correcting underrepresentation and intersectional bias for classification</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Nikolaos Theologitis, Dimitrios Rontogiannis, Dimitris Fotakis, and Ioannis Emiris. Fairness aware counterfactuals for subgroups</title>
		<author>
			<persName><forename type="first">Loukas</forename><surname>Kavouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Tsopelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Sacharidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Psaroudaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="58246" to="58276" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Preventing fairness gerrymandering: Auditing and learning for subgroup fairness</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2564" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Multicalibration: Calibration for the (Computationally-identifiable) masses</title>
		<author>
			<persName><forename type="first">Ursula</forename><surname>Hebert-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Rothblum</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/hebert-johnson18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Minimax group fairness: Algorithms and experiments</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Diana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashidul</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Characterizing intersectional group fairness with worst-case comparisons</title>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Genuit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Reagan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Diversity, Belonging, Equity, and Inclusion</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Benchmarking intersectional biases in nlp</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendall</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference of the North American chapter of the Association for Computational Linguistics: Human language technologies</title>
		<meeting>the conference of the North American chapter of the Association for Computational Linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3598" to="3609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Detecting intersectionality in ner models: A data-driven approach</title>
		<author>
			<persName><forename type="first">Ida</forename><surname>Marie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Almasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Enevoldsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Deans Kristensen-Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</title>
		<meeting>the 7th joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="116" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="122" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Elisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models</title>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yennie</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haider</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Benussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Volpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><forename type="middle">A</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Shtedritski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04130</idno>
		<ptr target="http://arxiv.org/abs/2102.04130" />
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Intersectional stereotypes in large language models: Dataset and analysis</title>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8589" to="8597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">We don&apos;t talk about that: case studies on intersectional analysis of social bias in large language models</title>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Devinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Björklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Björklund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Gender Bias in Natural Language Processing (GeBNLP)</title>
		<meeting><address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-08-16">16th August, 2024. 2024</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Explicitly unbiased large language models still form biased associations</title>
		<author>
			<persName><forename type="first">Xuechunzi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2416228122</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Socialcounterfactuals: Probing and mitigating intersectional social biases in vision-language models with counterfactual examples</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Madasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">Lujan</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anahita</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasudev</forename><surname>Lal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="11975" to="11985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Racial and intersectional debiasing of contrastive language image pretraining</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Hoepfinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Georgia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Image representations learned with unsupervised pretraining contain human-like biases</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Steed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="701" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on machine Learning</title>
		<meeting>the International conference on machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Towards automatic conceptbased explanations</title>
		<author>
			<persName><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Concept whitening for interpretable image recognition</title>
		<author>
			<persName><forename type="first">Chun-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with selfexplaining neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">This looks like that: deep learning for interpretable image recognition</title>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Concept bottleneck models</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Hassani Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Disentangled interventional video explanation (dive)</title>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Learning model-agnostic counterfactual explanations for tabular data</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pawelczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Broelemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Countergan: Generating realistic counterfactuals with residual gans</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalmali</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Robustness in Sequence Modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Fido: Feature importance for deep networks via orthogonal projections</title>
		<author>
			<persName><forename type="first">Chun-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Explain yourself! leveraging language models for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Vqa-x: Visual explanations for visual question answering</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Multimodal explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Can language models learn from explanations in context?</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02329</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Measuring faithfulness in natural language explanations with diagnostic classification</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Loreaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Blanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08641</idno>
		<title level="m">Best of both worlds: local and global explanations with human-understandable concepts</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Model-agnostic interpretability of machine learning</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05386</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Learning not to learn: Training deep neural networks with biased data</title>
		<author>
			<persName><forename type="first">Byungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9012" to="9020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations</title>
		<author>
			<persName><forename type="first">Laleh</forename><surname>Seyyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">Ba</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2176" to="2182" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Gender artifacts in visual datasets</title>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4837" to="4848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Revise: A tool for measuring and mitigating bias in visual datasets</title>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anat</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iroha</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1790" to="1810" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Fair loss: Margin-aware reinforcement learning for deep face recognition</title>
		<author>
			<persName><forename type="first">Bingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10052" to="10061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Fair sa: Sensitivity analysis for fairness in face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aparna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nivedha</forename><surname>Suau Cuadros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName><surname>Apostoloff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Fairness Through the Lens of Causality and Robustness Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="40" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Overwriting pretrained bias with finetuning data</title>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3957" to="3968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">It&apos;s all in the name: Mitigating gender bias with name-based counterfactual data substitution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rowan</surname></persName>
		</author>
		<author>
			<persName><surname>Maudslay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Hila Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><surname>Teufel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1530</idno>
		<ptr target="https://aclanthology.org/D19-1530" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Adversarial examples in physical world</title>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4925" to="4926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Geoda: a geometric framework for black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyu</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8446" to="8455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks on video recognition models</title>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Narodytska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.172</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE/CVF Conference Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pawelczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalmali</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohini</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4574" to="4594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Interpretable counterfactual explanations guided by prototypes</title>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Van Looveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janis</forename><surname>Klaise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="650" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00075</idno>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Semanticadv: Generating adversarial examples via attribute-conditioned image editing</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="19" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Biasadv: Bias-adversarial augmentation for model debiasing</title>
		<author>
			<persName><forename type="first">Jongin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byungjai</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanho</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3832" to="3841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Score-cam: Score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Haofan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Teacher-student curriculum learning</title>
		<author>
			<persName><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3732" to="3740" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Adaptive curriculum learning</title>
		<author>
			<persName><forename type="first">Yajing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5067" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Assessing multilingual fairness in pre-trained multimodal representations</title>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2681" to="2695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Stable bias: Evaluating societal representations in diffusion models</title>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Mitigating the effect of incidental correlations on part-based learning</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Iti-gen: Inclusive text-to-image generation</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanbai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3969" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Social biases in nlp models as barriers for persons with disabilities</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Denuyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5491" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Predictive biases in natural language processing models: A conceptual framework and overview</title>
		<author>
			<persName><forename type="first">Deven Santosh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Andrew</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5248" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ismael</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Arturo</forename><surname>Montejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernando Martínez-S</surname></persName>
		</author>
		<author>
			<persName><surname>Alfonso Ureña-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A survey on bias in deep nlp. Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3184</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Mitigating language-dependent ethnic bias in bert</title>
		<author>
			<persName><forename type="first">Jaimeen</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="533" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Identifying implicit social biases in vision-language models</title>
		<author>
			<persName><forename type="first">Kimia</forename><surname>Hamidieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hartvigsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">The bias amplification paradox in text-toimage generation</title>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00755</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Scaling fair learning to hundreds of intersectional groups</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Fair-vqa: Fairnessaware visual question answering through sensitive attribute prediction</title>
		<author>
			<persName><forename type="first">Sungho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunhee</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongkwang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="215091" to="215099" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<author>
			<persName><forename type="first">Lavisha</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01888</idno>
		<title level="m">Fairness in ai systems: Mitigating gender bias from language-vision models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Gender and racial bias in visual question answering datasets</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1280" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09478</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Munechika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14896</idno>
		<ptr target="https://arxiv.org/abs/2210.14896" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Divide, evaluate, and refine: Evaluating and improving text-to-image alignment with iterative vqa feedback</title>
		<author>
			<persName><forename type="first">Jaskirat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11897</idno>
		<title level="m">Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Mapping the margins: Intersectionality, identity politics, and violence against women of color</title>
		<author>
			<persName><forename type="first">Kimberle</forename><surname>Crenshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Stanford Law Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1241" to="1299" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Kamyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation</title>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3533101</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3531146.3533101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22</title>
		<meeting>the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title/>
		<author>
			<persName><surname>Blackforestlabs</surname></persName>
		</author>
		<author>
			<persName><surname>Flux</surname></persName>
		</author>
		<ptr target="https://github.com/black-forest-labs/flux" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Openbias: Open-set bias detection in text-to-image generative models</title>
		<author>
			<persName><forename type="first">Elia</forename><surname>Moreno D'incà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Peruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejia</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidit</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingqian</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="12225" to="12235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<monogr>
		<title level="m" type="main">Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation</title>
		<author>
			<persName><forename type="first">Daiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleks</forename><surname>Kamko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Akhgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhail</forename><surname>Doshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Arseniy</forename><surname>Shakhmatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Razzhigaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Nikolich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Arkhipkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><surname>Kandinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-02-02">2.2, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">An improved text-to-image synthesis with image prior and latent diffusion</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Razzhigaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arseniy</forename><surname>Shakhmatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Maltseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Arkhipkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Ryabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Kuts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><surname>Kandinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Exploring clip for assessing the look and feel of images</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><forename type="middle">C K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2555" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Unified concept editing in diffusion models</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Gandikota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzyńska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5111" to="5120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">AI now report</title>
		<author>
			<persName><forename type="first">Meredith</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roel</forename><surname>Dobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Kaziunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varoon</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Mysers</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashida</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
		<respStmt>
			<orgName>AI Now Institute at New York University New York</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<title level="m" type="main">Mitigating the effect of incidental correlations on part-based learning</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Challenges of big data analysis</title>
		<author>
			<persName><forename type="first">Jianqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="314" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
