<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation</title>
				<funder ref="#_ZQx2s3h #_AdZ4q2p #_tCNhs7W #_3y5CqpJ">
					<orgName type="full">Research Fund of Guangxi Key Lab of Multi-source Information Mining &amp; Security</orgName>
				</funder>
				<funder>
					<orgName type="full">Project of Guangxi Science and Technology (GuiKeAB23026040)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-10-14">14 Oct 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Guangxi Key Lab of MSMI; MOE Key Lab of EBIT</orgName>
								<orgName type="institution">Guangxi Normal University</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hainan University</orgName>
								<address>
									<postCode>570228</postCode>
									<settlement>Haikou Hainan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Guangxi Key Lab of MSMI; MOE Key Lab of EBIT</orgName>
								<orgName type="institution">Guangxi Normal University</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Guangxi Key Lab of MSMI; MOE Key Lab of EBIT</orgName>
								<orgName type="institution">Guangxi Normal University</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-14">14 Oct 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.00382v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Fairness Graph neural networks Causality Knowledge distillation Representation learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilization of available graph information. To address this tradeoff, we propose an effective strategy to balance fairness and utility in knowledge distillation. Specifically, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation, leveraging a causal graph model to guide and optimize the design of the distillation process. In particular, FairDTD employs two fairness-oriented teacher models: a feature teacher and a structure teacher, to facilitate dual distillation, with the student model learning fairness knowledge from the teachers while also leveraging full data to mitigate utility loss. To enhance information transfer, we incorporate graph-level distillation to provide an indirect supplement of graph information during training, as well as a node-specific temperature module to improve the comprehensive transfer of fair knowledge. Experiments on diverse benchmark datasets demonstrate that FairDTD achieves optimal fairness while preserving high model utility, showcasing its effectiveness in fair representation learning for GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) have garnered widespread attention in recent years due to their exceptional ability to model non-Euclidean structured data, such as graphs, achieving remarkable success across various domains. For instance, in drug discovery, GNNs effectively capture the complex interactions between atoms and chemical bonds within molecular graphs, thereby improving the accuracy of molecular property prediction and drug screening <ref type="bibr" target="#b0">[1]</ref>. In recommendation systems, GNNs model the interaction graph between users and items to uncover latent collaborative filtering patterns, significantly enhancing recommendation performance <ref type="bibr" target="#b1">[2]</ref>. Unlike traditional machine learning models, the key advantage of GNNs lies in their structure-aware capability: through iterative neighborhood feature aggregation, GNNs enable context-aware node representations, making them highly expressive and adaptable in realworld tasks where graph-structured data is prevalent.</p><p>However, due to the neighborhood aggregation mechanism inherent in GNNs, the model may inherit and even amplify underlying social biases present in graph data, leading to discriminatory predictions influenced by sensitive attributes such as gender and race <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In applications involving high-risk decision-making, such as fraud detection <ref type="bibr" target="#b4">[5]</ref> and credit scoring <ref type="bibr" target="#b5">[6]</ref>, these biased predictions can exacerbate social inequities <ref type="bibr" target="#b6">[7]</ref>. Additionally, GNNs model on non-independent and identically distributed graph-structured data, while most traditional fair machine learning methods rely on the independent and identically distributed assumption. This makes it difficult for them to be directly applied to graph neural networks. Therefore, achieving fairness within the GNNs framework is of higher complexity and challenge. To address this, researchers have proposed fair graph representation learning. Existing research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> attributes bias in GNNs to both node features and graph structure. Through feature propagation, initially unbiased node features can become correlated with sensitive attributes, leading to unintended leakage of sensitive information in node representations <ref type="bibr" target="#b9">[10]</ref>. Additionally, nodes sharing similar sensitive attributes are frequently more interconnected in graph-structured data <ref type="bibr" target="#b10">[11]</ref>, causing similarity in representations that can result in prediction bias.</p><p>Recent studies have focused on reducing bias in GNNs by enhancing model fairness while preserving utility. A common strategy is to modify the training data to preemptively mitigate bias. For instance, this can involve increasing the representation of disadvantaged groups <ref type="bibr" target="#b11">[12]</ref> or updating the adjacency matrix with fairness-based constraints <ref type="bibr" target="#b12">[13]</ref>. Such preprocessing steps aim to reduce inherited bias during training. Another approach introduces fairness constraints directly into the GNN's objective function. For example, FairGAT <ref type="bibr" target="#b13">[14]</ref> employs a bias-mitigating attention mechanism, while FairGNN <ref type="bibr" target="#b14">[15]</ref> employs adversarial learning to generate node representations independent of sensitive attributes. However, these methods face limitations: <ref type="bibr" target="#b0">(1)</ref> bias exists in both node features and graph structure, which requires complex model design to handle both sources simultaneously; <ref type="bibr" target="#b1">(2)</ref> methods that eliminate sensitive attributes are often highly specific, reducing their generalizability to diverse applications. Note that there is a critical question: can we develop a simpler yet effective approach to improve fairness by addressing biases in both node features and graph structure directly within the training data?</p><p>Inspired by reducing the sources of data bias <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, it has been confirmed that simply training with partial data (i.e., using only node features or only graph structure) can significantly improve fairness and is readily extensible to other models <ref type="bibr" target="#b19">[20]</ref>. However, as we analyzed in Section 3.3, while partial data training substantially enhances fairness, it often comes at the cost of reduced model utility.</p><p>Therefore, the key challenge in training with partial data lies in effectively balancing fairness and utility. Existing studies have explored different approaches to achieve a better trade-off. For instance, FUGNN <ref type="bibr" target="#b20">[21]</ref>, based on spectral graph theory, addresses this balance by optimizing the feature vector distribution and truncating the spectrum. Similarly, FairSAD <ref type="bibr" target="#b21">[22]</ref> balances fairness and utility by disentangling sensitive attributes and employing a channel masking mechanism to separate sensitive information while preserving task-relevant information. However, these methods are primarily designed for training on complete data and fail to address the trade-off between fairness and utility when working with partial data.</p><p>In this context, knowledge distillation <ref type="bibr" target="#b22">[23]</ref> offers a novel and promising solution for balancing fairness and utility by passing knowledge from a teacher model to a student model. Building on this idea, we propose a strategy where a student model extracts fairness knowledge from a teacher model trained on partial data while also learning from complete data to maintain utility. However, this strategy faces two challenges: (1) Utility loss in the teacher model: although teacher models trained on partial data can enhance fairness, their utility often declines due to limited data completeness, which may in turn limit its guiding role for the student model; (2) Limited knowledge transfer capability: although teacher models trained on partial data may acquire fairness-related knowledge, it remains unclear whether this knowledge can be transferred to the student model in a stable and effective manner. Moreover, most existing knowledge distillation methods employ a fixed temperature parameter, which fails to account for the varying complexity of prediction across different samples and may lead to imbalanced guidance during the distillation process <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To tackle these challenges, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation. Drawing on a causal modeling perspective, we first conduct a systematic theoretical analysis of fair representation learning from partial data training. To address utility loss, FairDTD introduces two fairnessaware teacher models: a feature teacher and a structure teacher. It enables the student model to learn fairness-aware knowledge from these complementary sources through a dual distillation mechanism. Graph distillation is integrated to compensate for missing structural information during training. To tackle limited knowledge transfer, we design a node-specific temperature module that dynamically adjusts the distillation temperature based on prediction confidence, improving both precision and adaptability in knowledge transfer.</p><p>Our contributions can be summarized as follows:</p><p>â€¢ We introduce a causal graph model and conduct both theoretical and empirical analyses of fair representation learning under partial data training.</p><p>â€¢ We propose a novel dual-teacher distillation approach (FairDTD) combined with graph distillation and an adaptive temperature mechanism for more effective and fairness-aware knowledge transfer.</p><p>â€¢ Experiments on multiple real-world datasets show that FairDTD achieves a better balance between fairness and utility, significantly enhancing fairness without compromising model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we discuss relevant studies pertinent to our presented FairDTD framework, focusing on developments in GNNs and methods aimed at promoting fairness within GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph neural networks</head><p>In recent years, GNNs have attracted considerable attention owing to their exceptional performance in handling graph-structured data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. To be used in a variety of downstream tasks, including node classification <ref type="bibr" target="#b28">[29]</ref>, graph classification <ref type="bibr" target="#b29">[30]</ref>, and link prediction <ref type="bibr" target="#b30">[31]</ref>, GNNs aim to generate embedding vectors that efficiently integrate both graph structure and node features.</p><p>Existing GNN methods primarily focus on spatial-based approaches, which perform message passing directly on the graph's adjacency structure, aggregating information from neighboring nodes layer by layer. For instance, Graph Convolutional Network (GCN) <ref type="bibr" target="#b31">[32]</ref> uses a convolutional operation to aggregate the features of neighboring nodes to update node representations. Graph Isomorphism Network (GIN) <ref type="bibr" target="#b32">[33]</ref> introduces a straightforward yet potent messagepassing mechanism to ensure that different graph structures yield distinct node representations.</p><p>The impressive performance of GNNs has facilitated their extensive adoption in a variety of real-world applications, including key decision-making <ref type="bibr" target="#b33">[34]</ref> and anomaly detection in online banking transactions <ref type="bibr" target="#b34">[35]</ref>. For instance, HGNRec <ref type="bibr" target="#b35">[36]</ref> employs homogeneous GNNs for third-party library recommendations, LA-MGFM <ref type="bibr" target="#b36">[37]</ref> uses graph neural networks with multi-graph fusion to predict legal judgments, and MLAGs <ref type="bibr" target="#b37">[38]</ref> leverages multi-view graph convolutional networks to predict loan default risk. Additionally, a category of methods based on GNNs models local statistical channels across multiple geographic grids by integrating heterogeneous Markov graphs with enhanced variational Bayesian inference, achieving efficient, accurate, and highly interpretable wireless channel angle power spectrum estimation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>However, the success of GNNs has prompted concerns about their potential to propagate and amplify biases in graph-structured data. Studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref> have highlighted that GNNs may unintentionally produce biased outcomes against certain demographic groups, underscoring the need for designing fairer GNN models for graph-based tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fairness in graph neural networks</head><p>When processing graph-structured data, GNNs are susceptible to biases resulting from data imbalances, just like traditional machine learning models, which might produce discriminatory outcomes <ref type="bibr" target="#b41">[42]</ref>. This has prompted growing interest in fairness within GNNs, often categorized into group fairness <ref type="bibr" target="#b42">[43]</ref>, counterfactual fairness <ref type="bibr" target="#b43">[44]</ref>, and individual fairness <ref type="bibr" target="#b44">[45]</ref>. In this work, we concentrate on group fairness, aiming to ensure that predictions are unbiased across demographic groups defined by sensitive attributes.</p><p>The two main methods to improve group fairness in GNNs are pre-processing methods and in-training methods. Pre-processing methods aim to mitigate data bias before training. For example, EDITS <ref type="bibr" target="#b45">[46]</ref> reduces distributional discrepancies between demographic groups by modifying graph structure and node attributes. FairDrop <ref type="bibr" target="#b46">[47]</ref> applies a fair edge dropout strategy to address structural bias in graphs. SRGNN <ref type="bibr" target="#b47">[48]</ref> balances graph structure bias by reducing neighbors of high-degree nodes and increasing connections for low-degree nodes. In-training methods modify the GNN objective function to promote fairness during representation learning. For example, FairVGNN <ref type="bibr" target="#b9">[10]</ref> generates fair representations through adversarial debiasing. FairSIN <ref type="bibr" target="#b48">[49]</ref> incorporates fairness-promoting features before message passing to enrich final node representations. FairINV <ref type="bibr" target="#b49">[50]</ref> reduces spurious correlations between sensitive attributes and labels to produce fairer GNN predictions. FG-SMOTE <ref type="bibr" target="#b50">[51]</ref> introduces a desensitized node representation mechanism and a fairness-aware link predictor, systematically addressing fairness concerns in minority oversampling caused by subgroup under-representation and structural bias. Themis <ref type="bibr" target="#b51">[52]</ref> leverages a Bayesian variational autoencoder to infer proxy sensitive attributes and incorporates disentangled latent variables alongside a fairness normalization module, enabling fair graph representation learning without requiring on explicit demographic information.</p><p>Different from the methods reviewed above, our proposed FairDTD effectively reduces bias by training teacher models on partial data. By integrating this with knowledge distillation, FairDTD alleviates the fairness-utility tradeoff commonly encountered in GNNs. This strategy of integrating "partial data training" and "knowledge distillation" provides a new perspective for enhancing the fairness of GNNs, especially demonstrating innovation in directions not deeply explored by previous work. Through the dual-teacher design and dynamic distillation mechanism, FairDTD achieves an effective balance between fairness enhancement and model performance maintenance, further expanding the research paradigm for fair GNN learning.</p><p>Moreover, fair graph representation learning has significant potential in various real-world applications, particularly those involving graph-structured data and requiring fairness-aware automated decision-making. For example, in the financial services sector, including applications, such as credit scoring and fraud detection, fair graph representations help mitigate potential discrimination against sensitive demographic groups (e.g., by gender or race), thereby enhancing fairness, transparency, and interpretability. Similarly, in tasks highly dependent on graph-structured modeling, such as public policy-making, intelligent recruitment systems, and personalized recommendation, incorporating fairness mechanisms helps improve the credibility, reliability, and social sustainability of artificial intelligence systems <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>In this section, we define the pertinent notations and provide an overview of knowledge distillation. Subsequently, we perform a causal analysis of our research problem using a causal graph and formally define the problem addressed in our work, which serves as the foundation for our FairDTD framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and problem formulation</head><p>We consider an undirected graph î‰ = (î‰œ, î‰‹, ğ—), where î‰œ represents the set of nodes and î‰‹ represents the set of edges. |î‰œ| = ğ‘ represents the number of nodes. ğ— âˆˆ â„ ğ‘Ã—ğ¹ represents the node feature matrix, where ğ¹ is the dimension of the node feature. ğ€ âˆˆ {0, 1} ğ‘Ã—ğ‘ represents the adjacency matrix, where ğ€ ğ‘–ğ‘— = 1 if and only if (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ î‰‹. Additionally, ğ‘† âˆˆ {0, 1} ğ‘ contains the binary-sensitive attribute of each node.</p><p>The majority of existing GNNs operate based on the message-passing method, which aggregates messages from neighboring nodes to update node representations iteratively. Formally, the ğ‘™-th layer of a GNN is defined as:</p><formula xml:id="formula_0">ğ‘š (ğ‘™) ğ‘£ = AGG (ğ‘™) ({ â„ (ğ‘™-1) ğ‘¢ âˆ¶ ğ‘¢ âˆˆ îˆº (ğ‘£) }) ,<label>(1)</label></formula><formula xml:id="formula_1">â„ (ğ‘™) ğ‘£ = UPD (ğ‘™) ( â„ (ğ‘™-1) ğ‘£ , ğ‘š (ğ‘™) ğ‘£ ) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ğ‘™ is the layer index, AGG (ğ‘™) (â‹…) denotes the aggregation function, and UPD (ğ‘™) (â‹…) denotes the update function at the ğ‘™-th layer. Here, îˆº (ğ‘£) denotes the collection of nodes adjacent to node ğ‘£. The node representation at the final layer is</p><formula xml:id="formula_3">ğ‘§ ğ‘£ = â„ (ğ¿)</formula><p>ğ‘£ , where ğ¿ is the network's total number of layers. In this paper, we aim to address the challenges associated with training on partial data. To this end, we formally define the problem as follows: Problem formulation. Given a graph î‰ = (î‰œ, î‰‹, ğ—) and a GNN-based teacher-student framework, which consists of a trained structure teacher ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ , a trained feature teacher ğ‘“ ğ‘“ ğ‘’ğ‘ , and a student model ğ‘“ ğ‘ ğ‘¡ğ‘¢ to be trained, our objective is to maintain the overall utility of the model while developing a fairer student model under the guidance of the two fairness teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge distillation</head><p>Knowledge distillation seeks to transfer knowledge from a cumbersome, complicated teacher model to a more lightweight student model, enabling the student to achieve performance comparable to the teacher. In this work, we emphasize the transfer of fair and rich knowledge during the distillation process, rather than solely focusing on model compression.</p><p>To facilitate knowledge transfer, the knowledge distillation approach was designed by Hinton et al. <ref type="bibr" target="#b22">[23]</ref> to align the softened outputs generated by both the teacher and student models. Two supervisory signals are utilized in the training of the student model: (1) Hard Labels: The true labels from the training dataset. (2) Soft Labels: The label predictions of the teacher model. Formally, let ğ™ ğ‘¡ğ‘’ğ‘ represent the output logits from the teacher model, and ğ™ ğ‘ ğ‘¡ğ‘¢ denote the output logits from the student model, the loss to be minimized is defined as:</p><formula xml:id="formula_4">îˆ¸ = âˆ‘ ğ‘£âˆˆî‰‚ îˆ¸ ğ¶ğ¸ ( ğ‘¦ ğ‘£ , (sof tmax(ğ‘§ ğ‘ ğ‘¡ğ‘¢ ğ‘£ )) ) + ğ›¼îˆ¸ ğ¾ğ· ,<label>(3)</label></formula><p>where îˆ¸ ğ¶ğ¸ denotes the cross-entropy loss and the hyperparameter ğ›¼ regulates the relative contribution of the two loss functions. The knowledge distillation loss, îˆ¸ ğ¾ğ· , is given by:</p><formula xml:id="formula_5">îˆ¸ ğ¾ğ· = îˆ´ ğ¾ğ¿ ( sof tmax( ğ™ ğ‘ ğ‘¡ğ‘¢ ğœ )â€– sof tmax( ğ™ ğ‘¡ğ‘’ğ‘ ğœ ) ) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where îˆ´ ğ¾ğ¿ is the KL-divergence, and ğœ is a temperature hyperparameter that regulates the smoothness of the two logit distributions by appropriately scaling them. A higher temperature makes the distribution flatter, generating softer predictions; a lower temperature enlarges the difference between the two distributions, generating harder predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of partial data training</head><p>In this subsection, we present the motivation behind partial data training from both theoretical and experimental perspectives. Recent studies have demonstrated that incorporating causal learning techniques into GNNs can more effectively address trustworthiness concerns by capturing the underlying causal relationships within the data <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. To this end, we employ a causal graph <ref type="bibr">[57,</ref><ref type="bibr" target="#b56">58]</ref> to represent the causal relationships and analyze the sources of bias, illustrating how the sensitive attribute influences predictions through both node features and graph structure. Guided by this analysis, we further investigate the task of partial data training experimentally. While fairness can be improved, it often comes at the expense of reduced model utility. We first illustrate the causal relationships among seven distinct variables: observed sensitive attribute ğ‘†, unobserved environmental feature E, unobserved content feature C, observed graph structure A, observed node feature X, node embedding Z, and true label ğ‘Œ . Specifically, ğ‘† â†’ E indicates that the latent environmental feature E is decided by the sensitive attribute ğ‘†. For instance, if ğ‘† represents gender, different genders may have distinct physical traits represented by E. The relationship C E with a cross indicates that any spurious correlation between C and E should be disentangled. Ideally, C should represent content features that are independent of E, ensuring that C captures only information unrelated to the environmental features represented by E. The relationships C â†’ X â† E and C â†’ A â† E demonstrate that the environmental feature E and content feature C jointly shape the observed contextual subgraph's node feature X and graph structure A. Furthermore, X â†’ Z â† A and Z â†’ ğ‘Œ denote that GNNs generate embeddings Z and predictions ğ‘Œ grounded in the observed contextual subgraph.</p><formula xml:id="formula_7">S E A X Z Y C S E C Y G (a)Prior Work (b)Our Work</formula><p>Compared with conventional methods, our approach introduces key advancements at the causal modeling level <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b58">60]</ref>. Existing fair graph representation learning techniques often fail to distinguish the different paths through which sensitive attributes propagate via node features and graph structures. This results in a mixture of bias sources and makes it difficult to achieve targeted intervention. As shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, traditional methods typically depict the sensitive attribute ğ‘† influencing the subgraph G through a latent variable E, which in turn affects the prediction ğ‘Œ , however, these causal paths are neither explicitly modeled or systematically blocked. In contrast, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), our method leverages causal graphs to disentangle the influence of sensitive attributes into two distinct paths: the feature path (ğ‘† â†’ E â†’ X â†’ Z â†’ ğ‘Œ ) and the structure path (ğ‘† â†’ E â†’ A â†’ Z â†’ ğ‘Œ ). We also introduce intermediate variables, such as content variable C and node embedding Z, to more accurately model representation learning and prediction mechanisms. Overall, our FairDTD approach provides a novel and principled perspective for advancing fairness in GNNs. Theoretical Analysis. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, our constructed causal graph reveals two critical causal paths through which the sensitive attribute ğ‘† can influence the prediction ğ‘Œ : ğ‘† â†’ E â†’ X â†’ Z â†’ ğ‘Œ and ğ‘† â†’ E â†’ A â†’ Z â†’ ğ‘Œ . These two paths constitute causal channels through which sensitive information may introduce bias into model predictions.</p><p>To achieve this, it is essential to identify and block the critical transmission points along these paths that carry sensitive information. To formalize this, we introduce the following definition:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Path-Specific Blocking [61]). Given a causal path p, if we intervene on intermediate variables along this path (such as node features X or graph structure A) in a way that prevents the sensitive attribute from influencing the prediction outcome ğ‘Œ through ğ‘, then the path is said to be specifically blocked.</head><p>Building on this definition, we propose a path-decoupled strategy, which trains two models that each rely exclusively on a single type of pathway variable (either node features or graph structure) to achieve path-specific blocking and thereby enhance model fairness. The results in Fig. <ref type="figure" target="#fig_1">2</ref> show that compared with complete data training, partial data training achieves improved fairness performance but at the expense of reduced model utility. A possible explanation for these results is that training on partial data reduces the source of bias, thereby improving fairness. However, compared to complete data training, the model lacks some key classification information, leading to diminished utility.</p><p>In summary, we identify the challenges confronted by partial data training: how does the model achieve the trade-off between fairness and model utility when training with partial data?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>To tackle the challenges related to training on partial data, we present a framework called FairDTD for learning fair GNNs through dual-teacher distillation of features and structure. We present a thorough description of FairDTD, with the framework presented in Fig. <ref type="figure" target="#fig_3">3</ref>. In dual-teacher distillation, we employ two fairness teachers of feature and structure to guide student learning. In graph-level distillation, we introduce intermediate layer representation alignment as additional supervision to guide the training of students. In learning node-specific temperatures, we adaptively soften the teacher's predictions based on the confidence levels of their outputs, facilitating effective knowledge transfer.</p><p>In the following sections, we present a thorough explanation of each component that constitutes the FairDTD framework. Finally, we present the training algorithm to enhance understanding of the FairDTD process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dual-teacher distillation based on feature and structure</head><p>In this subsection, we describe the key components of dual-teacher distillation along with its associated optimization objective. Our approach is motivated by empirical analyses of partial data training. The primary challenge to address is avoiding the loss of utility caused by the lack of information inherent in partial data training.</p><p>To tackle this issue, we employ two fairness-oriented teacher models, ğ‘“ ğ‘“ ğ‘’ğ‘ and ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ , to provide targeted guidance to the student model. The fairness expert ğ‘“ ğ‘“ ğ‘’ğ‘ takes only the node feature X as the input and obtains the output logits ğ™ ğ‘“ ğ‘’ğ‘ using a two-layer MLP, defined as follows:</p><formula xml:id="formula_8">ğ™ ğ‘“ ğ‘’ğ‘ = ğ‘“ ğ‘“ ğ‘’ğ‘ (ğ—),<label>(5)</label></formula><p>where ğ‘“ ğ‘“ ğ‘’ğ‘ is a two-layer MLP with the Relu activation function, and its training objective is as follows:</p><formula xml:id="formula_9">min ğœƒ ğ‘“ ğ‘’ğ‘ îˆ¸ ğ‘“ ğ‘’ğ‘ = -ğ”¼ ğ‘£ ğ‘– âˆ¼î‰‚ ( ğ‘¦ ğ‘– log ( ğœ(ğ‘§ ğ‘“ ğ‘’ğ‘ ğ‘– ) ) + (1 -ğ‘¦ ğ‘– ) log ( 1 -ğœ(ğ‘§ ğ‘“ ğ‘’ğ‘ ğ‘– ) ) ) . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>The fair expert ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ takes only the graph structure A (all-one node attribute matrix) as the input and then obtains the output logits ğ™ ğ‘ ğ‘¡ğ‘Ÿ through a two-layer GCN, defined as follows:</p><formula xml:id="formula_11">ğ™ ğ‘ ğ‘¡ğ‘Ÿ = ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ (ğ€),<label>(7)</label></formula><p>where ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ is a two-layer GCN, and its training objective is defined as follows:</p><p>min</p><formula xml:id="formula_12">ğœƒ ğ‘ ğ‘¡ğ‘Ÿ îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ = -ğ”¼ ğ‘£ ğ‘– âˆ¼î‰‚ ( ğ‘¦ ğ‘– log ( ğœ(ğ‘§ ğ‘ ğ‘¡ğ‘Ÿ ğ‘– ) ) + (1 -ğ‘¦ ğ‘– ) log ( 1 -ğœ(ğ‘§ ğ‘ ğ‘¡ğ‘Ÿ ğ‘– ) ) ) . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>This framework ensures that the feature-based teacher model ğ‘“ ğ‘“ ğ‘’ğ‘ focuses on fairness-driven learning from nodelevel information, effectively complementing the structural guidance provided by ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ .</p><p>As discussed in Section 3.3, ğ‘“ ğ‘“ ğ‘’ğ‘ and ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ help mitigate the source of bias, ensuring that ğ™ ğ‘“ ğ‘’ğ‘ and ğ™ ğ‘ ğ‘¡ğ‘Ÿ contain fairer node prediction information. However, since ğ‘“ ğ‘“ ğ‘’ğ‘ and ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ are trained on partial data, the accuracy of these predictions may be compromised.</p><p>To address this, we employ a dual-teacher model, aiming to provide comprehensive supervisory information. We design a dual distillation loss to maximize the guidance provided to the student model, balancing fairness and utility. The student model is motivated to imitate the predictions of the fairness-oriented teacher models, thereby facilitating the transfer of knowledge. The student model ğ‘“ ğ‘ ğ‘¡ğ‘¢ takes complete data as the input and obtains the output logits ğ™ ğ‘ ğ‘¡ğ‘¢ , defined as follows:</p><formula xml:id="formula_14">ğ™ ğ‘ ğ‘¡ğ‘¢ = ğ‘“ ğ‘ ğ‘¡ğ‘¢ (ğ—, ğ€),<label>(9)</label></formula><p>where ğ‘“ ğ‘ ğ‘¡ğ‘¢ is a two-layer GNN, and its training objective is defined as follows:</p><p>min</p><formula xml:id="formula_15">ğœƒ ğ‘ ğ‘¡ğ‘¢ îˆ¸ â„ğ‘ğ‘Ÿğ‘‘ = -ğ”¼ ğ‘£ ğ‘– âˆ¼î‰‚ ( ğ‘¦ ğ‘– log ( ğœ(ğ‘§ ğ‘ ğ‘¡ğ‘¢ ğ‘– ) ) + (1 -ğ‘¦ ğ‘– ) log ( 1 -ğœ(ğ‘§ ğ‘ ğ‘¡ğ‘¢ ğ‘– ) ) ) . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Finally, leveraging ğ™ ğ‘“ ğ‘’ğ‘ , ğ™ ğ‘ ğ‘¡ğ‘Ÿ and ğ™ ğ‘ ğ‘¡ğ‘¢ , the dual-teacher distillation loss is defined as follows:</p><formula xml:id="formula_17">îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘ ğ‘œğ‘“ ğ‘¡ = îˆ´ ğ¾ğ¿ ( sof tmax( ğ™ ğ‘ ğ‘¡ğ‘¢ ğœ )â€– sof tmax( ğ™ ğ‘“ ğ‘’ğ‘ ğœ ) ) , (<label>11</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘ ğ‘œğ‘“ ğ‘¡ = îˆ´ ğ¾ğ¿ ( sof tmax( ğ™ ğ‘ ğ‘¡ğ‘¢ ğœ )â€– sof tmax( ğ™ ğ‘ ğ‘¡ğ‘Ÿ ğœ ) ) ,<label>(12)</label></formula><p>where ğœ denotes a temperature parameter that regulates the smoothness of the output distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graph-level distillation</head><p>In this subsection, we introduce graph-level distillation to further enhance the teacher-student model. While the dual-teacher distillation generates soft labels through the output of the last layer to provide additional supervisory signals for the student, it does not account for intermediate supervision within the teacher model. This limitation may hinder the model's ability to learn fair representations effectively, as deep neural networks excel at capturing feature representations at various levels in multi-layer architectures <ref type="bibr" target="#b60">[62]</ref>.</p><p>To address this, we adopt intermediate representation distillation as an additional supervisory signal to guide the student model's training. This method also indirectly supplements the missing information caused by partial data training. In FairDTD, we use the intermediate graph-level representations ğ‘ ğ‘“ ğ‘’ğ‘ and ğ‘ ğ‘ ğ‘¡ğ‘Ÿ from the dual-teacher model as supervisory signals. These representations encapsulate more latent information, enabling the student model to learn effectively. We redefine Eqs. ( <ref type="formula" target="#formula_8">5</ref>), <ref type="bibr" target="#b6">(7)</ref>, and (9) to include intermediate representations as follows:</p><formula xml:id="formula_20">ğ™ ğ‘“ ğ‘’ğ‘ , ğ‘ ğ‘“ ğ‘’ğ‘ = ğ‘“ ğ‘“ ğ‘’ğ‘ (ğ—), ğ™ ğ‘ ğ‘¡ğ‘Ÿ , ğ‘ ğ‘ ğ‘¡ğ‘Ÿ = ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ (ğ€), ğ™ ğ‘ ğ‘¡ğ‘¢ , ğ‘ ğ‘ ğ‘¡ğ‘¢ = ğ‘“ ğ‘ ğ‘¡ğ‘¢ (ğ—, ğ€).<label>(13)</label></formula><p>In particular, our goal is to maximize the consistency of the intermediate representations of the teacher and student models. The graph-level distillation loss is formulated as follows:</p><formula xml:id="formula_21">îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘šğ‘–ğ‘‘ = â€– ğ‘ ğ‘ ğ‘¡ğ‘¢ â€–ğ‘ ğ‘ ğ‘¡ğ‘¢ â€– 2 - ğ‘ ğ‘“ ğ‘’ğ‘ â€–ğ‘ ğ‘“ ğ‘’ğ‘ â€– 2 â€– 2 2 , (<label>14</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘šğ‘–ğ‘‘ = â€– ğ‘ ğ‘ ğ‘¡ğ‘¢ â€–ğ‘ ğ‘ ğ‘¡ğ‘¢ â€– 2 - ğ‘ ğ‘ ğ‘¡ğ‘Ÿ â€–ğ‘ ğ‘ ğ‘¡ğ‘Ÿ â€– 2 â€– 2 2 , (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where â€– â‹… â€– 2 denotes the ğ¿ 2 norm, which measures the difference between the normalized intermediate representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning node-specific temperatures</head><p>In this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graph-level distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance.</p><p>Learning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows:</p><formula xml:id="formula_25">ğ‘ (ğ™, ğœ) = exp (ğ™âˆ•ğœ) âˆ‘ exp (ğ™âˆ•ğœ) , (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>where the temperature parameter ğœ is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance <ref type="bibr" target="#b61">[63]</ref>.</p><p>To confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature Ï„ğ‘– to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as:</p><formula xml:id="formula_27">ğ‘ ( ğ‘§ ğ‘– , Ï„ğ‘– ) = exp ( ğ‘§ ğ‘– âˆ• Ï„ğ‘– ) âˆ‘ exp ( ğ‘§ ğ‘– âˆ• Ï„ğ‘– ) ,<label>(17)</label></formula><p>where the temperature parameter Ï„ğ‘– is the specific temperature of the ğ‘–-th node. Obviously, Ï„ğ‘– determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node <ref type="bibr" target="#b62">[64]</ref>. Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:</p><formula xml:id="formula_28">Entropy ( zğ‘– ) = - ğ¶ âˆ‘ ğ‘=1 ğ‘§ ğ‘ ğ‘– log ( ğ‘§ ğ‘ ğ‘– ) , (<label>18</label></formula><formula xml:id="formula_29">) Ï„ğ‘– = sof tmax ( MLP ( Concat ( zğ‘– , Entropy ( zğ‘– )))) , (<label>19</label></formula><formula xml:id="formula_30">)</formula><p>where ğ‘§ ğ‘ ğ‘– is the probability that the node ğ‘– belongs to class ğ‘ and Concat represents the concatenation function. We use Ï„ğ‘– = Ï„ğ‘– Ã— (ğœ ğ‘šğ‘ğ‘¥ -ğœ ğ‘šğ‘–ğ‘› ) + ğœ ğ‘šğ‘–ğ‘› to constrain the temperature within a predetermined range [ğœ ğ‘šğ‘–ğ‘› , ğœ ğ‘šğ‘ğ‘¥ ].</p><p>In summary, our method assigns lower temperatures to predictions with higher entropy (lower confidence) and higher temperatures to predictions with lower entropy (higher confidence). This adjustment ensures sharper predictions are softened more, while overly smooth predictions are sharpened, facilitating more effective knowledge transfer from teacher models to the student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fair GNN student training</head><p>In this subsection, we introduce the final training objective for the fair GNN student. FairDTD trains a fair student model ğ‘“ ğ‘ ğ‘¡ğ‘¢ using the trained ğ‘“ ğ‘“ ğ‘’ğ‘ and ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ as dual teacher models. The primary objective is for the student model to imitate the outputs of the fair dual-teacher models. To achieve this, we combine the dual distillation loss and the graph-level loss into the final distillation loss between the student and teacher models, defined as:</p><formula xml:id="formula_31">îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘‘ğ‘¢ğ‘ğ‘™ = îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘ ğ‘œğ‘“ ğ‘¡ + îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘šğ‘–ğ‘‘ ,<label>(20)</label></formula><formula xml:id="formula_32">îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘‘ğ‘¢ğ‘ğ‘™ = îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘ ğ‘œğ‘“ ğ‘¡ + îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘šğ‘–ğ‘‘ . (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>By incorporating the student model's own training loss, the final complete training loss is expressed as:</p><formula xml:id="formula_34">îˆ¸ ğ‘ ğ‘¡ğ‘¢ ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ = îˆ¸ â„ğ‘ğ‘Ÿğ‘‘ + ğ›¼îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘‘ğ‘¢ğ‘ğ‘™ + (1 -ğ›¼)îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘‘ğ‘¢ğ‘ğ‘™ , (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>where ğ›¼ is a hyperparameter that balances the feature teacher and the structural teacher.</p><p>To facilitate a clearer understanding of FairDTD, we provide its complete training process in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Theorem and proof for FairDTD</head><p>In this subsection, we present Theorem 1 to formally derive the feasibility of FairDTD's path-specific blocking approach under the dual fairness teacher distillation framework, and prove that it achieves fairness at the causal level, as detailed below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. If multiple undesirable causal paths exist between the sensitive attribute ğ‘† and the prediction output ğ‘Œ , and each path relies on a different intermediate variable (e.g., node features X or graph structure A), then training models that each use only a single intermediate variable allows for path-specific blocking. This strategy weakens the overall impact of ğ‘† on the learned representation Z, thereby improving the fairness.</head><p>Proof. Consider the following two causal paths: ğ‘ 1 : ğ‘† â†’ E â†’ X â†’ Z â†’ ğ‘Œ and ğ‘ 2 : ğ‘† â†’ E â†’ A â†’ Z â†’ ğ‘Œ . We construct two models that each rely exclusively on a single intermediate variable: (1) A feature-based model, which learns the representation ğ™ ğ‘“ ğ‘’ğ‘ solely from node features X. Since it does not depend on the graph structure A, path ğ‘ 2 is explicitly blocked. (2) A structure-based model, which learns the representation ğ™ ğ‘ ğ‘¡ğ‘Ÿ solely from the graph structure A. As it excludes node features X, path ğ‘ 1 is effectively blocked. If the final representation Z is learned by combining both models, then by the sub-additivity property of mutual information, we have:</p><formula xml:id="formula_36">ğ¼(ğ™; ğ‘†) â‰¤ ğ¼(ğ™ ğ‘“ ğ‘’ğ‘ ; ğ‘†) + ğ¼(ğ™ ğ‘ ğ‘¡ğ‘Ÿ ; ğ‘†), (<label>23</label></formula><formula xml:id="formula_37">)</formula><p>where ğ¼(ğ™; ğ‘†) denotes the mutual information between the learned representation ğ™ and the sensitive attribute ğ‘†. A smaller value of ğ¼(ğ™; ğ‘†) indicates that less sensitive information is encoded in Z. This demonstrates that blocking the transmission of the sensitive attribute ğ‘† along each specific causal path can effectively reduce the overall dependency between ğ‘† and the final representation ğ™, thereby enhancing fairness at the causal level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present comprehensive experiments to assess the efficacy of the suggested approach. Specifically, we perform experiments on multiple fairness datasets to investigate and address the following questions:</p><p>ğ‘ğğŸ: Compared with the baseline methods, can FairDTD maintain the utility performance while improving fairness? ğ‘ğğŸ: What is the impact of each component within the FairDTD framework on the overall model performance? ğ‘ğğŸ‘: Does FairDTD generate node representations that effectively distinguish between different groups compared to vanilla? ğ‘ğğŸ’: How does the time cost of FairDTD compare to the baseline methods? ğ‘ğğŸ“: How do the relevant hyperparameters affect FairDTD?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Datasets</head><p>We use three commonly used real-world datasets for our experiments: Pokec-z, Pokec-n, and Credit. The statistical details of each dataset are summarized in Table <ref type="table" target="#tab_0">1</ref>. A brief overview of the datasets is as follows:</p><p>â€¢ Pokec-z/n <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b63">65]</ref> is sourced from Pokec, a widely used social networking platform in Slovakia. These datasets reflect social network data collected from two distinct provinces. Where nodes represent individual users, while edges denote the follower relationships between them. The "region" is treated as the sensitive attribute, and the users' working fields are binarized to serve as the label for prediction. â€¢ Credit <ref type="bibr" target="#b5">[6]</ref> is a dataset containing information related to credit card users. Where nodes stand in for credit card users, and edges are based on how similar customers' payment habits are. The aim is to estimate if a user would be able to make timely repayments of their credit card debt in the upcoming month, with "age" being regarded as the sensitive attribute. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Baselines</head><p>We conduct a comparative analysis of FairDTD against four state-of-the-art fairness methodologies: NIFTY, FairVGNN, CAF, FairGKD and FairGP. A brief overview of these methodologies is as follows:</p><p>â€¢ NIFTY <ref type="bibr" target="#b43">[44]</ref> enforces counterfactual fairness constraints by maximizing the similarity between the counterfactual graph generated by flipping the sensitive attribute and the original graph, but it overlooks latent biases within the graph structure and struggles to produce realistic and reliable counterfactuals.</p><p>â€¢ FairVGNN <ref type="bibr" target="#b9">[10]</ref> mitigates the leakage of sensitive attributes by masking the sensitive-related channels and clamping the encoder weights but lacks explicit modeling of the sensitive attribute propagation mechanism.</p><p>â€¢ CAF <ref type="bibr" target="#b64">[66]</ref> aims to identify counterfactuals from the training data and uses them as supervisory signals for learning fair node representations through disentangled representation learning. However, this approach depends on the quality of the counterfactual samples and has limited scalability to large-scale graphs.</p><p>â€¢ FairGKD <ref type="bibr" target="#b19">[20]</ref> constructs a synthetic teacher with contrastive learning and a fixed temperature parameter for distillation. While informative, this setup may constrain effective knowledge transfer. â€¢ FairGP <ref type="bibr" target="#b65">[67]</ref> proposes a partition-based fairness-aware graph transformer model that mitigates the influence of high-order sensitive nodes and optimizes the global attention mechanism, significantly enhancing fairness in graph representation learning while improving computational efficiency. While computationally efficient, it focuses on structural interventions and lacks a causal framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Evaluation metrics</head><p>We use the accuracy of the model to assess its utility. For fairness performance, we adopt two widely recognized fairness measures: Statistical Parity (SP) <ref type="bibr" target="#b66">[68]</ref> and Equal Opportunity (EO) <ref type="bibr" target="#b67">[69]</ref>. To achieve statistical parity, we use Î” ğ‘ ğ‘ as the evaluation metric. Specifically, it is defined as</p><formula xml:id="formula_38">Î” ğ‘ ğ‘ = |ğ‘ƒ ( Å· = 1 | ğ‘  = 0) -ğ‘ƒ ( Å· = 1 | ğ‘  = 1)|,</formula><p>which assesses the ratio of instances from different groups of sensitive attributes that are categorized as positive or negative. Similarly, for equal opportunity, we use Î” ğ‘’ğ‘œ as the evaluation metric. It is defined as</p><formula xml:id="formula_39">Î” ğ‘’ğ‘œ = |ğ‘ƒ ( Å· = 1 | ğ‘¦ = 1, ğ‘  = 0) -ğ‘ƒ ( Å· = 1 | ğ‘¦ = 1, ğ‘  = 1)|,</formula><p>which evaluates if the model generates consistent predictions for people with similarly non-sensitive attributes. Smaller values of Î” ğ‘ ğ‘ and Î” ğ‘’ğ‘œ indicate a fairer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Implementation details</head><p>To assess the generalizability of FairDTD across various architectures, we employ GCN and GIN as the encoder backbones. For consistency across all datasets, the hidden dimension for each GNN backbone is uniformly set to 64. For baseline methods, we use results obtained from their respective open-source code repositories, adhering to the hyperparameter settings provided by the authors. The experiments are repeated five times, and the average outcomes are documented.</p><p>In FairDTD, we utilize a two-layer MLP for the feature teacher, while both the structure teacher and the student model are constructed using a two-layer GCN or GIN. For the GCN and GIN backbones, we use the Adam optimizer during training, with learning rates (lr) of {0.01, 0.0001}. The number of training epochs is set to 700. In addition, we adjust the balance parameter ğ›¼ mentioned in the method in {0.1, 0.2, ..., 0.9}. A single NVIDIA A800 GPU with 80GB memory is used for all experiments. The PyTorch framework is used to implement the models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance comparison</head><p>To answer RQ1, we use GCN or GIN as the backbone models and compare FairDTD against four baseline approaches for the node classification task. Table <ref type="table" target="#tab_4">2</ref> displays the utility and fairness of the baseline approach as well as FairDTD. From this table, we make the following key observations:</p><p>1. Compared with the baseline methods, FairDTD achieves the best fairness on the three datasets, which proves the effectiveness of FairDTD in learning fair GNN methods by reducing the sources of bias.</p><p>2. FairDTD maintains a comparable or even better classification performance, which proves that FairDTD can solve the problem of the decline in model utility due to the training with partial data and also indicates that FairDTD effectively achieves an improved balance between fairness and model utility. 4. In most cases, FairDTD achieves optimal performance on both GCN and GIN backbones, demonstrating its strong generalization capability across different GNN architectures, which allows for flexible application in various scenarios. Additionally, we can observe that the performance of GIN in classification has declined compared to GCN. This might be due to the following reasons: Compared to GCN, which aggregates feature information through simple convolution operations, GIN introduces a more complex domain aggregation mechanism and MLP for representation learning for the graph isomorphism problem. This may cause excessive aggregation and instead blur the discrimination ability of nodes.</p><p>5. FairDTD achieves the best performance on the Credit dataset for both GCN and GIN. We speculate that this discrepancy may be attributed to the increased complexity of the training data in the Credit dataset, relative to the Pokec-z and Pokec-n datasets. As a more complicated model, GIN can fully utilize its stronger representational ability when processing complex data and thus performs well in this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study</head><p>To answer RQ2, we perform ablation experiments to systematically assess the impact of each component in FairDTD on its overall performance. Specifically, we study the influence of four components, namely the feature teacher, the structure teacher, the graph-level distillation, and the learning of node-specific temperatures, on learning fair GNNs. We adopt w/o FT to represent the exclusion of the feature teacher. We adopt w/o ST to represent the exclusion of the structure teacher. We adopt w/o GD to represent the exclusion of the graph-level distillation. We adopt w/o NST to represent the exclusion of the learning of node-specific temperatures and the adoption of a fixed temperature. Table <ref type="table" target="#tab_5">3</ref> presents the results for the different model variants.</p><p>From Table <ref type="table" target="#tab_5">3</ref>, the four variants exhibit inferior performance compared to FairDTD in balancing utility and fairness, proving each component's effectiveness and necessity. Compared with the fixed temperature, learning node-specific temperatures can help FairDTD learn more effective knowledge from the dual-teacher model. Moreover, graph-level distillation can help the dual-teacher model learn richer information in the multi-layer neural network to guide the fair GNN student. Finally, compared with w/o FT and w/o ST, FairDTD achieves better performance, which proves that dual-teacher distillation can promote knowledge transfer from the teacher to the student more effectively. It is worth noting that when comparing w/o FT with w/o ST, we observe that although both teachers contain fairness information, the feature teacher takes into account the model utility, while the structure teacher pays more attention to the fairness performance. Therefore, we need to better balance the influence of the feature teacher and the structure teacher's weights on the model to provide targeted guidance to the fair GNN student. Finally, by comparing the best results across the three datasets, we observe that the model's performance declines regardless of which component is removed. This further demonstrates that the contributions of each component in FairDTD generalize across datasets, rather than being specific to any single dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Node representations visualization</head><p>To answer RQ3, we visualize the node representations learned by FairDTD and the vanilla GNN model (i.e., GCN) on the Pokec-z dataset, as shown in Fig. <ref type="figure">4</ref>. To obtain better visibility, we select samples from the test set. Specifically, the learned representations are projected into a two-dimensional space for visualization using t-SNE. We plot the node representations for both models concerning sensitive attributes and true labels.</p><p>First, comparing Fig. <ref type="figure">4</ref>(a) and Fig. <ref type="figure">4</ref>(b), we observe that FairDTD, guided by dual-teacher distillation, exhibits stronger discriminatory ability, resulting in clearer classification boundaries for nodes with different sensitive attributes. However, comparing Fig. <ref type="figure">4</ref>(b) and Fig. <ref type="figure">4</ref>(d), it is evident that in FairDTD, the classification boundary for sensitive attributes is orthogonal to that of the true labels, indicating that the model's predictions for true labels do not rely on sensitive attributes <ref type="bibr" target="#b68">[70]</ref>. This proves the excellent fairness of FairDTD on Pokec-z. Then, comparing Fig. <ref type="figure">4(c</ref>) and Fig. <ref type="figure">4</ref>(d), we can observe that the discrimination of nodes with different true labels on FairDTD is further improved, demonstrating that FairDTD improves the model's utility.</p><p>Additionally, it is evident that nodes with distinct true labels exhibit significant overlap in both Fig. <ref type="figure">4</ref>(c) and Fig. <ref type="figure">4(d)</ref>. This is attributed to the fact that all models currently exhibit suboptimal predictive performance on the Pokec-z dataset, failing to differentiate these confounding node representations effectively.</p><p>In summary, these results demonstrate FairDTD's excellence in balancing fairness and model utility, achieving clear classification boundaries while ensuring predictions are not influenced by sensitive attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Training time comparison</head><p>To answer RQ4, we analyze the training time overhead of FairDTD with that of the baseline methods on the Pokecz and Credit datasets, as illustrated in Fig. <ref type="figure">5</ref>. Overall, we can observe that FairDTD shows a lower computational cost among all methods, proving our method's efficiency. FairVGNN shows the highest computational cost among all methods because it encompasses a considerable number of parameters and requires a reverse training process. In addition, CAF performs well on Pokec-z but poorly on Credit. This might be related to its counterfactual augmentation module, which aims to find counterfactuals from the training data, resulting in a higher computational cost on larger datasets. Furthermore, compared to FairGKD, which is also based on knowledge distillation, FairDTD exhibits lower time overhead, further validating the efficiency and effectiveness of our proposed FairDTD method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Hyper-parameter sensitivity analysis</head><p>To answer RQ5, we performed a sensitivity analysis of the hyperparameters in FairDTD. Analysis of distillation temperature ğœ. First, we further verify the influence of the temperature hyperparameter ğœ on the model. From Table <ref type="table" target="#tab_5">3</ref>, we have already confirmed the effectiveness of learning node-specific temperatures. Here, we further compare learning node-specific temperatures with fixed temperatures ranging from 1 to 5. Fig. <ref type="figure" target="#fig_6">6</ref> shows that learning node-specific temperatures can achieve the best balance between model utility and fairness, indicating that learning node-specific temperatures can effectively distinguish between confounding information and true information to facilitate knowledge transfer. Analysis of balance parameter ğ›¼. Next, we analyzed the sensitivity of the balance parameter ğ›¼, which determines the relative weights of the feature and structure teachers. To assess its influence on FairDTD, we varied ğ›¼ from 0.1 to 0.9 in increments of 0.1 and recorded the experimental results. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, increasing ğ›¼ improves model accuracy but reduces fairness. This observation aligns with Fig. <ref type="figure" target="#fig_1">2</ref>, where the feature teacher exhibits higher accuracy while the structure teacher prioritizes fairness. Consequently, the selection of an appropriate ğ›¼ is essential to strike an optimal balance between utility and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>From an applied perspective, FairDTD demonstrates significant practical value across various real-world scenarios. In the social network scenario, evaluations on the Pokec-z and Pokec-n datasets indicate that FairDTD achieves group fairness with respect to the region, significantly reducing prediction disparities among users from different regions. In the credit scoring scenario, experiments on the Credit dataset show that FairDTD effectively mitigates decisionmaking bias caused by age, thereby enhancing model fairness and transparency. Furthermore, in socially sensitive graph-structured application scenarios such as healthcare and public governance, it is essential to adopt FairDTD for fair representation learning to prevent traditional GNNs from inadvertently amplifying group biases.</p><p>Although FairDTD performs exceptionally well in terms of fairness, it does have some limitations. Specifically, when dealing with changes in sensitive attributes, FairDTD may require retraining to achieve optimal performance. In future work, we plan to explore integrating FairDTD with invariant learning as a potential solution to this issue. Additionally, FairDTD currently focuses on a single sensitive attribute; future research will aim to extend the framework to scenarios involving multiple sensitive attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a novel perspective for learning fair GNNs. By constructing causal structure models, we identify that bias originates from both node features and graph structures. Through our analysis, we find that a simple partial data training strategy, where the model is trained exclusively on node features or graph structures, can achieve fairness that is on par with, or potentially exceeds, the performance of current state-of-the-art fair GNN methods, albeit at the expense of model utility. To address this trade-off, we propose FairDTD, a framework that combines knowledge distillation with partial data training to learn fair GNNs. Specifically, FairDTD employs two fairness-oriented teacher models, referred to as the feature expert and the structure expert, to guide the learning of a fair GNN student. Additionally, it incorporates graph-level distillation and node-specific temperature learning to facilitate the transfer of knowledge, improving both fairness and model utility. Experimental results on three real-world datasets demonstrate the effectiveness of FairDTD in balancing fairness and model utility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRediT authorship contribution statement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two causal graphs are used to illustrate the comparison between prior work (a) and our proposed method (b). The causal graphs illustrate the predictions made by GNNs. Observed variables are depicted in brown, while unobserved variables are shown in blue. Solid edges represent direct causal relationships, whereas a dashed double arrow with a cross between E and C denotes statistical independence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance of model utility and fairness of the three training strategies, full data, only nodes, and only topology, on the Pokec-z, Bail, and Credit datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>â„’Figure 3 :</head><label>3</label><figDesc>Figure 3: The proposed FairDTD framework consists of three components: (a) dual-teacher distillation, (b) graph-level distillation, and (c) learning node-specific temperatures. The GNN student learns fair and informative knowledge under the guidance of the feature teacher and the structure teacher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Visualizations of the node representations learned on the Pokec-z dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of FairDTD with node-specific temperature learning and fixed temperatures ranging from 1 to 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sensitivity analysis of the balance hyperparameter ğ›¼ on the three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Chengyu Li :</head><label>Li</label><figDesc>Writing -original draft, Software, Methodology, Conceptualization, Visualization. Debo Cheng: Conceptualization, Writing -review &amp; editing, Formal analysis. Guixian Zhang: Conceptualization, Writing -review &amp; editing, Formal analysis. Yi Li: Writing -review &amp; editing, Formal analysis. Shichao Zhang: Writing -review &amp; editing, Supervision, Funding acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Training Algorithm of FairDTD Input : îˆ³ = (î‰‚, îˆ±, X), a two-layer MLP feature teacher ğ‘“ ğ‘“ ğ‘’ğ‘ , a two-layer GCN structure teacher ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ , a two-layer GNN student ğ‘“ ğ‘ ğ‘¡ğ‘¢ , ğ›¼, ğœ ğ‘šğ‘ğ‘¥ , ğœ ğ‘šğ‘–ğ‘› . Output: The GNN student model trained with parameter ğœƒ ğ‘ ğ‘¡ğ‘¢ .</figDesc><table /><note><p>1 // Feature</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>teacher training 2 while not converge do 3 for 1</head><label></label><figDesc>â†’ ğ‘’ğ‘ğ‘œğ‘â„ to ğ‘’ğ‘ğ‘œğ‘â„ ğ‘“ ğ‘’ğ‘ do ğ™ ğ‘“ ğ‘’ğ‘ , ğ‘ ğ‘“ ğ‘’ğ‘ â† ğ‘“ ğ‘“ ğ‘’ğ‘ (ğ—); ğ‘ ğ‘ ğ‘¡ğ‘Ÿ â† ğ‘“ ğ‘ ğ‘¡ğ‘Ÿ (ğ€);</figDesc><table><row><cell>5</cell><cell>Calculate îˆ¸ ğ‘“ ğ‘’ğ‘ according to Eq. (6);</cell></row><row><cell>6</cell><cell>Back-propagation to update parameters ğœƒ ğ‘“ ğ‘’ğ‘ ;</cell></row><row><cell>7</cell><cell>ğğ§ğğ°ğ¡ğ¢ğ¥ğ</cell></row><row><cell cols="2">8 // Structure teacher training</cell></row><row><cell cols="2">9 while not converge do</cell></row><row><cell>10</cell><cell>for 1 â†’ ğ‘’ğ‘ğ‘œğ‘â„ to ğ‘’ğ‘ğ‘œğ‘â„ ğ‘ ğ‘¡ğ‘Ÿ do</cell></row><row><cell cols="2">11 ğ™ ğ‘ ğ‘¡ğ‘Ÿ , 12 Calculate îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ according to Eq. (8);</cell></row></table><note><p>4 13 Back-propagation to update parameters ğœƒ ğ‘ ğ‘¡ğ‘Ÿ ; 14 ğğ§ğğ°ğ¡ğ¢ğ¥ğ 15 // Fair</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>GNN student training 16 while not converge do 17 for</head><label></label><figDesc></figDesc><table /><note><p>1 â†’ ğ‘’ğ‘ğ‘œğ‘â„ to ğ‘’ğ‘ğ‘œğ‘â„ ğ‘ ğ‘¡ğ‘¢ do 18 ğ™ ğ‘ ğ‘¡ğ‘¢ , ğ‘ ğ‘ ğ‘¡ğ‘¢ â† ğ‘“ ğ‘ ğ‘¡ğ‘¢ (ğ—, ğ€); 19 Calculate Ï„ğ‘“ğ‘’ğ‘ ğ‘– and Ï„ğ‘ ğ‘¡ğ‘Ÿ ğ‘– based on ğ™ ğ‘“ ğ‘’ğ‘ and ğ™ ğ‘ ğ‘¡ğ‘Ÿ according to Eq. (18) and Eq. (19); 20 Calculate îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘ ğ‘œğ‘“ ğ‘¡ and îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘ ğ‘œğ‘“ ğ‘¡ based on Ï„ğ‘“ğ‘’ğ‘ ğ‘– and Ï„ğ‘ ğ‘¡ğ‘Ÿ ğ‘– according to Eq. (11) and Eq. (12); 21 Calculate îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘šğ‘–ğ‘‘ and îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘šğ‘–ğ‘‘ based on ğ‘ ğ‘“ ğ‘’ğ‘ , ğ‘ ğ‘ ğ‘¡ğ‘Ÿ and ğ‘ ğ‘ ğ‘¡ğ‘¢ according to Eq. (14) and Eq. (15); 22 Calculate îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘‘ğ‘¢ğ‘ğ‘™ based on îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘ ğ‘œğ‘“ ğ‘¡ and îˆ¸ ğ‘“ ğ‘’ğ‘ ğ‘šğ‘–ğ‘‘ according to Eq. (20); 23 Calculate îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘‘ğ‘¢ğ‘ğ‘™ based on îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘ ğ‘œğ‘“ ğ‘¡ and îˆ¸ ğ‘ ğ‘¡ğ‘Ÿ ğ‘šğ‘–ğ‘‘ according to Eq. (21); 24 Calculate îˆ¸ â„ğ‘ğ‘Ÿğ‘‘ according to Eq. (10); 25 Calculate îˆ¸ ğ‘ ğ‘¡ğ‘¢ ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ according to Eq. (22); 26 Back-propagation to update parameters ğœƒ ğ‘ ğ‘¡ğ‘¢ ; 27 ğğ§ğğ°ğ¡ğ¢ğ¥ğ 28 return ğœƒ ğ‘ ğ‘¡ğ‘¢ .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>An overview of the statistics for the three real-world datasets.</figDesc><table><row><cell>Dataset</cell><cell>Pokec-z</cell><cell>Pokec-n</cell><cell>Credit</cell></row><row><cell>#Nodes</cell><cell>7,659</cell><cell>6,185</cell><cell>30,000</cell></row><row><cell>#Edges</cell><cell>29,476</cell><cell>21,844</cell><cell>2,843,716</cell></row><row><cell cols="2">#Features 59</cell><cell>59</cell><cell>13</cell></row><row><cell>Sens.</cell><cell>Region</cell><cell>Region</cell><cell>Age</cell></row><row><cell>Label</cell><cell cols="3">Working field Working field Future default</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 A</head><label>2</label><figDesc>comparative analysis of our proposed method, FairDTD, against baseline approaches is presented, focusing on accuracy and fairness metrics. The best performance for each backbone GNN is highlighted in bold.</figDesc><table><row><cell>Encoder</cell><cell>Method</cell><cell></cell><cell>Pokec-z</cell><cell></cell><cell></cell><cell>Pokec-n</cell><cell></cell><cell></cell><cell>Credit</cell></row><row><cell></cell><cell></cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell></row><row><cell></cell><cell>GCN</cell><cell cols="3">67.83Â±0.47 7.31Â±0.59 6.52Â±0.30</cell><cell cols="3">67.24Â±0.33 6.39Â±0.39 7.27Â±0.67</cell><cell cols="2">73.37Â±0.02 12.65Â±0.18 10.49Â±0.57</cell></row><row><cell></cell><cell>NIFTY</cell><cell cols="3">67.25Â±0.28 4.38Â±1.02 3.89Â±0.72</cell><cell cols="3">66.58Â±0.37 5.61Â±0.24 4.15Â±0.86</cell><cell cols="2">73.41Â±0.04 11.63Â±0.07</cell><cell>9.02Â±0.42</cell></row><row><cell>GCN</cell><cell cols="4">FairVGNN 68.39Â±3.01 3.63Â±1.75 4.74Â±1.94 CAF 67.90Â±0.82 2.70Â±0.14 3.67Â±0.12</cell><cell cols="3">68.11Â±0.35 4.13Â±1.65 6.14Â±3.04 67.25Â±1.05 2.12Â±1.50 2.10Â±1.70</cell><cell>78.51Â±0.39 73.90Â±1.52</cell><cell>4.39Â±3.75 5.83Â±1.78</cell><cell>2.70Â±2.72 5.60Â±1.42</cell></row><row><cell></cell><cell>FairGKD</cell><cell cols="3">69.18Â±0.54 4.05Â±1.09 3.06Â±1.03</cell><cell cols="3">67.79Â±0.35 0.74Â±0.48 2.07Â±1.03</cell><cell>79.63Â±0.39</cell><cell>3.42Â±1.61</cell><cell>2.78Â±1.12</cell></row><row><cell></cell><cell>FairGP</cell><cell cols="3">68.39Â±0.43 2.58Â±0.21 1.86Â±0.52</cell><cell cols="3">68.09Â±0.39 1.06Â±0.42 2.20Â±1.32</cell><cell>79.31Â±0.64</cell><cell>2.62Â±0.41</cell><cell>1.70Â±0.27</cell></row><row><cell></cell><cell>FairDTD</cell><cell cols="7">69.71Â±0.15 1.93Â±1.03 1.76Â±0.82 69.49Â±0.53 0.49Â±0.38 1.98Â±0.55 80.83Â±0.30</cell><cell>2.41Â±0.26</cell><cell>1.90Â±0.14</cell></row><row><cell></cell><cell>GIN</cell><cell cols="3">66.82Â±0.53 5.14Â±0.52 4.56Â±0.50</cell><cell cols="3">66.54Â±0.93 5.92Â±1.48 4.56Â±1.68</cell><cell cols="2">73.65Â±0.67 13.24Â±3.08 10.45Â±3.70</cell></row><row><cell></cell><cell>NIFTY</cell><cell cols="3">65.57Â±1.34 2.70Â±1.28 3.23Â±1.92</cell><cell cols="3">66.37Â±1.51 3.84Â±1.05 3.24Â±1.60</cell><cell>74.43Â±0.42</cell><cell>6.01Â±4.19</cell><cell>5.41Â±3.03</cell></row><row><cell>GIN</cell><cell cols="4">FairVGNN 68.24Â±0.95 2.23Â±1.44 3.88Â±1.19 CAF 67.71Â±0.75 2.64Â±0.78 2.30Â±0.31</cell><cell cols="3">67.43Â±1.31 4.04Â±2.56 6.92Â±3.87 67.57Â±1.87 4.10Â±0.64 2.87Â±1.32</cell><cell>77.50Â±0.89 74.57Â±1.06</cell><cell>2.62Â±0.97 4.26Â±1.29</cell><cell>1.71Â±0.78 6.32Â±1.81</cell></row><row><cell></cell><cell>FairGKD</cell><cell cols="3">67.58Â±2.05 1.83Â±0.93 2.23Â±1.02</cell><cell cols="3">68.36Â±0.53 1.37Â±0.84 2.99Â±1.53</cell><cell>79.18Â±0.65</cell><cell>3.08Â±3.53</cell><cell>2.57Â±3.10</cell></row><row><cell></cell><cell>FairGP</cell><cell cols="6">67.57Â±0.60 1.67Â±0.37 1.29Â±0.18 68.81Â±0.82 1.85Â±0.54 1.69Â±0.28</cell><cell>78.21Â±0.33</cell><cell>1.30Â±0.28</cell><cell>1.58Â±0.67</cell></row><row><cell></cell><cell>FairDTD</cell><cell cols="7">67.73Â±0.77 1.01Â±0.70 0.94Â±0.85 67.74Â±0.71 1.16Â±0.98 1.27Â±0.54 80.29Â±0.27</cell><cell>1.06Â±0.52</cell><cell>1.18Â±0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Analysis of model fairness and node classification bias among distinct FairDTD variants. The optimal results for each GNN backbone are highlighted in bold.</figDesc><table><row><cell cols="2">Encoder Model Variants</cell><cell></cell><cell>Pokec-z</cell><cell></cell><cell></cell><cell>Pokec-n</cell><cell></cell><cell></cell><cell>Credit</cell></row><row><cell></cell><cell></cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell><cell>ACC â†‘</cell><cell>Î” ğ‘ ğ‘ â†“</cell><cell>Î” ğ‘’ğ‘œ â†“</cell></row><row><cell></cell><cell>FairDTD</cell><cell cols="8">69.71Â±0.15 1.93Â±1.03 1.76Â±0.82 69.49Â±0.53 0.49Â±0.38 1.98Â±0.55 80.83Â±0.30 2.41Â±0.26 1.90Â±0.14</cell></row><row><cell></cell><cell>w/o FT</cell><cell cols="3">66.88Â±0.94 2.69Â±0.70 2.41Â±0.23</cell><cell cols="3">67.68Â±0.42 2.15Â±0.45 3.18Â±0.52</cell><cell cols="2">77.08Â±0.29 3.86Â±0.48 2.60Â±0.23</cell></row><row><cell>GCN</cell><cell>w/o ST w/o GD</cell><cell cols="3">69.35Â±0.97 3.43Â±1.67 3.36Â±1.56 68.83Â±1.20 5.82Â±0.25 5.33Â±1.58</cell><cell cols="3">68.88Â±1.58 3.71Â±0.29 4.41Â±0.51 68.71Â±0.68 2.94Â±0.22 3.24Â±2.74</cell><cell cols="2">78.76Â±0.30 4.84Â±0.15 3.46Â±0.35 78.60Â±1.14 5.60Â±1.50 3.06Â±0.62</cell></row><row><cell></cell><cell>w/o NST</cell><cell cols="3">68.74Â±0.60 2.64Â±1.11 3.16Â±2.30</cell><cell cols="3">68.52Â±0.56 2.01Â±0.84 4.08Â±1.60</cell><cell cols="2">79.11Â±0.78 4.40Â±0.63 2.54Â±0.79</cell></row><row><cell></cell><cell>FairDTD</cell><cell cols="8">67.73Â±0.77 1.01Â±0.70 0.94Â±0.85 67.74Â±0.71 1.16Â±0.98 1.27Â±0.54 80.29Â±0.27 1.06Â±0.52 1.18Â±0.53</cell></row><row><cell></cell><cell>w/o FT</cell><cell cols="6">65.60Â±1.36 3.22Â±1.63 0.44Â±0.26 63.24Â±1.01 1.92Â±2.38 1.89Â±0.40</cell><cell cols="2">76.47Â±0.81 1.78Â±1.54 0.81Â±0.38</cell></row><row><cell>GIN</cell><cell>w/o ST w/o GD</cell><cell cols="3">68.44Â±0.40 4.93Â±1.30 3.35Â±0.48 65.76Â±0.78 2.07Â±1.68 3.04Â±0.46</cell><cell cols="3">67.11Â±0.67 1.70Â±0.24 2.81Â±0.29 65.80Â±0.20 2.68Â±1.81 3.40Â±1.16</cell><cell cols="2">77.12Â±0.40 3.96Â±0.21 3.53Â±0.26 77.80Â±0.67 3.72Â±0.11 2.59Â±0.17</cell></row><row><cell></cell><cell>w/o NST</cell><cell cols="3">67.15Â±0.62 3.67Â±2.12 0.83Â±0.32</cell><cell cols="3">66.32Â±0.38 2.41Â±1.13 2.04Â±0.24</cell><cell cols="2">78.56Â±1.16 4.32Â±0.58 1.73Â±0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>3. Compared with NIFTY, FairDTD avoids graph-level generation by modeling sensitive attribute propagation through causal paths, enabling more efficient and scalable fairness interventions. Compared with FairVGNN, FairDTD addresses this gap by using a causal graph to trace both feature-based and structure-based pathways of sensitive influence, enabling targeted, theory-driven blocking strategies. Compared with CAF, FairDTD uses principled causal interventions instead, enhancing generalization and robustness without depending on external counterfactual data. Compared with FairGKD's synthetic fair teacher model, FairDTD demonstrates superior performance across nearly all aspects. This highlights the effectiveness of our dual fair teacher model in providing richer knowledge to the student model during training. Additionally, unlike FairGKD's use of a fixed temperature, FairDTD learns node-specific temperatures to adaptively soften the teacher logits, thereby providing more accurate guidance for the student model and achieving a better balance between fairness and utility. Compared with FairGP, FairDTD explicitly models and blocks causal pathways of sensitive attributes via both node features and structure, supporting more targeted fairness control.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported in part by the <rs type="funder">Project of Guangxi Science and Technology (GuiKeAB23026040)</rs>, the <rs type="funder">Research Fund of Guangxi Key Lab of Multi-source Information Mining &amp; Security</rs> (Nos. <rs type="grantNumber">20-A-01-01</rs> and <rs type="grantNumber">MIMS21-M01</rs>), the <rs type="projectName">Guangxi Collaborative Innovation Center of Multi-Source Information Integration and Intelligent Processing</rs>, the <rs type="funder">Research Fund of Guangxi Key Lab of Multi-source Information Mining &amp; Security</rs> (<rs type="grantNumber">MIMS24-03</rs>), the <rs type="funder">Research Fund of Guangxi Key Lab of Multi-source Information Mining &amp; Security</rs> (<rs type="grantNumber">MIMS24-13</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZQx2s3h">
					<idno type="grant-number">20-A-01-01</idno>
				</org>
				<org type="funded-project" xml:id="_AdZ4q2p">
					<idno type="grant-number">MIMS21-M01</idno>
					<orgName type="project" subtype="full">Guangxi Collaborative Innovation Center of Multi-Source Information Integration and Intelligent Processing</orgName>
				</org>
				<org type="funding" xml:id="_tCNhs7W">
					<idno type="grant-number">MIMS24-03</idno>
				</org>
				<org type="funding" xml:id="_3y5CqpJ">
					<idno type="grant-number">MIMS24-13</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey on trustworthy graph neural networks: Privacy, robustness, fairness, and explainability</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaisheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-view hypergraph contrastive learning for attribute-aware recommendation</title>
		<author>
			<persName><forename type="first">Ang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">103701</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph similarity learning for cross-level interactions</title>
		<author>
			<persName><forename type="first">Cuifang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangquan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxia</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103932</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Contrastive learning for fair graph representations via counterfactual graph augmentation. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">112635</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The devil is in the conflict: Disentangled information graph neural networks for fraud detection</title>
		<author>
			<persName><forename type="first">Zhixun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1059" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients</title>
		<author>
			<persName><forename type="first">I-Cheng</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che-Hui</forename><surname>Lien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert systems with applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2473" to="2480" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning for fair graph representations</title>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page">106781</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quantum support vector machine for classifying noisy data</title>
		<author>
			<persName><forename type="first">Jiaye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiagang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2233" to="2247" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving fairness in graph neural networks via mitigating sensitive attribute leakage</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining</title>
		<meeting>the 28th ACM SIGKDD conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1938" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randomization tests for distinguishing social influence and homophily effects</title>
		<author>
			<persName><forename type="first">Timothy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Fond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairegm: fair link prediction and recommendation via emulated graph modification</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Current</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Gurukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</title>
		<meeting>the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On dyadic fairness: Exploring and mitigating bias in graph connections</title>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fairgat: Fairness-aware graph attention networks</title>
		<author>
			<persName><forename type="first">Kose</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanning</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyper-class representation of data</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsong</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">503</biblScope>
			<biblScope unit="page" from="200" to="218" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data preparation for data mining</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="375" to="381" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding and improving early stopping for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinian</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24392" to="24403" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Challenges in knn classification</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4663" to="4675" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The devil is in the data: Learning fair graph neural networks via partial knowledge distillation</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 17th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1012" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fugnn: Harmonizing fairness and utility in graph neural networks</title>
		<author>
			<persName><forename type="first">Renqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huafei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estrid</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2072" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fair graph representation learning via sensitive attribute disentanglement</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1182" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic temperature knowledge distillation</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.12711</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Curriculum temperature for knowledge distillation</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1504" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian graph local extrema convolution with long-tail strategy for misinformation detection</title>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tts-norm: Forecasting tensor time series via multi-way normalization</title>
		<author>
			<persName><forename type="first">Jiewen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renhe</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mitigating propensity bias of large language models for recommender systems</title>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.20052</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Heterogeneous graph convolutional network for multi-view semi-supervised classification</title>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page">106438</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention presents low-dimensional knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Baghershahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reshad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Moradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page">110124</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Himul-lgg: A hierarchical decision fusion-based local-global graph neural network for multimodal emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">Changzeng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengkui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Toshinori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page">106764</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5782" to="5799" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Homogeneous graph neural networks for third-party library recommendation</title>
		<author>
			<persName><forename type="first">Duantengchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">103831</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">La-mgfm: A legal judgment prediction method via sememe-enhanced graph neural networks and multi-graph fusion mechanism</title>
		<author>
			<persName><forename type="first">Qihui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">103455</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view gcn for loan default risk prediction</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yakun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Synchronization of markovian jump neural networks for sampled data control systems with additive delay components: Analysis of image encryption technique</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Authorea Preprints</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Input-to-state stability of stochastic markovian jump genetic regulatory networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Velusamy</forename><surname>Radhika</surname></persName>
		</author>
		<author>
			<persName><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics and Computers in Simulation</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="174" to="187" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Ã–ykÃ¼</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">KÃ¶se</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yanning</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05391</idno>
		<title level="m">Fairness-aware node representation learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The frontiers of fairness in machine learning</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08810</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fpgnn: Fair path graph neural network for mitigating discrimination</title>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3119" to="3136" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards a unified framework for fair and stable graph representation learning</title>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Individual fairness for graph neural networks: A ranking based approach</title>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edits: Modeling and mitigating data bias for graph neural networks</title>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM web conference 2022</title>
		<meeting>the ACM web conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fairdrop: Biased edge dropout for enhancing fairness in graph representation learning</title>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="344" to="354" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning fair representations via rebalancing graph structure</title>
		<author>
			<persName><forename type="first">Guixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">103570</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fairsin: Achieving fairness in graph neural networks through sensitive information neutralization</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9241" to="9249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">One fits all: Learning fair graph neural networks for various sensitive attributes</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4688" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fg-smote: Towards fair node classification with graph neural network</title>
		<author>
			<persName><forename type="first">Zichong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Pissinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards fair graph learning without demographic information</title>
		<author>
			<persName><forename type="first">Zichong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhat</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sundararaja</forename><surname>Sitharama Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Trustworthy graph neural networks: Aspects, methods, and trends</title>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingliang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Partdependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7597" to="7610" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning the latent causal structure for modeling label noise</title>
		<author>
			<persName><forename type="first">Yexiong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="120549" to="120577" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24934" to="24946" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fair feature selection: A causal perspective</title>
		<author>
			<persName><forename type="first">Zhaolong</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data-driven causal effect estimation based on graphical causal modelling: A survey</title>
		<author>
			<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuc</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Instance-dependent label-noise learning under a structural causal model</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4409" to="4420" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Path-specific causal reasoning for fairness-aware cognitive diagnosis</title>
		<author>
			<persName><forename type="first">Dacao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4143" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Self-distillation as instance-specific label smoothing</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2184" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Boosting graph neural networks via adaptive knowledge distillation</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7793" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Data analysis in public social networks</title>
		<author>
			<persName><forename type="first">Lubos</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Zabovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International scientific conference and international workshop present day trends of innovations</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards fair graph neural networks via graph counterfactual</title>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 32nd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fairgp: A scalable and fair graph transformer using graph partitioning</title>
		<author>
			<persName><forename type="first">Renqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huafei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="12319" to="12327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
		<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rethinking fair graph neural networks from re-balancing</title>
		<author>
			<persName><forename type="first">Zhixun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1736" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
