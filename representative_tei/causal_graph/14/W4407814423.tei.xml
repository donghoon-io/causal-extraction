<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Internal Incoherency Scores for Constraint-based Causal Discovery Algorithms</title>
				<funder ref="#_sztB7WU">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_KjjapAQ">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sofia</forename><surname>Faltenbacher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Wahl</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rebecca</forename><surname>Herman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University</orgName>
								<address>
									<settlement>Dresden</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Internal Incoherency Scores for Constraint-based Causal Discovery Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery aims to infer causal graphs from observational or experimental data. Methods such as the popular PC algorithm are based on conditional independence testing and utilize enabling assumptions, such as the faithfulness assumption, for their inferences. In practice, these assumptions, as well as the functional assumptions inherited from the chosen conditional independence test, are typically taken as a given and not further tested for their validity on the data. In this work, we propose internal coherency scores that allow testing for assumption violations and finite sample errors, whenever detectable without requiring ground truth or further statistical tests. We provide a complete classification of erroneous results, including a distinction between detectable and undetectable errors, and prove that the detectable erroneous results can be measured by our scores. We illustrate our coherency scores on the PC algorithm with simulated and real-world datasets, and envision that testing for internal coherency can become a standard tool in applying constraint-based methods, much like a suite of tests is used to validate the assumptions of classical regression analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causal discovery, also known as causal structure learning, is a highly active research field with new methods being published at almost every major machine learning conference (see <ref type="bibr" target="#b32">Zanga et al. [2022]</ref>, <ref type="bibr" target="#b0">Assaad et al. [2022]</ref>, <ref type="bibr" target="#b2">Brouillard et al. [2024]</ref> for recent surveys on methods and applications). The field aims to provide tools for inferring qualitative causeand-effect relationships from observational and experimental data while elucidating the assumptions that are needed to do so. One of the seminal causal discovery algorithms that inspired many others is the PC-algorithm <ref type="bibr" target="#b27">[Spirtes and Glymour, 1991]</ref>. Starting from a fully connected graph, the PC-algorithm sequentially executes statistical tests for conditional independence (CI), pruning the graph whenever an independence is found and choosing the next tests based on the current state of the graph. Finally, it uses previously tested or additional conditional independence statements to orient as many edges as possible.</p><p>The correctness of the PC algorithm and its descendants, such as FCI <ref type="bibr" target="#b26">[Spirtes, 2001]</ref> and PCMCI <ref type="bibr">[Runge et al., 2019b</ref><ref type="bibr" target="#b21">, Runge, 2020]</ref>, is typically proven for an idealized setting without statistical finite sample errors. In this setting, it is shown that the output is sound and maximally informative as long as the data generation process satisfies the mechanistic assumptions of the algorithm <ref type="bibr" target="#b29">[Spirtes et al., 2000]</ref>. But theoretical results on finite sample performance are much harder to prove and therefore rare (see <ref type="bibr" target="#b11">[Kalisch and Bühlmann, 2007]</ref>). Assessments of finite sample performance are common using any of a variety of performance metrics <ref type="bibr" target="#b30">[Tsamardinos et al., 2006</ref><ref type="bibr" target="#b7">, Henckel et al., 2024</ref><ref type="bibr" target="#b31">, Wahl and Runge, 2024]</ref>, but these metrics usually require knowledge of the ground truth causal graph, limiting these evaluations to simulated data that may have exploitable unrealistic artifacts <ref type="bibr" target="#b19">[Reisach et al., 2021]</ref>. In addition, it is often unclear how well real-world data conforms to the assumptions of causal discovery. As a consequence of statistical errors and assumption violations, PC-like methods may return an output graph with conspicuous inconsistencies, such as conflicting edge orientations where the algorithm attempted to orient the same edge in different directions at different points of the procedure.</p><p>The earliest versions of PC-like algorithms imposed orientation-conflict resolution strategies implicitly; for example, the first pseudo-code of the PC and FCI algorithms by <ref type="bibr" target="#b28">Spirtes et al. [1993]</ref> is often implemented with an orderdependent last-come-last-orient implicit conflict resolution, for instance in the package causal-learn <ref type="bibr" target="#b35">Zheng et al. [2024]</ref>. Later versions offered explicit strategies. <ref type="bibr" target="#b16">Ramsey [2016]</ref> used a conflict resolution strategy that prioritizes some con-arXiv:2502.14719v1 [stat.ML] 20 Feb 2025 ditional independencies over others by sorting the p-values. While such strategies force an algorithm to return a graph with fewer or no explicit orientation conflicts, they do not address the extent to which underlying assumption violations or finite sample errors affect the pruning of the graph or the overall correctness of the results.</p><p>An existing approach which directly addresses this question is the reformulation of constraint-based causal discovery as an optimization problem based on Boolean satisfiability solvers <ref type="bibr" target="#b9">[Hyttinen et al., 2014]</ref>. It retains high accuracy under statistical errors, but the accuracy comes at the price of superexponential growth in computational cost as the number of variables increases. <ref type="bibr" target="#b17">Ramsey et al. [2006]</ref> proposed conservative-PC which executes additional statistical tests to find local ambiguities in the graph. <ref type="bibr" target="#b4">Colombo and Maathuis [2014]</ref> suggested resolving these with a majority vote and explicitly marking any remaining orientation conflicts.</p><p>As finite sample errors and some degree of assumption violations are unavoidable for real-world data, there is a need to develop better tests for model plausibility and robustness of causal discovery methods in general and PC-like methods in particular. Such tests must not rely on an underlying ground truth that is unavailable for real data. In this article, we propose a class of plausibility checks tailored to PC-like algorithms. Importantly, our checks only make use of information that the algorithm has already gained, thus avoiding additional statistical testing or prediction. We then prove that our proposed check will detect any erroneous result that are theoretically detectable given the information gained in one run of a PC-like method.</p><p>More broadly, we examine manifestations of assumption violations and errors of PC-based algorithms. We have a closer look at the aforementioned orientation conflicts and observe that their presence implies that there is no graph of the graph type assumed by the algorithm (e.g., directed acyclic graphs for PC, maximal ancestral graphs for FCI) that can accommodate the conditional independencies measured by the algorithm. We illustrate in Section 3.3 that if conflicts are present, there is no guarantee that even seemingly undisputed orientations in a method's output graph are correct. In the presence of conflicts, it is thus impossible to arrive at a valid method output without a strategy that prioritizes some test results above others.</p><p>Even in graphs without explicit orientation conflicts and ambiguities, it is possible that some of the conditional (in)dependencies implied by the method's output graph contradict those that the algorithm has measured during its run. While a perfect match is often unachievable on finitesample data, we argue that the degree of their mismatch provides valuable information on the correctness of statistical and mechanistic assumptions, and in Section 4 we propose scores that quantify this mismatch. In the absence of assumption violations and statistical errors, any sound algorithm will receive a coherency score of one. In the presence of assumption violations and statistical errors, a high score indicates that the output graph is a near-optimal representation of the CI test results while a low score indicates that the graph does not serve as a useful interpretation of the data. We therefore argue that coherency scores are powerful tools to test the plausibility of method assumptions without access to the ground truth. Since explicit resolution of orientation conflicts or ambiguities always leads to incoherencies, different resolution strategies can lead to different incoherency scores, and these scores can in turn be used to find an optimal-in the sense that it minimizes internal incoherencies-resolution strategy.</p><p>This work is structured as follows. Section 2 introduces necessary notations and conventions. In Section 3, we discuss and classify different types of errors and their consequences in PC-based causal discovery. We illustrate our classification through simple examples for different failure modes and prove that incoherencies capture all errors that are detectable without further information. In Section 4, we define the coherency scores that are the central tool of this work. These scores are then applied to several toy examples, simulations, and the auto MPG real world data set <ref type="bibr" target="#b15">[Quinlan, 1993]</ref> in Section 5. Additional examples, experiments and proofs are summarized in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>While the PC algorithm and its variants remain among the most popular causal discovery tools, which is exactly why we believe improved error quantification is necessary, other independence test-based methods have been introduced as a response to some of the PC algorithm's shortcomings. In addition to the aforementioned Boolean satisfiability solvers <ref type="bibr" target="#b8">Hyttinen et al. [2013</ref><ref type="bibr" target="#b9">Hyttinen et al. [ , 2014]]</ref>, <ref type="bibr" target="#b3">Claassen and Heskes [2012]</ref> and <ref type="bibr" target="#b10">Jabbari et al. [2017]</ref> combine CI testing with Bayesian approaches that provide a best fit to the tested constraints. In <ref type="bibr">[Wahl and Runge, 2024, Appendix F]</ref>, the difficulties connected to orientation conflicts are discussed in the context of distance metrics for causal graphs. For general causal discovery methods, different plausibility tests have been proposed in <ref type="bibr" target="#b6">Faller et al. [2023]</ref>, <ref type="bibr" target="#b5">Eulig et al. [2023]</ref>, <ref type="bibr" target="#b24">Schkoda et al. [2024]</ref>. These checks can be applied more broadly than ours, but they also require additional testing or several method runs on different sets of variables, while ours only uses information that the algorithm has already gained but is specific to PC-like methods. <ref type="bibr">Ramsey et al. [2024]</ref> presented a test for whether the Markov condition holds or does not hold in DAGs and CPDAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND NOTATION</head><p>In this work, X = (X 1 , . . . , X d ) with d ∈ N denotes a set of nodes that represent random variables in a structural causal model (SCM) as defined by Pearl <ref type="bibr">[2009]</ref>, and G denotes a causal graph over X. If the data generating process is a known SCM, the causal graph implied by this SCM is denoted by G true . Causal graphs come with a notion of graphical separation such as d-separation for directed acyclic graphs <ref type="bibr">(DAGs, e.g. Pearl [2009]</ref>), m-separation for maximal ancestral graphs (MAGs, e.g. <ref type="bibr" target="#b33">[Zhang, 2008]</ref>), and σ-separation for cyclic graphs (e.g. <ref type="bibr" target="#b1">[Bongers et al., 2021]</ref>). Unless indicated otherwise, statements in this work do not depend on the type of graph as long as one consistently uses the corresponding criterion for separation and its counterpart: connection. We represent separation and connection statements with tuples consisting of two singular nodes and a (possibly empty) disjoint subset of nodes. The set of all such statements is denoted by</p><formula xml:id="formula_0">C = (X, Y, S) X, Y ∈ X, X ̸ = Y, S ⊆ X\{X, Y } ,</formula><p>and given a causal graph G, the relevant notion of separation specifies for each tuple (X, Y, S) whether it is a separation statement X ▷◁ G Y | S (X and Y are separated given S) or a connection statement X ̸ ▷◁ G Y | S (X and Y are connected given S). We define the separation indicator function</p><formula xml:id="formula_1">ι G : C → {0, 1} as ι G (X, Y, S) = 1 if X ▷◁ G Y | S and as ι G (X, Y, S) = 0 otherwise. The set of separations is denoted by C sep = (X, Y, S) ∈ C ι G (X, Y, S) = 1 .</formula><p>and the set of connections by</p><formula xml:id="formula_2">C con = (X, Y, S) ∈ C ι G (X, Y, S) = 0 .</formula><p>In a causal graph, an unshielded triple is a triple of nodes (X, Y, Z) such that the outer nodes are adjacent to the middle node but not to each other. An unshielded triple is a collider if the edges (X, Y ) and (Y, Z) point towards Y (e.g. X → Y ← Z), otherwise the triple is called a noncollider. M will be our symbol for a constraint-based causal discovery method. For example, M could be a stand-in for variants of the PC algorithm like the classical PC or FCI algorithm (see, e.g. <ref type="bibr" target="#b28">Spirtes et al. [1993]</ref>), the PC-stable algorithm (see, e.g. <ref type="bibr" target="#b4">Colombo and Maathuis [2014]</ref>), or the PCMCI algorithm family (see, e.g. <ref type="bibr">Runge et al. [2019a]</ref>, <ref type="bibr" target="#b21">Runge [2020]</ref>).</p><p>Let (x 1 , . . . , x n ) ∈ R (d×n) be n ∈ N samples drawn from the d-dimensional distribution P X . Any constraint-based causal discovery method uses one or several statistical tests for conditional independence (CI), or dependence <ref type="bibr" target="#b13">[Malinsky, 2024]</ref> for which all statements can be formulated analogously. Let T be a CI test and let α be the significance level. For brevity, we assume that the PC-like method uses only one type of CI test on all tested statements, but all results can be generalized in a straightforward way to algorithm variants using different types of tests.</p><p>Common CI tests include Fisher's Z test for conditional independence in the linear Gaussian setting, kernel-based methods <ref type="bibr" target="#b34">[Zhang et al., 2011]</ref>, regression-based tests including the generalized covariance measure <ref type="bibr" target="#b25">[Shah and Peters, 2020]</ref>, or CMIknn <ref type="bibr" target="#b20">[Runge, 2018]</ref>, which is based on a nearest neighbour estimator of the conditional mutual information.</p><p>As we did for graphical separation, we now define a conditional independence test indicator function ι (T,α) :</p><formula xml:id="formula_3">C → {0, 1} for a CI test (T, α) as ι (T,α) (X, Y, S) = 1 if X |= (T,α) Y | S (i.</formula><p>e. the tuple tested independent), and ι (T,α) (X, Y, S) = 0 otherwise. By T (M, T, α) ⊆ C we denote the set of all statements that were tested during the execution of the algorithm M using test T at significance α. We simply write T if the referenced method, test, and threshold are clear from the context. The statements which tested conditionally independent (or for which dependence was rejected, e.g. in <ref type="bibr" target="#b13">[Malinsky, 2024]</ref>) are denoted by</p><formula xml:id="formula_4">T ind = (X, Y, S) ∈ T ι (T,α) (X, Y, S) = 1 .</formula><p>The statements for which independence was rejected (or dependence was accepted) are denoted by</p><formula xml:id="formula_5">T dep = (X, Y, S) ∈ T ι (T,α) (X, Y, S) = 0 .</formula><p>We call the the output graph G out of a PC-like method M erroneous if it is not Markov equivalent to G true -the causal graph corresponding to the data generating SCM-or if there are marked conflicts or ambiguities in G out such that Markov equivalence is not well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SOURCES AND DETECTABILITY OF ERRONEOUS RESULTS</head><p>This section lays the foundation for incoherency detection. We stress that the true causal graph G true is in general not known for real-world problems. The goal of this paper is to work out how to detect erroneous results without access to the ground truth whenever possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SOURCES OF ERRONEOUS RESULTS</head><p>This subsection summarizes the various sources of error.</p><p>We cluster the issues that can cause erroneous results into four error codes. This coding is complete in the sense that every assumption that could be violated is covered and finite sample issues are included.</p><p>E1: Faithfulness Violations. Faithfulness is violated if</p><formula xml:id="formula_6">X |= P X Y | S but (X, Y, S) / ∈ C sep , i.e.</formula><p>, if X and Y are conditionally independent given S, but not separated given S. Two examples of faithfulness violations due to counteractive mechanisms are discussed in Examples 2 and 6.</p><p>E2: Structural Violations. Under this error code, we summarize all other violations of structural assumptions of PClike algorithms, including acyclicity (which is assumed in all classical versions of PC, FCI and PCMCI+), causal sufficiency (e.g. in PC, PCMCI+) or causal stationarity (only for time series data, e.g. in PCMCI+). In other words, this error applies when the class of graphs considered by the algorithm is too limited, e.g. the class of DAGs cannot capture hidden confounding or cycles.</p><p>E3: Conditional Independence Test Assumption Violations. The assumptions of the conditional independence test T might not hold in P X . This case covers violations of parametric or statistical assumptions such as linearity, additive noise, and the assumption that the samples are generated by independent, identically distributed variables. We also observe that nonlinear regression-based CI tests can be particularly tricky in the context of PC-like causal discovery, as the tests' assumptions on the functional relationship of regressor and regressand under the null might not be appropriate for every tested (in)dependence statement even if they match the functional relationship of a variable to its causes in the underlying SCM, as we illustrate in Appendix D, Example 4. E4: Finite Sample Errors. Even if all assumptions of the CI tests hold, Type I and Type II errors are still expected on finite data. These errors lead to mistakenly retained or deleted edges and may influence edge orientations later on. Such errors can be due to unlucky draws from the underlying distribution. They can also be present in most simulations for certain unlucky causal structures. For example, for a fixed finite sample size, almost determinism may lead to wrong test results in most simulations (see Table <ref type="table" target="#tab_5">9</ref>) Figure <ref type="figure" target="#fig_6">6</ref> in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MANIFESTATIONS OF ERRONEOUS RESULTS</head><p>When applying a PC-like method M to real data, we typically do not know G true , but only the tuples T tested by the CI test T with threshold α, the corresponding test results, and the output graph G out . From the latter, we can compute the value of ι Gout (X, Y, S) for any tuple (X, Y, S) when needed.</p><p>We cluster the consequences of erroneous results on a distribution level into three cases.</p><p>F1: There is no distribution P X that can represent the observed conditional (in)dependence test results. More formally, there is no P</p><formula xml:id="formula_7">X such that (X, Y, S) ∈ T ind implies X |= P X Y | S and (X, Y, S) ∈ T dep implies X ̸ |= P X Y | S.</formula><p>This happens if the test results violate basic properties of conditional independence relations stated, for example, in <ref type="bibr">Pearl [2009]</ref> as the graphoid axioms.</p><p>F2: A distribution P X can represent the conditional independence test results, but none is Markovian and faithful to G out . In Example 2, the distribution is neither Markovian and faithful to any DAG nor any MAG.</p><p>F3: A distribution P X can represent the conditional independence test results and is Markovian and faithful to G out . In Example 6, we show such an erroneous result for which the conditional independencies T ind are Markovian and faithful to G out .</p><p>Orientation Conflicts If the assumptions of the PC algorithm, PC-stable, or PCMCI+ are perfectly satisfied, their output is a graph representing a Markov equivalence class, i.e. a CPDAG (PAGs for versions of FCI). As mentioned in the introduction, this is not always true in practice, as these algorithms may want to orient an edge in opposite directions due to conflicting CI test results, see Fig. <ref type="figure">1</ref> and Example 1 below. This problem can be approached in two alternative ways; either by marking conflicts explicitly in the graph, or by forcing the algorithm to make a decision through a conflict-resolution strategy, for instance by ranking tested conditional independencies by some heuristic such as the p-value <ref type="bibr" target="#b16">[Ramsey, 2016]</ref>, or by allowing the algorithm to overwrite previous orientations. Clearly, these strategies have their own downsides, as overwriting makes the outcome depend on the variable order, while the relationship of p-values and test reliability can only serve as a heuristic due to the uniform distribution of the p-value under the null hypothesis <ref type="bibr">[Ramsey et al., 2024]</ref>. On the other hand, marking conflicts only where they appear may also be problematic. Example 1 shows that in the presence of conflicts, even seemingly orientable edges may no longer be trustworthy. At the same time, the appearance of conflicts is valuable information as they are an indicator of assumption violations or finite sample issues, and when employing a conflict-resolution strategy, this information is no longer directly visible in the output graph. The coherency scores that we will introduce in Section 4 offer a middle ground and will be able to capture this information also after applying resolution strategies, so that it can be presented to the user.</p><p>Ambiguities Some PC-like algorithms including conservative PC <ref type="bibr" target="#b17">[Ramsey et al., 2006]</ref> and PC with the majority rule <ref type="bibr" target="#b4">[Colombo and Maathuis, 2014]</ref> may additionally mark some unshielded triples as ambiguous, indicating that the algorithm refuses to take a decision as to whether a triple is a collider or a non-collider. Ambiguities can be considered an honest version of marking conflicts as the algorithm explicitly indicates its uncertainty and does not base orientations on ambiguous triples (when using conservative PC) or picks the option for which there is more evidence and marks ambiguities in case of a tie (when applying the majority rule). Conflict-resolution strategies can also be applied to ambiguities and the to-be-defined coherency score accounts for them as well.</p><p>We now define incoherent results of a PC-like method.</p><p>Definition 1. Let M be a PC-like method using a CI test (T, α) with output G out without marked orientation conflicts or ambiguities. Let T = T ind ∪ T dep be the CI test results and C sep and C con the conditionally separated or connected tuples in G out . We call the result of the PC-like algorithm</p><formula xml:id="formula_8">incoherent if T ind ̸ ⊆ C sep or T dep ̸ ⊆ C con .</formula><p>Note that testing for incoherencies is a straightforward task: The results of the CI tests can be saved while performing the method, and separation queries can be answered for all common graph types that are outputs of PC-like algorithms such as CPDAGs and MAGs after potentially resolving marked conflicts and ambiguities. For d-separation queries in CPDAGs and m-separation queries in MAGs, see, e.g. <ref type="bibr" target="#b14">Perković et al. [2015]</ref>.</p><p>The output of a PC-like method belongs to one of the following three categories.</p><p>G1: Orientation conflicts or ambiguities are marked in G out . G2: No conflicts or ambiguities are marked in G out , but the results of the PC-like method are incoherent. Example 2 shows a faithfulness violation (E1) that leads to no conflicts but incoherent results.</p><p>G3: No conflicts or ambiguities are marked in G out and the results of the PC-like method are coherent. Example 6 shows a faithfulness violation (E1) that leads to such an erroneous result without conflicts or incoherencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLASSIFICATION AND DETECTION OF ERRONEOUS RESULTS</head><p>We demonstrate important observations with concrete examples leading up the main results about classification and detection of erroneous results. We start with the aforementioned Example 1 that stresses how orientation conflicts not only reflect local, but global unreliability of the orientations.</p><p>Example 1. We consider the PC algorithm without any conflict resolution and the graph in Figure <ref type="figure">1</ref>. We assume that the algorithm has found the colliders W 1 → X 3 ← W 2 and W 3 → X 4 ← W 4 , all other unshielded triples have been classified as non-colliders. According to PC's orientation rules applied to the first collider, all edges between X 3 and X 6 have to be directed to the right. On the other hand, applying the same rule to the second collider directs these edges to the left, resulting in a conflict which is flagged at one or more of these edges (depending on the implementation). The edges left of W 1 → X 3 ← W 2 are not flagged as the orientation rule that directs them away from the collider can be executed without conflicts. However, we argue that there is no reason to believe that even these orientations are still correct. After all, the underlying reason of the conflict is that there exists no DAG with the depicted skeleton for which the set of unshielded colliders is exactly</p><formula xml:id="formula_9">{W 1 → X 3 ← W 2 , W 3 → X 4 ← W 4</formula><p>}, so we cannot trust the edges oriented because of them.</p><formula xml:id="formula_10">X1 X2 X3 X4 X5 X6 W1 W2 W3 W4</formula><p>Figure <ref type="figure">1</ref>: Without conflict resolution, only the dotted edges are marked as orientation conflicts, while also the seemingly orientable edges are no longer reliable.</p><p>Next is an example of a result without orientation conflicts which is incoherent.</p><p>Example 2. We look at the following ground truth structural causal model: <ref type="figure" target="#fig_0">2</ref>. As in Example 6 there is a perfect counteractive mechanism (E1). Assuming no other sources of errors, this leads to the classical PC algorithm testing T ind = (X, Y, ∅), (Z, Y, {X, W }), (W, X, {Z}) and all other tuples to be dependent. There is a distribution capturing these independencies and dependencies, but it is not Markovian and faithful to G out (F2). There are no orientation conflicts that arise while we perform the method, but we can detect that X and Y are tested conditionally independent, but they are not d-separated in G out (G2), see Figure <ref type="figure" target="#fig_0">2</ref>. Note that an undirected path in general does not imply d-connection, however X and Y are d-separated in this case as the path consists of undirected unshielded triples which fulfills the criterion for d-connection in CPDAGs which can for example be found in <ref type="bibr" target="#b14">Perković et al. [2015]</ref>. We can detect the incoherency using the scores that will be introduced in Section 4. Looking at the previous example, we can formulate the following observation.</p><formula xml:id="formula_11">X = η X , Z = X + η Z , W = Z + η W , Y = 2X -2W + η Y ; with independent, exogenous noise variables η X , η Z , η W , η Y ∼ N (0, 1). The corresponding true causal graph is G true in Figure</formula><p>Observation 1. If the output of a PC-like algorithm has no conflicts, the result can still be incoherent. However, even checking for incoherencies is not sufficient to find all erroneous results as the next example shows.</p><formula xml:id="formula_12">X Y Z W 2 1 1 -2 X Y Z W G true G out</formula><p>Example 3. We look at the following ground truth structural causal model:</p><formula xml:id="formula_13">X = η X , Y = 0.2 • X + η Y , Z = 0.2 • Y + η Z ,</formula><p>with independent, exogenous noise variables η X , η Y , η Z ∼ N (0, 1). The corresponding true causal graph is G true in Figure <ref type="figure" target="#fig_1">3</ref>. Note that the total causal effect of X on Z is only 0.04. Assuming no other sources of errors, fixing a sample size (e.g. 1000 samples), and choosing the classical PC algorithm as the PC-like method M, this leads to testing X and Z to be unconditionally independent (E4) in most simulations, see Section C in the Appendix. Note that the tuple (X, Z, {Y }) is not tested anymore as the edge between X and Z has already been removed. The only tuple tested conditionally independent is therefore</p><formula xml:id="formula_14">T ind = (X, Y, ∅).</formula><p>There is a distribution capturing these independencies and dependencies which is also Markovian to G out (F3), see Figure <ref type="figure" target="#fig_1">3</ref>. There are no orientation conflicts and the method has no internal incoherencies (G3). In this case, we have no chance to detect that in fact there was a finite sample error and out output is erroneous.</p><p>The example illustrates the following observation.</p><p>Observation 2. If the output of a PC-like algorithm has no conflicts and no incoherencies, the result can still be erroneous.</p><p>The consequence of Observation 2 is that without access to the ground truth or doing additional tests, we cannot in general detect erroneous results of PC-like methods. In other words: There can be output graphs that are coherent but still wrong -and there is no chance to detect it without further effort. We formalize this property with the following definition.</p><p>Definition 2. We call an erroneous result internallydetectable if it can be detected by only accessing the results of the conditional independence tests for all tuples that were tested by the PC-like method M, i.e. T ind ∪ T dep , as well as the output graph G out of M.</p><p>The following proposition yields the exact subset of results for which this is the case. All proofs of the following results can be found in Section A in the Appendix.</p><p>Proposition 1 (Distributional Characterization of Internally-Detectable Erroneous Results). Erroneous results of a method M are orientation-conflict-free and coherent with the CI test results T ind ∪ T dep (G3) if and only if there is a distribution that can represent the conditional independencies T ind and it is Markovian and faithful to G out (F3).</p><p>We formulate a corollary which stresses why after resolving conflicts and ambiguities, the results stay internallydetectable as incoherecies.</p><p>Corollary 1.1 (Resolved Conflicts and Ambiguities Imply Incoherencies). Resolving conflicts or ambiguities always leads to incoherencies.</p><p>Remark 1. For the sake of completeness, we note that resolution strategies can lead to a resolved output graph that is incoherent given the CI test results-reflecting that there is no graph of the graph type assumed by the algorithm that matches all CI test results-but still Markov equivalent to the unknown true causal graph G true . As a simple example, take an ambiguity resolution that ignores the one incorrect CI test result.</p><p>Proposition 2 (Graphical Characterization of Internally-Detectable Erroneous Results). The internally-detectable erroneous results of a PC-like method M is the union of results with marked conflicts or ambiguities and results with incoherencies.</p><p>Combining Corollary 1.1 and Proposition 2 yields that after resolving all conflicts and ambiguities in output graphs, the internally-detectable erroneous results are exactly the ones with incoherencies.</p><p>We have now stressed why detecting incoherencies is exhaustive to find internally-detectable erroneous results. Next, we show how to compute a score which not only detects them, but also quantifies different kinds of incoherencies which indicate in which way the result is flawed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QUANTIFYING INTERNAL COHERENCY</head><p>In this section, we quantify the overall incoherencies of a method by introducing three coherency scores. A high score represents a good performance. To define these scores, we distinguish two different kinds of incoherent tuples given the output graph and the conditional independence tests performed within the PC-like method of choice.</p><p>Definition 3. A Markov incoherent tuple (X, Y, S) is in C sep and in T dep , i.e. a tuple that has been tested dependent, but is separated in the output graph.</p><p>Definition 4. A faithfulness incoherent tuple (X, Y, S) is in C con and in T ind , i.e. a tuple that has been tested independent but is connected in the output graph.</p><p>Markov incoherent tuples may for instance appear because an edge has been deleted from a path connecting two nodes that were tested dependent. Conversely, a faithfulness incoherent tuple can be the result of an edge orientation that leaves a path open between nodes that were tested independent. As separations in the output graph are often considered more consequential for practitioners, Markov incoherencies may be particularly worrisome.</p><p>We now define the coherency score. Let M be a PC-like method using a test T with threshold α. Let w : T → R + be a non-trivial weight function.</p><p>Definition 5. Let M be a PC-like method using a test T with threshold α. Let w : T → [0, ∞) be a non-trivial weight function. The total coherency score is defined as </p><formula xml:id="formula_15">sc(w, M) = 1 - (X,Y,S)∈T w(X, Y, S)ι(X, Y, S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>For all following experiments, we choose M as the classical PC algorithm <ref type="bibr" target="#b28">[Spirtes et al., 1993]</ref>. We choose the conditional independence test T to be Fisher's Z test with threshold α = 0.05, and apply the methods to simulated as well as real-world data.</p><p>We first show how different assumption violations and finite sample errors affect the coherency scores on simulated data. We begin by evaluating the finite sample properties of common causal structures when only finite sample issues are present and continue with isolating different assumption violations to measure their impact. Eventually, we compute the score on the Auto MPG data set <ref type="bibr" target="#b15">[Quinlan, 1993]</ref> on which PC in the stated configuration is applied in the causal-learn repository <ref type="bibr" target="#b35">[Zheng et al., 2024]</ref>, and show in which ways the results are incoherent on this data set. </p><formula xml:id="formula_16">X = η X , Y = η Y , Z = X + Y + η Z W = Z + η W , V = Z + η V ,</formula><p>where η X , η Y , η Z , η V , η W ∼ N (0, 1) are independent, exogenous noise terms. Incoherencies on this toy model will only be caused by finite sample errors (E4) as the assumptions are fulfilled by design. To compute the average scores, we take the average of 100 repetitions. In Figure <ref type="figure" target="#fig_3">4</ref> we compare the average standard total (blue), faithfulness (orange), and Markov (green) coherency scores for SCMs of this form at different sample sizes (x axis, logarithmic scale).</p><p>We observe that the result has a high coherency score from a sample size of 300 and a very high coherency scores from a sample size of 1000. In Section C of the Appendix, we examine the same model with different effect strengths. We also include extended tables including the standard deviation of the scores and the average number of orientation conflicts. In addition, we computed the average scores for different sample and effect sizes for other causal structures that are frequently analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption violations</head><p>We generate data sets from the SCM in Example 2 and compute the average standard total, Markov and faithfulness coherency score in the presence of E1 errors. In almost all simulations, the detectable faithfulness violation leads to one faithfulness incoherent tuple (X, Y, ∅) and one Markov incoherent tuple (X, Y, {W }).</p><p>In contrast to the previous example, where increasing the sample size reduced sampling errors and incoherencies, the average score for this example, which contains an assumption violation, is almost constant across sample sizes (see Table <ref type="table" target="#tab_1">1</ref>). An extended version of the table including the standard deviation and more sample sizes is provided in Table <ref type="table" target="#tab_1">14</ref>. Additional simulations that illustrate incoherency analysis with the coherency score for several assumption violations can be found in Section C of the appendix. Auto MPG data set We apply PC in the described configuration to the well-known auto mpg data set <ref type="bibr" target="#b15">[Quinlan, 1993]</ref>. We find that there is an ambiguous triple mpg -horsepower -displacement. Horsepower is in some conditioning sets (of cardinality two)-leading to the test result that mpg and displacement are conditionally independent-but not all (of cardinality two). This yields an order-dependence, as the result changes according to which conditioning set of cardinality two is tested first. We resolve this ambiguity in both possible ways-orienting the unshielded triple as a collider or not during the collider orientation phase-and compute the scores for both resulting graphs. The two resulting graphs are shown in Figure <ref type="figure" target="#fig_4">5</ref> of the appendix, and the scores are shown in Table <ref type="table" target="#tab_3">3</ref> of the appendix and repeated here in Table <ref type="table" target="#tab_2">2</ref> in a compact form.</p><p>The standard total coherency score is 0.917 (rounded to 3 decimal places) for both resulting graphs. However, computing the other scores, we gain further insight: If we orient the triple as a collider we get more faithfulness incoherent tuples, and if we do not orient the triple as a collider we get more Markov incoherent tuples. More importantly, we have discovered at all that the output graph is not fully coherent with the CI test results even though there are no orientation conflicts, and we can further investigate which tuples are problematic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND OUTLOOK</head><p>We have suggested a coherency score given the CI test results and the output graph of a PC-like method and have proven that it can detect all erroneous results that are detectable without further information. We suggest including the total, Markov, and faithfulness coherency scores in implementations of PC-like methods, such as the different versions of PC, FCI or PCMCI+, to provide the user with a measure of the internal coherency of the applied method.</p><p>The score may then be used for hyperparameter, CI test, and even method selection, e.g. by testing if the coherency score improves when using a nonlinear instead of a linear CI test or an algorithm that imposes weaker assumptions, e.g. that allows for hidden confounding and selection bias.</p><p>Auto MPG. As described in Section 5, the variables mpg and displacement show an ambiguity, in particular, they are tested independent given the three sets [{acceleration, weight}, {horsepower, weight}, {cylinders, weight}]. Depending on which conditioning set is tested first, horsepower is or is not in the conditioning set for which mpg and displacement are tested independent. As mpg -horsepower -displacement is an unshielded triple in the skeleton, it changes the output graph which one is tested first. This is an ambiguity which is labeled by methods like conservative or resolved by rules like the majority rule. We resolved the ambiguity in both ways possible, the output graphs are shown in Figure <ref type="figure" target="#fig_4">5</ref>. Table <ref type="table" target="#tab_3">3</ref> shows an extended version of the score results for the results stated in the main paper. The experiments show that even in this model example, the weighted Markov score barely reaches the 90% mark pointing to worrysome Markov incoherencies. mpg weight acceleration horsepower displacement cylinders (a) Graph if we lay more emphasis on the test that says that horsepower is in the conditioning set yielding that mpg and displacement are independent and do not orient the unshielded triple as a collider. mpg weight acceleration horsepower displacement cylinders (b) Graph if we lay more emphasis on the test that says that horsepower is not in the conditioning set yielding that mpg and displacement are independent and do orient the unshielded triple as a collider.  This provides a foundation for robustness tests and finite sample analysis. We start with one of the simplest possible models. We round the results and their standard deviation (in brackets) to three decimal places.  Model 1 (Mediated Effect (three nodes)). Consider the following SCM for a constant c ∈ R:</p><formula xml:id="formula_17">X = η X , Y = cX + η Y , Z = cY + η Z ,</formula><p>where η X , η y , η Z ∼ N(0, 1) are independent exogenous noise variables. This is the model for a simple mediated effect from X to Z. We now simulate data from this SCM for different effect strengths c, run the PC algorithm and compute the scores (in particular, the standard total, faithfulness and Markov coherency score) for the results. We do so for different sample sizes and average over 100 repetitions for each sample size. The resulting graphs are shown in Figure <ref type="figure" target="#fig_6">6</ref>. The average scores are shown in Table <ref type="table">4</ref> for c = 0.2 (most simulations yield the graph in Figure <ref type="figure" target="#fig_6">6</ref> (d) as the output graph), Table <ref type="table">5</ref> (most simulations yield the graph in Figure <ref type="figure" target="#fig_6">6</ref> (b) as the output graph) and Table <ref type="table" target="#tab_5">9</ref> (simulations with smaller samples often yield the graph in Figure <ref type="figure" target="#fig_6">6</ref> (c) as the output, with increasing sample size (b) becomes predominant).</p><p>Model 2 (Mediated Effect (four nodes)). Consider the following SCM for a constant c ∈ R and one more node than in Model 1:</p><formula xml:id="formula_18">X = η X , Y = cX + η Y , Z = cY + η Z , W = cW + η W ,</formula><p>where η X , η y , η Z , η W ∼ N(0, 1) are independent exogenous noise variables. This is the model for a mediated effect from X to W with two mediators. Again, we simulate data from this SCM for different effect strengths c, run the PC algorithm and compute the scores (in particular, the standard total, faithfulness and Markov coherency score) for the results. We do so for different sample sizes and average over 100 repetitions for each sample size. The tables show while a very small effect strength of c = 0.2 leads to a coherent erroneous result for three nodes, see Figure <ref type="figure" target="#fig_6">6</ref> (d), the erroneous result becomes detectable for four nodes. </p><formula xml:id="formula_19">U 1 = η U1 , U 2 = η U2 , U 3 = η U3 , U 4 = η U4 X = U 1 + U 2 + η X , Y = U 2 + U 3 + η Y , Z = U 3 + U 4 + η Z , V = U 4 + U 1 + η V .</formula><p>where η X , η y , η Z , η V , η U1 , η U2 , η U3 , η U4 ∼ N(0, 1) are independent exogenous noise variables and U 1 , U 2 , U 3 , U 4 unobserved. We run the PC algorithm on the observed variables and note that there is a structural assumption violation (error code E2), in particular, a causal insufficiency. We note that for the detectable faithfulness violation discussed in Section 5, the score is almost constant between sample sizes and detects the incoherency. For an undetectable causal insufficiency, see Example 7.</p><p>Model 4 (Classical Five Node Toy Model). Consider the following SCM:</p><formula xml:id="formula_20">X = η X , Y = η Y , Z = cX + cY + η Z W = cZ + η W , V = cZ + η V ,</formula><p>where η X , η Y , η Z , η V , η W ∼ N (0, 1) are independent, exogenous noise terms.</p><p>Last, we want to give the full version of the table in the main paper corresponding to the model described in Example 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL EXAMPLES</head><p>We first present an example that show how the assumptions of additivity in a nonlinear regression-based CI test may not be in line with the assumption of additivity in the underlying data generating mechanism.</p><p>Example 4. Consider the following additive noise data generating mechanism</p><formula xml:id="formula_21">Z 1 = η Z1 , Z 2 = η Z2 , Z 3 = η Z3 X = Z 1 • Z 2 + η X , Y = Z 2 • Z 3 + η Y ,</formula><p>with independent, exogenous noise termsη Z1 , η Z2 , η Z3 , η X , η Y . The corresponding causal graph is shown in Figure <ref type="figure">8</ref>. Since every structural equation above is expressed in the form X = f (S) + η X , it is tempting to use a test T that evaluates X |= Y |S by first regressing X and Y on S and then testing for independence of the residuals (for instance, GPDC or HSIC). But this example shows that even when S * is an additive noise model, the additive noise assumption does not hold for every tuple (X, Y, S) ∈ T examined in a run of M.</p><p>In the graph above, we can see that X ▷◁ G Y |Z 2 . However, while X (and similarly Y ) can be expressed as a function of Z 1 (Z 3 ) and Z 2 with purely additive noise, neither can be expressed as a function of solely Z 2 with purely additive noise: for X, the parent Z 1 -which is not in the conditioning set-acts as a multiplicative noise term in the regression of X on Z 2 . Therefore, even in the infinite sample limit there is no guarantee that a regression-based test will correctly identify the conditional independence X |= Y |Z 2 if the test relies on an assumption of additive noise. When running a method M that employs the test T , every causal and non-causal regression executed within M must satisfy those assumptions -not just the structural equations in the underlying data-generating mechanism.</p><p>The next example shows that what we call an incoherency is not the same as what is called an inconsistency in <ref type="bibr" target="#b12">[Li et al., 2019]</ref>.</p><p>Example 5. Consider four variables W, X, Y, Z and assume that the PC-algorithm measures the following during its execution: first, all variables will be judged pairwise dependent (tests with |S| = 0). In the next step, tests with |S| = 1 yield</p><formula xml:id="formula_22">W ̸ |= (T,α) Z|X, W |= (T,α) Z|Y, W |= (T,α) Y |X, X |= (T,α) Z|Y.</formula><p>All remaining tests with |S| = 2 yield dependence. PC will output the unoriented chain W -X -Y -Z, on which W and Z are d-separated by X, even though W ̸ |= (T,α) Z|X was measured. Every separating set lies on a path connecting the two variables it separates, i.e. all separating sets are consistent in the sense of <ref type="bibr" target="#b12">[Li et al., 2019]</ref>.</p><p>The next example shows that a faithfulness violation can lead to an undetectable erroneous result. It is another possible example to illustrate Observation 2.    Example 6. We consider the following ground truth structural causal model:</p><formula xml:id="formula_23">X = η X , Z = X + η Z , Y = 2X -2Z + η Y ,</formula><p>with independent, exogenous noise variables η X , η Z , η Y ∼ N (0, 1). The corresponding true causal graph is G true in Figure <ref type="figure">9</ref>. Note that there is a perfect counteractive mechanism: the direct positive effect of X on Y plus the mediated negative effect of X on Y via Z cancel out, i.e. a faithfulness violation (E1). Assuming no other sources of errors, and choosing the classical PC algorithm as the PC-like method M, this leads to testing X and Y to be unconditionally independent and all other tuples to be dependent, i.e. T ind = (X, Y, ∅). There is a distribution capturing these independencies and dependencies which is also Markovian to G out (F3), see Figure <ref type="figure">9</ref>. There are no orientation conflicts and the method has no internal incoherencies (G3). In this case, we have no chance to detect that in fact there was a faithfulness violation and out output is erroneous.</p><p>Example 7. We want to stress that there can also be undetectable causal insufficiencies. Consider the following SCM:</p><formula xml:id="formula_24">U 1 = η U1 , U 2 = η U2 , U 3 = η U3 , X = U 1 + U 2 + η X , Y = U 2 + U 3 + η Y , Z = U 3 + U 1 + η Z .</formula><p>where η X , η y , η Z , η U1 , η U2 , η U3 ∼ N(0, 1) are independent exogenous noise variables and U 1 , U 2 , U 3 unobserved. The resulting graphs are shown in Figure <ref type="figure">10</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Faithfulness violation leading to an incoherent erroneous result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of the true causal graph and an undetectable erroneous graph structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>X,Y,S)∈T w(X, Y, S) , where ι(X, Y, S) = |ι G (X, Y, S) -ι (T,α) (X, Y, S)|. Different weight functions define different scores, see Appendix B. Some natural weight function choices are: • Standard Total Coherency Score: For w(X, Y, S) = 1, the coherency score simplifies to the total coherency rates. • Faithfulness Coherency Score: Setting w(X, Y, S) = 0 for (X, Y, S) ∈ T dep yields the faithfulness coherency score. (Setting w(X, Y, S) = 1 for all other tuples, i.e. (X, Y, S) ∈ T ind , yields the standard faithfulness coherency score.) • Markov Coherency Score: Setting w(X, Y, S) = 0 for (X, Y, S) ∈ C con yields the Markov coherency score. (Setting w(X, Y, S) = 1 for all other tuples, i.e. (X, Y, S) ∈ C sep , yields the standard Markov coherency score.) • Conditioning-Set-Size-Adjusted Score: Choosing w(X, Y, S) = exp(-|S|) is a strictly decreasing weight function of the cardinality of the conditioning set and places more weight on incoherencies that occur for small conditioning sets. The faithfulness and Markov coherency score and the conditioning-set-size-adjusted score can be combined. Further weight functions and combinations are explicitely written down in the Appendix in Section B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Small sample analysis for classical toy model</figDesc><graphic coords="7,324.19,508.78,198.90,175.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of output graphs of PC on auto mpg data after ambiguity resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>result: Gout is Markov equivalent to Gtrue. Markov incoherent tuples in Gout due to large effects, e.g. (X, Z, ∅). Undetectable erroneous result due to effects much smaller than one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Three output graphs after performing PC on data generated by the SCM in (a) with different effect sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Model 3 (Causal Insufficiency). Consider the following SCM:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Orientation conflicts on all edges in Gout after applying PC. The graph Gout after applying a conflict resolution strategy: faithfulness incoherent tuple (Y, V, ∅).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The graph Gout after applying another conflict resolution strategy: faithfulness incoherent tuple (X, Z, ∅).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of outputs of PC on causally insufficient data with and without conflict resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Figure8: The causal graph G true of the data generating SCM of Example 4 which is correctly discovered by the PC-like method in the infinite sample limit. However, the tuple (X, Y, {Z 2 }) is not guaranteed to be in T ind even though clearly (X, Y, {Z 2 }) ∈ C sep .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Simulations for the SCM in Example 2</figDesc><table><row><cell></cell><cell>10 1</cell><cell>10 2</cell><cell>10 3</cell></row><row><cell>Total</cell><cell cols="3">0.879 0.876 0.884</cell></row><row><cell>Faithfulness</cell><cell cols="3">0.938 0.936 0.939</cell></row><row><cell>Markov</cell><cell cols="3">0.941 0.940 0.945</cell></row><row><cell cols="2">Orientation Conflicts 0.01</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Coherency scores for PC on auto MPG data with ambiguity resolution as a collider and as a non-collider.</figDesc><table><row><cell></cell><cell cols="2">Collider Non-collider</cell></row><row><cell>Total (Standard)</cell><cell>0.9174</cell><cell>0.9174</cell></row><row><cell>Markov (Standard)</cell><cell>0.9504</cell><cell>0.9669</cell></row><row><cell>Faithfulness (Standard)</cell><cell>0.9669</cell><cell>0.9504</cell></row><row><cell>Total (Weighted)</cell><cell>0.8895</cell><cell>0.9004</cell></row><row><cell>Markov (Weighted)</cell><cell>0.9185</cell><cell>0.9358</cell></row><row><cell>Faithfulness (Weighted)</cell><cell>0.9709</cell><cell>0.9646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Resolve as collider Resolve as non-collider Coherency scores for PC on auto mpg data with the two possible ambiguity resolutions.Toy Models. It is interesting to analyze small sample properties for different configurations of well-known causal structures. It can show us from which sample size the true causal graph or a coherent but erroneous result is discovered. In combination with comparing the output graphs with the known ground truth--the causal graph of SCM generating the data--we can evaluate which causal structures are correctly discovered and from which sample size the result is almost always correct.</figDesc><table><row><cell>Standard Total Coherency Score</cell><cell>0.9174</cell><cell>0.9174</cell></row><row><cell>Standard Markov Coherency Score</cell><cell>0.9504</cell><cell>0.9669</cell></row><row><cell>Standard Faithfulness Coherency Score</cell><cell>0.9669</cell><cell>0.9504</cell></row><row><cell>Conditioning Set Size Adjusted Total Coherency Score</cell><cell>0.8895</cell><cell>0.9004</cell></row><row><cell>Conditioning Set Size Adjusted Markov Coherency Score</cell><cell>0.9185</cell><cell>0.9358</cell></row><row><cell>Conditioning Set Size Adjusted Faithfulness Coherency Score</cell><cell>0.9709</cell><cell>0.9646</cell></row><row><cell>Number of Orientation Conflicts</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Mediated Effect from Model 2, c = 0.2</figDesc><table><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="4">0.988 (0.051) 0.976 (0.061) 0.906 (0.079) 0.923 (0.028)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.993 (0.033) 0.98 (0.054) 0.906 (0.079) 0.924 (0.025)</cell></row><row><cell>Markov</cell><cell cols="2">0.994 (0.04) 0.996 (0.031)</cell><cell>1.0 (0.0)</cell><cell>0.999 (0.011)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.04</cell><cell>0.15</cell><cell>0.7</cell><cell>0.02</cell></row><row><cell cols="4">Table 8: Mediated Effect from Model 2, c = 1</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="4">0.922 (0.098) 0.966 (0.069) 0.986 (0.035) 0.984 (0.039)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.966 (0.051) 0.984 (0.043) 0.996 (0.024) 0.99 (0.034)</cell></row><row><cell>Markov</cell><cell cols="4">0.957 (0.058) 0.982 (0.037) 0.99 (0.025) 0.994 (0.019)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.01</cell><cell>0.0</cell><cell>0.0</cell><cell>0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 :</head><label>9</label><figDesc>Mediated Effect from Model 2, c = 10 The causal graph Gtrue of the data generating SCM.</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell></cell><cell cols="4">0.637 (0.08) 0.642 (0.083) 0.774 (0.068) 0.805 (0.071)</cell></row><row><cell cols="2">Faithfulness</cell><cell>1.0 (0.0)</cell><cell cols="3">0.997 (0.019) 0.916 (0.05) 0.901 (0.041)</cell></row><row><cell>Markov</cell><cell></cell><cell cols="4">0.637 (0.08) 0.645 (0.09) 0.858 (0.089) 0.905 (0.036)</cell></row><row><cell cols="2">Orientation Conflicts</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Y</cell><cell>Z</cell><cell></cell><cell></cell><cell></cell></row><row><cell>X</cell><cell>V</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 :</head><label>10</label><figDesc>Causal Insufficiency Model 3</figDesc><table><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="4">0.891 (0.084) 0.846 (0.044) 0.843 (0.033) 0.849 (0.039)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.896 (0.08) 0.846 (0.044) 0.843 (0.033) 0.849 (0.039)</cell></row><row><cell>Markov</cell><cell>0.995 (0.037)</cell><cell>1.0 (0.0)</cell><cell>1.0 (0.0)</cell><cell>1.0 (0.0)</cell></row><row><cell>Orientation Conflicts</cell><cell>1.27</cell><cell>3.14</cell><cell>3.58</cell><cell>3.44</cell></row><row><cell cols="4">Table 11: Five Node Model (Model 4) c = 0.2</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="4">0.981 (0.054) 0.959 (0.055) 0.938 (0.053) 0.99 (0.021)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.992 (0.026) 0.988 (0.03) 0.969 (0.027) 0.991 (0.018)</cell></row><row><cell>Markov</cell><cell cols="4">0.988 (0.047) 0.971 (0.052) 0.969 (0.027) 0.999 (0.006)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.1</cell><cell>0.19</cell><cell>0.0</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 12 :</head><label>12</label><figDesc>Five Node Model (Model 4) c = 1</figDesc><table><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="4">0.724 (0.079) 0.789 (0.094) 0.99 (0.023) 0.992 (0.018)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.947 (0.043) 0.937 (0.034) 0.99 (0.023) 0.992 (0.018)</cell></row><row><cell>Markov</cell><cell cols="2">0.777 (0.078) 0.852 (0.093)</cell><cell>1.0 (0.0)</cell><cell>1.0 (0.0)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.07</cell><cell>0.04</cell><cell>0.0</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13 :</head><label>13</label><figDesc>Five Node Model (Model 4) c = 10</figDesc><table><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell cols="3">0.79 (0.044) 0.792 (0.036) 0.788 (0.042)</cell><cell>0.78 (0.04)</cell></row><row><cell>Faithfulness</cell><cell>1.0 (0.002)</cell><cell>1.0 (0.002)</cell><cell>1.0 (0.0)</cell><cell>0.996 (0.015)</cell></row><row><cell>Markov</cell><cell cols="4">0.79 (0.043) 0.792 (0.036) 0.788 (0.042) 0.784 (0.035)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.0</cell><cell>0.01</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="4">Table 14: Faithfulness Violation from Example 2</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Total</cell><cell>0.9 (0.056)</cell><cell cols="3">0.879 (0.038) 0.876 (0.03) 0.884 (0.041)</cell></row><row><cell>Faithfulness</cell><cell cols="4">0.945 (0.033) 0.938 (0.019) 0.936 (0.013) 0.939 (0.019)</cell></row><row><cell>Markov</cell><cell cols="2">0.954 (0.06) 0.941 (0.033)</cell><cell>0.94 (0.02)</cell><cell>0.945 (0.025)</cell></row><row><cell>Orientation Conflicts</cell><cell>0.08</cell><cell>0.01</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Z 1</cell><cell></cell><cell>Z 2</cell><cell>Z 3</cell><cell></cell></row><row><cell></cell><cell>X</cell><cell>Z</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>RH, JR and JW received funding from the <rs type="funder">European Research Council (ERC)</rs> <rs type="grantName">Starting Grant CausalEarth</rs> (Grant Agreement No. <rs type="grantNumber">948112</rs>). JW also received funding from the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> as part of the project <rs type="projectName">MAC-MERLin</rs> (Grant Agreement No. <rs type="grantNumber">01IW24007</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KjjapAQ">
					<idno type="grant-number">948112</idno>
					<orgName type="grant-name">Starting Grant CausalEarth</orgName>
				</org>
				<org type="funded-project" xml:id="_sztB7WU">
					<idno type="grant-number">01IW24007</idno>
					<orgName type="project" subtype="full">MAC-MERLin</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. F1 to F3 and G1 to G3 each partition the set of erroneous results. We prove that F3 = G3.</p><p>⊆: F3 is ensured to be a subset of G3 by the soundness of the PC-like algorithm: If given the CI test results, a distribution exists that is Markovian and faithful to G out , then the equivalence class of the graph will correctly be found by the PC-like algorithm, in particular there will be no conflicts, ambiguities or incoherencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⊇:</head><p>We prove that G3 is a subset of F3 by showing that F1 ∩ G3 = ∅ and F2 ∩ G3 = ∅.</p><p>F1 ∩ G3 = ∅: F1 states that no distribution can represent the CI test results T ind ∪ T dep . The equivalence class represented by G out represents a set of separation statements C sep out such that a corresponding set of conditional independence statements that can be represented by a family of distributions. As the CI test results cannot be represented by a distribution, this one-to-one correspondence must be violated for at least one tuple (X, Y, S), i.e.</p><p>F2 ∩ G3 = ∅: F2 states that the distribution that represents the conditional independencies T ind is not Markovian to any graph in the equivalence class represented by G out . By definition, that means for at least one tuple (X, Y, S),</p><p>Together, F3 = G3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Corollary 1.1</head><p>Proof. Proposition 1 shows that if there are marked conflicts or ambiguities, there is no graph of the graph type assumed by the algorithm that can represent the CI test results. Therefore, resolving all conflicts and ambiguities leads to incoherent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 2</head><p>Proof. Given G out and T ind ∪ T dep , we have to show that the subset of erroneous results that are internally detectable is G1 ∪ G2.</p><p>⊆: Proposition 1 shows that a result is in G3 if and only if there is a distribution that represents T ind and is Markovian and faithful to G out . In that case, the PC-like algorithm is deterministic, orientation-conflict-free and coherent by its soundness proven in <ref type="bibr" target="#b28">Spirtes et al. [1993]</ref>. Therefore, a detectable erroneous result always lies in G1 ∪ G2.</p><p>⊇: Marked orientation conflicts and ambiguities are detectable as part of G out . Incoherencies are detectable as for at least one tuple (X, Y, S) with X, Y ∈ X, S ⊆ X:</p><p>Together, among the erroneous results, G1 ∪ G2 is exactly the subset that is internally-detectable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SCORES FOR DIFFERENT WEIGHT FUNCTIONS</head><p>We explicitly formulate the scores for different weight functions used in the paper and introduce additional interesting weight functions.</p><p>• Faithfulness Coherency Score. Choosing w(X, Y, S) = 0 for (X, Y, S) ∈ T dep yields the faithfulness coherency score:</p><p>,</p><p>• Markov Coherency Score. Setting w(X, Y, S) = 0 for (X, Y, S) ∈ C con yields the Markov coherency score:</p><p>,</p><p>• Standard Faithfulness Coherency Score. The weight function</p><p>yields the Standard Faithfulness Coherency Score:</p><p>• Standard Markov Coherency Score. The weight function</p><p>yields the standard Markov coherency score:</p><p>• Conditioning Set Size Adjusted Total Score. The conditioning set size adjusted total score is:</p><p>• Conditioning Set Size Adjusted Faithfulness Coherency Score. The conditioning set size adjusted faithfulness coherency score is:</p><p>• Conditioning Set Size Adjusted Markov Coherency Score. The conditioning set size adjusted Markov coherency score is:</p><p>, Other weight functions that can be used to further analyze the incoherencies taht were not mentioned in the main part of the paper:</p><p>• Path Length Adjusted Faithfulness Coherency Score. We can compute a path length adjusted faithfulness coherency score by choosing</p><p>where d(X, Y ) denotes the length of the shortest collider free path between X and Y . This accounts for the fact that we want to place less weight on incoherencies of tuples that are connected by a long path on which small direct effects could lead to a very low total effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL RESULTS</head><p>The code to reproduce all results in this article can be found here: <ref type="url" target="https://github.com/this-is-sofia/pc-coherency">https://github.com/this-is-sofia/pc-coherency</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey and evaluation of causal discovery methods for time series</title>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Charles K Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="767" to="819" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Foundations of structural causal models with cycles and latent variables</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2885" to="2915" />
			<date type="published" when="2021">2021</date>
			<publisher>Publisher: Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The landscape of causal discovery data: Grounding causal discovery in real-world applications</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01953</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Bayesian approach to constraint based causal inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI&apos;12</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI&apos;12<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Order-Independent Constraint-Based Causal Structure Learning</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="3921" to="3962" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Toward falsifying causal graphs using a permutation-based test</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Eulig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atalanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mastakouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Blöbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><surname>Janzing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09565</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-Compatibility: Evaluating Causal Discovery without Ground Truth</title>
		<author>
			<persName><forename type="first">M</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leena</forename><forename type="middle">Chennuru</forename><surname>Faller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atalanti</forename><forename type="middle">A</forename><surname>Vankadara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Mastakouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><surname>Janzing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09552</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adjustment identification distance: A gadjid for causal structure learning</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Henckel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Würtzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Weichwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 40th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models with latent variables: A general sat-based procedure</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Ederhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery: Conflict resolution with answer set programming</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovery of causal models that contain latent variables through bayesian scoring of independence constraints</title>
		<author>
			<persName><forename type="first">Fattaneh</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017</title>
		<meeting><address><addrLine>Skopje, Macedonia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 18-22, 2017. 2017</date>
			<biblScope unit="page" from="142" to="157" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 10</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the pcalgorithm</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constraint-based causal structure learning with consistent separating sets</title>
		<author>
			<persName><forename type="first">Honghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A cautious approach to constraint-based causal model selection</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.18232</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A complete generalized adjustment criterion</title>
		<author>
			<persName><forename type="first">Emilija</forename><surname>Perković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Textor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence-Proceedings of the Thirty-First Conference</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Quinlan</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5859H</idno>
		<ptr target="https://doi.org/10.24432/C5859H" />
		<title level="m">Auto MPG. UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving accuracy and scalability of the pc algorithm by maximizing p-value</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00378</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adjacencyfaithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06</title>
		<meeting>the Twenty-Second Conference on Uncertainty in Artificial Intelligence, UAI&apos;06</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Choosing dag models using markov and minimal edge count in the absence of ground truth</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Joseph D Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><surname>Spirtes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.20187</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy to Game</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Reisach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Weichwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27772" to="27784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="938" to="947" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring causation from time series in earth system sciences</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bathiany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bollt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dim</forename><surname>Coumou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">D</forename><surname>Mahecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Muñoz-Marí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2553</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>American Association for the Advancement of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Schkoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Faller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.05625</idno>
		<title level="m">Patrick Blöbaum, and Dominik Janzing. Cross-validating causal discovery via leave-one-variable-out</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The hardness of conditional independence testing and the generalised covariance measure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rajen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1514" to="1538" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An anytime algorithm for causal inference</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science computer review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Statistics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<date type="published" when="1993">1993</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Separation-based distance measures for causal graphs</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.04952" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on causal discovery: theory and practice</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Zanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Ozkirimli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Stella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="101" to="129" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal reasoning with ancestral graphs</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1437" to="1474" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernelbased conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal-learn: Causal discovery in python</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
