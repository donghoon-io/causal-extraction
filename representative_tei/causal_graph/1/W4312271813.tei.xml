<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Inference for Knowledge Graph based Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaoyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<title level="a" type="main">Causal Inference for Knowledge Graph based Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender System</term>
					<term>Knowledge Graph</term>
					<term>Causal Inference</term>
					<term>Counterfactual Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graph (KG), as a side-information, tends to be utilized to supplement the collaborative filtering (CF) based recommendation model. By mapping items with the entities in KGs, prior studies mostly extract the knowledge information from the KGs and inject it into the representations of users and items. Despite their remarkable performance, they fail to model the user preference on attribute in the KG, since they ignore that (1) the structure information of KG may hinder the user preference learning, and (2) the user's interacted attributes will result in the bias issue on the similarity scores. With the help of causality tools, we construct the causal-effect relation between the variables in KG-based recommendation and identify the reasons causing the mentioned challenges. Accordingly, we develop a new framework, termed Knowledge Graph-based Causal Recommendation (KGCR), which implements the deconfounded user preference learning and adopts counterfactual inference to eliminate bias in the similarity scoring. Ultimately, we evaluate our proposed model on three datasets, including Amazon-book, LastFM, and Yelp2018 datasets. By conducting extensive experiments on the datasets, we demonstrate that KGCR outperforms several state-of-the-art baselines, such as KGNN-LS [1], KGAT [2]  and KGIN [3]. We release our codes in the public domain: <ref type="url" target="https://github.com/weiyinwei/KGCR">https://github.com/weiyinwei/KGCR</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graph (KG), as an external knowledge, has been witnessed in improving the collaborative filtering (CF) based recommender system. Typically, KG is a directed graph consisting of real-world facts, where the nodes function as the entities and the edges reflect the relations between the entities <ref type="bibr" target="#b3">[4]</ref>. By mapping the items to entities in the KG, it is able to offer the external knowledge to CF-based models <ref type="bibr" target="#b4">[5]</ref>. For the sake of description, we term the entities corresponding to items as item entities and the other entities for depicting the items in KGs as the attribute entities.</p><p>By investigating the existing KG-based recommendation models, we roughly divide the prior studies into three groups <ref type="bibr" target="#b5">[6]</ref>: Embedding-based models which conduct the knowledge graph embedding (KGE) algorithms on KGs, and then enrich the CF-based user and item representations by incorporating the embeddings of item entities <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>; Path-based models that leverage the similarity of connectivity patterns between the item entities in KGs, in order to provide the external cues to enhance the collaborative signal <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>; and GCN-based models which explicitly pass the knowledge information to the item and</p><p>• Yinwei Wei and Tat-Seng Chua are with School of Computing, National University of Singapore, Singapore. (E-mail: weiyinwei@hotmail.com; chuats@comp.nus.edu.sg) • Xiang Wang is with School of Information Science and Technology, University of Science and Technology of China, China. (E-mail: xiangwang1223@gmail.com) • Liqiang Nie and Shaoyu Li are with Shandong University, China. (E-mail: nieliqiang@gmail.com; lssy97@163.com) • D. Wang is with Ranking team, Search Science Department, eBay Inc, China. (E-mail: diwang@ebay.com) • Liqiang Nie is the corresponding author. user embeddings upon user-item graphs and KGs in an end-to-end manner <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Accordingly, we can find that they mostly utilize the knowledge information in KGs through the item entity and propagate it along the user-item interactions.</p><p>Despite their promising results, we argue that these methods fail to model the fine-grained user preference on attribute entities in the KG. In the real scenario, people are prone to select the items for some particular attributes, e.g., someone may watch a movie for its director or buy shoes due to its brand. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, User's preference on the attributes Fiction, Michael Biehn, and James Cameron which she/he interacted through the movie Aliens II heavily affects the recommendation of Clockstoppers and T itanic. It seems that the exiting models, such as KGAT <ref type="bibr" target="#b1">[2]</ref> and KGIN <ref type="bibr" target="#b2">[3]</ref>, can learn the fine-grained user preference from her/his interacted attributes (short for the attributes that user interacted through items) and score the similarity between the user and item representations <ref type="bibr" target="#b10">[11]</ref>. However, they suffer from the following two challenges: is because the information decides which attributes associate with items and how the attributes (e.g., Fiction, Michael Biehn, and James Cameron) are entangled by the items (e.g., Aliens II). Merely depending on the user-item interactions, it is hard to distinguish the user preferences to these different attributes, inevitably affecting the user preference learning on attributes. Therefore, how to utilize the user-item interactions to learn the user preference on attribute is the first challenge we are facing.</p><p>• The user's interacted attributes will result in the bias issue on the similarity scores, which may offer inaccurate supervision signal during the training phase. Specifically, the items with more attributes exposed in users' interacted attributes are more likely to achieve the higher scores <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the same sample shown in Figure <ref type="figure" target="#fig_0">1</ref>, the Fiction movie Clockstoppers acted by Michael Biehn may score higher than Titanic directed by James Cameron. Because the attributes provide more shortcuts (denoted as blue curves) to Clockstoppers, which bypasses the user preference to James Cameron (denoted as red curve). Thus, how to eliminate the bias from the similarity scores is the other key challenge.</p><p>To address the challenges, we resort to the language of causal inference <ref type="bibr" target="#b12">[13]</ref> to explore the reasons causing the limitations. Specifically, we construct a causal graph to model the causal-effect factors of the similarity scoring, as shown in Figure <ref type="figure" target="#fig_1">2</ref>(a). We first consider how the structure information (K) affects the user preference on attribute (U ). In the causal graph, K → A illustrates that the structure information decides the user's interacted attributes A, and K → U reflects the entanglement of attributes observed by user. Obviously, K, as a confounder, opens up a back-door path U ← K → A, giving rise to the spurious relation between the user preference and interacted attributes. To reduce its negative influence, we conduct the intervention operation on the causal graph and devise a Knowledge Graph-based Causal Recommendation (KGCR) model, which implements the deconfounded user preference learning and models the user-item similarity score. As for the bias issue in similarity scoring, we attribute it to the direct effect of A on S. The interacted attribute (A) provides a shortcut for the prediction of user-item interaction, bypassing the user preference on attribute U . Hence, we adopt the counterfactual inference to eliminate the bias caused by A → S. In particular, we estimate total effects (TE) on the similarity score in the factual world and natural direct effect (NDE) of the interacted attributes on the score in the counterfactual world. Thereafter, we can remove NDE from TE during the inference phase to answer the question: what the score would be, if we only obtain the user and item representations based on the attributes.</p><p>To evaluate our proposed model, we conduct extensive experiments on three publicly accessible datasets, including Amazon-book, LastFM, and Yelp2018 datasets. The results show that our proposed model outperforms several stateof-the-art baselines, such as KGNN-LS <ref type="bibr" target="#b0">[1]</ref>, KGAT <ref type="bibr" target="#b1">[2]</ref> and KGIN <ref type="bibr" target="#b2">[3]</ref>. Furthermore, we do ablation studies to verify the effectiveness and rationality of our constructed causal graph and implementations in KGCR.</p><p>In a nutshell, the main contributions of this work are threefold: set of the positive and negative user-item pairs. y ui prediction of interaction between user u and item i.</p><p>• By investigating the existing KG-based recommendation models, we identify two key challenges in modeling the user preference on attribute in KGs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>Given the user-item interactions in the history, CF-based recommendation model is designed to represent the users and items with the latent vectors, and predict their interactions according to the similarity between their representations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Formally, let U and I separately be the sets of O users and M items. The CF-based recommendation model can be formulated as:</p><formula xml:id="formula_0">S cf ui = f cf (u cf , i cf ),<label>(1)</label></formula><p>where u cf and i cf are the representations of user u ∈ U and item i ∈ I, respectively. And, S cf ui is the predicted score of interaction between u and i, measuring how likely item i will be interacted by user u based on CF signal. In addition, the score function f cf (•) can be implemented with the inner product <ref type="bibr" target="#b15">[16]</ref> or neural networks <ref type="bibr" target="#b16">[17]</ref>.</p><p>KG, as a side-information, can be introduced into the CFbased models <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> to supplement the interaction information. It is a heterogeneous graph consisted by the triples (e.g., Titanic-directed by-James Cameron). Formally, KG can be represented as G kg = {(h, r, t)|h, t ∈ E, r ∈ R}, where E and R are the sets of entities and relations in the KG, respectively. Each triplet describes that there is a relationship r from head entity h to tail entity t. By mapping the items into the entities in the KG, the external knowledge learned from the KG could be injected into the representations of users and items. We list the important symbols in Table <ref type="table" target="#tab_0">1</ref> to clarify their descriptions.</p><p>In this work, we explore to learn the user and item representations based on the attributes and estimate the user-item similarity S u,i,a at the fine-grained level, formally,</p><formula xml:id="formula_1">S u,i,a = f (u, i), where u = U (O + u , G kg ).<label>(2)</label></formula><p>Wherein, f () and U () are the similarity scoring and user preference modeling functions, respectively. And, the O + u is the historical interactions of user u and G kg denotes the knowledge graph. The task can be formulated as:</p><p>• Input: the set of user-item interactions O + = {(u, i)|u ∈ U, i ∈ I} and knowledge graph G kg . • Output: the user-item interaction prediction y ui by integrating the user-item CF score S cf ui and attribute-based similarity score S u,i,a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Graph Construction and Analysis</head><p>Causal graph is a directed acyclic graph, which offers us a tool to illustrate the causal-effect relation between the variables <ref type="bibr" target="#b20">[21]</ref>. In the causal graph, the nodes are used to represent the variables and the edges reflect the causal relation between them. In the following, we first introduce the causal graph constructed for our task. Then, we describe its nodes (variables) and edges (relations) in the causal-effect language and present the motivations of our designs.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>(a), we construct the causal graph consisting of five variables: U, I, A, K, S. Noted that we omit the CF signal in this causal graph to emphasize the generation of user preference on attribute and the attributebased similarity score. In particular,</p><p>• U represents the user preference on attribute. For each user, its specific value u can be used to measure her/his interests to the item in a fine-grained level. • I denotes the representation of item characterized by the attributes. For the item, the specific value i is learned from its related entities and relations in the KG. • A represents the user's interacted attributes. Here, we slightly abuse the notation a = {a 1 , a 2 , . . . , a A } to denote the collection of the attributes interacted by the specific user. Whereinto, a j is the representation of j-th attribute and A is the number of interacted attributes. • K describes the structure information of items observed by user. Its specific value k = {k 1 , . . . , k K } is a collection of K item' structure information. Thereinto, k j ∈ [0, 1] 1×N is j-th item's distribution over all N attributes, reflecting how the attributes are entangled by j-th item. • S whose specific value s ∈ R is the score to measure the attribute-based similarity of user-item pair. With these variables, we next describe the directed edges in the causal graph, along which the successor nodes are affected by the ancestor nodes. In particular,</p><p>• K → A : the structure information decides the exposure of attributes for the user. Since there is no observed record of user-attribute pairs, the collection of user's interacted attributes relies on the structure of items they consumed before. • (K, A) → U : the user preference on attribute is caused by the representations of attributes and structure information. Since the attribute cannot be exposed to users independently, we suggest the user preference learning is affected not only by which attributes are exposed to users but by how they are entangled by the items which the user observed. • (A, U, I) → S : the edges illustrate that the similarity score S is affected by U , I, and A. As for U and I, their similarity is the desired score to measure the user's interests to the item. While, the interacted attributes A may connect with the candidate items in the KG, causing the bias score via the shortcuts. Applying the causal theory to the constructed causal graph, we observe that the structure information (K) as a confounder brings the spurious relation between the user preference (U ) and user interacted attributes (A), which probably causes the suboptimal user representation. Furthermore, the edge A → S accounts for the direct effects on the score, which probably results in the bias issue for interaction prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Graph based Causal Recommendation</head><p>According to our proposed causal graph, we conclude that the reasons causing the issues mainly come from two fork structures (i.e., A ← K → U and U ← A → S), which hinder the user preference learning and bias the similarity scores, respectively. Therefore, we do the intervention on A for deconfounded user preference modeling and adopt the counterfactual inference to debiased the similarity scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Causal Intervention</head><p>As the before mentioned, K acts as the confounder in our proposed causal graph, opening a backdoor path: A ← K → U , which both affects (or causes) either A or U . It leads to spurious correlations by only learning from the likelihood P (U |A). To resolve its negative impact on the user preference learning, we use do-calculus operator to conduct the intervention on A. As shown in Figure <ref type="figure" target="#fig_1">2(b)</ref>, the intervention could be visually treated as cutting off on the edge pointing to A in the causal graph. By interrupting the edge K → A, we can remove the effect of K to A, and then capture the user preference on attribute. Accordingly, we perform the backdoor adjustments to "virtually" achieve the formulation:</p><formula xml:id="formula_2">S u,i,a = P (S|U = u, I = i, do(A = a)) = k∈K P (S|U (a, k), i, a)P (k)<label>(3)</label></formula><p>where P (k) = {P (k 1 ), . . . , P (k K )} reflects how likely the item associated its structure information observed by user. U (•) outputs the representation of user preferences based on their interacted attributes and structure information of items. Within Eq. 3, we force the user interacted attributes fairly interact with all items to implement the intervention. As such, it is capable of opening their entanglement caused by the items. Thereafter, we can estimate Eq. 3 with:</p><formula xml:id="formula_3">P (S|U = u, I = i, do(A = a)) = k∈K P (S|U (a, k), i, a)P (k) = k∈K f (U (a, k), i, a)P (k),<label>(4)</label></formula><p>where f (•) is the function to score the similarity between the user and item. The detail of its implementation will be described in the next section.</p><p>Intuitively, we can estimate the similarity score by calculating the expected value of function f (•) of k. However, the size of sample space K could be infinite in practice and expensive to conduct the computation on each possible value in K. To overcome this drawback, we follow the prior work <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and adopt the Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b23">[24]</ref> to approximate the formulation, as</p><formula xml:id="formula_4">P (S|U = u, I = i, do(A = a)) ≈ f (U (a, ( k∈K kP (k))), i, a)<label>(5)</label></formula><p>It can approximately take outer sum k∈K into the function U (•) designed for the representation of user preference, so as to alleviate the computational burden. It is worth noting that if f (•) is a linear function with a random variable X as the input, then</p><formula xml:id="formula_5">E[f (X)] = f (E[X]</formula><p>) holds under any probability distribution P (X). The proof can be found in the prior studies <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Thus, U (•) and f (•) should be implemented with the linear function to minimize the error of approximation. Beyond this, we deep into the U (•) and explore its inputs k. According to the description of variable K, the specific value k consists of K vectors (i.e., {k 1 , . . . , k K }), each of which describes the distribution of one item over the attributes. Therefore, we can decompose k into two components. The first component is the distribution over all M items, denoted as k ∈ [0, 1] 1×M , reflecting which items are sampled. And, the second component is a matrix K ∈ R M ×N , where each row describes the corresponding item's distribution over attributions. As shown in Figure <ref type="figure" target="#fig_0">1</ref> </p><formula xml:id="formula_6">matrix K = [[1, 1, 0], [1, 1, 1], [0, 0, 1]</formula><p>] over the attributes. Accordingly, Eq. 5 can be rewritten as,</p><formula xml:id="formula_7">P (S|U = u, I = i, do(A = a)) ≈f (U (a, ( k∈K kP (k))), i, a) =f (U (a, k∈ K( kP ( k)), K), i, a).<label>(6)</label></formula><p>As such, we can change the distribution k in the counterfactual world to enforce a fairly exposed to the different structure information of item, instead of perturbing the connection between items and attributes in the KG.</p><p>Assuming that each item has equal opportunity 1 M to be the interacted by users, we derive the formulation:</p><formula xml:id="formula_8">P (S|U = u, I = i, do(A = a)) ≈f (U (a, E( k), K), i, a), = 1 M f (U (a, K), i, a), = 1 M f (U (ã, A, K), i, a).<label>(7)</label></formula><p>Whereinto, we also decompose a into ã ∈ [0, 1] 1×N and A ∈ R N ×D , which represent the distribution of interacted attributes and the matrix of all attributes' representations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Model Implementation</head><p>In this part, we present our proposed model KGCR to implement deconfounded user preference learning U (•) and the similarity scoring f (•), as shown in Figure <ref type="figure" target="#fig_2">3</ref>. In particular, we first extract the knowledge information from the KG to initialize the item and attribute representations.</p><p>We perform the TransE model <ref type="bibr" target="#b26">[27]</ref> on the KG due to its efficiency and robustness. More specifically, it learns the embeddings of entities and relations by minimizing the following loss function,</p><formula xml:id="formula_9">L KG = (h,r,t)∈T (h ,r,t )∈T</formula><p>[γ+d(e h +e r , e t )-d(e h +e r , e t )] + , (8) where [x] + denotes the positive part of x. (h, r, t) and (h , r, t ) represent the samples from the sets of triplets T and T , respectively. Different with T collected from the KG, T consists of the broken triplets constructed by randomly replacing one entity in a real-world triplet. In addition, γ is the margin hyperparameter. And d() is implemented by Euclidean distance function to estimate the distance of the input representation pairs in a latent space.</p><p>Implementing U (•). Observing the derived function U (•) in Eq 7, we find that the user representation is depended on the representation of attributes (A), distribution of interacted attribution (ã), and the structure information of KG (K). To learn the representation of the user preference and attribute, we propose to construct two bipartite graphs: the itemattribute and user-attribute graphs, and separately conduct the graph convolutional operations on two bipartite <ref type="bibr" target="#b27">[28]</ref>. Considering that U (•) should be a linear function, we implement the graph convolutional operations, as,</p><formula xml:id="formula_10">p (l+1) = q∈Np 1 |N p ||N q | q (l) ,<label>(9)</label></formula><p>where p (l+1) is the representation of p-th node at (l + 1)-th graph convolutional layer. |N p | and |N q | represent the number of neighbors of nodes p and q, respectively. By performing the operations, we aggregate the structure information into the attribute representation, and then model the fine-grained user preference based on the interacted attributes associated with the structure information.</p><p>Implementing f (•). Recalling our proposed causal graph, we suggest that the similarity score comes from the user preference to item (i.e., (U, I) → S) and affinity between the item and user's interacted attributes (i.e., (A, I) → S).</p><p>Hence, we define the score function f (•) as:</p><formula xml:id="formula_11">S u,i,a = f (u, i, a) = g(u, i)σ(h(i, a)),<label>(10)</label></formula><p>where u ∈ U is the user representations from function U (•) and σ(•) denotes the sigmoid function. In addition, g(•) and h(•) are the functions to separately model (U, I) → S and (A, I) → S. This design, named Multiplication fusion strategy, is inspired by the prior studies <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which provides non-linearity for sufficient representation capacity of the fusion strategy. For simplicity, we conduct the inner product on the input vectors to implement g(•), formally,</p><formula xml:id="formula_12">g(u, i) = u • i ,<label>(11)</label></formula><p>where i ∈ I is the item representation. As for the function h(•), we formulate it as,</p><formula xml:id="formula_13">S i,a = h(i, a) = i • a , where a = 1 M a∈A a,<label>(12)</label></formula><p>where we empirically represent the interacted attributes a with the mean of their representations.</p><p>In addition to the similarity score based on the knowledge information, we learn the id embeddings of users and items and score the user-item similarity based on the collaborative information with:</p><formula xml:id="formula_14">S cf u,i = u cf • i cf ,<label>(13)</label></formula><p>where we use u cf ∈ U cf and i cf ∈ I cf to denote the user and item id embeddings based on the collaborative information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Optimization</head><p>To optimize the proposed model, we adopt Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b29">[30]</ref> in this work. In particular, if an item i has been interacted by user u, then it is assumed that u prefers i over her/his unobserved item j. Incorporating the attribute-based and collaborative-based similarity scores <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, formulate BPR loss as:</p><formula xml:id="formula_15">L 1 = (u,i,j)∈O -ln σ(S u,i,a + S cf u,i -S u,j,a -S cf u,j ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_16">O = {(u, i, j)|(u, i) ∈ O + , (u, j) ∈ O -} denotes</formula><p>the training set consisted of the positive (observed) and negative (unobserved) user-item pairs. Moreover, we also devise a loss function to supervise the output of h(•), as</p><formula xml:id="formula_17">L 2 = (u,i,j)∈O max(0, σ(S i,a ) -σ(S j,a ) -m),<label>(15)</label></formula><p>where m is a pre-defined margin value to control the difference between the predicted values of the positive and negative pairs. This is based on the assumption that the representation of positive item should be more similar with that of user's interacted attributes. Empirically, we find that the difference should be limited to a pre-defined range, in order to achieve the promising results. Combining these two loss functions, we obtain the objective function:</p><formula xml:id="formula_18">L = L 1 + αL 2 + ||Θ|| 2 , (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>where α is a hyper-parameter to balance the two loss functions and ||Θ|| 2 is the regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Counterfactual Inference Strategy</head><p>Although we eliminate the impact of the confounder A in the user preference learning, we still suffer from the bias issue in the user-item similarity score as the before mentioned. To resolve this problem, we opt to do the counterfactual inference <ref type="bibr" target="#b32">[33]</ref> by answering the question: what the similarity score would be, if we only consider the attribute-based representations of users and items.</p><p>According to the prior work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b33">[34]</ref>, the counterfactual inference is able to estimate what the descendant variables would be if the value of one treatment variable changed from its value in the factual world. Leveraging this counterfactual thought, we can estimate NDE of variable A to S in the counterfactual world and remove it from TE in the factual world to answer the above question. In particular, TE is the causal effect of the treatment variable (e.g., A) on the response variable (e.g., S), which can be estimated by the change of response variable when for each layer l ∈ {1, 2, . . . , L} do 6:</p><formula xml:id="formula_20">Algorithm 1 Knowledge Graph-based Causal Recommen- dation Input: O + = {(u, i)|u ∈ U, i ∈ I}, G kg = {(h, r, t)|h, t ∈ E, r ∈ R} Output: Θ = {U, I, U cf , I cf }, {S cf u,i , S u,i,</formula><p>obtain u l and a l w.r.t. Eq.9;</p><p>7:</p><p>end for 8:</p><p>for each layer l ∈ {1, 2, . . . , L} do 9:</p><p>obtain i l and a l w.r.t. Eq.9;</p><p>10:</p><p>end for 11:</p><formula xml:id="formula_21">g(u, i) = u • i ; 12: S i,a = h(i, a) = i • a , where a = 1 M a∈A a; 13:</formula><p>S u,i,a = f (u, i, a) = g(u, i)σ(h(i, a)); the treatment variable changed from the reference value to the expected value. Formally, TE can be estimated by the following formulation:</p><formula xml:id="formula_22">T E = S u,i,a -S u * ,i,a * ,<label>(17)</label></formula><p>where, u * and a * are the reference values of the variables U and A. Different from TE, NDE is the change of the response variable when only changing the treatment variable on the direct path, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>(a). In the counterfactual world, we formulate the NDE as,</p><formula xml:id="formula_23">N DE = S u * ,i,a -S u * ,i,a * ,<label>(18)</label></formula><p>where (u * , i, a) is the counterfactual instance generated by conducting the do-calculus operator on variable U , i.e., do(U = u * ). Following, by removing NDE from TE, we can obtain the debiased effect of user and item to the score, which is termed as the total indirect effect (TIE). Formally, TIE could be formulated as,</p><formula xml:id="formula_24">T IE = T E -N DE = S u,i,a -S u * ,i,a * -S u * ,i,a + S u * ,i,a * = g(u, i)σ(h(i, a)) -g(u * , i)σ(h(i, a)) = (g(u, i) -s i )σ(h(i, a)).<label>(19)</label></formula><p>Thereinto, since the reference value u * is independent with the specific user, s i could be computed by the mean of the output of g(u, i).</p><p>Finally, we incorporate the debiased similarity score with the collaborative score to predict the interactions between the users and items, formally,  </p><formula xml:id="formula_25">y ui = S cf ui + S kg ui = u cf • i cf + (g(u, i) -s i )σ(h(i, a)). (<label>20</label></formula><formula xml:id="formula_26">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset</head><p>In our work, we do the experiments on Amazon-book, LastFM, and Yelp2018 datasets, which are released by KGAT and widely used by KG-based recommendation models. These datasets involve three different applications in real scenarios, including online shopping, music, and business recommendations. According to the description in KGAT, each of them contains the user-item interactions and knowledge graph consisted of multiple triplets, i.e., object, relation, subject . In summary, we detail the information of datasets associated with the knowledge graph in Table <ref type="table" target="#tab_4">2</ref>.</p><p>For each dataset, we use the training set provided in KGAT <ref type="bibr" target="#b1">[2]</ref> to optimize the parameters of our proposed model and baselines. In the training phase, we take each user-item interaction in the set and randomly select one negative item that is unseen by the user to form the training triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>We compare KGCR with several state-of-the-art KG-based recommendation models 1 , including,</p><p>• CKE <ref type="bibr" target="#b6">[7]</ref> As one of the representative KG-based recommendation models, CKE conducts TransR method to learn the knowledge information from the KG and injects the information into the id embeddings to supplement the CF-based recommendation.</p><p>• RippleNet <ref type="bibr" target="#b34">[35]</ref> RippleNet represents the user preferences as the set of her/his interacted items in the KG and extends the set by propagating the preference along the 1. For fairness, we discard KV-MN model <ref type="bibr" target="#b10">[11]</ref>, since it is designed for sequential recommendation task. edge of the KG, so as to leverage the external knowledge in the recommendation.</p><p>• KTUP <ref type="bibr" target="#b35">[36]</ref> Treating the user-item interaction as one type of relation between entities (i.e., user and item entity), KTUP jointly optimizes the task of recommendation and knowledge graph completion, which is able to transfer the information learned from KG into the user-item interaction prediction.</p><p>• MKR <ref type="bibr" target="#b36">[37]</ref> To assist the recommendation model with KGE, MKR conducts the multi-task training strategy to simultaneously optimize KGE and recommendation tasks, in which a cross&amp;compress unit is devised to integrate the learned knowledge and collaborative information. • KGNN-LS <ref type="bibr" target="#b0">[1]</ref> To generate user-specific item representations, this model converts the KG into user-specific graphs, and considers user preference on KG relations and label smoothness in the information aggregation phase.</p><p>• CKAN <ref type="bibr" target="#b37">[38]</ref> CKAN explicitly encodes the collaborative signals by collaboration propagation and applies a knowledge-aware attention mechanism to incorporate the collaborative information with knowledge information.</p><p>• KGAT <ref type="bibr" target="#b1">[2]</ref> KGAT applies the graph attentive network on a holistic graph combined by the KG and user-item graph, which is able to explicitly construct the high-order relation among the user, item, and attributes. It encodes the collaborative and knowledge information into the user and item in an end-to-end manner.</p><p>• KGIN <ref type="bibr" target="#b2">[3]</ref> This is a state-of-the-art KG-based recommendation model. It models the user intents behind useritem interactions with the knowledge information learned from the KG, so as to improve the performance and interpretability of recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Metrics</head><p>For each user, we take all items she/he does not interact with during the training phase, including the groundtruth and negative items, as the candidate samples. Using the trained model, we score the interactions of the useritem pairs and rank them in descending order. Then, we adopt Recall@K and Normalized Discounted Cumulative Gain (NDCG@K for short) to evaluate the effectiveness of top-K recommendation. By default, we set K=20 and report the average values of the above metrics for all users during the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Parameter Settings</head><p>With the help of Pytorch 2 and torch-geometric package 3 , we implement the baselines and our proposed model. We utilize Xavier <ref type="bibr" target="#b38">[39]</ref> and Adam <ref type="bibr" target="#b39">[40]</ref> algorithms in the experiments to initialize and optimize the parameters of the models. For fairness, we set the dimensions of the id embedding and entity representation as 64 for all models, and adopt the three-layer graph convolutional network to learn the id embeddings in KGCR and GCN-based models.</p><p>In terms of the hyper-parameters, we tune the learning rate in range of {0.0001, 0.001, 0.01, 0.1} and regularization weight in range of {0.0001, 0.001, 0.01, 0.1}. Besides, we employ the same early stopping strategy with KGAT, which stops the training if Recall@20 does not increase for 10 successive epochs. For the baselines, we do the same options and follow the designs (e.g., the number of graph convolutional layers and multilayer perceptron) in their articles to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison</head><p>To demonstrate the effectiveness of our proposed model, we start by doing the comparison between KGCR and the baselines w.r.t. Recall@20 and NDCG@20. Specifically, we list the results on three datasets in Table <ref type="table" target="#tab_5">3</ref>, where Improv.% represents the relative improvements of the best performing method (bolded) over the strongest baselines (underlined). Observing the table from the bottom to top, we have the following findings:</p><p>• Without any doubts, our proposed model consistently achieves the best performance on the three datasets, which is able to justify the effectiveness of our proposed model. In particular, KGCR improves over the strongest baselines w.r.t. Recall@20 by 3.79%, 5.93%, and 6.66% in Amazon-Book, LastFM, and Yelp2018, respectively. We attribute the improvements to modeling the fine-grained user preference based on the knowledge information and supplement the collaborative information with the attribute-based similarity between the user and item. • Analyzing the results of our KGCR across the datasets, we find that its improvement on Yelp2018 is more significant than that on Amazon-book dataset. The reason might be from their different applications. Since the items from Yelp2018 involve more topics 4 , learning the finergrained user preference on Yelp2018 could yield better performance than that on Amazon-book just constructed for the book recommendation, which is consistent with the findings in KGIN. • KGIN outperforms the other baselines by a margin over three datasets. We suggest that its advantages mainly come from its obtained user intents. Although KGIN hardly distinguishes the knowledge information from the collaborative one, the results of KGIN and KGCR verify the effectiveness of modeling fine-grained user intents/preferences in the recommendation method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To evaluate the designs in our proposed model, we first justify the effectiveness of the deconfounded user preference and debiased similarity score in KGCR. Then, we delve into them to test the implementation of functions U (•), h(•), and loss function L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Impact of Deconfounded User Preference and Debiased Similarity Score</head><p>We devise two variants: KGCR w/o DC and KGCR w/o CI , which remove the deconfounded user preference and counterfactual inference from KGCR, respectively. To implement KGCR w/o DC , we ignore the user interacted attributes and estimate the user-item interaction on attributes. Thus, we replace the term S u,i,a in Equ. 14 by:</p><formula xml:id="formula_27">S u,i = g(u, i) = u • i .<label>(21)</label></formula><p>As for KGCR w/o CI , we omit the term S u * ,i,a in Equ. <ref type="bibr" target="#b18">19</ref> and obtain the interaction of user-item pair:</p><formula xml:id="formula_28">y ui = u cf • i cf + g(u, i)σ(h(i, a)).<label>(22)</label></formula><p>Observing the results shown in Table <ref type="table" target="#tab_6">4</ref>, we find that the results w.r.t. NDCG and Recall are dramatically reduced after removing the two components on three datasets. It demonstrates the necessity of our designed deconfounded user preference learning and debiased similarity scoring in KGCR. Jointly analyzing Table <ref type="table" target="#tab_5">3</ref>, KGCR w/o CI yields better results than KGIN. We suggest that the improvements come from the deconfounded user preference learning, while KGIN suffers from the negative influence of structure information to fine-grained user preference/intent modeling. It verifies the effectiveness of eliminating the confounder from user preference learning. Moreover, we observe that KGCR w/o DC outperforms the baselines in most Causal Inference for Knowledge Graph-based Recommendation cases. It justifies our argumentation that the bias issue challenges the KG-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Impact of Implementation of U (•)</head><p>After evaluation deconfounded user preference, we further test the implementation U (•), which is at the core of deconfounded user preference learning.</p><p>As mentioned before, we conduct the graph convolutional operations on the user-attribute (U-A for short) and itemattribute (I-A for short) graphs to represent the user preference on attribute. To justify our implementation, we separately discard U-A and I-A to devise the variants: KGCR w/o U-A and KGCR w/o I-A 5 . After removing the graph from the proposed model, we capture the representations of user nodes from a trainable matrix and optimize them by conducting the backpropagation operation under the supervision of user-item interactions, instead of the message propagation of GCN model. By comparing the variants with KGCR model, we have the following observations:</p><p>From their results listed in Table <ref type="table">5</ref>, we observe that KGCR is superior to all variants on three datasets. It verifies the rationality and effectiveness of the implementation of function U (•). In addition, we find the performance is decreased when we remove I-A graph from function U (•). This indicates that the spurious relation caused by the item structure information hinders the user preference on attribute. More importantly, the better results of KGCR demonstrate that we eliminate the confounder by using the structure information extracted from I-A graph. Jointly analyzing Table <ref type="table" target="#tab_6">4</ref>, KGCR w/o U-A unexpectedly outperforms KGCR w/o DC , even U-A is removed. But, it is reasonable since we explicitly model the causal-effect relation K → A, where K constructs the statistic correlation between user preference and attributes via the back-door path A ← K → U . We suggest that it quantitatively illustrates our constructed causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Impact of Implementation of h(•)</head><p>Exploring the function h(•), it is easy to find that the representation of user's interacted attributes plays a pivotal role in debiased similarity scoring, which directly affects the estimation of NDE of A to S. Thus, we adopt three different methods to model the representation and study their influence to the prediction. Beyond the mean value of the interacted attributes, we also utilize the multi-layer 5. Discarding both of two graphs equals removing the user preference learning, that is KGCR w/o DC in Table <ref type="table" target="#tab_5">3</ref>. perceptron (MLP) to optimize the representation and use the weighted average to adjust the different frequencies of interacted attributes. Conducting the experiments, we summarize their results in Table <ref type="table" target="#tab_7">6</ref>, where MLP, Weighted, and Mean denote three methods, respectively.</p><p>According to the results in Table <ref type="table" target="#tab_7">6</ref>, Mean method achieves the comparable results to Weighted method in LastFM and Yelp2018 datasets, while slightly performing better than Weighted in Amazon-book dataset. The reason might be that the dense attributes per item on Amazonbook brings much more uninterested attributes to users, whereas the users tend to prefer several attributes of items in practice. Moreover, we observe MLP method performs poor across three datasets. We attribute the suboptimal performance to the overfitting problem. It is consistent with the findings in LightGCN <ref type="bibr" target="#b40">[41]</ref> that performing multiple layers of nonlinear feature transformation may bring no benefits, even negative influence, to recommendation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Impact of implementation of loss function</head><p>Here, we explore the impact of loss function L 2 , which is used to supervise the similarity between the representations of the user's interacted attributes and item. We compare our KGCR with two different variants, formally,</p><formula xml:id="formula_29">     KGCR LOG : L 2 = log(σ(S i,a -S j,a )) KGCR M AX : L 2 = max(0, S i,a -S j,a -m) KGCR : L 2 = max(0, σ(S i,a ) -σ(S j,a ) -m)<label>(23)</label></formula><p>We summarize their results on three datasets in Table <ref type="table">7</ref>.</p><p>As expected, KGCR equipped with original L 2 achieves the better results on three datasets. It can be attributed to that the function controls the difference between the scores of positive and negative pairs (i.e., S i,a and S j,a ) in an appropriate range. Specifically, KGCR LOG underperforms the other two methods on all datasets. It might be that the positive pair is scored higher than the negative pair by a large margin. Comparing it with KGCR MAX , we observe that, in most cases, the later one yields the better performance by limiting the margin between the scores. It justifies that the score of the positive pair is just slightly higher than that of the negative one in the real scenario. However, KGCR MAX may result in the overlarge scores of all user-item pairs, which makes the difference vanish after conducting the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>We randomly selected two users #47847 and #50248 from Amazon-Book, and illustrate their predicted scores of top 20 items in Figure To be more specific, we decompose the predicted scores into CF scores (i.e., bars) and debiased similarity scores (i.e., green bars), and emphasize the ground-true with red rectangles.</p><p>Obviously, the positive items (i.e., ground-truth samples) achieve higher similarity scores than the others, justifying the debiased similarity scoring again. More importantly, it is easy to find that the debiased similarity scores help the prediction of user-item interactions. In particular, as shown in the left subfigure, we see that four positive items (i.e., ranking at 5, 6, 7, and 18) will drop out of the top 20 if we remove the debiased similarity scores. Moreover, in the right subfigure, when forgoing the debiased similarity scores, the ground-truth samples inevitably fall behind in the rankings. These demonstrate the effectiveness of fine-grained user preference on attribute and verify the rationality of our constructed causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameter Studies</head><p>Studying the object function Eq. 20, we suggest that two hyper-parameters, i.e., m and α, affect the performance of our proposed model. We hence conduct the experiments on three datasets to evaluate them and illustrate the result w.r.t. Recall@20 and NDCG@20 in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>Testing KGCR by varying the value of margin m in range of {0.1, 0.3, 0.5, 0.7, 0.9}, we can see the results w.r.t. Recall and NDCG present the different curves on three datasets. It indicates the difference of scores between positive and negative pairs are various. The possible reason is the different number of observed items per user on three datasets. The dense one, like LastFM dataset, is more likely to sample the hard negative item that is similar to the positive item, making their scores closer.</p><p>As for α, we test it in the range of {0.1, 0.5, 1, 2, 5, 10} and observe that KGCR with the larger value of α probably performs better. We attribute it to our devised loss function L 2 . It limits the difference of scores between positive and negative pairs, which may cause the gradient vanishing with the value of α decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">KG-based Recommendation</head><p>KG-based recommendation roughly falls into three types: embedding-based, path-based, and GCN-based models. The embedding-based models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[36]</ref> mainly focus on learning the knowledge information from the triplets in the KG. Following the assumption of semantic relatedness (e.g., head +relation = tail), they conduct KGE algorithms (e.g., TransE <ref type="bibr" target="#b26">[27]</ref> and TransR <ref type="bibr" target="#b41">[42]</ref>) on the KG, and then incorporate the learned entity embedding with collaborative signal to improve the recommendation models. For example, CKE <ref type="bibr" target="#b6">[7]</ref> adopts TransE algorithm and takes the entity embeddings of items as the context information feeding into the CF-based model. Path-based models instead model the patterns of connections among items in the KG to construct the long-range path connecting the user and target item. As one of the representative studies, RippleNet <ref type="bibr" target="#b34">[35]</ref> stores the path from the user to historical items and collects these items' representation as the knowledge signal to enrich user representations. Accordingly, it reorganizes the item entities in the KG as the various paths to inject the knowledge information into representations of users. More currently, GCN-based models are developed to improve the KG-based recommendation. Typically, they leverage the information propagation mechanism to aggregate the local structure information in the KG. Then, by explicitly or implicitly embedding the learned knowledge into the user and item representations. For instance, KGCN <ref type="bibr" target="#b42">[43]</ref> conducts the graph convolutional operations in an end-toend manner and captures inter-item relatedness effectively by mining their associated attributes on the KG. KGAT combines the KG and user-item bipartite graph as a and applies the graph convolutional operations on it. KGIN, a follow-up work, is also built upon the same heterogeneous graph and designs an adaptive aggregation method to learn the fine-grained user intents from the KG.</p><p>In summary, these methods either treat the KG as complementary information or expect to construct the itemitem correlation via the edge in the KG, where knowledge information merely be used to supplement the collaborative information. Different from the prior studies, we distinguish the knowledge information from the collaborative one and learn the fine-grained user preference on attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Causal Inference</head><p>Causal inference have been widely adopted on the visual computing <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b43">[44]</ref>, neural language processing <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, and information retrieval <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref> tasks. The core of causal inference is to abstract the causal-effect relation between the variables from the target task. Based on the causal-effect factor, the spurious relation can be removed from the truly causal relation, so as to empower the robustness and fairness of models.</p><p>In the recommendation domain, Wang et al., <ref type="bibr" target="#b11">[12]</ref> constructed the causal graph to eliminate the bias caused by user historical interactions in the CF-based recommendation model. Zhang et.al., <ref type="bibr" target="#b46">[47]</ref> focused on the popularity bias issue in the recommendation model and used the causaleffect tools to flexibly control the bias for the high-quality prediction. Beyond the CF-based recommendation model, Wang et al., <ref type="bibr" target="#b21">[22]</ref> mitigated the clickbait issue in the multimedia recommendation model with the counterfactual inference. To resolve the visual bias from the gap between click and purchase behaviors, Qiu et al., <ref type="bibr" target="#b47">[48]</ref> developed a new method to retain the supportive visual information and perform visual debiasing.</p><p>Different from these methods, we construct the causal graph for the KG-based recommendation model. By exploring the graph, we point out the spurious relations from the causal-effect relation between the variables and develop a new model to eliminate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose to model the fine-grained user preference on attribute to improve the recommender system. Towards this end, we construct the causal graph to resolve the challenges in a causal view, which is the first attempt in the KG-based recommendation to the best of our knowledge. Analyzing the constructed causal graph, we attribute the challenges into the spurious relation negatively affecting the prediction and develop a Knowledge Graph based Causal Recommendation model (KGCR) to address the problem. Specifically, we design the deconfounded user preference learning to model the user preference on attribute by removing the confounder between the user preference and her/his interacted attributes. Furthermore, we leverage the counterfactual inference to eliminate the bias misleading the attribute-based similarity scores of user-item pairs. Despite the promising performance, there remains some problem that should be explored in future work. For instance, the distribution of attributes over all items may cause popularity bias in the interaction prediction. And, how to distinguish the causal-effect relation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> between entities in the KG and leverage it in the recommendation is also a challenging problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of KG-based recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our constructed causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of Counterfactual Inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The predicted score of top 20 ranking of users #47847 (left) and #50428 (right). Best in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Key notations and descriptions. , M , and N number of users, items, and attributes, respectively. u ∈ R 1×D representation of user preference on attribute i ∈ R 1×D representation of item characterized by attributes.u cf ∈ R 1×D user id embedding. i cf ∈ R 1×D item id embedding. k ∈ [0, 1] 1×M distribution of items interacted by user. ãi ∈ [0, 1] 1×N distribution of the attributes characterizing item i. e (•) ∈ R 1×Dembeddings of nodes and edges in the KG.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>G kg</cell><cell>knowledge graph (KG).</cell></row><row><cell>l</cell><cell>the number of graph convolutional layer.</cell></row><row><cell>S u,i,a</cell><cell>similarity score between user and item.</cell></row><row><cell>S i,a</cell><cell>affinity between item and user interacted attributes.</cell></row><row><cell>S cf u,i</cell><cell>similarity score between user and item embeddings.</cell></row></table><note><p>OO + , O -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a } 1: Initialization all model parameters in Θ; 2: Calculate embeddings E of KG w.r.t. Eq.8;</figDesc><table /><note><p>3: Initialization I and A with E; 4: while stopping criteria is not met do 5:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Summary of the datasets. (U-I means the user-item interaction.)</figDesc><table><row><cell></cell><cell></cell><cell>Amazon-book</cell><cell>LastFM</cell><cell>Yelp2018</cell></row><row><cell></cell><cell>#Users</cell><cell>70,679</cell><cell>23,566</cell><cell>45,919</cell></row><row><cell>U-I</cell><cell>#Items</cell><cell>24,915</cell><cell>48,123</cell><cell>45,538</cell></row><row><cell></cell><cell>#Inter.</cell><cell>847,733</cell><cell cols="2">3,034,796 1,185,068</cell></row><row><cell></cell><cell>#Entities</cell><cell>88,572</cell><cell>58,266</cell><cell>90,961</cell></row><row><cell>KG</cell><cell>#Relations</cell><cell>39</cell><cell>9</cell><cell>42</cell></row><row><cell></cell><cell>#Triplets</cell><cell>2,557,746</cell><cell>464,567</cell><cell>1,853,704</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Overall performance comparison.</figDesc><table><row><cell>Model</cell><cell>Amazon-book Recall NDCG Recall NDCG Recall NDCG LastFM Yelp2018</cell></row><row><cell>CKE</cell><cell>0.1343 0.0698 0.0736 0.0630 0.0651 0.0414</cell></row><row><cell cols="2">RippleNet 0.1336 0.0691 0.0791 0.0684 0.0664 0.0428</cell></row><row><cell>KTUP</cell><cell>0.1369 0.0680 0.0783 0.0681 0.0640 0.0420</cell></row><row><cell>MKR</cell><cell>0.1286 0.0676 0.0743 0.0642 0.0634 0.0409</cell></row><row><cell cols="2">KGNN-LS 0.1362 0.0560 0.0880 0.0642 0.0637 0.0402</cell></row><row><cell>CKAN</cell><cell>0.1442 0.0698 0.0812 0.0660 0.0604 0.0377</cell></row><row><cell>KGAT</cell><cell>0.1489 0.0799 0.0870 0.0744 0.0712 0.0443</cell></row><row><cell>KGIN</cell><cell>0.1687 0.0915 0.0978 0.0848 0.0736 0.0482</cell></row><row><cell>KGCR</cell><cell>0.1751 0.0949 0.1036 0.0883 0.0785 0.0518</cell></row><row><cell cols="2">Improv% 3.79% 3.72% 5.93% 4.13% 6.66% 7.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Impact of Deconfounded User Preference and Debiased Similarity.</figDesc><table><row><cell>Model</cell><cell>Amazon-book Recall NDCG Recall NDCG Recall NDCG LastFM Yelp2018</cell></row><row><cell cols="2">KGCR 0.1751 0.0949 0.1036 0.0883 0.0786 0.0518</cell></row><row><cell cols="2">w/o DC 0.1565 0.0847 0.0867 0.0737 0.0721 0.0469</cell></row><row><cell cols="2">w/o CI 0.1716 0.0926 0.0996 0.0838 0.0774 0.0510</cell></row><row><cell></cell><cell>TABLE 5</cell></row><row><cell></cell><cell>Impact of Implementation of U (•)</cell></row><row><cell>Model</cell><cell>Amazon-book Recall NDCG Recall NDCG Recall NDCG LastFM Yelp2018</cell></row><row><cell>KGCR</cell><cell>0.1751 0.0949 0.1036 0.0883 0.0786 0.0518</cell></row><row><cell cols="2">w/o U-A 0.1703 0.0924 0.0927 0.0742 0.0732 0.0476</cell></row><row><cell cols="2">w/o I-A 0.1662 0.0901 0.0955 0.0747 0.0750 0.0487</cell></row></table><note><p>2. https://pytorch.org/. 3. https://pytorch-geometric.readthedocs.io/.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Impact of Implementation of h(•).Comparing with the GCN-based models (i.e., KGNN-LS, CKAN, KGAT, KGIN, and KGCR), the other models are suboptimal in most cases. It indicates that the graph convolutional operation is capable of capturing the collaborative and knowledge information from the graph and then enhancing the representations of users and items.</figDesc><table><row><cell>Model</cell><cell>Amazon-book Recall NDCG Recall NDCG Recall NDCG LastFM Yelp2018</cell></row><row><cell>MLP</cell><cell>0.1735 0.0933 0.1018 0.0876 0.0781 0.0515</cell></row><row><cell cols="2">Weighted 0.1742 0.0942 0.1036 0.0885 0.0786 0.0518</cell></row><row><cell>Mean</cell><cell>0.1751 0.0949 0.1036 0.0883 0.0785 0.0518</cell></row><row><cell></cell><cell>TABLE 7</cell></row><row><cell></cell><cell>Impact of Implementation of L 2 .</cell></row><row><cell>Model</cell><cell>Amazon-book Recall NDCG Recall NDCG Recall NDCG LastFM Yelp2018</cell></row><row><cell cols="2">LOG 0.1630 0.0877 0.1008 0.0853 0.0772 0.0506</cell></row><row><cell cols="2">MAX 0.1738 0.0924 0.1012 0.0858 0.0770 0.0509</cell></row><row><cell cols="2">KGCR 0.1751 0.0949 0.1036 0.0883 0.0786 0.0518</cell></row></table><note><p>4. https://www.yelp.com/dataset/documentation/main •</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Performance of KGCR in terms of margin m and weight α on Amazon-book, LastFM, and Yelp2018. Best viewed in color.</figDesc><table><row><cell></cell><cell>0.1753</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0955</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.095</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1748</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.0945</cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>0.1738 0.1743</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG</cell><cell cols="2">0.0935 0.094</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.093</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1723 0.1728</cell><cell cols="2">alpha=0.1 alpha=2</cell><cell>alpha=0.5 alpha=5</cell><cell></cell><cell>alpha=1 alpha=10</cell><cell></cell><cell></cell><cell cols="2">0.0925</cell><cell>alpha=0.1 alpha=2</cell><cell>alpha=0.5 alpha=5</cell><cell>alpha=1 alpha=10</cell></row><row><cell></cell><cell cols="2">0.1</cell><cell>0.3</cell><cell>0.5 m</cell><cell>0.7</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5 m</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell></cell><cell cols="7">(a) Recall on Amazon-book</cell><cell></cell><cell cols="5">(b) NDCG on Amazon-book</cell></row><row><cell></cell><cell>0.103 0.104</cell><cell>0.1036</cell><cell>0.1027</cell><cell>0.1021</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.088 0.0883 0.089</cell><cell>0.0876</cell><cell></cell></row><row><cell>Recall</cell><cell>0.101 0.102</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1014</cell><cell>0.1007</cell><cell>NDCG</cell><cell>0.087 0.086</cell><cell></cell><cell></cell><cell>0.0865</cell><cell>0.0866</cell><cell>0.0862</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.085</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.099</cell><cell></cell><cell>alpha=0.1</cell><cell>alpha=0.5</cell><cell></cell><cell>alpha=1</cell><cell></cell><cell></cell><cell>0.084</cell><cell></cell><cell>alpha=0.1</cell><cell>alpha=0.5</cell><cell>alpha=1</cell></row><row><cell></cell><cell>0.098</cell><cell></cell><cell>alpha=2</cell><cell>alpha=5</cell><cell></cell><cell>alpha=10</cell><cell></cell><cell></cell><cell>0.083</cell><cell></cell><cell>alpha=2</cell><cell>alpha=5</cell><cell>alpha=10</cell></row><row><cell></cell><cell cols="2">0.1</cell><cell>0.3</cell><cell>0.5 m</cell><cell>0.7</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5 m</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell cols="5">(c) Recall on LastFM</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(d) NDCG on LastFM</cell></row><row><cell></cell><cell>0.079</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>0.0785 0.0775 0.078</cell><cell>0.0781</cell><cell>0.0783</cell><cell>0.0786</cell><cell cols="2">0.0782</cell><cell>0.0775</cell><cell>NDCG</cell><cell cols="2">0.052 0.0518 0.0514 0.0516 0.0512 0.0514</cell><cell>0.0517</cell><cell>0.0518</cell><cell>0.0513</cell><cell>0.0515</cell></row><row><cell></cell><cell>0.077</cell><cell></cell><cell>alpha=0.1</cell><cell>alpha=0.5</cell><cell></cell><cell>alpha=1</cell><cell></cell><cell></cell><cell>0.051</cell><cell></cell><cell>alpha=0.1</cell><cell>alpha=0.5</cell><cell>alpha=1</cell></row><row><cell></cell><cell>0.0765</cell><cell>0.1</cell><cell>alpha=2 0.3</cell><cell>alpha=5 0.5 m</cell><cell>0.7</cell><cell>alpha=10</cell><cell>0.9</cell><cell></cell><cell>0.0508</cell><cell>0.1</cell><cell>alpha=2 0.3</cell><cell>alpha=5 0.5 m</cell><cell>alpha=10 0.7</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell cols="5">(e) Recall on Yelp2018</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(f) NDCG on Yelp2018</cell></row><row><cell cols="2">Fig. 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>• The structure information of KGs hinders the representation of user preference on attribute entities. This</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kgat: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning intents behind interactions with knowledge graph for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Research commentary on recommendations with side information: A survey and research directions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECRA</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph-based recommender systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dkn: Deep knowledgeaware network for news recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1835" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging metapath based context for top-n recommendation with a neural coattention model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1531" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving sequential recommendation with knowledgeenhanced memory networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deconfounded recommendation for alleviating bias amplification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hs-gcn: Hamming spatial graph convolutional networks for recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Make it a chorus: knowledge-and time-aware item modeling for sequential recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personalized news recommendation with knowledge-aware interactive matching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect. Basic books</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR, 2021</title>
		<imprint>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deconfounded video moment retrieval with causal intervention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some new estimates of the &apos;jensen gap</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abramovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-E</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIA</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bounds on the jensen gap, and implications for mean-concentrated distributions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sitharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Roitberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05267</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interest-aware message-passing gcn for recommendation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference. ACM, 2021</title>
		<imprint>
			<biblScope unit="page" from="1296" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rubi: Reducing unimodal biases for visual question answering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph-refined convolutional network for multimedia recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3541" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical user intent graph network for multimedia recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Direct and indirect effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.2300</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect. Basic books</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ripplenet: Propagating user preferences on the knowledge graph for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task feature learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2000" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ckan: Collaborative knowledge-aware attentive network for recommender systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3307" to="3313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">770</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Empowering language understanding with counterfactual reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03046</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Should graph convolution trust neighbors? a simple causal inference method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR, 2021</title>
		<imprint>
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Causal intervention for leveraging popularity bias in recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causalrec: Causal inference for visual debiasing in visuallyaware recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2021</title>
		<imprint>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04477</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Causal discovery from incomplete data: a deep learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05343</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
