<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural Agnostic Modeling: Adversarial Learning of Causal Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-07-25">25 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
							<email>diviyan@fentech.ai</email>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
							<email>olivier.goudet@univ-angers.fr</email>
						</author>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
							<email>guyon@chalearn.org</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
							<email>sebag@lri.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TAU</orgName>
								<address>
									<addrLine>20 Rue Raymond Aron</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LRI</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Université Paris-Saclay</orgName>
								<address>
									<addrLine>660 Rue Noetzlin 6 Rue Ménars</addrLine>
									<postCode>75002</postCode>
									<settlement>Gif-Sur-Yvette Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">TAU</orgName>
								<orgName type="institution" key="instit2">LRI</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<orgName type="institution" key="instit5">Université Paris-Saclay</orgName>
								<address>
									<addrLine>660 Rue Noetzlin</addrLine>
									<settlement>Gif-Sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Univ. Paris-Saclay and Diviyan Kalainathan&apos;s PhD at Univ. Paris-Saclay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structural Agnostic Modeling: Adversarial Learning of Causal Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-25">25 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1803.04929v5[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new causal discovery method, Structural Agnostic Modeling (SAM), is presented in this paper. Leveraging both conditional independencies and distributional asymmetries, SAM aims to find the underlying causal structure from observational data. The approach is based on a game between different players estimating each variable distribution conditionally to the others as a neural net, and an adversary aimed at discriminating the generated data against the original data. A learning criterion combining distribution estimation, sparsity and acyclicity constraints is used to enforce the optimization of the graph structure and parameters through stochastic gradient descent. SAM is extensively experimentally validated on synthetic and real data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper addresses the problem of uncovering causal structure from multivariate observational data. This problem is receiving more and more attention with the increasing emphasis on model interpretability and fairness <ref type="bibr" target="#b14">(Doshi-Velez and Kim, 2017)</ref>. While the gold standard to establish causal relationships remains randomized controlled experiments <ref type="bibr" target="#b55">(Pearl, 2003;</ref><ref type="bibr" target="#b27">Imbens and Rubin, 2015)</ref>, in practice these often happen to be costly, unethical, or simply infeasible. Therefore, hypothesizing causal relations from observational data, often referred to as observational causal discovery, has attracted much attention from the machine learning community <ref type="bibr" target="#b40">(Lopez-Paz et al., 2015;</ref><ref type="bibr" target="#b49">Mooij . et al., 2016;</ref><ref type="bibr" target="#b59">Peters et al., 2017)</ref>. Observational causal discovery has found many applications, e.g. in economics to understand and model the impact of monetary policies <ref type="bibr" target="#b8">(Chen et al., 2007)</ref>, or in bio-informatics to infer network structures from gene expression data <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref> and prioritize exploratory experiments.</p><p>Observational causal discovery aims to learn the causal graph from samples of the joint probability distribution of observational data. Four main approaches have been proposed in the literature (more in Section 2.4).</p><p>A first approach refers to score based methods, using local search operators to navigate in the space of Directed Acyclic Graphs (DAGs) in order to find the Markov equivalence class of the graph optimizing the considered score <ref type="bibr" target="#b9">(Chickering, 2002;</ref><ref type="bibr" target="#b62">Ramsey, 2015)</ref>. A second approach includes constraint-based methods leveraging conditional independence tests to identify the skeleton of the graph and the v-structures <ref type="bibr" target="#b70">(Spirtes et al., 1993;</ref><ref type="bibr" target="#b12">Colombo et al., 2012)</ref>. A third approach embodies hybrid algorithms, combining ideas from constraint-based and score-based algorithms <ref type="bibr" target="#b76">(Tsamardinos et al., 2006;</ref><ref type="bibr" target="#b54">Ogarrio et al., 2016)</ref>. The fourth approach goes beyond the Markov equivalence class limitations by exploiting asymmetries in the joint distribution, e.g. based on the assumption that p(x)p(y|x) is simpler than p(y)p(x|y) (for some appropriate notion of simplicity) when X causes Y <ref type="bibr" target="#b25">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b82">Zhang and Hyvärinen, 2010;</ref><ref type="bibr" target="#b48">Mooij et al., 2010)</ref>. Another stream of work closely related to causal discovery is the causal feature selection, aiming at recovering the Markov Blanket of target variables <ref type="bibr" target="#b81">(Yu et al., 2018)</ref>. It leverages the estimation of mutual information among variables <ref type="bibr" target="#b4">(Bell and Wang, 2000;</ref><ref type="bibr" target="#b6">Brown et al., 2012;</ref><ref type="bibr" target="#b78">Vergara and Estévez, 2014)</ref> or uses classification or regression models to support variable selection <ref type="bibr">(Aliferis et al., 2003</ref><ref type="bibr" target="#b1">(Aliferis et al., , 2010))</ref>.</p><p>The contribution of this paper is a new causal discovery algorithm called Structural Agnostic Modeling (SAM), <ref type="foot" target="#foot_0">1</ref> restricted to continuous variables, which aims to exploit both conditional independence relations and distributional asymmetries from observational data. SAM searches for an acyclic Functional Causal Model (FCM) <ref type="bibr" target="#b55">(Pearl, 2003)</ref>. SAM proceeds as follows: i) the distribution of each variable conditionally to its parents, referred to as Markov kernel <ref type="bibr" target="#b31">(Janzing and Scholkopf, 2010)</ref>, is learned from the observational data as a neural net; ii) sparsity and acyclicity constraints are defined on the graph derived from these Markov kernels, inspired from <ref type="bibr" target="#b38">Leray and Gallinari (1999)</ref>; <ref type="bibr" target="#b81">Yu et al. (2018)</ref>; <ref type="bibr" target="#b84">Zheng et al. (2018)</ref>; iii) all Markov kernels are learned in parallel, subject to the above constraints, through an adversarial mechanism, discriminating the true data distribution from the partial distributions generated after the Markov kernels <ref type="bibr" target="#b17">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b47">Mirza and Osindero, 2014)</ref>. The critical combinatorial optimization problem at the core of (causal) graph learning thus is tackled through a single continuous optimization problem.</p><p>Overall, SAM relies on Occam's razor principle to infer the causal graph, using compound structural and functional complexity scores to assess the complexity of each candidate graph.</p><p>This paper is organized as follows: Section 2 introduces the problem of learning an FCM, presents the main underlying assumptions and briefly describes the state of the art in causal modelling. Section 3 presents the principle of the proposed approach and its loss function. Section 4 describes the SAM algorithm devised to optimize this loss function. Section 5 presents the experimental setting used for the empirical validation of SAM and provides illustrative examples on causal graph learning. Section 6 reports on SAM empirical results compared to the state of the art. Section 7 discusses the contribution and presents some perspectives for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Observational Causal modeling: Formal Background</head><p>Let X = [X 1 , . . . X d ] denote a vector of d continuous random variables, with unknown joint probability distribution p(X) and joint density p(x). The observational causal discovery setting considers n iid samples drawn from p(X), noted D = {x (1) , . . . , x (n) }, with x ( ) = (x ( ) 1 , . . . , x ( ) d ) and x ( ) j the -th sample of X j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Functional Causal Models</head><p>The underlying generative model of the data is assumed to be a Functional Causal Model (FCM) <ref type="bibr" target="#b55">(Pearl, 2003)</ref>, defined as a pair (G, f ), with G a directed acyclic graph and f = (f 1 , . . . , f d ) a set of d causal mechanisms. Formally, we assume that each variable X j follows a distribution described as:</p><formula xml:id="formula_0">X j = f j (X Pa(j;G) , E j ).<label>(1)</label></formula><p>For notational simplicity, X j denotes both a variable and the associated node in graph G. Pa(j; G) is the set of parents of X j in G, f j is a function from R |Pa(j;G)|+1 → R and E j is a random noise variable modelling the effects of non-observed variables.</p><p>A 5-variable FCM is depicted on Figure <ref type="figure" target="#fig_0">1</ref>. </p><formula xml:id="formula_1">E 1 f 1 X 1 E 3 E 2 E 4 f 4 X 4 E 5 f 2 f 3 X 3 f 5 X 5 X 2                X 1 = f 1 (E 1 ) X 2 = f 2 (X 1 , E 2 ) X 3 = f 3 (X 1 , E 3 ) X 4 = f 4 (E 4 ) X 5 = f 5 (X 3 , X 4 , E 5 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Notations and Definitions</head><p>All notations used in the paper are listed in Appendix A. X \i denotes the set of all variables but X i . Conditional independence: (X i ⊥ ⊥ X j |X k ) means that variables X i and X j are independent conditionally to X k , i.e. p(x i , x j |x k ) = p(x i |x k )p(x j |x k ).</p><p>Markov blanket: a Markov blanket MB(X i ) of a variable X i is a minimal subset of variables in X \i such that any disjoint set of variables in the network is independent of X i conditioned on MB(X i ).</p><p>V-structure: Variables {X i , X j , X k } form a v-structure iff their causal structure, in the induced subgraph of G with these three variables, is :</p><formula xml:id="formula_2">X i → X k ← X j .</formula><p>Skeleton of the DAG: the skeleton of the DAG is the undirected graph obtained by replacing all edges by undirected edges.</p><p>Markov equivalent DAG: two DAGs with same skeleton and same v-structures are said to be Markov equivalent <ref type="bibr" target="#b56">(Pearl and Verma, 1991)</ref>. A Markov equivalence class is represented by a Completed Partially Directed Acyclic Graph (CPDAG) having both directed and undirected edges.</p><p>Variables X i and X j are said to be adjacent according to a CPDAG iff there exists an edge between both nodes. If directed, this edge models causal relationship X i → X j or X j → X i . If undirected, it models a causal relationship in either direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Causal Assumptions and Properties</head><p>In this paper, we make the following assumptions:</p><p>Acyclicity: The causal graph G (Equation ( <ref type="formula" target="#formula_0">1</ref>)) is assumed to be a Directed Acyclic Graph (DAG).</p><p>Causal Markov Assumption (CMA): Noise variables E j (Equation ( <ref type="formula" target="#formula_0">1</ref>)) are assumed to be independent from each other. This assumption together with the above DAG assumption yields the classical causal Markov property, stating that all variables are independent of their non-effects (non descendants in the causal graph) conditionally to their direct causes (parents) <ref type="bibr" target="#b71">(Spirtes et al., 2000)</ref>. Under the causal Markov assumption, the distribution described by the FCM satisfies all conditional independence relations<ref type="foot" target="#foot_1">foot_1</ref> among variables in X via the notion of d-separation <ref type="bibr" target="#b55">(Pearl, 2009)</ref>. Accordingly the joint density p(x) can be factorized as the product of the densities of each variable conditionally on its parents in the graph:</p><formula xml:id="formula_3">p(x) = d j=1</formula><p>p(x j |x Pa(j;G) )</p><p>(2)</p><p>Causal Faithfulness Assumption (CFA): The joint density p(x) is assumed to be faithful to graph G, that is, every conditional independence relation that holds true according to p is entailed by G <ref type="bibr" target="#b69">(Spirtes and Zhang, 2016)</ref>. It follows from causal Markov and faithfulness assumptions that every causal path in the graph corresponds to a dependency between variables, and vice versa.</p><p>Causal Sufficiency assumption (CSA): X is assumed to be causally sufficient, that is, a pair of variables {X i , X j } in X has no common cause external to X \i,j . In other words, we assume that there is no hidden confounder. This corresponds to making the assumption that the noise variables E j for j = 1, .., d entering in Equation ( <ref type="formula" target="#formula_0">1</ref>) are independent of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Background</head><p>This section briefly presents a formal background of observational causal discovery, referring the reader to <ref type="bibr" target="#b71">(Spirtes et al., 2000;</ref><ref type="bibr" target="#b59">Peters et al., 2017)</ref> for a comprehensive survey.</p><p>Observational causal discovery algorithms are structured along four categories:</p><p>I A first category of methods are score-based methods which aim to find the best CPDAG in the sense of some global score: using search heuristics, graph candidates are iteratively evaluated using a scoring criterion such as the AIC score or the BIC score and compared with the best graph obtained so far. One of the most popular score-based method is the Greedy Equivalent Search (GES) algorithm <ref type="bibr" target="#b9">(Chickering, 2002)</ref>. GES aims to find the best CPDAG in the sense of the Bayesian Information Criterion (BIC). The CPDAG space is navigated using local search operators, e.g. add edge, remove edge, and reverse edge. GES starts with an empty graph. In a first forward phase, edges are iteratively added to greedily improve the global score. In a second backward phase, edges are iteratively removed to greedily improve the score. Under CSA, CMA and CFA assumptions, GES identifies the true CPDAG in the large sample limit, if the score used is decomposable, score-equivalent and consistent <ref type="bibr" target="#b9">(Chickering, 2002)</ref>. More recently, <ref type="bibr" target="#b61">Ramsey et al. (2017)</ref> proposed a GES extension called Fast Greedy Equivalence Search (FGES) algorithm aimed to alleviate the computational cost of GES. It leverages the decomposable structure of the graph to optimize all the subgraphs in parallel. This optimization greatly increases the computational efficiency of the algorithms, enabling score-based methods to run on millions of variables.</p><p>II A second category of approaches are constraint-based methods leveraging conditional independence tests to identify a skeleton of the graph and v-structures, in order to output the CPDAG of the graph. One of the most famous constraint-based algorithm is the celebrated PC algorithm <ref type="bibr" target="#b70">(Spirtes et al., 1993)</ref>: under CSA, CMA and CFA, and assuming that all conditional independences have been identified, PC returns the CPDAG of the functional causal model, respecting all v-structures. It has notably been shown that for graphs with bounded degree, the PC algorithm has a running time that is polynomial in the number of variables. When very fast independence tests such as partial correlation tests are employed, the PC algorithm can handle high dimensional graphs <ref type="bibr" target="#b33">(Kalisch and Bühlmann, 2007)</ref>. For non Gaussian data generated with non-linear mechanisms and complex interactions between the variables, more powerful but also more time consuming tests have been proposed such has the Kernel Conditional Independence test (KCI) <ref type="bibr" target="#b83">(Zhang et al., 2012)</ref> leveraging the kernel-based Hilbert-Schmidt Independence Criterion (HSIC) <ref type="bibr" target="#b19">(Gretton et al., 2005)</ref>.</p><p>III The third category of approaches are hybrid algorithms which combine ideas from constraintbased and score-based algorithms. According to <ref type="bibr" target="#b50">Nandy et al. (2015)</ref>, such methods often use a greedy search like the GES method on a restricted search space for the sake of computational efficiency. This restricted space is defined using conditional independence tests. The Max-Min Hill climbing algorithm (MMHC) <ref type="bibr" target="#b76">(Tsamardinos et al., 2006)</ref> firstly builds the skeleton of a Bayesian network using conditional independence tests (using constraint-based approaches) and then performs a Bayesian-scoring hill-climbing search to orient the edges (using scorebased approaches). The skeleton recovery phase, called Max-Min Parents and Children (MMPC) selects for each variable its parents and children in the variable set. Note that this task is different from recovering the Markov blanket of variables as the spouses are not selected.</p><p>The orientation phase is a hill-climbing greedy search involving 3 operators: add, delete and reverse edge.</p><p>IV The above-mentioned three categories of methods can learn at best the Markov equivalence class of the DAG which can be a significant limitation in some cases 3 . Therefore, new methods exploiting asymmetries or causal footprints in the data generative process have been proposed to uniquely identify the causal DAG. According to <ref type="bibr" target="#b60">Quinn et al. (2011)</ref>, the first approach in this direction is LiNGAM <ref type="bibr" target="#b68">(Shimizu et al., 2006)</ref>. LiNGAM handles linear structural equation models on continuous variables, where each variable is modeled as the weighted sum of its parents and noise. Assuming further that all noise variables are non-Gaussian, <ref type="bibr" target="#b68">Shimizu et al. (2006)</ref> show that the causal structure is fully identifiable (all edges can be oriented).</p><p>Such methods, taking into account the full information from the observational data <ref type="bibr" target="#b69">(Spirtes and Zhang, 2016)</ref> such as data asymmetries induced by the causal directions, have been proposed and primarily applied to the bivariate DAG case<ref type="foot" target="#foot_3">foot_3</ref> , referred to as cause-effect pair problem <ref type="bibr" target="#b25">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b13">Daniusis et al., 2012;</ref><ref type="bibr" target="#b49">Mooij et al., 2016;</ref><ref type="bibr" target="#b82">Zhang and Hyvärinen, 2010)</ref>.</p><p>The reader is referred to <ref type="bibr" target="#b73">Statnikov et al. (2012);</ref><ref type="bibr" target="#b49">Mooij et al. (2016)</ref>; <ref type="bibr" target="#b21">Guyon et al. (2019)</ref> for a thorough presentation of the bivariate problem. The extension of the bivariate approaches to the multivariate setting has been tackled by <ref type="bibr" target="#b16">Friedman and Nachman (2000)</ref>; <ref type="bibr" target="#b7">Bühlmann et al. (2014)</ref> assuming additive noise, and identifiability results have been obtained for the causal additive models (CAM) <ref type="bibr" target="#b7">(Bühlmann et al., 2014)</ref>.</p><p>As noted by <ref type="bibr" target="#b48">Mooij et al. (2010)</ref>, identifiability results (see e.g. <ref type="bibr" target="#b25">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b82">Zhang and Hyvärinen, 2010;</ref><ref type="bibr" target="#b7">Bühlmann et al., 2014)</ref> most often rely on restrictions on the class of admissible causal mechanisms; however, such restrictions might be too strong for real-world data.</p><p>In order to overcome such a limitation and build more expressive models, Mooij et al. ( <ref type="formula">2010</ref>) have proposed the fully non-parametric GPI approach. The key idea is to define appropriate priors on marginal distributions of the causes and on causal mechanisms in order to favor a model of low complexity. This method, designed for the bivariate setting, has shown very good results on a wide variety of data as it is not restricted to a specific class of mechanisms.</p><p>Extending this complexity-based search to the multivariate case, the Causal Generative Neural Networks (CGNN) <ref type="bibr" target="#b18">(Goudet et al., 2018)</ref> uses generative neural networks to model the causal mechanisms. CGNN starts from a given skeleton and explores the space of DAGs using a hill climbing algorithm aimed to optimize the global score of the network computed as the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b20">(Gretton et al., 2007)</ref> between the true empirical distribution P and the generated distribution P .</p><p>The proposed SAM approach ambitions to combine the best of all the above: exploiting conditional independence relations as methods in the first three categories, and exploiting distributional asymmetries, achieving some trade-off between model complexity and data fitting in the line of the GPI method <ref type="bibr" target="#b48">(Mooij et al., 2010)</ref>.</p><p>SAM aims at addressing the limitations of CGNN. The first limitation of CGNN is a quadratic computational complexity w.r.t. the size of the dataset, as its learning criterion is based on the Maximum Mean Discrepancy between the generated and the observed data. In contrast, SAM uses an adversarial learning approach (GAN) <ref type="bibr" target="#b17">(Goodfellow et al., 2014)</ref> that scales linearly with the data size. Moreover as opposed to non-parametric methods such as kernel density estimates and nearest neighbor methods, adversarial learning suffers less from the curse of dimensionality, being able to model complex high-dimensional distributions <ref type="bibr" target="#b39">(Lopez-Paz and Oquab, 2016;</ref><ref type="bibr" target="#b35">Karras et al., 2017)</ref>.</p><p>The second limitation of CGNN is a scalability issue w.r.t. the number of variables, due to the greedy search exploration in the space of DAGs, as all generative networks modeling the causal mechanisms in the causal graph must be retrained when a new graph structure is evaluated. SAM tackles this second issue by using an unified framework for structure optimization, inspired by <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref>, where the mechanisms and the structure are simultaneously learned within a DAG learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem settings</head><p>As said, this paper focuses on causal discovery, that is, finding the DAG G involved in the Functional Causal Model generating the data (section 2.1). The SAM approach is based on simultaneously learning d Markov kernels, where the j-th Markov kernel q j expresses the conditional density of X j given its parents in a candidate graph G (Janzing and Scholkopf, 2010) for j ranging in [ <ref type="bibr">[1, d]</ref>].</p><p>More precisely, these d learning problems are jointly tackled through optimizing the likelihood of the data according to the conditional distributions q j (X j |X Pa(j; G) ), with X Pa(j; G) denoting the estimated causes of X j , while enforcing the sparsity and acyclicity of the graph G defined from all edges X k → X j for k ranging in Pa(j; G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Markov Kernels as functional causal mechanisms</head><p>Let D = {x (1) , . . . , x (n) } denote the observational dataset, including n iid samples (with</p><formula xml:id="formula_4">x ( ) = (x ( ) 1 , . . . , x ( ) d ) for ranging in [[1, n]]</formula><p>), sampled from the unknown joint distribution p(X) corresponding to the sought FCM.</p><p>Each Markov kernel q j is sought as a functional causal mechanism f j :</p><formula xml:id="formula_5">Xj = fj ([a j X, E j ], θ j ).<label>(3)</label></formula><p>where</p><p>• a j = (a 1,j , . . . a d,j ) is a binary vector referred to as j-th structural gate. Coefficient a i,j is 1 iff variable X i is used to generate X j , that is, edge X i → X j belongs to graph G. Otherwise, a i,j is set to 0. Coefficient a i,i is set to 0 to avoid self-loops. Pa(j; G), defined as the set of indices i such that a i,j = 1, corresponds to the set of causes of X j according to fj ;</p><p>• θ j is a set of parameters (e.g. neural weights) used to compute fj ;</p><p>• E j is a noise variable modelling all non observed causes of X j .</p><p>In summary, function fj takes as input all variables X k such that a j,k = 1, augmented with the noise variable E j , and it is parameterized by θ j .</p><p>For each sample x = (x 1 , . . . , x d ), let x -j be defined as (x 1 , . . . x j-1 , x j+1 , . . . , x d ). Model fj thus defines a generative model of X j conditionally to its estimated causes, noted q j (x j |x -j , a j , θ j ), or for simplicity q j (x j |x Pa(j; G) , θ j ), as the set Pa(j; G) is fully characterized by the binary vector a j .</p><p>As all noise variables E j for j in [ <ref type="bibr">[1, d]</ref>] are independent, all Markov kernels q j (x j |x Pa(j; G) , θ j ) are independent models, making it possible to learn them all in parallel from the observational dataset D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning independent Markov kernels</head><p>Learning q j (x j |x Pa(j; G) , θ j ) consists of learning f j and selecting a (minimal) subset of parents Pa(j; G). The solution a j and θ j is obtained by minimizing the conditional log-likelihood of the data, given by:</p><formula xml:id="formula_6">S n j (a j , θ j , D) = - 1 n n =1 log q j (x ( ) j |x ( ) Pa(j; G) , θ j )<label>(4)</label></formula><p>Following <ref type="bibr" target="#b6">(Brown et al., 2012)</ref>, each conditional log-likelihood term is decomposed into three terms as follows, where p is the data distribution: log q j (x</p><formula xml:id="formula_7">( ) j |x ( ) Pa(j; G) , θ j ) = log q j (x ( ) j |x ( ) Pa(j; G) , θ j ) p(x ( ) j |x ( ) Pa(j; G) ) + log p(x ( ) j |x ( ) Pa(j; G) ) p(x ( ) j |x ( ) -j ) + log p(x ( ) j |x ( ) -j ) (5)</formula><p>Note that the sum</p><formula xml:id="formula_8">1 n n =1 log p(x ( ) j |x ( )</formula><p>-j ) converges toward the constant H(X j |X -j ) as n goes to infinity; it is thus discarded in the following.</p><p>Let X Pa(j; G) denote the complementary set of X j and its parent nodes in G. Then, after Brown</p><formula xml:id="formula_9">et al. (2012), 1 n n =1 log p(x ( ) j |x ( ) -j ) p(x ( ) j |x ( ) Pa(j; G)</formula><p>) is equal to the empirical conditional mutual information term between X j and X Pa(j; G) , conditioned on the parent variables X Pa(j; </p><p>Eventually, the negative conditional log-likelihood score (Eq. ( <ref type="formula" target="#formula_6">4</ref>)) can be rewritten as:</p><formula xml:id="formula_11">S n j (a j , θ j , D) = În (X j , X Pa(j; G) |X Pa(j; G) ) + 1 n n =1 log p(x ( ) j |x ( ) Pa(j; G) ) q(x ( ) j |x ( ) Pa(j; G) , θ j ) + cst<label>(7)</label></formula><p>The term În (X j , X Pa(j; G) |X Pa(j; G) ) is used to identify the Markov equivalence class of the true G, while the term</p><formula xml:id="formula_12">1 n n =1 log p(x ( ) j |x ( ) Pa(j; G) ) q(x ( ) j |x ( ) Pa(j; G) ,θ j )</formula><p>is used to disambiguate graphs within the Markov equivalence class of G. Both terms are discussed in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structural loss</head><p>For each Markov kernel, the minimization of În (X j , X Pa(j; G) |X Pa(j; G) ) (Eq. ( <ref type="formula" target="#formula_11">7</ref>)) corresponds to a feature selection problem, the selection of X Pa(j; G) . As shown by <ref type="bibr" target="#b81">(Yu et al., 2018)</ref>, 5 the solution of this feature selection problem converges toward the Markov Blanket M B(X j ) of X j in the true causal graph G in the large sample limit. This feature selection problem is classically tackled by optimizing the log-likelihood of the data augmented with a regularization term of the form λ S |Pa(j; G)|, with |Pa(j; G)| the number of parents of X j in G and hyper-parameter λ S &gt; 0 controlling the sparsity of selection in the Markov Blanket. 6  Therefore, without acyclicity constraint, the optimization of the following structural loss enables to identify the moral graph associated with the true causal graph G:</p><formula xml:id="formula_13">L n S ( G, D) = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) + λ S | G|.<label>(8)</label></formula><p>A first contribution of the proposed approach is to establish that, searching a DAG minimizing Eq. ( <ref type="formula" target="#formula_13">8</ref>), leads to identify the Markov equivalence class of G (CPDAG) in the large sample limit. The intuition is that the acyclicity constraint prevents the children nodes from being selected as parents, hence the spouse nodes do not need be selected either. 7   Theorem 1 (CPDAG identification by structural loss minimization)</p><p>Under CMA, CFA and CSA assumptions, two results of convergence in probability, hold: i) For every DAG G in the equivalence class of G,</p><formula xml:id="formula_14">lim n→∞ L n S ( G, D) = L n S (G, D)</formula><p>ii) For every DAG G not in the equivalence class of G, there exists λ S &gt; 0 such that:</p><formula xml:id="formula_15">lim n→∞ P(L n S ( G, D) &gt; L n S (G, D)) = 1 Proof in Appendix B</formula><p>Experimental and analytical illustrations of this result on the toy 3-variable skeleton A -B -C are presented in Appendix B.</p><p>The limitation of the structural loss is that it does not allow one to disambiguate among equivalent DAGs. Typically in the bivariate case, both graphs (X → Y and Y → X) get the same structural loss in the large sample limit (I(X, Y ) + λ S ). We shall see that the parametric loss addresses this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parametric loss</head><p>The second term in Eq. ( <ref type="formula" target="#formula_11">7</ref>),</p><formula xml:id="formula_16">1 n n =1 log p(x ( ) j |x ( ) Pa(j; G) ) q j (x ( ) j |x ( ) Pa(j; G) ,θ j )</formula><p>, measures the ability of fj to fit the conditional distribution of X j based on its parents X Pa (j; G). 6. By construction, |Pa(j; G)| = d i=1 ai,j corresponds to the L1 norm of vector aj. 7. Note that algorithms such as GENIE3 <ref type="bibr" target="#b29">(Irrthum et al., 2010)</ref>, winner of the DREAM4 and DREAM5 challenges, also rely on solving d independent feature selection problems in parallel, but without any acyclicity constraint. They might thus incur some false discovery rate (selecting edges that are not in G). We shall return to this point in section 5.</p><p>Note that in the large sample limit, this term converges towards E p log p(x j |x Pa(j; G) ) q j (x j |x Pa(j; G) ,θ j ) , and it goes to 0 when considering sufficiently powerful causal mechanisms, irrespective of whether G = G: As shown by <ref type="bibr" target="#b26">Hyvärinen and Pajunen (1999)</ref>, it is always possible to find a function fj such that X j ∼ fj (X Pa(j; G) , E j ), with E j ⊥ ⊥ X Pa(j; G) , corresponding to a probabilistic conditional model q such that q j (x j |x Pa(j; G) , θ j ) = p(x j |x Pa(j; G) ).</p><p>In order to support model identification within the Markov equivalence class of the true DAG, a principled approach is to restrict the hypothesis space <ref type="bibr" target="#b25">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b82">Zhang and Hyvärinen, 2010)</ref>. In counterpart, such restrictions limit the generality of the approach and may cause practical problems, particularly so when there is no information available about the true generative mechanisms of the data. Therefore, taking inspiration from GPI pioneering approach <ref type="bibr" target="#b48">(Mooij et al., 2010)</ref>, we propose to restrict the capacity of the causal mechanisms fj through a regularization term. Algorithmically, the complexity of the causal mechanisms is controlled through using the Frobenius norm of the parameters in fj as regularization term <ref type="bibr" target="#b51">(Neyshabur et al., 2017)</ref>, with regularization weight λ F . Considering other regularization terms is left for further work.</p><p>Eventually, the parametric loss is defined as the sum of the data fitting terms and the regularization term:</p><formula xml:id="formula_17">L n F ( G, θ, D) = d j=1   1 n n =1 log p(x ( ) j |x ( ) Pa(j; G) ) q j (x ( ) j |x ( ) Pa(j; G) , θ j ) ] + λ F θ j F  <label>(9)</label></formula><p>How this parametric loss can disambiguate among the different models in the CPDAG is illustrated in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>Eventually, the proposed approach aims to search a DAG G optimizing a trade-off between the data fitting loss, the structural and parametric regularization terms:</p><formula xml:id="formula_18">S n ( G, θ, D) := d j=1 S n j (a j , θ j , D) + λ S d j=1 d i=1 a i,j + λ F d j=1 θ j F = - 1 n d j=1 n =1 log q j (x ( ) j |x ( ) Pa(j; G) , θ j ) data fitting + λ S d j=1 d i=1 a i,j + λ F d j=1 θ j F model complexity = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) + λ S | G| structural loss + d j=1   1 n n =1 log p(x ( ) j |x ( ) Pa(j; G) ) q(x ( ) j |x ( ) Pa(j; G) , θ j ) ] + λ F θ j F   parametric loss</formula><p>(10) As said, the model complexity of the causal mechanisms is decomposed into the structural complexity (the L 0 norm of the structural gates, that is the number of edges in G) and the functional complexity (the Frobenius norm of the parameters involved in each fj ). Seen differently, the proposed approach aims to search a DAG that simultaneously minimizes the structural loss (section 3.3) and the parametric loss (section 3.4).</p><p>The structural loss akin category I, II and III approaches <ref type="bibr" target="#b71">(Spirtes et al., 2000;</ref><ref type="bibr" target="#b9">Chickering, 2002</ref>) (section 2.4) aims to identify the Markov equivalence class of the true G, while the parametric loss akin cause effect pair methods <ref type="bibr" target="#b48">(Mooij et al., 2010)</ref>, exploits distribution asymmetries to disambiguate models in the CPDAG equivalence class of G.</p><p>Note that this approach can accommodate any available prior knowledge about the generative mechanisms of the data, regarding either the type and complexity of the causal mechanisms (e.g. linear or polynomial functions) or the noise distributions (e.g. Gaussian or uniform noise).</p><p>In order to demonstrate the applicability of the approach in the general case (where there exists little or no information about the generative mechanisms of the data), the Structural Agnostic Modelling algorithm uses neural networks to model the causal mechanisms fj , and relies on adversarial learning to optimize the data fitting (conditional likelihood) terms. Note that the minimisation of Eq. ( <ref type="formula">10</ref>) does not guarantee to obtain a DAG. Therefore, a global acyclicity constraint will be introduced in section 4.3. It will serve to couple the learning of the d Markov kernels in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structural Agnostic model</head><p>As said, the Structural Agnostic Model (SAM) implements the above proposed settings, minimizing the global score (Eq. ( <ref type="formula">10</ref>)). It addresses its optimization challenges using three original algorithmic choices:</p><p>1. Firstly, the space of admissible causal mechanism is not explicitly restricted, and each Markov kernel is modelled as a conditional generative neural network <ref type="bibr" target="#b47">(Mirza and Osindero, 2014)</ref>. All d Markov kernels are learned in parallel, enforcing the scalability of the approach up to thousands of variables (see section 6).</p><p>2. Secondly, the conditional likelihood scores attached with each Markov kernel are approximated and optimized using an adversarial neural network. This approach does not require any assumption about the true distribution p of the data (such as the Gaussianity of noise).</p><p>3. Lastly, the combinatorial optimization issues related with finding a DAG are avoided as follows.</p><p>On one hand, an acyclicity constraint inspired from <ref type="bibr" target="#b84">Zheng et al. (2018)</ref> is added to the learning criterion (Eq. ( <ref type="formula">10</ref>)), to learn a DAG by solving a continuous optimization problem (section 4.3). On the other hand, a Bernoulli reparametrization trick <ref type="bibr" target="#b42">(Maddison et al., 2016;</ref><ref type="bibr" target="#b30">Jang et al., 2016)</ref> is used to simultaneously optimize the structure of the model (i.e. the a j 's) and the causal mechanisms (i.e. parameters θ j ), using stochastic gradient descent.</p><p>These algorithmic choices are presented in the next three subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modeling each Markov kernel with a conditional generative neural network</head><p>In order to model each Markov kernel q j (x j |x, a j , θ j ), each causal mechanism fj is implemented as a H-hidden layer neural network, with n h nodes at the h-th hidden layer for h = 1, ..., H. The input is of dimension n 0 = d + 1, the output is of dimension n H+1 = 1. The mathematical expression of each deep neural network is given by:</p><formula xml:id="formula_19">Xj = fj (X, E j ) = L j,H+1 • σ • L j,H • • • • • σ • L j,1 ([a j X, E j ])<label>(11)</label></formula><p>where a j X corresponds to the element wise product between the two vectors a j and X, and E j is a Gaussian noise variable with zero mean and unit variance.</p><formula xml:id="formula_20">L j,h : R n h-1 → R n h is an affine linear map defined by L j,h (x) = W j,h • x + b j,h for given n h × n h-1 dimen- sional weight matrix W j,h (with coefficients {w j,h k,l } 1≤k≤n h 1≤l≤n h-1</formula><p>), n h dimensional bias vector b j,h</p><p>(with coefficients {b j,h k } 1≤k≤n h ) and σ : R n h →] -1, 1[ n h the element-wise nonlinear activation map defined by σ(z) := (tanh(z 1 ), ..., tanh(z n h )) T . We denote by θ j , the set of all weight matrices and bias vector of the neural network modeling the j-th causal mechanism fj : At every evaluation of noise variable E j , a value is drawn anew from distribution N (0, 1). All the noise variables E j for j ∈ 1, d are drawn from independent distributions.</p><formula xml:id="formula_21">θ j := {(W j,1 , b 1 ), (W j,2 , b 2 ), . . . , (W j,H+1 , b H+1 )}. a 1j X 1 a (j-1)j X j-1 a (j+1)j X j+1 a dj X d 1 E j Xj • • • • • • • • • • • • • • • • • • • • • X -j Structural gates</formula><p>Parallel computation with three dimensional tensor operations For a better computational efficiency on GPU devices, the d causal mechanisms fj for j = 1, .., d (Equation ( <ref type="formula" target="#formula_19">11</ref>)) are computed in parallel with three dimensional tensor operations by stacking all the generative neural networks along a third dimension. The generation of each Xj is independent from the generations of the other variables Xi , with i = 0, . . . , j -1, j + 1, . . . , d. As these d variable generations are independent calculation, they can be done in parallel. Specifically, the output vector X = ( X1 , . . . , Xd ) is computed from X as:</p><formula xml:id="formula_22">X = L H+1 • σ • L H • • • • • σ • L 1 ([A X, E]),<label>(12)</label></formula><p>where A denote the structural gate matrix of size d × d (the adjacency matrix of the graph) formed by the d vectors a j for j = 1, .., d, and X corresponds to a matrix formed by d replications of the vector X. We denote by [A X, E] the matrix of size (d + 1) × d and resulting from the concatenation between the d × d matrix A X and the d dimensional noise vector</p><formula xml:id="formula_23">E = (E 1 , . . . , E d ). L h : R d×(n h-1 ) → R d×n h is the affine linear map defined by L h (x) = Wh • x + bh with the d × n h × n h-1 dimensional</formula><p>weight tensor Wh (corresponding to the aggregation of the d matrices W j,h for j = 1, .., d) and the d × n h dimensional bias matrix bh (aggregation of the d vectors b j,h for j = 1, .., d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SAM learning criterion</head><p>This section describes how SAM tackles the optimization problem defined in Eq. ( <ref type="formula">10</ref>), assessing each candidate DAG G as:</p><formula xml:id="formula_24">S n ( G, θ, D) := - 1 n d j=1 n =1 log q j (x ( ) j |x ( ) -j , a j , θ j ) fit loss + λ S d j=1 d i=1 a i,j + λ F d j=1 θ j F model complexity</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model complexity</head><p>The complexity of each causal mechanism fj is the sum of two terms, with respectively regularization weights λ S &gt; 0 and λ F &gt; 0 :</p><p>• the structural complexity measured by the L 0 norm of the structural gate a j , representing the number of parents of X j .</p><p>• the functional complexity θ j F of the causal mechanism, measured as the Frobenius norm of the weight matrix, providing a good measure of the functional complexity of a deep neural network <ref type="bibr" target="#b51">(Neyshabur et al., 2017)</ref>. More precisely,</p><formula xml:id="formula_25">θ j F = H+1 h=1 W j,h F + H+1 h=1 b j,h F ,<label>(13)</label></formula><p>with</p><formula xml:id="formula_26">W j,h F = 1≤k≤n h 1≤l≤n h-1 |w j,h k,l | 2 and b j,h F = 1≤k≤n h |b j,h k | 2 .</formula><p>Data fitting loss When the number n of samples goes to infinity, the data fitting term goes to data log-likelihood expectation under the sought generative distribution. With same notations as in section 3:</p><formula xml:id="formula_27">lim n→∞ 1 n n =1 log q j (x ( ) j |x ( ) -j , a j , θ j ) = E p(x) log q j (x j |x -j , a j , θ j )<label>(14)</label></formula><p>Let us denote X = X 1 , . . . X j-1 , Xj , X j+1 . . . , X d , the vector of d variables, where the only variable Xj is generated from model fj , all other variables being the observed variables. We denote qj ( X) (or simply qj ) its joint distribution and qj (x j , x -j , a j , θ j ) its joint density. By construction, qj (x j , x -j , a j , θ j ) = p(x -j )q j (x j |x -j , a j , θ j ). Therefore, we have:</p><formula xml:id="formula_28">lim n→∞ 1 n n =1 log q j (x ( ) j |x ( ) -j , a j , θ j ) = E p(x) log q j (x j |x -j , a j , θ j ) p(x j |x -j ) + E p(x) log p(x j |x -j ) (15) = E p(x) log q j (x j |x -j , a j , θ j )p(x -j ) p(x j |x -j )p(x -j ) + E p(x) log p(x j |x -j ) (16) = -E p(x) log p(x) qj (x j , x -j , a j , θ j ) + E p(x) log p(x j |x -j ) (17) = -D KL [p qj ] + H(X j |X -j ),<label>(18)</label></formula><p>with</p><formula xml:id="formula_29">D KL [p qj ] = E p(x) log p(x) qj (x j ,x -j ,a j ,θ j )</formula><p>the Kullback-Leibler divergence between the distributions p(X) and qj ( X), and H(X j |X -j ) the constant, domain-dependent entropy of X j conditionally to X -j (neglected in the following).</p><p>Therefore, the optimization task needs to estimate the quantity D KL [p qj ] for j = 1, . . . , d.</p><p>As the estimation of each D KL [p qj ] is intractable in practice for continuous data, we estimate instead its variational dual representation as f -divergence. Let T be an arbitrary class of functions T : R d → R. For two distributions p and q defined over R d , <ref type="bibr" target="#b52">Nguyen et al. (2010)</ref> establish the following lower bound (tight for sufficiently large families T ):</p><formula xml:id="formula_30">D KL [p q] ≥ sup T ∈T E p(x) [T (x)] -E q(x) [e T (x)-1 ],<label>(19)</label></formula><p>The f-gan approach proposed by <ref type="bibr" target="#b53">Nowozin et al. (2016)</ref> relies on defining T as the family of functions T ω : R d → R parameterized by a deep neural network with parameter ω ∈ Ω, and minimizing the lower bound on D KL [p q] defined as:</p><formula xml:id="formula_31">D KL [p q] ≥ sup ω∈Ω E p(x) [T ω (x)] -E q(x) [e Tω(x)-1 ]<label>(20)</label></formula><p>Taking inspiration from the f-gan, SAM simultaneously trains the d neural networks fj , as follows. For = 1, . . . , n, let x ( ) -j be defined from x ( ) by taking all its coordinates but the j-th, let e ( ) j be drawn from Gaussian N (0, 1), and let scalar</p><p>x( ) j be computed from fj (Eq. ( <ref type="formula" target="#formula_19">11</ref>)) as:</p><formula xml:id="formula_32">x( ) j = fj (x ( ) -j , e ( ) j )</formula><p>Let the pseudo-sample x( ) j be defined from x ( ) by setting its j-th coordinate to x( ) j , and let the dataset Dj include all pseudo-samples x( ) j for = 1 . . . n. For j = 1, . . . , d, let T j ω be trained to discriminate between the dataset D drawn from the original p(X) distribution, and the dataset Dj drawn from qj ( X). After Eq. ( <ref type="formula" target="#formula_31">20</ref>),</p><formula xml:id="formula_33">D KL [p qj ] ≥ sup ω∈Ω j lim n→∞ 1 n n =1 T j ω (x ( ) ) + 1 n n =1 [ -exp(T j ω ( xj ( ) ) -1) ]<label>(21)</label></formula><p>One could indeed use d different adversarial neural networks T j ω to estimate each D KL [p qj ]. However, the use of a single discriminator T ω to achieve the d discrimination tasks is both more computationally efficient, and more stable: it empirically avoids the gradient vanishing phenomena that were observed when solving separately the d min-max optimization problems with d different discriminators.</p><p>By using a single shared discriminator T ω , it comes:</p><formula xml:id="formula_34">d j=1 D KL [p qj ] ≥ d j=1 sup ω∈Ω lim n→∞ 1 n n =1 T ω (x ( ) ) + n =1 [ -exp(T ω ( xj ( ) ) -1)] (22) ≥ sup ω∈Ω lim n→∞   d n n =1 T ω (x ( ) ) + 1 n d j=1 n =1 [ -exp(T ω ( xj ( ) ) -1)]  <label>(23)</label></formula><p>Accordingly, SAM tackles the minimization of the empirical approximation of the above lower bound on d j=1 D KL [p qj ], defined as:</p><formula xml:id="formula_35">sup ω∈Ω   d n n =1 T ω (x ( ) ) + 1 n d j=1 n =1 [ -exp(T ω ( xj ( ) ) -1)]  <label>(24)</label></formula><p>Evaluation of the global penalized min-max loss optimization problem. Eventually, SAM is trained to solve the min-max penalized optimization problem defined as: 8</p><formula xml:id="formula_36">L n ( G * , θ * , D) = min A,θ        λ S d i=1,j=1 a i,j + λ F d j=1 θ j F model complexity + sup ω∈Ω   d n n =1 T ω (x ( ) ) + 1 n d j=1 n =1 [ -exp(T ω ( f θ j ,a j j (x ( ) , e ( ) j ), x ( ) -j ) -1)]   fit loss        (25)</formula><p>where the minimization is carried over the parameters θ = (θ 1 , . . . , θ d ) of the fj and over the matrix A = (a i,j ) representing the structural gates.</p><p>8. Generator fj is written with superscripts θj and aj to indicate that it depends on both parameters θj and aj. </p><formula xml:id="formula_37">a 21 X 2 a 31 X 3 a 41 X 4 1 E 1 X1 • • • • • • • • • • • • • • • X -1 a 14 X 1 a 24 X 2 a 34 X 3 1 E 4 X4 • • • • • • • • • • • • • • • X -4 D1 D4    ∼ D KL [p| q1 ] . . . ∼ D KL [p|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Enforcing the acyclicity of the causal graph</head><p>Note that Equation (25) does not ensure that the optimal G be a DAG: the sparsity constraint on G through the model complexity term (minimizing a j 0 ) leads to independently identify the Markov blanket of each variable X j , selecting all causes, effects and spouses thereof <ref type="bibr" target="#b81">(Yu et al., 2018)</ref>.</p><p>In order to ensure that the solution is a DAG and avoid the associated combinatorial optimization issues (section 2.4), it is proposed to augment the learning criterion with an acyclicity term inspired from <ref type="bibr" target="#b84">Zheng et al. (2018)</ref>. Letting A denote the structural gate matrix (the adjacency matrix of the graph), G is a DAG iff</p><formula xml:id="formula_38">d k=1 tr A k k! = 0.</formula><p>Accordingly, the learning criterion is augmented with an acyclicity term, with:</p><formula xml:id="formula_39">L n ( G * , θ * , D) = min A,θ max ω∈Ω 1 n n =1 d j=1 [ T ω (x ( ) ) -exp(T ω ( f θ j ,a j j (x ( ) , e ( ) j ), x ( ) -j ) -1)] + λ S i,j a i,j + λ F j θ j F + λ D d k=1 tr A k k! ,<label>(26)</label></formula><p>with λ D ≥ 0 a penalization weight.<ref type="foot" target="#foot_5">foot_5</ref> This acyclicity constraint creates a coupling among the d feature selection problems, implying that at most one arrow between pairs of variables can be selected, and more generally leading to remove effect variables from the set of parents of any X i ; the removal of effect variables in turn leads to removing spouse variables as well (section 3.3).</p><p>As the use of the L 0 norms of the vectors a j , if naively done, could entail computational issues (retraining the network from scratch for every new graph structure or neural architecture), an approach based on the Bernoulli reparameterization trick is proposed to end-to-end train the SAM architecture and weights using stochastic gradient descent <ref type="bibr" target="#b72">(Srivastava et al., 2014;</ref><ref type="bibr" target="#b41">Louizos et al., 2017)</ref> and the Binary Concrete relaxation approach <ref type="bibr" target="#b42">(Maddison et al., 2016;</ref><ref type="bibr" target="#b30">Jang et al., 2016)</ref>. This solution corresponds to a learned dropout of edges of the neural network.</p><p>Overall, the optimization of the learning criterion in Equation ( <ref type="formula" target="#formula_39">26</ref>) with the acyclicity and sparsity constraints defines the Structural Agnostic Model SAM (Alg. 1, Figure <ref type="figure">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">First experimental analysis</head><p>This section first describes the synthetic datasets considered and the hyper-parameter configurations used in the experiments. We also present a sensitivity analysis of the main hyper-parameters λ S and λ F in order to show the importance of the structural and regularization terms in the global loss function used by the algorithm. Then we present an illustrative toy example in order to give insights of the sensitivity of SAM to the random initialization of the neural nets and to highlight the usefulness of the DAG penalization term. Finally, we present an analysis of the sensitivity of SAM results to graph density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic dataset generation</head><p>The synthetic datasets involved in a first experimental analysis are DAGs with 20 or 100 variables. Six categories of causal mechanisms have been considered: besides those considered for the experimental validation of the CAM algorithm <ref type="bibr" target="#b58">(Peters et al., 2014)</ref>, a more complex one is considered, leveraging the non-linearity of neural nets.</p><p>1. The DAG structure is such that the number of parents for each variable is uniformly drawn in {0, . . . , 5};</p><p>2. For the i-th DAG, the mean µ i and variance σ i of the noise variables are drawn as µ i ∼ U(-2, 2) and σ i ∼ U(0, 0.4) and the distribution of the noise variables is set to N (µ i , σ i );</p><p>3. For each graph, a 500 sample-dataset is iid generated following the topological order of the graph, with for = 1 to 500:</p><formula xml:id="formula_40">x ( ) = (x ( ) 1 , . . . , x ( ) d ), x ( ) i ∼ f i (X Pa(i) , E i ), with E i ∼ N (µ i , σ i )</formula><p>All variables are then normalized to zero-mean and unit-variance. Six categories of causal mechanisms are considered:</p><p>Algorithm 1 The Structural Agnostic Modeling Algorithm for number of iterations do • Forward phase : i) sample the structural gate matrix A: for i, j = 1, d × 1, d , a i,j = cst(H(l i,j + a i,j ))cst(sigmoid(l i,j + a i,j )) + sigmoid(l i,j + a i,j ) with l i,j drawn from logistic distribution and H the Heavyside step function. ( * ) A denotes the matrix with the a i,j coefficients. iii) generate n samples {x</p><formula xml:id="formula_41">( ) } n l=1 = {(x ( ) 1 , . . . , x( ) d )} n l=1 such that for = 1 . . . , n : x( ) = ( f θ 1 ,a 1 1 (x ( ) , e ( ) 1 ), . . . , f θ d ,a d d (x ( ) , e ( ) d )) = L H+1 • σ • L H • • • • • σ • L 1 ([A x( ) , e ( ) ]),</formula><p>where x( ) corresponds to the matrix formed by d copies of the vector x ( ) .</p><p>• Backward phase : i) update the discriminator by ascending its stochastic gradient:</p><formula xml:id="formula_42">∇ ω   d n n =1 T ω (x ( ) ) + 1 n d j=1 n =1 [ -exp(T ω (x ( ) j , x ( ) -j ) -1) ]  </formula><p>ii) update the all the conditional generators by descending their stochastic gradients w.r.t the set of parameters θ = (θ 1 , . . . , θ d ) and the set of parameters a i,j of the structural gates adjacency matrix A :</p><formula xml:id="formula_43">∇ =∇ θ   1 n d j=1 n =1 [ -exp(T ω ( f θ j ,a j j (x ( ) , e ( ) j ), x ( ) -j ) -1) ] + λ F j θ j F   + ∇ A   1 n d j=1 n =1 [ -exp(T ω ( f θ j ,a j j (x ( ) , e ( ) j ), x ( ) -j ) -1) ] + λ S i,j a i,j + λ D d k=1 tr A k k!   end for</formula><p>Return A and θ ( * ) : cst() represents the copy by value operator transforming the input into a constant with the same value but zero gradient. With this trick the value of ai,j is equal to H(li,j + a i,j ) (forward pass) but its gradient w.r.t a i,j is equal to ∇ a i,j sigmoid(li,j + a i,j ) (backward pass).</p><p>I. Linear:</p><formula xml:id="formula_44">X i = j∈Pa(i) a i,j X j + E i , where a i,j ∼ N (0, 1) II. Sigmoid AM: X i = j∈Pa(i) f i,j (X j ) + E i , where f i,j (x j ) = a • b•(x j +c) 1+|b•(x j +c)| with a ∼ Exp(4) + 1, b ∼ U([-2, -0.5] ∪ [0.5, 2]) and c ∼ U([-2, 2]). III. Sigmoid Mix: X i = f i ( j∈Pa(i) X j + E i )</formula><p>, where f i is as in the previous bullet-point.</p><p>IV. GP AM: X i = j∈Pa(i) f i,j (X j ) + E i where f i,j is an univariate Gaussian process with a Gaussian kernel of unit bandwidth.</p><p>V. GP Mix:</p><formula xml:id="formula_45">X i = f i ([X Pa(i) , E i ])</formula><p>, where f i is a multivariate Gaussian process with a Gaussian kernel of unit bandwidth.</p><p>VI. NN:</p><formula xml:id="formula_46">X i = f i (X Pa(i) , E i ),</formula><p>with f i a 1-hidden layer neural network with 20 tanh units, with all neural weights sampled from N (0, 1).</p><p>The generators Sigmoid AM, GP AM and GP Mix used for the validation of the CAM algorithm <ref type="bibr" target="#b58">(Peters et al., 2014)</ref> can be found at <ref type="url" target="https://github.com/cran/CAM">https://github.com/cran/CAM</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental settings</head><p>The SAM algorithm is implemented in Python 3.5 with Pytorch 1.4 library for tensor calculation with Cuda 10.0. The datasets and the SAM algorithm used in these experiments are available at <ref type="url" target="https://github.com/Diviyan-Kalainathan/SAM">https://github.com/Diviyan-Kalainathan/SAM</ref>. It is specifically designed to run on GPU devices. In this work we use an Nvidia RTX 2080Ti graphics card with 12 GB memory.</p><p>Each causal mechanism fj is sought as a 2-hidden layer NN with 20 neurons, using tanh activation. Note that this activation function enables to represent linear mechanisms when deemed appropriate.</p><p>The discriminator is a 2-hidden layer NN with n D h = 200 LeakyReLU units on each layer and batch normalization <ref type="bibr" target="#b28">(Ioffe and Szegedy, 2015)</ref>. Structural gates a i,j are initialized to 0 with probability 1/2, except for the self-loop terms a i,i set to 0. SAM is trained for n iter = 3, 000 epochs using Adam (Kingma and Ba, 2014) with initial learning rate 0.01 for the generators and 0.001 for the discriminator.</p><p>In all experiments, we set the acyclicity penalization weight to</p><formula xml:id="formula_47">λ D = 0 if t &lt; 1, 500 0.01 × (t -1, 500) otherwise (27)</formula><p>with t the number of epochs: the first half of the training does not take into account the acyclicity constraint and focuses on the identification of the Markov blankets for each variable; the acyclicity constraint intervenes in the second half of the run and its weight increases along time. At the end of the learning, the value of λ D takes a sufficiently high value such that all resulting graphs presented in the experiments of this section are acyclic graphs.</p><p>To identify appropriate values for the main sensitive SAM parameters λ S (respectively λ F ), we applied a grid search on domain 0, 2 (resp. 0, 0.002 ) while keeping the other parameters with their default values ; each candidate (λ S , λ F ) is assessed over the problem set involving 20 variables synthetic graphs in each of the above-mentioned six categories.</p><p>The performance indicator is the area under the Precision-Recall curve (AUPR, see section 6.3). The AUPR curves for each set of parameters are displayed on Figure <ref type="figure" target="#fig_5">4</ref>, the greener the better.</p><p>First we observe that the most sensitive parameter is λ S , which controls the sparsity of the graph. The best values of λ S are between 0.002 and 0.02 depending on the graph. The parameter λ F controlling the complexity of the causal mechanisms is less sensitive. Still, it is observed that a low λ F value is preferable on datasets involving complex mechanisms and complex interactions between the parent variables such as the datasets Sigmoid Mix or NN, enabling SAM to flexibly reproduce the data. For simple datasets generated with simpler mechanisms such as Sigmoid AM, better results are obtained with higher values of λ F which imposes more constraints on the mechanisms of the model thus avoiding overfitting. The hyper-parameter configuration is set to (λ S = 0.02, λ F = 2 × 10 -6 ) in the comparative benchmark evaluation presented in next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sensitivity to SAM weights initialization</head><p>The variability of the results w.r.t. the initialization of both generator and adversarial networks is assessed by considering 100 independent SAM runs on a 20 variable graph with 500 data points generated with multivariate Gaussian process as causal mechanisms (FCM category V, section 5.1). <ref type="foot" target="#foot_6">10</ref>Figure <ref type="figure" target="#fig_7">6</ref> displays the confidence scores: the 30 green (i, j) dots correspond to true positives where over 50% runs rightly select the X i → X j edge; blue dots correspond to true negatives (less than 50% runs select a wrong X i → X j edge); the 9 red dots correspond to false positive (more than 50% runs select a wrong edge) and 14 yellow dots correspond to false negative (50% runs fail to select a true edge).</p><p>By inspecting a low confidence case (54% runs select the true direction X 1 → X 2 vs 35% for the wrong direction X 2 → X 1 ), the mistakes can be explained as variable X 2 has a single parent (Figure <ref type="figure" target="#fig_6">5</ref>). As there is no v-structure, SAM can uniquely rely on the functional fit score to orient this edge (like in pairwise methods), which makes the decision more uncertain. Note that due to the DAG penalization constraint, the algorithm cannot choose at the same time X 1 → X 2 and X 2 → X 1 in a same run.  In a word, the algorithm is sensitive to the initialization of the weights. This sensitivity and the variance of the results is addressed by averaging: running SAM multiple times and retaining the edges selected in a majority of runs.</p><p>Figure <ref type="figure">7</ref>: Averaged AuPR vs the number of runs, with and without the DAG constraint. The weight of the DAG constraint is given by Equation <ref type="formula">27</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of the DAG constraint</head><p>The impact of the DAG constraint is assessed by running SAM with and without the acyclicity penalization constraint (with λ D = 0 in the latter case). The experiments consider the same setting as above (section 5.3), and the results are displayed on Figure <ref type="figure" target="#fig_9">9</ref>. The average score increases with the number of runs and reaches a plateau, while the variance of the results decreases.</p><p>While SAM retrieves almost the same true and false positive edges (respectively in green and yellow), it retrieves a lot more false negative edges (particularly so under the diagonal). This is explained as SAM tends to retrieves the Markov blanket of each node; when there is no DAG constraint, it tends to retrieve edges in both directions, e.g. both X 1 → X 2 and X 2 → X 1 edges are selected almost 100% of the time. Additionally, it tends to retrieve the spouse nodes, e.g. retaining the edge X 8 → X 6 as both nodes have a common child in the true graph (Figure <ref type="figure" target="#fig_8">8</ref>). This edge X 6 -X 8 is not in the true DAG skeleton, but it is in the moralized graph of the true DAG. Therefore, removing the acyclicity constraint (pink curve in Figure <ref type="figure">7</ref>) increases the number of false positives and degrades the global score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sensitivity to graph density</head><p>The variability of the results w.r.t. the graph density is assessed by considering 20 variables graphs of different densities with 500 data points and generated with Gaussian process as causal mechanisms (FCM category V, section 5.1).</p><p>Figure <ref type="figure" target="#fig_10">10</ref> displays the area under the precision-recall curve and area under the ROC curve (AUPR and AUC, see section 6.3) for different densities of graphs from 0.1 to 0.95.</p><p>The best result is obtained for a density of 0.2. It corresponds to an average of almost 2 parents per variable. We observe that this score is better than for a density of 0.1 (with almost 1 parent per variables). It is explained by the fact that with 2 parents per variables there are v-structures which appear, which facilitates the orientation of the edges. Otherwise when the density is greater than 0.2, we observe that the results slightly decrease with the density. There are indeed more edges to recover and it becomes more difficult to find them all. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental validation on causal discovery benchmarks</head><p>The goal of the validation is to experimentally answer two questions. The first one regards SAM performance compared to the state of the art, depending on whether the underlying joint distribution complies with the usual assumptions (Gaussian distributions for the variables and the noise, linear causal mechanisms). The second question regards the merits and drawbacks of SAM strategy of learning non-linear causal mechanisms, and relying on adversarial learning.</p><p>This section first describes different SAM variants used in the experiments, followed by the baseline algorithms and their hyper-parameter settings. Then we describe the performance indicators used in the benchmarks. Subsection 6.4 reports on the experimental results obtained on synthetic datasets of 20 and 100 variables. Realistic biological data coming from the SynTREN simulator (Van den <ref type="bibr" target="#b77">Bulcke et al., 2006)</ref> on 20-and 100-node graphs, and from GeneNetWeaver <ref type="bibr" target="#b65">(Schaffter et al., 2011)</ref> on the DREAM4 and DREAM5 challenges are thereafter considered (section 6.5), and we last consider the extensively studied flow cytometry dataset <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref> (section 6.6). A t-test is used to assess whether the score difference between any two methods is statistically significant with a p-value 0.001. The detail of all results is given in Appendix D, reporting the average performance indicators, standard deviation, and computational cost of all considered algorithms. A sensitivity analysis to the sample size is given in Appendix E. Appendix F reports a comparison of the SAM algorithm with pairwise methods for the task of Markov equivalence class disambiguation. Finally, an analysis of the robustness of the various methods to non-Gaussian noise is presented in appendix G.</p><p>For convenience and reproducibility, all considered algorithms have been integrated in the publicly available CausalDiscovery Toolbox, 11 including the most recent baseline versions at the time of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Different SAM variants</head><p>In the benchmarks, four variants have been considered: the full SAM (Alg. 1) and three lesioned variants designed to assess the benefits of non-linear mechanisms and adversarial training. Specifically, SAM-lin desactivates the non-linear option and only implements linear causal mechanisms, replacing Equation ( <ref type="formula" target="#formula_19">11</ref>) with:</p><formula xml:id="formula_48">Xj = d i=1 W j,i a j,i X i + W j,d+1 E j + W j,0<label>(28)</label></formula><p>A second variant, SAM-mse, replaces the adversarial loss with a standard mean-square error loss, replacing the f-gan term in Equation ( <ref type="formula" target="#formula_33">21</ref>) with</p><formula xml:id="formula_49">1 n d j=1 n =1 (x ( ) j -</formula><p>x( ) j ) 2 . A third variant, SAM-lin-mse, involves both linear mechanisms and mean square error losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baseline algorithms</head><p>The following algorithms have been used, with their default parameters: the score-based methods GES <ref type="bibr" target="#b9">(Chickering, 2002)</ref> and GIES <ref type="bibr" target="#b23">(Hauser and Bühlmann, 2012)</ref> with Gaussian scores; the hybrid method MMHC <ref type="bibr" target="#b76">(Tsamardinos et al., 2006)</ref>, the L 1 penalized method for causal discovery CCDr <ref type="bibr" target="#b2">(Aragam and Zhou, 2015)</ref>, the LiNGAM algorithm <ref type="bibr" target="#b68">(Shimizu et al., 2006)</ref> and the causal additive model CAM <ref type="bibr" target="#b58">(Peters et al., 2014)</ref>. Lastly, the PC algorithm <ref type="bibr" target="#b71">(Spirtes et al., 2000)</ref> has been considered with four conditional independence tests in the Gaussian and non-parametric settings:</p><p>• PC-Gauss: using a Gaussian conditional independence test on z-scores;</p><p>• PC-HSIC: using the HSIC independence test <ref type="bibr" target="#b83">(Zhang et al., 2012)</ref> with a Gamma null distribution <ref type="bibr" target="#b19">(Gretton et al., 2005)</ref>;</p><p>• PC-RCIT: using the Randomized Conditional Independence Test (RCIT) with random Fourier features <ref type="bibr" target="#b74">(Strobl et al., 2017)</ref>;</p><p>• PC-RCOT: the Randomized conditional Correlation Test (RCOT) <ref type="bibr" target="#b74">(Strobl et al., 2017)</ref>.</p><p>PC, 12 GES and LINGAM versions are those of the pcalg package <ref type="bibr" target="#b34">(Kalisch et al., 2012)</ref>. MMHC is implemented with the bnlearn package <ref type="bibr" target="#b66">(Scutari, 2009)</ref>. CCDr is implemented with the sparsebn package <ref type="bibr" target="#b3">(Aragam et al., 2017)</ref>.</p><p>The GENIE3 algorithm <ref type="bibr" target="#b29">(Irrthum et al., 2010)</ref> is also considered, though it does not focus on DAG discovery per se as it achieves feature selection, retains the Markov Blanket of each variable using 11. <ref type="url" target="https://github.com/diviyan-kalainathan/causaldiscoverytoolbox">https://github.com/diviyan-kalainathan/causaldiscoverytoolbox</ref>. 12. The more efficient order-independent version of the PC algorithm proposed by <ref type="bibr" target="#b11">Colombo and Maathuis (2014)</ref> is used. random forest algorithms. Nevertheless, this method won the DREAM4 In Silico Multifactorial challenge <ref type="bibr">(Marbach et al., 2009)</ref>, and is therefore included among the baseline algorithms (using the GENIE3 R package).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance indicators</head><p>For the sake of robustness, 16 independent runs have been launched for each dataset-algorithm pair with a bootstrap ratio of 0.8 on the observational samples. The average causation score c i,j for each edge X i → X j is measured as the fraction of runs where this edge belongs to G. When an edge is left undirected, e.g with PC algorithm, it is counted as appearing with both orientations with weight 1/2. Area under the Precision Recall Curve (AUPR) and Area under the Receiver Operating Characteristic Curve (AUC) A true positive is an edge X i → X j of the true DAG G which is correctly recovered by the algorithm; T p is the number of true positive. A false negative is an edge of G which is missing in G; F n is the number of false negatives. A false positive is an edge in G which is not in G (reversed edges and edges which are not in the skeleton of G); F p is the number of false positives. The precision-recall curve, showing the tradeoff between precision (T p /(T p + F p )) and recall (T p /(T p + F n )) for different causation thresholds (Figure <ref type="figure" target="#fig_13">14</ref>), is summarized by the Area under the Precision Recall Curve (AUPR), ranging in [0,1], with 1 being the optimum. The Receiver Operating Characteristic Curve show the the relationship between the sensitivity (T p /(T p + F n )) and the specificity (F p /(F p + T n )). It can be summarized by the Area under the Receiver Operating Characteristic Curve (AUC) ranging in [0,1], with 1 being the optimum. <ref type="foot" target="#foot_7">13</ref>Structural Hamming Distance Another performance indicator used in the causal graph discovery framework is the Structural Hamming Distance (SHD) <ref type="bibr" target="#b76">(Tsamardinos et al., 2006)</ref>, set to the number of missing edges and redundant edges in the found structure. This SHD score is computed in the following by considering all edges X i → X j with c i,j &gt; .5. Note that a reversal error (retaining X j → X i while G includes edge X i → X j ) is counted as a single mistake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHD(</head><formula xml:id="formula_50">Â, A) = i,j | Âi,j -A i,j | - 1 2 i,j (1 -max(1, Âi,j + A j,i )),<label>(29)</label></formula><p>with A (respectively Â) the adjacency matrix of G (resp. the found causal graph G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experiments on synthetic datasets</head><p>We first consider the 6 types of datasets with different causal mechanisms presented in section 5. 1. 14  The synthetic datasets include 10 DAGs with 20 variables and 10 DAGs with 100 variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20">variable-graphs</head><p>The comparative results (Figure <ref type="figure" target="#fig_11">11</ref>) demonstrate SAM robustness in term of Area under the Precision Recall Curve (AUPR) on all categories of 20-node graphs. Specifically, SAM is dominated by PC-G, GES and CCDr on linear mechanisms and by CAM for datasets with additive noise, reminding that PC-G, GES and CCDr (resp. CAM) specifically focuses on linear (resp. additive noise) mechanisms. Note that, while the whole ranking of the algorithms may depend on the considered performance indicator, the best performing algorithm is most often the same regardless of whether the AUPR, the AUC or the Structural Hamming distance is considered. For non-linear cases with complex interactions (the Sigmoid Mix and NN cases), SAM significantly outperforms other non-parametric methods such as PC-HSIC, PC-RCOT and PC-RCIT. In the linear Gaussian setting, SAM aims to the Markov equivalence class of the true graph (under causal Markov and faithfulness assumptions) and performs less well than for e.g. the GP mix where SAM can exploit both conditional independence relations and distribution asymmetries. Though seemingly counter-intuitive, a graph with more complex interactions between noise and variables may be actually easier to recover than a graph generated with simple mechanisms (see also <ref type="bibr" target="#b79">Wang and Blei (2018)</ref>).</p><p>The SAM computational cost is bigger than for simple linear methods such as GES or PC-Gauss, but often lower than the other non-linear methods such as CAM or PC-HSIC (Table <ref type="table">3</ref> in Appendix D).</p><p>The lesioned versions, SAM-lin, SAM-mse and SAM-line-mse have significantly worse performances than SAM (except for the linear mechanism and additive Gaussian noise cases), demonstrating the merits of the NN-based and adversarial learning approach in the general case.</p><p>Figure <ref type="figure" target="#fig_2">12</ref>: Performance of causal graph discovery methods on 100-node synthetic graphs measured by the Area under the Precision Recall Curve (the higher, the better); the error bar indicates the standard deviation. On datasets relying on Gaussian processes, CAM tops the leaderboard by a significant margin as its search space matches the sought causal mechanisms. SAM demonstrates its robustness with respect to the underlying generative models (better seen in color).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>100-variable graphs</head><p>The comparative results on the 100-node graphs (Figure <ref type="figure" target="#fig_2">12</ref>) confirm the good overall robustness of SAM. As could have been expected, SAM is dominated by CAM on the GP AM, GP Mix and Sigmoid AM settings; indeed, focusing on the proper causal mechanism space yields a significant advantage, all the more so as the number of variables increases. Nevertheless, SAM does never face a catastrophic failure, and it even performs quite well on linear datasets. A tentative explanation is based on the fact that the tanh activation function enables to capture linear mechanisms; another explanation is based on the adversarial loss, empirically more robust than the MSE loss in high-dimensional problems.</p><p>In terms of computational cost, SAM scales well at d = 100 variables even when using a CPU, particularly so when compared to its best competitor CAM, that uses a combinatorial graph search. The PC-HSIC algorithm had to be stopped after 50 hours; more generally, constraint-based methods based on the PC algorithm do not scale well w.r.t. the number of variables, when using costly non-linear conditional independence tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Simulated biological datasets</head><p>The SynTREN (Van den <ref type="bibr" target="#b77">Bulcke et al., 2006)</ref> and GeneNetWeaver (GNW) <ref type="bibr" target="#b65">(Schaffter et al., 2011)</ref> simulators of genetic regulatory networks have been used to generate observational data reflecting realistic complex regulatory mechanisms, high-order conditional dependencies between expression patterns and potential feedback cycles, based on an available causal model.  <ref type="bibr" target="#b46">(Mendes et al., 2003)</ref>. Overall, ten 10-nodes and ten 100-nodes graphs have been considered. <ref type="foot" target="#foot_9">15</ref> For each graph, 500-sample datasets are generated by SynTREN.</p><p>Likewise, the comparative results on all SynTREN graphs (Figure <ref type="figure" target="#fig_12">13</ref>) demonstrate the good performances of SAM. Overall, the best performing methods take into account both distribution asymmetry and multivariate interactions. Constraint-based methods are hampered by the lack of v-structures, preventing the orientation of many edges to be based on CI tests only (PC-HSIC algorithm was stopped after 50 hours and LiNGAM did not converge on any of the datasets). The benefits of using non-linear mechanisms on such problems are evidenced by the difference between SAM-lin-mse and SAM-mse (Appendix D). The Precision-Recall curve is displayed on Figure <ref type="figure" target="#fig_13">14</ref> for representative 20-node and 100-node graphs, confirming that SAM can be used to infer networks having complex distributions, complex causal mechanisms and interactions.</p><p>GeneNetWeaver simulator -DREAM4 Five 100-nodes graphs generated using the GeneNetWeaver simulator define the In Silico Size 100 Multifactorial challenge track of the Dialogue for Reverse Engineering Assessments and Methods (DREAM) initiative. These graphs are sub-networks of transcriptional regulatory networks of E. coli and S. cerevisiae; their dynamics are simulated using a kinetic gene regulation model, with noise added to both the dynamics of the networks and the measurement of expression data. Multifactorial perturbations are simulated by slightly increasing or decreasing the basal activation of all genes of the network simultaneously by different random amounts. In total, the number of expression conditions for each network is set to 100. As the   The comparative results on these five graphs (Figure <ref type="figure" target="#fig_14">15</ref>) show that GENIE3 outperforms all other methods on networks 1, 2 and 5, while SAM is better on network 3. The Precision/Recall curves (Figure <ref type="figure" target="#fig_15">16</ref>) show that SAM is slightly better than GENIE3 in the low recall region, but worst in the high recall region. Overall, on such complex problem domains, it seems preferable to make few assumptions on the underlying generative model (like GENIE3 and SAM), while being able to capture high-order conditional dependencies between variables. Note that LiNGAM did not converge on this Dream4 dataset.</p><p>GeneNetWeaver simulator -DREAM5 The largest three networks of the DREAM5 challenge <ref type="bibr" target="#b44">(Marbach et al., 2012)</ref> are considered to assess the scalability of SAM. Network 1 is a simulated network with simulated expression data (GeneNetWeaver software), while both other expression datasets are real expression data collected for E. coli (Network 3) and S. cerevisiae (Network 4). <ref type="foot" target="#foot_10">16</ref>On these datasets, the set T of potential causes (Transcription Factors or TF) is known and constitutes a subset of the genes (T ⊂ G). The task is to infer all directed edges (t, g) with t ∈ T and g ∈ G. The ground truth graph is cyclical but self-regulatory relationships are excluded. The number of available transcription factors, genes and observations is displayed on Table <ref type="table" target="#tab_0">1</ref>. SAM is adapted to the specifics of the DREAM5 problems by removing the acyclicity constraint (λ D = 0); all other hyperparameters are set to their values used in this section; the edge scores are averaged on 32 runs. SAM is compared with the best results reported by the organizers of the challenge: the Trustful Inference of Gene REgulation using Stability Selection (TIGRESS) <ref type="bibr" target="#b22">(Haury et al., 2012)</ref>, the Context likelihood of relatedness (CLR) <ref type="bibr" target="#b15">(Faith et al., 2007)</ref>, the Algorithm for the Reconstruction of Accurate Cellular Networks (ARACNE) <ref type="bibr" target="#b45">(Margolin et al., 2006)</ref>, the Max-Min Parent and Children algorithm (MMHC) <ref type="bibr" target="#b75">(Tsamardinos et al., 2003)</ref>, the Markov blanket algorithm (HITON-PC) <ref type="bibr" target="#b1">(Aliferis et al., 2010)</ref>, the GENIE3 algorithm <ref type="bibr" target="#b29">(Irrthum et al., 2010)</ref> and the ANOVA algorithm <ref type="bibr" target="#b37">(Küffner et al., 2012)</ref>. For SAM and all other methods, the AuPR score is computed with the same evaluation script used in the challenge. <ref type="foot" target="#foot_11">17</ref>The results are displayed on Figure <ref type="figure" target="#fig_0">17</ref> (details are given in Table <ref type="table" target="#tab_13">16</ref>, Appendix D). A first remark is that all methods present degraded performance on Networks 3 and 4; a tentative interpretation is that the set of interactions for real data is not always accurate nor complete.</p><p>Figure <ref type="figure" target="#fig_0">17</ref>: Performance of causal graph discovery methods on the three networks of the Dream5 Challenge measured by the Area under the Precision Recall Curve (the higher, the better). SAM achieves the best performance on NET1, while ANOVA is better on NET3. On NET4, all results are very low (better seen in color).</p><p>On Network 1, the best results are obtained by SAM, GENIE3 and TIGRESS, with similar performances. A tentative interpretation is that, without the acyclicity constraint, SAM tackles gene regulatory inference through selecting the relevant features to predict each target gene, akin GENIE3 and TIGRESS. The main difference is that GENIE3 aggregates the features selected by regression with decision trees, while TIGRESS aggregates the features selected by LARS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Real-world biological data</head><p>This well-studied protein network problem is associated with gene expression data including 7,466 observational samples for 11 proteins (variables). The signaling molecule causal graph, conventionally accepted as ground truth and used to measure the performance of the different causal discovery methods, is displayed on Figure <ref type="figure" target="#fig_8">18</ref>. As this network contains feedback loops, SAM is launched without the acyclicity penalization term on this dataset.</p><p>Figure <ref type="figure" target="#fig_8">18</ref>: Conventionally accepted signaling molecule interactions between the 11 variables of the dataset: PKC, PLC γ , PIP3, PIP2, Akt, PKA, Raf, Mek1/2, Erk1/2, p38 and JNK. From <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref>.</p><p>The same experimental setting is used as for the other problems. According to the AUPR indicator (cf. Figure <ref type="figure" target="#fig_9">19</ref> and 20), SAM significantly outperforms the other methods. Notably, SAM recovers the transduction pathway raf →mek→erk corresponding to direct enzyme-substrate causal effect <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Perspectives</head><p>The main contribution of the paper is to propose a new causal discovery method, exploiting both structural independence and distributional asymmetries through optimizing structural and functional Figure <ref type="figure" target="#fig_9">19</ref>: Performance of causal graph discovery methods on the protein network problem <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref>. Area under the Precision Recall curve (the higher the better). SAM significantly outperforms all other methods on this dataset (better seen in color). criteria. This framework is implemented in the SAM algorithm,<ref type="foot" target="#foot_12">foot_12</ref> leveraging the representational power of Generative Adversarial Neural networks (GANs) to learn a generative model using stochastic gradient descent, and enforcing the discovery of sparse acyclic causal graphs through adequate regularization terms.</p><p>The choices made in the construction of the model (joint log-likelihood estimation of the conditional distributions with the use of an adversarial f-gan neural network; usage of structural, functional and acyclicity constraints) are supported by a theoretical analysis.</p><p>In the general case, the identifiability of the causal graph with neural networks as causal mechanisms remains an open question, left for further work. In practice, SAM robustness is supported by extensive empirical evidence across diverse synthetic, realistic and real-world problems, suggesting that SAM can be used as a powerful tool for the practitioner in order to prioritize exploratory experiments when working on real data with no prior information about neither the type of functional mechanisms involved, nor the underlying data distribution.</p><p>Lesion studies are conducted to assess whether and when it is beneficial to learn non-linear mechanisms and to rely on adversarial learning as opposed to MSE minimization. As could have been expected, in particular settings SAM is dominated by algorithms specifically designed for these settings, such as CAM <ref type="bibr" target="#b7">(Bühlmann et al., 2014)</ref> in the case of additive noise model and Gaussian process mechanisms, and GENIE3 when facing causal graphs with feedback loops for some networks. Nevertheless, SAM most often ranks first and always avoids catastrophic failures. SAM has good overall computational efficiency compared to other non-linear methods as it uses an embedded framework for structure optimization, where the mechanisms and the structure are simultaneously learned within an end-to-end DAG learning framework. It can also easily be trained on a GPU device, thus leveraging on massive parallel computation power available to learn the DAG mechanisms and the adversarial neural network. SAM scalability is demonstrated on the Network 1 of the DREAM5 challenge, obtaining very good performances with a relatively high number of variables (ca 1,500).</p><p>This work opens up four avenues for further research. An on-going extension regards the case of categorical and mixed variables, taking inspiration from discrete GANs <ref type="bibr" target="#b24">(Hjelm et al., 2017)</ref>. Another perspective is to relax the causal sufficiency assumption and handle hidden confounders, e.g. by introducing statistical dependencies between the noise variables attached to different variables <ref type="bibr" target="#b63">(Rothenhäusler et al., 2015)</ref>, or creating shared noise variables <ref type="bibr" target="#b32">(Janzing and Schölkopf, 2018)</ref> or proxies of confounders <ref type="bibr" target="#b80">(Wang and Blei, 2021)</ref>.</p><p>A longer term perspective is to extend SAM to simulate interventions on target variables. Lastly, the case of causal graphs with cycles will be considered, leveraging the power of recurrent neural nets to define a proper generative model from a graph with feedback loops.</p><p>If we assume CMA, CFA and CSA: i) For every DAG G in the equivalence class of G, L n S ( G, D) -L n S (G, D) converges to zero in probability when n tends to infinity. ii) For every DAG G not in the equivalence class of G, there exists λ S &gt; 0 such that P(L n S ( G, D) &gt; L n S (G, D)) goes toward 1 when n tends to infinity.</p><p>Proof i) According to Chickering (2013) (Theorem 2), for every DAG G in the equivalence class of G, there exists a sequence of distinct edge reversals in G with the following properties:</p><formula xml:id="formula_51">• Each edge reversed in G is a covered edge (an edge X i → X j is said covered in G if X Pa(j; G) = X Pa(i; G) ∪ X i ).</formula><p>• After each reversal, G is a DAG and G is equivalent to G.</p><p>• After all reversals, G = G.</p><p>Let G be defined from G by reversing a single covered edge X i → X j .</p><p>Let us compare the two quantities</p><formula xml:id="formula_52">I( G ) = d j=1 I(X j , X Pa(j; G ) |X Pa(j; G ) ) and I( G) = d j=1 I(X j , X Pa(j; G) |X Pa(j; G) ) : ∆I = I( G ) -I( G) = I(X Pa(j; G ) , X j |X Pa(j; G ) ) + I(X Pa(i; G ) , X i |X Pa(i; G ) ) -I(X Pa(j; G) , X j |X Pa(j; G) ) -I(X Pa(i; G) , X i |X Pa(i; G) ) = I(X j , X -j ) -I(X j , X Pa(j; G ) ) + I(X j , X -j ) -I(X i , X Pa(i; G ) ) -I(X j , X -j ) -I(X j , X Pa(j; G) ) + I(X j , X -j ) -I(X i , X Pa(i; G) ) = -I(X j , X Pa(j; G ) ) -I(X i , X Pa(i; G ) ) + I(X j , X Pa(j; G) ) + I(X i , X Pa(i; G) )</formula><p>By definition of a covered edge, we have X Pa(j; G) = X Pa(i; G) ∪ X i and after the edge reversal in G , X Pa(j; G ) = X Pa(i;G) and X Pa(i; G ) = X Pa(i;G) ∪ X j . Then:</p><formula xml:id="formula_53">∆I = -I(X j , X Pa(i; G) ) -I(X i , X Pa(i; G) ∪ X j ) + I(X j , X Pa(i; G) ∪ X i ) + I(X i , X Pa(i; G) ) = I(X j , X Pa(i; G) ∪ X i ) -I(X j , X Pa(i; G) ) -I(X i , X Pa(i; G) ∪ X j ) -I(X i , X Pa(i; G) ) = -I(X j , X i |X Pa(i; G) ) + I(X i , X j |X Pa(i; G) ) = 0</formula><p>Therefore for every DAG G in the equivalence class of G there is a sequence of covered edge reversals that do not change the global conditional mutual information score and such that after all reversals G = G, thus I( G) = I(G).</p><p>Therefore, for every DAG G in the equivalence class of G, if we now compare the structural losses of G and G, we obtain :</p><formula xml:id="formula_54">∆L n S = L n S ( G, D) -L n S (G, D) = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) + λ S | G| - d j=1 În (X j , X Pa(j;G) |X Pa(j;G) ) -λ S |G|</formula><p>We know that | G| = |G| and I( G) = I(G), thus we obtain:</p><formula xml:id="formula_55">∆L n S = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) -I( G) + I(G) - d j=1 În (X j , X Pa(j;G) |X Pa(j;G) ) As d j=1</formula><p>În (X j , X Pa(j;G) |X Pa(j;G) ) converges toward I(G) in probability for any graph G, it gives the result.</p><p>ii) a) Consider some graph G that implies an independence assumption that G does not support. We must have:</p><formula xml:id="formula_56">d j=1 I(X j , X Pa(j; G) |X Pa(j; G) ) - d j=1 I(X j , X Pa(j;G) |X Pa(j;G) ) = ∆ &gt; 0<label>(30)</label></formula><p>Therefore,</p><formula xml:id="formula_57">∆L n S = L n S ( G, D) -L n S (G, D) = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) + λ S | G| - d j=1 În (X j , X Pa(j;G) |X Pa(j;G) ) -λ S |G| = d j=1 În (X j , X Pa(j; G) |X Pa(j; G) ) -I( G) + ∆ + I(G) - d j=1 În (X j , X Pa(j;G) |X Pa(j;G) ) + λ S (| G| -|G|) Therefore, ∆L n S converges toward L = ∆ + λ S (| G| -|G|) in probability. Since G corresponds to more independence assumptions than G, there is a lower number of edges in G than in G. If λ S is chosen such that 0 &lt; λ S &lt; ∆ |G|-| G| , then L &gt; 0. Thus, P(L n S ( G, D) &gt; L n S (G, D))</formula><p>goes toward 1 when n tends to infinity. b) Now, if we assume that G implies all the independence assumptions in G, but that G implies an independence assumption that G does not, we have:</p><formula xml:id="formula_58">d j=1 I(X j , X Pa(j; G) |X Pa(j; G) ) - d j=1 I(X j , X Pa(j;G) |X Pa(j;G) ) = 0<label>(31)</label></formula><p>Therefore, ∆L n S converges toward λ S (| G| -|G|) in probability. Now, since G corresponds to fewer independence assumptions than G, there is a higher number of edges in G than in G. Thus, | G| &gt; |G|. Then, P(L n S ( G, D) &gt; L n S (G, D)) goes toward 1 when n tends to infinity.</p><p>Both results i) and ii) establish the consistency of the structural loss L n S .</p><p>Theoretical illustration with three variables DAGs A toy example with three variables A, B and C is used to show that the structural loss may lead to identify the proper orientation of causal edges, based on the Markov property of the data distribution. We assume that the associated graph skeleton is A -B -C.</p><p>In the large sample limit, if we remove the structural penalty for simplicity, as all DAGs have the same number of edges, for each of the four possible DAGs from this skeleton the structural scores are Experimental illustration Let us consider the three variables A, B, C, assuming linear dependency and Gaussian noise (therefore only conditional independence can be used to orient the edges <ref type="bibr" target="#b68">(Shimizu et al., 2006;</ref><ref type="bibr" target="#b25">Hoyer et al., 2009)</ref>).</p><p>The four possible DAGs based on this skeleton are used to generate 2,000 sample datasets, where the noise variables are independently sampled from N (0, 1). The experiments are conducted using SAM-lin (section 5.2) to avoid the structural regularization impact; the data fitting loss measured by the discriminator is averaged over 128 independent runs.</p><p>The overall loss associated to all candidate structures, respectively denoted: i) L ABC and L CBA for the chain structures <ref type="figure" target="#fig_19">21</ref> in the case where the true causal graph is a v-structure, showing that the structural loss indeed enables to statistically significantly identify the true v-structure with as few as 100 examples.  </p><formula xml:id="formula_59">A → B → C and A ← B ← C; ii) L V struct for the V structure A → B ← C; iii) L revV for the reversed V structure A ← B → C are reported on Figure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parametric loss : illustration of Markov equivalence class disambiguation</head><p>Let us consider the 2-variable toy dataset (Figure <ref type="figure" target="#fig_2">22</ref>), where all candidate structures lie in the Markov equivalence class X -Y : </p><formula xml:id="formula_60">   X ∼ U (-1, 1) E y ∼ U (-.33, .33) Y = 4(X 2 -0.5) 2 + E y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detail of the experimental results</head><p>This Appendix details the experimental results reported in section 6. CPU computational times are measured on a 48-core Intel(R) Xeon(R) CPU E5-2650 CPU. For SAM, GPU computational times are measured on a Nvidia RTX 2080Ti GPU.</p><p>As the results of the algorithms reported in this table where obtain using different programming languages, the timing information is provided for indicative purposes only. We also mention that other methods such as PC-HSIC, PC-RCOT or PC-RCIT are mostly matrix operations which could benefit greatly from being computed on a GPU. <ref type="table">Tables 3,</ref><ref type="table" target="#tab_1">4</ref> and 5 show the robustness of SAM w.r.t. diverse types of mechanisms. In terms of average precision (Table <ref type="table">3</ref>) SAM is respectively dominated on linear (resp. CAM on GP AM and Sigmoid AM) mechanisms, which is explained as GES (resp. CAM) is specifically designed to identify linear (resp. additive noise) causal mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20-variable artificial graphs</head><p>Table <ref type="table">3</ref>: Artificial graphs with 20 variables: Average Area under the precision-recall curve (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined. The computational time is per run and per graph, in seconds.  100-variable artificial graphs Tables 6, and 8 show the scalability of SAM w.r.t. the number of variables. In terms of AUPR precision (Table <ref type="table" target="#tab_3">6</ref>), SAM is dominated by CAM on the GP AM, GP Mix and Sigmoid AM causal mechanisms (noting that CAM is tailored to Gaussian Processes). Most interestingly, its computational time favorably compares to that of CAM on 100-variable problems. Note that PC-HSIC had to be stopped after 50 hours.  Realistic problems (SynTReN, GENIE3, and Cyto) Tables 9, 10 and 11 show the robustness of SAM on realistic problems generated with the SynTReN simulator (20 graphs of 20 nodes and 100 nodes) and on the so-called Sachs problem <ref type="bibr" target="#b64">(Sachs et al., 2005)</ref> (Cyto) in terms of structural Hamming distance (the lower the better).   The Dream4 In Silico Multifactorial Challenge. Tables <ref type="table" target="#tab_10">13</ref> and<ref type="table" target="#tab_11">14</ref> show the robustness of SAM on 5 artificial graphs of the Dream4 In Silico Multifactorial Challenge, respectively in terms of average area under the precision recall curve, area under the ROC curve and structural Hamming distance. GENIE3 achieves the best performance on network 1, 2 and 5, and SAM is first on network 3. GENIE 3 and SAM achieve similar results on network 4.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sensitivity to the sample size compare to other algorithms</head><p>Let us consider synthetic graphs with 20 variables with different number of points from 50 to 2000 and generated with Gaussian process as causal mechanisms with and without additive noise (FCM IV and V section 5.1). For each number of points, 10 graphs of each type are generated. We compare SAM with all the methods presented in Section 6.2 (except PC-HSIC which reaches the time limit).</p><p>Figure <ref type="figure" target="#fig_21">24</ref> displays the area under the precision-recall curve and area under the ROC curve (see section 6.3) for the graph generated with Gaussian process and additive noise (GP AM -FCM IV) and Figure <ref type="figure" target="#fig_21">24</ref> displays the scores for the graph generated with Gaussian process and non additive noise (GP Mix -FCM V)).</p><p>As mentioned in Section 6.4, the CAM algorithm is specifically designed for the setting with additive noise model and Gaussian process mechanisms. This is why CAM obtain most of the time the best results in Figure <ref type="figure" target="#fig_21">24</ref> for this type of graph. However, we observe that SAM can achieve the same results as CAM for this dataset as the sample size increases.</p><p>The datasets with non additive noise (GP Mix) is less favorable to CAM compared to SAM when the sample size is greater than 500 as display on figure 25. However, when the sample size is less than 200, the results of CAM are better than those of SAM. In general we observe that the performance of SAM is more dependant of the sample size than the other best competitors. This can be explained by the use of neural networks in SAM (generators and discriminator) which require more data to be trained. Let us consider synthetic graphs where each variable has a single parent. In this graphs there are no v-structures. The only way for SAM to distinguishing within the same Markov equivalence is to use the parametric loss. We consider 10 synthetic graphs with 20 variables generated with the different causal mechanisms presented in section 5.1 and we compare SAM with causal pairwise methods able to disambiguate Markov equivalent DAG:</p><p>• the IGCI algorithm <ref type="bibr" target="#b13">(Daniusis et al., 2012)</ref> with entropy estimator and Gaussian reference measure.</p><p>• the ANM algorithm <ref type="bibr" target="#b49">(Mooij et al., 2016)</ref> with Gaussian process regression and HSIC independence test of the residual.</p><p>• the RECI algorithm <ref type="bibr" target="#b5">(Blöbaum et al., 2018)</ref> comparing regression errors.</p><p>For all methods we provide the true skeleton of the graph. It only remains to orient each edge. The performance of each algorithm is assessed by measuring the AUPR and AUC scores of this binary decision task. The results obtained by the different methods are displayed on Table <ref type="table" target="#tab_14">17</ref> and<ref type="table" target="#tab_15">18</ref>. Unsurprisingly, no method can orient the edges of Gaussian linear datasets as the pairs are completely symmetrical. It explains why the scores provided by the various methods in section 6.4 on synthetic datasets generated with linear mechanisms are in general lower than those generated with more complex mechanisms.</p><p>We also observe in Table <ref type="table" target="#tab_14">17</ref> and 18 that the ANM method obtains the best scores for the datasets generated with additive noise (GP AM and Sigmoid AM) as the ANM algorithm is specifically designed to identify the causal direction when the noise is additive. Moreover the results are almost perfect for the GP AM dataset generated with Gaussian process and additive noise as it corresponds perfectly to the setting of the ANM method used here (Gaussian process regression and additive noise). However this ANM method is less good for other the datasets generated with other type of noises compared to the SAM algorithm.</p><p>We observe that SAM obtains overall good results for all type of mechanisms compared to the other pairwise methods in the task of disambiguating Markov equivalence classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Robustness of the various methods to non-gaussian noise</head><p>Let us consider synthetic graphs with 20 variables with 500 points and generated with the mechanisms presented in section 5.1, except that the distribution of the noise variables E i is set to σ i * U(0, 1) + µ i instead of N (µ i , σ i ). 10 DAGs are generated for each type of mechanism. All the methods presented in Section 6 are launched on these datasets. Table <ref type="table" target="#tab_16">19</ref>, 20 and 21 respectively display the AUPR, AUC and SHD scores for all the methods launched with the same hyperparameters (see Section 6). In particular, in the SAM algorithm, we keep the Gaussian noise variable as input of each generator.</p><p>We observe that SAM obtain the best scores in term of AUPR, AUC and SHD for all the datasets, except for the distribution with additive noise (GP AM and Sigmoid AM). Overall, the results are comparable to those obtained with Gaussian noise and displayed in Table <ref type="table">3</ref>, 4 and 5, showing to some extent that the SAM algorithm can be robust to a change in the noise distribution.</p><p>However, we notice that SAM actually performs better for the linear dataset when the noise is uniform instead of Gaussian. Indeed when the noise is uniform instead of Gaussian, it introduces distributional asymmetries in the data generative process, which allows SAM to leverage on the parametric fitting loss to orient edges in the Markov equivalence class of the DAG.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a Functional Causal Model (FCM) on X = [X 1 , . . . , X 5 ]. Left: causal graph G. Right: causal mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>G) :În (X j , X Pa(j; G) |X Pa(j; G)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram of the conditional generative neural network modeling the causal mechanism Xj = fj (X, E j ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: A four-variable example: diagram of the SAM structure for variables X 1 , . . . , X 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ii) sample noise vector, e ( ) = (e ( ) 1 , . . . , e ( ) d ) from multivariate normal distribution N (µ, Σ) with µ = (1, . . . , 1) and Σ = I (independent noise variables).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SAM sensitivity to λ S and λ F measured by the Area under the Precision Recall curve (AuPR) obtained for different causal graphs datasets. The graphs are generated with different causal mechanisms (Category I to VI presented in section 5.1). The color corresponds to the quality of the causal inference, the greener the better.</figDesc><graphic coords="20,90.00,171.33,432.02,297.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: View of the first four variables of the true graph.</figDesc><graphic coords="21,241.20,182.64,129.61,122.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average number of times each directed edge is selected by SAM after training. For example, the value 54 in position (1,2) indicates that the edge from variable X 1 to X 2 has been selected in 54 out of 100 runs. Green values corresponds to true positives, red values to false positive, blue values to true negatives and yellow values to false negatives.</figDesc><graphic coords="21,133.20,353.22,345.59,245.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: View of the variables 6, 8 and 9 of the true graph.</figDesc><graphic coords="23,241.20,90.86,129.60,116.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average number of times each directed edge is selected by SAM after training and without the acyclicity penalization constraint. Green values corresponds to true positives, red values to false positives, blue values to true negatives and yellow values to false negatives.</figDesc><graphic coords="23,133.20,299.19,345.60,257.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Averaged AUPR and AUC scores for different densities of graph from 0.1 to 0.95.</figDesc><graphic coords="24,154.80,208.13,302.40,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Performance of causal graph discovery methods on 20-node synthetic graphs measured by the Area under the Precision Recall Curve (the higher, the better); the error bar indicates the standard deviation. SAM ranks among the top-three methods, being only dominated for linear mechanisms and by CAM for additive noise mechanisms (better seen in color).</figDesc><graphic coords="27,90.00,90.86,431.99,222.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Performance of causal graph discovery methods on SynTREN graphs measured by the Area under the Precision Recall Curve (the higher, the better); the Figure bar indicates the standard deviation. Left: 20 nodes. Right: 100 nodes (better seen in color)..</figDesc><graphic coords="29,111.60,90.86,388.81,190.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Precision/Recall curve for two SynTREN graphs: Left, 20 nodes; Right, 100 nodes (better seen in color).</figDesc><graphic coords="30,90.00,90.86,432.01,146.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Performance of causal graph discovery methods on 5 artificial datasets of the Dream4 In Silico Multifactorial Challenge measured by the Area under the Precision Recall Curve (the higher, the better); the error bar indicates the standard deviation. GENIE3 achieves the best performance on NET1 and NET2, while SAM is first on NET3 and NET4. (better seen in color).</figDesc><graphic coords="30,90.00,285.40,432.02,324.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Precision/Recall curve for the Dream4 In Silico Multifactorial Challenge (better seen in color).</figDesc><graphic coords="31,133.20,90.86,345.62,259.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Precision/Recall curve for the protein network problem (better seen in color).</figDesc><graphic coords="34,133.20,328.86,345.62,259.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>19 : L S,A→B→C = I(A, {B, C}) + I(B, C|A) + I(C, A|B) = I(A, C) + I(A, B|C) + I(B, C|A) + I(A, C|B) L S,A←B←C = I(A, C|B) + I(B, A|C) + I(C, {A, B}) = I(A, C) + I(A, B|C) + I(B, C|A) + I(A, C|B) L S,A←B→C = I(A, C|B) + I(B, {A, C}) + I(C, A|B) = I(A, C|B) + I(A, C|B) + I(B, C) + I(B, A|C) = I(A, C|B) + I(C, {A, B} + I(A, B|C) = I(A, C) + I(A, B|C) + I(B, C|A) + I(A, C|B) L S,A→B←C = I(A, {B, C}) + I(C, {A, B}) = I(A, C) + I(A, B|C) + I(A, C) + I(B, C|A) Thus, we have L S,A→B→C = L S,A←B←C = L S,A←B→C as the three DAGs A → B → C, A ← B ← C and A ← B → C are Markov equivalent. The difference of score between the two equivalent classes A -B -C and A → B ← C is L S,A→B←C -L S,A←B←C = I(A, C) -I(A, C|B). Therefore, if A ⊥ ⊥ C and A ⊥ ⊥ C|B then L S,A→B←C &lt; L S,A←B←C and the structure v-structure A → B ← C is preferred. However if A ⊥ ⊥ C and A ⊥ ⊥ C|B, L S,A←B←C &lt; L S,A→B←C and the equivalence class A -B -C is preferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>Figure21: Overall loss of all candidate 3-variable structures (linear dependencies, Gaussian noise) versus number of samples, in the case where the sought graph is a v-structure (the lower, the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Frobenius regularization loss and fit loss for both X → Y and Y → X based on dataset plotted on Figure22: Impact of parameter λ F for models X → Y (solid lines) and Y → X (dashed lines). For sufficiently high values of λ F , the Frobenius regularization loss (in blue) and fit loss (in red) are lower for the true causal direction, showing SAM ability to leverage distributional asymmetries.</figDesc><graphic coords="48,133.20,90.86,345.60,205.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Averaged and standard deviation of AUPR and AUC for the GES, CAM and SAM methods for graphs of 20 variables with different number of points from 50 to 2000 and generated with Gaussian process and additive noise (GP AM)</figDesc><graphic coords="56,110.24,236.84,194.40,138.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="22,90.00,144.23,432.01,277.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="28,90.00,90.86,432.01,246.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="32,90.00,385.13,432.01,175.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="33,133.20,232.14,345.60,284.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dream5 challenge</figDesc><table><row><cell>Network</cell><cell cols="4"># TF # Genes # Observations # Verified interactions</cell></row><row><cell>DREAM5 Network 1 (in-silico)</cell><cell>195</cell><cell>1643</cell><cell>805</cell><cell>4012</cell></row><row><cell>DREAM5 Network 3 (E.coli)</cell><cell>334</cell><cell>4511</cell><cell>805</cell><cell>2066</cell></row><row><cell cols="2">DREAM5 Network 4 (S.cerevisiae) 333</cell><cell>5950</cell><cell>536</cell><cell>3940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Artificial graphs with 20 variables: Average Area Under the ROC Curve (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUPR</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell></cell><cell cols="4">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell><cell>CPU Time GPU Time in s.</cell></row><row><cell>PC-Gauss</cell><cell cols="4">0.37 (0.02) 0.20 (0.04) 0.39 (0.03)</cell><cell cols="2">0.60 (0.03)</cell><cell cols="2">0.27 (0.02)</cell><cell>0.24 (0.04) 0.34 (0.14)</cell><cell>1</cell></row><row><cell>PC-HSIC</cell><cell cols="4">0.35 (0.02) 0.41 (0.06) 0.38 (0.03)</cell><cell cols="2">0.41 (0.04)</cell><cell cols="2">0.27 (0.03)</cell><cell>0.34 (0.05) 0.36 (0.05)</cell><cell>46,523</cell></row><row><cell>PC-RCOT</cell><cell cols="4">0.34 (0.03) 0.43 (0.03) 0.38 (0.02)</cell><cell cols="2">0.36 (0.05)</cell><cell cols="2">0.23 (0.01)</cell><cell>0.31 (0.03) 0.34 (0.07)</cell><cell>356</cell></row><row><cell>PC-RCIT</cell><cell cols="4">0.33 (0.03) 0.36 (0.02) 0.36 (0.03)</cell><cell cols="2">0.46 (0.02)</cell><cell cols="2">0.24 (0.02)</cell><cell>0.31 (0.05) 0.34 (0.07)</cell><cell>181</cell></row><row><cell>GES</cell><cell cols="4">0.33 (0.06) 0.25 (0.03) 0.34 (0.04)</cell><cell cols="2">0.58 (0.05)</cell><cell cols="2">0.39 (0.05)</cell><cell>0.32 (0.08) 0.37 (0.12)</cell><cell>1</cell></row><row><cell>GIES</cell><cell cols="4">0.34 (0.03) 0.24 (0.03) 0.35 (0.04)</cell><cell cols="2">0.58 (0.05)</cell><cell cols="2">0.38 (0.04)</cell><cell>0.33 (0.07) 0.37 (0.11)</cell><cell>1</cell></row><row><cell>MMHC</cell><cell cols="4">0.36 (0.01) 0.16 (0.02) 0.29 (0.01)</cell><cell cols="2">0.37 (0.01)</cell><cell cols="2">0.23 (0.01)</cell><cell>0.24 (0.02) 0.28 (0.08)</cell><cell>1</cell></row><row><cell>LiNGAM</cell><cell cols="4">0.30 (0.02) 0.11 (0.01) 0.12 (0.01)</cell><cell cols="2">0.24 (0.02)</cell><cell cols="2">0.12 (0.01)</cell><cell>0.11 (0.02) 0.17 (0.08)</cell><cell>2</cell></row><row><cell>CAM</cell><cell cols="4">0.19 (0.02) 0.78 (0.06) 0.78 (0.05)</cell><cell cols="2">0.77 (0.05)</cell><cell cols="2">0.22 (0.02)</cell><cell>0.43 (0.08) 0.53 (0.27)</cell><cell>2,880</cell></row><row><cell>CCDr</cell><cell cols="4">0.49 (0.04) 0.23 (0.07) 0.38 (0.02)</cell><cell cols="2">0.55 (0.02)</cell><cell cols="2">0.43 (0.03)</cell><cell>0.29 (0.06) 0.40 (0.12)</cell><cell>2</cell></row><row><cell>GENIE3</cell><cell cols="4">0.33 (0.01) 0.48 (0.02) 0.56 (0.01)</cell><cell cols="2">0.47 (0.01)</cell><cell cols="2">0.19 (0.01)</cell><cell>0.33 (0.03) 0.40 (0.12)</cell><cell>54</cell></row><row><cell cols="5">SAM-lin-mse 0.31 (0.01) 0.21 (0.03) 0.33 (0.01)</cell><cell cols="2">0.41 (0.02)</cell><cell cols="2">0.30 (0.02)</cell><cell>0.28 (0.04) 0.30 (0.06)</cell><cell>332</cell><cell>70</cell></row><row><cell>SAM-mse</cell><cell cols="4">0.28 (0.01) 0.42 (0.07) 0.61 (0.04)</cell><cell cols="2">0.52 (0.02)</cell><cell cols="2">0.28 (0.02)</cell><cell>0.36 (0.05) 0.41 (0.13)</cell><cell>2,411</cell><cell>83</cell></row><row><cell>SAM-lin</cell><cell cols="4">0.30 (0.02) 0.24 (0.04) 0.37 (0.02)</cell><cell cols="2">0.50 (0.03)</cell><cell cols="2">0.35 (0.03)</cell><cell>0.28 (0.06) 0.34 (0.09)</cell><cell>2,526</cell><cell>110</cell></row><row><cell>SAM</cell><cell cols="4">0.32 (0.02) 0.66 (0.06) 0.82 (0.03)</cell><cell cols="2">0.62 (0.04)</cell><cell cols="2">0.48 (0.04)</cell><cell>0.55 (0.10) 0.57 (0.16)</cell><cell>13,121</cell><cell>113</cell></row><row><cell>AUC</cell><cell></cell><cell>Linear</cell><cell>GP AM</cell><cell cols="2">GP Mix</cell><cell cols="3">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell cols="2">PC-Gauss</cell><cell cols="4">0.75 (0.01) 0.61 (0.03) 0.71 (0.01)</cell><cell cols="2">0.82 (0.01)</cell><cell>0.68 (0.02)</cell><cell>0.65 (0.03) 0.70 (0.07)</cell></row><row><cell cols="2">PC-HSIC</cell><cell cols="4">0.74 (0.02) 0.76 (0.03) 0.69 (0.02)</cell><cell cols="2">0.72 (0.03)</cell><cell>0.68 (0.02)</cell><cell>0.71 (0.02) 0.72 (0.06)</cell></row><row><cell cols="2">PC-RCOT</cell><cell cols="4">0.73 (0.02) 0.76 (0.02) 0.76 (0.02)</cell><cell cols="2">0.75 (0.03)</cell><cell>0.63 (0.01)</cell><cell>0.71 (0.02) 0.72 (0.05)</cell></row><row><cell cols="2">PC-RCIT</cell><cell cols="4">0.73 (0.02) 0.73 (0.01) 0.75 (0.02)</cell><cell cols="2">0.79 (0.02)</cell><cell>0.64 (0.03)</cell><cell>0.72 (0.03) 0.73 (0.05)</cell></row><row><cell>GES</cell><cell></cell><cell cols="4">0.77 (0.03) 0.74 (0.04) 0.75 (0.03)</cell><cell cols="2">0.87 (0.03)</cell><cell>0.84 (0.03)</cell><cell>0.76 (0.07) 0.79 (0.06)</cell></row><row><cell>GIES</cell><cell></cell><cell cols="4">0.77 (0.03) 0.75 (0.03) 0.76 (0.03)</cell><cell cols="2">0.87 (0.03)</cell><cell>0.84 (0.03)</cell><cell>0.76 (0.07) 0.79 (0.06)</cell></row><row><cell>MMHC</cell><cell></cell><cell cols="4">0.80 (0.01) 0.59 (0.03) 0.72 (0.01)</cell><cell cols="2">0.81 (0.01)</cell><cell>0.67 (0.02)</cell><cell>0.67 (0.04) 0.71 (0.08)</cell></row><row><cell cols="2">LiNGAM</cell><cell cols="4">0.62 (0.01) 0.44 (0.04) 0.50 (0.02)</cell><cell cols="2">0.61 (0.02)</cell><cell>0.43 (0.02)</cell><cell>0.45 (0.04) 0.51 (0.09)</cell></row><row><cell>CAM</cell><cell></cell><cell cols="4">0.65 (0.02) 0.96 (0.02) 0.94 (0.01)</cell><cell cols="2">0.94 (0.02)</cell><cell>0.72 (0.02)</cell><cell>0.80 (0.05) 0.83 (0.12)</cell></row><row><cell>CCDr</cell><cell></cell><cell cols="4">0.82 (0.02) 0.66 (0.06) 0.68 (0.02)</cell><cell cols="2">0.80 (0.01)</cell><cell>0.82 (0.02)</cell><cell>0.69 (0.07) 0.74 (0.08)</cell></row><row><cell>GENIE3</cell><cell></cell><cell cols="4">0.77 (0.01) 0.85 (0.01) 0.92 (0.01)</cell><cell cols="2">0.89 (0.01)</cell><cell>0.65 (0.01)</cell><cell>0.78 (0.04) 0.81 (0.09)</cell></row><row><cell cols="6">SAM-lin-mse 0.81 (0.01) 0.70 (0.02) 0.73 (0.03)</cell><cell cols="2">0.85 (0.01)</cell><cell>0.79 (0.01)</cell><cell>0.74 (0.03) 0.77 (0.05)</cell></row><row><cell cols="2">SAM-mse</cell><cell cols="4">0.80 (0.01) 0.84 (0.02) 0.87 (0.02)</cell><cell cols="2">0.90 (0.01)</cell><cell>0.80 (0.01)</cell><cell>0.80 (0.03) 0.83 (0.04)</cell></row><row><cell>SAM-lin</cell><cell></cell><cell cols="4">0.77 (0.02) 0.69 (0.02) 0.75 (0.02)</cell><cell cols="2">0.83 (0.02)</cell><cell>0.79 (0.02)</cell><cell>0.74 (0.04) 0.76 (0.05)</cell></row><row><cell>SAM</cell><cell></cell><cell cols="4">0.76 (0.02) 0.90 (0.02) 0.95 (0.01)</cell><cell cols="2">0.90 (0.01)</cell><cell>0.85 (0.01)</cell><cell>0.86 (0.04) 0.87 (0.06)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Artificial graphs with 20 variables: Average Structural Hamming Distance (std. dev.) of all compared algorithms over all six types of distributions (the lower the better). Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>SHD</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>PC-Gauss</cell><cell cols="3">37.4 (1.0) 57.9 (4.0) 41.4 (2.2)</cell><cell>26.6 (1.7)</cell><cell>46.5 (2.1)</cell><cell>47.4 (2.0)</cell><cell>42.9 (9.9)</cell></row><row><cell>PC-HSIC</cell><cell cols="3">38.8 (2.0) 42.9 (3.1) 41.7 (5.2)</cell><cell>41.3 (2.2)</cell><cell>47.5 (3.3)</cell><cell>40.7 (2.0)</cell><cell>42.1 (6.1)</cell></row><row><cell>PC-RCOT</cell><cell cols="3">40.0 (1.4) 42.8 (3.2) 42.6 (1.8)</cell><cell>43.1 (2.1)</cell><cell>44.1 (0.5)</cell><cell>41.7 (2.1)</cell><cell>42.4 (2.4)</cell></row><row><cell>PC-RCIT</cell><cell cols="3">40.1 (1.2) 46.9 (1.8) 44.9 (1.7)</cell><cell>34.7 (1.1)</cell><cell>43.8 (0.5)</cell><cell>41.6 (1.8)</cell><cell>41.9 (4.2)</cell></row><row><cell>GES</cell><cell cols="3">59.2 (5.3) 70.7 (5.4) 50.2 (3.5)</cell><cell>36.0 (3.8)</cell><cell>58.6 (6.8)</cell><cell cols="2">67.3 (10.8) 57.0 (13.1)</cell></row><row><cell>GIES</cell><cell cols="2">60.8 (4.7) 71.7 (4.9)</cell><cell>50.3 (3.9)</cell><cell>35.4 (2.9)</cell><cell>61.0 (5.1)</cell><cell cols="2">69.1 (10.1) 58.0 (13.5)</cell></row><row><cell>MMHC</cell><cell cols="2">41.9 (1.4) 73.8 (5.0)</cell><cell>56.2 (1.9)</cell><cell>46.4 (1.2)</cell><cell>53.1 (1.3)</cell><cell>47.8 (2.7)</cell><cell>53.2 (10.6)</cell></row><row><cell>LiNGAM</cell><cell cols="2">40.8 (2.2) 51.3 (1.6)</cell><cell>48.3 (0.5)</cell><cell>42.2 (0.9)</cell><cell>51.0 (2.0)</cell><cell>55.9 (7.3)</cell><cell>48.3 (6.2)</cell></row><row><cell>CAM</cell><cell cols="2">79.4 (6.7) 37.4 (6.0)</cell><cell>38.3 (4.3)</cell><cell>37.4 (4.5)</cell><cell>85.1 (4.3)</cell><cell>61.4 (6.9)</cell><cell>56.5 (20.9)</cell></row><row><cell>CCDr</cell><cell cols="2">66.2 (6.2) 57.0 (5.6)</cell><cell>39.0 (1.6)</cell><cell>28.4 (2.7)</cell><cell>40.4 (1.9)</cell><cell>57.3 (9.8)</cell><cell>48.0 (14.2)</cell></row><row><cell>GENIE3</cell><cell cols="2">41.1 (3.6) 42.5 (2.9)</cell><cell>43.2 (4.1)</cell><cell>41.8 (3.5)</cell><cell>43.9 (5.2)</cell><cell>69.2 (9.2)</cell><cell>47.0 (6.9)</cell></row><row><cell cols="4">SAM-lin-mse 45.2 (1.1) 52.5 (2.3) 44.8 (1.1)</cell><cell>42.1 (1.4)</cell><cell>45.8 (1.7)</cell><cell>42.7 (1.5)</cell><cell>45.5 (3.7)</cell></row><row><cell>SAM-mse</cell><cell cols="3">47.9 (1.6) 46.6 (4.5) 34.7 (2.5)</cell><cell>37.2 (2.9)</cell><cell>53.1 (1.7)</cell><cell>43.7 (3.0)</cell><cell>43.9 (6.9)</cell></row><row><cell>SAM-lin</cell><cell cols="3">42.3 (2.3) 57.1 (3.9) 50.3 (3.7)</cell><cell>42.8 (3.9)</cell><cell>45.2 (2.3)</cell><cell>47.3 (4.0)</cell><cell>47.5 (6.1)</cell></row><row><cell>SAM</cell><cell cols="3">44.0 (2.1) 33.9 (4.8) 21.8 (2.9)</cell><cell>34.3 (2.8)</cell><cell>43.6 (2.5)</cell><cell>37.6 (4.6)</cell><cell>35.9 (8.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Artificial graphs with 100 variables: Average Area under the precision-recall curve (std. dev.) of all compared algorithms over all six types of distributions (the higher the better).</figDesc><table><row><cell>Significantly</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>Artificial graphs with 100 variables: Average Area Under the ROC Curve (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUC</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>PC-Gauss</cell><cell>0.66 (0.005)</cell><cell>0.74 (0.01)</cell><cell>0.74 (0.01)</cell><cell>0.88 (0.01)</cell><cell>0.71 (0.01)</cell><cell>0.68 (0.01)</cell><cell>0.73 (0.07)</cell></row><row><cell>PC-HSIC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PC-RCOT</cell><cell>0.66 (0.01)</cell><cell>0.79 (0.01)</cell><cell>0.77 (0.01)</cell><cell>0.82 (0.01)</cell><cell>0.69 (0.01)</cell><cell>0.68 (0.01)</cell><cell>0.73 (0.06)</cell></row><row><cell>PC-RCIT</cell><cell>0.65 (0.01)</cell><cell>0.76 (0.01)</cell><cell>0.74 (0.01)</cell><cell>0.80 (0.01)</cell><cell>0.67 (0.01)</cell><cell>0.67 (0.01)</cell><cell>0.72 (0.05)</cell></row><row><cell>GES</cell><cell>0.92 (0.01)</cell><cell>0.87 (0.01)</cell><cell>0.81 (0.01)</cell><cell>0.94 (0.01)</cell><cell>0.89 (0.01)</cell><cell>0.91 (0.01)</cell><cell>0.89 (0.04)</cell></row><row><cell>GIES</cell><cell>0.92 (0.01)</cell><cell>0.87 (0.01)</cell><cell>0.81 (0.01)</cell><cell>0.95 (0.01)</cell><cell>0.90 (0.01)</cell><cell>0.91 (0.01)</cell><cell>0.89 (0.04)</cell></row><row><cell>MMHC</cell><cell>0.69 (0.01)</cell><cell>0.74 (0.01)</cell><cell>0.75 (0.01)</cell><cell>0.89 (0.01)</cell><cell>0.71 (0.01)</cell><cell cols="2">0.69 (0.005) 0.75 (0.07)</cell></row><row><cell>LiNGAM</cell><cell>0.49 (0.01)</cell><cell>0.49 (0.01)</cell><cell>0.50 (0.01)</cell><cell>0.49 (0.01)</cell><cell>0.51 (0.01)</cell><cell>0.50 (0.02)</cell><cell>0.50 (0.01)</cell></row><row><cell>CAM</cell><cell>0.77 (0.01)</cell><cell>0.99 (0.003)</cell><cell>0.95 (0.01)</cell><cell>0.96 (0.01)</cell><cell>0.79 (0.01)</cell><cell>0.85 (0.02)</cell><cell>0.89 (0.09)</cell></row><row><cell>CCDr</cell><cell>0.75 (0.01)</cell><cell>0.75 (0.02)</cell><cell>0.66 (0.02)</cell><cell>0.80 (0.02)</cell><cell>0.81 (0.01)</cell><cell>0.80 (0.02)</cell><cell>0.76 (0.06)</cell></row><row><cell>GENIE3</cell><cell>0.76 (0.01)</cell><cell>0.97 (0.01)</cell><cell>0.97 (0.01)</cell><cell>0.97 (0.01)</cell><cell>0.88 (0.01)</cell><cell>0.83 (0.02)</cell><cell>0.90 (0.08)</cell></row><row><cell>SAM-lin-mse</cell><cell>0.77 (0.01)</cell><cell>0.76 (0.02)</cell><cell>0.69 (0.02)</cell><cell>0.85 (0.01)</cell><cell>0.82 (0.01)</cell><cell>0.78 (0.02)</cell><cell>0.78 (0.05)</cell></row><row><cell>SAM-mse</cell><cell>0.80 (0.004)</cell><cell>0.81 (0.01)</cell><cell>0.71 (0.02)</cell><cell>0.87 (0.01)</cell><cell>0.85 (0.004)</cell><cell>0.82 (0.02)</cell><cell>0.81 (0.05)</cell></row><row><cell>SAM-lin</cell><cell>0.89 (0.003)</cell><cell>0.84 (0.01)</cell><cell>0.79 (0.01)</cell><cell>0.92 (0.01)</cell><cell>0.92 (0.005)</cell><cell>0.89 (0.01)</cell><cell>0.87 (0.05)</cell></row><row><cell>SAM</cell><cell>0.92 (0.004)</cell><cell>0.93 (0.01)</cell><cell>0.92 (0.01)</cell><cell>0.95 (0.01)</cell><cell>0.95 (0.002)</cell><cell>0.96 (0.01)</cell><cell>0.93 (0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Artificial graphs with 100 variables: Average Structural Hamming Distance (std. dev.) of all compared algorithms over all six types of distributions (the lower the better). Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>SHD</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>PC-Gauss</cell><cell>251.4 (3.7)</cell><cell>239.9 (8.3)</cell><cell>216.7 (5.8)</cell><cell>141.2 (4.3)</cell><cell>236.0 (6.4)</cell><cell>241.6 (10.4)</cell><cell>221 (37.8)</cell></row><row><cell>PC-HSIC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PC-RCOT</cell><cell>257.9 (2.8)</cell><cell>221.3 (6.2)</cell><cell>217.7 (5.8)</cell><cell>176.5 (2.1)</cell><cell>246.1 (2.9)</cell><cell>244.1 (5.7)</cell><cell>227.2 (27.1)</cell></row><row><cell>PC-RCIT</cell><cell>257.4 (2.6)</cell><cell>236.3 (3.2)</cell><cell>229.1 (3.7)</cell><cell>182.3 (2.9)</cell><cell>250.0 (2.3)</cell><cell>246.0 (3.8)</cell><cell>233.5 (24.9)</cell></row><row><cell>GES</cell><cell cols="3">211.2 (16.1) 360.7 (14.9) 256.6 (10.6)</cell><cell>155.4 (7.5)</cell><cell>353.5 (22.3)</cell><cell cols="2">323.4 (35.7) 276.8 (78.5)</cell></row><row><cell>GIES</cell><cell cols="2">211.3 (13.2) 360.1 (22.4)</cell><cell>258.6 (7.3)</cell><cell>151.7 (9.1)</cell><cell>348.8 (24.1)</cell><cell cols="2">321.4 (25.5) 275.3 (77.9)</cell></row><row><cell>MMHC</cell><cell>249.9 (2.1)</cell><cell>368.4 (10.1)</cell><cell>310.9 (8.8)</cell><cell>236.2 (4.0)</cell><cell>305.8 (6.3)</cell><cell>270.8 (6.0)</cell><cell>290.3 (44.7)</cell></row><row><cell>LiNGAM</cell><cell>273.8 (5.4)</cell><cell>258.2 (2.4)</cell><cell>252.0 (1.0)</cell><cell>223.9 (1.6)</cell><cell>273.5 (2.8)</cell><cell>258.3 (6.1)</cell><cell>256.6 (17.1)</cell></row><row><cell>CAM</cell><cell>293.2 (6.7)</cell><cell>72.9 (7.5)</cell><cell>114.4 (7.3)</cell><cell>111.1 (7.4)</cell><cell>365.5 (6.2)</cell><cell>292.0 (8.2)</cell><cell>208.2 (112)</cell></row><row><cell>CCDr</cell><cell>282.6 (4.9)</cell><cell>237.1 (8.9)</cell><cell>215.7 (4.2)</cell><cell>163.8 (3.9)</cell><cell>224.6 (9.4)</cell><cell cols="2">223.3 (18.0) 224.5 (36.2)</cell></row><row><cell>GENIE3</cell><cell>243.5 (15.3)</cell><cell>257.6(18.1)</cell><cell>247.6 (11.8)</cell><cell>235.6 (20.2)</cell><cell>235.0 (17.6)</cell><cell>240.2 (14.9)</cell><cell>243.3 (8.5)</cell></row><row><cell>SAM-lin-mse</cell><cell>246.8 (1.7)</cell><cell>255.4 (0.8)</cell><cell>248.0 (0.6)</cell><cell>220.0 (0.0)</cell><cell>257.5 (0.9)</cell><cell>248.3 (2.2)</cell><cell>246.0 (11.9)</cell></row><row><cell>SAM-mse</cell><cell>233.1 (2.9)</cell><cell>239.9 (6.1)</cell><cell>241.2 (1.7)</cell><cell>214.8 (0.9)</cell><cell>268.9 (2.9)</cell><cell>236.6 (7.0)</cell><cell>239 (16.5)</cell></row><row><cell>SAM-lin</cell><cell>210.9 (4.1)</cell><cell>247.7 (6.9)</cell><cell>262.2 (7.1)</cell><cell>207.4 (7.5)</cell><cell>215.3 (4.4)</cell><cell>200.8 (8.6)</cell><cell>224.1 (23.6)</cell></row><row><cell>SAM</cell><cell>196.8 (7.9)</cell><cell>186.7 (14.5)</cell><cell>189.7 (7.8)</cell><cell>154.5 (6.7)</cell><cell>204.0 (5.5)</cell><cell cols="2">152.6 (13.9) 180.7 (22.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Realistic problems: Average area under the precision recall curve (std dev.) over 20 graphs (the higher the better). Left: 20 nodes. Middle: 100 nodes. Right: real protein network. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUPR</cell><cell cols="2">SynTREN 20 nodes SynTREN 100 nodes</cell><cell>Cyto</cell></row><row><cell>PC-Gauss</cell><cell>0.16 (0.06)</cell><cell>0.06 (0.01)</cell><cell>0.16 (0.04)</cell></row><row><cell>PC-HSIC</cell><cell>0.06 (0.01)</cell><cell>-</cell><cell>-</cell></row><row><cell>PC-RCOT</cell><cell>0.16 (0.05)</cell><cell>0.07 (0.02)</cell><cell>0.36 (0.03)</cell></row><row><cell>PC-RCIT</cell><cell>0.16 (0.05)</cell><cell>0.07 (0.01)</cell><cell>0.37 (0.04)</cell></row><row><cell>GES</cell><cell>0.14 (0.06)</cell><cell>0.06 (0.01)</cell><cell>0.14 (0.06)</cell></row><row><cell>GIES</cell><cell>0.12 (0.04)</cell><cell>0.06 (0.01)</cell><cell>0.22 (0.05)</cell></row><row><cell>MMHC</cell><cell>0.14 (0.05)</cell><cell>0.07 (0.01)</cell><cell>0.25 (0.07)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>0.16 (0.03)</cell></row><row><cell>CAM</cell><cell>0.21 (0.08)</cell><cell>0.19 (0.04)</cell><cell>0.28 (0.004)</cell></row><row><cell>CCDr</cell><cell>0.18 (0.12)</cell><cell>0.21 (0.05)</cell><cell>0.22 (0.027)</cell></row><row><cell>GENIE3</cell><cell>0.23 (0.07)</cell><cell>0.13 (0.02)</cell><cell>0.32 (0.08)</cell></row><row><cell>SAM-lin-mse</cell><cell>0.19 (0.08)</cell><cell>0.07 (0.01)</cell><cell>0.30 (0.03)</cell></row><row><cell>SAM-mse</cell><cell>0.54 (0.12)</cell><cell>0.22 (0.05)</cell><cell>0.21 (0.04)</cell></row><row><cell>SAM-lin</cell><cell>0.16 (0.09)</cell><cell>0.12 (0.03)</cell><cell>0.40 (0.03)</cell></row><row><cell>SAM</cell><cell>0.55 (0.15)</cell><cell>0.34 (0.05)</cell><cell>0.42 (0.04)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Realistic problems: Average area under the ROC curve (std dev.) over 20 graphs (the higher the better). Left: 20 nodes. Middle: 100 nodes. Right: real protein network. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUC</cell><cell cols="2">SynTREN 20 nodes SynTREN 100 nodes</cell><cell>Cyto</cell></row><row><cell>PC-Gauss</cell><cell>0.67 (0.06)</cell><cell>0.64 (0.02)</cell><cell>0.60 (0.02)</cell></row><row><cell>PC-HSIC</cell><cell>0.50 (0.002)</cell><cell>-</cell><cell>-</cell></row><row><cell>PC-RCOT</cell><cell>0.66 (0.05)</cell><cell>0.65 (0.02)</cell><cell>0.72 (0.05)</cell></row><row><cell>PC-RCIT</cell><cell>0.66 (0.05)</cell><cell>0.62 (0.01)</cell><cell>0.69 (0.04)</cell></row><row><cell>GES</cell><cell>0.74 (0.06)</cell><cell>0.80 (0.01)</cell><cell>0.63 (0.03)</cell></row><row><cell>GIES</cell><cell>0.76 (0.08)</cell><cell>0.80 (0.03)</cell><cell>0.69 (0.04)</cell></row><row><cell>MMHC</cell><cell>0.69 (0.08)</cell><cell>0.68 (0.03)</cell><cell>0.64 (0.03)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>0.44 (0.02)</cell></row><row><cell>CAM</cell><cell>0.21 (0.08)</cell><cell>0.19 (0.04)</cell><cell>0.71 (0.02)</cell></row><row><cell>CCDr</cell><cell>0.66 (0.12)</cell><cell>0.77 (0.04)</cell><cell>0.62 (0.03)</cell></row><row><cell>GENIE3</cell><cell>0.78 (0.05)</cell><cell>0.087 (0.02)</cell><cell>0.72 (0.03)</cell></row><row><cell>SAM-lin-mse</cell><cell>0.77 (0.04)</cell><cell>0.83 (0.04)</cell><cell>0.69 (0.03)</cell></row><row><cell>SAM-mse</cell><cell>0.91 (0.04)</cell><cell>0.87 (0.03)</cell><cell>0.52 (0.02)</cell></row><row><cell>SAM-lin</cell><cell>0.68 (0.09)</cell><cell>0.86 (0.03)</cell><cell>0.75 (0.03)</cell></row><row><cell>SAM</cell><cell>0.91 (0.16)</cell><cell>0.93 (0.02)</cell><cell>0.77 (0.05)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>Realistic problems: Structural Hamming distance (std. dev.) over 20 graphs (the higher the better). Left: 20 nodes. Middle: 100 nodes. Right: real protein network.. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>SHD</cell><cell cols="2">SynTREN 20 nodes SynTREN 100 nodes</cell><cell>Cyto</cell></row><row><cell>PC-Gauss</cell><cell>53.42 (6.13)</cell><cell>262.65 (19.87)</cell><cell>28 (2.9)</cell></row><row><cell>PC-HSIC</cell><cell>24.13 (4.08)</cell><cell>-</cell><cell>-</cell></row><row><cell>PC-RCOT</cell><cell>34.21 (7.99)</cell><cell>213.51 (8.60)</cell><cell>22 (1.9)</cell></row><row><cell>PC-RCIT</cell><cell>33.20 (7.54)</cell><cell>204.95 (8.77)</cell><cell>23 (1.49)</cell></row><row><cell>GES</cell><cell>67.26 (12.26)</cell><cell>436.02 (18.99)</cell><cell>38 (0.47)</cell></row><row><cell>GIES</cell><cell>69.31 (12.55)</cell><cell>430.55 (22.80)</cell><cell>41 (3.2)</cell></row><row><cell>MMHC</cell><cell>67.2 (8.42)</cell><cell>346 (14.44)</cell><cell>38 (3.4)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>23 (3.2)</cell></row><row><cell>CAM</cell><cell>57.85 (9.10)</cell><cell>222.9 (12.38)</cell><cell>28 (1.32)</cell></row><row><cell>CCDr</cell><cell>54.97 (16.68)</cell><cell>228.8 (21.15)</cell><cell>35 (4.8)</cell></row><row><cell>GENIE3</cell><cell>23.6 (4.14)</cell><cell>153.2 (4.59)</cell><cell>20 (4.1)</cell></row><row><cell>SAM-lin-mse</cell><cell>25.44 (4.97)</cell><cell>240.1 (3.92)</cell><cell>19 (2.1)</cell></row><row><cell>SAM-mse</cell><cell>25.67 (6.96)</cell><cell>173.78 (6.36)</cell><cell>22 (3.2)</cell></row><row><cell>SAM-lin</cell><cell>30.45 (8.09)</cell><cell>168.89 (5.63)</cell><cell>20 (2.8)</cell></row><row><cell>SAM</cell><cell>19.02 (5.83)</cell><cell>153.5 (13.03)</cell><cell>17 (3.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Average area under the precision recall curve on 5 artificial graphs of the Dream4 In Silico Multifactorial Challenge (the higher, the better). The best results are in bold. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUPR</cell><cell>NET1</cell><cell>NET2</cell><cell>NET3</cell><cell>NET4</cell><cell>NET5</cell></row><row><cell>PC-Gauss</cell><cell cols="5">0.113 (0.01) 0.072 (0.01) 0.144 (0.02) 0.130 (0.01) 0.136 (0.01)</cell></row><row><cell>PC-HSIC</cell><cell cols="5">0.116 (0.01) 0.070 (0.01) 0.151 (0.02) 0.121 (0.01) 0.127 (0.02)</cell></row><row><cell>PC-RCOT</cell><cell cols="5">0.094 (0.02) 0.054 (0.01) 0.113 (0.01) 0.097 (0.01) 0.079 (0.01)</cell></row><row><cell>PC-RCIT</cell><cell cols="5">0.084 (0.01) 0.046 (0.01) 0.104 (0.01) 0.083 (0.01) 0.086 (0.01)</cell></row><row><cell>GES</cell><cell cols="5">0.051 (0.01) 0.053 (0.01) 0.061 (0.01) 0.080 (0.01) 0.081 (0.01)</cell></row><row><cell>GIES</cell><cell cols="5">0.047 (0.01) 0.062 (0.01) 0.065 (0.01) 0.076 (0.01) 0.073 (0.01)</cell></row><row><cell>MMHC</cell><cell cols="5">0.116 (0.01) 0.073 (0.01) 0.148 (0.02) 0.133 (0.01) 0.141 (0.02)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CAM</cell><cell cols="5">0.116 (0.01) 0.080 (0.01) 0.210 (0.02) 0.147 (0.02) 0.121 (0.01)</cell></row><row><cell>CCDr</cell><cell cols="5">0.088 (0.01) 0.099 (0.01) 0.114 (0.01) 0.119 (0.01) 0.165 (0.02)</cell></row><row><cell>GENIE3</cell><cell cols="5">0.159 (0.01) 0.151 (0.02) 0.226 (0.02) 0.208 (0.02) 0.209 (0.02)</cell></row><row><cell cols="6">SAM-lin-mse 0.03 (0.001) 0.055 (0.001) 0.10 (0.002) 0.08 (0.001) 0.05 (0.01)</cell></row><row><cell>SAM-mse</cell><cell cols="5">0.035 (0.01) 0.050 (0.01) 0.105 (0.01) 0.13 (0.01) 0.135 (0.01)</cell></row><row><cell>SAM-lin</cell><cell cols="5">0.115 (0.01) 0.085 (0.01) 0.175 (0.02) 0.16 (0.01) 0.134 (0.01)</cell></row><row><cell>SAM</cell><cell cols="5">0.131 (0.01) 0.111 (0.01) 0.274 (0.03) 0.208 (0.02) 0.194 (0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Average area under the ROC curve on 5 artificial graphs of the Dream4 In Silico Multifactorial Challenge (the higher, the better). The best results are in bold. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUC</cell><cell>NET1</cell><cell>NET2</cell><cell>NET3</cell><cell>NET4</cell><cell>NET5</cell></row><row><cell>PC-Gauss</cell><cell>0.61 (0.01)</cell><cell>0.58 (0.01)</cell><cell>0.66 (0.01)</cell><cell>0.66 (0.02)</cell><cell>0.65 (0.01)</cell></row><row><cell>PC-HSIC</cell><cell>0.63 (0.01)</cell><cell>0.55 (0.01)</cell><cell>0.65 (0.01)</cell><cell>0.64 (0.01)</cell><cell>0.65 (0.01)</cell></row><row><cell>PC-RCOT</cell><cell>0.57 (0.01)</cell><cell>0.53 (0.01)</cell><cell>0.60 (0.01)</cell><cell>0.59 (0.01)</cell><cell>0.58 (0.01)</cell></row><row><cell>PC-RCIT</cell><cell>0.54 (0.01)</cell><cell>0.52 (0.01)</cell><cell>0.58 (0.01)</cell><cell>0.57 (0.01)</cell><cell>0.55 (0.01)</cell></row><row><cell>GES</cell><cell cols="5">0.051 (0.01) 0.053 (0.01) 0.061 (0.01) 0.080 (0.01) 0.081 (0.01)</cell></row><row><cell>GIES</cell><cell>0.60 (0.01)</cell><cell>0.60 (0.01)</cell><cell>0.67 (0.01)</cell><cell>0.65 (0.02)</cell><cell>0.68 (0.01)</cell></row><row><cell>MMHC</cell><cell>0.59 (0.01)</cell><cell>0.66 (0.01)</cell><cell>0.65 (0.02)</cell><cell>0.66 (0.01)</cell><cell>0.63 (0.01)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CAM</cell><cell>0.61 (0.01)</cell><cell>0.68 (0.01)</cell><cell>0.70 (0.02)</cell><cell>0.68 (0.01)</cell><cell>0.67(0.01)</cell></row><row><cell>CCDr</cell><cell>0.62 (0.01)</cell><cell>0.62 (0.01)</cell><cell>0.64 (0.01)</cell><cell>0.66 (0.01)</cell><cell>0.66 (0.01)</cell></row><row><cell>GENIE3</cell><cell>0.75 (0.01)</cell><cell>0.73 (0.02)</cell><cell>0.77 (0.01)</cell><cell>0.79 (0.01)</cell><cell>0.80 (0.01)</cell></row><row><cell cols="2">SAM-lin-mse 0.54 (0.01)</cell><cell>0.57 (0.01)</cell><cell>0.63 (0.02)</cell><cell>0.61 (0.01)</cell><cell>0.63 (0.01)</cell></row><row><cell>SAM-mse</cell><cell>0.59 (0.01)</cell><cell>0.55 (0.01)</cell><cell>0.69 (0.02)</cell><cell>0.61 (0.01)</cell><cell>0.62 (0.01)</cell></row><row><cell>SAM-lin</cell><cell>0.65 (0.01)</cell><cell>0.66 (0.01)</cell><cell>0.71 (0.02)</cell><cell>0.69 (0.02)</cell><cell>0.71 (0.01)</cell></row><row><cell>SAM</cell><cell>0.69 (0.01)</cell><cell>0.69 (0.01)</cell><cell>0.73 (0.02)</cell><cell>0.73 (0.02)</cell><cell>0.77 (0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 14 :</head><label>14</label><figDesc>Structural Hamming distance on 5 artificial graphs of the Dream4 In Silico Multifactorial Challenge (the lower, the better). The best results are in bold. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>SHD</cell><cell>NET1</cell><cell>NET2</cell><cell>NET3</cell><cell>NET4</cell><cell>NET</cell></row><row><cell>PC-Gauss</cell><cell cols="5">183 (15) 261 (18) 200 (22) 223 (24) 203 (18)</cell></row><row><cell>PC-HSIC</cell><cell cols="5">170 (15) 249 (25) 193 (18) 210 (16) 192 (15)</cell></row><row><cell>PC-RCOT</cell><cell cols="5">174 (15) 248 (26) 193 (17) 211 (21) 191 (17)</cell></row><row><cell>PC-RCIT</cell><cell cols="5">172 (14) 248 (23) 193 (22) 211 (22) 191 (17)</cell></row><row><cell>GES</cell><cell cols="5">252 (27) 333 (26) 279 (21) 286 (22) 266 (19)</cell></row><row><cell>GIES</cell><cell cols="5">261 (27) 314 (18) 281 (31) 304 (26) 274 (20)</cell></row><row><cell>MMHC</cell><cell cols="5">188 (15) 263 (24) 206 (22) 223 (23) 203 (21)</cell></row><row><cell>LiNGAM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CAM</cell><cell cols="5">178 (15) 250 (21) 182 (16) 213 (14) 196 (15)</cell></row><row><cell>CCDr</cell><cell cols="5">187 (15) 248 (20) 209 (20) 227 (22) 189 (22)</cell></row><row><cell>GENIE3</cell><cell cols="5">172 (17) 245 (22) 190 (17) 208 (19) 193 (20)</cell></row><row><cell cols="6">SAM-lin-mse 176 (16) 249 (23) 195 (25) 211 (24) 193 (19)</cell></row><row><cell>SAM-mse</cell><cell cols="5">171 (15) 253 (23) 197 (16) 211 (20) 192 (23)</cell></row><row><cell>SAM-lin</cell><cell cols="5">175 (17) 249 (25) 190 (21) 204 (19) 191 (17)</cell></row><row><cell>SAM</cell><cell cols="5">182 (18) 252(19) 179 (20) 208 (19) 191 (17)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15 :</head><label>15</label><figDesc>Area under the precision recall curve (AuPR) on 3 graphs of the Dream5 Challenge (the higher the better), computed with the evaluation script proposed by the organizer of the challenge. The best results are in bold. The standard deviation of the results is not available; for each method is indicated the best result reported by the organizer of the challenge.</figDesc><table><row><cell>AUPR</cell><cell cols="2">NET1 NET3 NET4</cell></row><row><cell>TIGRESS</cell><cell cols="2">0.301 0.069 0.020</cell></row><row><cell>CLR</cell><cell cols="2">0.255 0.075 0.021</cell></row><row><cell>ARACNE</cell><cell cols="2">0.187 0.069 0.018</cell></row><row><cell>MMHC</cell><cell cols="2">0.042 0.021 0.020</cell></row><row><cell>HITON-PC</cell><cell>0.08</cell><cell>0.021 0.020</cell></row><row><cell>GENIE3</cell><cell cols="2">0.291 0.093 0.021</cell></row><row><cell>ANOVA</cell><cell cols="2">0.245 0.119 0.022</cell></row><row><cell cols="3">SAM-lin-mse 0.272 0.065 0.019</cell></row><row><cell>SAM-mse</cell><cell cols="2">0.271 0.063 0.017</cell></row><row><cell>SAM-lin</cell><cell cols="2">0.283 0.068 0.020</cell></row><row><cell>SAM</cell><cell cols="2">0.317 0.071 0.020</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 16 :</head><label>16</label><figDesc>Area under the ROC curve (AUC) on 3 graphs of the Dream5 Challenge (the higher the better), computed with the evaluation script proposed by the organizer of the challenge. The best results are in bold. The standard deviation of the results is not available; for each method is indicated the best result reported by the organizer of the challenge.</figDesc><table><row><cell>AUC</cell><cell>NET1 NET3 NET4</cell></row><row><cell>TIGRESS</cell><cell>0.789 0.589 0.514</cell></row><row><cell>CLR</cell><cell>0.773 0.590 0.516</cell></row><row><cell>ARACNE</cell><cell>0.763 0.572 0.504</cell></row><row><cell>MMHC</cell><cell>0.543 0.512 0.513</cell></row><row><cell>HITON-PC</cell><cell>0.582 0.535 0.515</cell></row><row><cell>GENIE3</cell><cell>0.815 0.617 0.518</cell></row><row><cell>ANOVA</cell><cell>0.780 0.671 0.519</cell></row><row><cell cols="2">SAM-lin-mse 0.761 0.563 0.512</cell></row><row><cell>SAM-mse</cell><cell>0.772 0.578 0.511</cell></row><row><cell>SAM-lin</cell><cell>0.781 0.561 0.512</cell></row><row><cell>SAM</cell><cell>0.814 0.582 0.512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 :</head><label>17</label><figDesc>AUPR scores for the orientation of the edges in the skeleton of 20 variable graphs. The best results are in bold. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUPR</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>IGCI</cell><cell cols="3">0.58 (0.09) 0.61 (0.05) 0.79 (0.09)</cell><cell>0.82 (0.03)</cell><cell>0.58 (0.01)</cell><cell cols="2">0.57 (0.04) 0.66 (0.12)</cell></row><row><cell>ANM</cell><cell cols="3">0.53 (0.06) 0.99 (0.01) 0.78 (0.04)</cell><cell>0.83 (0.04)</cell><cell>0.52 (0.07)</cell><cell cols="2">0.72 (0.08) 0.73 (0.17)</cell></row><row><cell>RECI</cell><cell cols="3">0.57 (0.07) 0.52 (0.05) 0.51 (0.04)</cell><cell>0.47 (0.01)</cell><cell>0.60 (0.01)</cell><cell cols="2">0.53 (0.10) 0.54 (0.07)</cell></row><row><cell cols="4">SAM-lin-mse 0.56 (0.08) 0.59 (0.05) 0.62 (0.03)</cell><cell>0.56 (0.03)</cell><cell>0.55 (0.03)</cell><cell cols="2">0.54 (0.07) 0.57 (0.07)</cell></row><row><cell>SAM-mse</cell><cell cols="3">0.56 (0.05) 0.82 (0.06) 0.77 (0.05)</cell><cell>0.80 (0.07)</cell><cell>0.72 (0.04)</cell><cell cols="2">0.75 (0.06) 0.74 (0.13)</cell></row><row><cell>SAM-lin</cell><cell cols="3">0.57 (0.08) 0.78 (0.08) 0.75 (0.03)</cell><cell>0.79 (0.05)</cell><cell>0.75 (0.03)</cell><cell>0.68 (0.09)</cell><cell>0.72(0.12)</cell></row><row><cell>SAM</cell><cell cols="3">0.57 (0.09) 0.85 (0.09) 0.87 (0.04)</cell><cell>0.79 (0.02)</cell><cell>0.98 (0.01)</cell><cell cols="2">0.95 (0.04) 0.83 (0.15)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 :</head><label>18</label><figDesc>AUC scores for the orientation of the edges in the skeleton of 20 variable graphs. The best results are in bold. Significantly better results (t-test with p-value 0.001) are underlined.</figDesc><table><row><cell>AUC</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>IGCI</cell><cell>0.54 (0.13)</cell><cell cols="2">0.59 (0.048) 0.76 (0.083)</cell><cell>0.80 (0.049)</cell><cell>0.61 (0.016)</cell><cell>0.57 (0.077)</cell><cell>0.64 (0.13)</cell></row><row><cell>ANM</cell><cell cols="3">0.54 (0.073) 0.99 (0.013) 0.79 (0.056)</cell><cell>0.82 (0.050)</cell><cell>0.49 (0.087)</cell><cell cols="2">0.70 (0.111) 0.72 (0.185)</cell></row><row><cell>RECI</cell><cell>0.54 (0.10)</cell><cell cols="2">0.46 (0.069) 0.48 (0.041)</cell><cell>0.36 (0.05)</cell><cell>0.39 (0.018)</cell><cell>0.44 (0.18)</cell><cell>0.45 (0.11)</cell></row><row><cell>SAM-lin-mse</cell><cell>0.51 (0.11)</cell><cell>0.57 (0.07)</cell><cell>0.65 (0.05)</cell><cell>0.58 (0.05)</cell><cell>0.49 (0.02)</cell><cell>0.56 (0.06)</cell><cell>0.56 (0.08)</cell></row><row><cell>SAM-mse</cell><cell>0.53 (0.04)</cell><cell>0.79 (0.08)</cell><cell>0.73 (0.07)</cell><cell>0.79(0.09)</cell><cell>0.81 (0.05)</cell><cell>0.72 (0.07)</cell><cell>0.73 (0.10)</cell></row><row><cell>SAM-lin</cell><cell>0.49 (0.02)</cell><cell>0.76 (0.05)</cell><cell>0.69 (0.04)</cell><cell>0.78 (0.07)</cell><cell>0.76 (0.07)</cell><cell>0.66 (0.05)</cell><cell>0.69 (0.08)</cell></row><row><cell>SAM</cell><cell cols="3">0.51 (0.085) 0.85 (0.085) 0.84 (0.048)</cell><cell>0.74 (0.023)</cell><cell>0.98 (0.009)</cell><cell>0.95 (0.040)</cell><cell>0.81 (0.17)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 19 :</head><label>19</label><figDesc>Artificial graphs with 20 variables generated with uniform noise: Average precision (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined</figDesc><table><row><cell>AUPR</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>PC-Gauss</cell><cell cols="3">0.25 (0.01) 0.45 (0.04) 0.27 (0.02)</cell><cell>0.38 (0.03)</cell><cell>0.27 (0.02)</cell><cell cols="2">0.29 (0.05) 0.32 (0.08)</cell></row><row><cell>PC-HSIC</cell><cell cols="3">0.25 (0.02) 0.52 (0.05) 0.28 (0.01)</cell><cell>0.43 (0.03)</cell><cell>0.23 (0.03)</cell><cell cols="2">0.32 (0.05) 0.35 (0.08)</cell></row><row><cell>PC-RCOT</cell><cell cols="3">0.24 (0.02) 0.53 (0.06) 0.28 (0.01)</cell><cell>0.44 (0.04)</cell><cell>0.31 (0.01)</cell><cell cols="2">0.35 (0.05) 0.36 (0.09)</cell></row><row><cell>PC-RCIT</cell><cell cols="3">0.25 (0.02) 0.50 (0.05) 0.26 (0.02)</cell><cell>0.40 (0.03)</cell><cell>0.31 (0.01)</cell><cell cols="2">0.33 (0.04) 0.34 (0.10)</cell></row><row><cell>GES</cell><cell cols="3">0.46 (0.04) 0.45 (0.06) 0.24 (0.03)</cell><cell>0.48 (0.04)</cell><cell>0.37 (0.02)</cell><cell cols="2">0.30 (0.07) 0.38 (0.10)</cell></row><row><cell>GIES</cell><cell cols="3">0.52 (0.05) 0.47 (0.05) 0.25 (0.03)</cell><cell>0.48 (0.05)</cell><cell>0.38 (0.03)</cell><cell cols="2">0.31 (0.09) 0.40 (0.11)</cell></row><row><cell>MMHC</cell><cell cols="3">0.25 (0.02) 0.27 (0.02) 0.21 (0.01)</cell><cell>0.35 (0.01)</cell><cell>0.27 (0.01)</cell><cell cols="2">0.27 (0.01) 0.27 (0.04)</cell></row><row><cell>LiNGAM</cell><cell cols="3">0.41 (0.04) 0.14 (0.02) 0.13 (0.01)</cell><cell>0.19 (0.01)</cell><cell>0.11 (0.004)</cell><cell cols="2">0.12 (0.02) 0.19 (0.11)</cell></row><row><cell>CAM</cell><cell cols="3">0.28 (0.02) 0.91 (0.04) 0.54 (0.03)</cell><cell>0.37 (0.04)</cell><cell>0.35 (0.04)</cell><cell cols="2">0.32 (0.08) 0.46 (0.22)</cell></row><row><cell>CCDr</cell><cell cols="3">0.32 (0.03) 0.43 (0.03) 0.26 (0.02)</cell><cell>0.46 (0.04)</cell><cell>0.25 (0.02)</cell><cell cols="2">0.42 (0.06) 0.36 (0.09)</cell></row><row><cell>GENIE3</cell><cell cols="3">0.22 (0.04) 0.48 (0.05) 0.49 (0.04)</cell><cell>0.34 (0.03)</cell><cell>0.28 (0.03)</cell><cell cols="2">0.36 (0.04) 0.36 (0.11)</cell></row><row><cell cols="4">SAM-lin-mse 0.62 (0.06) 0.52 (0.03) 0.32 (0.03)</cell><cell>0.31 (0.02)</cell><cell>0.32 (0.03)</cell><cell cols="2">0.40 (0.05) 0.42 (0.09)</cell></row><row><cell>SAM-mse</cell><cell cols="3">0.59 (0.05) 0.68 (0.04) 0.58 (0.03)</cell><cell>0.40(0.02)</cell><cell>0.28 (0.01)</cell><cell cols="2">0.51 (0.03) 0.51 (0.06)</cell></row><row><cell>SAM-lin</cell><cell cols="3">0.72 (0.04) 0.51 (0.02) 0.35 (0.02)</cell><cell>0.34 (0.03)</cell><cell>0.39 (0.03)</cell><cell cols="2">0.42 (0.04) 0.46 (0.07)</cell></row><row><cell>SAM</cell><cell cols="3">0.68 (0.03) 0.76 (0.05) 0.72 (0.03)</cell><cell>0.46 (0.03)</cell><cell>0.67 (0.02)</cell><cell cols="2">0.70 (0.09) 0.67 (0.11)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 20 :</head><label>20</label><figDesc>Artificial graphs with 20 variables generated with uniform noise: area under the ROC curve (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined</figDesc><table><row><cell>AUC</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell cols="2">Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row><row><cell>PC-Gauss</cell><cell cols="3">0.64 (0.01) 0.77 (0.02) 0.59 (0.01)</cell><cell>0.75 (0.02)</cell><cell>0.70 (0.02)</cell><cell cols="2">0.70 (0.02) 0.69 (0.06)</cell></row><row><cell>PC-HSIC</cell><cell cols="3">0.63 (0.02) 0.81 (0.02) 0.66 (0.02)</cell><cell>0.80 (0.02)</cell><cell>0.66 (0.02)</cell><cell cols="2">0.73 (0.04) 0.72 (0.07)</cell></row><row><cell>PC-RCOT</cell><cell cols="3">0.64 (0.01) 0.83 (0.03) 0.66 (0.02)</cell><cell>0.81 (0.02)</cell><cell>0.73 (0.01)</cell><cell cols="2">0.75 (0.02) 0.36 (0.11)</cell></row><row><cell>PC-RCIT</cell><cell cols="3">0.64 (0.02) 0.83 (0.03) 0.64 (0.02)</cell><cell>0.78 (0.02)</cell><cell>0.71 (0.01)</cell><cell cols="2">0.73 (0.03) 0.72 (0.07)</cell></row><row><cell>GES</cell><cell cols="3">0.81 (0.02) 0.79 (0.04) 0.67 (0.03)</cell><cell>0.85 (0.02)</cell><cell>0.81 (0.02)</cell><cell cols="2">0.75 (0.08) 0.78 (0.07)</cell></row><row><cell>GIES</cell><cell cols="3">0.83 (0.03) 0.80 (0.04) 0.67 (0.02)</cell><cell>0.84 (0.03)</cell><cell>0.82 (0.02)</cell><cell cols="2">0.75 (0.08) 0.79 (0.07)</cell></row><row><cell>MMHC</cell><cell cols="3">0.66 (0.02) 0.75 (0.02) 0.60 (0.01)</cell><cell>0.78 (0.02)</cell><cell>0.74 (0.01)</cell><cell cols="2">0.72 (0.02) 0.71 (0.06)</cell></row><row><cell>LiNGAM</cell><cell cols="3">0.77 (0.02) 0.57 (0.02) 0.47 (0.02)</cell><cell>0.46 (0.02)</cell><cell>0.50 (0.02)</cell><cell cols="2">0.50 (0.04) 0.55 (0.11)</cell></row><row><cell>CAM</cell><cell cols="3">0.73 (0.01) 0.97 (0.01) 0.88 (0.01)</cell><cell>0.80 (0.02)</cell><cell>0.76 (0.02)</cell><cell cols="2">0.74 (0.05) 0.81 (0.09)</cell></row><row><cell>CCDr</cell><cell cols="3">0.74 (0.02) 0.74 (0.03) 0.64 (0.02)</cell><cell>0.83 (0.02)</cell><cell>0.69 (0.02)</cell><cell cols="2">0.78 (0.05) 0.74 (0.07)</cell></row><row><cell>GENIE3</cell><cell cols="3">0.69 (0.02) 0.79 (0.02) 0.71 (0.01)</cell><cell>0.78 (0.01)</cell><cell>0.68 (0.02)</cell><cell cols="2">0.70 (0.02) 0.73 (0.07)</cell></row><row><cell cols="4">SAM-lin-mse 0.76 (0.02) 0.73 (0.02) 0.75 (0.03)</cell><cell>0.71 (0.02)</cell><cell>0.72 (0.04)</cell><cell cols="2">0.69 (0.03) 0.73 (0.06)</cell></row><row><cell>SAM-mse</cell><cell cols="3">0.72 (0.02) 0.79 (0.03) 0.82 (0.02)</cell><cell>0.82 (0.01)</cell><cell>0.69 (0.03)</cell><cell cols="2">0.75 (0.02) 0.75 (0.07)</cell></row><row><cell>SAM-lin</cell><cell cols="3">0.85 (0.02) 0.73 (0.02) 0.72 (0.03)</cell><cell>0.76 (0.02)</cell><cell>0.81 (0.02)</cell><cell cols="2">0.73 (0.02) 0.77 (0.06)</cell></row><row><cell>SAM</cell><cell cols="3">0.91 (0.01) 0.94 (0.01) 0.92 (0.01)</cell><cell>0.84 (0.01)</cell><cell>0.91 (0.01)</cell><cell cols="2">0.90 (0.03) 0.91 (0.03)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 21 :</head><label>21</label><figDesc>Artificial graphs with 20 variables generated with uniform noise: structural hamming distance (std. dev.) of all compared algorithms over all six types of distributions (the higher the better). Significantly better results (t-test with p-value 0.001) are underlined</figDesc><table><row><cell>AUPR</cell><cell>Linear</cell><cell>GP AM</cell><cell>GP Mix</cell><cell>Sigmoid AM Sigmoid Mix</cell><cell>NN</cell><cell>Global</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at https://github.com/Diviyan-Kalainathan/SAM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It must be noted however that the data might satisfy additional independence relations beyond those in the graph; see the faithfulness assumption.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In the case where the sought G graph does not include v-structures (e.g., due to its being star-shaped), the cited methods are unable to orient the edges (see section 6.5).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that in the bivariate case, both X → Y and Y → X DAGs are Markov equivalent; methods in categories I, II and III do not apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that În (Xj, X Pa(j; G) |X Pa(j; G) ) converges in probability toward I(Xj, X Pa(j; G) |X Pa(j; G) ), the mutual information term between Xj and X Pa(j; G) , conditioned on the parent variables X Pa(j; G) , as n goes to infinity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>In practice, λD is small at the initialization and increases along time; in this way, the structural penalization term λS i,j ai,j can operate and prune the less relevant edges before considering the DAG constraint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>The computational training time is 113 seconds on a Nvidia RTX 2080Ti graphic card, with niter = 3000 iterations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>For AUPR and AUC evaluations, we use the scikit-learn v0.20.1 library<ref type="bibr" target="#b57">(Pedregosa et al., 2011)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>The datasets GP AM, GP MIX and Sigmoid AM were considered for the experimental validation of the CAM algorithm<ref type="bibr" target="#b58">(Peters et al., 2014)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9"><p>Random seeds set to 1. . .10 are used for the sake of reproducibility. SynTREN hyper-parameters include a probability of 1.0 (resp. 0.1) for complex 2-regulator interactions (resp. for biological noise, experimental noise and noise on correlated inputs).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10"><p>Note that we do not use in our experiments Network 2 of DREAM5, because no verified interaction is provided for this dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_11"><p>available at http://dreamchallenges.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_12"><p>Available at https://github.com/Diviyan-Kalainathan/SAM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_13"><p>We use the formula I(X, {Y, Z}) = I(X, Y ) + I(X, Z|Y )</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We would like to thank <rs type="person">Dr. Mikael Escobar-Bach</rs> for proofreading the paper. This work was granted access to the HPC resources of <rs type="institution">CCIPL (Nantes, France</rs>).</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations and definitions</head><p>The 500 sample dataset generated after the above FCM was processed using SAM (section 5). The overall average Frobenius regularization loss 2 j=1 θ j F (in blue) and the total fit loss of the two generators (in red) and for the two models X → Y (solid lines) and Y → X (dashed lines) are displayed on Figure <ref type="figure">23</ref> for different values of the functional regularization parameter λ F (average results on 32 runs). The error bars corresponds to the standard deviation.</p><p>Note that simple tests cannot be used to disambiguate the sought causal graph in its Markov equivalence class: the Pearson coefficient is 0 as non-linear models are needed to explain the relation between both variables, conditional independences do not apply as only 2 variables are considered.</p><p>However, for all values of λ F both the Frobenius regularization loss and the fit loss are lower for the model X → Y than for the model Y → X and the difference is statistically significant (t-test with p-value 0.001).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hiton: a novel Markov blanket algorithm for optimal variable selection</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Constantin F Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local causal and Markov blanket induction for causal discovery and feature selection for classification part I: Algorithms and empirical evaluation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Constantin F Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramani</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xenofon</forename><forename type="middle">D</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><surname>Koutsoukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concave penalized estimation of sparse gaussian bayesian networks</title>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2273" to="2328" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning large-scale bayesian networks with the sparsebn package</title>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04025</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A formalism for relevance and its application in feature subset selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="195" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cause-effect inference by comparing regression errors</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Blöbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="900" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional likelihood maximisation: a unifying framework for information theoretic feature selection</title>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pocock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Jie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Luján</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="27" to="66" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cam: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causal analysis in economics: Methods and applications</title>
		<author>
			<persName><forename type="first">Pu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chihying</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Flaschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Semmler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4938</idno>
		<title level="m">A transformational characterization of equivalent bayesian network structures</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inferring deterministic causal relations</title>
		<author>
			<persName><forename type="first">Povilas</forename><surname>Daniusis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Steudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards a rigorous science of interpretable machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles</title>
		<author>
			<persName><forename type="first">Jeremiah</forename><forename type="middle">J</forename><surname>Faith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hayete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">T</forename><surname>Thaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilaria</forename><surname>Mogno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamey</forename><surname>Wierzbowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Cottarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">S</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaussian process networks</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iftach</forename><surname>Nachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;00</title>
		<meeting>the Sixteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;00<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="211" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning functional causal models with generative neural networks</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Caillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable and Interpretable Models in Computer Vision and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="39" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel methods for measuring independence</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cause Effect Pairs in Machine Learning</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berna</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batu</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tigress: trustful inference of gene regulation using stability selection</title>
		<author>
			<persName><forename type="first">Anne-Claire</forename><surname>Haury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fantine</forename><surname>Mordelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Vera-Licona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC systems biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Boundary-seeking generative adversarial networks</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athul</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08431</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petteri</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inferring regulatory networks from expression data using tree-based methods</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Irrthum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">12776</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal inference using the algorithmic Markov condition</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5168" to="5194" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting confounding in multivariate linear models via spectral analysis</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the pc-algorithm</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal inference using graphical models with the R package pcalg</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inferring gene regulatory networks by anova</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Tavakkolkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Windhager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1376" to="1382" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature selection with neural networks</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Leray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="166" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Revisiting classifier two-sample tests</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06545</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards a learning theory of cause-effect inference</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya O Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<title level="m">Learning sparse neural networks through l 0 regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The dream4 in-silico network challenge</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schaffter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Prill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Stolovitzky</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Draft, version 0.3, 2009</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wisdom of crowds for robust gene network inference</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">M</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Prill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Kyle R Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Kellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Stolovitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Riccardo Dalla Favera, and Andrea Califano. Aracne: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Adam A Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katia</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><surname>Stolovitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMC bioinformatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">S7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Artificial gene networks for objective comparison of analysis algorithms</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keying</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">suppl 2</biblScope>
			<biblScope unit="page" from="122" to="129" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<title level="m">Conditional generative adversarial nets. arXiv</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic latent variable models for distinguishing between cause and effect</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Stegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distinguishing cause from effect using observational data: methods and benchmarks</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-dimensional consistency in scorebased and hybrid structure learning</title>
		<author>
			<persName><forename type="first">Preetam</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5947" to="5956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A hybrid causal search algorithm for latent variable models</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ogarrio</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Probabilistic Graphical Models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">675-685</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2003">2003. 2009</date>
			<publisher>Judea Pearl. Causality</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A formal theory of inductive causation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Verma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2009" to="2053" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference -Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning of causal relations</title>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>John A Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName><surname>Biehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESANN</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of data science and analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scaling up greedy causal search for continuous variables</title>
		<author>
			<persName><forename type="first">Ramsey</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Backshift: Learning causal cyclic graphs from unknown shift interventions</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1513" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Genenetweaver: in silico benchmark generation and performance profiling of network inference methods</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schaffter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Floreano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2263" to="2270" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning bayesian networks with the bnlearn r package</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Network motifs in the transcriptional regulation network of escherichia coli</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Shai S Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmoolik</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Mangan</surname></persName>
		</author>
		<author>
			<persName><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature genetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied informatics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Causation, prediction and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Statistics</title>
		<imprint>
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">New methods for separating causes from effects in genomics data</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><forename type="middle">I</forename><surname>Lytkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC genomics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Visweswaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03877</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Time and sample efficient discovery of Markov blankets and direct causal relations</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Statnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Syntren: a generator of synthetic gene expression data for design and analysis of structure learning algorithms</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Van Den Bulcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koenraad</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Naudts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piet</forename><surname>Van Remortel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Verschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Marchal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A review of feature selection methods based on mutual information</title>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">R</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Estévez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computing and applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="186" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>CoRR, abs/1805.06826</idno>
		<ptr target="http://arxiv.org/abs/1805.06826" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A proxy variable view of shared confounding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10697" to="10707" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">A unified view of causal and non-causal feature selection</title>
		<author>
			<persName><forename type="first">Kui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05844</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Distinguishing causes from effects using nonlinear acyclic causal models</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality: Objectives and Assessment</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Dags with NO TEARS: continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sam-Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>mse 45.3 (2.8) 36.2 (2.5) 40.3 (2.7</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sam-Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
