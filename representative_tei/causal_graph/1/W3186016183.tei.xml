<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Search for Causality: A Comparison of Different Techniques for Causal Inference Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-29">July 29, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jolanda</forename><forename type="middle">J</forename><surname>Kossakowski</surname></persName>
							<email>jolanda.kossakowski@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lourens</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
							<idno type="ORCID">0000-0001-8278-319X</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Jolanda</surname></persName>
							<idno type="ORCID">0000-0002-6946-1732</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Kossakowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="laboratory">Research and Documentation Centre (WODC)</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">Dutch Ministry of Security and Justice</orgName>
								<address>
									<addrLine>Koningskade 4</addrLine>
									<postCode>2596 AA</postCode>
									<settlement>The Hague</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Search for Causality: A Comparison of Different Techniques for Causal Inference Graphs</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1082-989X</idno>
						<imprint>
							<date type="published" when="2021-07-29">July 29, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1037/met0000390</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal inference</term>
					<term>perturbation</term>
					<term>transitive reduction</term>
					<term>invariant causal prediction</term>
					<term>experimental design</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating causal relations between two or more variables is an important topic in psychology. Establishing a causal relation between two variables can help us in answering that question of why something happens. However, using solely observational data are insufficient to get the complete causal picture. The combination of observational and experimental data may give adequate information to properly estimate causal relations. In this study, we consider the conditions where estimating causal relations might work and we show how well different algorithms, namely the Peter and Clark algorithm, the Downward Ranking of Feed-Forward Loops algorithm, the Transitive Reduction for Weighted Signed Digraphs algorithm, the Invariant Causal Prediction (ICP) algorithm and the Hidden Invariant Causal Prediction (HICP) algorithm, determine causal relations in a simulation study. Results showed that the ICP and the HICP algorithms perform best in most simulation conditions. We also apply every algorithm to an empirical example to show the similarities and differences between the algorithms. We believe that the combination of the ICP and the HICP algorithm may be suitable to be used in future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supplemental materials: <ref type="url" target="https://doi.org/10.1037/met0000390.supp">https://doi.org/10.1037/met0000390.supp</ref> Some tens of thousands of years ago, humans began to realize that certain things cause other things and that tinkering with the former can change the latter. No other species grasps this, certainly not to the extent that we do. From this discovery came organized societies, then towns and cities, and eventually the science-and technology-based civilization we enjoy today. All because we asked a simple question: Why? <ref type="bibr" target="#b19">(Pearl &amp; Mackenzie, 2018)</ref>.</p><p>The quest for causality is one that people have been striving for decades. Establishing a causal relation between two phenomena or variables can help us in answering that big question of why something happens. In psychology, we study the (possible) causal relation between psychological constructs, like sleep, concentration, or feelings of guilt. For example, does sleep deprivation lead to concentration problems? And could sleep deprivation be caused by increased feelings of guilt? Knowing what the cause is of something so intrusive as sleep problems may in turn lead to finding the solution to help an individual with sleep problems. If we know what causes a problem, we can help to solve it.</p><p>What is a causal relation? We confine ourselves here to an interventional (or, equivalently, a counterfactual) definition of causality. The idea is that, if one changes (perturbs) variable X, then this should have effects only on variables Y with which X has a causal relation. For instance, if we consider the structure X ! Y / Z, then we expect that changing X will change Y, but this change in X will not change Z. Therefore, we use the following definition of a unidirectional causal relation: "a relation between two variables (X ! Y) where, when one changes one variable (X), one observes a change in the other variable (Y), and if we change variable Y we observe no change in variable X. In general, when more than three variables are involved, we require that conditioned on all other variables, a direct cause X is one that changes the distribution of Y. If we go back to the example of sleep, our definition of a causal relation states that, when there is an increase in sleep problems, there should be a change in the level of concentration as well, or any other aspect of the distribution of the effect variable. The definition of a causal relation that we use here is also a counterfactual relationship <ref type="bibr" target="#b18">(Pearl, 2009;</ref><ref type="bibr" target="#b22">Peters et al., 2017)</ref>. It may be argued that our definition is not sufficient to capture all aspects of a causal relation. For instance, we ignore the question of what kinds of events could have a causal relation; thereby, interpreting the causal relations.</p><p>To infer causal relations from the data (and hence the probability distribution obtained from the data) we require that any change in a causal relation in the graph implies a corresponding change in the probability distribution. This is known as the causal Markov assumption. Reversely, a change in the probability distribution implies a change in the graph, known as the faithfulness assumption. The relations between variables in the graph are referred to as d-separation <ref type="bibr" target="#b18">(Pearl, 2009)</ref>. Two variables are d-separated if the path between the variables in the graph is blocked by a third variable. The causal Markov assumption then implies that the set of d-separations in the graph implies a set of conditional independence relations in the probability distribution. The faithfulness assumption implies that a change in the conditional independencies implies a change in the set of d-separations. To illustrate, consider Figure <ref type="figure" target="#fig_0">1</ref> In the left panel we observe a chain structure X ! Z ! Y, where X and Y are d-separated if we block (or condition on) Z. The causal Markov assumption then implies that we should find variables X and Y conditionally independent given variable Z. Conversely, the faithfulness assumption implies that if X and Y are conditionally independent given Z in the probability distribution, then X and Y should also be d-separated in the graph.</p><p>The type of data that is most often used to estimate causal relations between variables are observational data. These are (empirical) data in which no perturbations have taken place. Observational data includes cross-sectional data that one collects with questionnaires for example. The most widely used technique to estimate causal relations with observational data are the algorithm developed by <ref type="bibr" target="#b18">Pearl (2009)</ref> and <ref type="bibr" target="#b29">Spirtes et al. (2000)</ref> or variations thereof. Pearl uses the notion of (conditional) dependence and independence between sets of three variables to determine a causal relation. The ideas from <ref type="bibr" target="#b20">Pearl and Verma (1991)</ref> and <ref type="bibr" target="#b29">Spirtes et al. (2000)</ref> indicate that, if one were to solely use multivariate normal observational data, we can infer causal relations using the notion of conditional (in)dependence. Based on the raw (simple, Pearson) and partial correlations, four different causal structures can be obtained for an example with three variables, as shown in Figure <ref type="figure" target="#fig_0">1</ref> In the first three situations (the two chain structures and the common cause structure), nodes X and Y have a nonzero correlation, but their partial correlation is zero when conditioning on node Z. Nodes X and Y are then said to be separated in the graph by Z. In the fourth structure (collider structure), nodes X and Y have zero correlation, but a nonzero partial correlation when conditioning on node Z. The set of conditional independence relations in the probability distribution is different for the collider structure in the right panel of Figure <ref type="figure" target="#fig_0">1</ref> in comparison with the other three structures. The three structures in the left three panels in Figure <ref type="figure" target="#fig_0">1</ref> (two chains and a common cause structure) cannot be distinguished in terms of their conditional independence, nor in terms of their d-separations; they are Markov equivalent (see, e.g., <ref type="bibr">Peters et al., 2017, p. 102)</ref>.</p><p>As the rules for conditional independence are equal for the first three causal structures, they are statistically equivalent and one cannot distinguish them from one another. It is only possible to identify the fourth (collider) structure from the other three <ref type="bibr" target="#b17">(Pearl, 2000</ref>; but see <ref type="bibr" target="#b16">Mooij et al., 2016</ref>, for some interesting cases). These ideas have been used in different methods to obtain causal relations. Tetrad <ref type="bibr" target="#b2">(Glymour &amp; Scheines, 1986</ref>) applied a conditional independence test to each possible alternative path, an implementation in R called ggm (short for Gaussian Graphical Models; <ref type="bibr" target="#b0">Drton &amp; Richardson, 2004</ref>) uses a likelihood based method for a complete set of conditional independencies. Temporal ordering has also been used <ref type="bibr" target="#b4">(Hamaker et al., 2015;</ref><ref type="bibr" target="#b31">Usami et al., 2019;</ref><ref type="bibr" target="#b35">Zyphur et al., 2019)</ref>. However, using observational data exclusively will not resolve all causal relations. This led <ref type="bibr" target="#b3">Granger (1980)</ref> to state that an "observed relationship does not allow one to say anything about causation between the variables," and <ref type="bibr" target="#b7">Holland (1986)</ref> argued that there can be "no causation without manipulation." Although one can use observational data to estimate some causal relations, this alone is not enough to properly estimate all relationships between variables. As implied by our definition of a causal relation, one needs to perturb one variable and observe its effect to establish causal relations between variables. This means that we also need so-called experimental The Different Causal Structures That Can be Detected With the Peter and Clark (PC)-Algorithm Note. The chain structures and the common cause structure are statistically equivalent, whereas the collider structure is statistically unique. data to estimate causal relations. These are (empirical) data where some perturbation has taken place. Real-world examples include a pre-posttest comparison with therapy as a perturbation (see, e.g., <ref type="bibr" target="#b11">Kossakowski et al., 2021)</ref>, or experimental designs in which participants are presented with hypothetical scenarios to change their attitude toward a construct (see <ref type="bibr" target="#b6">Hoekstra et al., 2018</ref>, for an empirical example).</p><p>In an experimental study, one needs a control and an experimental condition to see if a manipulation significantly changes an outcome variable. Just like an experiment, to estimate causal relations, we need both observational data that serves as a baseline measurement, and experimental data that may show us which causal relations survive the manipulation and which ones change. We assume here that a perturbation does not alter the underlying causal structure. Thus, the combination of observational and experimental data gives us a complete picture of the causal relations between variables, which in turn may be used to set up a treatment plan where the causes of constructs like concentration problems are intervened upon, instead of the effect. We need both observational and experimental data to determine the difference after some perturbation compared with a baseline (observation, without some perturbation).</p><p>We selected four algorithms for this study that are potentially suitable for psychological data using both observational and experimental data. Two of these assume a variable-specific perturbation, meaning that a perturbation take place on each variable individually. Although this approach may work in theory, in practice it is difficult to single out symptoms of psychological disorders and perturb them accordingly. It is more likely that perturbations in psychology occur in a "fat finger" fashion, which means that multiple variables or symptoms are perturbed simultaneously. The two other algorithms do not assume that only a single variable can be perturbed at a time and so may be more useful in psychology. For comparison we also considered an algorithm that uses only observational data.</p><p>The goal of this article is threefold: (a), we want to provide an overview of a set of algorithms that stem from different fields, describing and illustrating each algorithm using both observational and experimental data; (b) we want to investigate how well each of these algorithms can estimate causal relations by means of a simulation study; and (c) we want to show how these algorithms perform when empirical data are used. First, we will describe the algorithms that can be used to estimate causal relations. For each algorithm we use a simulated dataset as an illustration. Then we will describe the simulation study that we have set up to test not only the performance of these techniques individually, but also in comparison to one another. Lastly, we will apply each algorithm to an empirical example to show how the algorithms work in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods of Causal Inference</head><p>The goal of this study is to compare different algorithms for inferring causal graphs. The algorithms we use all work nodewise, that is, we consider each variable (node associated with that variable) in turn and determine the variables directly connected to it; a regression basically. Within each regression we assume that if nodes are separated in the graph then this corresponds to a conditional independence in the probability distribution (Markov condition). Also, we assume that a conditional independence (partial correlation for multivariate normal data) translates to a separation of nodes in the graph (faithfulness condition). See introduction above and Appendix A for more details on this.</p><p>Identifying causal relations is not an easy task. Take for example the causal graph shown in Figure <ref type="figure" target="#fig_1">2</ref>, where one can see that there is no direct relation between variables 2 and 5 due to the chain structures 2 ! 3 ! 5 and 2 ! 4 ! 5. So, there are three possible paths when one does not know the true graph. The trick is then to remove the path 2 ! 5 in this case. Here, we focus on two different types of methods. The first is called transitive reduction <ref type="bibr" target="#b10">(Klamt et al., 2010;</ref><ref type="bibr" target="#b23">Pinna et al., 2013)</ref>. Here, a causal graph is set up, and direct connections are removed if there is enough evidence to suggest that two variables are not directly connected. When there is a direct causal relation between two variables, any alternative path between these variables should be removed with transitive reduction. However, when a direct causal relation is small, algorithms that use transitive reduction may erroneously remove the direct connection in favor of the alternative paths. Transitive reduction may not always work in practice. We expand on this more in the section where we discuss the Down-Ranking of Feed-Forward Loops algorithm and the Transitive Reduction for Weighted Signed Digraphs algorithm as well as Appendix B.</p><p>The second method is by conditioning on the remaining variables <ref type="bibr" target="#b14">(Meinshausen et al., 2016;</ref><ref type="bibr" target="#b22">Peters et al., 2017)</ref>. In our example shown in Figure <ref type="figure" target="#fig_1">2</ref> this is guaranteed to work, since conditioning on both 3 and 4 will remove the correlation between variables 2 and 5. Note that even if we were to perturb the variable associated with node 2, then conditioning on nodes 3 and 4 would still lead to no change in the variable associated with node 5, allowing the correct inference that there is no direct relation from node 2 to node 5.</p><p>To explain these algorithms, we will use one simulated causal graph and associated dataset that contains five variables, visualized in Figure <ref type="figure" target="#fig_1">2</ref>. For illustration purposes we simulated data for 1,000 measurements. We will compare five algorithms: the Peter and Clark algorithm (PC; <ref type="bibr" target="#b8">Kalisch &amp; Bühlmann, 2007)</ref>; the Down-Ranking of Feed-Forward Loops algorithm (DR-FFL; <ref type="bibr" target="#b23">Pinna et al., 2013)</ref>, the Transitive Reduction for Weighted Signed Digraphs algorithm (TRANSWESD; <ref type="bibr" target="#b10">Klamt et al., 2010)</ref>; the Invariant Causal Prediction algorithm (ICP; <ref type="bibr" target="#b14">Meinshausen et al., 2016)</ref> and the Hidden Invariant Causal Prediction algorithm (HICP; <ref type="bibr" target="#b22">Peters et al., 2017)</ref>. We chose to include the PC-algorithm, even though it only uses observational data, to compare its results to algorithms that include experimental data next to observational data. Other algorithms that use observational data include a directional dependence model using copulas <ref type="bibr" target="#b30">(Sungur, 2005)</ref>, a linear causal acyclic model <ref type="bibr" target="#b28">(Shimizu et al., 2006)</ref>, or a directional dependence analysis with possible confounding variables <ref type="bibr" target="#b32">(Wiedermann &amp; Sebastian, 2019)</ref>. We chose to restrict our study to these algorithms because we were interested in combining observational and experimental data and different types of perturbations.</p><p>The data used to illustrate the different algorithms are publicly available, so that the reader may use the data to replicate our examples. Throughout this section we use the example graph in Figure <ref type="figure" target="#fig_1">2</ref> with p = 5 variables and n = 1,000 observations (independent and identically distributed). Edge e ij denotes a directed edge i ! j. Symbols that are associated with specific algorithms will be explained when we introduce the symbol for the first time. At the end of this section, we provide a summary table (see Table <ref type="table" target="#tab_0">1</ref>) that gives an overview of the algorithms that are discussed here, and their properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PC-Algorithm</head><p>The PC-algorithm <ref type="bibr" target="#b29">(Spirtes et al., 2000)</ref> has a two-step procedure that solely uses observational data. We used the R-package pcalg (Version 2.6-2; <ref type="bibr" target="#b9">Kalisch et al., 2012)</ref> to run the PC-algorithm. The first step in the PC-algorithm is to find the skeleton of the causal graph: an undirected graph that shows all possible causal relations. For each node individually, we look at every possible relation with every other node in the graph. The raw correlation between each pair of nodes is calculated (matrix r in (1)). Then the partial correlations are calculated between every pair of nodes (matrix r p in (1)), conditioning on subsets of the remaining variables, increasing in size of the subsets. All possible partial correlations are calculated until either the algorithm has calculated the partial correlation for all possible subsets, or until a partial correlation returns zero when conditioning on a specific subset. In the latter case the correlation in r is explained away by another variable. This can be seen, for instance, in the partial correlation matrix r p below where the partial correlation between nodes 2 and 5 drops from .723 to .038 when conditioning on the remaining three nodes.</p><p>In the second step of the PC-algorithm, the direction of the relation is determined by considering collider structures (fourth panel, Figure <ref type="figure" target="#fig_0">1</ref>). Because the correlational pattern for a collider (nonzero partial correlation between nodes X and Y) is different from the chain and common cause structure (zero partial correlation between nodes X and Y), the collider structure can be distinguished, and hence gives information about the direction of the causal relations. This can be seen from the partial correlation matrix r p below where the partial correlation between nodes 1 and 3 is À.468 (conditioning on all three remaining nodes), while without conditioning, the (Pearson) correlation is À.019. The fact that there is no or a very small correlation without conditioning, but a large partial correlation when conditioning implies that there must be a collider structure (see Figure <ref type="figure" target="#fig_1">2</ref>). Note that the PC-algorithm does not automatically correct for multiple testing.</p><p>r ¼ 1:000 À0:028 1:000 À0:019 0:711 1:000 À0:073 0:717 0:517 1:000 0:298 0:723 0:759 0:748 1:000</p><formula xml:id="formula_0">0 B B B B @ 1 C C C C A r p ¼</formula><p>1:000 À0:006 1:000 À0:468 0:389 1:000 À0:515 0:392 À0:467 1:000 0:686 0:038 0:685 0:693 1:000</p><formula xml:id="formula_1">0 B B B B @ 1 C C C C A (1)</formula><p>Figure <ref type="figure">3</ref> (left panel) shows the skeleton based on our illustration data, using a significance level of .05. The right panel of Figure <ref type="figure">3</ref> shows the final result of our illustration. Four out of six edges that are present are correctly identified. What is interesting in this example is the edge e 23 . This edge is undirected, the PC-algorithm could not determine a direction. This makes sense when we look at the causal structure in Figure <ref type="figure" target="#fig_1">2</ref> that is formed between nodes 2, 3, and 5, and between nodes 3, 2, and 4. No matter the direction of the edge between nodes 2 and 3, the causal structures between nodes 2, 3, and 5, and between nodes 3, 2, and 4 will remain statistically equivalent. It is impossible for the PC-algorithm to determine a direction. This illustration shows one of the prime disadvantages of the PC-algorithm, in that it obtains an equivalence class of graphs that are all equally likely to be true, and so some directions of edges cannot be resolved. The other edge that stands out is e 14 . In this case the correlation between the variables associated with nodes 1 and 4 exists because of the induced path 4 À 3 À 1 when conditioning on node 5. Hence it shows up in the skeleton and in the final graph. Overall, in this illustration the PCalgorithm performs reasonably well, only one edge is incorrectly estimated, and one edge is left undirected. Most of the edges that are present in the true causal graph are correctly estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DR-FFL-Algorithm</head><p>The Downward-Ranking of Feed-Forward Loops algorithm (DR-FFL; <ref type="bibr" target="#b23">Pinna et al., 2013)</ref> has an advantage over de PC-algorithm in that it uses both observational data and experimental data to estimate a causal graph. The DR-FFL-algorithm <ref type="bibr" target="#b23">(Pinna et al., 2013)</ref> originates from the field of gene biology and estimates unweighted (no edge weights), unsigned (edge can be positive or negative, there is no information on this) causal graphs for single subjects and single measurements (where each node was perturbed once). The DR-FFL-algorithm uses a two-step procedure. In the first step, the algorithm compares the effect of perturbing a node to the average effect that includes the observational data as well to create a perturbation graph (PG). In the second step, the DR-FFL-algorithm applies transitive reduction to remove direct causal relations from the perturbation graph where indirect effects are in order.</p><p>The DR-FFL-algorithm needs two components to infer the causal graph: observational data for each of the nodes (G wt ; also known as wild-type data) and experimental data (G ko ; also called knockout data) where each node in the data are perturbed. The observational data are given in (3) for the example data based on Figure <ref type="figure" target="#fig_1">2</ref>. The experimental data in (3) consists of results of a particular node being perturbed. For example, row 1 of the matrix in (3) depicts the new values that the nodes in the graph have after perturbing node 1. </p><formula xml:id="formula_2">0 B B B B @ 1 C C C C A (3)</formula><p>The first step is to obtain a PG where an effect is determined by normalizing and comparing the perturbed effect using a z-score:</p><formula xml:id="formula_3">jz ij j ¼ G ko ij À l j r j (4) Figure 3</formula><p>Visualization of the Skeleton (Left Panel), and the Causal Graph (Right Panel) Estimated With the Peter and Clark (PC)-Algorithm <ref type="bibr" target="#b9">(Kalisch et al., 2012)</ref> where m j is the mean, and r j the standard deviation of node j across different perturbations.</p><p>Both include the observation for node j. The PG is then generated by selecting those edges whose jzj-score (shown in (5)) is larger than a prespecified threshold b . The resulting PG with the edges that survive a threshold of b = .60 is seen in the left panel of Figure <ref type="figure" target="#fig_4">4</ref>. This threshold b = .60 is arbitrary in that no heuristic is known for sensible values. Note the we averaged over the sample to highlight the differences between the DR-FFL and the TRANSWESD-algorithm (discussed in the next section). jzj ¼ 0:000 0:651 0:405 0:090 0:542 0:370 0:000 0:425 0:072 0:567 0:641 0:872 0:000 0:233 0:611 0:452 0:513 0:423 0:000 0:718 1:047 0:463 0:416 0:693 0:000</p><formula xml:id="formula_4">0 B B B B @ 1 C C C C A (5)</formula><p>The second step of the DR-FFL-algorithm is transitive reduction. In this step, the algorithm narrows its search to edges that connect strongly connected components. A strongly connected component is a (sub)set of nodes where any node can be reached (i.e., there must be a directed path) from any other node in the component. Such a subset is called transitive. The DR-FFL-algorithm only focuses on edges between strongly connected components because cycles exists between the nodes within a strongly connected component. For each edge e ij that connects two strongly connected components, the DR-FFL-algorithm searches for alternative paths, and removes the direct edge e ij if the alternative path satisfies two criteria (1), edge e ij can only be removed when e ij connects different strongly connected components in the PG and (2), edge e ij can only be removed when there is an alternative route from node i to node j without using e ij .</p><p>In this illustration based on the example graph in Figure <ref type="figure" target="#fig_1">2</ref> Overall, the DR-FFL-algorithm does not perform well in this illustration. Only two edges that exist in the true causal graph (see Figure <ref type="figure" target="#fig_1">2</ref>) are also estimated here. Two edges are estimated in the wrong direction, one edge is incorrectly estimated and one edge is incorrectly absent from the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transitive Reduction for Weighted Signed Digraphs</head><p>The TRANSitive reduction for WEighted Signed Digraphs (TRANSWESD; <ref type="bibr" target="#b10">Klamt et al., 2010;</ref><ref type="bibr" target="#b23">Pinna et al., 2013)</ref> returns a causal graph with weighted edges that indicate a positive or a negative relationship, while applying transitive reduction to estimate a causal graph at the same time. Furthermore, where the DR-FFL algorithm mostly handles single-subjects data, the TRANSWESDalgorithm can be solely applied to between-subjects data.</p><p>As a first step, we generate the PG. Like the DR-FFL-algorithm, we calculate |z|-scores. In addition to the jzj-score, we calculate an absolute change score c (shown in ( <ref type="formula">6</ref>)) between G wt and G ko that shows the absolute effect of perturbing a node. Edges are retained in the PG when their associated jcj-scores exceed a prespecified threshold c. Each edge in the PG gets a sign s ij that reflects the direction of the change that node j has made after node i was perturbed: if the change score is positive, then the edge will be blue, and when the change score is negative, the edge will be red. Each edge also has a weight w ij that reflects the uncertainty of the causal relation, where a higher weight indicates a lower certainty. The weight w ij is determined by 1 À jq ij j, where q ij is the conditional correlation <ref type="bibr" target="#b25">(Rice et al., 2005)</ref>. The idea of a conditional correlation is that a variable is influenced by another if it is similar (in terms of correlation) in both the observational and experimental condition. Upon perturbation of one variable the other variable behaves similarly and so the correlation will be high. The assumption is, of course, that the causal relations remain the same in the observed and experimental conditions. A conditional correlation is a correlation between two nodes i and j, given that node i was perturbed, and is calculated as follows: <ref type="table" target="#tab_1">jcj ¼   0:000 0:131 0:013 0:079 0:087  0:096 0:000 0:029 0:038 0:048  0:079 0:129 0:000 0:068 0:076  0:086 0:094 0:017 0:000 0:067  0:108 0:090 0:018 0:064</ref> </p><formula xml:id="formula_5">0:000 0 B B B B @ 1 C C C C A (6) q ij ¼ X 2n a¼1 x i;a À x i ð Þxj;a À x j ð Þ X 2n a¼1 x i;a À x i ½ 2 1=2 X 2n a¼1 x j;a À x j ½ 2 1=2 (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>For nodes i and j Equation ( <ref type="formula" target="#formula_5">7</ref>) only uses the observational data and the experimental data where node i was perturbed. This gives us two vectors, x i and x j , each of length 2n, assuming the same number of data points for observational and experimental data. Parameters x i and x j represent the means of these two vectors. To illustrate, row 3 in ( <ref type="formula" target="#formula_7">8</ref>) states that the first node is not really influenced by node 3 as evidenced by the small conditional correlation of À.029. In contrast, the second node has a conditional correlation of .965, suggesting that it is very much influenced by node 3. Due to the design of the conditional correlation, the resulting matrix, is not symmetric. The resulting PG is shown in Figure <ref type="figure" target="#fig_5">5</ref> (with b = .60 and c = .05). q ¼ 0:000 À0:015 À0:034 À0:008 0:032 À0:039 0:000 0:068 0:108 0:087 À0:029 0:965 0:000 0:695 0:707 À0:034 0:953 0:676 0:000 0:700 0:344 0:755 0:788 0:804 0:000</p><formula xml:id="formula_7">0 B B B B @ 1 C C C C A<label>(8)</label></formula><p>In the second step of the TRANSWESD-algorithm, the algorithm removes an edge e ij when there is an alternative path between nodes i and j and when that alternative path satisfies the following four conditions: (1) the alternative path must not contain a cycle, (2) the alternative path cannot contain the edge e ij that is under consideration, (3) the overall sign of the alternative path must be equal to that of the edge e ij under consideration (obtained by multiplying the signs of all edges on the alternative path) and (4), the maximum weight of all edges on the alternative path must be lower than the weight of the edge e ij under consideration multiplied by a prespecified threshold a. For all analyses, we set a = .95, the default value used by <ref type="bibr" target="#b10">Klamt et al. (2010)</ref>. All edges that exist in the PG are sorted based on their edge weight. The transitive reduction starts with the edge that has the highest weight (and the lowest certainty).</p><p>In the PG in Figure <ref type="figure" target="#fig_5">5</ref> that is based on the example graph in Figure <ref type="figure" target="#fig_1">2</ref>, four edges have no alternative path (e 15 , e 51 , e 45 , and e 54 ), and of the remaining five edges, two edges contain a cycle on their alternative paths (condition 1). With three edges left, two of these satisfied the third condition (the product of the signs of the alternative path must match the sign of the edge e ij that is under consideration). Both of these edges did not meet the final requirement that states that the maximum weight of all edges on the alternative path cannot exceed the weight of the edge e ij under consideration multiplied by a. This means that no edges are removed from the causal graph, and that the perturbation graph in Figure <ref type="figure" target="#fig_5">5</ref> will not change after the transitive reduction step.</p><p>Similar to the DR-FFL-algorithm, the performance of the TRANSWESD-algorithm seems subpar. Three edges that exist in the true causal graph (see Figure <ref type="figure" target="#fig_1">2</ref>) are correctly estimated, two edges are estimated in the wrong direction, two edges are incorrectly estimated and two edges are incorrectly deemed absent from the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant Causal Prediction</head><p>The ICP-algorithm <ref type="bibr" target="#b14">(Meinshausen et al., 2016)</ref> combines both the advantage of the PC-algorithm in that it considers a multivariate system, and uses both observational and experimental data in a single analysis. Another advantage of the ICP-algorithm is that the perturbations inflicted on the data do not have to be node-specific: perturbations can be nonspecific and generic for subsets of nodes. The idea of the ICP is somewhat similar to that of the conditional correlation, used in the TRANSWESD-algorithm. If one variable is causally related to another variable, then the correlation in both the observational and the perturbation conditions will be similar. The ICP generalizes this idea to more conditions (called environments here) and uses the distribution of the residuals instead of considering the correlation. Then in a multivariate system, if the direct causes are obtained and conditioned on, then perturbing one of the variables will not lead to a change (is invariant) in the residuals. Hence, the core assumption of the ICP-algorithm is that the conditional distribution of an individual node, controlling for its direct causes, does not change across perturbations <ref type="bibr">(Peters et al., 2016)</ref>. In other words, a causal relation between two nodes only exists when the residuals do not change when a perturbation has taken place on any node, except for the dependent node in the regression, called the target node here. Recall that we use a nodewise procedure where each node is the dependent variable in turn.</p><p>The ICP-algorithm needs two components: the observational and experimental data, and an instrumental variable e that distinguishes between different perturbations, which we call environments, following <ref type="bibr" target="#b21">Peters et al. (2016)</ref>. This is similar to the experimental data matrix used in the DR-FFL and TRANSWESDalgorithms, where the rows indicate each separate perturbation. Another example of a situation where multiple environments exist is in data sets where every participant is measured on two or more time points. Every time point is then a unique environment. The minimal requirement is that the data must have at least two environments. Typically, one environment consists of observational data.</p><p>The ICP-algorithm first select a target node and then uses the remaining nodes to identify all possible subsets, similar to the PCalgorithm. Subsets can range from an empty subset (where the target node had no cause) to a subset that contains all remaining nodes. Figure <ref type="figure" target="#fig_6">6</ref> shows all possible subsets for a regression when node 5 is the target node; the true graph from Figure <ref type="figure" target="#fig_1">2</ref> is shown at the top. The ICP-algorithm regresses the target node onto each subset separately, and obtains its associated residual distribution. For the correct subset (indicated by the black square in Figure <ref type="figure" target="#fig_6">6</ref>) the conditional residual distribution (given the predictors) will be the same for any change on one of the other nodes. For if the distribution were to change, then it is clear that an incorrect subset of direct causes is obtained. The residual distribution is tested against the residuals of all remaining environments using a Kolmogorov-Smirnov test. Subsets whose residual distribution is equal across environments are called "invariant"; only those nodes that are part of each invariant subset (intersection) are said to be causes of the target node, and edges are drawn from those nodes to the target node.</p><p>To illustrate, Figure <ref type="figure" target="#fig_8">7</ref> shows the residual distribution of two subsets, the empty subset (left panel) and the true subset (right panel). The residual data for the two environments is separated by a dashed vertical line. It is obvious that the residual distributions for the empty subset are not equal (using a significance level of .05). In contrast, the residual distributions of the true subset (right panel) do not show a visual difference between the two environments. Based on this illustration, we conclude that the empty subset holds the incorrect subset of direct causes, while the true subset is "invariant" across environments. In the situation where more than one subset is accepted, the ICP-algorithm will only select the nodes that appear in all accepted subsets (the intersection of the subsets) and will return that set as the set of nodes that have a causal relation for which the target node is on the receiving end. The ICP-algorithm is then repeated for each node in the data, ensuring that every node is the target node once. Because the null hypothesis of the distribution of the residuals must hold for all subsets and all environments, the probability of obtaining a spurious connection is extremely small. This is only true if the test (an F-test on the residuals) is a level-alpha test (see <ref type="bibr" target="#b21">Peters et al., 2016</ref>; Theorem 1 for details). In our current set-up this implies that the data should be approximately normally distributed. Using the example graph in Figure <ref type="figure" target="#fig_1">2</ref>, we end up with the same graph shown in the top row of Figure <ref type="figure" target="#fig_6">6</ref>. It is important to note that the direct causes of a target node and its associated residuals must be independent of each other. This assumption guarantees that in this set-up there are no confounding variables. The next section will describe a solution when this assumption cannot be satisfied.</p><p>An assumption of the ICP and the HICP-algorithm (discussed in the next section) is that the target node cannot be intervened upon. Depending on the type of research this may or may not hold in practice. We provide two examples to clarify this point. First, suppose that a researcher has data from a therapeutic intervention with only sleep medication. Then it seems reasonable that a target variable like lack of concentration is not intervened upon directly; but is of course affected by sleep indirectly, as is investigated. As a second example, consider a treatment where therapy sessions are guided by psychoanalytical principles. While many aspects are intervened upon at once, it may be difficult to find target variables that satisfy the assumption of the (H) ICP-algorithm. In the empirical data analysis we provide an example where we believe the that ICP and HICP-algorithm are valid.</p><p>In constructing a graph with the causal relations obtained from different interventions, we may encounter cycles. Similar to <ref type="bibr" target="#b14">Meinshausen et al. (2016)</ref>, we assume in that case that the interventions were in terms of a shift where the parameters in the cycle are not too large (see <ref type="bibr" target="#b26">Rothenhäusler et al., 2015</ref>, for more details). This assumption entails that the causal relation is a solution to a difference equation and so the causal relations are not to be interpreted as being simultaneous.</p><p>The data we used to illustrate the ICP-algorithm contains six unique environments: one environment that contains the observational data, and five environments in which we perturbed all nodes except node 5 (see the section on data simulation for a more detailed description). We only select these two environments because one of the main assumptions of the ICP-algorithm states that perturbations can take place at all nodes but the target node <ref type="bibr" target="#b21">(Peters et al., 2016)</ref>. We use the R-package InvariantCausalPrediction (Version .7-2, <ref type="bibr" target="#b13">Meinshausen, 2018)</ref> to run the ICP and the HICP-algorithm.</p><p>Overall, the ICP-algorithm performs exceptionally well in this illustration. All edges that exist in the true causal graph (see Figure <ref type="figure" target="#fig_1">2</ref>) are correctly estimated, and there are no edges that are incorrectly estimated or determined to be absent from the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden Invariant Causal Prediction</head><p>The HICP-algorithm <ref type="bibr" target="#b22">(Peters et al., 2017)</ref> is similar to the ICPalgorithm discussed previously. The major difference between the two algorithms is that the HICP-algorithm controls for hidden variables, variables that are unobserved, but may affect the observed variables. Where the ICP-algorithm assumes that a target node's direct causes and its residuals are independent, this assumption can no longer be satisfied when hidden variables exist. To illustrate, see Figure <ref type="figure" target="#fig_7">8</ref>, after <ref type="bibr" target="#b22">Peters et al. (2017)</ref>. Here, Y denotes the target node, X its direct cause, H the hidden variable, and Z the instrumental variable. As seen in Figure <ref type="figure" target="#fig_7">8</ref>, the hidden variable affects both the target node and its direct cause. Thus, H is a confounding variable. If one were to use the ICPalgorithm, where hidden variables are not accounted for, the correlation between the target node Y and its direct cause X will be inflated due to the influence of the hidden variable H. It is impossible to infer the unique influence between X and Y in this illustration. Consider the setup in Figure <ref type="figure" target="#fig_7">8</ref>. For explanatory purposes, the HICP-algorithm implements an instrumental variable Z to remove the effect of the hidden variable on X ! Y. This instrumental variable is assumed not to directly influence the target node Y. Here, the environmental variable e is used as the instrumental variable, as the division of the data into two separate environments does not directly influence the target node Y. By using the environmental variable e as the instrumental variable Z, the regression of the target node onto the remaining variables will be split for the different time points, and the difference between these time points is used to estimate the causal effect.</p><p>For explanatory purposes, we name the causal effect from X to Y to be a (as shown in Figure <ref type="figure" target="#fig_7">8</ref> and as described by <ref type="bibr" target="#b22">Peters et al., 2017)</ref>. The variables X and Y are defined as follows:</p><formula xml:id="formula_8">X ¼ bZ þ cH þ N x Y ¼ aX þ dH þ N y (9)</formula><p>which follows directly from Figure <ref type="figure" target="#fig_7">8</ref>. The terms N x and N y denote the error terms (here we assume normally distributed variables with mean 0 and variance r). The estimate of the causal effect from X to Y, â is defined as follows:</p><formula xml:id="formula_9">â ¼ cov½X; Y var½X ¼ a þ dc var½H var½X (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where a denotes the causal effect from X to Y, and dc var½H var½X the bias term to account for the hidden variable. When there are no hidden variables, d equals 0, and the bias term will disappear as a result of this. When hidden variables exist but not accounted for, one ends up with a biased estimate for the causal effect from X to Y. This shows that, in this situation, the estimate for the causal effect is not representative of the true causal effect. The HICP-algorithm follows a two-step regression to estimate a. It first regresses X on Z to estimate b, where the estimate is denoted by b. Then, this coefficient is used to estimate a:</p><formula xml:id="formula_11">â ¼ cov½ bZ; Y b 2 var½Z ¼ a b2 var½Z b 2 var½Z (11)</formula><p>When the sample size becomes large, b and b will be arbitrarily close. Equation (11) shows that, in the limit, the estimate for â will be equal to the true causal effect. See Appendix C for a more detailed description. It is important to note that the HICPalgorithm assumes that the hidden variable H and the instrumental variable Z (the environmental variable e) are independent of one another. The ICP-algorithm has to satisfy the assumption that the causes of a target node and its associated residuals are uncorrelated. In contrast, the HICP-algorithm frees up this assumption. Another difference between the HICP and the ICPalgorithm is that the HICP-algorithm does not create subsets of the set of nodes that remain after selecting a target node. To speed up computations, all variables are simultaneously tested to see if they are a cause of the target node.</p><p>When we select node 5 as our target node, the correct subset of direct causes is depicted in Figure <ref type="figure" target="#fig_6">6</ref>. The causal coefficients a are estimated as follows. Let X be a n 3 (p À 1) matrix that contains the raw data (both observational and experimental data) for all variables but the target node:  One can immediately see that nodes 1, 3, and 4 have a causal coefficient that is very close to 1 and that node 2 has an almost nonexistent causal coefficient. The causal coefficients in a are then tested for significance. When we repeat the HICP-algorithm for each node in our illustration data (that does not contain any hidden variables), we end up with the causal graph depicted in Figure <ref type="figure" target="#fig_9">9</ref>. It is noticeable that, next to the edges that are present in the true causal graph (shown in Figure <ref type="figure" target="#fig_1">2</ref>), many spurious edges exist. Because the HICP-algorithm tests all variables in the data simultaneously, spurious edges can arise as a result of partialing out the effect of the hidden variables. With a large sample size (like the sample size of the illustration data), these spurious edges are easier deemed as significant causal relations.</p><p>We have added a detailed description of the entire estimation part as it occurs in the R-package for the interested reader in Appendix C. We use the R-package InvariantCausalPrediction (Version .7-2; <ref type="bibr" target="#b13">Meinshausen, 2018)</ref> to run the HICP-algorithm. We programmed a wrapper function so that both the ICP and the HICP-algorithm are repeated for every variable in the data, and then combined into a single adjacency matrix, which is publicly available at <ref type="url" target="https://osf.io/n8gxh/">https://osf.io/n8gxh/</ref>. Overall, the HICP-algorithm does not seem to perform too well in this illustration. All edges that exist in the true causal graph (see Figure <ref type="figure" target="#fig_1">2</ref>) are correctly estimated, but there are many edges (6) that are incorrectly estimated.</p><p>Table <ref type="table" target="#tab_0">1</ref> gives an overview of the algorithms discussed in this section. In this overview, one can see which of the five algorithms is most suitable for a specific dataset. For example, all algorithms can be used to estimate a within-subjects causal graph with the exception of the TRANSWESD-algorithm, and the DR-FFL-algorithm is unsuitable for a between-subjects analysis. Table <ref type="table" target="#tab_0">1</ref> also shows every algorithm's limitations. Based on the criteria that we set for possible application to psychological data, the ICP and the HICP-algorithms seem the most suitable and the most versatile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Simulation</head><p>To study the accuracy of the algorithms that we described in the previous section, we simulate data according to a DAG, and apply each of the five algorithms to estimate a causal graph. In this section, we first discuss how we constructed the DAGs, after which we show how we simulated data based on these DAGs.</p><p>We start out with a p 3 p adjacency matrix that consists of solely 0s. Then, we randomly select k cells and set them to 1s: a 0 indicates the absence of an edge, and a 1 the presence of an edge. The parameter k here denotes the number of edges that is dependent on the prespecified density d (d • p 2 À p = k). The diagonal of the matrix is always 0, as self-loops are not permitted at this point in time. All graphs were constructed such that the number of edges in the graph must equal k, (2) each node in the graph must have at least one connection, (3) all edge weights in the graph must equal 1, (4) all nodes in G must be connected in one component and ( <ref type="formula">5</ref>), the graph cannot contain any cycles. This process may result in the following adjacency matrix: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p><formula xml:id="formula_12">0 B B B B @ 1 C C C C A !</formula><p>0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0</p><formula xml:id="formula_13">0 B B B B @ 1 C C C C A ! 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 B B B B @ 1 C C C C A</formula><p>Figure <ref type="figure" target="#fig_10">10</ref> shows the visualization of the graph after the initiation process (left panel) and after all criteria are satisfied (right panel). After the graph is finalized, its adjacency matrix is ordered so that all nonzero elements are in the lower-diagonal part of the matrix. This process does not alter the graph itself, solely its representation.</p><p>Based on the adjacency matrix of the simulated DAG that we call B, we simulate data in which some sort of perturbation has taken place. We create an n 3 p matrix (X) that we fill with numbers drawn from a normal distribution with a mean of 0 and a standard deviation of .5. We then select a node (called the target node), and we create n error terms called u drawn from a normal distribution with a mean of 0 and a standard deviation of .5. We then create observational data in a following manner: where b contains the coefficient for the selected target node, and X b the column in X corresponding with the target node. The next step is to create perturbation data. For each node independently, we select all data from our original matrix X, excluding the target node. This n 3 (p À 1) matrix is then multiplied with p À 1 values (called a) drawn from a normal distribution with a predetermined mean (m) and standard deviation (SD), creating a new matrix X per . We then create experimental data similar to the previous step:</p><formula xml:id="formula_14">y per ¼ b aX per ð Þþ u<label>(15)</label></formula><p>We repeat this process for all nodes in the graph. We also simulate data in which we added hidden variables, following <ref type="bibr" target="#b22">Peters et al. (2017)</ref>. The equations for the simulation of data with hidden variables are similar to the regular data simulation (shown in ( <ref type="formula">14</ref>) and ( <ref type="formula" target="#formula_14">15</ref>)) with the following addition. We create hidden variables by drawing n values from a normal distribution (with a mean of 1 and a standard deviation of 1). These n values are then multiplied by a parameter h that we have set to be 5. We then take the outer product of h and a vector of 1s of length p, and add this to our matrix X. This matrix is then used in a similar manner as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical Evaluation of Causal Inference Algorithms</head><p>In this simulation study we evaluated the performance of five methods of causal inference. We simulated six DAGs: three with p = 5 nodes, and three with p = 10 nodes. We varied the density of the graphs (the proportion of edges present in the graph) d [ {.1, .25, .5}. Figure <ref type="figure" target="#fig_11">11</ref> depicts all causal graphs that were created for this simulation study. We varied the number of participants n [ {50, 100, 500, 1000; 5000} and the mean of the perturbation distribution m [ {1, 5}. These values correspond to a small and large perturbation effect. The standard deviation of the perturbation distribution varied SD [ {.5, 5}. These values correspond to an effective and a noisy perturbation. For the DR-FFL and the TRANSWESD-algorithms, we varied b [ {.5, 1, 1.64, 1.96, 2.58}, and c was set to 0. We simulated data with and without hidden variables to see how the addition of hidden variables affected the performance of the algorithms. We ran each simulation condition 100 times for each combination of parameters. We set the significance level for the PC, ICP, and HICP-algorithm to be .05. All simulated data, as well as the used R-code are publicly available at <ref type="url" target="https://osf.io/n8gxh/">https://osf.io/n8gxh/</ref> and the online supplemental materials.</p><p>In the numerical evaluation we focus on Matthew's correlation coefficient (MCC; <ref type="bibr" target="#b24">Powers, 2011)</ref>. The MCC takes both true and false positives and negatives into account and gives a good overview of the overall performance of the different algorithms. The MCC is calculated as follows:</p><formula xml:id="formula_15">MCC ¼ TP3TN À FP3FN ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðTP þ FPÞðTP þ FNÞðTN þ FPÞðTN þ FNÞ p (<label>16</label></formula><formula xml:id="formula_16">)</formula><p>where TP represents the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. The MCC can be interpreted the same way as a regular correlation coefficient <ref type="bibr" target="#b12">(Matthews, 1975)</ref>. The more positive the MCC, the better the correspondence between simulated and estimated edges. We also calculated other metrics (e.g., positive/negative predictive rate, false negative/positive rate), but we chose not to present these here. Results for all metrics can be found online at <ref type="url" target="https://osf.io/n8gxh/">https:// osf.io/n8gxh/</ref> and the online supplemental materials. Figure <ref type="figure" target="#fig_12">12</ref> shows the MCC for the different algorithms. For clarity of presentation, we only show results for p = 10 nodes, with a graph density d = .25. All other results can be found online. Overall, the ICP and HICP-algorithms have the highest MCC. The MCC of the PC-algorithm is generally low. The PC-algorithm seems to benefit from a density that is not too high. The MCC increases when the graph density d increases from .1 to .25, but decreases again when d is increased to .5. Also, the size of the graph (reflected by p) has an impact on the MCC: when p is increased from 5 to 10, the MCC decreases from on average .55 to .23. The PC-algorithm can have issues determining the direction of directed edges. In around 18.75% of the simulations, the PC-algorithm returned an undirected graph. It turns out that the MCC is almost always higher when we do not take the direction of the edges into account. This effect is especially present when the density of the graphs is low. To illustrate, when d = .1, the average MCC increases from .47 to .87 when we do not take the direction of the edges into account, but when d = .5, the difference in average MCC is only .05 on average. These results indicate that the PC-algorithm can be useful in determining the pairs of variables between which a causal  relation exists, but that it may not be the appropriate algorithm to determine the direction of these causal relations.</p><p>In general, both the DR-FFL and the TRANSWESD-algorithms perform badly, as seen in Figure <ref type="figure" target="#fig_12">12</ref> Both the mean of the perturbation distribution (m) and its standard deviation (SD) do not have an effect on their performances. When p = 5, both algorithms seem to perform a little better, but the difference in MCC is only about .1 between the two network sizes. The root cause of this seems to lie in the first step of the transitive reduction scheme that the two algorithms apply. Where on average each simulated graph had about 13.45 edges, both the DR-FFL and the TRANSWESDalgorithms returned on average 2.95. When there are only a few edges present in the graph, transitive reduction works sub optimally, as even fewer edges can be removed from the graph. Our findings indicate that transitive reduction may not be the best way to go when estimating a causal graph. We tried a different approach in the data simulation, but we found similar results. We also simulated data with an extremely large perturbation mean (m ¼ 100), but the MCC hardly improved.</p><p>The ICP-algorithm seems to do a better job at correctly estimating the causal graphs in some cases. Overall, the ICP-algorithm works best at d = .25. The MCC when the network density is lower (or higher) is much lower. In general, when p = 5, the ICP-algorithm performs best (at d = .25). In that situation, the mean and standard deviation of the perturbation distribution do not have an effect on the MCC. In contrast, when p = 10, the influence of these two parameters is much bigger. As shown in Figure <ref type="figure" target="#fig_12">12</ref>, only when m ¼ 5 and SD = .5 is the MCC high, in all other cases it is mediocre at best. The ICP-algorithm can be conservative: of all the edges that it finds, the algorithm will only take the intersection as the (sub)set of causes of a target node. The results shown here suggest that the ICP-algorithm can estimate causal graphs pretty accurately when there are not too few or too many edges in the graph, and when the perturbation effect is strong and precise enough.</p><p>Lastly, the HICP-algorithm displays a mixed performance. When p = 5, we see a similar pattern as with the ICP-algorithm, but less extreme. With a graph density d = .25, the HICP-algorithm has a high MCC, but for the other two graph densities, the MCC is smaller. The declining effect that we observe in Figure <ref type="figure" target="#fig_12">12</ref> returns for other graph sizes and densities as well: when the sample size n increases, the MCC decreases. The cause of the decrease can be found in the number of false positives. Like we saw in the illustration in the previous section, the causal graph that is the result of the HICP-algorithm often contains spurious edges. This is likely due to the fact that the HICP-algorithm only investigates the entire set of remaining nodes in the graph to determine the causes of the target node, in contrast to the ICPalgorithm that investigates each possible subset separately. Because of this set-up, spurious edges can occur, which become significant more easily with a larger sample size. This indicates that the HICP-algorithm does not need a large sample to estimate a causal graph.</p><p>Figure <ref type="figure" target="#fig_13">13</ref> gives a more detailed insight into the strengths and weaknesses of each algorithm. To increase readability of the causal graphs, we chose to display the results for p = 5 and d = .25. These graphs are publicly available for all simulation conditions. The number of false positives (red edges) for the DR-FFL, TRANSWESD, and HICP-algorithm immediately stand out. The HICP-algorithm stands out from the DR-FFL and the TRANSWESD-algorithm because it also has a high number of true positives, indicated by the thickness and saturation of the blue edges. The conservativeness of the ICP-algorithm is less visible, but present nonetheless. Where the HICP and the PCalgorithm correctly identified the present edges (as shown in the true graph) in either 99À100% of the simulations, the ICP-algorithm's rate lies around 90À96%, which is still very high. The PC-algorithm's struggle with determining the direction of the edges is also depicted in Figure <ref type="figure" target="#fig_13">13</ref>. The direction of two edges here (e 42 and e 32 ) are just as often correctly as incorrectly identified.</p><p>We also ran the simulation study using data that contained hidden variables. The results are very similar to the results using data without hidden variables. See Appendix D for the results using data with hidden variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to Empirical Perturbation Data</head><p>The dataset used here is a dataset collected by <ref type="bibr" target="#b6">Hoekstra et al. (2018)</ref> and comprises 30 participants who completed a questionnaire that measures their attitude toward meat consumption. The questionnaire consists of 11 items that focused on the cognitive (six items) and affect (five items) side of one's attitude toward meat consumption. Responses were measured on a 6-point Likert scale ranging from 1 (completely disagree) to 6 (completely agree). The experiment started with the questionnaire serving as a baseline measure, after which a hypothetical scenario was presented intended to perturb a participant's response to a single item. For example, if a participant responded to the item "animals are inferior to humans" with "completely agree," this participant would get the hypothetical scenario "imagine animals are not inferior to humans. How does this influence your attitude toward the consumption of meat?." Participants then completed the same questionnaire. This process was repeated for every item in the questionnaire. This resulted in 12 questionnaires for each participant. All participants (N = 30) were included in the analyses. We removed four missing measurements of three participants as one item was missing from these measurements. Participants had a mean age of 23.63 years (SD = 9.01 years) and 70% identified themselves as female. Table <ref type="table" target="#tab_1">2</ref> gives an overview of the items, including the means, standard deviation and the mean change between the baseline and the perturbation environment. All raw data, including the questionnaire and hypothetical scenarios, are published online <ref type="bibr" target="#b6">(Hoekstra et al., 2018)</ref>.</p><p>Similar to the simulation, we only used the observational data to run the PC-algorithm. In the illustration of the DR-FFL-algorithm, we averaged the data across the sample to create a between-subjects graph. We arbitrarily set b to be 1.3 for both the DR-FFL and the TRANSWESD-algorithm and c to be 1. For the ICP and HICPalgorithms, we followed a similar approach to that used in <ref type="bibr" target="#b11">Kossakowski et al. (2021)</ref>. We compare every combination of environments (one observational and 11 perturbation environments, resulting in 66 pairs of environments) and combine these results into a single causal graph. The thicker and more saturated an edge between two nodes, the more often this causal relation is found and the more confident we can be in the true existence of that causal relation. For the PC, the ICP and the HICP-algorithms, we set a = .01. Similar to <ref type="bibr" target="#b14">Meinshausen et al. (2016)</ref>, combining the separate graphs from the different combinations of interventions may result in cycles. These cycles do not invalidate the causal interpretation only if the parameters in the cycle are not too large (i.e., the product of the parameters in the cycle is smaller than 1; see <ref type="bibr">Rothenhäusler et al., 2015, for further details)</ref>.</p><p>Figure <ref type="figure" target="#fig_14">14</ref> shows the results for each algorithm. We standardized the layout to improve visual inspection. The results follow the simulation results with a small sample size. The PC-algorithm only found two edges, but could not determine their causal direction due to a lack of information that is needed for this; more edges are needed to determine the causal direction. The DR-FFL-algorithm removed four edges from the perturbation graph that is initially formed (using b; = 1.3), while the TRANSWESD-algorithm did not remove any edges (using b; = 1.3 and c = 1). The ICP-algorithm returned many edges due to the low number of participants. Note. The mean perturbation effect was measured by taking the mean of the difference between the baseline measurement and the measurement in which the specific item was perturbed. The graph returned by the HICP-algorithm is very promising and interpretable. The edge a_taste ! c_suffering is often found. This means that, when a participant's taste in meat changes, a change in their opinion on the contribution to animal suffering when eating meat changes as well. The reverse relation c_suffering ! a_taste was found as well, but to a lesser extent. The results for the HICP-algorithm also appear to be quite stable. The graph does not change when changing a from .05 to .01. All in all, this empirical example suggests that especially the HICPalgorithm is suitable for psychological data of this size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study compared five different algorithms that are used for causal inference. We provided a simulation study in which we showed how well each algorithm is able to estimate the causal graph under which the data were simulated. We simulated data from causal graphs with different properties to assess the effect of the number of nodes and the density of the graph on the estimation of the graph itself. The results that we showed did not present us with a clear winner: only under specific circumstances did each algorithm perform well. The exception to this are the DR-FFL and TRANSWESD-algorithms that never showed a good performance, contrary to earlier studies on these algorithms <ref type="bibr" target="#b23">(Pinna et al., 2013)</ref>. We also applied the various algorithms to empirical data that measured participants' attitude toward meat consumption. The HICP-algorithm seemed to perform best for this dataset, and provided a clear and interpretable graph.</p><p>Every algorithm that is discussed here had its own advantages and disadvantages. The PC-algorithm only uses observational data to estimate the causal relations between variables. Although this implies that less data are needed to run the algorithm, we did see that the MCC of the PC-algorithm is never higher than .8. Furthermore, we found that the PC-algorithm often has the location of a causal relation correctly identified, but not its direction. This may suggest that the PC-algorithm is useful to find the location of the causal relations, but that some other algorithm is needed to identify the direction of that relation.</p><p>It is quite noticeable that the DR-FFL and the TRANSWESDalgorithms do not perform as well as the ICP and HICP-algorithm. In nearly 75% of all simulations 10% or less of the number of possible edges survived the first step of these algorithms. This is why the MCC for both the DR-FFL and the TRANSWESD is so low across simulation conditions.</p><p>We performed a similar simulation study where we only select 1 node (instead of p À 1) that was perturbed and used to create our experimental data. However, results from this design are similar to the results presented here. We also varied the b; and c; values to obtain better results, but there was no clear picture for the settings in our simulations. It is possible that a different data simulation design might give different results, but this remains to be investigated.</p><p>The ICP-algorithm has proven to be able to correctly identify edges that are present in the true graphs in our simulations. It also often makes the correct decision when edges are absent in the true graphs. However, the ICP-algorithm will only perform well when specific conditions are satisfied with respect to the data. The high MCC was obtained in this study when the mean of the perturbation distribution was strong, and its associated standard deviation small. In all other conditions, the MCC was mediocre and in some cases poor. The ICPalgorithm has a low false positive rate at the cost of a relatively low true positive rate. A conservative attitude is not necessarily a disadvantage of an algorithm, but when it is applied to empirical data, the resulting graph may be sparser than one would hope for when the sample size is large (see <ref type="bibr" target="#b11">Kossakowski et al., 2021</ref>, for an example), or more dense when the sample size is small. Also, the ICP-algorithm investigates every possible subset of the variables that remain after selecting a target variable. This step leads to computational issues when graphs with a large number of nodes are studied. When a graph has p = 5 nodes, the number of subsets per target variable is 16. When a graph has p = 10 nodes, the number of subset grows to 516, and for p = 15 nodes, the number of subsets equals 16,384. In the future we hope to develop an adaptation for the ICP-algorithm where a subset selection is made in such a way that the computation time decreases while maintaining similar specificity and sensitivity values.</p><p>Lastly, the HICP-algorithm outperforms the other algorithms in terms of the MCC in many simulation conditions. However, as is clearly shown in Figure <ref type="figure" target="#fig_13">13</ref>, many edges are incorrectly seen as present (false positives; red edges) next to the correctly identified edges (true positives; blue edges). This phenomenon most likely occurs because not every possible subset is investigated separately, as is the case in the ICP-algorithm. This approach was implemented because of computational issues.</p><p>Both the ICP and the HICP-algorithm have a nodewise approach where each variable in the graph is the target node in turn. This implies that a reciprocal relation between two variables (X ! Y and Y ! X) is possible, as each variable is the target node during the analyses. When one wants to estimate reciprocal causal relations, more environments are needed to accurately estimate these relations. We chose to exclude reciprocal causal relations from our study due to the fact that the other algorithms that are discussed are not able to estimate these. In future research, reciprocal causal relations could be investigated by adding them to the graph in a simulation study.</p><p>Next to the disadvantages that we discussed previously, we made some arbitrary decisions for the algorithms in this study. The PC, ICP, and HICP-algorithm all require a significance level that we set to be .05. The DR-FFL and the TRANSWESD-algorithms have one or two threshold parameters that need to be set before the analysis. We chose to use different values to evaluate the effect of these parameters. Results of our simulation study showed that the value of these thresholds impact the MCC of the algorithm: the higher the threshold, the fewer edges are returned after the first step of these two algorithms and the fewer edges can be reduced from the graph. Choosing a value for these threshold parameters is not trivial. Ideally, one would want to set these parameters in such a way that the false and true positive rate are balanced. It remains that, with the DR-FFL and the TRANSWESDalgorithm, setting the threshold parameter(s) is no trivial matter and confounds the results tremendously. Future research could look into the possibility of using maximum likelihood estimation to obtain a reasonable threshold parameter based on the data.</p><p>We simulated data without and with the addition of hidden variables. Although they are present, it is hard to find differences between the results for data without, and data with hidden variables. Due to the high number of simulation conditions that we already have, we chose not to add any by varying the strength of the hidden variable. It is possible that a hidden variable with a stronger effect may result in larger differences in specificity and sensitivity. In a future extension of this study, one can vary the hidden variable to investigate the effect of a hidden variable on the results.</p><p>As every algorithm has its own advantages and disadvantages, a possible combination of two or more algorithm may be the solution. For instance, one can use the PC-algorithm to determine the skeleton of a causal graph, and use that input for the ICP-algorithm. In this combination, the number of subsets decreases substantially. Another option would be to copy the subset design of the ICP-algorithm, and use in with the HICP-algorithm. Investigating multiple subsets may result in a lower number of false positives and a more accurate depiction of the true causal graph.</p><p>The set of algorithms discussed in this study is not complete. Other algorithms exist in the literature that are potentially interesting to further explore. However, these algorithms do not combine observational and perturbation data but only use observational data. Algorithms that use observational data include a directional dependence model using copulas <ref type="bibr" target="#b30">(Sungur, 2005)</ref>, a linear causal acyclic model <ref type="bibr" target="#b28">(Shimizu et al., 2006)</ref>, or a directional dependence analysis with possible confounding variables <ref type="bibr" target="#b32">(Wiedermann &amp; Sebastian, 2019)</ref>, and general cyclic linear models <ref type="bibr" target="#b1">(Drton et al., 2019)</ref>. One could even extend the existing simulation study to include algorithms that estimate causal relations between latent variables (e.g., <ref type="bibr" target="#b27">Shimizu et al., 2009)</ref>, or estimate nonlinear causal relations (e.g., <ref type="bibr" target="#b5">Heinze-Deml et al., 2018)</ref> instead of just linear causal relations, or use those nonlinearity algorithms to resolve causal relations <ref type="bibr" target="#b15">(Mooij et al., 2011)</ref>. The HICP-algorithm is a suitable option when variables and their residuals are correlated, which is an indication of hidden or confounding variables.</p><p>Measurement error will often, if not always be present in psychological research. In our simulation study we added measurement error to reflect this. Because the basic elements in our study were regressions, naturally, with increasing amounts of measurement error, the power will decrease. In our study, we found measurement error, but no systematic errors like spurious edges that appear in every simulation result. It is possible to add a SEM (latent variable) model to model the measurement error. This would entail several indicator variables per construct; thus, modeling the measurement error more explicitly, and then creating the network between the latent variables of the SEM model. A different option would be the model presented by <ref type="bibr" target="#b34">Zhang et al. (2018)</ref> that can also manage measurement error.</p><p>To our knowledge, this is the first study that compared different algorithms for causal inference based on experimental data. Based on the simulation results, we gain more insight into the accuracy of each algorithm, and how suited they are for empirical (psychological) data. The ICP and HICP-algorithm are the top candidates to be used in psychological research. As hidden variables are a common problem in psychological research, a possible combination of the ICP and HICP-algorithm may be the best plan of attack to estimate causal relations between psychological variables. Results from the empirical application demonstrated that the ICP and the HICP-algorithms have the most potential to be suitable for psychological data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A D-Separation in a DAG</head><p>Earlier we described the four different causal structures that can exist in a causal graph (see Figure <ref type="figure" target="#fig_0">1</ref>). By determining the conditional (in)dependencies between sets of variables, the PC-algorithm estimates a directed acyclic graph (DAG). The notion of conditional independencies can be extended to the idea of directed (d) separation). D-separation generalizes a separation in a graph between two variables. Consider the graph in Figure <ref type="figure" target="#fig_0">1</ref>, where we have a path from variable X to variable Y (Figure <ref type="figure" target="#fig_0">1</ref>, left panel). Variables X and Y are d-separated in graph G (denoted by X ╨ G Y j Z) given a variable Z if Z blocks the path from any node in X to any node in Y. D-separation is relatively easy in the case of a chain or a common cause structure (Figure <ref type="figure" target="#fig_0">1</ref>, three left panels). With these specific structure, X and Y are d-separated given Z when Z is observed. When Z is observed, it "blocks" the path from X to Y. The reverse is true for the collider structure (Figure <ref type="figure" target="#fig_0">1</ref>, right panel): X and Y are d-separated as long as Z, or any of its descendants are not conditioned on. For a disjoint set of random variables X, Y, and Z with joint probability distribution P, we note that X is conditionally independent of Y given Z by X ╨ P Y j Z. The notion of d-separation is used in the following assumption: Assumption 1: We assume that for disjoint sets of variables X, Y, and Z the causal Markov condition is satisfied, which specifies that</p><formula xml:id="formula_17">X ╨ G Y j Z ) X ╨ P Y j Z (17)</formula><p>This assumption guarantees that, when we find that two variables are d-separated, in the graph G these two variables are conditionally independent given a third variable in the probability distribution P. While the Markov assumption allows to move from the graph to the distribution, the reverse is not guaranteed. To be able to infer from conditional independencies obtained from the probability distribution that certain causal relations are valid, we also require that any conditional independence in the probability distribution implies a d-separation in the graph. This is entailed in the following assumption: Assumption 2: We assume that for disjoint sets of variables X, Y, and Z the causal faithfulness condition is satisfied, which specifies that</p><formula xml:id="formula_18">X ╨ P Y j Z ) X ╨ G Y j Z<label>(18)</label></formula><p>This assumption ensures that, when two variables are conditionally independent given a third in the probability distribution P, they are also d-separated in the graph G given that third variable. The Markov and faithfulness assumptions allow for consistent inference for a causal graph. See <ref type="bibr" target="#b18">Pearl (2009)</ref> or <ref type="bibr" target="#b22">Peters et al. (2017)</ref> for more details.</p><p>(Appendices continue)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Theory of Transitive Reduction</head><p>Both the DR-FFL and the TRANSWESD-algorithm use transitive reduction to estimate a causal graph. Both algorithms first draw up a perturbation graph in which causal relations between variables exist that exceed a prespecified threshold. Often (but not always), the correlation between two variables is used. The idea here is that, when a correlation between two variables is nonzero, then there must be either a direct or an indirect relation between these two variables. Transitive reduction aims to remove direct effects where there should not be one, by considering alternative paths between two variables.</p><p>To illustrate transitive reduction, we have set up two examples, visualized in Figure <ref type="figure" target="#fig_15">B1</ref>. Here, we consider the causal relation between variables W and Y. <ref type="bibr" target="#b33">Wright (1921)</ref> showed that the correlation between W and Y is sum of the product of the path coefficients, denoted by b ij . In the first example, shown in the left panel of Figure <ref type="figure" target="#fig_15">B1</ref>, two paths exist from W to Y: W ! X ! Y and W ! Z ! Y. The correlation q WY then becomes (À.20) (À.20) þ (À.20)(À.20) = .08. The following criterion is used to remove a direct effect from the perturbation graph: min q f;;sg WX1 ; . . . ; q</p><formula xml:id="formula_19">f;;sg XkY &amp; ' &gt; q f;;sg WY (<label>19</label></formula><formula xml:id="formula_20">)</formula><p>where {;, s} denote the observational environment in which no perturbations have taken place (;), and the experimental environment in which perturbations have taken place on variable s.</p><p>The variables X 1 . . . X k denote the variables that lie on the path from W to Y. In other words, (19) states that if the smallest absolute path coefficient is larger than the direct effect between two variables, then the direct effect is to be removed from the perturbation graph. In our illustration, the correlation between W and Y (q WY = .08) is smaller than the smallest path coefficient on either path (all path coefficients are .20) and there should not be a direct effect from W to Y. Therefore, the left panel of Figure <ref type="figure" target="#fig_15">B1</ref> shows that transitive reduction is able to come to the right conclusion. However, the criterion in ( <ref type="formula" target="#formula_19">19</ref>) is necessary, but it turns that that it is not sufficient to find the true causal graph. This is shown with the example in the right panel of Figure <ref type="figure" target="#fig_15">B1</ref>. Here, there is a direct effect from W to Y. Now q WY becomes À.02, which is still smaller than the smallest path coefficient. Here, transitive reduction would erroneously remove the direct effect from W to Y. This shows that transitive reduction may not reach the correct causal graph, especially when the path coefficients are small. Specifically, the criterion in (19) will not work when the sum of the direct effect and q ij is smaller than the smallest absolute path coefficient on any path between i and j. The HICP-algorithm controls for hidden variables by using an instrumental variable Z. This instrumental variable cannot directly influence the target node Y, as shown in Figure <ref type="figure" target="#fig_7">8</ref>. By using the instrumental variable Z, the regression of the target node onto the remaining variables will be split for the different time points, and the difference between these time points is used to estimate the causal effect. The causal effect from X to Y, denoted by â, is calculated as follows:</p><formula xml:id="formula_21">â ¼ cov½X; Y var½X ¼ a þ dcvar½H var½X (20) â ¼ cov½X; Y var½X ¼ cov½X; abZ þ ðac þ dÞH var½X ¼ ab cov½X; Z þ ðac þ dÞ cov½X; H var½X ¼ ab cov½bZ þ cH; Z þ ðac þ dÞ cov½bZ þ cH; H var½X ¼ ab 2 var½Z þ cðac þ dÞ var½H b var½Z þ c 2 var½H þ var N x ½ ¼ ab 2 þ c 2 a þ dc b 2 þ c 2 þ r 2 x ¼ a b 2 þ c 2 À Á þ dc b 2 þ c 2 À Á ¼ a þ dc var½X (21)</formula><p>where a, b, d, and c represent relations between X, Y, H, and Z. See Figure <ref type="figure" target="#fig_7">8</ref> for a visual representation. We can rewrite Equation (21) as follows:</p><formula xml:id="formula_22">â ¼ cov½X; Y var½X ¼ cov½X; aX þ cov½X; dH var½X ¼ a var½X þ dc var½H var½X ¼ a þ dc var½H var½X (22)</formula><p>where the term dc var½H var ½X will be 0 when there are no hidden variables. To estimate â, the HICP-algorithm uses a two-step procedure. It first estimates b (the effect from the instrumental variable Z to variable X) , after which b is used to estimate â :</p><formula xml:id="formula_23">â ¼ cov½ bZ; Y b 2 var½Z ¼ b cov½Z; bZ þ e b 2 var½Z ¼ b2 var½Z b 2 var½Z (23)</formula><p>Note that the equations here are used after the target node is selected. The computational steps that are taken to estimate all the causal effects are described below for a target node. We programmed a function that repeats these steps for every variable in the data. The HICP-algorithm uses the instrumental variable to divide the data into two subsets. The first subset contains data from the first environment (often that part of the data in which no perturbation has taken place). The second subset consists of all the remaining data. We can rewrite Equation <ref type="formula">21</ref>to make it computationally appropriate:</p><formula xml:id="formula_24">â ¼ X 0 X ð Þ À1 X 0 y ¼ X 0 1 X 1 n 1 À X 0 2 X 2 n 2 ! À1 Á X 0 1 Y 1 n 1 À X 0 2 Y 2 n 2 ! ¼ cov X 1 ; Y ½ À cov X 2 ; Y ½ var X 1 ½ À var X 2 ½ (24)</formula><p>where X 1 and X 2 represent the predictor variables for the two environments, and Y 1 and Y 2 denote the scores on the target node for the two environments. The parameters n 1 and n 2 denote the number of participants that exist in the two environments. The result of Equation ( <ref type="formula">24</ref>) is a p 3 1 matrix that holds all the regression coefficients from every remaining node to the target node. After calculating â we proceed with the calculation of Z-values for all participants per environment:</p><formula xml:id="formula_25">Z i;e ¼ Às Á X P p¼1 s X 0 1 Y 1 n 1 À X 0 2 Y 2 n 2 ! þ Y i;e s<label>(25)</label></formula><p>where s ¼</p><formula xml:id="formula_26">X 0 1 X 1 n 1 À X 0 2 X 2 n 2 h i À1</formula><p>• X i,e . The parameter s is created for each participant i and environment e individually. The matrix X i,e is a 1 3 p vector that holds the observational data for participant i and p variables. Two separate n 3 p matrices emerge from this equation: one for the first environment, and one for the second environment. The next step includes the calculation of r:</p><formula xml:id="formula_27">(Appendices continue) r ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi diag s 2 Z 1 ð Þ n 1 þ s 2 Z 2 ð Þ n 2 s (26)</formula><p>where s 2 (Z 1 ) and s 2 (Z 2 ) denote the covariance matrix of the Z-values that we calculated previously for environments 1 and 2, and n 1 and n 2 the number of participants in the first and second environment. The term diag here indicates that we only take the diagonal of the result of</p><formula xml:id="formula_28">s 2 Z 1 ð Þ n 1 þ s 2 Z 2 ð Þ n 2 .</formula><p>In the last step we calculate the p-values associated with â.</p><p>These are calculated in the following manner:</p><formula xml:id="formula_29">p ¼ max 2K Á 1 À t j bj=max 10 À10 ; r À Á 1 ( (<label>27</label></formula><formula xml:id="formula_30">)</formula><p>where K is the number of environments (2 in this study). The parameter t() denotes the critical value in a t-distribution for a value of j bj max 10 À10 ;r ð Þ , with degrees of freedom n -1 (the total sample size). To estimate the maximal effect for each variable, we first determine the Z-value:</p><p>Z ¼ qnorm ½maxð0:5; 1 À a=ð2KÞÞr (28)</p><p>which is then used in combination with â to calculate the maximal effect:</p><p>h ¼ ð bÞ Á maxð0; j bj À ZÞ (29)</p><p>The maximal effects for insignificant variables is set to be 0 due to the max term that exists in h .</p><p>(Appendices continue)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Numerical Evaluation of Causal Inference Algorithms With Hidden Variables</head><p>We also ran the simulation study using data that contained hidden variables. Figure <ref type="figure" target="#fig_0">D1</ref> shows the MCC for the five algorithms that we investigated. The results are very similar to the results using data without hidden variables. For the PC-algorithm, we again see that the graph density d influences the MCC, where it reaches the highest numbers when d = .25. This effect only appears when the graph size p = 5. With p = 10, the MCC is generally low (average MCC = .09) at d = .5 and will only increase to mediocre (average MCC = .56) values when d = .1.</p><p>The picture we painted earlier for the DR-FFL and the TRANSWESD-algorithms does not improve when hidden variables are included. When the graph size p = 5, the average MCC lies around .15, whereas when p = 10, the average MCC is around .03. The sample size n does not seem to influence the performance of both algorithms. On the other hand, the threshold parameter b has a big impact. The lower b, the higher the MCC is. To illustrate, when b = .5, the average MCC is .50, whereas when b = 2.58, the average MCC is close to zero. The threshold parameter b influences how many edges are retained after the first step in both the DR-FFL and the TRANSWESD-algorithm. The higher the threshold, the lower the number of edges that are present in the perturbation graph and the lower the MCC.</p><p>The ICP-algorithm has the best performance when there is a medium number of edges in the graph (d = .25). As we saw before, we observe high MCC values with the smaller graphs (p = 5). When p is increased to 10, the ICP-algorithm becomes more conservative, resulting in a lower MCC. Only when the mean of the perturbation distribution (m ) is high and the standard deviation small can the ICP-algorithm accurately estimate causal graphs. This indicates that the ICP-algorithm needs a strong and effective perturbation to correctly identify causal relations.</p><p>The mixed performance that we saw earlier with respect to the HICP-algorithm is also present when we add hidden variables to the data. This means that the HICP-algorithm can accurately estimate causal graphs with a small sample size. When the sample size increases, the accuracy decreases. This effect is present in almost every simulation condition. The only exception is when the graph density is low (d = .1). In that case, the MCC increases when the sample size increases.</p><p>Figure <ref type="figure" target="#fig_1">D2</ref> paints a similar picture that we saw in the previous section. The lack of accuracy of the DR-FFL and the TRANSWESD-algorithm is clearly visible, as are the spurious edges that are estimated by the HICP with a large sample size. Even though hidden variables are added to these data, the ICPalgorithm shows the highest number of true positives, combined with the lowest number of false positives for this simulation condition. Lastly, the PC-algorithm can have issues with determining the direction of an edge. This problem emerges independent of the presence of hidden variables, as we have seen </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="3,307.79,112.65,236.35,66.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Visualization of the Causal Graph That We Use to Illustrate the Different Algorithms</figDesc><graphic coords="4,336.02,515.00,180.00,180.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, four strongly connected components exist (see the middle panel of Figure 4): nodes 4 and 5 form a strongly connected component (component A), and nodes 1, 2, and 3 each from their own individual component (components B, C, and D, respectively). There are only five edges that connect these strongly connected components, shown by the middle panel in Figure 4. For each of these five edges, the DR-FFL-algorithm determines whether an alternative path exists to connect these two components. There are no alternative paths between components A and B, components A and C, and components A and D. There is an alternative path between components D and B (D ! A ! B) and components D and C (D ! A ! B ! C). Thus, the edges that directly connect components D and B (e 31 ) and components D and C (e 32 ) are removed from the causal graph, resulting in the graph shown in Figure 4 (right panel).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Visualization of the Down-Ranking of Feed-Forward Loops (DR-FFL) Process</figDesc><graphic coords="7,50.40,522.82,495.55,159.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>Figure 5Visualization of the Perturbation Graph Generation</figDesc><graphic coords="8,335.40,101.65,181.19,181.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>Figure 6The True Causal Graph (Most Upper Panel) and All Possible Subsets That May Potentially Cause the Target Node 5</figDesc><graphic coords="10,51.82,101.65,491.98,602.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8</head><label>8</label><figDesc>Figure 8 Illustration of the Hidden Invariant Causal Prediction (HICP)-Algorithm</figDesc><graphic coords="11,49.78,575.66,236.35,129.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7</head><label>7</label><figDesc>Figure 7 Visualization of the Residual Distribution for the Empty Subset (Left Panel) and the True Subset (Right Panel)</figDesc><graphic coords="11,50.40,101.65,495.55,115.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9</head><label>9</label><figDesc>Figure 9Illustration of the Hidden Invariant Causal Prediction (HICP)-Algorithm</figDesc><graphic coords="12,78.52,534.56,181.19,181.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10</head><label>10</label><figDesc>Figure 10Visualization of the Directed Acyclic Graph (DAG) Simulation</figDesc><graphic coords="13,117.01,101.65,360.00,165.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 Directed</head><label>11</label><figDesc>Figure 11Directed Acyclic Graphs (DAGs) That Were Used to Simulate Data</figDesc><graphic coords="14,87.25,101.65,419.98,257.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12</head><label>12</label><figDesc>Figure 12 Matthew's Correlation Coefficient (MCC) for p = 10 Nodes With a Network Density of d = 0.25 When No Hidden Variables Were Simulated</figDesc><graphic coords="14,117.01,499.18,360.00,175.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13</head><label>13</label><figDesc>Figure 13 Visualization of the Number of True Positives and False Positives for p = 5, d = 0.25, n = 5,000, m ¼ 5SD = 0.5, and b = 0.5 Without the Addition of Hidden Variables</figDesc><graphic coords="16,117.01,112.65,360.00,260.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14</head><label>14</label><figDesc>Figure 14Results for Every Algorithm</figDesc><graphic coords="17,50.40,101.65,495.55,337.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure B1 Two</head><label>B1</label><figDesc>Figure B1 Two Examples of Perturbation Graphs One for Which Transitive Reduction is Appropriate (Left Panel) and One for Which it is Not (Right Panel)</figDesc><graphic coords="21,307.79,167.30,236.35,109.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure D1 Matthew's Correlation Coefficient (MCC) for p = 10 Nodes With a Network Density of d = 0.25 With the Addition of Hidden Variables</figDesc><graphic coords="24,117.01,449.52,360.00,176.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="25,87.25,112.65,419.98,303.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Overview of the Algorithms</figDesc><table><row><cell>Observational Experimental Within-Between-Correction for Cyclic</cell><cell>Algorithm data data N = 1 N . 1 subjects subjects multiple testing graphs Limitations Sensitivity Specificity</cell><cell>PC H --H H H --Uses only observational data Partially high High</cell><cell>DR-FFL H H H H H ---Resulting graph is unweighted and unsigned Low High</cell><cell>TRANSWESD H H -H -H --Uses arbitrary threshold Low High</cell><cell>ICP H H H H H H H Bonferroni Computationally slow with many variables Partially high High</cell><cell>HICP H H H H H H H Bonferroni Contains spurious relations High Partially high</cell><cell>Note. PC = Peter and Clark; DR-FFL = Down-Ranking of Feed-Forward Loops; TRANSWESD = Transitive Reduction for Weighted Signed Digraphs; ICP = Invariant Causal Prediction; HICP =</cell><cell>H = algorithm can handle that specific property; -= algorithm cannot handle that specific property. Partially high means that the sen-Hidden Invariant Causal Prediction; N = number of participants;</cell><cell>sitivity/specificity is high depending on certain conditions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Items of the Attitude Toward Meat Consumption Questionnaire With Their Assigned Item Label, Means (SD) Across Measurements, and M (SD) Changes Between the Baseline Measurement and the Perturbation Measurement</figDesc><table><row><cell>Item</cell><cell>Item label</cell><cell>M (SD)</cell><cell>Mean perturbation effect (SD)</cell></row><row><cell>Eating is morally wrong</cell><cell>c_moral</cell><cell>3.73 (1.95)</cell><cell>À1.27 (3.73)</cell></row><row><cell>Meat contains important nutrients for your body</cell><cell>c_nutrients</cell><cell>3.66 (1.73)</cell><cell>0.43 (1.87)</cell></row><row><cell>The production of meat if harmful for the environment</cell><cell>c_environment</cell><cell>3.66 (1.83)</cell><cell>2.50 (1.76)</cell></row><row><cell>Animals are inferior to people</cell><cell>c_inferior</cell><cell>3.05 (1.89)</cell><cell>À0.83 (1.32)</cell></row><row><cell>By consuming meat you contribute to animal suffering</cell><cell>c_suffering</cell><cell>3.68 (1.79)</cell><cell>1.63 (1.88)</cell></row><row><cell>There should be a tax on meat</cell><cell>c_tax</cell><cell>3.65 (1.87)</cell><cell>1.70 (2.39)</cell></row><row><cell>I like the taste of meat</cell><cell>a_taste</cell><cell>3.65 (2.02)</cell><cell>3.90 (1.56)</cell></row><row><cell>Meat reminds me of death and suffering of animals</cell><cell>a_death</cell><cell>3.29 (1.88)</cell><cell>À1.50 (1.66)</cell></row><row><cell>If I had to stop eating meat I would feel sad</cell><cell>a_sad</cell><cell>3.86 (1.90)</cell><cell>À1.83 (2.79)</cell></row><row><cell>If I eat meat I feel guilty</cell><cell>a_guilty</cell><cell>3.38 (1.95)</cell><cell>À2.50 (2.32)</cell></row><row><cell>If I eat meat I feel disgust</cell><cell>a_disgust</cell><cell>3.12 (1.98)</cell><cell>À0.03 (1.3)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use the individual user and is not to be disseminated broadly.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Iterative conditional fitting for Gaussian Ancestral Graph models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th conference on uncertainty in artificial intelligence</title>
		<meeting>the 20th conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computation of maximum likelihood estimates in cyclic structural equation models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1214/17-AOS1602</idno>
		<ptr target="https://doi.org/10.1214/17-AOS1602" />
	</analytic>
	<monogr>
		<title level="m">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="663" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal modeling with the TETRAD program</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Testing for causality</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
		<idno type="DOI">10.1016/0165-1889(80)90069-X</idno>
		<ptr target="https://doi.org/10.1016/0165-1889" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">90069</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A critique of the cross-lagged panel model</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P P P</forename><surname>Grasman</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0038889</idno>
		<ptr target="https://doi.org/10.1037/a0038889" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="116" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant causal prediction for nonlinear models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2017-0016</idno>
		<ptr target="https://doi.org/10.1515/jci-2017-0016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="Arti" to=" cle" />
			<date type="published" when="2018">2018. 20170016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Psychological perturbation data on attitudes towards the consumption of meat</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H A</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kossakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Van Der Maas</surname></persName>
		</author>
		<idno type="DOI">10.5334/jopd.37</idno>
		<ptr target="https://doi.org/10.5334/jopd.37" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Psychology Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistics and causal inference</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1986.10478354</idno>
		<ptr target="https://doi.org/10.1080/01621459.1986.10478354" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">396</biblScope>
			<biblScope unit="page" from="945" to="960" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PC-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal inference using graphical models with the R package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v047.i11</idno>
		<ptr target="https://doi.org/10.18637/jss.v047.i11" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TRANSWESD: Inferring cellular networks with transitive reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klamt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Flassig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sundmacher</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btq342</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btq342" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2160" to="2168" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introducing the causal graph approach to psychopathology: An illustration in patients with obsessive-compulsive disorder</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kossakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Van Oudheusden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Riemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/ed2v5</idno>
		<ptr target="https://doi.org/10.31234/osf.io/ed2v5" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Manuscript in preparation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of T4 phage lysozyme</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-2795(75)90109-9</idno>
		<ptr target="https://doi.org/10.1016/0005-2795" />
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta -Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90109" to="90109" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">InvariantCausalPrediction: Invariant causal prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/package=InvariantCausalPrediction" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Computer software manual</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Methods for causal inference from gene perturbation experiments and validation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1510493113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1510493113" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7361" to="7368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On causal discovery with cyclic additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="539" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinguishing cause from effect using observational data: Methods and benchmarks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="102" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Causality: Models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, reasoning, and inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The book of why: The new science of cause and effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge representation and reasoning: Proceedings of the second international conference</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fikes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">E</forename><surname>Sandewall</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Causal inference using invariant prediction: Identification and confidence intervals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12167</idno>
		<ptr target="https://doi.org/10.1111/rssb.12167" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Royal Statistical Society</publisher>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="947" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Elements of causal inference: Foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reconstruction of large-scale regulatory networks based on perturbation graphs and transitive reduction: Improved methods and their evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pinna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Flassig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De La Fuente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klamt</surname></persName>
		</author>
		<idno type="DOI">10.1186/1752-0509-7-73</idno>
		<ptr target="https://doi.org/10.1186/1752-0509-7-73" />
	</analytic>
	<monogr>
		<title level="j">BMC Systems Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing biological networks using conditional correlation analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stolovitzky</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bti064</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/bti064" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="765" to="773" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backshift: Learning causal cyclic graphs from unknown shift interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1513" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimation of linear non-Gaussian acyclic models for latent factors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2008.11.018</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2008.11.018" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<idno type="DOI">10.1002/sim.1415</idno>
		<ptr target="https://doi.org/10.1002/sim.1415" />
	</analytic>
	<monogr>
		<title level="m">Computation, causation, and discovery</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Press</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A note on directional dependence in regression setting</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sungur</surname></persName>
		</author>
		<idno type="DOI">10.1080/03610920500201228</idno>
		<ptr target="https://doi.org/10.1080/03610920500201228" />
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="1957" to="1965" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified framework of longitudinal models to examine reciprocal relations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Usami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000210</idno>
		<ptr target="https://doi.org/10.1037/met0000210" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="637" to="657" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direction dependence analysis in the presence of confounders: Applications to linear mediation models using observational data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiedermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sebastian</surname></persName>
		</author>
		<idno type="DOI">10.1080/00273171.2018.1528542</idno>
		<ptr target="https://doi.org/10.1080/00273171.2018.1528542" />
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="515" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Correlation and causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Agricultural Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal discovery with linear non-gaussian models under measurement error: Structural identifiability results</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in artificial intelligence: Proceedings of the thirty-fourth conference</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</editor>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From data to causes I: Building a General Cross-Lagged Panel Model (GCLM)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zyphur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Voelkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Preacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamsollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Pierides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Diener</surname></persName>
		</author>
		<idno type="DOI">10.1177/1094428119847278</idno>
		<ptr target="https://doi.org/10.1177/1094428119847278" />
	</analytic>
	<monogr>
		<title level="j">Organizational Research Methods</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="687" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
