<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Causal Graphs in Manufacturing Domains using Structural Equation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-10-26">26 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Kertel</surname></persName>
							<email>maximilian.kertel@bmw.de</email>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
							<email>stefan.harmeling@tu-dortmund.de</email>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Pauly</surname></persName>
							<email>pauly@statistik.tu-dortmund.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Technology Development Battery Cell BMW Group Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University Dortmund</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">TU Dortmund University Dortmund</orgName>
								<address>
									<country>Germany Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center Trustworthy Data Science and Security UA</orgName>
								<address>
									<settlement>Ruhr</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Causal Graphs in Manufacturing Domains using Structural Equation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-26">26 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.14573v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal Discovery</term>
					<term>Bayesian Networks</term>
					<term>Industry 4.0</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many production processes are characterized by numerous and complex cause-and-effect relationships. Since they are only partially known they pose a challenge to effective process control. In this work we present how Structural Equation Models can be used for deriving cause-and-effect relationships from the combination of prior knowledge and process data in the manufacturing domain. Compared to existing applications, we do not assume linear relationships leading to more informative results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>To be published in the Proceedings of IEEE AI4I 2022. Complex manufacturing processes as, e.g. for battery cells show high scrap rates and thus high production costs and large environmental footprints. One of the driving factors is the missing knowledge on the interdependencies between the process parameters, intermediate product properties and the quality characteristics <ref type="bibr" target="#b0">[1]</ref>. Together we call this the causeand-effect relationships (CERs). CERs can be visualized as a network with the process and product characteristics as nodes and the CERs as directed edges <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. It is the goal of our paper to unify expert knowledge and process data to derive such a network, which allows the visual identification of • root-causes of erroneous products,</p><p>• relevant parameters for process control during successive production steps and • important characteristics to predict the quality of the final product.</p><p>In complex manufacturing domains, CERs form a linked mesh of hundreds of involved factors <ref type="bibr" target="#b0">[1]</ref>. Typically, CERs are derived by running Designs of Experiments (DOEs). However, DOEs can be time-demanding and the production line has to be stopped in the meantime leading to prohibitively high costs.</p><p>Moreover, if there are many potential CERs, the number of experiments can become infeasible. At the same time, the Internet of Things (IoT) allows data processing and storage along the whole production line, leading to a vast amount of accessible information. It is thus desirable to derive the CERs from the existing observational (or nonexperimental) data. For this purpose, Bayesian Networks can be used to unify expert knowledge and data. From these, CERs can be derived under the assumption of causal sufficiency <ref type="bibr" target="#b2">[3]</ref>. This approach is called causal discovery or structure learning.</p><p>The most common example in the manufacturing domain <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, is the PC algorithm <ref type="bibr" target="#b2">[3]</ref>. This algorithm relies on the assumption of faithfulness and on efficient statistical tests for conditional independence. In principle the PC algorithm can be applied with any test for conditional independence. However, existing nonparametric tests do not scale well <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Most of the applications of the PC algorithm either discretize the measurements, or researchers approximate the joint distribution of the variables by a multivariate normal distribution. For discrete data and normally distributed data fast tests for conditional independence exist. However, the former leads to a loss of information, while the latter requires a linear dependency between the variables to be exact. In case of manufacturing data this is most likely a misspecification <ref type="bibr" target="#b8">[9]</ref>. Simulation studies show, that the performance of the PC algorithm can be poor in case of non-linearity <ref type="bibr" target="#b9">[10]</ref>. This questions the application of the PC algorithm for large or highdimensional manufacturing data.</p><p>In recent years, Structural Equation Models (SEM), which can incorporate arbitrary functional relationships, were increasingly proposed to derive Causal Bayesian Networks. They replace the assumption on faithfulness by a functional form of the conditional distributions (see Equation ( <ref type="formula" target="#formula_3">1</ref>)). While the PC algorithm returns a set of graphs, methods based on SEMs often derive a single graph. To the best of our knowledge, we are the first to apply SEMs to derive such graphical models in the manufacturing domain.</p><p>The paper is structured as follows.</p><p>In Section II we present potential prior knowledge and available data in manufacturing domains. We continue in Section III by reviewing Bayesian Networks and SEMs and explain Causal Additive Models (CAM). In Section IV we present an extension of CAM, called TCAM, which efficiently incorporates prior knowledge. We apply our method in Section V to process data of the assembly of battery modules at BMW. We conclude in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA AND CHALLENGES IN COMPLEX MANUFACTURING DOMAINS</head><p>In this section we describe the data sources and propose a preprocessing of the data. Then, we explain the broad prior knowledge in manufacturing domains. Finally, we mention common challenges with production data.</p><p>A. Data Sources along the Production Line</p><p>The assembly of products consists of production lines, which again contain several stations, which are passed in a fixed order and where process steps are carried out. During those process steps the piece is transformed or it is combined with other parts in order to achieve a predefined outcome. All involved parts are assigned to unique identifiers. Data of different types is collected along the production process:</p><p>• Process data: the stations take measurements of the involved parts (e.g. thickness of the piece) and the parameters of the machine (e.g. weight of applied glue). • End-of-Line (EoL) tests take additional quality measurements of the intermediate or final products. • Station information: at some production steps the pieces are spread out to identical stations, such that parts can be processed in parallel and every piece is assigned to one of the stations. • Bill of Material (BoM): the BoM contains the information which pieces were merged together and on which position they have been worked in. • Supplier data: suppliers transmit data on provided goods. The preprocessing of the data, which is depicted in Figure <ref type="figure">1</ref>, consists of the following steps:</p><p>1) Collect the data for every intermediate product.</p><p>2) Iteratively merge the data of all subcomponents of a final product. Measurements of identical subcomponents, which are placed in the same position, can be found in the same column. Eventually, the final tabular data set contains all measurements that can be associated with a final product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prior Knowledge</head><p>As the stations are passed in a fixed order, we know that CERs across different stations can only act forward in time. Additonally, in many manufacturing organizations, tools as the Failure Mode and Effect Analysis (FMEA) <ref type="bibr" target="#b10">[11]</ref> are implemented to extract expert knowledge on CERs in the production process and to provide the information in a structured form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Challenges of Data Analysis in Manufacturing</head><p>Often, similar information is recorded multiple times along the production line, leading to multicollinearity <ref type="bibr" target="#b3">[4]</ref>. Also, sensors might deliver non-informative data by recording implausible values. Industrial data is also reported to be drifting over time. However, even in shorter time intervals, data of a series production contains thousands of observations. This distinguishes the manufacturing domain from other applications of causal discovery as medicine, genetics or the social sciences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STRUCTURE LEARNING OF GRAPHICAL MODELS A. Some Preliminaries on Graphical Models</head><p>Let G = (V, E) be a directed acyclic graph (DAG) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Chapter 6]</ref> </p><formula xml:id="formula_0">with nodes V = (V 1 , . . . , V p ) and edges E. The node V i is called a parent of V j if the edge V i → V j is in E.</formula><p>We denote the set of all parents of V j as pa(V j ). A tuple of nodes (V j1 , . . . , V j ), such that V j k is a parent of V j k+1 for all k = 1, . . . , ( -1), is called a directed path. Nodes that can be reached from X j through a directed path are called the descendants of X j . In the following we denote random vectors with bold letters as Z and random variables as Z. Let X = (X 1 , . . . , X p ) be a random vector representing the data generating process. For a graph G with nodes X 1 , . . . , X p , we call (X, G) a Bayesian network if the local Markov property holds, i.e.</p><formula xml:id="formula_1">X i ⊥ X j |pa(X i )</formula><p>for any X j that is not a descendant of X i in G. Here, X ⊥ Y |Z denotes the conditional independence of X and Y given Z. In that case, we can deduce additional conditional independencies for X from the graph G using the concept of d-separation <ref type="bibr" target="#b11">[12]</ref>. For a Bayesian Network (X, G), it then holds that X i ⊥ X j |S if X i and X j are d-separated by S in G. On the other hand, if there is a graph G, such that X i ⊥ X j |S implies that X i and X j are d-separated given S in G, then X is called faithful with respect to G. As multiple graphs can contain the same d-separations, this graph G is in general not unique. To promote the intuition, assume that X has a joint density f . Then X i ⊥ X j |S can be characterized by</p><formula xml:id="formula_2">f (x i |X j = x j , S = s) = f (x i |S = s) ,</formula><p>where f (x i |Z = z) denotes the conditional density function of X i given Z = z. Thus, if we already know S, then X j does not provide additional information on X i . Assume that we are interested which variable in {X j , X S } causes the variable X i to be out of the specification limits. Then we know, that the root causes can be found within S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Learning with Structural Equation Models</head><p>While the PC algorithm is the classic approach for deriving a Causal Bayesian Network, recent research focused on identifying it using acyclic SEMs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. They assume that there exists a permutation Π 0 (1, . . . , p) = π 0 (1), . . . , π 0 (p) and functions {f , = 1, . . . , p}, such that</p><formula xml:id="formula_3">X = f (X 1 , . . . , X v , ε ), = 1, . . . , p,<label>(1)</label></formula><p>where π 0 ( k ) &lt; π 0 ( ) for all k = 1, . . . , v and ε 1 , . . . , ε p are i.i.d. noise terms. As the estimation of f in Equation ( <ref type="formula" target="#formula_3">1</ref>) is difficult in high dimensions, one typically restricts the function class and the distribution of the noise terms. In this work, we assume that the functions follow the additive form</p><formula xml:id="formula_4">f (X 1 , . . . , X v , ε ) = c + k:π 0 (k)&lt;π 0 ( ) f k, (X k ) + ε ,<label>(2)</label></formula><p>where ε ∼ N (0, σ ) and c ∈ R. To ensure the uniqueness of the f k, and without loss of generality, we set E (X ) = 0 Fig. <ref type="figure">1</ref>. Visualization of the data preparation described in Section II. The same measurements are collected for piece 1 to piece L. Then they are placed in their mother piece with identifier M . Finally, the resulting data set consists of all measurements of M and those from piece 1 to piece L, where the positioning of the measurements of the child pieces within the data frame depends on their placement according to the BoM. This step is carried out repeatedly, if M itself is positioned in another mother piece.</p><p>and E (f k, (X k )) = 0, for all = 1, . . . , p, π 0 (k) &lt; π 0 ( ).</p><p>From Equations ( <ref type="formula" target="#formula_3">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>) we derive that</p><formula xml:id="formula_5">X ⊥ X k | X v1 , . . . , X vj ,</formula><p>with π 0 (k) &lt; π 0 ( ), π 0 (v 1 ) &lt; π 0 ( ), . . . , π 0 (v j ) &lt; π 0 ( ) if and only if f k, = 0. Let G 0 be the graph on X 1 , . . . , X p , that contains the edge X i → X j if and only if f i,j = 0 for π 0 (i) &lt; π 0 (j). Then (X, G 0 ) is a Bayesian network, as it is fulfilling the Markov property.</p><p>If we assume that the functions f k, in Equation ( <ref type="formula" target="#formula_4">2</ref>) are nonlinear and smooth, then <ref type="bibr" target="#b14">[15]</ref> show that G 0 is identifiable from observational data. This is in contrast to the PC algorithm, which typically returns a class of graphs. Note that we do not presume that the distribution is faithful to some DAG, which is a central assumption of the PC algorithm. We emphasize that for the PC algorithm non-linearity is an obstacle as efficient conditional independence testing is just feasible for multivariate normal data. In contrast, we can utilize the nonlinearity for identifying SEMs to receive more informative results (under the assumption of Equation (2)). An example of a learning algorithm for SEMs is the Causal Additive Model (CAM, <ref type="bibr" target="#b9">[10]</ref>). We will focus on CAM due to its applicability to high-dimensional data, its ability to capture non-linearity and due to the theoretical justification that G 0 can be identified, if the functions on the right-hand side of Equation 2 are nonlinear and smooth. <ref type="bibr" target="#b9">[10]</ref> propose to find G 0 with the following steps: 1) Find the underlying node ordering Π 0 of X 1 , . . . , X p .</p><p>2) Identify the influential functions f k, with feature selection methods. To make things more precise, consider N observations (x i1 , . . . , x ip ) , i = 1, . . . , N from X and call the data matrix D ∈ R N ×p .</p><p>1) Finding the node ordering: <ref type="bibr" target="#b9">[10]</ref> show that if</p><p>• the functions f k, are smooth and non-linear and can be approximated well and</p><p>• the derivatives of f k, and the fourth moments of f k, (X k ) and X k are bounded. then the following estimator for Π 0 is consistent as N → ∞:</p><formula xml:id="formula_6">Π = argmin Π p =1 ||x - π(k)&lt;π( ) f k, (x k )|| 2 2,N<label>(3)</label></formula><p>Here, we define</p><formula xml:id="formula_7">||x k || 2 2,N := 1 N N k=1</formula><p>x 2 k and f k, is found by running an additive model regression <ref type="bibr" target="#b15">[16]</ref> of X on {X k : π(k) &lt; π( )}. For large p, <ref type="bibr" target="#b9">[10]</ref> propose a greedy method to find Π. Let G be a DAG on X with edges E(G). For simplicity we denote the edge X k → X by (k, ). A score for G is defined by</p><formula xml:id="formula_8">S(G) = p =1 ||x - (k,l)∈E(G) f k, (x k )|| 2 2,N .</formula><p>The functions f k, are estimated by running an additive model regression of X on its parents in G. Intuitively, S(G) indicates how much variation of D is captured by G. The edges that can be added to G without causing cycles are denoted by</p><formula xml:id="formula_9">A(G) := {(i, j) ∈ {1, . . . , p} × {1, . . . , p} : (X, E(G) ∪ {(i, j)}) is DAG}.</formula><p>Starting with the empty graph G 0 , <ref type="bibr" target="#b9">[10]</ref> iteratively add the edge</p><formula xml:id="formula_10">(k 0 , 0 ) = argmax (k , )∈A(Gt) M t (k , ),<label>where</label></formula><formula xml:id="formula_11">M t (k , ) = ||x - (k, )∈E(Gt) f k, (x k )|| 2 2,N -||x - (k,l)∈E(Gt)∪{(k , )} f k, (x k )|| 2 2,N .<label>(4)</label></formula><p>The functions f k, are found by regressing X on its parents in G t , while f k, are found by regressing X on its parents in G = (X, E(G t ) ∪ {(k , )}). Thus, the edge (k 0 , 0 ) maximally reduces the unexplained variance. We set G t+1 = (X, E(G t ) ∪ {(k 0 , 0 )}) and continue until we obtain a complete DAG, which implies the node ordering. This greedy method is still computationally intense for large p. Thus, <ref type="bibr" target="#b9">[10]</ref> propose to take advantage of sparse structures, where p is large but the number of edges in the graph is assumed to be small: to this end they start by a preliminary neighborhood selection (PNS) step. Here, initially for every ∈ {1, . . . , p} a superset of neighbors of X in G 0 is identified. In the subsequent node ordering step, one only considers the superset of the neighbors, when greedily adding new edges. This reduces the computation time of the algorithm significantly, if the sizes of the supersets are considerably smaller than p.</p><p>2) Identifying edges: After the node ordering is set, we need to identify the influential characteristics for every X among those X k for which π(k) &lt; π( ). The idea is to detect those f k, which are not 0, using feature selection methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. For those k, a change in X k has an effect on X . For a comparison of CAM and the PC algorithm based on simulated data sets with known ground truth, see <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>The goal of this section is to derive a method that combines the current results on structure learning of SEMs with the features of the manufacturing domain in Section II.</p><p>The energy storage of electric vehicles is called a battery pack which is composed of battery modules, which in turn contain a fixed number of battery cells. A battery module connects the battery cells in series or parallel and it protects those cells against shock, vibration and heat. Thus, the battery module is a key component for the safety of battery-electric vehicles. We apply TCAM to data collected at the assembly at BMW. The data set under investigation contains 7254 battery modules with 738 variables each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preparation</head><p>As the missing values rate is low (around 2.4%) we apply naive mean imputation instead of more sophisticated method as <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. We then continue by removing features that have only one distinct value and hence provide no information. As this data set also shows multicollinearity, we apply an expert-based approach. We asked experts to identify clusters of variables containing similar information and to define representatives for them. For a purely data-driven approach in manufacturing, see <ref type="bibr" target="#b4">[5]</ref>. Those steps reduced the number of characteristics from 738 to 491. Finally, we standardize the data so the variables' empirical mean and standard deviation is 0 and 1 respectively. Beyond the temporal ordering of the stations, it is reasonable that the production measurements of identical intermediate products as depicted in Figure <ref type="figure">1</ref> are independent. Thus, it is possible to restrict the potential edges that have to be considered. Additionally, we assume that some of the recorded measurements as the facility temperature and the selection of the stations are not affected by other measurements. We can mark those values as root nodes, meaning that they have no incoming edges. This further restricts the number and orientation of possible edges.</p><p>B. Choice of Software and Hyperparameters 1) Preliminary Neighborhood Selection: For our application of TCAM, we find supersets of the neighbors by applying the LASSO. For ∈ {1, . . . , p}, we run a regression of X on those components of X, which are possible parents according to our prior knowledge. Going forward we mark those variables as potential parents of X , where the corresponding regression coefficient is above 10 -2 . The penalty parameter λ is chosen via cross-validation. Let λ min be the penalty parameter that minimizes the mean squared cross-validation error. Then we choose the maximal λ such that the mean cross-validation error is within one standard deviation of the minimum λ min .</p><p>2) Node Ordering and Pruning: For the node ordering we employ the package mgcv by <ref type="bibr" target="#b15">[16]</ref>. Let us call the graph after node ordering G N O . In the pruning step we run a sparse additive regressions of X on its parents in G N O for = 1, . . . , p. This step returns p-values for the parents of X in G N O . We follow <ref type="bibr" target="#b9">[10]</ref> and set the regressands as parents of X in the final graph, whose p-values are below the threshold of 10 -3 . Fig. <ref type="figure">2</ref>. Resulting graph of TCAM where the nodes correspond to characteristics of the product and edges correspond to detected CERs. The node coloring is according to the station, where the variable was measured. Edge colors are according to respective source node's color. The blue box highlights the detected relationship between the choice of the stations (green nodes) and the product quality (blue nodes). The red box depicts the similarities between structures of identical subcomponents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>The resulting graph is depicted in Figure <ref type="figure">2</ref> and contains 491 nodes and 859 edges. We observe that there are a few nodes that have a large number of neighbors. In general this poses a difficulty for most structure learning algorithms and CAM did not finish in reasonable time. For details on the runtime for a low-dimensional special case, see Section V-D. With TCAM and the inclusion of prior knowledge we were able to overcome those obstacles. Further, substructures of identical parts show similar patterns. The red box in Figure <ref type="figure">2</ref>, highlights patterns consisting of two linked clusters, where one cluster consists of four nodes, while the other one consists of three nodes. Together with process experts we could further verify that many CERs detected by TCAM are plausible. This application is confidential, but we would still like to share one of the insights. TCAM discovered a CER between one station that processed the part and the part's quality. Experts derived that the maintenance of that station was overdue and the CER can be used to find better maintenance intervals. This is one example how graphical models can contribute to an effective and proactive process control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation against Expert Knowledge</head><p>For the characteristics of one of the subcomponents, we derived an expert-based graph, which is depicted in Figure <ref type="figure">3</ref>. Here, the blue CERs potentially exist, while green CERs surely Fig. <ref type="figure">3</ref>. Expert-based graph on measurements for subcomponents. The green edges are known to exist, while the blue edges potentially exist. Edges beyond the ones depicted are known to be absent. The darker the node, the later the corresponding variable is measured in the production process. exist. Other CERs can be ruled out. We compare the estimated graphs and runtimes of TCAM, CAM and a variant of the PC algorithm called TPC <ref type="bibr" target="#b20">[21]</ref>, which allows the inclusion of temporal background knowledge. The significance level is set to 0.01. We run 500 experiments, where we randomly draw 500 subcomponents, while each of them appears in at most one of the runs. We define an adapted Structural Hamming Distance (aSHD) <ref type="bibr" target="#b21">[22]</ref> between an estimated graph G est and the one in Figure <ref type="figure">3</ref> by the sum over the number of green edges that are not in G est and the number of edges G est that do not appear in Figure <ref type="figure">3</ref>. The results are depicted in Table <ref type="table">I</ref>. TPC and TCAM perform better than CAM, which shows the advantage of the inclusion of prior knowledge. Additionally, even in this low-dimensional setting the average runtime for TCAM is smaller than for CAM. Further, we observe that the aSHD of TCAM and TPC is on average quite similar. However, the standard deviation of the aSHD and the standard deviation of the number of edges is smaller for TCAM. This indicates that TCAM delivers more stable and informative results in the manufacturing domain. The original PC algorithm performed worse than TPC and is omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented a method to derive the graphical representation of CERs of manufacturing processes based on SEMs. While existing approaches for causal discovery in the manufacturing domain assumed linear relationships between the process characteristics, we applied CAM to find arbitrary additive functional relationships in data. We showed how existing prior domain knowledge can be included and improves the computational burden of CAM. A case study on manufacturing data reveals that the learned graph detects unknown root-causes, delivers more informative results and paves the way to an efficient and proactive process control.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>THE</head><label></label><figDesc>AVERAGE ASHD (ASHD), THE STANDARD DEVIATION OF THE ASHD (SD(ASHD)), THE AVERAGE NUMBER OF EDGES (#EDGES) AND THE STANDARD DEVIATION OF THE NUMBER OF EDGES (SD(#EDGES)) FOR ALLTHREE METHODS OF SECTION V-D AND FOR 500 REPLICATIONS.</figDesc><table><row><cell></cell><cell cols="3">aSHD sd(aSHD) #edges</cell><cell cols="2">sd(#edges) time (s)</cell></row><row><cell>CAM</cell><cell>3.496</cell><cell>1.442</cell><cell>9.464</cell><cell>0.948</cell><cell>1.342</cell></row><row><cell>TCAM</cell><cell>1.120</cell><cell>0.343</cell><cell>8.084</cell><cell>0.778</cell><cell>1.000</cell></row><row><cell>TPC</cell><cell>1.108</cell><cell>0.866</cell><cell>7.463</cell><cell>1.623</cell><cell>0.013</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE I</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Recap of Common Prior Knowledge</head><p>Compared to other applications of causal discovery, it is typical for the manufacturing domain, that there exists prior knowledge, see Section II. In particular, there is a partial and transitive ordering of the variables implied by the stations' ordering. Additionally, we include expertise on the absence of edges. Both facets shall improve the algorithm's runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptions to CAM</head><p>The data generating process behind manufacturing data sets often leads to a low number of conditional independencies in X, when compared to p. Thus, the Causal Bayesian Network of X is not sparse. This poses a challenge to many structural learning algorithms in higher dimensions. We show in this subsection how prior knowledge on the node ordering and the existence of edges can be incorporated so that structure learning remains feasible. To formalize our prior knowledge, let t : {1, . . . , p} → {1, . . . , T }, so that t(k) &lt; t( ) means that there can only be edges from X k to X but not vice versa. Further, let F be a boolean matrix, where F k, = True if the edge from X k to X is known to be absent.</p><p>1) Preliminary Neighborhood Selection: For every measurement X , we determine a set of possible parents among those k, where F k, = False and t(k) ≤ t( ). Denote that set for index by P .</p><p>2) Node Ordering: We start by adding all potential edges that go across stations and add them to the initial graph G 0 , as those can not cause any cycle. The score of G 0 hence is</p><p>We continue by determining the node ordering as in Section III-B. Note that we only need to determine the node ordering for indices k, so that t(k) = t( ). The initial inclusion of across-station-edges saves update steps of M (Equation <ref type="formula">4</ref>). This makes the algorithm feasible even for non-sparse highdimensional settings, if the number of tiers T or the number of edges known to be absent is sufficiently large.</p><p>3) Pruning: The pruning step is identical to CAM. In the manufacturing industry, the prior knowledge on t(k) &lt; t(l) is often given by the temporal nature of the production process. We therefore call our adaption TCAM (Temporal Causal Additive Models). It is sketched in Algorithm 1. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataand expert-driven analysis of cause-effect relationships in the production of lithium-ion batteries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kornas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Karamat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An approach to monitoring quality in manufacturing using supervised machine learning on product state data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wuest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Irgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-D</forename><surname>Thoben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1167" to="1180" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal discovery in manufacturing: A structured literature review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vuković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Manufacturing and Materials Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Causal discovery for manufacturing domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marazopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1605.04056" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge discovery from observational data for process control using causal bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IIE transactions</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="690" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, ser. UAI&apos;11</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, ser. UAI&apos;11<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Manufacturing analytics and industrial internet of things</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernest</surname></persName>
		</author>
		<idno type="DOI">10.1214/14-AOS1260</idno>
		<ptr target="https://doi.org/10.1214/14-AOS1260" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Failure mode and effect analysis: FMEA from theory to execution</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Stamatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Quality Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Directlingam: A direct method for learning a linear non-gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009-2053, 01 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generalized additive models: an introduction with R</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Chapman and Hall/CRC</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variable selection in nonparametric additive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2282</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Flexible Imputation of Missing Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Buuren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Chapman and Hall/CRC</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the relation between prediction and imputation accuracy under missing covariates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ramosaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tulowietzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Estimating gaussian copulas with missing data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.05565" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A practical guide to causal discovery with cohort data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Foraita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Didelez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Witte</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.13395" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The max-min hillclimbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
