<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Causal Graph Convolutional Network for Traffic Prediction</title>
				<funder ref="#_Aj4UAsn">
					<orgName type="full">BNSF</orgName>
				</funder>
				<funder>
					<orgName type="full">SenseTime-Tsinghua Research Collaboration Funding</orgName>
				</funder>
				<funder ref="#_NyZ4GcH #_RUBaBmQ">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-07">7 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junpeng</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziyue</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhishuai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Causal Graph Convolutional Network for Traffic Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-07">7 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.07019v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling complex spatiotemporal dependencies in correlated traffic series is essential for traffic prediction. While recent works have shown improved prediction performance by using neural networks to extract spatiotemporal correlations, their effectiveness depends on the quality of the graph structures used to represent the spatial topology of the traffic network. In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. We then use graph convolutional networks to generate traffic forecasts. To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs. Our experimental results on a real traffic dataset demonstrate the superior prediction performance of the proposed method. The code is available at <ref type="url" target="https://github.com/MonBG/DCGCN">https: //github.com/MonBG/DCGCN</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Urban transportation plays a crucial role in modern life. With the rapid development of urbanization, the fast growing urban population and the number of cars bring great pressure to urban transportation system. In this situation, traffic prediction can help understand traffic propagation patterns and serves as the cornerstone of traffic management.</p><p>In the last decades, thousands of data-driven methods for traffic prediction have been proposed. Classical statistical methods, e.g., vector auto-regression (VAR) <ref type="bibr" target="#b0">[1]</ref>, autoregressive integrated moving average (ARIMA) <ref type="bibr" target="#b1">[2]</ref>, are pioneering works used to model traffic series. Later, deep learning based methods <ref type="bibr" target="#b2">[3]</ref> began to be increasingly popular, and achieved the overwhelming results. Most state-of-the-art deep learning methods integrate graph convolution network (GCN) <ref type="bibr" target="#b3">[4]</ref> into recurrent neural networks (RNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> or 1-D convolution neural networks (CNNs) on the time axis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to model spatiotemporal dependencies of traffic data. However, the effectiveness of GCN generally depends on the quality of the graph adjacency matrix, which is expected to fully capture the spatial characteristics of the real traffic network. Some popularly used graph adjacency matrices 1 Junpeng Lin and Chen Zhang are with the Department of Industrial Engineering, Tsinghua University, Beijing, China (Email: linjp22@mails.tsinghua.edu.cn; zhangchen01@tsinghua.edu.cn) 2 Ziyue Li is with University of Cologne, 50923 Cologne, NRW, Germany (Email: zlibn@wiso.uni-koeln.de) 3 Zhishuai Li is with SenseTime Research, Shanghai, China (Email: lizhishuai@sensetime.com) 4 Lei Bai is with The Shanghai AI Laboratory, Shanghai, China (Email: baisanshi@gmail.com) 5 Rui Zhao is with SenseTime Research and Qing Yuan Research Institute of Shanghai Jiao Tong University, Shanghai, China (Email: zhaorui@sensetime.com) include traffic road topology <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>, traffic data correlation <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>, etc. In recent years, casual discovery emerges and is being explored in many machine learning applications. Casual discovery aims to analyze causal relationships behind statistical correlations of different variables and facilitate better machine learning. Typical approaches to incorporate causal discovery include encoding features from domain-specific causal models as input to downstream tasks <ref type="bibr" target="#b18">[19]</ref> and learning the structure of causal relationships between features for graph-based models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. As a powerful graph-based tool for modeling directed causal relationships between variables, Bayesian network (BN) is being applied in traffic prediction <ref type="bibr" target="#b19">[20]</ref>. By representing the traffic propagation trend with directed links, BN shows its potential for explicitly modeling traffic propagation pattern <ref type="bibr" target="#b21">[22]</ref>. Furthermore, BN could be extended to dynamic Bayesian networks (DBNs) <ref type="bibr" target="#b22">[23]</ref> to capture temporal dependencies in traffic series.</p><p>A limitation of current casual-embedded traffic prediction models is the assumption of stationary temporal dependencies. However, in reality, the dependencies of traffic data in different places do change over time. Recent works <ref type="bibr" target="#b23">[24]</ref> based on GCN have proposed to generate a time-invariant adaptive graph from trainable node embeddings during model training. However, to our best knowledge, there is no work to incorporate causal learning into adaptive spatial topology construction, which is expected to learn time-varying graphs with good explainability for traffic prediction.</p><p>In this work, we propose a novel causal-embedded approach for traffic prediction. It represents the spatiotemporal traffic network topology using a time-varying DBN (TVDBN), which is designed to adapt to the time-varying traffic propagation patterns by learning DBNs step by step. The learned TVDBN is able to summarize the dynamic spatiotemporal dependencies between nodes. Built upon it, graph convolution is applied to capture spatial dependencies for traffic prediction. We propose a complete deep learning based causal structure learning module that serves as a pretrained hyper-network to generate the graphs of the TVDBN. The predefined distance graph is further incorporated into graph generator and traffic prediction module as additional prior information to improve the performance. The contributions of our work are summarized as follows:</p><p>• We propose an approach based on GCN and RNN to learn a TVDBN that describes the time-varying causal relationships between different locations in traffic network. The augmented Lagrange method is applied for model training to ensure the acyclicity of the TVDBN. • To our knowledge, we are the first to learn the structure of TVDBN using deep learning. Compared with traditional causal structure learning methods by machine learning or statistics, our method can capture nonlinear causal relationships between nodes more flexibly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Deep Learning Based Traffic Prediction</head><p>The main challenge of traffic prediction is capturing spatiotemporal dependencies in traffic data. This involves three tasks including spatial topology construction, spatial dependency modeling and temporal dependency modeling <ref type="bibr" target="#b23">[24]</ref>.</p><p>Spatial topology construction aims to summarize the structural information of the traffic network into well-defined graph data structures, which are then used to extract spatial dependencies. CNN-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> were first proposed to divide maps into equally sized grids as images, and used convolution to describe correlations between neighbouring girds. Later to better process non-Euclidean correlations, graph neural networks (GNNs) provide a more flexible representation of the traffic network. For GNNs, it is crucial to construct a suitable adjacency matrix. The most common way is to predefine a proximity metric between pairs of nodes, such as geographic distance or connectivity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. However, the predefined adjacency matrix is static and has limited ability to describe highly dynamic spatial correlations between traffic data. To address this, some methods set the adjacency matrix as learnable parameters <ref type="bibr" target="#b26">[27]</ref> or generate it from learnable node embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>, consequently allowing the graph to adapt to the actual data. The Dynamic Graph Convolutional Recurrent Network (DGCRN) <ref type="bibr" target="#b23">[24]</ref> goes further by generating a self-adaptive dynamic adjacency matrix at each time step. While this approach offers high representational flexibility, the resulting dynamic graphs lack interpretability in practical applications, limiting their generalization to other traffic analysis scenarios.</p><p>Spatial dependency modeling targets at mining correlations between spatial nodes from the constructed spatial topology. For grid-based spatial modeling, CNN-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> apply 2-D convolution to capture spatial dependencies in Euclidean data. For graphs represented spatial modeling, most state-of-the-art methods use GNNs to process spatial information by aggregating messages from neighboring nodes through graph convolution or attention mechanisms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. Recent works propose more complicated graph convolution modules to enhance the expressive power and address the over-smoothing issue of GCN <ref type="bibr" target="#b27">[28]</ref>.</p><p>Temporal dependency modeling is critical in time series analysis. Recurrent neural networks are widely used to capture temporal dependencies between parts of the sequence. GRU <ref type="bibr" target="#b28">[29]</ref> and LSTM <ref type="bibr" target="#b29">[30]</ref> are further proposed to improve the ability to model long-term dependencies by introducing a gate mechanism to control the ratio of retaining longterm information. In traffic prediction, to incorporate spatial information, an intuitive approach is to use the outputs of spatial modules as input to RNN <ref type="bibr" target="#b30">[31]</ref>. Some works also modify the computation of gates to include graph convolution in GRU and LSTM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. To reduce the computational cost of RNN, CNN can be used in modeling temporal dependencies with 1-D convolution on the time axis <ref type="bibr" target="#b6">[7]</ref>. Recently, attention mechanisms are also applied for modeling long-term dependencies, e.g., ASTGCN <ref type="bibr" target="#b7">[8]</ref> and GMAN <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bayesian Network</head><p>As an efficient method for modeling directed causal relationships between variables, Bayesian networks (BNs) have been applied in many applications <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. BN is a probabilistic graphical model that takes the form of a directed acyclic graphs (DAG). In a BN, nodes represent variables, and directed edges represent casual dependencies between nodes. By learnisng the edges and parameters of the BN, the joint distribution of all the variables can be analyzed. To further capture causal relationships in time series, dynamic Bayesian networks (DBNs) <ref type="bibr" target="#b22">[23]</ref> extend this to multivariate time series data by accounting for causality between variables across and within time steps. By further assuming the causality structure changes over time, time-varying DBN (TVDBN) is further proposed <ref type="bibr" target="#b33">[34]</ref>. In particular, different types of time-varying patterns include step-wise change, smooth change, etc. As to casual structure learning of BN and its variants, there are two types of methods in general: constraint-based and score-based methods <ref type="bibr" target="#b34">[35]</ref>. Constraintbased methods <ref type="bibr" target="#b35">[36]</ref> rely on the validity of statistical assumptions and score-based methods <ref type="bibr" target="#b36">[37]</ref> require an appropriate structure score with an efficient combinatorial structure search strategy. By replacing the acyclicity constraint of BN with a continuous penalty, recent works <ref type="bibr" target="#b36">[37]</ref> transform scorebased methods into the well-studied continuous optimization problem.</p><p>Traffic analysis can benefit from the use of Bayesian networks to reveal traffic propagation patterns, which is an alternative to representing spatial topology. Liu et al. <ref type="bibr" target="#b21">[22]</ref> proposed to learn a DBN with tree structure to uncover causal interactions between traffic events. DBGCN <ref type="bibr" target="#b19">[20]</ref> applys graph convolution to extract spatiotemporal dependencies from a DBN learned by statistics. However, most existing BN and DBN-based models in traffic analysis learn graphs through statistical methods, which are simple linear models with the assumption of a stationary traffic process. Furthermore, to our best knowledge, there is no work embedding TVDBN into traffic prediction to capture time-varying causal relationships of traffic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>The task of traffic prediction aims to predict the future traffic variables, such as speed and flow, using historical data. The traffic network is denoted as G = (V, E, A), where V is a set of N = |V| nodes representing different locations (e.g., sensors or road segments) in the traffic network, and E is a set of edges representing the geographic connection between nodes. A ∈ R N ×N is the weighted adjacency matrix corresponding to E where each element represents the connectivity or proximity between nodes. A is treated as the prior information about the network structure.</p><p>The multivariate traffic series are denoted as a feature tensor </p><formula xml:id="formula_0">X ∈ R T in ×N ×D of G,</formula><formula xml:id="formula_1">[X t-T in +1∶t , G] f → Xt+1∶t+T out (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where F is the number of traffic variables to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Time-Varying Dynamic Bayesian Network</head><p>We start with DBN that captures stationary spatiotemporal causal dependencies between time series. The value of the i-th time series at time t is represented by x i,t ∈ R D .</p><p>We assume that time series influence each other in both a contemporaneous and a time-lagged manner, which are called intra-slice and inter-slice dependencies, respectively. Such dependencies can be determined by a graph structure B and a set of parameterized functions f i,t . If the value of x j,t-k affects the value of x i,t , the time series j belongs to the k-lag parent set of the time series i, denoted by π k i . We model the dependencies using the following general structural equation model (SEM):</p><formula xml:id="formula_3">x i,t = f i (X π 0 i ,t , ..., X π K i ,t-K ; B),<label>(2)</label></formula><p>where K is the lag order and</p><formula xml:id="formula_4">X π k i ,t-k ∈ R |π k i |×D</formula><p>represents the feature tensor of the k-lag parent of the time series i at time t. For the linear condition, the SEM follows the standard structural VAR model <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_5">x i,t = K ∑ k=0 ∑ i∈π k i b k ij ⋅ x j,t-k + z i,t ,<label>(3)</label></formula><p>where b k ij is the influence coefficient that captures the directional relationship of the time series j to the time series i with lag k. z i,t is a vector of centered error terms that are independent within and across time. b k ij is the element of the k-lag weighted adjacency matrices of B, which allows us to write the equation (3) in matrix form:</p><formula xml:id="formula_6">X t = K ∑ k=0 B k X t-k + Z t ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">B k = (b k ij ) and b k ij = 0 if i / ∈ π k j .</formula><p>The directed graph represented by intra-slice matrix B 0 is required to be acyclic in causal meaning.</p><p>Traditional DBN assumes a stationary data generation process, i.e., {B k } K k=0 remains the same for each t, which is not always valid in reality. As in Fig. <ref type="figure" target="#fig_0">1</ref>, time-varying DBN allows the graph structure and parameters to change over time. The SEM of TVDBN is:</p><formula xml:id="formula_8">x i,t = f i,t (X π 0 i,t ,t , ..., X π K i,t ,t-K ; B t )<label>(5)</label></formula><p>with</p><formula xml:id="formula_9">B t = {B k t } K k=0</formula><p>and the corresponding linear SEM is:</p><formula xml:id="formula_10">X t = K ∑ k=0 B k t X t-k + Z t<label>(6)</label></formula><p>IV. METHODOLOGY</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Framework</head><p>In this section, we introduce the architecture of the proposed Dynamic Causal Graph Convolutional Network (DCGCN). As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, DCGCN is composed of three blocks including feature extraction, GCN-based recurrent causal structure learning (GRCSL) and dynamic graph convolution prediction module (DGCPM). The feature extraction block incorporates time of day and prior graph structures to augment the traffic data. The resulting features are fed into GRCSL to generate graphs of TVDBN. GRCSL is trained as a hyper-network based on the causal reconstruction loss of the input sequence. DGCPM applies graph convolution to the dynamic causal graphs of TVDBN generated by the pre-trained GRCSL and produces the final traffic forecasts. The details of each block are explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>To accurately model traffic conditions, it is crucial to consider the dynamic spatiotemporal dependencies of the traffic network, which are influenced by the topology of the network and exhibit diverse characteristics on short and longterm time horizons. Therefore, it is necessary to integrate geographic and temporal information into the input features to describe the dynamic traffic status.</p><p>At each discrete time step t, V t is the value of traffic parameters (speed or flow) and T t represents time of day. Given the weighted adjacency matrix A of geographical traffic network as prior information, we extract the spatial features by spectral-based graph convolution <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_11">S t = GCONV 1 L (V t , A),<label>(7)</label></formula><p>where GCONV 1 L (X, A) is a L layers spectral-based graph convolution operation with skip connection defined as</p><formula xml:id="formula_12">GCONV 1 L (X, A) = H (L) , H (0) = XΘ (0) , H (l) = ReLU( ÂH (l-1) Θ (l) ) + H (l-1) , Ã = A + I, Dii = ∑ j Ãij , Â = D-1 2 Ã D 1 2 ,<label>(8)</label></formula><p>with adjacency matrix A, identity matrix I and learnable parameters Θ (l) for l = 1, ..., L. The final features are obtained by concatenation</p><formula xml:id="formula_13">X t = V t ∥ T t ∥ S t .<label>(9)</label></formula><p>where concatenation operator || represents joining two matrices or tensors along the last dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GCN-Based Recurrent Causal Structure Learning</head><p>We propose to capture the time-varying causal structure in traffic series by RNN, which generates the dynamic causal graphs stepwise from its hidden state. The challenge is to construct suitable input features for RNN that contain information about the dynamics of the causal structure. To address this challenge, we use an attention mechanism with multihead scaled dot-product <ref type="bibr" target="#b38">[39]</ref>, to describe the spatiotemporal correlations between nodes, i.e., </p><formula xml:id="formula_14">sdot(Q, K) = (QW Q )(KW K ) ⊤ √ d ∈ R N ×N , msdot(Q, K) = [sdot 1 (Q, K), ..., sdot h (Q, K)] ∈ R N ×N ×h , C0 t = msdot(X t , X t ), Ck t = msdot(X t , X t-k ), k = 1, ..., K,<label>(10)</label></formula><formula xml:id="formula_15">W k cr + H k t-1 W k hr + b k r ), Z k t =σ(C k t W k cz + H k t-1 W k hz + b k z ), Hk t = tanh(C k t W k ch + (R k t ⊙ H k t-1 )W k hh + b k h ), H k t =Z k t ⊙ H t-1 + (1 -Z k t ) ⊙ Hk t ,<label>(11)</label></formula><p>where </p><formula xml:id="formula_16">W k cr , W k cz , W k ch , W k hr , W k hz , W</formula><formula xml:id="formula_17">Bk t = CONV 1×1 (Unflat(H k t )) ∈ R N ×N ,<label>(12)</label></formula><p>where ReLU is added after each layer except the last. Following MCSL, we apply Gumbel-Sigmoid to force the elements of Bk t to be close to 0 or 1. In addition, the diagonal elements of the intra-slice matrices are masked as zero to satisfy the acyclic constraint of B 0 t . Considering the complex nonlinear dynamics in real traffic data, we extend the linear SEM Eq. ( <ref type="formula" target="#formula_10">6</ref>) to a nonlinear version by spatial-based graph convolution <ref type="bibr" target="#b39">[40]</ref>, i.e.,</p><formula xml:id="formula_18">X t = K ∑ k=0 GCONV 2 L (X t-k , B k t ) + Z t ,<label>(13)</label></formula><p>where GCONV</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>L is a L layers spatial-based graph convolution operation with skip connections defined as: 1) ,</p><formula xml:id="formula_19">GCONV 2 L (X, A) = H (L) , H (0) = XΘ (0) , H (l) = ReLU( ÂH (l-1) Θ (l) ) + H (l-</formula><formula xml:id="formula_20">D ii = ∑ j A ij , Â = D -1 A.</formula><p>(14) With the GCN-based SEM, we reconstruct the current state value with the generated graphs by</p><formula xml:id="formula_21">Xt = MLP ( K ∑ k=0 GCONV 2 L (X t-k , B k t )) , (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>where MLP is the multilayer perceptron. By minimizing the sum of squares between Xt and X t and increasing the sparsity of {B k t } K k=0 under the acyclicity constraint of intra-slice graphs, we learn the GCN-based recurrent casual structure learning (GRCSL) network. Note that when the input time series length is set to T in , GRCSL can only generate K-lag DBNs of the last T in -K time steps. To balance the number of time steps for generating causal graphs and the number of lag in causal learning, both of which impact the representation capability in the DGCPM, we choose K = 1 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dynamic Graph Convolution Prediction Module</head><p>Built upon GRCSL, we propose a dynamic graph convolution prediction module (DGCPM) to predict multi-step traffic status. In DGCPM, we redefine X t = V t ∥ T t and incorporate time-varying causal structure B t together with the static graph A.</p><p>Note that SEM is designed to reconstruct the current state of a variable from the current and historical states of its parent variables. However, since the future state of the variable is unknown, intra-slice dependencies are not available in prediction. To address this problem, we decompose the general SEM in prediction into a two-step process:</p><formula xml:id="formula_23">xi,t = f inter i,t (X π 1 i,t-1 ,t-1 , ..., X π K i,t-1 ,t-K ; B 1 t-1 , ..., B K t-1 ), xi,t = f intra i,t ( Xπ 0 i,t-1 ,t ; B 0 t-1 ),<label>(16)</label></formula><p>where xi,t and xi,t are the initial and final predictions of x i,t , respectively.</p><formula xml:id="formula_24">f inter i,t</formula><p>takes historical data as input to capture inter-slice dependencies and f intra i,t takes the initial prediction of Xπ 0 i ,t as input to capture intra-slice dependencies. Given K = 1, for GCN-based SEM at the future time t, we have:</p><formula xml:id="formula_25">Xt = GCONV 2 L (X t-1 , B 1 t-1 ), H t-1 = GCONV 2 L ( Xt , B 0 t-1 ),<label>(17)</label></formula><p>where (B</p><formula xml:id="formula_26">0 t-1 , B 1 t-1 ) is treated as an approximation to (B 0 t , B 1 t ).</formula><p>To highlight the importance of the current state of a node in predicting its future state, we add an identity matrix to the adjacency matrices before graph convolution. We define the above two-step graph convolution process on TVDBN as dynamic graph convolution</p><formula xml:id="formula_27">DyGCONV L (X t-1 , B 0 t-1 , B 1 t-1 ). Given the dynamic causal graphs {(B 0 t , B 1 t )} P t=1 generated</formula><p>from GRCSL, we fuse the output features of dynamic graph convolution at all time steps with node embeddings generated from spectral-based prior graph convolution, i.e.,</p><formula xml:id="formula_28">S t = GCONV 1 L (V t ∥A), Ht = H t ∥S t , H = [ Ht-T in +2 , ..., Ht ] ∈ R (T in -1)×N ×H .<label>(18)</label></formula><p>Then we obtain the traffic prediction by applying a linear transformation to the above features:</p><formula xml:id="formula_29">H = Reshape(H) ∈ R (T in -1)×(N ×H) , Xt+1∶t+T out = Reshape( HW out ) ∈ R T out ×N ×F ,<label>(19)</label></formula><p>where W out is the learnable parameter of linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Training</head><p>To reduce the computation complexity, we train GRCSL and DGCPM sequentially. In particular, GRCSL combined with the feature extraction block, is treated as a hypernetwork and trained by optimizing the following problem:</p><formula xml:id="formula_30">min Θ c f (Θ c ) = 1 T in -1 T in ∑ t=2 [ 1 2 ∥ Xt -X t ∥ 2 2 + λ (∥B 0 t ∥ 1 + ∥B 1 t ∥ 1 ) ], s.t. h(B 0 t ) =tr(e B 0 t ⊙B 0 t ) -N = 0, t = 2, ..., T in ,<label>(20)</label></formula><p>where Θ c represents all the learnable parameters of GRCSL,</p><formula xml:id="formula_31">∥A∥ 1 = ∑ i ∑ j |A ij |, ∥A∥ 2 = √ ∑ i ∑ j A 2 ij , h(A)</formula><p>= 0 is the NOTEARS constraint to ensure the acyclicity of intraslice graphs, and tr(⋅) means the trace of a matrix. Note that we ignore the offset in time index here to improve readability. We use the augmented Lagrangian approach to solve problem Eq. ( <ref type="formula" target="#formula_30">20</ref>) with the following augmented Lagrangian function <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_32">L ρ (Θ c , α) = f (Θ c ) + α T in ∑ t=2 |h(B 0 t )| + ρ 2 ( T in ∑ t=2 |h(B 0 t )|) 2 ,<label>(21)</label></formula><p>where ρ &gt; 0 is the step size parameter and α is the Lagrange multiplier. When ρ goes to positive infinity, the minimizer of L ρ (Θ c , α) must satisfy the NOTEARS constraint. We update Θ c , α and ρ with the following strategy:</p><formula xml:id="formula_33">Θ k+1 c =argmin Θ c L ρ k (Θ c , µ k ),<label>(22)</label></formula><formula xml:id="formula_34">α k+1 =α k + ρ k T in ∑ t=2 |h(B 0 t )|,<label>(23)</label></formula><formula xml:id="formula_35">ρ k+1 = { ηρ k , if [∑ T in t=2 |h(B 0 t )|] k+1 &gt; γ [∑ T in t=2 |h(B 0 t )|] k , ρ k , o.w. (<label>24</label></formula><formula xml:id="formula_36">)</formula><p>where k is the iteration index, η &gt; 1 and 0 &lt; γ &lt; 1 are hyperparameters. We stop iterations when ∑</p><formula xml:id="formula_37">T in t=2 |h(B 0 t )</formula><p>| is less than a small number ξ. The subproblem Eq. ( <ref type="formula" target="#formula_33">22</ref>) is to learn GRCSL with L ρ k (Θ c , µ k ) as the loss function, which can be solved by stochastic gradient descent. Built upon the pre-trained GRCSL, we train the DGCPM by using the mean absolute error loss as the objective function and minimizing the multi-step prediction loss with curriculum learning. We start by training the model to make one-step predictions and gradually increase the prediction length to its maximum T out as the training iterations progress. This approach helps the algorithm to find a good local minimum for the model in the early stages, which is advantageous for achieving better prediction performance in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Experimental Settings</head><p>We conduct experiments on a public real-world traffic dataset METR-LA, which contains traffic speed data collected from loop detectors in the highway of Los Angeles <ref type="bibr" target="#b41">[42]</ref>. METR-LA collects 4 months of data ranging from Mar 1st 2012 to Jun 30th 2012 from 207 selected sensors. In our experiments, the sensors in different locations are viewed as nodes in the traffic network.</p><p>Following DCRNN <ref type="bibr" target="#b4">[5]</ref>, we set the time granularity of speed data to 5 minutes and apply Z-Score normalization. 70% of data is used for training, 20% are used for testing while the remaining 10% for validation. The predefined weighted adjacency matrix A is constructed by calculating the pairwise road network distances between sensors:</p><formula xml:id="formula_38">A ij = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ exp (- dist(v i ,v j ) 2 σ 2 ) , if dist(v i , v j ) ≤ κ 0, o.w. (<label>25</label></formula><formula xml:id="formula_39">)</formula><p>where dist(v i , v j ) is the road network distance from sensor v i to sensor v j . σ is the standard deviation of distances and κ = 0.1 is the threshold parameter. The input and output sequence length T in and T out are set to 12. The prediction target is traffic speed with dimension F = 1. Number of layers in graph convolution L is 4. For missing values marked as zero in METR-LA, we filter out the missing values in loss function and metrics calculation. Following previous work <ref type="bibr" target="#b40">[41]</ref>, we empirically set the hyperparameters τ = 0.2, η = 10 and γ = 0.5. The l 1 penalty weight λ is chosen as 2 × 10 -5 to slightly control false discoveries. A detailed selection of the hyperparameters could also be achieved by a grid search, but this involves high computational costs. The initial value of the Lagrange multiplier is set to 0, i.e. α 0 = 0. And ρ 0 is set to 10 -3 so that the optimization program focuses on minimizing the reconstruction error at an early stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines and Evaluation Metrics</head><p>We compare our proposed method with the classical and state-of-the-art deep learning based models, including (1) VAR <ref type="bibr" target="#b0">[1]</ref>: a statistical model which captures linear correlations between future traffic series and historical data; For all the baselines, we use the default settings from their original papers. The performance of traffic prediction is measured by three commonly used metrics, including (1) Mean Absolute Error (MAE), (2) Root Squared Error (RMSE), and (3) Mean Absolute Percentage Error (MAPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison</head><p>We compare the prediction performances for the future step 3 (15 minutes), step 6 (30 minutes), and step 12 <ref type="bibr">(1 hour)</ref>. The comparison between all methods is shown in Table <ref type="table" target="#tab_3">I</ref>. Note that since the computational cost of learning DBN for DBGCN on a large dataset is intolerable, we compare the proposed method with DBGCN on a subset of METR-LA with 20 nodes. The results show that: (1) methods with adaptive spatial dependence graphs, such as AGCRN, generally achieve better performance than other methods based on a static predefined graph, indicating the importance of adaptive spatial topologies. (2) Our methods outperform </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>To further demonstrate the effectiveness of the key modules of our DCGCN, we conduct an ablation study at METR-LA. Considering the computational burden of certain models, we perform the ablation experiments on a subset of METR-LA with 20 nodes. We name DCGCN without certain modules as follows: (1) dist: DCGCN without GRCSL. We replace the dynamic causal graphs generated by GRCSL with a static predefined distance graph. (2) static DBN: DCGCN without GRCSL. We replace the dynamic causal graphs from GRCSL with a static DBN learned using classical casuality discovery methods <ref type="bibr" target="#b42">[43]</ref>. (3) w/o prior: DCGCN without incorporating prior information (i.e., the predefined distance graph) in feature extraction module and DGCPM.</p><p>We report the results in Fig. <ref type="figure" target="#fig_4">3</ref>, and can draw some conclusions as follows: (1) Compared with the predefined distance graph, DGCPM based on causal graphs including static DBN and TVDBN shows better prediction performance. It validates that causal discovery is helpful for constructing finer spatial-temporal topology representation of the traffic network. (2) DCGCN outperforms models based on static DBN, demonstrating the need to capture nonlinear and timevarying causal traffic propagation patterns. (3) Incorporating prior information such as distance-based spatial topology of the traffic network helps to construct better dynamic causal graphs, leading to better prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, we propose a novel approach to enhance the performance of GCN-based methods for traffic prediction by incorporating time-varying dynamic causal graphs that can capture time-varying spatiotemporal relationships of traffic series. By leveraging deep learning to model the generation mechanism of causal graphs, we can significantly reduce the computational cost of structure learning for TVDBN. Our experimental results on the METR-LA dataset show that our proposed method achieves superior prediction performance in both the short-term and long-term scales. Overall, our approach offers a promising solution for accurately modeling time-varying spatiotemporal dependencies in traffic prediction with the help of causal discovery. Further investigation into the dynamic causal graphs learned from GRCSL can be beneficial in uncovering the time-varying traffic propagation patterns in the future. Such insights can help us better understand the underlying spatial dependence relationships of traffic systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of time-varying DBN with 7 nodes and lag K = 1.</figDesc><graphic coords="3,329.82,50.08,208.09,90.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the DCGCN. (a) shows the overall framework of the DCGCN. (b) shows the details of the pre-trained causal learning hyper-network, including the feature extraction block to incorporate prior information and the GCN-based recurrent causal structure learning (GRCSL) module. (c) shows the details of the dynamic graph convolution prediction module (DGCPM) used to generate traffic forecasts.</figDesc><graphic coords="4,54.00,50.08,504.00,238.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>are learnable parameters. ⊙ represents element-wise product between two matrices or tensors.H k t ∈ R N 2 ×His the hidden state with dimension H. Then we unflatten the hidden states as tensors in R H×N ×N and generate causal graphs by three layers 1 × 1 convolution:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2 )</head><label>2</label><figDesc>DCRNN [5]: an RNN-based model which integrates diffusian graph convolutions into RNN with encoder-decoder structure; (3) STGCN<ref type="bibr" target="#b6">[7]</ref>: a complete convolutional model which capture spatial and temporal dependencies by graph convolution and 1D-convolution on time axis, respectively; (4) ASTGCN<ref type="bibr" target="#b7">[8]</ref>: adding an attention mechanisms into STGCN by using spatial and temporal attention to adjust the input before convolution. (5) AGCRN<ref type="bibr" target="#b5">[6]</ref>: an RNN-based model which learns an adaptive static graph and integrates graph convolution with node adaptive parameter learning into GRU gate; (6) DBGCN<ref type="bibr" target="#b19">[20]</ref>: an GCN-based model that learns an adptive stationary DBN through statistical methods and applies graph convolution sequentially to graphs of the DBN. This is the only work also considering causality for spatial dependence analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Prediction performance of ablation study at each horizon.</figDesc><graphic coords="7,102.66,50.08,403.19,92.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>We uses the learned TVDBN as dynamic causal adjacency graphs in the downstream GCN-based traffic prediction module. Curriculum learning is used in training to achieve better performance. The training of TVDBN structure learning and traffic prediction modules are conducted separately and sequentially, which improves performance and reduces the training time.</figDesc><table><row><cell>• We conduct experiments on a widely used traffic bench-</cell></row><row><cell>mark dataset METR-LA to evaluate the traffic predic-</cell></row><row><cell>tion performance of the proposed model. The results</cell></row><row><cell>show that our model can outperform the classical and</cell></row><row><cell>state-of-the-art baselines in both short-term and long-</cell></row><row><cell>term prediction performance.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>where W Q and W K are learnable parameters of linear transformation, h is the number of heads and the learnable parameters of each head are different.</figDesc><table><row><cell>C0 t and</cell><cell>Ck t are</cell></row><row><cell cols="2">indicators of intra-slice and k-lag inter-slice spatial corre-</cell></row><row><cell cols="2">lations, respectively. We further flatten the tensors { to matrices in R N 2 ×h , denoted by {C k t } K k=0 , such that each Ck K t } k=0 row of C k t captures the dynamics of an element in B k t . Subsequently, C k t is treated as the input of GRU, i.e.,</cell></row><row><cell>R k t =σ(C k t</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I TRAFFIC</head><label>I</label><figDesc>FORECASTING PERFORMANCE COMPARISON FOR METR-LA</figDesc><table><row><cell></cell><cell>Horizon 3</cell><cell></cell><cell>Horizon 6</cell><cell></cell><cell>Horizon 12</cell><cell></cell></row><row><cell cols="7">Methods MAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSE</cell></row><row><cell>VAR</cell><cell cols="2">4.42 10.20% 7.89</cell><cell cols="2">5.41 12.70% 9.13</cell><cell cols="2">6.52 15.80% 10.11</cell></row><row><cell>DCRNN</cell><cell>2.77 7.30%</cell><cell>5.38</cell><cell>3.15 8.80%</cell><cell>6.45</cell><cell>3.60 10.50%</cell><cell>7.6</cell></row><row><cell>STGCN</cell><cell>2.88 7.62%</cell><cell>5.74</cell><cell>3.47 9.57%</cell><cell>7.24</cell><cell>4.59 12.70%</cell><cell>9.4</cell></row><row><cell cols="2">ASTGCN 3.09 8.06%</cell><cell>5.47</cell><cell>3.67 9.87%</cell><cell>6.46</cell><cell cols="2">4.43 12.57% 7.64</cell></row><row><cell>AGCRN</cell><cell>2.87 7.70%</cell><cell>5.58</cell><cell>3.23 9.00%</cell><cell>6.58</cell><cell cols="2">3.62 10.38% 7.51</cell></row><row><cell>DCGCN</cell><cell>2.73 7.04%</cell><cell>5.23</cell><cell>3.11 8.62%</cell><cell>6.28</cell><cell cols="2">3.57 10.47% 7.41</cell></row><row><cell cols="2">DBGCN 20 2.79 7.29%</cell><cell>5.13</cell><cell>3.16 8.60%</cell><cell>6.04</cell><cell cols="2">3.69 10.34% 7.14</cell></row><row><cell cols="2">DCGCN 20 2.72 7.06%</cell><cell>5.01</cell><cell>3.05 8.37%</cell><cell>5.92</cell><cell>3.48 9.94%</cell><cell>6.94</cell></row><row><cell cols="6">1 Methods with subscript 20 runs on a sub-dataset with 20 nodes.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This paper was supported by the <rs type="funder">NSFC</rs> Grant <rs type="grantNumber">72271138</rs> and <rs type="grantNumber">71932006</rs>, and the <rs type="funder">BNSF</rs> Grant <rs type="grantNumber">9222014</rs>. This work is also partly funded by <rs type="funder">SenseTime-Tsinghua Research Collaboration Funding</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NyZ4GcH">
					<idno type="grant-number">72271138</idno>
				</org>
				<org type="funding" xml:id="_RUBaBmQ">
					<idno type="grant-number">71932006</idno>
				</org>
				<org type="funding" xml:id="_Aj4UAsn">
					<idno type="grant-number">9222014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vector autoregressive models for multivariate time series</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zivot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling financial time series with S-PLUS®</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="385" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on modern deep neural network for traffic prediction: Trends, methods and challenges</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Tedjopurnomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1544" to="1561" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01926</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional recurrent network for traffic forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lsgcn: Long short-term traffic prediction with graph convolutional networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2355" to="2361" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensor topic models with graphs and applications on individualized travel patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ziyue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2756" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Individualized passenger travel pattern multi-clustering based on graph regularized tensor latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1247" to="1278" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tensor completion for weakly-dependent data on graph for metro passenger flow prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Sergin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4804" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly contrastive representation learning on road network and trajectory</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep multi-view spatial-temporal network for taxi demand prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discussion of &quot;a novel approach to the analysis of spatial and functional data over complex domains</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quality Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="196" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Profile decomposition based hybrid transfer learning for cold-start data anomaly detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long-short term spatiotemporal tensor prediction for passenger flow profile</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5010" to="5017" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Correlated time series self-supervised representation learning via spatiotemporal bootstrapping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06994</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causalgnn: Causal-based graph neural networks for spatio-temporal epidemic forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadilek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="12" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Traffic congestion propagation inference using dynamic bayesian graph convolution network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation research part C: emerging technologies</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page">103526</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mm-dag: Multi-task dag learning for multi-modal data-with application for traffic congestion analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ketter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02831</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering spatio-temporal causal interactions in traffic data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1010" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dynamic bayesian networks: representation, inference and learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepstn+: Contextaware spatial-temporal neural network for crowd flow prediction in metropolis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1020" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gman: A graph multi-attention network for traffic prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph structure learning for traffic forecasting</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1177" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">T-gcn: A temporal graph convolutional network for traffic prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3848" to="3858" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A framework for modeling and assessing system resilience using a bayesian network: A case study of an interdependent electrical infrastructure system</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">U I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marufuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Buchanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Critical Infrastructure Protection</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="62" to="83" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new scalable bayesian network learning algorithm with applications to economics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsagris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Economics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="367" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-stationary dynamic bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">D&apos;ya like dags? a survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structural vector autoregressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research methods and applications in empirical macroeconomics</title>
		<imprint>
			<publisher>Edward Elgar Publishing</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="515" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dag-gnn: Dag structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Big data and its technical challenges</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Labrinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic mmhc: A local search algorithm for dynamic bayesian network structure learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Data Analysis XII: 12th International Symposium</title>
		<title level="s">Proceedings</title>
		<meeting><address><addrLine>IDA; London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-10-17">2013. October 17-19, 2013. 2013</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="392" to="403" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
