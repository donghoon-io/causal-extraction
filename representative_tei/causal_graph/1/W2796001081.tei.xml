<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects</title>
				<funder ref="#_jFUtnqc">
					<orgName type="full">DARPA XAI</orgName>
				</funder>
				<funder ref="#_eNtfNzv #_Gv7pn6s">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_F2uKnFf">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Q8pPE5u">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-03-29">29 Mar 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
							<email>yuanluxu@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Qin</surname></persName>
							<email>qinlei@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Inst. Computing Technology</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
							<email>xiaobai.liu@mail.sdsu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. Computer Science</orgName>
								<orgName type="institution">San Diego State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
							<email>jianwen.xie@hikvision.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Hikvision Research Institute</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-29">29 Mar 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1709.05437v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causaleffect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tracking objects of interest in videos is a fundamental computer vision problem that has great potentials in many video-based applications, e.g., security surveillance, disas-Figure <ref type="figure">1</ref>. Illustration of visibility fluent changes. There are three states: visible, occluded, contained. When a person approaches a vehicle, its state changes from "visible" to "occluded" to "contained", such as the person1 and person2 (a-e). When a vehicle passes, the person4 is occluded. The state of person4 changes from "visible" to "occluded" in (d-e). ter response, and border patrol. In these applications, a critical problem is how to obtain the complete trajectory of the object of interest while observing it moving in the scene through camera view. This is a challenging problem since an object of interest might undergo frequent interactions with the surrounding, e.g., entering a vehicle or a building, or with the other objects, e.g., passing behind another sub-ject. With these interactions, the visibility status of a subject will be varying over time, e.g., changing from "invisible" to "visible" and vice versa. In the literature, most state-of-theart trackers utilize appearance or motion cues to localize subjects in video sequences and are likely to fail to track the subjects whose visibility status keep changing.</p><p>To deal with the above challenges, in this work, we propose to explicitly reason subjects' visibility status over time, while tracking the subjects of interests in surveillance videos. Traditional trackers are likely to fail when the target become invisible due to occlusion, our proposed method could jointly infer objects' locations and visibility fluent changes, thus helping to recover the complete trajectories. The proposed techniques, with slight modifications, can be generalized to other scenarios, e.g., hand-held cameras, driverless vehicles, etc.</p><p>The key idea of our method is to introduce a fluent variable for each subject of interest to explicitly indicate his/her visibility status in videos. Fluent was firstly used by Newton to denote the time varying status of an object. It is also used to represent the varying object status in commonsense reasoning <ref type="bibr" target="#b26">[27]</ref>. In this paper, the visibility status of objects can be described as fluents varying over time. As illustrated in Fig. <ref type="figure">1</ref>, the person 3 and person 5 are walking through the parking lot, while the person 1 and person 2 are entering a sedan. The visibility status of person 1 's and person 2 's changes first from "visible" to "occluded", and then to "contained". This group example demonstrates how objects' visibility fluents change over time along with their interactions to the surrounding.</p><p>We introduce a graphical model, i.e. Causal And-Or graph (C-AOG), to represent the causal relationships between object's activities (actions/sub-events) and object's visibility fluent changes. The visibility status of an object might be caused by multiple actions, and we need to reason the actual causality from videos. These actions are alternative choices that lead to the same occlusion status, and form the Or-nodes. Each leaf node indicates an action or sub-event that can be described by And-nodes. Taking the videos shown in Fig. <ref type="figure">1</ref> for instance, the status of "occluded" can be caused by the following actions: (i) walking behind a vehicle; (ii) walking behind a person; or (iii) inertial action that maintains the fluent unchanged.</p><p>The basic hypothesis of this model is that, for a particular scenario (e.g., parking-lot), there are only a limited number of actions that can cause the fluent to change. Given a video sequence, we need to create the optimal C-AOG and select the best choice for each Or-node in order to obtain the optimal causal parse graph, which is shown as red lines in Fig. <ref type="figure" target="#fig_3">3(a)</ref>.</p><p>We develop a probabilistic graph model to reason object's visibility fluent changes using C-AOG representation. Our formula integrates object tracking purposes as well to enable joint solution of tracking and fluent change reasoning, which are mutually beneficial. In particular, for each subject of interest, our method uses two variables to represent (i) subjects' positions in videos; and (ii) visibility status as well as the best causal parse graph. We utilize a Markov Chain Prior model to describe the transitions of these variables, i.e., the current state of a subject is only dependent on the previous state. We then reformulate the problem into an Integer Linear Programming model, and utilize dynamic programming to search the optimal states over time.</p><p>In experimental evaluations, the proposed method is tested on a set of challenging sequences that include frequent human-vehicle or human-human intersections. Results show that our method can readily predict the correct visibility status and recover the complete trajectories. In contrast, most of the alternative trackers can only recover part of the trajectories due to the occlusion or containment.</p><p>Contributions. There are three major contributions of the proposed framework: (i) a Causal And-Or Graph (C-AOG) model to represent object visibility fluents varying over time; (ii) a joint probabilistic formulation for object tracking and fluent reasoning; and (iii) a new occlusion reasoning dataset to cover objects with diverse fluent changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The proposed research is closely related to the following three research streams in computer vision and AI.</p><p>Multiple object tracking has been extensively studied in the past decades. In the past literatures, tracking-bydetection has become the mainstream framework <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. Specifically, a general detector <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> is first applied to generate detection proposals, and then data association techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref> are employed to link detection proposals over time in order to get object trajectories. Our approach also follows this pipeline, but is more focused on the reasoning of object visibility status.</p><p>Tracking interacting objects studies a more specific problem of tracking entangled objects. Some works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25]</ref> try to model the object appearing and disappearing phenomena globally, yielding strong assumptions on appearance, location or motion cues. On the contrary, other works attempt to model human-object and human-human interactions under specific scenarios, such as social activities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>, team sports <ref type="bibr" target="#b23">[24]</ref>, and people carrying luggage <ref type="bibr" target="#b2">[3]</ref>. In this paper, we propose a more principled way to track objects with both short-term interactions, e.g., passing behind another object, or long-term interactions, e.g., entering a vehicle and moving together.</p><p>Causal-effect reasoning is a popular topic in AI but has not received much attentions in the field of computer vision. It studies, for instances, the difference between cooccurrence and causality, and aims to learn causal knowledge automatically from low-level observations, e.g., im-  ages or videos. There are two popular causality models: Bayesian Network <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> and grammar models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Grammar models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref> are powerful tool for modeling high-level human knowledge in specific domains. Notably, Fire and Zhu <ref type="bibr" target="#b12">[13]</ref> have introduced a causal grammar to infer causal-effect relationship between object's status, e.g., door open/close, and agent's actions, e.g., pushing the door. They studied this problem using manually designed rules and video sequences in lab settings. In this work, we extend the causal grammar models to infer objects' visibility fluent and ground the task on challenging videos in surveillance systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Representation</head><p>In this paper, we define three states for visibility fluent reasoning: visible, (partially/fully) occluded, and contained. Most multiple object tracking methods are based on tracking-by-detection framework, which obtain good performance in visible and partially occluded situations. However, when full occlusions take place, these trackers usually regard the disappearing-and-reappearing objects as new objects. Although objects in fully occluded and contained states are invisible, there are still evidences to infer the locations of objects and fill-in the complete trajectory. We can distinguish object being fully occluded and object being contained from three empirical observations. Firstly, motion independence. In fully occluded state, such as a person staying behind a pillar, the motion of the person is independent of the pillar. While in contained state, such as a person sitting in a vehicle, or a bag in the trunk, the position and motion of the person/bag would be the same as the vehicle. Therefore, the inference of the visibility fluent of the object is important in tracking objects accurately in a complex environment.</p><p>Secondly, coupling actions and object fluent changes. Thirdly, visibility in the alternative camera views. In full occlusion state, such as a person occluded by a pillar, though the person could not be observed from the current viewpoint, he/she could be seen from the other viewpoints; while in contained state, such as a person in a vehicle, this person could not be seen from any viewpoints.</p><p>In this work, we mainly study the interactions of humans and the developed methods can also be expanded to other objects, e.g., animals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Causal And-Or Graph</head><p>In this paper, we propose a Causal And-Or Graph (C-AOG) to represent the action-fluent relationship, as illustrated in Fig. <ref type="figure" target="#fig_3">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a). A C-AOG has two types of nodes: (i)</head><p>Or-nodes that represent the variations or choices, and (ii) And-nodes that represent the decompositions of the toplevel entities. The arrows indicate the causal relations between actions and fluent transitions. For example, a C-AOG can be used to expressively model a series of action-fluent relations.</p><p>The C-AOG is capable of representing multiple alternatives for causes of occlusion and potential transitions. There are four levels in our C-AOG: visibility fluents, possible states, state transitions and agent actions. Or nodes represent alternative causes in visibility fluents and state levels; that is, one fluent can have multiple states and one state can have multiple transitions. An event can be decomposed into several atomic actions and represented by an And-node, e.g., an event of a person getting into a vehicle is a composition of four atomic actions: approaching the vehicle, opening the door, entering the vehicle, and closing the door.</p><p>Given a video sequence I with length T and camera calibration parameters H, we represent the scene R as</p><formula xml:id="formula_0">R = {O t : t = 1, 2, ..., T } , O t = o i t : i = 1, 2, ..., N t ,<label>(1)</label></formula><p>where O t denotes all the objects at time t, and N t is the size of O t , i.e., the number of objects at time t. N t is unknown and will be inferred from observations. Each object o i t is represented with its location l i t (i.e., bounding boxes in the image) and appearance features φ i t . To study the visibility fluent of a subject, we further incorporate a state variable s i t Each atomic action describes interactions among people and interacting objects. "P", "D", "T", "B" denotes "person", "door", "trunk", "bag", respectively. The dash triangle denotes fluent. The corresponding fluent could be "visible", "occluded" or "contained" for a person; "open", "closed" or "occluded" for a vehicle door or truck. See text for more details.</p><p>and an action label a i t , that is,</p><formula xml:id="formula_1">o i t = l i t , φ i t , s i t , a i t .<label>(2)</label></formula><p>Thus, the state of a subject is defined as</p><formula xml:id="formula_2">s i t ∈ S = { visible, occluded, contained } .<label>(3)</label></formula><p>We define a series of atomic actions Ω = {a i : i = 1, . . . , N a } that might change the visibility status, e.g., walking, opening vehicle door, etc. Fig. <ref type="figure" target="#fig_3">3</ref>(b) illustrates a small set of actions Ω covering the most common interactions among people and vehicles.</p><p>Our goal is to jointly find subject locations in video frames and estimate their visibility fluents M from the video sequence I. Formally, we have</p><formula xml:id="formula_3">M = {pg t : t = 1, 2, . . . , T }, pg t = {o i t = (l i t , φ i t , s i t , a i t ) | i = 1, 2, ..., N t },<label>(4)</label></formula><p>where pg t can be determined by the optimal causal parse graph at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Problem Formulation</head><p>According to Bayes' rule, we can solve our joint object tracking and fluent reasoning problem by maximizing a posterior (MAP),</p><formula xml:id="formula_4">M * = arg max M p(M |I; θ) ∝ arg max M p(I|M ; θ) • p(M ; θ) = arg max M 1 Z exp {-E(M ; θ) -E(I|M ; θ)}.<label>(5)</label></formula><p>The prior term E(M ; θ) measures the temporal consistency between successive parse graphs. Assuming G is a Markov</p><p>Chain structure, we can decompose E(M ; θ) as</p><formula xml:id="formula_5">E(M ; θ) = T -1 t=1 E(pg t+1 |pg t ) = T -1 t=1 Nt i=1 Φ(l i t+1 , l i t , s i t ) + Ψ(s i t+1 , s i t , a i t ).<label>(6)</label></formula><p>The first term Φ(•) measures the location displacement. It calculates the transition distance between two successive frames and is defined as:</p><formula xml:id="formula_6">Φ(l i t+1 , l i t , s i t ) = δ(Ds(l i t+1 , l i t ) &gt; τs), s i t = Visible, 1, s i t = Occ, Con,<label>(7)</label></formula><p>where D s (•, •) is the Euclidean distance between two locations on the 3D ground plane, τ s is the speed threshold and δ(•) is an indicator function. The location displacement term measures the motion consistency of object in successive frames.</p><p>The second term Ψ(•) measures the state transition energy and is defined as:</p><formula xml:id="formula_7">Ψ(s i t+1 , s i t , a i t ) = -log p(s i t+1 |s i t , a i t ),<label>(8)</label></formula><p>where p(s i t+1 |s i t , a i t ) is the action-state transition probability, which can be learned from the training data.</p><p>The likelihood term E(I|M ; θ) measures how well each parse graph explains the data, which can be decomposed as  where Υ(•) measures the likelihood between data and object fluents, and Γ(•) measures the likelihood between data and object actions. Given each object o i t , the energy function Υ(•) is defined as:</p><formula xml:id="formula_8">E(I|M ; θ) = T t=1 E(I t |pg t ) = T t=1 Nt i=1 Υ(l i t , φ i t , s i t ) + Γ(l i t , φ i t , a i t ),<label>(9)</label></formula><formula xml:id="formula_9">Υ(l i t , φ i t , s i t ) =      1 -h o (l i t , φ i t ), s i t = Visible, σ(D ς (ς i 1 , ς i 2 )), s i t = Occluded, 1 -h c (l i t , φ i t ), s i t = Contained,<label>(10)</label></formula><p>where h o (•) indicates the object detection score, h c (•) indicates the container (i.e., vehicles) detection score, and σ(•) is the sigmoid function. When an object is in either visible or contained state, appearance information can describe the probability of the existence of itself or the object containing it (i.e., container) at this location. When an object is occluded, there is no visual evidence to determine its state. Therefore, we utilize temporal information to generate candidate locations. We employ the SSP algorithm <ref type="bibr" target="#b28">[29]</ref> to generate trajectory fragments (i.e., tracklets). The candidate locations are identified as misses in complete object trajectories. The energy is thus defined as the cost of generating a virtual trajectory at this location. We compute this energy by computing the visual discrepancy between a neighboring tracklet ς i 1 before this moment and a neighboring tracklet ς i 2 after this moment. The appearance descriptor of a tracklet is computed as the average pooling of image descriptor over time. If the distance is below a threshold τ ς , a virtual path is generated to connect these two tracklets using B-spline fitting. The term Γ(l i t , φ i t , a i t ) is defined over the object actions observed from data. In this work, we study the fluents of human and vehicles, that is,</p><formula xml:id="formula_10">Γ(l i t , φ i t , a i t ) = σ(D h (l i t , φ i t |a i t )) + σ(D v (l i t , φ i t |a i t )),<label>(11)</label></formula><p>where σ(•) is the sigmoid function. The definitions of the two data-likelihood terms D h and D v are introduced in the rest of this section.</p><p>A human is represented by his/her skeleton, which consists of multiple joints estimated by sequential prediction technology <ref type="bibr" target="#b35">[36]</ref>. The feature of each joint is defined as the relative distances of this joint to four saddle points(two shoulders, the center of the body, and the middle between the two hipbones). The relative distances are normalized by dividing the length of head to eliminate the influence of scale. A feature vector ω h t concatenating the features of all joints is extracted, which is assumed to follow a Gaussian distribution:</p><formula xml:id="formula_11">D h (l i t , φ i t |a i t ) = -log N (ω h t ; µ a i t , Σ a i t ), (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>where µ a i t and Σ a i t are the mean and the covariance of the action a i t respectively, which are obtained from the training data.</p><p>A vehicle is described with its viewpoint, semantic vehicle parts, and vehicle part fluents. The vehicle fluent is represented by a Hierarchical And-Or Graph, as illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>. The feature vector of vehicle fluent ω v is obtained by computing fluent scores on each vehicle part and concatenating them together. We compute the average pooling feature ai for each action a i over the training data as the vehicle fluent template. Given vehicle fluent ω v t computed on image I t , the distance D v (l i t , φ i t |a i t ) is defined as</p><formula xml:id="formula_13">D v (l i t , φ i t |a i t ) = ω v t -a i t 2 . (<label>13</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Inference</head><p>We cast the intractable optimization of Eqn. ( <ref type="formula" target="#formula_4">5</ref>) as an Integer Linear Formulation (ILF) in order to derive a scalable and efficient inference algorithm. We use V to denote the locations of vehicles, and E to denote the edges between all possible pairs of nodes, whose time is consecutive and locations are close. The whole transition graph G = (V, E) is shown as Fig. <ref type="figure" target="#fig_5">5</ref>. Then the energy function Eqn. ( <ref type="formula" target="#formula_4">5</ref>) can be re-written as:</p><formula xml:id="formula_15">f * = arg max f mn∈Eo cmnfmn, cmn = -Φ(ln, lm, sm) -Ψ(sn, sm, am) -Υ(lm, φm, sm) -Γ(lm, φm, am), s.t. fmn ∈ {0, 1}, m fmn ≤ 1, m fmn = k f nk ,<label>(14)</label></formula><p>where f mn is the number of object moving from node V m to node V n , c mn is the corresponding cost. Since the subject of interest can only enter a nearby container (e.g., vehicle), to discover the optimal causal parse graph, we need to jointly track the container and the subject of interest. Similar to Eqn. ( <ref type="formula" target="#formula_15">14</ref>), the energy function of container is as follows:</p><formula xml:id="formula_16">g * = arg max g mn∈Ec dmn gmn, dmn = hc(lm, φm) -1, s.t. gmn ∈ {0, 1}, m gmn ≤ 1, m gmn = k g nk ,<label>(15)</label></formula><p>where h c (l m , φ m ) is the container detection score at location l m . Then we add the contained constrains as:</p><formula xml:id="formula_17">mn∈Ec g mn ≥ ij∈Eo f ij , s.t. t n = t j , l n -l j 2 &lt; τ c ,<label>(16)</label></formula><p>where τ c is the distance threshold. Finally, we combine Eqn. ( <ref type="formula" target="#formula_15">14</ref>)-( <ref type="formula" target="#formula_17">16</ref>) to obtain objective function for our model:</p><formula xml:id="formula_18">f * , g * = max f,g mn∈Eo cmnfmn + ij∈Ec dmn gmn, s.t. fmn ∈ {0, 1}, m fmn ≤ 1, m fmn = k f nk , gmn ∈ {0, 1}, i gmn ≤ 1, i gmn = k g nk , tn = tj, ln -lj 2 &lt; τc. (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>The re-formulated graph still follows a directed acyclic graph (DAG). Thus we can adopt the Dynamic Programming technique to efficiently search for the optimal solution, as illustrated in the Fig. <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We apply the proposed method on two tracking interacting objects datasets and evaluate the improvement in visual tracking brought by the outcomes of visibility status reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>We first utilize the Faster R-CNN models <ref type="bibr" target="#b29">[30]</ref> trained on the MS COCO dataset to detect involved agents (e.g., person and suitcase). The used network is the VGG-16 net, with score threshold 0.4 and NMS threshold 0.3. The tracklets similarity threshold τ ς is set as 0.8. The contained distance threshold τ c is set as the width of container 3 meters. The maximum number of contained objects in a container is set to 5. For appearance descriptors φ, we employ the dense sampling ColorNames descriptor <ref type="bibr" target="#b42">[43]</ref>, which applies square root operator <ref type="bibr" target="#b1">[2]</ref> and Bag-of-word encoding to the original ColorNames descriptors. For human skeleton estimation, we use the public implementation of <ref type="bibr" target="#b35">[36]</ref>. For vehicle detection and semantic part status estimation, we use the implementation provided by <ref type="bibr" target="#b21">[22]</ref> with default parameters mentioned in their paper.</p><p>We adopt the widely used CLEAR metrics <ref type="bibr" target="#b18">[19]</ref> to measure the performances of tracking methods. It includes four metrics, i.e., Multiple Object Detection Accuracy (MODA), Detection Precision (MODP), Multiple Object Tracking Accuracy (MOTA) and Tracking Precision (MOTP), which take into account three kinds of tracking errors: false positives, false negatives and identity switches. We also report the number of false positives (FP), false negatives (FN), identity switches (IDS) and fragments (Frag). A higher value means better for MODA, MODP, MOTA and MOTP, while a lower value means better for FP, FN, IDS and Frag. If the Intersection-over-Union (IoU) ratio of tracking results to groundtruth is above 0.5, we accept the tracking result as a correct hit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Datasets</head><p>People-Car dataset <ref type="bibr" target="#b33">[34]</ref> <ref type="foot" target="#foot_0">foot_0</ref> . This dataset consists of 5 groups of synchronized sequences on a parking lot, recorded from two calibrated bird-view cameras, with length of 300 ∼ 5100 frames. In this dataset, there are many instances of people getting in and out of cars. This dataset is challenging for the frequent interactions, light variation and low object resolution.</p><p>Tracking Interacting Objects (TIO) dataset. For current popular multiple object tracking datasets (e.g., PETS09 <ref type="bibr" target="#b11">[12]</ref>, KITTI dataset <ref type="bibr" target="#b14">[15]</ref>), most tracked objects are pedestrian and no evident interaction visibility fluent changes. Thus we collect two new scenarios with typical human-object interactions: person, suitcase, and vehicle on several places.</p><p>Plaza. We capture 22 video sequences in a plaza that describe people walking around, getting in/out vehicles.</p><p>ParkingLot. We capture 15 video sequences in a parking lot that shows vehicles entering/exiting the parking lot, people getting in/out vehicles, people interacting with People-Car Metric Our-full Our-1 Our-2 POM <ref type="bibr" target="#b13">[14]</ref> SSP <ref type="bibr" target="#b28">[29]</ref> LP2D <ref type="bibr" target="#b20">[21]</ref> LP3D <ref type="bibr" target="#b20">[21]</ref>  trunk/suitcase. All video sequences are captured by a GoPro camera, with frame rate 30fps and resolution 1920 × 1080. We use the standard chessboard and Matlab camera calibration toolbox to obtain camera parameters. The total number of frames of TIO dataset is more than 30K. There exist severe occlusions and large scale changes, making this dataset very challenging for traditional tracking methods.</p><p>Beside the above testing data, we collect another set of video clips for training. To avoid over-fitting, we set up different camera positions, different people and vehicles from the testing settings. The training data consists of 380 video clips covering 9 events: walking, opening vehicle door, entering vehicle, exiting vehicle, closing vehicle door, opening vehicle trunk, loading baggage, unloading baggage, closing vehicle trunk. Each action category contains 42 video clips on average.</p><p>Both the datasets and short video clips are annotated with bounding boxes for people, suitcases, vehicles, and visibility fluents of people and suitcases. The types of status are "visible", "occluded", and "contained". We utilize VATIC <ref type="bibr" target="#b31">[32]</ref> to annotate the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results and Comparisons</head><p>For People-Car dataset, we compare our proposed method with 5 baseline methods and their variants: successive shortest path algorithm (SSP) <ref type="bibr" target="#b28">[29]</ref>, K-Shortest Paths Algorithm (KSP-fixed, KSP-free, KSP-seq) <ref type="bibr" target="#b3">[4]</ref>, Probability Occupancy Map (POM) <ref type="bibr" target="#b13">[14]</ref>, Linear Programming (LP2D, LP3D) <ref type="bibr" target="#b20">[21]</ref>, and Tracklet-Based Intertwined Flows (TIF-IP, TIF-MIP) <ref type="bibr" target="#b34">[35]</ref>. We refer the reader to <ref type="bibr" target="#b34">[35]</ref> for more details about the method variants. The quantitative results are reported in Table <ref type="table" target="#tab_2">1</ref>. From the results, we can observe that the proposed method obtains better performance than the baseline methods.</p><p>For TIO dataset, we compare the proposed method with 6 state-of-the-arts: successive shortest path algorithm (SSP) <ref type="bibr" target="#b28">[29]</ref>, multiple hypothesis tracking with distinctive appearance model (MHT D) <ref type="bibr" target="#b19">[20]</ref>, Markov Decision Processes with Reinforcement Learning (MDP) <ref type="bibr" target="#b37">[38]</ref>, Discrete-Continuous Energy Minimization (DCEM) <ref type="bibr" target="#b25">[26]</ref>, Discretecontinuous optimization (DCO) <ref type="bibr" target="#b0">[1]</ref> and Joint Probabilistic Data Association (JPDA m) <ref type="bibr" target="#b17">[18]</ref>. We use the public implementations of these methods.</p><p>We report quantitative results and comparisons in Table <ref type="table" target="#tab_3">2</ref> TIO-Plaza TIO-ParkingLot People-Car for TIO dataset. From the results, we can observe that our method obtains superior performance to the other methods on most metrics. It validates that the proposed method can not only track visible objects correctly, but also reason locations for occluded or contained objects. The alternative methods do not work well mainly due to lack of the ability to track objects under long-term occlusion or containment in other objects. We set up three baselines to analyze the effectiveness of different components in the proposed method:</p><p>• Our-1: no likelihood term and only prior term is used.</p><p>• Our-2: only human data-likelihood term and prior term are used.</p><p>• Our-full: all terms are used, including prior terms, human and vehicle data-likelihood terms.</p><p>Based on comparisons of Our-1, Our-2 and Our-full, we can also conclude that each type of fluent plays its role in improving the final tracking results. Some qualitative results are displayed in Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>We further report fluent estimation results on TIO-Plaza sequences and TIO-ParkingLot sequences in Fig. <ref type="figure">8</ref>. From the results, we can see that our method can successfully reason the visibility status of subjects. Note that the precision of containment estimation is not high, since some people get in/out the vehicle from the opposite side towards the camera, as shown in Fig. <ref type="figure">7</ref>. Under such situation, there are barely any image evidence to reason the object status and multi-view setting might be a better way to reduce the ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a Causal And-Or Graph (C-AOG) model to represent the causal-effect relations between object visibility fluents and various human interactions. By jointly modeling short-term occlusions and longterm occlusions, our method can explicitly reason the visibility of subjects as well as their locations in the videos. Our method clearly outperforms the alternative methods in complicated scenarios with frequent object interactions. In this work, we focus on the human-interactions as a runningcase of the proposed technique, and we will explore the extension of our method to other types of objects (e.g., animal, drones) in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1. Illustration of visibility fluent changes. There are three states: visible, occluded, contained. When a person approaches a vehicle, its state changes from "visible" to "occluded" to "contained", such as the person1 and person2 (a-e). When a vehicle passes, the person4 is occluded. The state of person4 changes from "visible" to "occluded" in (d-e). (f) shows the corresponding top-view trajectories of different persons. The numbers are the persons' IDs. The arrows indicate the moving direction.</figDesc><graphic coords="1,309.66,435.86,114.55,74.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of a person's actions and her visibility fluent changes when she enters a vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>For</head><label></label><figDesc>example, as illustrated in Fig.2, if a person gets into a vehicle, the related sequential atomic actions are: approaching a vehicle, opening the vehicle door, getting into the vehicle, and closing the vehicle door; the related object fluent changes are vehicle door closed → open → closed. The fluent change is a consequence of agent actions. If the fluentchanging actions do not happen, the object should maintain its current fluent. For example, a person that is contained in a vehicle will remain contained unless he/she opens the vehicle door and gets out of the vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) The proposed Causal And-Or Graph (C-AOG) model for the fluent of visibility. We use a C-AOG to represent the visibility status of an subject. Each OR node indicates a possible choice and an arrow shows how visibility fluent transits among states. (b) A series of atomic actions that could possibly cause visibility fluent change.Each atomic action describes interactions among people and interacting objects. "P", "D", "T", "B" denotes "person", "door", "trunk", "bag", respectively. The dash triangle denotes fluent. The corresponding fluent could be "visible", "occluded" or "contained" for a person; "open", "closed" or "occluded" for a vehicle door or truck. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Illustration of Hierarchical And-Or Graph. The vehicle is decomposed into different views, semantic parts and fluents. Some detection results are drawn below, with different colored bounding boxes denoting different vehicle parts, solid/dashed boxes denoting state "closed"/"open".</figDesc><graphic coords="5,170.86,208.11,89.89,52.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Transition graph utilized to formulate the integer linear programming. Each node m has its location lm, state sm, and time instant tm. Black solid arrows indicate the possible transitions in the same state. Red dashed arrows indicate the possible transitions between different states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Sampled qualitative results of our proposed method on TIO dataset and People-Car dataset. Each color represents an object. The solid bounding box means the visible object. The dash bounding box denotes the object is contained by other scene entities. Best viewed in color and zoom in.</figDesc><graphic coords="8,370.33,196.60,100.64,60.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. Sampled failure cases. When people stay behind vehicles, it is hard to determine whether or not they are interacting with the vehicle, e.g., entering, exiting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>KSP-fixed<ref type="bibr" target="#b3">[4]</ref> KSP-free<ref type="bibr" target="#b3">[4]</ref> KSP-seq<ref type="bibr" target="#b3">[4]</ref> TIF-LP<ref type="bibr" target="#b34">[35]</ref> TIF-MIP<ref type="bibr" target="#b34">[35]</ref> Quantitative results and comparisons of false positive (FP) rate, false negative (FN) rate and identity switches (IDS) rate on People-Car Dataset. The best scores are marked in bold.</figDesc><table><row><cell></cell><cell>FP ↓</cell><cell>0.17</cell><cell cols="2">0.34 0.20</cell><cell>0.06</cell><cell>0.04</cell><cell>0.05</cell><cell>0.05</cell><cell>0.46</cell><cell>0.10</cell><cell>0.46</cell><cell>0.07</cell><cell>0.07</cell></row><row><cell>Seq.0</cell><cell>FN ↓ IDS ↓</cell><cell>0.08 0.05</cell><cell cols="2">0.53 0.12 0.07 0.05</cell><cell>0.47 -</cell><cell>0.76 0.04</cell><cell>0.48 0.06</cell><cell>0.53 0.06</cell><cell>0.61 0.07</cell><cell>0.41 0.07</cell><cell>0.61 0.07</cell><cell>0.25 0.04</cell><cell>0.25 0.04</cell></row><row><cell></cell><cell cols="2">MODA ↑ 0.71</cell><cell cols="2">0.27 0.63</cell><cell>0.47</cell><cell>0.20</cell><cell>0.47</cell><cell>0.42</cell><cell>-0.07</cell><cell>0.49</cell><cell>-0.07</cell><cell>0.67</cell><cell>0.67</cell></row><row><cell></cell><cell>FP ↓</cell><cell>0.21</cell><cell cols="2">0.70 0.28</cell><cell>0.98</cell><cell>0.75</cell><cell>0.77</cell><cell>0.75</cell><cell>0.77</cell><cell>0.71</cell><cell>0.75</cell><cell>0.17</cell><cell>0.17</cell></row><row><cell>Seq.1</cell><cell>FN ↓ IDS ↓</cell><cell>0.12 0.04</cell><cell cols="2">0.26 0.14 0.13 0.04</cell><cell>0.23 -</cell><cell>0.25 0.12</cell><cell>0.21 0.17</cell><cell>0.25 0.21</cell><cell>0.25 0.06</cell><cell>0.25 0.12</cell><cell>0.25 0.15</cell><cell>0.25 0.04</cell><cell>0.25 0.04</cell></row><row><cell></cell><cell cols="2">MODA ↑ 0.62</cell><cell cols="2">0.09 0.54</cell><cell>-0.21</cell><cell>0.00</cell><cell>0.02</cell><cell>0.00</cell><cell>-0.02</cell><cell>0.04</cell><cell>0.00</cell><cell>0.58</cell><cell>0.58</cell></row><row><cell></cell><cell>FP ↓</cell><cell>0.03</cell><cell cols="2">0.05 0.04</cell><cell>0.03</cell><cell>0.00</cell><cell>0.03</cell><cell>0.00</cell><cell>0.05</cell><cell>0.00</cell><cell>0.05</cell><cell>0.03</cell><cell>0.03</cell></row><row><cell>Seq.2</cell><cell>FN ↓ IDS ↓</cell><cell>0.28 0.01</cell><cell cols="2">0.58 0.32 0.03 0.02</cell><cell>0.47 -</cell><cell>0.59 0.01</cell><cell>0.62 0.02</cell><cell>0.58 0.01</cell><cell>0.72 0.03</cell><cell>0.59 0.01</cell><cell>0.72 0.03</cell><cell>0.47 0.01</cell><cell>0.47 0.01</cell></row><row><cell></cell><cell cols="2">MODA ↑ 0.57</cell><cell cols="2">0.39 0.48</cell><cell>0.50</cell><cell>0.41</cell><cell>0.35</cell><cell>0.42</cell><cell>0.23</cell><cell>0.41</cell><cell>0.23</cell><cell>0.50</cell><cell>0.50</cell></row><row><cell></cell><cell>FP ↓</cell><cell>0.18</cell><cell cols="2">0.39 0.21</cell><cell>0.59</cell><cell>0.35</cell><cell>0.43</cell><cell>0.27</cell><cell>0.46</cell><cell>0.43</cell><cell>0.43</cell><cell>0.14</cell><cell>0.14</cell></row><row><cell>Seq.3</cell><cell>FN ↓</cell><cell>0.07</cell><cell cols="2">0.32 0.10</cell><cell>0.17</cell><cell>0.31</cell><cell>0.23</cell><cell>0.40</cell><cell>0.19</cell><cell>0.23</cell><cell>0.19</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell></cell><cell>IDS ↓</cell><cell>0.06</cell><cell cols="2">0.26 0.06</cell><cell>-</cell><cell>0.27</cell><cell>0.34</cell><cell>0.33</cell><cell>0.19</cell><cell>0.25</cell><cell>0.21</cell><cell>0.07</cell><cell>0.05</cell></row><row><cell></cell><cell cols="2">MODA ↑ 0.68</cell><cell cols="2">0.35 0.62</cell><cell>0.24</cell><cell>0.34</cell><cell>0.34</cell><cell>0.33</cell><cell>0.35</cell><cell>0.34</cell><cell>0.38</cell><cell>0.65</cell><cell>0.65</cell></row><row><cell></cell><cell>FP ↓</cell><cell>0.16</cell><cell cols="2">0.27 0.18</cell><cell>0.40</cell><cell>0.19</cell><cell>0.26</cell><cell>0.13</cell><cell>0.32</cell><cell>0.25</cell><cell>0.31</cell><cell>0.08</cell><cell>0.07</cell></row><row><cell>Seq.4</cell><cell>FN ↓</cell><cell>0.10</cell><cell cols="2">0.18 0.13</cell><cell>0.15</cell><cell>0.19</cell><cell>0.16</cell><cell>0.18</cell><cell>0.17</cell><cell>0.17</cell><cell>0.16</cell><cell>0.16</cell><cell>0.15</cell></row><row><cell></cell><cell>IDS ↓</cell><cell>0.05</cell><cell cols="2">0.15 0.05</cell><cell>-</cell><cell>0.14</cell><cell>0.13</cell><cell>0.15</cell><cell>0.12</cell><cell>0.12</cell><cell>0.11</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell></cell><cell cols="2">MODA ↑ 0.82</cell><cell cols="2">0.59 0.73</cell><cell>0.45</cell><cell>0.62</cell><cell>0.58</cell><cell>0.69</cell><cell>0.51</cell><cell>0.58</cell><cell>0.53</cell><cell>0.76</cell><cell>0.78</cell></row><row><cell>Plaza</cell><cell cols="7">MOTA ↑ MOTP ↑ FP ↓ FN ↓ IDS ↓ Frag ↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-full</cell><cell></cell><cell>46.0%</cell><cell>76.4%</cell><cell>99</cell><cell>501</cell><cell>5</cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-1</cell><cell></cell><cell>31.9%</cell><cell>75.1%</cell><cell>40</cell><cell>643</cell><cell>29</cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-2</cell><cell></cell><cell>32.5%</cell><cell>75.3%</cell><cell>75</cell><cell>605</cell><cell>25</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MHT D [20]</cell><cell>34.3%</cell><cell>73.8%</cell><cell>56</cell><cell>661</cell><cell>15</cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDP [38]</cell><cell></cell><cell>32.9%</cell><cell>73.2%</cell><cell>24</cell><cell>656</cell><cell>9</cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DCEM [26]</cell><cell>32.3%</cell><cell>76.5%</cell><cell>2</cell><cell>675</cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSP [29]</cell><cell></cell><cell>31.7%</cell><cell>72.1%</cell><cell>19</cell><cell>678</cell><cell>21</cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCO [1]</cell><cell></cell><cell>29.5%</cell><cell>76.4%</cell><cell>22</cell><cell>673</cell><cell>6</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">JPDA m [18]</cell><cell>13.5%</cell><cell>72.2%</cell><cell cols="2">163 673</cell><cell>6</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ParkingLot</cell><cell cols="7">MOTA ↑ MOTP ↑ FP ↓ FN ↓ IDS ↓ Frag ↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-full</cell><cell></cell><cell>38.6%</cell><cell cols="3">78.6% 418 1954</cell><cell>6</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-1</cell><cell></cell><cell>28.7%</cell><cell>78.4%</cell><cell cols="2">451 2269</cell><cell>15</cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our-2</cell><cell></cell><cell>28.9%</cell><cell>78.4%</cell><cell cols="2">544 2203</cell><cell>14</cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDP [38]</cell><cell></cell><cell>30.1%</cell><cell>76.4%</cell><cell cols="2">397 2296</cell><cell>26</cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DCEM [26]</cell><cell>29.4%</cell><cell>77.5%</cell><cell cols="2">383 2346</cell><cell>16</cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSP [29]</cell><cell></cell><cell>28.9%</cell><cell>75.0%</cell><cell cols="2">416 2337</cell><cell>12</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MHT D [20]</cell><cell>25.6%</cell><cell>75.7%</cell><cell cols="2">720 2170</cell><cell>15</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCO [1]</cell><cell></cell><cell>24.3%</cell><cell>78.1%</cell><cell cols="2">536 2367</cell><cell>38</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">JPDA m [18]</cell><cell>12.3%</cell><cell cols="3">74.2% 1173 2263</cell><cell>28</cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results and comparisons of false positive (FP), false negative (FN), identity switches (IDS), and fragments (Frag) on TIO dataset. The best scores are marked in bold.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This dataset is available at cvlab.epfl.ch/research/surv/ interacting-objects</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported by <rs type="funder">ONR</rs> <rs type="projectName">MURI</rs> Project <rs type="grantNumber">N00014-16-1-2007</rs>, <rs type="funder">DARPA XAI</rs> Award <rs type="grantNumber">N66001-17-2-4029</rs>, and <rs type="grantNumber">NSF IIS 1423305</rs>, <rs type="grantNumber">1657600</rs>, and in part by <rs type="funder">National Natural Science Foundation of China</rs>: <rs type="grantNumber">61572465</rs>, 61390510, 61732007. The correspondence author is <rs type="person">Xiaobai Liu</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Q8pPE5u">
					<idno type="grant-number">N00014-16-1-2007</idno>
					<orgName type="project" subtype="full">MURI</orgName>
				</org>
				<org type="funding" xml:id="_jFUtnqc">
					<idno type="grant-number">N66001-17-2-4029</idno>
				</org>
				<org type="funding" xml:id="_eNtfNzv">
					<idno type="grant-number">NSF IIS 1423305</idno>
				</org>
				<org type="funding" xml:id="_F2uKnFf">
					<idno type="grant-number">1657600</idno>
				</org>
				<org type="funding" xml:id="_Gv7pn6s">
					<idno type="grant-number">61572465</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discretecontinuous optimization for multi-target tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking people and their objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified framework for multitarget tracking and collective activity recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gmmcptracker:globally optimal generalized maximum multi clique problem for multiple object tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Target identityaware network flow for online multiple target tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recogntion</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization for tracking with continuous deep qlearning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion-aware real-time object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="763" to="771" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pets2009: Dataset and challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning perceptual causality from video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multicamera people tracking with a probabilistic occupancy map</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lengagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure and strength in causal induction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="384" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Two proposals for causal grammars. Causal learning: Psychology, philosophy, and computation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="323" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint probabilistic data association revisited</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boonstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korzhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="336" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing car fluents from video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What are where: Inferring containment relations from videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking sports players with context-conditioned motion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What players do with the ball: A physically constrained interaction modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maksai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-target tracking by discrete-continuous energy minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2054" to="2068" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Commonsense reasoning: An event calculus based approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Causality: Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Globallyoptimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cern: Confidenceenergy recurrent network for group activity recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attentive fashion grammar network for fashion landmark detection and clothing category classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tracking interacting objects optimally using integer programming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tracking interacting objects using intertwined flows</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2312" to="2326" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiple target tracking based on undirected hierarchical relation hypergraph</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recogntion</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human reidentification by matching compositional template with cluster sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-view people tracking via hierarchical trajectory composition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-view people tracking by scene-centered spatio-temporal parsing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The solution path algorithm for identity-aware multi-object tracking</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recogntion</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
