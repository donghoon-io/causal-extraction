<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Fourier Analysis on Directed Acyclic Graphs and Posets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-09">9 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Bastian</forename><surname>Seifert</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Chris</forename><surname>Wendler</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Markus</forename><surname>Püschel</surname></persName>
						</author>
						<title level="a" type="main">Causal Fourier Analysis on Directed Acyclic Graphs and Posets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-09">9 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.07970v3[eess.SP]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph signal processing</term>
					<term>DAG</term>
					<term>partial order</term>
					<term>causality</term>
					<term>structural equation model</term>
					<term>Moebius inversion</term>
					<term>Fourier transform</term>
					<term>convolution</term>
					<term>non-Euclidean</term>
					<term>Fourier sparsity</term>
					<term>dynamic graph</term>
					<term>infection spreading</term>
					<term>binary classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel form of Fourier analysis, and associated signal processing concepts, for signals (or data) indexed by edge-weighted directed acyclic graphs (DAGs). This means that our Fourier basis yields an eigendecomposition of a suitable notion of shift and convolution operators that we define. DAGs are the common model to capture causal relationships between data values and in this case our proposed Fourier analysis relates data with its causes under a linearity assumption that we define. The definition of the Fourier transform requires the transitive closure of the weighted DAG for which several forms are possible depending on the interpretation of the edge weights. Examples include level of influence, distance, or pollution distribution. Our framework is specific to DAGs and leverages, and extends, the classical theory of Moebius inversion from combinatorics. For a prototypical application we consider the reconstruction of signals from samples assuming Fourier-sparsity, i.e., few causes. In particular, we consider DAGs modeling dynamic networks in which edges change over time. We model the spread of an infection on such a DAG obtained from real-world contact tracing data and learn the infection signal from samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Causality studies which events influence others building on powerful classical theories including Bayesian networks and structural causal models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. However, understanding and deriving causality from data continues to be a challenging problem in data science and machine learning <ref type="bibr" target="#b2">[3]</ref>. The common index domains for causal data are directed acyclic graphs (DAGs), in which the nodes represent events and the directed edges causal relationships. Motivated by their importance, we propose a novel form of Fourier analysis for signals (or data) on DAGs, including associated signal processing (SP) concepts of shift, convolution, spectrum, frequency response, and others. DAGs are closely related to partially ordered sets (posets), where the partial order determines whether a node is a predecessor of another node. Thus, our SP framework can equivalently be considered for signals on posets.</p><p>Our framework is specific for DAGs and fundamentally different from prior graph SP based on Laplacian or adjacency matrix <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which fails for DAGs due to a collapsing spectrum. The causal nature of our framework is reflected in both shift and associated Fourier transform as will become clear later. Before we state our contribution in greater detail we</p><p>The authors are with the Department of Computer Science, ETH Zurich, Switzerland (email: seifert.bastian@protonmail.com, chris.wendler@inf.ethz.ch, pueschel@inf.ethz.ch )</p><p>Manuscript received ???; revised ???</p><p>provide the context of prior work. A more detailed discussion of related work is provided later in Section V. Graph signal processing. Prior graph SP uses the eigenbasis of adjacency matrix or Laplacian as Fourier basis and to define related concepts <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>. For undirected graphs both exist and are even orthogonal. For directed graphs (digraphs) this is not the case, the more general Jordan normal form is not computable, and thus a proper digraph SP was still considered an open problem in [6, Sec. III.A] despite various applications <ref type="bibr" target="#b6">[7]</ref>. Several solutions have been proposed, mostly based on a form of approximation or relaxation of requirements <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>DAGs constitute a worst case in digraph SP since they are associated with triangular Laplacian or adjacency matrices. In particular the latter have only one eigenvalue zero and thus never an eigenbasis. The lack of a proper form of DAG Fourier analysis prevents the application of SP methods to causal data indexed by DAGs.</p><p>Causality. Classical models for causality include Bayesian networks <ref type="bibr" target="#b0">[1]</ref>, which encode multivariate probability distributions and enable different forms of causal reasoning. Structural causal models (SCMs), also called structural equation models (SEMs) <ref type="bibr" target="#b1">[2]</ref>, define how variables associated with DAG nodes are computed from parent nodes. Both approaches are probabilistic and model data as random vectors on DAGs, in which the edges represent causal dependencies. Despite powerful theory, learning causality from data is hard with many pitfalls <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b2">[3]</ref>. In particular, causal data typically has a DAG as index domain, but the converse does not hold: if data is given on a DAG, its edges do not generally imply causal relationships due to possible hidden confounding variables, and detecting causality does require additional techniques such as interventions <ref type="bibr" target="#b1">[2]</ref>. One important line of work in causal reasoning addresses the problem of learning the DAG from observed data <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>Our proposed Fourier analysis could bring a novel view point and SP-inspired tools to the analysis of causal data. As a first step, we have tackled a novel variant of DAG learning <ref type="bibr" target="#b17">[18]</ref>, based on the assumption of sparsity in the Fourier domain.</p><p>Contribution. We present a novel form of Fourier analysis, and associated basic SP concepts, for signals (or data) indexed by the nodes of a weighted DAG, extending and completing our preliminary work in <ref type="bibr" target="#b18">[19]</ref>. Our framework can be used on any DAG signal, whether the DAG captures causality or not. However, in the causal case, and under assumptions and in a sense that we define, the causes of a signal become its spectrum on the DAG in our Fourier analysis. Further, our framework can be related to the special class of linear SEMs, providing a novel Fourier-perspective.</p><p>In contrast to prior graph SP, our work leverages the partial order structure defined by a DAG, which also makes it specific to the acyclic case. The weighted DAG describes how the signal value at a node is determined (or caused in the causal case) by its parent nodes. But, by transitivity, this means that the value is determined by all predecessors. We assume this relation to be linear and thus it is obtained by a suitable form of weighted transitive closure of the DAG <ref type="bibr" target="#b19">[20]</ref>, whose form depends on the meaning of the weights, such as distance, level of influence, or fraction of propagation. Viewed as a matrix, the transitive closure determines the Fourier basis relating a signal to its causes, which become its spectrum. We define an associated notion of shift that operates in the frequency domain by removing causes, and show that the spectrum is partially ordered isomorphic to the DAG.</p><p>Our prototypical experiments consider the reconstruction of DAG signals from samples under the assumption of sparsity in the Fourier domain. We show a synthetic experiment as proof of concept. Then we consider one possible application domain for our work: dynamic networks whose edges change over time, which can be modeled as DAGs by unrolling the time dimension and connecting subsequent iterations of the graphs accordingly. We model the spread of a disease on such a DAG, derived from real-world contact tracing data. Then we learn the infection signal from samples. Our causal Fourier basis yields superior results when compared to prior graph Fourier bases, which require dropping the directionality of the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DAGS AND POSETS</head><p>We explain the necessary background on directed acyclic graphs (DAGs), partially ordered sets (posets), and their close relationship. Sets and graphs are denoted with calligraphic letters, matrices in upper case, vectors in bold lower case, and scalars in lower case.</p><p>DAGs. A directed graph (digraph) D = (V, E) consists of a finite set V of n nodes and a set E of m directed edges: E ⊆ {(y, x) | x, y ∈ V}. D is acyclic, and thus a DAG, if it contains no cycles. Since D is acyclic, we can sort V topologically, which means (y, x) ∈ E implies that x comes after y. We consider weighted DAGs (V, E, A), in which each edge (y, x) is assigned a nonzero (not necessarily positive) weight a x,y . These are collected in the matrix</p><formula xml:id="formula_0">A = (a x,y ) x,y∈V = a x,y if (y, x) ∈ E, 0 otherwise. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The topological sort makes A lower triangular with zeros on the diagonal. If all weights are = 1, A is just the adjacency matrix.</p><p>Posets. A partially ordered set (poset) <ref type="bibr" target="#b20">[21]</ref> is a finite set P with a partial order, i.e., a binary relation ≤ that satisfies for all x, y, z ∈ P 1) x ≤ x (reflexivity), 2) y ≤ x and x ≤ y implies x = y (antisymmetry), 3) z ≤ y and y ≤ x implies z ≤ x (transitivity). We write y &lt; x if y ≤ x but y ̸ = x. An element x ∈ P covers y ∈ P if y &lt; x and there is no z ∈ P in-between, i.e., with y &lt; z &lt; x <ref type="bibr" target="#b21">[22]</ref>. Relation between DAGs and Posets. Every DAG D = (V, E) induces a unique partial order on V, defined as y &lt; x if y is a predecessor of x, i.e., if there is a path from y to x.</p><p>Conversely, for a given poset P there are several DAGs that induce it, but two are special and unique. One is the reachability graph D = (P, E) with E = {(y, x) | y &lt; x}. This DAG is transitively closed, i.e., whenever there is a path from y to x there is also an edge (y, x). We mark this property with an overline. The other unique DAG inducing P is the cover graph D = (P, E) with E = {(y, x) | x covers y}. This graph is transitively reduced, i.e., the DAG with the fewest number of edges inducing P. It contains no edge (y, x) if there is another path from y to x in D.</p><p>Example. In Fig. <ref type="figure" target="#fig_0">1</ref> we show an example of a DAG together with its transitive reduction and its transitive closure. All three DAGs induce the same poset.</p><p>In summary, every DAG uniquely defines a poset, whereas a poset can be represented by several DAGs. These however, have the same transitive reduction or transitive closure. Next we will build our signal model, which will require the computation of a transitive closure that also takes the weights into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SIGNAL MODEL AND WEIGHTED TRANSITIVE CLOSURES</head><p>We define the signal model that underlies our proposed Fourier analysis and the associated related SP concepts that we derive in the following section. The key aspect here is that the model, and thus the associated Fourier analysis, is not uniquely determined by the given weighted DAG D but also requires a choice of weighted transitive closure that captures long-distance influences in D, i.e., for y ≤ x where y is not a parent of x. We motivate the need for transitive closure and discuss relevant choices that depend on the meaning of the edge weights in D, leveraging the theory in <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic signal model</head><p>We assume a given weighted DAG D = (V, E, A) with induced partial order ≤ on V, |V | = n. We consider signals on D as column vectors of the form</p><formula xml:id="formula_2">s = (s x ) x∈V ∈ R n ,</formula><p>where the order of the s x is determined by the chosen topological sort of V.</p><p>We call each x ∈ V an event and say that an event y is a cause of the event x if y ≤ x. Further, we assume that every event y ∈ V is associated with an unknown contribution (or input) c y ∈ R to the DAG and that the (measured) signal value s x at x ∈ V is given by the weighted sum of the c y over all causes y:</p><formula xml:id="formula_3">s x = y≤x w x,y c y , x ∈ V.<label>(2)</label></formula><p>By (slight) abuse of notation we also say that c y , for y ≤ x, is a cause of s x .</p><p>Intuitively, the weights in (2) determine the influence of the causes of x on x. Collecting the w x,y in a matrix yields the equivalent form</p><formula xml:id="formula_4">s = W c,<label>(3)</label></formula><p>where W is lower triangular and its nonzero entries correspond to edges in the reachability graph associated with D (e.g., Fig. <ref type="figure" target="#fig_0">1(c)</ref>).</p><p>Example and motivation. As a simple, high level example (that we also used in the follow-up work <ref type="bibr" target="#b17">[18]</ref>), consider a river network, which is a DAG D since water only flows downstream. The nodes x correspond to a set of fixed locations (e.g., cities). We assume that each node y inserts an unknown amount c y of pollution, and that s x is the pollution measured at x, accumulated from all predecessors. The weight a x,y in D could quantify what fraction of pollution at y reaches a direct successor x.</p><p>The measured pollution s x is determined by the c y of all causes y ≤ x of x (not just the parents of x) and w x,y in (2) would capture their relative contribution. As we will explain below, W is obtained from A through a weighted transitive closure. Its exact form will depend on the meaning of the edge weights a x,y .</p><p>On the use of the term "cause". We use the term cause since we believe it helps with understanding our model. However, as already mentioned in the introduction, we want to stress again that (2) does not imply causality but could just express a linear relation, excluding hidden, confounding variables. Thus, strictly speaking, the term "cause" for the c y is not correct in this case. We still use it to emphasize that if (2) is a causal relationship, it does relate signal values and causes, and since it helps with understanding the different forms of transitive closure discussed next. In any event, our Fourier analysis and entire framework is applicable to any signal on any DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weighted Transitive Closure</head><p>The DAG D and its edge weights captures how each node x is influenced by its parents. But, as we also saw in the river network example, if z is a parent of x and y is a parent of z, then, by transitivity, also y will influence x. The associated weight w x,y will depend on the meaning of the edge weights in D. We build on the theory in <ref type="bibr" target="#b19">[20]</ref>.</p><p>As a simple example, consider the transitive closure of the very small DAG in Fig. <ref type="figure">2</ref>, i.e., the computation of w x,y . In the river network example, fractions would multiply, i.e., w x,y = a z,y a x,z . If the weights denoted distance, w x,y = a z,y + a x,z , Fig. <ref type="figure">2:</ref> The weighted transitive closure problem for a simple DAG with three nodes and two edges. if they denoted throughput, w x,y = min(a z,y , a x,z ), and so on.</p><p>Next, we consider these and other choices that can be used to define w x,y for all y &lt; x, given D. This is equivalent to computing a weighted transitive closure D = (V, E, A) of D = (V, E, A) and setting w x,y = a x,y for y &lt; x. (V, E) is the reachability graph defining the same partial order as (V, E). Then we will define w x,x to obtain the entire matrix W in <ref type="bibr" target="#b2">(3)</ref>.</p><p>Boolean weights: Standard transitive closure. If A is the adjacency matrix, i.e., binary with all nonzero weights = 1, one obvious choice is to give all edges in the transitive closure the weight 1 as well, i.e., set A as the adjacency matrix of D.</p><p>Pollution. As in the example of the last section, the weights in A could encode what fraction of a potential pollutant inserted at a node y arrives at a direct successor x. Thus the weights are in [0, 1] and, for each node, the sum of the weights of outgoing edges should be ≤ 1. The transitive closure A would then contain the same information, but now for each pair (y, x) of nodes connected by a path. The fractions multiply along paths and have to be summed over all paths to obtain the result.</p><p>In this case there is a known formula <ref type="bibr" target="#b19">[20]</ref>, which uses that A n = 0:</p><formula xml:id="formula_5">1 A = A + A 2 + • • • + A n-1 = (I n -A) -1 -I n .<label>(4)</label></formula><p>Reliability/influence. The weights in A could encode reliability or influence factors in [0, 1], where a y,x = 1 means 100% reliability of the edge or influence of y on x and a y,x = 0 means none. Along paths, these influences multiply and a y,x could encode the most reliable/influential path from y to x.</p><p>Shortest path. The weights in A could encode distances in R + between nodes. In this case, the weights a y,x in A could be defined as the shortest path from y to x in D. Since one would assume causes y that are farther from x in D to have less influence, one could consider derivatives of a path length ℓ such as 1/ℓ or e -ℓ . The latter choice effectively converts distances to influences in the sense discussed right above.</p><p>Maximal capacity. The weights in A could encode capacity or throughput ∈ R + of edges. The capacity of a path is determined by the minimal capacity among its edges and a y,x could encode the maximal capacity path between y and x.</p><p>Computation. Various algorithms are available to compute transitive closures and their associated weights. The special cases that we just presented can be solved with one generic algorithm, instantiated in different ways <ref type="bibr" target="#b22">[23]</ref>. We show it in its simplest form in Fig. <ref type="figure" target="#fig_1">3</ref>; an optimized version can be found in <ref type="bibr" target="#b22">[23]</ref>. The algorithm is initialized with the weight matrix</p><formula xml:id="formula_6">A S u ⊕ v u ⊙ v 0 S 1 S Meaning of edge weight ax,y in closure {0, 1} u or v u and v 0 1 x is reachable from y, i.e., y ≤ x [0, 1] u + v u • v 0 1 Fraction of pollution from y reaching x [0, 1] max(u, v) u • v 0 1 Strongest influence/most reliable path from y to x R + ∪ {∞} min(u, v) u + v ∞ 0 Shortest path length from y to x R + ∪ {∞} max(u, v) min(u, v) 0 ∞</formula><p>Largest capacity path from y to x TABLE I: Examples of choices for semiring operations ⊕, ⊗ when operating on edge weights in the algorithm of Fig. <ref type="figure" target="#fig_1">3</ref> and the associated meaning of the edge weights. For the pollution interpretation in the second row, the weights of outgoing edges have to sum to ≤ 1. The table is adapted from <ref type="bibr" target="#b22">[23]</ref>.</p><formula xml:id="formula_7">function WEIGHTEDTRANSITIVECLOSURE(A) H (0) ← A for k = 1, . . . , n do for i = 1, . . . , n do for j = 1, . . . , n do h (k) i,j ← h (k-1) i,j ⊕ (h (k-1) i,k ⊙ h (k-1) k,j</formula><p>) end for end for end for return A = H (n) end function    <ref type="table">I</ref>. If these operations satisfy the semiring property, the algorithm in Fig. <ref type="figure" target="#fig_1">3</ref> works.</p><p>on which it performs n 3 iterations for a total runtime of O(n 3 ). It is generic in the choice of addition ⊕ and multiplication ⊗ used, which must satisfy a semiring property <ref type="foot" target="#foot_1">2</ref> .</p><p>Table <ref type="table">I</ref> shows several choices of semirings S and the associated result of the algorithm. Fig. <ref type="figure" target="#fig_3">4</ref> provides intuition: ⊗ determines how consecutive weights are combined (e.g., product for reliability, sum for path length), and ⊕ how weights of alternative paths are combined (e.g., sum for pollution, min for shortest path length). If these two operations satisfy the semiring property, then the algorithm in Fig. <ref type="figure" target="#fig_1">3</ref> works.</p><p>Note that with ⊕ and ⊗ also the definition of 0 (identity element for addition) and 1 (identity element for multiplication) changes as shown in the table. E.g., for shortest path,</p><formula xml:id="formula_8">0 S = ∞ since u ⊕ ∞ = min(u, ∞) = u = ∞ ⊕ u.</formula><p>Thus, in the algorithm, zeros in A have to be replaced with ∞ upon initialization.</p><p>For shortest path, the algorithm is equivalent to the classical Floyd-Warshall algorithm <ref type="bibr" target="#b23">[24]</ref>.</p><p>Other choices of weighted transitive closure may require other algorithms. E.g., overall capacity between two nodes requires max-flow algorithms <ref type="bibr" target="#b24">[25]</ref>. The related structural equation models (discussed later in Section V) use the pollution model but without constraints on the weights.</p><p>Examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reflexive closure</head><p>Finally, we need to define w x,x in (2), which accounts for reflexivity in the partial order, i.e., the fact that c x is a cause of s x . Our model requires w x,x ̸ = 0, since later we want the triangular matrix W in (3) to be invertible. In this paper we focus on the choice w x,x = 1. For the special case of pollution, we obtain a closed form using (4):</p><formula xml:id="formula_9">W = A + I n = (I n -A) -1 .</formula><p>(</p><formula xml:id="formula_10">)<label>5</label></formula><p>In other cases, W takes a different forms as computed by the algorithm in Fig. algo:ModifiedFloydWarshall.</p><p>In three of the fives cases in Table <ref type="table">I</ref> the choice of 1 coincides with 1 S (fraction of pollution or influence of a node on itself is = 1), but poses a problem for the others. For shortest paths this suggests, for example, and as we also do later, converting path length d to exponential decay e -d , which makes it a particular influence model, effectively converting the operations (min, +) to (max, •). For capacity one could choose e -1/c for capacity c.</p><p>Summary. Both A and A are lower triangular matrices with zeros on the diagonal and thus as the only eigenvalue. W = I n + A is lower triangular with ones on the diagonal and thus of full rank, i.e., its columns form a basis, which, in fact, will become our proposed Fourier basis as explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CAUSAL FOURIER ANALYSIS ON DAGS</head><p>Given a weighted DAG D = (V, E, A) with associated partial order ≤, we assume we have decided on a suitable transitive/reflective closure W of A as explained in Section III. We restate our signal model, which assumes that a signal on D is a linear combination of unknown causes associated with the nodes: In this section we will build on this equation to develop a linear SP framework for signals on DAGs. In short, we will argue that c can be interpreted as a form of spectrum of s, with W -1 as associated Fourier transform. We do so following the general theory in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>: we define a suitable notion of shift and convolution for which the columns of W form a joint eigenbasis. Our derivations leverage the classical theory of Moebius inversion from combinatorics <ref type="bibr" target="#b21">[22]</ref>, which is concerned with equations on posets of the form in <ref type="bibr" target="#b5">(6)</ref>. We start by inverting <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_11">s x = y≤x w x,y c y or s = W c.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Calculating Causes: Moebius Inversion</head><p>In the case of the standard Boolean transitive closure, i.e., trivial nonzero weights w x,y = 1, the calculation of c from s is provided by the classical Moebius inversion from <ref type="bibr" target="#b21">[22]</ref>. The extension to arbitrary transitive closures and weights needed here is straightforward and provides a formula for W -1 . Here µ w is the weighted Moebius function, recursively defined as</p><formula xml:id="formula_12">µ w (x, x) = 1, for x ∈ V, µ w (x, y) = - x≤z&lt;y w y,z µ w (x, z), for x ̸ = y.</formula><p>We provide a proof in the appendix. W -1 is lower triangular with ones on the diagonal since the same holds for W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Shift to Fourier Transform</head><p>The following definition of the shift operation is a key contribution of this paper. At first glance it is non-obvious but is the one that generalizes our prior SP framework on lattices <ref type="bibr" target="#b27">[28]</ref> to arbitrary posets and weighted DAGs. As we will see, it can be viewed as a form of causal delay, takes an intuitive form in the case of Boolean weights, makes the columns of W the associated Fourier basis, and thus the causes become the spectrum. Once the shifts are defined, the derivation of the remaining basic SP concepts is straightforward <ref type="bibr" target="#b25">[26]</ref>.</p><p>Causal shifts. We first provide the formal shift definition and then interpret it. For every q ∈ V we define a linear shift operator, given by a matrix T q , on s:</p><formula xml:id="formula_13">(T q s) x = y≤x and y≤q w x,y c y for all x ∈ V.<label>(8)</label></formula><p>In words, comparing to (6), the result is the signal with all causes removed, which are not common causes of q and x.</p><p>But to obtain a proper representation of this linear mapping we need to express the right-hand side of (8) as linear combination of signal values, not causes. We do this by replacing c y in (8) using the inversion formula in ( <ref type="formula">7</ref>): </p><formula xml:id="formula_14">(T q s) x =</formula><p>Inspecting <ref type="bibr" target="#b8">(9)</ref> shows that (T q s) x is a linear combination of signal values s z with z ≤ x and z ≤ q (i.e., "earlier" in the DAG order), as visualized in Fig. <ref type="figure" target="#fig_9">6a</ref>. Thus we consider it as a form of "causal delay."</p><p>We consider special cases to motivate the definition. Assume that x and q have a unique greatest lower bound in D denoted with x ∧ q, i.e.: for all y ̸ = x ∧ q with y ≤ x and y ≤ q we have x ∧ q &lt; y (which is the case if the poset is even a lattice <ref type="bibr" target="#b21">[22]</ref>). In this case, (9) simplifies to</p><formula xml:id="formula_16">(T q s) x = y≤x∧q w x,y z≤y µ w (z, y)s z ,<label>(10)</label></formula><p>i.e., it is linear combination of signal values of nodes ≤ x ∧ q. This situation is visualized in Fig. <ref type="figure" target="#fig_9">6b</ref> with e = g ∧ h.</p><p>Assume in addition that W is the Boolean transitive closure of an unweighted DAG given by A, which is the situation in the prior work <ref type="bibr" target="#b27">[28]</ref>. Then, now by specializing ( <ref type="formula" target="#formula_13">8</ref>),  i.e., the causal delay takes its most beautiful form (in Fig. <ref type="figure" target="#fig_9">6c</ref> the result is s e ) and can be conceptually compared to the classical shift of a discrete-time signal by k, which maps s n to s n-k . Equation <ref type="bibr" target="#b7">(8)</ref> shows that all shift matrices T q , q ∈ V, commute since they only affect the range of summation, and that they are idempotent, i.e., T q • T q = T q . Further, <ref type="bibr" target="#b8">(9)</ref> shows that signal values are shifted to linear combination of predecessors; thus the T q are lower triangular and in general not invertible.</p><formula xml:id="formula_17">(T q s) x = y≤x∧q c y = s x∧q ,<label>(11)</label></formula><p>Filters and convolution. The shifts generate the algebra (ring and vector space) of filters <ref type="bibr" target="#b25">[26]</ref>. Since the shifts commute, a filter is a polynomial in the shifts. Since T 2 q = T q , the most general filter, represented as matrix, is q∈V h q T q for h q ∈ R. Thus, for h = (h q ) q∈V ∈ R n , the associated convolution takes the form</p><formula xml:id="formula_18">h * s = q∈V h q T q s. (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>As polynomials in the shifts, filters are shift-invariant, i.e., h * T q s = T q (h * s) for all q ∈ V.</p><p>Fourier basis and transform. The Fourier basis consists of the joint eigenvectors of all T q and thus all filters. Its derivation is simple due the definition of T q . Let s = W c. Equation ( <ref type="formula" target="#formula_13">8</ref>) shows that shifting s by q performs a pointwise multiplication on c. Formally, we can write (8) as</p><formula xml:id="formula_20">T q s = W D q c, D q = diag y∈V (ι {y≤q} ),<label>(13)</label></formula><p>where ι {y≤q} is the indicator function</p><formula xml:id="formula_21">ι {y≤q} = ι {y≤q} (y, q) = 1 if y ≤ q, 0 else. (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>In words, D q removes the causes of any signals that are not also causes of q.</p><p>Replacing s = W c in <ref type="bibr" target="#b12">(13)</ref> shows that for all c ∈ R n we have T q W c = W D q c, and thus</p><formula xml:id="formula_23">T q W = W D q .</formula><p>Since W has full rank, the columns are the desired Fourier basis:</p><p>Theorem 2 (Fourier basis) The columns of W from a simultaneous eigenbasis of all shifts and filters, i.e., the Fourier basis vectors are</p><formula xml:id="formula_24">f y = (w x,y ) x∈V , y ∈ V. (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>The associated Fourier transform is obtained by inversion:</p><p>Theorem 3 (Fourier transform) The Fourier transform associated with the above Fourier basis is given by</p><formula xml:id="formula_26">s = c = W -1 s = F D s,</formula><p>with Fourier transform matrix (Theorem 1)</p><formula xml:id="formula_27">F D = W -1 = [µ w (x, y)ι {x≤y} ] y,x∈V .</formula><p>Frequency response and convolution theorem. Equation <ref type="bibr" target="#b12">(13)</ref> shows that the frequency response of a shift T q is the diagonal of D q , i.e., (ι {y≤q} ) y∈V by <ref type="bibr" target="#b12">(13)</ref>. Thus, for a general filter h corresponding to the matrix H = q∈V h q T q , it is the diagonal of q∈V h q D q .</p><p>Denoting the frequency response of h as h ′ we obtain</p><formula xml:id="formula_28">h ′ y = q≥y h q , y ∈ V,<label>(16)</label></formula><p>and the associated convolution theorem:</p><formula xml:id="formula_29">h * s = h ′ ⊙ s,<label>(17)</label></formula><p>where ⊙ denotes pointwise multiplication. In words, convolution (filtering) in the signal domain is equivalent to pointwise multiplication (by the frequency response of the filter) in the frequency domain. A few things are worth noting.</p><p>The Fourier transform and frequency response are computed differently, which is also the case in graph signal processing and generally due to the different roles of signal and filter space <ref type="bibr" target="#b25">[26]</ref>.</p><p>The frequency response is computed only based on the partial order defined by the DAG, i.e., independent of the weights and the chosen transitive closure.</p><p>Computing h ′ from h is a linear transform with an lower triangular transform matrix with ones on the diagonal. As such it is invertible, which means every frequency response can be achieved with a suitable filter. In particular, the trivial filter with h ′ y = 1, y ∈ V, i.e., H = I n is a suitable linear combination of the T q . The inversion of the frequency response can be done with the dual version of Theorem 1, i.e., in which ≤ is replaced with ≥.</p><p>Fast algorithms. Explicitly computing the Fourier transform matrix F D = W -1 by inverting W requires O(n 3 ) operations. In practice, since the matrix and its inverse are lower triangular, the spectrum s of a signal s can be computed without explicitly constructing F D using a triangular solve of</p><formula xml:id="formula_30">W s = s,<label>(18)</label></formula><p>using O(n 2 ) operations.</p><p>In the case of unweighted (i.e., Boolean weights) DAGs the Fourier transform and its inverse can be computed using O(nk) time and memory, where k is the width of the DAG, i.e., the longest antichain or maximal number of mutually noncomparable elements of the DAG <ref type="bibr" target="#b28">[29]</ref>. For small k this enables the computation for DAGs up to millions of nodes.</p><p>Total variation and frequency ordering. We complete our framework by a suitable definition of frequency ordering.</p><p>Here we follow the high-level idea used for graphs by <ref type="bibr" target="#b4">[5]</ref>, which relates frequency ordering to the shift via total variation (TV). Here, however, we have multiple shifts and thus we consider TV separately for each shift, as common for images (with two shifts: horizontal and vertical translation) and as was done previously in <ref type="bibr" target="#b27">[28]</ref> for meet/join lattices, which thus we generalize here. Definition 1 Let s be a signal on D. We define the variation w.r.t. a shift by q as TV q (s) = ∥s -T q s∥ 2 . The total variation of s is then the vector</p><formula xml:id="formula_31">TV(s) = (TV q (s)) q∈V<label>(19)</label></formula><p>and the sum total variation is the number</p><formula xml:id="formula_32">STV(s) = q∈V TV q (s).<label>(20)</label></formula><p>We next show that the Fourier basis, and thus the spectrum, is partially ordered in a way isomorphic to the partial order induced by D, with low frequencies corresponding to the early nodes in the DAG, i.e., the smallest ones in the induced partial order.</p><p>Theorem 4 We normalize f y to ∥f y ∥ 2 = 1. Then</p><formula xml:id="formula_33">TV(f y ) = (ι {y̸ ≤q} ) q∈V<label>(21)</label></formula><p>and thus</p><formula xml:id="formula_34">STV(f y ) = ∥TV(f y )∥ 1 = |{q ∈ V | y ̸ ≤ q}|. (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>The poset of total variations T = {TV(f y ) | y ∈ V} w.r.t componentwise comparison is isomorphic to the (unweighted) poset induced by D, i.e., x ≤ y if and only if TV(f x ) ≤ TV(f y ). STV provides a topological sort of the frequencies.</p><p>We provide a proof in the appendix. Note that the frequency ordering only depends on the partial order induced by the DAG and not by the chosen weighted transitive closure. Also, the frequency ordering is independent of the choice of the two norms occurring in Theorem 4.</p><p>Next we provide a numerical example for the concepts introduced and then we conclude this section by a discussion of salient aspects of our framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Small Example</head><p>For a small example we consider the DAG D in Fig. <ref type="figure" target="#fig_5">5c</ref>, which is given by</p><formula xml:id="formula_36">A =      0 0 0 0 0 0 0 0 0 0 0 0 0.3 0.2 0 0 0 0 0.7 0.7 0 0 0 0 0 0 1 0.5 0 0 0 0.1 0 0.5 0 0      .</formula><p>Further, we assume the pollution model, i.e., the transitive closure of A is given by Fig. <ref type="figure" target="#fig_5">5d</ref>, which, including the reflexive closure, yields the matrix Fourier transform. The inverse Fourier transform connecting signal and causes (spectrum) is given by s = F -1 D s = W c. Thus the Fourier transform becomes s = F D s = W -1 s. The Fourier transform matrix is given by F D = W -1 :</p><formula xml:id="formula_37">W =      1 0 0 0 0 0 0 1 0 0 0 0 0.3 0.</formula><formula xml:id="formula_38">F =      1 0 0 0 0 0 0 1 0 0 0 0 -0.3 -0.2 1 0 0 0 -0.7 -0.7 0 1 0 0 0 0 -1 -0.5 1 0 0 -0.1 0 -0.5 0 1      . (<label>23</label></formula><formula xml:id="formula_39">)</formula><p>Shifts. As an example, the shift T e is given by the matrix</p><formula xml:id="formula_40">T e =      1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0.1 0 0.5 0 0      . (<label>24</label></formula><formula xml:id="formula_41">)</formula><p>For example,</p><formula xml:id="formula_42">(T e s) f = 0.1s b + 0.5s d<label>(25)</label></formula><p>which is a linear combination of signal values on common predecessors of e and f in D.</p><p>Low-pass filter. In classical discrete-time SP, a basic lowpass filter is constructed by averaging a signal with its shifted version 1  2 (s n + s n-1 ) n∈Z . Analogously, we construct a lowpass filter by summing the trivial shift and all shifts by q, H = I + q∈V T q , and normalize by the largest eigenvalue:</p><formula xml:id="formula_43">H = 1/(|λ max |) H.</formula><p>In our example, the trivial filter I can be written as I = -T d + T e + T f and thus H = T a + T b + T c + 2T e + 2T f with coordinate vector h = 1 6 (1, 1, 1, 0, 2, 2). In matrix form it is</p><formula xml:id="formula_44">H = 1 6     </formula><p>6 0 0 0 0 0 0 6 0 0 0 0 0.9 0.6 3 0 0 0 1.4 1.4 0 4 0 0 1.6 1.3 1 1 2 0 0.7 1.1 0 1 0 2</p><formula xml:id="formula_45">     . (<label>26</label></formula><formula xml:id="formula_46">)</formula><p>The frequency response of this filter is h ′ = (1, 1, 1/2, 2/3, 1/3, 1/3) which shows that indeed higher frequencies (associated with later nodes in the DAG) are attenuated (the highest two by 1/3), while, in this case, the lowest two are maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Relation to structural equation models</head><p>Structural equation models (SEMs), also called structural causal models (SCMs), are an important tool for analyzing causal data <ref type="bibr" target="#b1">[2]</ref>, modeled as a DAG of causally dependent random variables that satisfy functional relationships. For the special class of linear SEMs (e.g. <ref type="bibr" target="#b29">[30]</ref>, which uses them to learn DAGs from data), this relationship is linear.</p><p>Assuming, as before, a weighted DAG D = (V, E, A) with n nodes, a linear SEM is defined as</p><formula xml:id="formula_47">X = AX + N,<label>(27)</label></formula><p>where X = (X 1 , . . . , X n ) T is a random vector and N = (N 1 , . . . , N n ) T a (usually i.i.d.) random noise vector, not necessarily Gaussian. Using (5) we can write <ref type="bibr" target="#b26">(27)</ref> as</p><formula xml:id="formula_48">(I n -A)X = N ⇔ X = W N = F -1 D N,<label>(28)</label></formula><p>if we choose the (+, •)-transitive closure associated with the pollution model (but now without restriction on the weights) to obtain W from A. In words, the noise chosen to sample the linear SEM is then, in our sense, the spectrum of the obtained signal.</p><p>Conversely, assume a weighted DAG D = (V, E, A) with n nodes and an arbitrarily chosen weighted transitive closure W (e.g., from Table <ref type="table">I</ref>), which defines a notion of spectrum in the sense of this paper. Let D ′ = (V, E ′ , A ′ ) be the DAG associated with A ′ = I n -W -1 . Then the linear SEM X = A ′ X + N is such that the noise chosen to sample is the spectrum of the obtained signal.</p><p>The latter allows the interpretation of any weighted transitive closure in the context of linear SEMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND RELATED WORK</head><p>We discuss related work and some of the salient aspects of our causal SP framework for DAGs.</p><p>Comparison to graph SP. Graph SP <ref type="bibr" target="#b5">[6]</ref> is concerned with signals indexed by the nodes of a graph and generalizes classical SP concepts by choosing adjacency matrix or Laplacian, or variants thereof as shift (or variation) operator <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which is known to be the defining concept of any linear SP framework <ref type="bibr" target="#b25">[26]</ref>. For undirected graphs, the eigendecomposition of the shift exists and yields an orthogonal Fourier transform. Other SP concepts and techniques take meaningful forms <ref type="bibr" target="#b5">[6]</ref>. For directed graphs (digraphs) a proper generalization was still considered an open problem in [6, Sec. III.A], since an eigendecomposition does not exist in general and the more general Jordan normal form is not computable. DAGs constitute, in a sense, a worst case among digraphs since the adjacency shift has only one eigenvalue zero.</p><p>Several solutions have been proposed for digraphs. An overview including applications of digraph signal processing can be found in <ref type="bibr" target="#b6">[7]</ref>. One approach computes a Fourier basis that minimizes the sum of directed variations <ref type="bibr" target="#b7">[8]</ref>, or evenly spreads them <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Others include changing the shift to the Hermitian Laplacian <ref type="bibr" target="#b10">[11]</ref>, using an approximation based on the Schur decomposition <ref type="bibr" target="#b11">[12]</ref>, or adding generalized boundary conditions, i.e., additional edges to the digraph <ref type="bibr" target="#b12">[13]</ref>.</p><p>All these are fundamentally different from our approach which is applicable only to acyclic digraphs and based on a very different notion of shift and Fourier basis. The fundamental difference is best captured in the shift definition: graph shifts capture the neighbor structure, whereas our causal shifts capture the partial order structure provided by DAGs, directly manipulating causes, i.e., values of predecessors inserted to the DAG. As a result, all derived concepts differ substantially. In particular, in our framework DAGs first need to be transitively closed (and their are choices) and the exclusive dependency on predecessors motivated by causality makes shifts and Fourier transform triangular.</p><p>Our work can equivalently be interpreted as Fourier analysis for signals on weighted posets, i.e., with a weight assigned to each pair (x, y) with x &lt; y.</p><p>Comparison to lattice SP. Our work substantially generalizes SP on meet/join lattices <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[32]</ref>, which, in turn, generalizes SP with set functions from <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>. These lattices are a special class of posets or DAGs, in which each two elements have a unique greatest lower bound, which yields the concise representation of the shift in <ref type="bibr" target="#b9">(10)</ref>. This paper drops this condition, which makes it applicable to arbitrary posets and thus arbitrary DAGs. Further, and equally important, we allow for non-trivial weights and thus interpretations like distance or influence, which should considerably expand applicability.</p><p>Causality and linear SEMs. Most closely related in causality research are linear SEMs as we formally explained in Section IV-D. Typically, in SEMs, linear or not, the causes of a node are the parents because of the form in <ref type="bibr" target="#b26">(27)</ref>. In this paper we use the term differently, namely for all predecessors of a node motivated by the form in (28). <ref type="foot" target="#foot_2">3</ref>Linear SEMs have been studied in <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>. One question is identifiability of the data distribution, which depends on the distribution of N in <ref type="bibr" target="#b26">(27)</ref>. If N is i.i.d. Gaussian, linear SEMs can express any n-variate distribution with a suitable DAG <ref type="bibr" target="#b38">[38]</ref>. In contrast, motivated by our proposed Fourier analysis, our assumption in the linear SEM experiment later in Section VI-A is that N is approximately sparse with random support.</p><p>An active research area is learning the DAG from data, with various approaches specifically targeting linear SEMs <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b15">[16]</ref>, [?], <ref type="bibr" target="#b16">[17]</ref>. In particular, <ref type="bibr" target="#b15">[16]</ref> captures acyclicity as a continuous constraint to obtain a solvable optimization problem. Our work <ref type="bibr" target="#b17">[18]</ref> builds on it but changes the data generation from <ref type="bibr" target="#b26">(27)</ref> to assume sparsity in the Fourier domain. Thus we believe that our has the potential to bring new, SPinspired method to the domain of causal data analytics. We also note that the interpretation of different transitive closures as linear SEMs (Section IV-D) appears to be novel.</p><p>Multiple shifts. Our framework is shift-invariant and based on multiple basic shifts instead of just one in graph SP. This is not uncommon: e.g., filters on images are composed from independent shifts in x and y-direction, so also there the spectrum is partially ordered. Fundamentally, it just means that the filter space is a polynomial algebra in multiple variables <ref type="bibr" target="#b25">[26]</ref>.</p><p>SP on non-Euclidean domains. Besides graph SP, other SP frameworks for non-Euclidean domains have been proposed.</p><p>One line of work is topological SP <ref type="bibr" target="#b40">[40]</ref> based on the Hodge Laplacian, which considers signals defined on simplicial complexes, with values assigned to nodes, edges, or higher-order faces. The framework was generalized to cell complexes in <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>.</p><p>Hypergraphs generalize graphs by allowing edges with more than two nodes. A topological approach to hypergraph SP similar to above was proposed in <ref type="bibr" target="#b43">[43]</ref>, whereas <ref type="bibr" target="#b44">[44]</ref> uses the adjacency tensor and tensor decomposition to define a notion of spectrum, sampling theory, and filters.</p><p>Quiver SP <ref type="bibr" target="#b45">[45]</ref> considers directed multigraphs (i.e., multiple directed edges between the same nodes are possible), and develops an SP theory based on the rich representation theory of these structures.</p><p>Graphon SP is a continuous extension of SP on undirected graphs, thus enabling sampling among other things <ref type="bibr" target="#b46">[46]</ref>.</p><p>SP on lattices and powersets <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[34]</ref> are direct predecessors of our work as already explained above and our first attempt to go to arbitrary unweighted DAGs was in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Several of the above generalized SP frameworks, and the work in this paper, build on the algebraic signal processing theory, which provides the axioms, insights, and derivation guidelines for any linear SP framework <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and was used to consider the first shifts beyond standard translation/delay <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATION: FOURIER SPARSITY</head><p>Our work provides a complete set of basic SP concepts for data on weighted DAGs. Thus, in principle, any SP method that builds on Fourier analysis or filtering can be ported. In this paper we focus on the concept of sparsity in the Fourier domain, which, in the causal setting, has the appealing equivalent interpretation of signals with few causes.</p><p>As an example, consider again the river network with measured pollution data from Section III-A. Fourier sparsity means that only few cities polluted in a data set, a reasonable assumption.</p><p>There are two problems that one can readily associate with Fourier sparsity:</p><p>• Reconstructing a DAG signal from samples under the assumption of Fourier sparsity. • Learning the DAG from DAG signals under the assumption of Fourier sparsity. We proposed a solution, called SparseRC, for the second problem in the follow-up work <ref type="bibr" target="#b17">[18]</ref> for the special case of linear SEMs, i.e., the (+, •)-transitive closure of the pollution model without weight restriction as explained in Section IV-D. The problem had not been considered before. We proved identifiability and showed that SparseRC could successfully learn DAGs up to thousands of nodes under mild assumptions. Further, in the recent CausalBench challenge <ref type="bibr" target="#b50">[50]</ref> on learning gene interactions from single cell data, SparseRC was among the three winning teams <ref type="bibr" target="#b51">[51]</ref>, showing that the assumption of few causes can be relevant in practice.</p><p>In this paper we focus on the first problem of reconstructing a signal from samples. First, as a proof of concept, we consider a synthetic problem on randomly generated graphs, again based on a linear SEM. Then, in the following Section VII, we consider a more realistic semi-synthetic experiment at a much larger scale modeling infection spreading on a dynamic network along time. In that experiment the assumption of Fourier-sparsity is intuitive but not explicitly built into the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning DAG signals from samples</head><p>We generate signals on weighted, random DAGs using a corresponding linear SEM under the assumption of approximate Fourier-sparsity with unknown support w.r.t. our associated Fourier basis. Then we reconstruct the signal from samples, using a Lasso-method <ref type="bibr" target="#b52">[52]</ref> (i.e., linear regression with a sparsity penalty) that is applicably with any basis. We compare against prior graph Fourier bases obtained by dropping directions in the DAGs.</p><p>Random graphs. We construct weighted DAGs D = (V, E, A) with 500 nodes using the Erdős-Rényi model <ref type="bibr" target="#b53">[53]</ref>, where each edge is created with probability p = 0.05 and given a random weight in [-1, 1]. To obtain a DAG, we order the nodes randomly and only keep the edges (y, x) with x ≥ y. We construct 100 DAGs, each one thus with approximately 6000 edges.</p><p>Signal generation. We generate approximately Fouriersparse signals on a random DAG using a linear SEM as described in <ref type="bibr" target="#b17">[18]</ref>. Namely, the inverse Fourier transform F -1 D = W is the (+, •)-transitive closure (Section IV-D) and the data is generated via</p><formula xml:id="formula_49">X = F -1 D (C + N c ) + N x ,<label>(29)</label></formula><p>where C is the sparse spectrum, i.e., the relevant few causes, N c is spectral noise, and N x models the noise in the measurement of X. Here the C + N c term corresponds to the N term in <ref type="bibr" target="#b27">(28)</ref>. Both N c and N x are assumed to be of negligible magnitude compared to C. Specifically, we choose C to be sparse with only 10% nonzero values at random locations In the river network example, C would be the inserted pollution by a city, N c negligible random pollution inserted by all cities, and N x the noise in the pollution measurement.</p><p>Reconstruction method. We reconstruct the generated signal s from samples s xi at nodes x 1 , . . . , x k by fitting a Lasso model. That is, we solve a linear regression with an L 1 -sparsity penalty:</p><formula xml:id="formula_50">min r∈R |V | k i=1 (s xi - y∈V r y f y xi ) 2 + λ∥ r∥ 1 .<label>(30)</label></formula><p>Here f y is the y-th Fourier basis vector <ref type="bibr" target="#b14">(15)</ref>, the linear regression approximates the samples s xi as a signal on the DAG, while the L 1 -penalty promotes sparsity in the Fourier spectrum. With the minimizing r found the obtained reconstructed signal is then r = F -1 D r. Baselines. As baselines we consider other Fourier bases in <ref type="bibr" target="#b29">(30)</ref>. Namely, the graph Fourier bases associated with adjacency and Laplacian matrices, for both the DAG and its transitive closure obtained by dropping the directions in the DAG, i.e., the eigenbases of A + A T and A + A</p><formula xml:id="formula_51">T = W + W T -2I.</formula><p>Further, we also consider our binary DAG Fourier basis obtained by setting all nonzero weights in A to one and computing the Boolean transitive closure.</p><p>Results. Fig. <ref type="figure" target="#fig_12">8</ref> shows the results: the reconstruction error ∥r-s∥/∥s∥ as a function of the fraction of the signal sampled. The shaded areas show the 95% confidence intervals over 100 repetitions.</p><p>As expected, given enough samples, the signal can be well reconstructed with the Fourier basis used in its construction. The other bases fail since the signal is not approximately sparse in their Fourier domain. This also applies to our unweighted DAG basis. In other words, the weights and chosen transitive closure matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. APPLICATION EXAMPLE: DYNAMIC NETWORKS</head><p>We present a more realistic example of learning a DAG signal from samples, again under the assumption of Fourier sparsity, i.e., few causes, but this time this sparsity is not present by construction. As signal domain we consider a class of DAGs that is obtained from graphs that change dynamically with time.</p><p>Examples of real-world (undirected) graphs include proximity of persons (used, e.g., for contact-tracing of infectious people during a pandemic), peer-to-peer networks between vehicles in traffic, or transactions between traders in a market. However, these graphs are often non-static, e.g., the edges change with time. The work in <ref type="bibr" target="#b54">[54]</ref> shows how to encode such a dynamic network as a DAG by assigning the graphs to discrete time steps and connecting subsequent graphs.</p><p>We consider such DAGs as one possible application domain of our work and present in this section a prototypical, semisynthetic example: we use real contact-tracing data to simulate the spread of an infection among n individuals along time. Then we try to reconstruct, or learn, this infection signal from samples under the assumption of sparsity in the Fourier domain. We presented a restricted, simplified version of this experiment using unweighted DAGs in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Infection Spreading on Dynamic DAGs</head><p>We explain the construction of DAGs from dynamically changing graphs and the model we use to generate infection signals from contact tracing data.</p><p>Dynamic networks as DAGs. We consider a dynamic network as a collection of (undirected) graphs G t = (V, E t ) where the set of edges E t changes with time t ∈ T = {t 1 , . . . , t m }. It can be modeled as a DAG D = (V ′ , E ′ ) using the idea from <ref type="bibr" target="#b54">[54]</ref>. Namely, we make a copy of the node set for each time point, i.e., the new node set is V ′ = {(v, t) | v ∈ V, t ∈ T ∪ {t m+1 }}, where t m+1 is an added, last time point.</p><p>Further, we connect nodes (u, t) with (v, t+1) if (u, v) ∈ E t and always (u, t) with (u, t + 1) to form E ′ . This construction is illustrated on a small example in Fig. <ref type="figure">9</ref>.</p><p>The Haslemere data set. We consider the data set from <ref type="bibr" target="#b55">[55]</ref>, which uses real smartphone proximity data to obtain a dynamic network on which the spread of a disease is then modeled and analyzed. Here we aim to learn the associated signal from samples. Concretely, the proximity of |V| = 469 participants was measured for three days every 5 minutes between 7am and 11pm using a smartphone app, resulting in 576 time points. Due to our infection model below we remove all edges with distance &gt; 20 meters.</p><p>Haslemere DAG. With the above construction we turn the Haslemere dynamic network into a DAG D = (V ′ , E ′ ). Since later we want to use standard graph SP as benchmark (with directions dropped), which requires an eigendecomposition of the Laplacian or adjacency matrix, we have to restrain the size of D. Thus, we sample the contact data only every hour, resulting in |T | = 37 time points leading to a DAG with |V ′ | = 17612 nodes and |E ′ | = 24596 edges.</p><p>Weights. At each time point the edges of the graphs G t are weighted by the distance of the participants. We convert the distances into influences as explained in Section III-B to ensure fading with large distances in space and, through the transitive closure, in time. Concretely, DAG edges of the Fig. <ref type="figure">9</ref>: Constructing a DAG from a dynamic network <ref type="bibr" target="#b54">[54]</ref>. form ((u, t), (u, t + 1)) obtain weight 1, and edges of the form ((u, t), (v, t + 1)) obtain weight e -du,v ∈ [0, 1], where d(u, v) is the distance at time t. The transitive closure is then computed using Algorithm 3.</p><p>Infection signals. <ref type="bibr" target="#b55">[55]</ref> uses the susceptible-exposedinfectious (SEI) model to simulate the spread of a disease from a number of initially infected individuals. In this model, a healthy individual is infected with a certain probability when exposed to an infected individual. Here, to make it more realistic, we include recovery and slightly extend it to a susceptible-infected-recovered (SIR) model.</p><p>Formally, from <ref type="bibr" target="#b55">[55]</ref>, the infection force λ u,v (t) from an infected individual (node) u to a non-infected individual (node) v at each time point is modeled using a cutoff exponential</p><formula xml:id="formula_52">λ u,v (t) = e -du,v(t)/ρ if d u,v (t) ≤ ϵ, 0 if d u,v (t) &gt; ϵ,<label>(31)</label></formula><p>where d u,v (t) is the distance between u and v at time t, ρ the characteristic distance set to ρ = 10 meters, and ϵ the cutoff distance set to ϵ = 20 meters. The overall infection force to a node v is then</p><formula xml:id="formula_53">λ v (t) = u infected λ u,v (t),<label>(32)</label></formula><p>and the probability that the individual v gets infected at time t is prob v (t) = 1 -e -λv(t) .</p><p>In our extension, a person which is infected at time t recovers at time t + 5 and is afterwards immune. The time of 5 hours is of course unrealistically short, but this is necessary due to the small number of considered time points. The exact choice is also irrelevant for our prototypical experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Fourier-Sparse Causal Signals</head><p>Using the model from Section VII-A we can generate binary (values are 0 or 1) infection signals on the DAG (V ′ , E ′ ) by starting with a small number of infected individuals at time point one and infecting individuals in subsequent time steps with the probabilities <ref type="bibr" target="#b33">(33)</ref>. Fig. <ref type="figure" target="#fig_14">10</ref> shows one such signal with nine initially infected individuals. Since the connectivity information (i.e., the edges of the DAG) is distracting from the signal (the infected individuals, i.e., the red dots) we will omit the edges in the following plots. Fig. <ref type="figure" target="#fig_14">10</ref> without edges is shown later in Fig. <ref type="figure" target="#fig_19">15g</ref> where it is compared to its reconstructions.    <ref type="figure" target="#fig_14">10</ref>. Now, we the goal is again to learn such a generated infection signal from a number of samples assuming sparsity in the Fourier domain. The approach is in concept similar to the one in Section VI-A, but the details are different. We explain it next and then show results, again comparing to standard graph SP Fourier bases obtained by dropping the direction of the DAG edges so they are well-defined.</p><p>Learning such signals from samples is hard. First, the signals are very sparse; thus a certain number of samples is needed to learn something about the signal at all. Second, the DAG model does not know the data generation process.</p><p>In particular, the fixed recovery time is not known or used. Finally, the data generation process is stochastic in nature and hence no model can be absolute certain about the infection status after exposure and along time.</p><p>Fourier-sparse learning. The basic idea is to approximate a binary infection signal s with τ (σ(r)), where r is Fouriersparse (i.e, r has few nonzero values), σ = 1/(1 + e -x ) is the logistic sigmoid function that converts elementwise real values to probabilities, and τ is a threshold function. In our case, the default τ simply rounds elementwise to 0 or 1.</p><p>Formally, with this approximation, the probability that a node x ∈ V ′ is of class 1 (infected) is then</p><formula xml:id="formula_55">p(x) = prob(x of class 1) = σ(r x ) = σ y∈V r y f y x ,<label>(34)</label></formula><p>with f y x from (15) and r y = 0 for most y ∈ V. We assume we observe k signal values s 1 , . . . , s k at random nodes x 1 , . . . , x k , respectively. We estimate the nonzero Fourier coefficients r y in (34) by solving a logistic regression problem, regularized by an L 1 -loss term to promote sparsity of r <ref type="bibr" target="#b56">[56]</ref>. The resulting optimization problem is given as</p><formula xml:id="formula_56">min r∈R |V | - k i=1 s i log p(x i ) + (1 -s i ) log(1 -p(x i )) + λ∥ r∥ 1 ,<label>(35)</label></formula><p>where λ ≪ 1 is a hyperparameter. We found that λ = 0.1 worked well for all bases.</p><p>Experiment. We generate a set of signals as follows. We start the SIR model with i = 5, 9, 11 participants infected at random at time t 1 and propagate the infections as described in Section VII-A. For each i we repeated the simulation ten times, resulting in 30 DAG signals overall. The signals have value 1 at node x = (u, t) if the individual u is infected at time t and value 0 otherwise.</p><p>We observe a fraction k/|V ′ | of the signal values and then use <ref type="bibr" target="#b35">(35)</ref> with three different notions of Fourier basis to obtain a sparse r, which in turn determines r and thus the prediction τ (σ(r)) of s. For each of the three Fourier bases we consider two variants for a total of six experiments.</p><p>The three bases are our proposed basis and Laplacian/adjacency matrix GSP bases, as in Section VI-A, obtained by dropping direction in the DAG (V ′ , E ′ ) and computing eigenbases. For our basis we consider the weighted version, obtained by the transitive closure in the influence model as explained above, but also an unweighted version, obtained by a standard transitive closure (first row in Table <ref type="table">I</ref>). For the Laplacian/adjacency matrix GSP bases we consider both the graph as is (but undirected) and its transitive closure.</p><p>All results shown are for the 30 considered signals: the respective solid lines show the mean and the shaded areas the 95% confidence interval.</p><p>Evaluation. The first idea is to compute reconstruction accuracy, computed as 1 -||(s -τ (σ(r))|| 2 /||s|| 2 , shown in Fig. <ref type="figure" target="#fig_17">12</ref> (note that the y-axis starts at 0.8, which emphasizes differences). Since the signals are binary and highly imbalanced (way more person-time combinations are non-infected than infected), this metric is not suitable: a trivial estimator setting  every value to "nobody infected" reaches about 0.9, shown as dotted line, but cannot detect any infected node and is thus useless. More generally, binary classifiers of similar such accuracy can have vastly different quality due to differences in the number of false positives.</p><p>Thus, instead, the quality of binary classifiers with imbalanced data is measured in machine learning with the receiver operator characteristic area under curve (ROC-AUC) <ref type="bibr" target="#b57">[57]</ref>.</p><p>The key underlying concept is the ROC that does a cost/benefit analysis <ref type="bibr" target="#b58">[58]</ref>. Namely, the so-called ROC curve measures how the true positive rate or TPR y (the probability of detecting an event, i.e., the benefit) changes with respect to the false positive rate or FPR x (the probability of a false alarm, i.e., the cost) by varying the classification threshold τ . A random classifier that chooses detection with probability p and not detected with probability 1 -p yields the ROC curve y = x if p is varied in [0, 1]. A perfect classifier would approach the curve y = 1. So higher is better and the area under the ROC curve (ROC-AUC) is used as metric. The perfect classifier has AUC 1 and the trivial one ("nobody infected") AUC 0.5. Fig. <ref type="figure" target="#fig_1">13</ref> shows the ROC curves of the considered classifiers obtained with 20% sampled data. The estimation based on our proposed causal Fourier basis performs best by a large margin, compared to both GSP Fourier bases associated with the Laplacian and adjacency matrix. Transitively closing the graphs makes it worse for them. The likely reason is that the for the GSP models the assumption of sparsity in the Fourier domain does not hold, and thus the signal could not be learned from samples using <ref type="bibr" target="#b35">(35)</ref>.</p><p>In contrast, in our Fourier domain sparsity indeed holds as already shown in Fig. <ref type="figure" target="#fig_15">11</ref>. In our terminology this means relatively few causes are responsible for the signal, which the Fourier-sparse reconstruction can leverage. It shows that for the considered signals our combinatorial causal Fourier basis yields a better representation than the more geometric GSP bases.</p><p>It is interesting that both considered DAG Fourier bases perform well in this experiment. There are two possible explanations. First, both provide a model for the data in which approximate Fourier sparsity holds. Second, the weighted model may be conceptually the better fit, but the binary nature The corresponding ROC-AUC curve used to measure the benefit in ROC curves is shown in Fig. <ref type="figure" target="#fig_3">14</ref>. It plots the AUC of the ROC lines as function of the fraction of data points sampled (higher is better). For the sample fraction of 20% the values correspond to Fig. <ref type="figure" target="#fig_1">13</ref>.</p><p>The superiority of our classifier compared to the benchmarks also becomes evident when looking at the reconstructed signals. Fig. <ref type="figure" target="#fig_19">15</ref> shows an example with the original signal shown in Fig. <ref type="figure" target="#fig_19">15g</ref>. We observe that the sparsity in the Laplacian/adjacency matrix-based reconstructions appears random, whereas for our novel DAG-based method the signals have structure as often values along time steps (horizontally) tend to stay constant which captures the causal nature of the infection signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We presented a novel linear SP framework including shift, convolution, and Fourier analysis for signals on DAGs, or, equivalently, posets. Doing so is significant both theoretically and practically. On the fundamental side we fill a blind spot in graph SP, for which digraphs are problematic and DAGs are a worst case. For applications, DAGs are the natural index domain for causal data, in which each data point causally depends on the values of predecessors. We argue that if this causal dependency is linear, and with coefficients obtained by a suitable transitive closure of the DAG, the signal and causes can be viewed as a Fourier pair. If the linear relation is not causal our proposed Fourier analysis is still mathematically sound but the spectrum cannot be interpreted as causes.</p><p>Importantly, our framework allows for edge-weighted DAGs, and, different from other non-Euclidean SP frameworks, there is a degree of freedom in their interpretation and thus the transitive closure needed to obtain a Fourier basis.</p><p>One particular application domain are DAGs obtained from dynamic graphs evolving over discrete time. We show an example of learning infection signals on such a graph from few samples by assuming Fourier-sparsity.</p><p>Overall, our work leverages but also extends the classical theory of Moebius inversion to define a new notion of Fourier analysis for use in signal processing and learning. Thus, the formula for c y in Theorem 1 implies the formula for s x , and since W is invertible, the reverse holds as well.</p><p>Proof of Theorem 4. Using (13), we get T q f y = f y if y ≤ q, 0 otherwise, which also holds after normalization. It follows TV q (f y ) = ∥f y -T q f y ∥ 2 = 1 if y ̸ ≤ q and = 0 otherwise, which yields <ref type="bibr" target="#b20">(21)</ref> and <ref type="bibr" target="#b21">(22)</ref>.</p><p>For the isomorphic partial ordering assume first x ≤ y for x, y ∈ V. Then y ≤ q implies x ≤ q, i.e., x ̸ ≤ q implies y ̸ ≤ q. It follows TV q (f x ) ≤ TV q (f y ).</p><p>For the reverse assume TV(f x ) ≤ TV(f y ), i.e., TV q (f x ) ≤ TV q (f y ) for all q ∈ V. It follows that x ̸ ≤ q implies y ̸ ≤ q, i.e., that y ≤ q implies x ≤ q. Setting q = y yields the result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) A DAG D, (b) its transitive reduction, and (c) its transitive closure D). All three induce the same poset for which (b) is the cover graph, and (c) the reachability graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Generic algorithm to compute various forms of weighted transitive closure of A in O(n 3 ) [23]. The genericity is in the choice of addition ⊕ and multiplication ⊗, which need to satisfy a semiring property. Possible choices and the associated results are shown in TableI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Product: Combing consecutive weights along a path. (b) Sum: Combining weights over alternative paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The purpose of product and sum in TableI. If these operations satisfy the semiring property, the algorithm in Fig.3works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>shows a few examples of transitive closures, using the DAG from Fig. 1(a) as starting point. Note that the transitive closure A may overwrite weights in A. E.g., the bottom edge in Fig. 5e has weight 4.5, but after closure, the shortest path from b to f has length 3.2 = 1.5 + 1.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Example DAG with different weights and meanings of weights (left) and their corresponding transitive closures (right). Modified and added weights and edges are colored blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>y≤x w x,y c y , if and only if c y = x≤y µ w (x, y)s x . (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>y≤x and y≤q w x,y z≤y µ w (z, y)s z .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Generic case (b) Unique largest lower bound e = g ∧ h. b d g a h (c) Unique largest lower bound e = g ∧ h and Boolean weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: s shifted with h at node g (i.e., (T h s) g ). (a) Generic case: linear combination of common predecessors of g and h such that common causes are removed. (b) Special case in which g and h have a unique largest lower bound in the partial order: linear combination of predecessors of e. (c) As in (b) but in addition with Boolean edge weights: (T h s) g = s e .</figDesc><graphic coords="6,128.76,174.01,88.21,69.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Partial ordering of the spectrum of the DAG in Fig. 5c. TV(f y ) is shown next to node y.</figDesc><graphic coords="7,387.74,73.79,80.51,52.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>.</head><label></label><figDesc>Fourier basis and frequency ordering. The columns of W constitute the Fourier basis {f a , . . . , f f } of the DAG. The first two have the lowest frequency w.r.t. the total variation in Definition 1. For example TV(f a ) = (ι {a̸ ≤q} ) q∈V = (0, 1, 0, 0, 0, 0). All TV(f y ) are shown in Fig.7. Note, for example, that TV(f c ) ≤ TV(f e ) but TV(f c ) ̸ ≤ TV(f f ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Relative error of reconstructed signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) A dynamic network (V, Et), where the edges change with time t = t1, t2, t3. (b) Copied graphs with new directed edges. The edges (u, t) → (u, t + 1) are not yet included. (c) The final DAG D = (V ′ , E ′ ); the nodes at each time step are drawn vertically aligned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: One example of a generated binary infection signal with nine initially infected persons. About 9% of the values are = 1 (infected), the others = 0 (not infected).</figDesc><graphic coords="11,343.36,187.28,188.29,141.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Spectrum of the signal in Fig. 10. About 11% of the values are nonzero, several of which are very small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 11</head><label>11</label><figDesc>Fig.11shows the spectrum of the signal (indexed by the same DAG because of Theorem 4, but now with edges omitted) in Fig.10. Now, we the goal is again to learn such a generated infection signal from a number of samples assuming sparsity in the Fourier domain. The approach is in concept similar to the one in Section VI-A, but the details are different. We explain it next and then show results, again comparing to standard graph SP Fourier bases obtained by dropping the direction of the DAG edges so they are well-defined.Learning such signals from samples is hard. First, the signals are very sparse; thus a certain number of samples is needed to learn something about the signal at all. Second, the DAG model does not know the data generation process.</figDesc><graphic coords="11,343.36,385.10,188.29,143.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Standard accuracy in the Euclidean norm is not a good measure for the quality of binary classifiers with imbalanced classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Receiver operation characteristics (ROC) curves for the above sample data and the classifiers with a sample of 20% node data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Examples of reconstructed signals from a sample of 20% of the node data, using the described classifier with τ = 0.5.</figDesc><graphic coords="14,216.04,542.89,179.93,134.95" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The equation follows from the telescoping sum A(In -A) = A -A n = A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See [23, Definition 2.1]. In particular, ⊕ is commutative, ⊕ and ⊗ are associative, have an identity element, and satisfy the distributivity law.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Thus, in our follow-up work<ref type="bibr" target="#b17">[18]</ref> we used the term root causes instead.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Panagiotis Misiakos</rs> for the insight on the relationship between structural equation models and our novel Fourier analysis for DAGs.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>Proof of Theorem 1. The proof of the weighted Moebius inversion <ref type="bibr" target="#b6">(7)</ref> generalizes the Moebius inversion in <ref type="bibr" target="#b21">[22]</ref> and is similar to the proof of [59, Lemma 2.2.1].</p><p>First we show that</p><p>The first case holds since w x,x = 1. For the second case, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Causality for machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="765" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending highdimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph Signal Processing: Overview, Challenges, and Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="828" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signal Processing on Directed Graphs: The Role of Edge Directionality When Processing and Learning From Network Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="99" to="116" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Graph Fourier Transform for Directed Graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Di Lorenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="796" to="811" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digraph Fourier Transform via Spectral Dispersion Minimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shafipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khodabakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, and Signal Process</title>
		<meeting>Int. Conf. Acoust., Speech, and Signal ess</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6284" to="6288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Directed Graph Fourier Transform with Spread Frequency Components</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shafipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khodabakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="946" to="960" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph Signal Processing for Directed Graphs based on the Hermitian Laplacian</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD</title>
		<meeting>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph Fourier Transform: A Stable Approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="4422" to="4437" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digraph Signal Processing with Generalized Boundary Conditions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1422" to="1437" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02582</idno>
		<title level="m">D&apos;ya like DAGs? A Survey on Structure Learning and Causal Discovery</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of learning causality with data: Problems and methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neur. Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the role of sparsity and DAG constraints for learning linear DAGs</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neur. Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17943" to="17954" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning dags from data with few root causes</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Misiakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Püschel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Fourier-Sparse Functions on DAGs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algebraic structures for transitive closure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="59" to="76" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cambridge Studies in Advanced Mathematics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enumerative</forename><surname>Combinatorics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the foundations of combinatorial theory. I. theory of Möbius functions</title>
		<author>
			<persName><forename type="first">G.-C</forename><surname>Rota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z. Wahrscheinlichkeitstheorie und Verwandte Gebiete</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="340" to="368" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transitive closure and related semiring properties via eliminants</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Abdali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="257" to="274" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithm 97 (SHORTEST PATH)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new approach to the maximum-flow problem</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="921" to="940" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Algebraic signal processing theory: Foundation and 1-D time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3572" to="3585" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algebraic signal processing theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<idno>abs/cs/0612077</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discrete Signal Processing on Meet/Join Lattices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="3571" to="3584" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pegolotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<title level="m">Fast Moebius and Zeta transforms</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<title level="m">A Discrete Signal Processing Framework for</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meet/Join Lattices with Applications to Hypergraphs and Trees</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, and Signal Process. (ICASSP)</title>
		<meeting>Int. Conf. Acoust., Speech, and Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5371" to="5375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wiener filter on meet/join lattices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, and Signal Process</title>
		<meeting>Int. Conf. Acoust., Speech, and Signal ess</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5355" to="5359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Discrete Signal Processing Framework for Set Functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, and Signal Process</title>
		<meeting>Int. Conf. Acoust., Speech, and Signal ess</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1935" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discrete signal processing with set functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1039" to="1053" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-dimensional learning of linear causal networks via inverse covariance estimation</title>
		<author>
			<persName><forename type="first">Po-Ling</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3065" to="3105" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Identifiability of Gaussian structural equation models with equal error variances</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning identifiable Gaussian Bayesian networks in polynomial time and sample complexity</title>
		<author>
			<persName><forename type="first">Asish</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Honorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neur. Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Concave penalized estimation of sparse gaussian bayesian networks</title>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2273" to="2328" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Linear Non-Gaussian Acyclic Model for Causal Discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">72</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topological signal processing over simplicial complexes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="2992" to="3007" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Signal processing on cell complexes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Roddenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hajij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05614</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Topological signal processing over cell complexes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Testa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06709</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An introduction to hypergraph signal processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsitsvero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, and Signal Process</title>
		<meeting>Int. Conf. Acoust., Speech, and Signal ess</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introducing hypergraph signal processing: Theoretical foundation and practical applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="639" to="660" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Quiver signal processing (qsp)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parada-Mayorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graphon signal processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F O</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="4961" to="4976" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Algebraic signal processing theory: 1-D space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3586" to="3599" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Algebraic signal processing theory: 2-D hexagonal spatial lattice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rötteler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1521" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Algebraic signal processing theory: 1-D nearest-neighbor models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2247" to="2259" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chevalley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Roohani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17283</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Results CausalBench challenge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc., B: Stat</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On random graphs i</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Debr</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">290</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal node centrality in complex networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="26107" to="26108" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Sparking &quot;The BBC Four Pandemic&quot;: Leveraging citizen science and mobile phones to model the spread of disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kissler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klepac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J K</forename><surname>Conlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selection, L 1 vs. L 2 regularization, and rotational invariance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Jr</forename><surname>Hall</surname></persName>
		</author>
		<title level="m">Combinatorial Theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
