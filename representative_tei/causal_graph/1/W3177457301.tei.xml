<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning</title>
				<funder ref="#_cQMJgTU">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_pv8gMu4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
							<email>xding@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Xiong</surname></persName>
							<email>kxiong@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of Ex-CAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal reasoning aims at understanding the general causal dependency between the cause and effect <ref type="bibr" target="#b10">(Luo et al., 2016)</ref>. Causality is commonly expressed by humans in the text of natural language, and is of great value for various Artificial Intelligence applications, such as question answering <ref type="bibr" target="#b15">(Oh et al., 2013)</ref>, event prediction <ref type="bibr" target="#b7">(Li et al., 2018)</ref>, and decision making <ref type="bibr" target="#b21">(Sun et al., 2018)</ref>.</p><p>Previous work mainly learns causal knowledge from manually annotated causal event pairs, and achieves promising performances <ref type="bibr" target="#b10">(Luo et al., 2016;</ref><ref type="bibr"></ref> Figure <ref type="figure">1</ref>: (a) Without the evidence event i, we can hardly reveal the implicit causation between a and b. (b) The absent of evidence events may restrict the performance of event-pair based methods. (c) Given an event pair, the ExCAR framework obtains evidence events from an event graph and conducts causal reasoning using the additional evidence events. (d) The causal strength (cs) of the same rule can vary with different antecedents. We define such phenomenon as superimposed causal effect. <ref type="bibr">Xie and Mu, 2019a;</ref><ref type="bibr" target="#b8">Li et al., 2019)</ref>. However, recent works have questioned the seemingly superb performance for some of these studies <ref type="bibr" target="#b11">(McCoy et al., 2019;</ref><ref type="bibr" target="#b19">Poliak et al., 2018;</ref><ref type="bibr" target="#b4">Gururangan et al., 2018)</ref>. Specifically, training data may contain exploitable superficial cues that are correlative of the expected output. The main concern is that these works have not learned the underlying mechanism of causation so that their inference models are not stable enough and their results are not explainable.</p><p>While we notice that there is plentiful evidence information outside the given corpus that can provide more clues for understanding the logical law of the causality. Figure <ref type="figure">1</ref> (a) exemplifies two clues I 1 : Excess Liquidity and I 2 : Invest Demand Increase for explaining how a: Quantitative Easing gradually leads to b: House Price Increases.</p><p>Without these important evidence information, on the other hand, as illustrated in Figure <ref type="figure">1 (b)</ref>, the causal relationship between a, d and between c, b could not be deducted from the known causation between a, b and between c, d . In contrast, with intermediate event I in hand, according to the transitivity of causality <ref type="bibr" target="#b5">(Hall, 2000)</ref>, the logic chain of a ⇒ i ⇒ d and c ⇒ i ⇒ b could be naturally derived from the observed logic chain a ⇒ i ⇒ b and c ⇒ i ⇒ d .</p><p>To fully exploit the potential of the evidence information, we present an Event graph knowledge enhanced explainable CAusal Reasoning (ExCAR) framework. In particular, as illustrated in Figure <ref type="figure">1</ref> (c), given an input event pair C, E , ExCAR firstly retrieves external evidence events such as I 1 , I 2 from a large-scale causal event graph (CEG, a causal knowledge base constructed by us), and defines the causation between C, I 1 , I 2 , E as a set of logical rules (e.g., r i = (E i ⇒ I i )), which rules are useful representations for the causal reasoning task because they are interpretable and can provide insight to inference results. <ref type="bibr" target="#b17">Pearl (2001)</ref> pointed out that the underlying logic of causality is a probabilistic logic. The advantage of using a probabilistic logic is that by equipping logical rules with probability, one can better model statistically complex and noisy data. However, learning such probabilistic logical rules in the causal reasoning scenario is quite difficult --it requires modeling the superimposed causal effect for each logical rule. Different from firstorder logical rules induced from some knowledge graphs, the probability of the logical rule (i.e. the causal strength of the cause-effect pair) in causal reasoning is uncertain, which varies with different antecedents. For example, as shown in Figure <ref type="figure">1 (d)</ref>, with the antecedent A: Catch a cold, a fever can hardly lead to life danger. While if fever is caused by the antecedent B: Septicemia, it can result in life danger with a high probability.</p><p>To address this issue, we further propose a Conditional Markov Neural Logic Network (CMNLN) for learning the conditional causal dependency of logical rules in an end-to-end fashion. Specifically, CMNLN first decomposes the logical rules set derived from the CEG into several distinct logic chains and learns a distributed representation for each logic chain in an embedding space. Subsequently, CMNLN estimates the conditional probability of each logical rule by an antecedent-aware potential function. Then CMNLN computes the probability of each logic chain by multiplying the probabilities of logical rules in the chain. Finally, CMNLN predicts the causality score of the input event pair based on the disjunction of chain-level causality information.</p><p>Experimental results show that our approach can effectively utilize the event graph information to improve the accuracy of causal reasoning by more than 5%. Adversarial evaluation and human evaluation show that ExCAR can achieve stable and explainable performance. The code is released at <ref type="url" target="https://github.com/sjcfr/ExCAR">https://github.com/sjcfr/ExCAR</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formalization</head><p>In this paper, both the COPA <ref type="bibr" target="#b10">(Luo et al., 2016)</ref> and the C-COPA causal reasoning task are defined as a multiple-choice task. Specifically, as the following example shows, given a premise event, one needs to choose a more plausible cause (effect) from two hypothesis events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example:</head><p>Premise: The company lost money.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ask-for: Cause</head><p>Hypothesis 1: Its products received favorable comments.</p><p>Hypothesis 2: Some of its products were defective.</p><p>Therefore, the causal reasoning task could be formalized as a prediction problem: given a causeeffect event pair C, E composed by the premise event and one of the hypothesis events, the prediction model is required to predict a score measuring the causality of the event pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Event Graph</head><p>CEG is a large-scale causal knowledge base constructed by us, from which we can retrieve a set of additional evidences for a given cause-effect event pair C, E . Formally, CEG is a directed acyclic graph and can be denoted as G = {V, R}, where V is the node set, R is the edge set. Each node V i ∈ V corresponds to an event, while each edge R ij ∈ R denotes that there is a causal relationship between the ith event and jth event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rule-based Reasoning Using Markov Logic Network</head><p>In this paper, to enhance the explainability and stability of causal reasoning, we cast the causal reasoning problem as a rule based reasoning task. Specifically, given an input causal event pair C, E , we retrieve a set of evidence events from the CEG. The evidence events together with C and E further form into a set of causal logical rules, where a rule describes the causal relationship between two events. Formally, a rule r i = (e i 1 ⇒ e i 2 ), where ⇒ is a logical connective indicating the causal relationship between two events e i 1 and e i 2 . With regard to these causal logical rules, the causal mechanism can be revealed and the causal reasoning can be conducted in an explainable way. However, the underlying logic is a probabilistic logic. Markov Logic Network (MLN) <ref type="bibr" target="#b16">(Pearl, 1988)</ref> can model such uncertainty by assigning each causal rule a causal strength, which measures the probability that this rule holds true. Let P (r i ) denote the causal strength of rule r i . MLN estimates P (r i ) using a potential function φ(r i ).</p><p>Thereafter, the causality score Y is predicted by simply multiplying the causal strength of obtained rules:</p><formula xml:id="formula_0">P (Y ) = 1 Z i P (ri) = 1 Z i φ(ri),<label>(1)</label></formula><p>where 1 Z is a normalization constant. However, there still remains two challenges for rule-based causal reasoning using MLN: 1) MLN defines potential functions as linear combinations of some hand-crafted features; 2) MLN cannot model the influence of antecedents of rules. Different from MLN, in this paper, we propose a Conditional Markov Neural Logic Network, which works on the embedding space of logic rules to model the conditional causal strength of rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref>, ExCAR consists of two components. Given an event pair C, E , ExCAR employs an evidence retrieval module to retrieves evidence events from a prebuilt causal event graph to generate a set of logical rules. Then ExCAR conducts causal reasoning based on the logical rules using a Conditional Markov Neural Logic Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evidence Events Retrieval</head><p>Given an event pair C, E outside the causal event graph, to obtain the evidences from the CEG, we first locate the cause and effect in the CEG. Intuitively, semantically similar events would have similar causes and effects, and share similar locations in the CEG. To this end, we employ a pretrained language model ELMo (Peters et al., 2018) to derive the semantic representation for events in the CEG, as well as the cause and effect event. Then events in the CEG which are semantically similar to the input cause and effect event can be found using cosine similarity of the semantic representations. These events can serve as anchors for locating the cause and effect event. Then as Figure <ref type="figure" target="#fig_0">2</ref> shows, taking the anchors of the cause event as start points, and taking the anchors of the effect event as end points, the evidence events can be retrieved by a Breadth First Search (BFS) algorithm.</p><p>After the retrieving process, the cause, effect and evidence events constitute a causal logical graph (CLG) G * = {V * , R * }, where V * and R * is the node set and edge set, respectively. Each node e i within V * is an event, each edge r j within R * describes the causal relationship between two events. Taking G * as the input, the following causal reasoning process is equipped with a set of logical rules for revealing the behind causal mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Markov Neural Logic Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overview</head><p>Given the CLG, we can derive a set of causal logical rules for supporting the causal reasoning process. However, as Figure <ref type="figure">1</ref> (d) shows, the causal strength of a rule may vary with different antecedents, where the antecedent can be an event, a simple rule or a complex of single rules. For clarity, we denote the antecedent of a rule r i as ANTE i . Influenced by a certain antecedent, the causal strength of a rule can be described by a conditional probability P (r i |ANTE i ).</p><p>As shown in Figure <ref type="figure" target="#fig_0">2</ref>, a single rule derived from the CLG can have multiple antecedents, and each of these antecedents can have its own influence on the causal strength of the rule. To address this issue by exploiting the effectiveness of neural models in representation learning, we propose the CMNLN that works on the embeddings of logical rules. To model the superimposed causal effect of rules, CMNLN regards the CLG as a composition of distinct causal logic chains {ρ 1 , • • • , ρ m }, and predicts causality score through combining information of each causal logic chain. Hence, within each causal logic chain, we can estimate a chainspecific causal strength for each rule r j k ∈ ρ j , using an antecedent-aware potential function. Then CMNLN aggregates the intra-chain causation information and inter-chain causation information to derive the causality score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Logic Chain Generation</head><p>For supporting the following reasoning process, we first explore the CLG to generate all possible causal logic chains {ρ 1 , • • • , ρ m }. As shown in Figure <ref type="figure" target="#fig_0">2</ref>,</p><formula xml:id="formula_1">ρ j = {r j 1 ∧, • • • , ∧r j l j }</formula><p>describes a serial of transitive causal logical rules starting from the cause event C and ending at the effect event E.</p><p>Considering that each rule r j k ∈ ρ j is composed by two events e j k-1 and e j k , a causal logic chain ρ j with l j rules contains totally l j + 1 events</p><formula xml:id="formula_2">{e j 0 , • • • , e j l j }</formula><p>, where e j 0 and e j l j are the cause event C and the effect event E, respectively. Taking C and E as the start and end point respectively, we can enumerate all distinct causal logic chains in the CLG using a Depth First Searching algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Event Encoding</head><p>A BERT-based encoder <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> is employed to encode all events within each causal logic chain into chain-specific distributed embeddings.</p><p>Specifically, for a causal logic chain ρ j containing lj+1 events {e j 0 , • • • , e j l j }, we first process the event sequence into the form of:</p><formula xml:id="formula_3">[CLS] e j 0 • • • [CLS] e j k • • • [CLS] e j l j .</formula><p>After that, the processed event sequence is fed into BERT. We define the final hidden state of the [CLS] token before each event as the representation of the corresponding event. In this way, we obtain an event embedding set</p><formula xml:id="formula_4">H = {h j 0 , • • • , h j l j },</formula><p>where h j k ∈ R d is the embedding of the kth event within the causal logic chain ρ j . Note that, h j 0 is the representation of the cause event C, and h j l j is the representation of the effect event E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Chain-specific Conditional Causal Strength Estimation</head><p>Given one of the causal logic chains ρ j = (r j 1 ∧, • • • , ∧r j l j ) and corresponding event representations H = {h j 0 , • • • , h j l j }, CMNLN estimates the chain-specific causal strength for each rule using an antecedent-aware potential function.</p><p>For a rule r j k ∈ ρ j , we define the chain-wise antecedent of r j k as (r j</p><formula xml:id="formula_5">1 ∧r j 2 ∧, • • • , ∧r j k-1</formula><p>) , and denote it as ANTE j k . Therefore, with regard to ANTE j k , we can derive the chain-specific causal strength using an antecedent-aware potential function as:</p><formula xml:id="formula_6">P (r j k |ANTE j k ) = φ a (r j k , ANTE j k ).<label>(2)</label></formula><p>Considering that each logical rule r j k is composed of two events e j k-1 and e j k , the input of φ a (•) is the distributed representation of ANTE j k , and the embedding of e j k-1 and e j k . We denote the representation of ANTE j k as s j k , and describe the specific process for deriving s j k in the following section. Given s j k , h j k-1 and h j k , to model the influence of ANTE j k , we first derive antecedent-aware representations of e j k-1 and e j k using an MLP:</p><formula xml:id="formula_7">h j k-1 =tanh(W c [s j k ||h j k-1 ] + b c ),<label>(3)</label></formula><formula xml:id="formula_8">h j k =tanh(W e [s j k ||h j k ] + b c ),<label>(4)</label></formula><p>where •||• is the concatenate operation, and Wc, We ∈ R d×2d are two different weight matrix modeling the influence of s j k on e j k-1 and e j k , respectively. Then based on the antecedent-aware event representations h j k-1 and h j k , we calculate the conditional causal strength of r j k as:</p><formula xml:id="formula_9">φ a (r j k , ANTE j k ) = σ(h j k-1 W cs h j k ),<label>(5)</label></formula><p>where Wcs ∈ R d×d are trainable parameters, and σ is a sigmoid function.</p><p>Antecedent Representation Along with the estimation of conditional causal strength, the representation of antecedents are also recursively updated. Specifically, at the first reasoning step, we initialize s j 0 with h j 0 . At the kth reasoning step, s j k is obtained based on s j k-1 , the conditional causal strength P (r j k |ANTE j k ), and the embedding of events within r j k :</p><formula xml:id="formula_10">s j k = tanh(P (r j k |ANTE j k )Wu[h j k-1 ||h j k ]) + s j k-1 ,<label>(6)</label></formula><p>where W u ∈ R d×2d is a parameter matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Intra-Chain Information Aggregation</head><p>We aggregate the intra-chain causality information to derive a distributed representation and a chainlevel causal strength for each causal logic chain. We notice that, in the conditional causal strength estimation process, at the l j th reasoning step, ANTE j l j +1 actually includes all the rules within ρ j . Hence, we utilize the representation of ANTE j l j +1 as the representation of ρ j , which we denote as s j .</p><p>Given the chain-specific conditional causal strength for each rule within ρ j , we can calculate a chain-level causal strength cs j for ρ j by multiplying the conditional causal strength of the rules:</p><formula xml:id="formula_11">cs j = l j k=1 P (r j k |ANTE j k ) = l j k=1 φa(r j k , ANTE j k ).<label>(7)</label></formula><p>Then we normalize the chain-level causal strengths as:</p><formula xml:id="formula_12">ĉs j = softmax j (cs j ).</formula><p>(8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Aggregating Chain-level Information for Predicting Causality Score</head><p>Finally, we obtain the disjunction of chain-level causality information to predict the causality score Y . Intuitively, a causal logic chain with higher causal strength should have a stronger influence on Y . Therefore, we aggregate the chain-level information through calculating a linear combination of logic chain representations {s 1 • • • , s m } using the normalized causal strengths { ĉs 1 , • • • , ĉs m }:</p><formula xml:id="formula_13">u = Σ j ĉs j • s j (9)</formula><p>where u ∈ R 1×d is a final state carrying information from the disjunction of {ρ</p><formula xml:id="formula_14">1 , • • • , ρ m }.</formula><p>The causality score Y is predicted based on u:</p><formula xml:id="formula_15">Y = softmax(W y u + b y ),<label>(10)</label></formula><p>where W y and b y are trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>In the training process, we introduce a causal logic driven negative sampling to improve the reliability of conditional causal strength estimation. In particular, if there exists a rule r i = (e i 1 ⇒ e i 2 ) within the CLG, due to the unidirectionality of causality, we can derive a corresponding false rule r F = (e i 2 ⇒ e i 1 ). From the CLG, we can also generate a wrong antecedent for the false rule through random sampling. Hence, ideally, the conditional causal strength of these false rules should equal 0. In addition, we also combine the unidirection-ality of causality with the transitivity of causality to generate false rules with more complex patterns (e.g.: if e 1 ⇒ e 2 ⇒ e 3 , then we can induce a r F = (e 3 ⇒ e 1 )). By sampling false rules and training the potential functions of these false rules φ a (r F , ANTE F ) to be zero, the reliability of conditional causal strength estimation can be enhanced.</p><p>With regard to the causal logic driven negative sampling process, the loss function of CMNLN is defined as:</p><formula xml:id="formula_16">L = L Causality Score + λL Conditional CS ,<label>(11)</label></formula><p>where both L Causality Score and L Conditional CS are cross entropy loss, measuring the difference between the predicted and ground truth causality score, and between the predicted and the ideal conditional causal strength, respectively; λ is a balance coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Construction of C-COPA Dataset</head><p>To evaluate the robustness of the ExCAR framework, we build an additional Chinese commonsense causal reasoning dataset C-COPA. The C-COPA dataset is built upon a large-scale web news corpus SogouCS <ref type="bibr" target="#b23">(Wang et al., 2008</ref>) by human annotation. We start the annotation process from manually extracting causal event pairs from raw texts within the corpus. Given a causal event pair, we first randomly generate an ask-for indicator, where ask-for ∈ ["effect", "cause"]. Then the ask-for indicator are used to decide whether the cause or effect event to be the premise or plausible hypothesis. Given the premise, an implausible effect (cause) events is generated by a human annotator. As a result, the same as the COPA dataset, each instance within the C-COPA consists a premise event p, a plausible and an implausible hypothesis event h + and h -, and an ask-for indicator a.</p><p>Three Chinese volunteers are enlisted for validating the dataset. Agreement between volunteers is high (Cohen's K = 0.923). Instances with diverged results between volunteers are removed from the dataset. After the annotation process, a total of 3,258 instances are left and we randomly split these instances into two equal-sized parts as the development set and the test set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Construction of Causal Event Graph</head><p>Before constructing the CEG, we have to collect a sufficient number of causal event pairs. To this end, we harvest English causal event pairs from the CausalBank Corpus <ref type="bibr" target="#b9">(Li et al., 2020)</ref>, which contains 314 million commonsense causal event pairs in total. While the Chinese causal event pairs are collected from a raw web text corpus crawled from multiple websites date from 2018 to 2019, and filtered with keywords. More details could be found in the Appendix.</p><p>Then an English and a Chinese CEG are build based on the corresponding causal event pair corpus. To balance the computation burden and coverage of the event graph, we build the English and the Chinese CEG based on 1,500,000 Chinese and 1,5000,000 English causal event pairs randomly sampled from the whole corpus, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>Given a cause or effect event, we find three most textually similar events from the causal event graph, and employ them as the anchors. In the evidence retrieving process, we limit the maximum searching depth of BFS to 3, and restrict the size of evidence event set to be no more than 8. We employ the pre-trained BERT-base model as the event encoder, which encodes each input event to a 768-dimension vector. On both datasets, for each instance, 5 negative rules are sampled to facilitate the estimation of conditional causal strength. Model is trained with the balance coefficient λ of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical-based Methods</head><p>These methods estimate words or phrase level causality from large-scale corpora. Then the causality of an input event pair could be obtained through synthesizing the word or phrase level causality.</p><p>• PMI <ref type="bibr" target="#b6">(Jabeen et al., 2014)</ref> measures the wordlevel causality using Point Mutual Information.</p><p>• PMI EX <ref type="bibr" target="#b3">(Gordon et al., 2011)</ref> is an asymmetric word-level PMI which takes the directionality of causal inference into consideration.</p><p>• CS <ref type="bibr" target="#b10">(Luo et al., 2016</ref>) measures word-level causality through integrating both the necessity causality and sufficiency causality.</p><p>• CS MWP <ref type="bibr" target="#b20">(Sasaki et al., 2017)</ref> measures the causality between words and prepositional phrases using the CS score.</p><p>Pre-trained-model-based Methods • BERT <ref type="bibr">Wang et al. [2019a]</ref> and <ref type="bibr" target="#b8">Li et al. [2019]</ref> finetune BERT base with different hyper parameters to predict the causality of each C, E pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExCAR-based Methods</head><p>Methods COPA C-COPA PMI <ref type="bibr" target="#b6">(Jabeen et al., 2014)</ref> 58.8 56.2 PMI EX <ref type="bibr" target="#b3">(Gordon et al., 2011)</ref> 65.4 62.3 CS <ref type="bibr" target="#b10">(Luo et al., 2016)</ref> 70.2 68.9 CS MWP <ref type="bibr" target="#b20">(Sasaki et al., 2017)</ref> 71.2 -BERT <ref type="bibr">(Wang et al., 2019a)</ref> 70.4 72.8 BERT <ref type="bibr" target="#b8">(Li et al., 2019)</ref> 73 We replace the CMNLN layer of ExCAR framework with different reasoning modules and get:</p><p>• ExCAR-w/ MLN refers to substitute the CMNLN layer by a classical Markov Logic Network layer.</p><p>• ExCAR-w/ fix-cs arbitrarily assign a fixed causal strength 0.5 for each logical rule.</p><p>• ExCAR-concat flattens the causal logical graph into a single event sequence and takes the event sequence as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quantitative Analysis</head><p>We list the results on both the COPA dataset and C-COPA dataset in Table <ref type="table">1</ref>. We find that:</p><p>(1) Statistical-based methods, such as CS <ref type="bibr" target="#b10">(Luo et al., 2016)</ref> and CS MWP <ref type="bibr" target="#b20">(Sasaki et al., 2017)</ref> achieve comparable performances with BERTbased methods, this is mainly because they harvest causal knowledge with elaborate patterns from large-scale corpus sized up to 10TB. Training BERT with such causal knowledge may provide potential space for improvement, which is left for future work.</p><p>(2) Compared to causal pair based BERT, Ex-CAR related methods show improved performance. This indicates that incorporating additional evidences from the event graph can be helpful for revealing the causal decision mechanism and then improve the accuracy of causal reasoning.</p><p>(3) ExCAR-w/ MLN and ExCAR -w/ CMNLN outperforms ExCAR-concat, which flats the CLG into an event sequence. This shows that exploiting the complex causal correlation patterns between logical rules can be helpful for the causal reasoning task.</p><p>(4) ExCAR-w/ MLN and ExCAR -w/ CMNLN shows improved performance compared to ExCAR -w/ fixed-cs. This confirms that neuralizing rules to account for the uncertainty of the logical rules is helpful for the causal reasoning task.    (5) ExCAR-w/ CMNLN further improves the prediction accuracy compared to ExCAR-w/ MLN, suggesting that by incorporating the antecedentaware potential function CMNLN can model the conditional causal strength of logical rules for causal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Stability Analysis</head><p>In this paper, we propose to enhance the stability of our approach through introducing additional evidence information. We investigate the specific influence of these evidences on the stability of our approach through an adversarial evaluation. Following <ref type="bibr" target="#b0">Bekoulis et al. [2018]</ref> and <ref type="bibr" target="#b27">Yasunaga et al. [2018]</ref>, we attack the reasoning systems by adding a perturbation term on the word embedding of inputs. The perturbation term is derived using a gradient-based method FGM <ref type="bibr" target="#b12">(Miyato et al., 2016)</ref>.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the prediction accuracy after adversary attack, and ∆ denotes the change of performance brought by adversary attack. For example, ∆ = -9.9 means a 9.9% decrease of prediction accuracy after the adversary attack. We find that, compared with event pair based BERT, ExCAR can significantly improve the stability of the prediction accuracy. These results show that by incorporating additional evidence events, ExCAR could reveal the behind causal mechanism to increase the stability of prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Human Evaluation for Explainability</head><p>We analyze the explainability of our approach quantitatively through human evaluations. In particular, we randomly sample 200 instances from the test set of C-COPA and make prediction using Ex-CAR. Then we employ three experts to give an explainability score belonging to {0, 1, 2} to evaluate whether the causality strengths derived by our approach are reasonable, where 0 stands for unexplainable, 1 stands for moderately explainable and 2 stands for explainable. For comparison, we further introduce two baselines: (1) Markov Logic Network (MLN); (2) Fixed-cs.</p><p>The average explainability scores are shown in Table <ref type="table" target="#tab_2">3</ref>, from which we can observe that: (1) The average explainability scores of CMNLN and MLN are higher than that of fixed-cs. This is because, through neuralizing the logical rules and equipping the logical rules with probability, CMNLN and MLN can better model the potential noise in the retrieved evidences, as well as the uncertainty of rules. ( <ref type="formula" target="#formula_6">2</ref>) The explainability score of CMNLN is further higher than that of MLN. This indicates that, CMNLN can model the conditional causal strength of logical rules using the antecedent-aware potential function, and then increase the reasonability of causal strength estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Case Study</head><p>Figure <ref type="figure" target="#fig_3">3</ref> provides an example of causal reasoning made by ExCAR on C-COPA. Given a cause event Reduction of grain production, E: Rise of Inflation Rate is more likely to be the effect of the cause. However, it is difficult to directly infer the effect E: Rise of Inflation Rate directly from the cause event C:Reduction of grain production. Correspondingly, given C and E, ExCAR can obtain evidence events such as I 1 : Food prices increase and I 2 : Grain prices out of control from the causal event graph. These results show that ExCAR can obtain relevant evidences and hence choose the correct effect event in an explainable manner.</p><p>We also examined the estimated causal strengths. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the causal strength between I 1 and E is higher in the logic chain ρ 2 compared to ρ 1 . Intuitively, with the additional antecedent I 2 : Grain prices out of control, I 1 : Food prices increase could be more likely to lead to E: Rise of Inflation Rate. These results indicate that CMNLN can model the conditional causal strength of rules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Effect of the Number of Evidence Events</head><p>We compare the reasoning accuracy of ExCAR on samples with different numbers of evidence events. Experiments are conducted on the test set of C-COPA. Results are shown in Figure <ref type="figure" target="#fig_4">4</ref>. We can find that, when the evidence events number increases from 0 to 3, the reasoning accuracy increases in general, since sufficient evidences are helpful for the reasoning task. However, the accuracy starts to decrease when evidence number exceeds 4. This indicates that noisy evidence events may be obtained. The inclusion of noisy evidence events emphasis the necessity of neutralizing the logical rules, as the symbolic logic based systems cannot accommodate for the noise in the rules.</p><p>5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Causal Reasoning</head><p>Causal reasoning remains a challenging problem for today's AI systems. Statistical-based methods can provide strong baselines, as they can find some useful cues from large-scale causal corpus. For example, <ref type="bibr" target="#b3">Gordon et al. (2011)</ref> measured the causality between words using PMI, and estimated the PMI based on a personal story corpus. While <ref type="bibr" target="#b10">Luo et al. (2016)</ref> and <ref type="bibr" target="#b20">Sasaki et al. (2017)</ref> further introduced direction information into a causal strength index. Then through synthesizing the word-level causality, the causality between events could be inferred.</p><p>Compared to statistical-based methods, deep neural networks enable models to learn the causality between events considering the semantics of events. To this end, Xie and Mu (2019b) devised attention-based models to capture the word-level causal relationships. While <ref type="bibr">Wang et al. and Li et al. (2019)</ref> finetuned the pretrained language model BERT on causal event pairs corpus to learn the pairwise causality knowledge between events.</p><p>In this paper, we argue that in addition to the event pair itself, causal reasoning also needs to involve more evidence information. To address this issue, we propose a novel inference framework ExCAR, which is able to incorporate the additional evidence events from an event graph for supporting the causal reasoning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Explainable Textual Inference</head><p>Explainability has been a long-pursued goal for textual inference systems, as it can help to unveil the decision making mechanism of black-box models and enhance the stability of reasoning, which can be crucial for applications in various domains, such as medical and financial domains. To introduce interpretability in textural inference process, previous studies can be mainly divided into two categories: generating explainable information and devising self-explaining mechanism.</p><p>Beyond the task related information, automated generated textual explanations are helpful for justifying the reliability of models. For example, <ref type="bibr" target="#b1">Camburu et al. (2018)</ref> and <ref type="bibr" target="#b13">Nie et al. (2019)</ref> train multitask learning models to learn to generate explanations for textual entailment inference. On the other hand, the incorporation of relevant external knowledge can not only increase the model performance compared to purely data-driven approaches, but also can be helpful for understanding the model behavior <ref type="bibr" target="#b14">(Niu et al., 2019;</ref><ref type="bibr">Wang et al., 2019b)</ref>.</p><p>Another line of work designs self-explaining models to reveal the reasoning process of models. Attention mechanism was devised to explicitly measure the relative importance of input textual features. Hence, it has been widely employed to enhance the interpretability of deep neural models.</p><p>In this paper, to conduct causal reasoning in an explainable manner, we propose to induce a set of logic rules from a pre-built causal event graph, and explicitly model the conditional causal strength of each logical rule. The probabilistic logical rules can provide clues to explain the prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We devise a novel explainable causal reasoning framework ExCAR. Given an event pair, ExCAR is able to obtain logical rules from a large-scale causal event graph to provide insight to inference results. To learn the conditional probabilistic of logical rules, we propose a conditional Markov neural logic network that combines the strengths of rulebased and neural models. Empirically, our method outperforms prior work on two causal reasoning datasets, including COPA and C-COPA. Furthermore, ExCAR is interpretable by providing explanations in terms of probabilistic logical rules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the ExCAR framework and the architecture of CMNLN.</figDesc><graphic coords="3,104.65,62.81,385.51,115.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of causal reasoning result made by ExCAR.</figDesc><graphic coords="7,318.19,62.81,196.44,66.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Reasoning accuracy of ExCAR with different number of evidence events on the test set of C-COPA.</figDesc><graphic coords="8,93.83,62.81,174.62,69.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Prediction accuracy (%) after adversary attack.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average explainability score of CMNLN, MLN and unified causal strength on C-COPA.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the <rs type="projectName">Technological Innovation "2030 Megaproject" -New Generation Artificial Intelligence of China</rs> (<rs type="grantNumber">2018AAA0101901</rs>), and the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61976073</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cQMJgTU">
					<idno type="grant-number">2018AAA0101901</idno>
					<orgName type="project" subtype="full">Technological Innovation &quot;2030 Megaproject&quot; -New Generation Artificial Intelligence of China</orgName>
				</org>
				<org type="funding" xml:id="_pv8gMu4">
					<idno type="grant-number">61976073</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">e-snli: Natural language inference with natural language explanations</title>
		<author>
			<persName><forename type="first">Oana-Maria</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9539" to="9549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NACCL 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commonsense causal reasoning using millions of personal stories</title>
		<author>
			<persName><surname>Andrew S Gordon</surname></persName>
		</author>
		<author>
			<persName><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cosmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02324</idno>
		<title level="m">Annotation artifacts in natural language inference data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causation and the price of transitivity</title>
		<author>
			<persName><forename type="first">Ned</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Philosophy</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="198" to="222" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Using asymmetric associations for commonsense causality detection</title>
		<author>
			<persName><forename type="first">Shahida</forename><surname>Jabeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Andreae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In PRICAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constructing narrative event evolutionary graph for script event prediction</title>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4201" to="4207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><surname>Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02079</idno>
		<title level="m">Learning to rank for plausible plausibility</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Guided generation of cause and effect</title>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Commonsense causal reasoning between short texts</title>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Kenny Q Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01007</idno>
		<title level="m">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14599</idno>
		<title level="m">Adversarial nli: A new benchmark for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge aware conversation generation with explainable reasoning over augmented graphs</title>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1782" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why-question answering using intra-and intersentential causal relations</title>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stijn</forename><surname>De Saeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyonori</forename><surname>Ohtake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1733" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems; networks of plausible inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Direct and indirect effects</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Collecting diverse natural language inference problems for sentence representation evaluation</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08207</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handling multiword expressions in causality estimation</title>
		<author>
			<persName><forename type="first">Shota</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading comprehension with graph-based temporalcasual reasoning</title>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="806" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic online news issue construction in web environment</title>
		<author>
			<persName><forename type="first">Canhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyun</forename><surname>Ru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Yixin Cao, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting causal embeddings via potential verb-mediated causal patterns</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiteng</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>I Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1921" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representation of words in cause and effect spaces</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiteng</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7330" to="7337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust multilingual part-of-speech tagging via adversarial training</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
