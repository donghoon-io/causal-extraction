<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Independent Noise Condition for Estimating Latent Variable Causal Graphs</title>
				<funder ref="#_xrDfybm">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Outstanding Young Scientific Research Talents International Cultivation Project Fund of Department of Education of Guangdong Province(40190001)</orgName>
				</funder>
				<funder ref="#_JaSPEUZ">
					<orgName type="full">Science and Technology Planning Project of Guangzhou</orgName>
				</funder>
				<funder ref="#_9pcGUHn">
					<orgName type="full">United States Air Force</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-11-18">18 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
							<email>xiefeng009@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
							<email>cairuichu@gdut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
							<email>biweih@andrew.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
							<email>zfhao@gdut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Mathematics and Big Data</orgName>
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Independent Noise Condition for Estimating Latent Variable Causal Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-18">18 Nov 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.04917v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery aims to recover causal structures or models underlying the observed data. Despite its success in certain domains, most existing methods focus on causal relations between observed variables, while in many scenarios the observed ones may not be the underlying causal variables (e.g., image pixels), but are generated by latent causal variables or confounders that are causally related. To this end, in this paper, we consider Linear, Non-Gaussian Latent variable Models (LiNGLaMs), in which latent confounders are also causally related, and propose a Generalized Independent Noise (GIN) condition to estimate such latent variable graphs. Specifically, for two observed random vectors Y and Z, GIN holds if and only if ω ⊺ Y and Z are statistically independent, where ω is a parameter vector characterized from the cross-covariance between Y and Z. From the graphical view, roughly speaking, GIN implies that causally earlier latent common causes of variables in Y d-separate Y from Z. Interestingly, we find that the independent noise condition, i.e., if there is no confounder, causes are independent from the error of regressing the effect on the causes, can be seen as a special case of GIN. Moreover, we show that GIN helps locate latent variables and identify their causal structure, including causal directions. We further develop a recursive learning algorithm to achieve these goals. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying causal relationships from observational data, known as causal discovery, has drawn much attention in the fields of empirical science and artificial intelligence <ref type="bibr" target="#b25">[Spirtes et al., 2010</ref><ref type="bibr" target="#b15">, Pearl, 2019]</ref>. Most causal discovery approaches focus on the situation without latent variables, such as the PC algorithm <ref type="bibr" target="#b22">[Spirtes and Glymour, 1991]</ref>, Greedy Equivalence Search (GES) <ref type="bibr" target="#b4">[Chickering, 2002]</ref>, and methods based on the Linear, Non-Gaussian Acyclic Model (LiNGAM) <ref type="bibr" target="#b16">[Shimizu et al., 2006]</ref>, the Additive Noise Model (ANM) <ref type="bibr" target="#b11">[Hoyer et al., 2009]</ref>, and the Post-NonLinear causal model (PNL) <ref type="bibr">[Zhang and</ref><ref type="bibr" target="#b28">Chan, 2006, Zhang and</ref><ref type="bibr">Hyvärinen, 2009]</ref>. However, although these methods have been used in a range of fields, they may fail to produce convincing results in cases with latent variables (or more specifically, confounders), because they do not properly take into account the influences from latent variables as well as many other practical issues <ref type="bibr" target="#b31">[Zhang et al., 2018]</ref>.</p><p>Causal discovery with latent variables has attracted much attention. Some approaches attempt to handle the question based on conditional independence constraints, including the FCI algorithm <ref type="bibr" target="#b24">[Spirtes et al., 1995]</ref>, RFCI <ref type="bibr" target="#b5">[Colombo et al., 2012]</ref>, and their variants. They focus on estimating the causal relationships between observed variables rather than that between latent variables. However, in real-world scenarios, it may not be the case-there are also causal relationships between latent variables. Later, it was shown that by utilizing vanishing Tetrad conditions <ref type="bibr" target="#b20">[Spearman, 1928]</ref> and, more generally, t-separation, one is able to identify latent variables in linear-Gaussian models <ref type="bibr" target="#b19">[Silva et al., 2006</ref><ref type="bibr" target="#b26">, Sullivant et al., 2010]</ref>. Furthermore, by leveraging an extended t-separation <ref type="bibr" target="#b21">[Spirtes, 2013]</ref>, a more reliable and faster algorithm, called FindOneFactorClusters (FOFC), was developed <ref type="bibr" target="#b14">[Kummerfeld and Ramsey, 2016]</ref>. However, these methods may not be able to identify causal directions between latent variables, and they require strong constraints that each latent variable should have at least three pure measurement variables.</p><p><ref type="foot" target="#foot_0">foot_0</ref> Such limitation is because they only rely on rank constraints on the covariance matrix, but fail to take into account higher-order statistics. To make use of higher-order information, one may apply overcomplete independent component analysis <ref type="bibr" target="#b10">[Hoyer et al., 2008</ref><ref type="bibr" target="#b17">, Shimizu et al., 2009]</ref>, but it does not consider the causal structure between latent variables and the size of the equivalence class of the identified structure could be very large <ref type="bibr" target="#b7">[Entner and</ref><ref type="bibr">Hoyer, 2010, Tashiro et al., 2014]</ref>. Another interesting work by <ref type="bibr" target="#b0">Anandkumar et al. [2013]</ref> extracts second-order statistics in identifying latent factors, while using non-Gaussianity when estimating causal relations between latent variables. <ref type="bibr" target="#b29">Zhang et al. [2017]</ref> and Huang* et al.</p><p>[2020] considered a special type of confounders due to distribution shifts.</p><p>Recently, a condition about a particular type of independence relationship between any three variables, called Triad condition, was proposed <ref type="bibr" target="#b3">[Cai et al., 2019]</ref>, together with the LSTC algorithm to discover the structure between latent variables. Nevertheless, this method does not apply to the case where there are multiple latent variables behind two observed variables.</p><p>It is well known that one may use the independent noise condition to recover the causal structure from linear non-Gaussian data without latent variables <ref type="bibr" target="#b18">[Shimizu et al., 2011]</ref>. Then a question naturally rises: is it possible to solve the latent-variable problem, by introducing non-Gaussianity and a condition similar to the independent noise condition? Interestingly, we find that it can be achieved by testing the independence between ω ⊺ Y and Z, where Y and Z are two observed random vectors, and ω is a parameter vector based on the cross-covariance between Y and Z. If ω ⊺ Y and Z are statistically independent, we term this condition Generalized Independent Noise (GIN) condition. We show that the well-known independent noise condition can be seen as a special case of GIN. From the view of graphical models, roughly speaking, if the GIN condition holds, then in the Linear Non-Gaussian Latent variable Model (LiNGLaM), the causally earlier latent common causes of variables in Y d-separate Y from Z. By leveraging GIN, we further develop a practical algorithm to identify important information of the LiNGLaM, including where the latent variables are, the number of latent variables behind any two observed variables, and the causal order of the latent variables.</p><p>The contributions of this work are three-fold. 1) We define the GIN condition for an ordered pair of variables sets, provide mathematical conditions that are sufficient for it, and show that the independent noise condition can be seen as its special case. 2) We then further establish a connection between the GIN condition and the graphical patterns in the LiNGLaM, including specific d-separation relations. 3) We exploit GIN to estimate the LiNGLaM, which allows causal relationships between latent variables and multiple latent variables behind any two observed variables. Compared to existing work, a uniquely appealing feature of the proposed method is that it is able to identify the causal order of the latent variables and determine the number of latent variables behind any two observed variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>In this paper, we focus on a particular type of linear acyclic latent variable causal models. We use V = X ∪ L to denote the total set of variables, where X denote the set of observed variables, with X = {X 1 , X 2 , ...X m }, and L denote the set of latent variables, with L = {L 1 , L 2 , ...L n }. We assume that any variable in V satisfy the following generating process:</p><formula xml:id="formula_0">V i = ∑ k(j)&lt;k(i) b ij V j + ε V i , i = 1, 2, ..., m + n,</formula><p>where k(i) represents the causal order of variables in a directed acyclic graph, so that no later variable causes any earlier variable, b ij denotes the causal strength from V j to V i , and ε V i are independent and identically distributed noise variables.</p><p>Without loss of generality, we assume that all variables have a zero mean (otherwise can be centered). The definition of our model is given below. ) pure measurement variables as children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4. [Purity Assumption]</head><p>There is no direct edge between observed variables.</p><formula xml:id="formula_1">L 1 L 2 L 3 L 4 X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 a 1 a 2 a3 a 4 b 1 b2 b 3 b 4 c1 c 2 d1 d 2 α γ β σ η θ</formula><p>Figure <ref type="figure" target="#fig_2">1</ref>: A causal structure involving 4 latent variables and 8 observed variables, where each pair of observed variables in {X 1 , X 2 , X 3 , X 4 } are affected by two latent variables.</p><p>The key difference to existing researches considering linear latent models, such as <ref type="bibr" target="#b1">Bollen [1989]</ref>, <ref type="bibr" target="#b19">Silva et al. [2006]</ref>, is that we introduce the assumptions A2∼A4, allowing us to identify the casual structure over latent variables, including casual directions. Figure <ref type="figure" target="#fig_2">1</ref> shows a simple example that satisfies the LiNGLaM. For Non-Gaussianity Assumption, the non-Gaussian distribution are expected to be ubiquitous, due to Cramér Decomposition Theorem <ref type="bibr" target="#b6">[Cramér, 1962]</ref>, as stated in <ref type="bibr" target="#b23">Spirtes and Zhang [2016]</ref>. ) + 1 pure observed variables. In Section 6, we will briefly discuss the situation where Assumption A4 is violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GIN Condition and Its Implications in LiNGLaM</head><p>In this section, we first briefly review the Independent Noise (IN) condition in linear non-Gaussian causal models with no latent variables. Then we formulate the Generalized Independent Noise (GIN) condition and show that it contains the independent noise condition as a special case. We further illustrate how GIN is applied to identify causal relations between latent variables of any two considered groups of observed variables. Finally, we present theoretical results regarding the graphical implications of the GIN condition, which can be used to discover latent variable structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Independent Noise Condition in Functional Causal Models</head><p>Below, we give the independent noise condition, which has been used in causal discovery of linear, non-Gaussian networks without confounders (e.g., in <ref type="bibr" target="#b18">Shimizu et al. [2011]</ref>). 3 Here, this assumption follows the definition in <ref type="bibr" target="#b19">Silva et al. [2006]</ref> and it is equivalent to say that there is no observed variable in X being an parent of any latent variables in L.  <ref type="bibr" target="#b3">[Cai et al., 2019]</ref> can be seen as a restrictive, special case of the GIN condition, where Dim(Y) = 2 and Dim(Z) = 1. We give an example to illustrate that there is a connection between this condition and the causal structure. According to the structure in Figure <ref type="figure" target="#fig_2">1</ref> and by assuming faithfulness, we have that ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition, as explained below. The causal models of latent variables is</p><formula xml:id="formula_2">L 1 = ε L 1 , L 2 = αL 1 + ε L 2 = αε L 1 + ε L 2 ,</formula><p>and</p><formula xml:id="formula_3">L 3 = βL 1 + σL 2 + ε L 3 = (β + ασ)ε L 1 + σε L 2 + ε L 3 , and {X 1 , X 2 , X 3 } and {X 4 , X 5 } can then be represented as ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X 1 X 2 X 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ a 1 b 1 a 2 b 2 a 3 b 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L 1 L 2 + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε X 1 ε X 2 ε X 3 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y , X 4 X 5 Z = a 4 b 4 βc 1 σc 1 L 1 L 2 + ε X 4 ε X ′ 5 E Z ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">ε X ′ 5 = c 2 ε L 3 + ε X 5 . According to the above equations, ω ⊺ E[YZ ⊺ ] = 0 ⇒ ω = [a 2 b 3 -b 2 a 3 , b 1 a 3 -a 1 b 3 , a 1 b 2 -b 1 a 2 ] ⊺ . Then we can see E Y||Z = ω ⊺ Y = ω ⊺ E Y ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and fur-</head><p>ther because E Y ⫫ Z, we have E Y||Z ⫫ Z. That is to say, ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition. Intuitively, we have E Y||Z ⫫ Z because although {X 1 , X 2 , X 3 } were generated by {L 1 , L 2 }, which are not measurable, E Y||Z , as a particular linear combination of Y = {X 1 , X 2 , X 3 }, successfully removes the influences of {L 1 , L 2 } by properly making use of Z = {X 4 , X 5 } as a "surrogate".</p><p>Next, we discuss a situation where GIN is violated. For example, in this structure, ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) violates GIN. Specifically, the corresponding variables satisfy the following equations:</p><formula xml:id="formula_5">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X 1 X 2 X 5 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ a 1 b 1 a 2 b 2 βc 1 σc 2 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L 1 L 2 + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε X 1 ε X 2 ε X ′ 5 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y , X 3 X 6 Z = a 3 b 3 βc 2 σc 2 L 1 L 2 + ε X 3 ε X ′ 6 E Z ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">ε X ′ 6 = c 2 ε L 3 + ε X 6 . Then under faithfulness assumption, we can see ω T Y Z because E Y E Z (there exists common component ε L 3 for ε X ′ 5 and ε X ′ 6 ), no matter ω ⊺ E[YZ ⊺ ] = 0 or not.</formula><p>In Section 3.3, we will further investigate graphical implications of GIN in LiNGLaM. For the example given in Figure <ref type="figure" target="#fig_2">1</ref>, we have the following observation. ({X 4 , X 5 }, {X 1 , X 2 , X 3 }) satisfies the GIN condition, and</p><formula xml:id="formula_7">{L 1 , L 2 }, the latent common causes for {X 1 , X 2 , X 3 }, d-separate {X 1 , X 2 , X 3 } from {X 4 , X 5 }. In contrast, ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) violates GIN, and {X 1 , X 2 , X 5 } and {X 3 , X 6 } are not d-separated conditioning on {L 1 , L 2 }, the latent common causes of {X 1 , X 2 , X 5 }.</formula><p>The following theorem gives mathematical characterizations of the GIN condition, by providing sufficient conditions for when (Z, Y) satisfies the GIN condition. In the next subsection, we give its implication in LiNGLaM; thanks to the constraints implied by the LiNGLaM, one is able to provide sufficient graphical conditions for GIN to hold.</p><p>Theorem 1. Suppose that random vectors L, Y, and Z are related in the following way: Theorem 1 gives the mathematical conditions, under which (Z, Y) satisfies the GIN condition. Continue the example in Figure <ref type="figure" target="#fig_2">1</ref>. Let Z = {X 4 , X 5 } and Y = {X 1 , X 2 , X 3 }, and thus L = {L 1 , L 2 }.</p><formula xml:id="formula_8">Y = AL + E Y , (4) Z = BL + E Z . (5) Denote by l the dimensionality of L. Assume A is of full column rank. Then, if 1) Dim(Y) &gt; l, 2) E Y ⫫ L, 3) E Y ⫫ E Z , 5<label>and</label></formula><p>One then can find the following facts:</p><formula xml:id="formula_9">Dim(Y) = 2 &gt; l, E Y ⫫ L and E Y ⫫ E Z according to Eq.</formula><p>2, and</p><formula xml:id="formula_10">Σ LZ = E[LZ ⊺</formula><p>] has full row rank, i.e., 2. Therefore, (Z, Y) satisfies the GIN condition.</p><p>All proofs are given in Supplementary Material.</p><p>The following proposition shows that the IN condition can be seen as a special case of the GIN condition with E Z = 0 (i.e., Z and L are linearly deterministically related).</p><p>Proposition 2. Let Ÿ ≔ (Y, Z). Then the following statements hold:</p><formula xml:id="formula_11">1. (Z, Ÿ ) follows the GIN condition if and only if (Z, Y ) follows it. 2. If (Z, Y ) follows the IN condition, then (Z, Ÿ ) follows the GIN condition.</formula><p>Proposition 2 inspires a unified method to handle causal relations between latent variables and those between latent and observed variables. Please see the discussion in Section 6 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graphical Criteria of GIN in Terms of LiNGLaM</head><p>In this section, we investigate graphical implications of the GIN condition in LiNGLaM, which then inspires us to exploit the GIN condition to discover the graph containing latent variables. Specifically, first, the following theorem shows the connection between GIN and the graphical properties of the variables in terms of LiNGLaM. We denote by L(X q ) the set of latent variables that are the parents of X q and by L(Y) the set of latent variables that are parents of any component of Y. We say variable set S 1 is an exogenous set relative to variable set S 2 if and only if 1) S 2 ⊆ S 1 or 2) for any variable V that is in S 2 but not in S 1 , according to the causal graph over {V } ∪ S 1 and the ancestors of variables in {V } ∪ S 1 , V does not cause any variable in S 1 , and the common cause for V and each variable in S 1 , if there is any, is also in S 1 (i.e., relative to {V } ∪ S 1 , V does not cause and is not confounded with any variable in S 1 ). For instance, according to the structure in Figure <ref type="figure" target="#fig_2">1</ref>, let S 1 = {L 1 } and S 2 = {L 3 , L 4 }, S 1 is an exogenous set relative to S 2 . In constrast, if S 1 = {L 2 , L 3 } and S 2 = {L 3 , L 4 }, S 1 is not an exogenous set relative to S 2 , because L 4 , which is in S 2 but not in S 1 , and L 2 (as well as L 3 ), which is in S 1 , has a common cause, L 1 , that is not in S 1 .</p><p>Theorem 2. Let Y and Z be two disjoint subsets of the observed variables of a LiNGLaM. Assume faithfulness holds for the LiNGLaM. (Z, Y) satisfies the GIN condition if and only if there exists a k-size subset of the latent variables Roughly speaking, S 1 is an exogenous set relative to S 2 if S 1 contains causally earlier variables (according to the causal order) in or before S 2 . Hence, intuitively, the theorem states that (Z, Y) satisfies the GIN condition when causally earlier common causes of Y d-separate Y from Z. We can then see the asymmetry of this condition for (Z, Y) relative to L(Y) and L(Z). For instance, assuming faithfulness, according to the structure in Figure <ref type="figure" target="#fig_2">1</ref></p><formula xml:id="formula_12">L, 0 ≤ k ≤ min(Dim(Y) -1, Dim(Z)), denoted by S k L , such that 1) S k L is an exogenous set relative to L(Y), that 2) S k L d-separates Y from Z,</formula><formula xml:id="formula_13">, ({X 1 , X 2 }, {X 3 , X 4 , X 5 }) satisfies GIN (with S 2 L = {L 1 , L 2 }), but ({X 1 , X 6 }, {X 3 , X 4 , X 5 }) does not.</formula><p>Next, we discuss how to identify the group of observed variables that share the same set of latent direct causes; we call such a set of observed variables a causal cluster. The following theorem formalizes the property of causal clusters and gives a criterion for finding such causal clusters.</p><p>Theorem 3. Let X be the set of all observed variables in a LiNGLaM and Y be a proper subset of X. If (X \ Y, Y) follows the GIN condition and there is no subset Ỹ ⊆ Y such that (X \ Ỹ, Ỹ) follows the GIN condition, then Y is a causal cluster and Dim(L(Y)) = Dim(Y) -1.</p><p>Consider the example in Figure <ref type="figure" target="#fig_2">1</ref>, for {X 5 , X 6 }, one can find ({X 1 , ..., X 4 , X 7 , X 8 }, {X 5 , X 6 }) follows the GIN condition, so {X 5 , X 6 } is a causal cluster and Dim(L({X 5 , X 6 })) = Dim({X 5 , X 6 }) -1 = 1(i.e., L 3 ). But, for {X 1 , X 2 , X 5 }, ({X 3 , X 4 , X 6 , X 7 , X 8 }, {X 1 , X 2 , X 5 }) violates the GIN condition, thus {X 1 , X 2 , X 5 } is not a causal cluster. Furthermore, we discuss how to identify the causal direction between latent variables based on their corresponding children. The following theorem shows the asymmetry between the underlying latent variables in terms of the GIN condition.</p><p>Theorem 4. Let S p and S q be two causal clusters of a LiNGLaM. Assume there is no latent confounder behind L(S p ) and L(S q ), and L(S p ) ∩ L(S q ) = ∅. Further suppose that S p contains 2Dim(L(S p )) number of variables with S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) } and that S q contains 2Dim(L(S q )) number of variables with</p><formula xml:id="formula_14">S q = {Q 1 , Q 2 , ..., Q 2Dim(L(S q )) }. Then if ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , Q 1 , ...Q Dim(L(S q )) }) follows the GIN condi- tion, L(S p ) → L(S q ) holds.</formula><p>Consider the example in Figure <ref type="figure" target="#fig_2">1</ref>. For two clusters {X 1 , X 2 , X 3 , X 4 } and {X 5 , Y 6 }, where their sets of latent direct causes do not have confounders, one can find</p><formula xml:id="formula_15">({X 3 , X 4 }, {X 1 , X 2 , X 5 }) follows GIN condition, so {L 1 , L 2 } → {L 3 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GIN Condition-Based Algorithm for Estimating LiNGLaM</head><p>In this section, we leverage the above theoretical results and propose a recursive algorithm to discover the structural information of LiNGLaM. The basic idea of the algorithm is that it first finds all causal clusters from the observed data (Step 1), and then it learns the causal order of the latent variables behind these causal clusters (Step 2). The completeness of the algorithm is shown in sections 4.1 (Theorem 3 and Proposition 3 for step 1) and 4.2 (Proposition 4 for step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Step 1: Finding Causal Clusters</head><p>To find causal clusters efficiently, one may start with finding clusters with a single latent variable and merge the overlapping culsters, and then increase the number of allowed latent variables until all variables are put in the clusters. We need to consider two practical issues involved in the algorithm. The first is how to find causal clusters and determine how many latent variables they contain, and the second is what clusters should be merged. Theorem 3 answers the first question. Next, for the merge problem, we find that the overlapping clusters can be directly merged into one cluster. This is because the overlapping clusters have the same latent variable as parents in LiNGLaM. The validity of the merging step is guaranteed by Proposition 3, with the algorithm given in Algorithm 1.</p><p>Proposition 3. Let S 1 and S 2 be two clusters of a LiNGLaM and Dim(L(S 1 )) = Dim(L(S 2 )). If S 1 and S 2 are overlapping, S 1 and S 2 share the same set of latent variables as parents.  until all subsets with length Len in P have been selected; To test the independence (line 5 in Algorithm 1) between two sets of variables, we check for the pairwise independence with the Fisher's method <ref type="bibr">[Fisher, 1950]</ref> instead of testing for the independence between E Y||Z and Z directly. In particular, denote by p k , with k = 1, 2, ..., c, all resulting p-values from pairwise independence tests. We compute the test statistic as -2 ∑ c k=1 log p k , which follows the chi-square distribution with 2c degrees of freedom when all the pairs are independent.</p><p>Example 1. Consider the example in Figure <ref type="figure" target="#fig_2">1</ref>. First, we set Len = 1 to find the clusters with a single latent variable, i.e., we find {X 5 , X 6 } and {X 7 , X 8 } based on Theorem 3 (Line 4-9). Then we set Len = 2 and find the clusters {X 1 , X 2 , X 3 , X 4 } with two latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>Step 2: Learning the Causal Order of Latent Variables After identifying all clusters, next, we aim to discover the causal order of the set of latent variables of corresponding causal clusters. As an immediate consequence of Theorem 4, the root latent variable can be identified by checking the GIN condition, as stated in the following lemma.</p><p>Lemma 1. Let S r be a cluster and S k , k ≠ r, be any other cluster of a LiNGLaM. Suppose that S r contains 2Dim(L(S r )) number of variables with S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) } and that S k contains 2Dim(L(S k )) number of variables with</p><formula xml:id="formula_16">S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) }. if ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condi- tion, then L(S r ) is a root latent variable set.</formula><p>Now, the key issue is how to use this lemma to recursively discover the "root variable"<ref type="foot" target="#foot_2">foot_2</ref> until the causal order of latent variables is fully determined. Interestingly, we find that in every iteration, we only need to add the children (i.e., the corresponding causal cluster) of the root variable set into the testing set, such that the number of testing latent variables increases when testing the GIN condition in the following steps. Recall the example discussed in Figure <ref type="figure" target="#fig_2">1</ref>. For L 3 , we find that ({X 6 , X 3 , X 4 }, {X 5 , X 7 , X 1 , X 2 }) satisfies the GIN condition, while for L 4 , ({X 8 , X 3 , X 4 }, {X 5 , X 7 , X 1 , X 2 }) violates the GIN condition,<ref type="foot" target="#foot_3">foot_3</ref> which means that L 3 is the "root variable". Intuitively speaking, adding the children of the root variable includes the information of the root variable set and create a new "root variable", which helps further remove the effect from them. Accordingly, we have the following proposition to guarantee the correctness of the above process. The details of the process are given in Algorithm 2. T = T ∪ S r ; 7: end while 8: Return: Causal order K Example 2. Continue to consider the example in Figure <ref type="figure" target="#fig_2">1</ref>. We have found the three causal clusters in step 1, i.e., S 1 = {X 1 , X 2 , X 3 }, S 2 = {X 5 , X 6 }, and S 3 = {X 7 , X 8 }. Now, we first find that L(S 1 ) is the root variable because ({X 3 , X 4 }, {X 1 , X 2 , X 5 }) and ({X 3 , X 4 }, {X 1 , X 2 , X 7 }) both satisfy the GIN condition (Line 3). Next, we find L(S 2 ) is the "root variable" because ({X 6 , X 3 , X 4 }, {X5, X7, X 1 , X 2 }) satisfies the GIN condition . Finally, we return the causal order K ∶ L(S 1 ) ≻ L(S 2 ) ≻ L(S 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>To show the efficacy of the proposed approach, we applied it to both synthetic and real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data</head><p>In the following simulation studies, we consider four typical cases: Case 1 &amp; Case 2 have two latent variables L 1 and L 2 , with L 1 → L 2 ; Case 3 has three latent variables L 1 , L 2 , and L 3 , with L 2 ← L 1 → L 3 , and L 2 → L 3 ; Case 4 has four latent variables {L 1 , L 2 }, L 3 , and <ref type="table"></ref>and<ref type="table" target="#tab_7">L 3 → L 4</ref> . In all four cases, the data are generated by LiNGLaM and the causal strength b is sampled from a uniform distribution between [-2, -0.5] ∪ [0.5, 2], noise terms are generated from uniform[-1,1] variables to the fifth power, and the sample size N = 500, 1000, 2000. The details of the graph structures are as follows. [Case 1]: Both L 1 and L 2 have two pure observed variables, i.e., L 1 → {X 1 , X 2 } and L 2 → {X 3 , X 4 }. [Case 2]: Add extra edges to the graph in Case 1, such that there exist multiple latent variables. In particular, we add two new variables {X 5 , X 6 }, such that {L1, L2} → {X 5 , X 6 }, and add the edge L 1 → {X 3 , X 4 }. [Case 3]: Each latent variable has three pure observed variables, i.e., L 1 → {X 1 , X 2 , X 3 }, L 2 → {X 4 , X 5 , X 6 }, and L 3 → {X 7 , X 8 , X 9 }. [Case 4]: Add extra latent variables and adjust the observed variables in Case 3 such that it becomes the structure in Figure <ref type="figure" target="#fig_2">1</ref>.</p><formula xml:id="formula_17">L 4 , with {L 1 , L 2 } → L 3 , {L 1 , L 2 } → L 4 ,</formula><p>We compared our algorithm with BPC <ref type="bibr" target="#b19">[Silva et al., 2006]</ref>, FOFC <ref type="bibr" target="#b14">[Kummerfeld and Ramsey, 2016]</ref>, 8 and LSTC <ref type="bibr" target="#b3">[Cai et al., 2019]</ref>. We measured the estimation accuracy on two tasks: 1) finding the causal clusters, i.e., locating latent variables, and 2) discovering the causal order of latent variables. Note that BPC and FOFC are only applicable to the first task.</p><p>To evaluate the accuracy of the estimated causal cluster, we followed the evaluation metrics from <ref type="bibr" target="#b3">Cai et al. [2019]</ref>. Specifically, we used Latent omission= OL T L , Latent commission= F L T L , and Mismea-surement= MO  T O , where OL is the number of omitted latent variables, F L is the number of falsely detected latent variables, T L is the total number of latent variables in the ground truth graph, M O is the number of falsely observed variables that have at least one incorrectly measured latent, and T O is the number of observed variables in the ground truth graph. To better evaluate the quality of the estimated causal order, we further used the correct-ordering rate as a metric. Each experiment was repeated 10 times with randomly generated data and the results were averaged. Here, we used the Hilbert-Schmidt Independence Criterion (HSIC) test <ref type="bibr" target="#b9">[Gretton et al., 2008]</ref> for the independence test because the data are non-Gaussian. 0.00(0) 0.00(0) 1.00(10) 0.50(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) Case 1 1000 0.00(0) 0.00(0) 1.00(10) 0.50(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 2000 0.00(0) 0.00(0) 1.00(10) 0.50(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 500 0.10(2) 0.20(4) 0.9(10) 0.50(10) 0.00(0) 0.05(1) 0.00(0) 0.00(0) 0.12(2) 0.12(4) 0.00(0) 0.20(10) Case 2 1000 0.05(1) 0.15(3) 1.00(10) 0.50(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.04(1) 0.12(3) 0.00(0) 0.20(10) 2000 0.00(0) 0.00(0) 1.00(10) 0.50(10) 0.00(0) 0.02(2) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.20(10) 500 0.20(3) 0.20(3) 0.13(9) 0.10(1) 0.00(0) 0.03(3) 0.00(0) 0.00(0) 0.19(3) 0.17(3) 0.00(0) 0.00(0) Case 3 1000 0.06(2) 0.13(2) 0.16(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.06(2) 0.00(0) 0.00(0) 0.00(0) 2000 0.00(0) 0.00(0) 0.50(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 500 0.13(4) 0.40(6) 0.90(10) 0.63(10) 0.00(0) 0.23(5) 0.00(0) 0.00(0) 0.04(2) 0.15(6) 0.02(2) 0.06(4) Case 4 1000 0.10(3) 0.26(6) 0.93(10) 0.66(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.05(3) 0.11(2) 0.01(1) 0.02(2) 2000 0.03(1) 0.32(6) 1.00(10) 0.70(10) 0.00(0) 0.00(0) 0.00(0) 0.00(0) 0.04(1) 0.11(3) 0.00(10) 0.00(0)</p><p>Note: The number in parentheses indicates the number of occurrences that the current algorithm cannot correctly solve the problem.</p><p>As shown in Table <ref type="table" target="#tab_4">1</ref>, our algorithm, GIN, achieves the best performance (the lowest errors) on almost all cases of the structures. We noticed that although the Mismeasurements of GIN are higher than LSTC in Case 3 when the sample size is small (N=500), the Latent commission of GIN are lower than LSTC. The BPC and FOFC algorithms (with distribution-free tests) do not perform well, which implies that the rank constraints on covariance matrix is not enough to recover more latent structures. Interestingly, although the LSTC algorithm has low errors of the Latent omission in Case 2 (it may be because the structure in Case 2 can be transformed into equivalent pure structures <ref type="bibr" target="#b3">[Cai et al., 2019]</ref>), it can not tell us the number of latent variables behind observed variables. Moreover, LSTC Considering that BPC and FOFC algorithms can not discover the causal directions of latent variables, we only reported the results of LSTC algorithm and our algorithm on causal order learning in Figure <ref type="figure">2</ref>. As shown in Figure <ref type="figure">2</ref>, the accuracy of the identified causal ordering of our method gradually increases to 1 with the sample size in all the four cases. LSTC can not handle Case 2 &amp; 4. These findings illustrate that our algorithm can discover the correct causal order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-World Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Clusters</head><p>Observed variables Barbara Byrne conducted a study to investigate the impact of organizational (role ambiguity, role conflict, classroom climate, and superior support, etc.) and personality (selfesteem, external locus of control) on three facets of burnout in full-time elementary teachers <ref type="bibr" target="#b2">[Byrne, 2010]</ref>. We applied our algorithm to this data set, with 28 observed variables in total.</p><formula xml:id="formula_18">S 1 (1) RC 1 , RC 2 , W O 1 , W O 2 , DM 1 , DM 2 S 2 (1) CC 1 , CC 2 ,CC 3 ,CC 4 S 3 (1) P S 1 , P S 2 S 4 (1) ELC 1 , ELC 2 ,ELC 3 ,ELC 4 , ELC 5 S 5 (2) SE 1 , SE 2 , SE 3 , EE 1 , EE 2 , EE 3 , DP 1 , P A 3 S 6 (3) DP 2 , P A 1 , P A 2</formula><p>In the implementation, the kernel width in the HSIC test is set to 0.05. We first applied Algorithm 1 and received six causal clusters, including one cluster with 2 latent variables and one cluster with 3 latent variables. The results were given in Figure <ref type="figure" target="#fig_7">3</ref>. Next, we applied Algorithm 2 and got the final causal order (from root to leaf):</p><formula xml:id="formula_19">L(S 1 ) ≻ L(S 2 ) ≻ L(S 3 ) ≻ L(S 5 ) ≻ L(S 4 ) ≻ L(S 6</formula><p>). Specifically, we had the following findings. 1. The identified clusters are similar to the domain knowledge, e.g, S 2 represents the classroom climate, S 3 represents the peer support, S 4 represents the external locus of control, et al.</p><p>2. The learned causal order is similar to Byrne's conclusion, e.g., personal accomplishment (L(S 6 )) are caused by other latent factors. In addition, role conflict and decision making (L(S 1 )), classroom climate (L(S 2 )), and peer support (L(S 3 )) cause burnout (including emotional exhaustion, depersonalization, and personal accomplishment (L(S 5 ) and L(S 6 ))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Further Work</head><p>The preceding sections presented how to use GIN conditions to locate the latent variables and identify their causal structure in the LiNGLaM. In this procedure we examine whether the ordered pair of two disjoint subsets of the observed variables satisfies GIN. As shown in Proposition 2, the GIN condition actually contains IN as a special case, in which the two subsets of variables have overlapping variables. For instance, suppose we have only two variables with X 1 → X 2 . Then (X 1 , X 2 ) satisties IN, and (X 1 , (X 2 , X 1 )) satisfies GIN. As a consequence, interestingly, even if we allow edges between observed variables in the LiNGLaM, the GIN condition may also be used to identify them, together with their connections. For instance, in Figure <ref type="figure" target="#fig_5">4</ref>(a), ({X 1 , X 3 }, {X 2 , X 3 , X 4 }) satisfies the GIN condition while ({X 1 , X 4 }, {X 2 , X 3 , X 4 }) violates the GIN condition, which means that there is an edge between X 3 and X 4 and X 3 → X 4 . In contrast, with the GIN condition on pairs of disjoint subsets of variables, one cannot distinguish between structures (a) and (b). Developing an efficient algorithm that is able to recover the LiNGLaM with directed edges between observed variables in a principled way is part of our future work. Furthermore, in this paper we focus on discovery of the structure of the LiNGLaM, more specifically, the locations of the latent variables and their causal order; as future work, we will also show the (partial) identifiability of the causal coefficients in the model and develop an estimation method for them, to produce a fully specified estimated LiNGLaM (further with edges between observed variables).</p><formula xml:id="formula_20">L 1 X 1 X 2 X 3 X 4 L 1 L 2 X 1 X 2 X 3 X 4</formula><p>(a) (b) Figure <ref type="figure" target="#fig_5">4</ref>: Two structures that are distinguishable by the GIN condition, while (a) has an edge between observed variables X 3 and X 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a Generalized Independent Noise (GIN) condition for estimating a particular type of linear non-Gaussian latent variable causal model, which includes the Independent Noise (IN) condition as a special case. We showed the graphical implications of the GIN condition, based on which we proposed a recursive learning algorithm to locate latent causal variables and identify their causal structure. Experimental results on simulation data and real data further verified the usefulness of our algorithm. Future research along this line includes allowing casual edges between observed variables and allowing nonlinear causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs and Illustrations</head><p>We first give an important theorem, which will be used in the proof. <ref type="bibr">Darmois-Skitovitch Theorem [Kagan et al., 1973]</ref> Define two random variables X 1 and X 2 as linear combinations of independent random variables e i (i = 1, ..., p):</p><formula xml:id="formula_21">X 1 = p i=1 α i e i , X 2 = q i=1 β i e i .<label>(6)</label></formula><p>Then, if X 1 and X 2 are independent, all variables e j for which α j β j ≠ 0 are Gaussian. In other words, if there exists a non-Gaussian e j for which α j β j ≠ 0, X 1 and X 2 are dependent.</p><p>A.1 Proof of Proposition 1 Proposition 1. Suppose all considered variables follow the linear non-Gaussian acyclic causal model. Let Z be a subset of those variables and Y be a single variable among them. Then the following statements are equivalent.</p><p>(A) 1) All variables in Z are causally earlier than Y , and 2) There is no common cause for each variable in Z and Y that is not in Z.</p><formula xml:id="formula_22">(B) (Z, Y ) satisfies the IN condition.</formula><p>The proof is straightforward, based on the assumption of linear, non-Gaussian acyclic causal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 1</head><p>Theorem 1. Suppose that random vectors L, Y, and Z are related in the following way:</p><formula xml:id="formula_23">Y = AL + E Y ,<label>(7)</label></formula><formula xml:id="formula_24">Z = BL + E Z . (<label>8</label></formula><formula xml:id="formula_25">)</formula><p>Denote by l the dimensionality of L. Assume A is of full column rank. Then, if 1) Dim(Y) &gt; l, 2) Proof. Without loss of generality, assume that each component of L has a zero mean, and that both E Y and E Z are zero-mean. If we can find a non-zero vector ω such that ω</p><formula xml:id="formula_26">E Y ⫫ L, 3) E Y ⫫ E Z , 9<label>and</label></formula><formula xml:id="formula_27">⊺ A = 0, then ω ⊺ Y = ω ⊺ AL + ω ⊺ E Y = ω ⊺ E Y ,</formula><p>which will be independent from Z in light of conditions 2) and 3), i.e., the GIN condition for Y given Z holds true. We now construct the vector ω. If conditions 2) and 3) hold, we have E[YZ ⊺ ] = AΣ LZ , which is determined by (Y, Z). We now show that under conditions 4), for any non-zero vector ω, ω</p><formula xml:id="formula_28">⊺ A = 0 if and only if ω ⊺ AΣ LZ = 0 or equivalently ω ⊺ E[YZ ⊺</formula><p>] = 0 and that such a vector ω exists.</p><p>Suppose ω ⊺ A = 0, it is trivial to see ω ⊺ AΣ LZ = 0. Notice that condition 4) implies that rank(AΣ LZ ) ≤ l because rank(AΣ LZ ) ≤ min(rank(A), rank(Σ LZ )) and rank(A) = l. Further according to Sylvester Rank Inequality, we have rank(AΣ LZ ) ≥ rank(A) + rank(Σ LZ ) -l = l. Therefore, rank(AΣ LZ ) = l. Because of condition 1), there must exists a non-zero vector ω, determined by (Y, Z), such that ω</p><formula xml:id="formula_29">⊺ E[YZ ⊺ ] = ω ⊺ AΣ LZ = 0. Moreover, this equality implies ω ⊺ A = 0</formula><p>because Σ LZ has l rows and has rank l. With this ω, we have E Y||Z = ω ⊺ E Y and is independent from Z. Thus the theorem holds. Proof. For Statement 1, we first show that (Z, Ÿ ) follows the GIN condition implies that (Z, Y ) follows the GIN condition. If (Z, Ÿ ) follows the GIN condition, then there must exist a non-zero vector ω so that ω⊺</p><formula xml:id="formula_30">E[ Ÿ Z ⊺ ] = 0. This equality implies ω⊺ E[ Y Z Z ⊺ ] = ω⊺ E[Y Z ⊺ ] E[ZZ ⊺ ] = 0. (9) Because E[ZZ ⊺ ] is non-singular, we further have ω⊺ E[Y Z ⊺ ]E -1 [ZZ ⊺ ] I = 0.</formula><p>Let ω be the first Dim(Y ) dimensions of ω. Then we have ω</p><formula xml:id="formula_31">⊺ E[Y Z ⊺ ]E -1 [ZZ ⊺ ] = 0, and thus ω ⊺ E[Y Z ⊺ ] = 0.</formula><p>Furthermore, based on the definition of the GIN condition, we have that</p><formula xml:id="formula_32">E Ÿ ||Z = ω⊺ Ÿ is independent from Z. It is easy to see that E Y ||Z = ω ⊺ Y is independent from Z. Thus, (Z, Y ) follows the GIN condition.</formula><p>Next, we show that (Z, Y ) follows the GIN condition implies that (Z, Ÿ ) follows the GIN condition. If (Z, Y ) follows the GIN condition, we have</p><formula xml:id="formula_33">ω ⊺ E[Y Z ⊺ ] = 0 (10) Let ω = [ω ⊺ , 0 ⊺ ] ⊺ . We have ω⊺ E[ Ÿ Z ⊺ ] = [ω ⊺ , 0 ⊺ ]E[ Y Z Z ⊺ ] = ω ⊺ E[Y Z ⊺ ] = 0.<label>(11)</label></formula><p>Furthermore, we have</p><formula xml:id="formula_34">ω⊺ Ÿ = [ω ⊺ , 0 ⊺ ] Y Z = ω ⊺ Y . Based on the definition of GIN, E Y ||Z = ω ⊺ Y</formula><p>is independent from Z. That is to say, ω⊺ E Ÿ is independent from Z. Thus, (Z, Ÿ ) follows the GIN condition.</p><p>For Statement 2, If (Z, Y ) follows the IN condition, we have</p><formula xml:id="formula_35">ω = E[Y Z ⊺ ]E -1 [ZZ ⊺ ]. (<label>12</label></formula><formula xml:id="formula_36">) Let ω = [1 ⊺ , -ω ⊺ ] ⊺ , we get ω⊺ E[ Ÿ Z ⊺ ] = [1 ⊺ , -ω ⊺ ]E[ Y Z Z ⊺ ] = [1 ⊺ , -ω ⊺ ] E[Y Z ⊺ ] E[ZZ ⊺ ] = E[Y Z ⊺ ] -ωE[ZZ ⊺ ].<label>(13)</label></formula><p>From Equations 12 and 13, we have ω⊺</p><formula xml:id="formula_37">E[ Ÿ Z ⊺ ] = 0. That is to say, ω satisfies ω⊺ E[ Ÿ Z ⊺ ] = 0 and that ω⊺ ≠ 0.</formula><p>Now, we show that ω⊺ Ÿ is independent from Z. We know that Y -ω⊺ Z is independent from Z based on the definition of the IN condition. It is easy to see that</p><formula xml:id="formula_38">ω⊺ Ÿ = [1 ⊺ , -ω ⊺ ] Y Z = Y - ω⊺ Z</formula><p>is independent from Z. Therefore, (Z, Ÿ ) follows the GIN condition. Proof. The "if" part: First suppose that there exists such a subset of the latent variables, S k L , that satisfies the three conditions. Because of condition 1), i.e., that S k L is an exogenous set relative to L(Y) and because according to the LiNGLaM, each Y i is a linear function of L(Y i ) plus independent noise, we know that S k L is also an exogenous set relative to Y. Hence, we know that each component of Y can be written as a linear function of S k L and some independent error (which is independent from S k L ). By a slight abuse of notation, here we use S k L also to denote the vector of the variables in S k L . Then we have</p><formula xml:id="formula_39">Y = AS k L + E ′ Y , (<label>14</label></formula><formula xml:id="formula_40">)</formula><p>where A is an appropriate linear transformation, E ′ Y is independent from S k L , but its components are not necessarily independent from each other. In fact, according to the LiNGLaM, each observed or hidden variable is a linear combination of the underlying noise terms ε i . In equation ( <ref type="formula" target="#formula_39">14</ref>), S k L and E ′ Y are linear combinations of disjoint sets of the noise terms ε i , implied by the directed acyclic structure over all observed and hidden variables.</p><p>Let us then write Z as linear combinations of the noise terms. We then show that because of condition 2), i.e., that S k L d-separates Y from Z, if any noise term ε i is present in E ′ Y , it will not be among the noise terms in the expression of Z. Otherwise, if Z j also involves ε i , then the direct effect of ε i , among all observed or hidden variables, is a common cause of Z j and some component of Y. This path between Z j and that component of Y, however, cannot be d-separated by S k L because no component of S k L is on the path, as implied by the fact that when S k L is written as a linear combination of the underlying noise terms, ε i is not among them. Consequently, any noise term in E ′ Y will not contribute to S k L or Z. Hence, we can express Z as The "only-if" part: Then we suppose (Z, Y) satisfies GIN (while with the same Z, no proper subset of Y does). Consider all sets S k L that are exogenous relative to L(Y) with k satisfying the condition in the theorem, and we show that at least one of them satisfies conditions 2) and 3). Otherwise, if 2) is always violated, then there is an open path between some leaf node in L(Y), denoted by L(Y k ), and some component of Z, denoted by Z j , and this open path does not go through any common cause of the variables in L(Y). Then they have some common cause that does not cause any other variable in L(Y). Consequently, there exists at least one noise term, denoted by ε i , that contributes to both L(Y k ) (and hence Y k ) and Z j , but not any other variables in Y. Because of the non-Gaussianity of the noise terms and Darmois-Skitovitch Theorem, if any linear projection of Y, ω ⊺ Y is independent from Z, the linear coefficient for Y k must be zero. Hence (Z, Y \ {Y k }) satisfies GIN, which contradicts the assumption in the theorem. Therefore, there must exists some S k L such that 2) holds. Next, if 3) is violated, i.e., the rank of the covariance matrix of S k L and Z is smaller than k. Then the condition ω ⊺ E[YZ ⊺ ] = 0 does not guarantee that ω ⊺ A = 0. Under the faithfulness assumptions, we then do not have that ω ⊺ Y is independent from Z. Hence, condition 3) also holds.</p><formula xml:id="formula_41">Z = BS k L + E ′ Z ,<label>(15) where E</label></formula><p>Remark. Roughly speaking, the conditions in this theorem can be interpreted the following way: i.) a causally earlier subset (according to the causal order) of the common causes of Y d-separate Y from Z, and ii.) the linear transformation from that subset of the common causes to Z has full column rank. For instance, for the structure in Figure <ref type="figure" target="#fig_2">1</ref> of the main paper, ({X 3 , X 4 }, {X 1 , X 2 , X 5 }) satisfies GIN, while ({X 3 , X 6 }, {X 1 , X 2 , X 5 }) does not-note that the difference is that in the latter case one of the variables in Z, X 6 , is not d-separated from a component of X, which is X 5 , given the common causes of X. However, when X 6 is replaced by X 4 in Z, whose direct cause is causally earlier, the d-separation relationship holds, and so is the GIN condition.</p><p>According to the linearity assumption, we have</p><formula xml:id="formula_42">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ P 1 ⋮ P m Q 1 ⋮ Q n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Y = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ C 11 ⋯ C m1 ⋮ ⋱ ⋮ C m1 ⋯ C mm D 11 ⋯ D n1 ⋮ ⋱ ⋮ D n1 ⋯ D nn ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ A ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ L p 1 ⋮ L p m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε P 1 ⋮ ε P m ε ′ Q 1 ⋮ ε ′ Q n ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Y (16) and ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ P m+1 ⋮ P 2m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ Z = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ B 11 ⋯ B m1 ⋮ ⋱ ⋮ B m1 ⋯ B mm ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ B ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ L p 1 ⋮ L p m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ L + ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ε P m+1 ⋮ ε P 2m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ E Z ,<label>(17)</label></formula><p>where ε</p><formula xml:id="formula_43">′ Q i = n ∑ k=1 f k ε L q k + ε Q i .</formula><p>Now, we verify conditions 1) ∼ 4) in Theorem 1. Based on Equations 16 and 17, we have</p><formula xml:id="formula_44">Dim(L) = m. For condition 1), Dim(Y) = m + n &gt; m. For condition 2), E Y = (ε P 1 , ..., ε P m , ε ′ Q 1 , ..., ε ′ Q n ) ⊺ is independent from L = {L p 1 , ..., L</formula><p>p m }, due to the fact that there is no common component between E Y and L and that each component is independent of each other. For condition 3), because ε</p><formula xml:id="formula_45">P k , k = 1, ..., 2m, is independent from L, E Z ⫫ L. For condition 4), Σ LZ = E[LZ ⊺ ] = Σ L B ⊺ . Because Dim(B) = m, we obtain that Σ LZ has rank m. Therefore, ({P m+1)+1 , ...P 2m }, {P 1 , ...., P m , Q 1 , ...Q n }) follows the GIN condition.</formula><p>Next, we consider case 2: L(S p ) ← L(S q ). According to the definition of the GIN condition, we need to find a vector ω ≠ 0 such that ω </p><formula xml:id="formula_46">, ...Q n }) violates the GIN condition. Therefore, L(S p ) → L(S q ).</formula><p>A.7 Proof of Proposition 3 Proposition 3. Let S 1 and S 2 be two clusters of a LiNGLaM and Dim(L(S 1 )) = Dim(L(S 2 )). If S 1 and S 2 are overlapping, S 1 and S 2 share the same set of latent variables.</p><p>Proof. Because S 1 and S 2 are overlapping, with loss of generality, assume that the shared element of S 1 and S 2 is X k . Furthermore, we have that L(S 1 ) and L(S 2 ) are both parents of X k . Based on the definition of causal cluster and that Dim(L(S 1 )) = Dim(L(S 2 )), we have L(S 1 ) = L(S 2 ). That is to say, S 1 and S 2 share the same set of latent variables.</p><p>A.8 Proof of Lemma 1 Lemma 1. Let S r be a cluster and S k , k ≠ r be any other cluster of a LiNGLaM. Suppose that S r contains 2Dim(L(S r )) number of variables with S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) } and that S k contains 2Dim(L(S k )) number of variables with S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) }. if ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition, then L(S r ) is a root latent variable set.</p><p>Proof. (i) Assume that L(S r ) is a root latent variable set. Due to the linearity assumption, there is no latent confounder between L(S r ) and another latent variable set. Based on Theorem 4, we have that ({R |L(S r )| , ...R 2Dim(L(S r )) , {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition.</p><p>(ii) Assume that L(S r ) is not a root latent variable set, that is , L(S r ) has at least one parent set. Let L(S p ) be the parent of L(S r ) and S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) }. Thus, every element in {P 1 , P 2 , ..., P 2Dim(L(S p )) } has the component ε L(S p ) . Based on the definition of the GIN condition, we easily obtain that there is no ω ≠ 0 such that ω From (ii), the lemma is proven.</p><p>Moreover, from (i) and (ii), we show that ({R |L(S r )| , ...R 2Dim(L(S r )) }, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) }) follows the GIN condition, if and only if L(S r ) is a root latent variable set.</p><p>A.9 Proof of Proposition 4 Proposition 4. Suppose that {S 1 , ...S i , ..., S n } are all the clusters of the LiNGLaM. Denote T = {L(S 1 ), ...L(S i )} and T = {L(S i+1 ), ...L(S n )}, where all elements in T are causally earlier than those in R. Let Ẑ contain the elements from the half children of each latent variable set in T, and Ŷ contain the elements from the other half children of each latent variable set in T. Furthermore, Let L(S r ) be a latent variable set of R and S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) }. If for any one of the remaining S k ∈ R, k ≠ r and S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) } such that ({R Dim(L(S r ))+1 , ..., R 2Dim(L(S r )) , Ẑ}, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) , Ŷ}) follows GIN condition, then L(S r ) is a root latent variable set in R.</p><p>Proof. One may treat the causally earlier sets as a new group. Then one can easily prove this result according to Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More experimental results of Synthetic data</head><p>Here, we add more results to show the performance of our algorithm for random generated graphs and more variables. In details, we generated graphs randomly with different numbers of latent variables, where each latent variable only have three observed variables. We run our method and obtain the following results in Table <ref type="table" target="#tab_6">2</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4) The cross-covariance matrix of L and Z, Σ LZ = E[LZ ⊺ ] has rank l, then E Y||Z ⫫ Z, i.e., (Z, Y) satisfies the GIN condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and that 3) the covariance matrix of S k L and Z has rank k, and so does that of S k L and Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Identifying Causal Clusters Input: Data set X = {X 1 , ..., X m } Output: Causal cluster set S 1: Initialize S = ∅, Len = 1, and P = X; subset P from P such that Dim(P) = Len; 5: if E P||(P\P) ⫫ (P \ P) holds then 6: S = S ∪ P;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>overlapping sets in S; 10: P ← P \ S, and Len ← Len + 1; 11: until P is empty or Dim(P) ≤ Len; 12: Return: S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 4 .</head><label>4</label><figDesc>Suppose that {S 1 , ...S i , ..., S n } contains all clusters of the LiNGLaM. Denote T = {L(S 1 ), ...L(S i )} and R = {L(S i+1 ), ...L(S n )}, where all elements in T are causally earlier than those in R. Let Ẑ contain the elements from the half set of the children of each latent variable set in T, and Ŷ contain the elements from the other half set of the children of each latent variable set in R. Furthermore, Let L(S r ) be a latent variable set of R and S r = {R 1 , R 2 , ..., R 2Dim(L(S r )) }. If for any one of the remaining elements L(S k ) ∈ R, with k ≠ r and S k = {K 1 , K 2 , ..., K 2Dim(L(S k )) } such that ({R Dim(L(S r ))+1 , ...R 2Dim(L(S r )) , Ẑ}, {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) , Ŷ}) follows the GIN condition, then L(S r ) is a root latent variable set in R. Algorithm 2 Learning the Causal Order of Latent Variables Input: Set of causal clusters S Output: Causal order K 1: Initialize L with the root variable sets of each cluster, T = ∅, and K = ∅; 2: while L ≠ ∅ do 3:Find the root node L(S r )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 2: (a-d) Accuracy of the estimated causal order with GIN (purple), and LSTC (green) for Cases 1-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The output of Algorithm 1 in the teacher's burnout study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>4) The cross-covariance matrix of L and Z, Σ LZ = E[LZ ⊺ ] has rank l, then E Y||Z ⫫ Z, i.e., (Z, Y) satisfies the GIN condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A. 3</head><label>3</label><figDesc>Proof of Proposition 2 Proposition 2. Let Ÿ ≔ (Y, Z). Then the following statements hold: 1. (Z, Ÿ ) follows the GIN condition if and only if (Z, Y ) follows it. 2. If (Z, Y ) follows the IN condition, then (Z, Ÿ ) follows the GIN condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>A. 4</head><label>4</label><figDesc>Proof of and Remark on Theorem 2 Theorem 2. Let Y and Z be two disjoint sets of observed variables of a LiNGLaM. Assume faithfulness holds for the LiNGLaM. (Z, Y) satisfies the GIN condition if and only if there exists a k-size subset of the latent variables L, 0 ≤ k ≤ min(Dim(Y) -1, Dim(Z)), denoted by S k L , such that 1) S k L is an exogenous set relative to L(Y), that 2) S k L d-separates Y from Z, and that 3) the covariance matrix of S k L and Z has rank k, and so does that of S k L and Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>′Z</head><label></label><figDesc>, which is determined by S k L and Z, is independent from E ′ Y . Further considering condition on the dimensionality of S k L and condition 3), one can see that the assumptions in Theorem 1 are satisfied. Therefore, (Z, Y) satisfies the GIN condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>⊺ E[{P 1 , ...., P Dim(L(S p )) , R 1 , ...R Dim(L(S r )) }), ({PDim(L(S p ))+1 , ...R 2Dim(L(S p )) ) ⊺ ] = 0 because the dimension of ε L(S p ) in {R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) } equals Dim(L(S r )). That is to say, ω ⊺ (R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k )) ) must have the component ε L(S p ) . Thus, ω ⊺ (R 1 , ...., R Dim(L(S r )) , K 1 , ...K Dim(L(S k ))) is dependent on(P Dim(L(S p ))+1 , ...R 2Dim(L(S p )) )⊺ based on the Darmois-Skitovitch Theorem. Therefore, ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , R 1 , ...R Dim(L(S r )) }) violates the GIN condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Definition 1 (Linear Non-Gaussian Latent Variable Model (LiNGLaM)). A LiNGLaM, besides linear and acyclic assumptions, has the following assumptions:A1. [Measurement Assumption] There is no observed variable in X being an ancestor of any latent variables in L.</figDesc><table><row><cell>3</cell></row><row><cell>A2. [Non-Gaussianity Assumption] The noise terms are non-Gaussian.</cell></row><row><cell>A3. [Double-Pure Child Variable Assumption] Each latent variable set L ′ , in which every latent</cell></row><row><cell>variable directly causes the same set of observed variables, has at least 2Dim(L ′</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>In other words, (Z, Y) violates the GIN condition if and only if E Y||Z is dependent on Z. Notice that the Triad condition</figDesc><table><row><cell cols="2">Proposition 1. Suppose all considered variables follow the linear non-Gaussian acyclic causal</cell></row><row><cell cols="2">model and are observed. Let Z be a subset of those variables and Y be a single variable. Then the</cell></row><row><cell cols="2">following statements are equivalent.</cell></row><row><cell cols="2">(A) 1) All variables in Z are causally earlier than Y , and 2) there is no common cause for each</cell></row><row><cell cols="2">variable in Z and Y that is not in Z. (B) (Z, Y ) satisfies the IN condition.</cell></row><row><cell cols="2">3.2 Generalized Independent Noise Condition</cell></row><row><cell cols="2">Below, we first give the definition of the GIN condition, followed by an illustrative example.</cell></row><row><cell cols="2">Definition 3 (GIN condition). Let Y and Z be two observed random vectors. Suppose the variables</cell></row><row><cell cols="2">follow the linear non-Gaussian acyclic causal model. Define the surrogate-variable of Y relative to</cell></row><row><cell cols="2">Z, as where ω satisfies ω only if E Y||Z is independent from Z. ⊺ E[YZ ⊺ ] = 0 and ω ≠ 0. We say that (Z, Y) follows the GIN condition if and E Y||Z ≔ ω ⊺ Y, (1)</cell></row><row><cell>4 2Dim(L ′</cell><cell>) denotes 2 times the dimension of L ′ .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>results by GIN, LSTC, FOFC, and BPC on learning causal clusters.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Latent omission</cell><cell></cell><cell></cell><cell cols="2">Latent commission</cell><cell></cell><cell></cell><cell cols="2">Mismeasurements</cell><cell></cell></row><row><cell>Algorithm</cell><cell>GIN</cell><cell>LSTC</cell><cell>FOFC</cell><cell>BPC</cell><cell>GIN</cell><cell>LSTC</cell><cell>FOFC</cell><cell>BPC</cell><cell>GIN</cell><cell>LSTC</cell><cell>FOFC</cell><cell>BPC</cell></row><row><cell>500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Due to the linearity assumption, each element in {P 1 , ...., P 2m } contains the component inε L P 1 , ..., ε L P m while {Q 1 , ...Q n } not. Because the dimension of ε L P i in {P 1 , ...., P m , Q 1 , ...Q n } is m and Dim(L(S p )) = m, ω 1 , ...., P m , Q 1 , ...Q n ) contains ε L P i , for any ω ≠ 0.According to the Darmois-Skitovitch Theorem, we have ω 1 , ...., P m , Q 1 , ...Q n ) (P m+1 , ...., P 2m )⊺ . That is to say, ({P m+1)+1 , ...P 2m }, {P 1 , ...., P m , Q 1</figDesc><table /><note><p>⊺ E[(P 1 , ...., P m , Q 1 , ...Q n )(P m+1 , ...., P 2m ) ⊺ ] = 0. ⊺ (P ⊺ (P</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>results with different numbers of variables and randomly generated graphs (with sample size=2000).For comparisons, we give the hypothesized factors formulated in<ref type="bibr" target="#b2">[Byrne, 2010]</ref> in Table 3. RC 2 , W O 1 , W O 2 , Decision Making DM 1 , DM 2 Classroom Climate CC 1 , CC 2 ,CC 3 ,CC 4 Self-Esteem SE 1 , SE 2 ,SE 3 Peer Support P S 1 , P S 2 External Locus of Control ELC 1 , ELC 2 ,ELC 3 ,ELC 4 ,ELC 5 Emotional Exhaustion EE 1 , EE 2 ,EE 3 Denationalization DP 1 , DP 2 , DP 3 Personal Accomplishment P A 1 , P A 2 , P A 3</figDesc><table><row><cell cols="5">Number of variables (latent variables) Latent omission Latent commission Mismeasurements Correct-ordering rate</cell></row><row><cell>15(5)</cell><cell>0.02(1)</cell><cell>0.00(0)</cell><cell>0.00(0)</cell><cell>0.90</cell></row><row><cell>30(10)</cell><cell>0.09(3)</cell><cell>0.05(3)</cell><cell>0.04(3)</cell><cell>0.85</cell></row><row><cell>60(20)</cell><cell>0.15(6)</cell><cell>0.12(6)</cell><cell>0.10(6)</cell><cell>0.79</cell></row><row><cell cols="2">C More details of Real-Word data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Factors</cell><cell></cell><cell cols="2">Observed variables</cell><cell></cell></row><row><cell cols="2">Role Conflict</cell><cell>RC 1 ,</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The hypothesized factors in<ref type="bibr" target="#b2">[Byrne, 2010]</ref>.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The variable is neither the cause nor the effect of other measurement variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Note that we do not assume E Z ⫫ L.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Note that here we call L a "root variable" after we have known the variables that causally earlier than L.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>Here, the boxes indicate the elements of the root variable set {L 1 , L 2 }.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>For BPC and FOFC algorithms, we used these implementations in the TETRAD package, which can be downloaded at http://www.phil.cmu.edu/tetrad/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>Note that we do not assume E Z ⫫ L.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by <rs type="funder">Natural Science Foundation of China</rs> (<rs type="grantNumber">61876043</rs>) and <rs type="funder">Science and Technology Planning Project of Guangzhou</rs>(<rs type="grantNumber">201902010058</rs>) and <rs type="funder">Outstanding Young Scientific Research Talents International Cultivation Project Fund of Department of Education of Guangdong Province(40190001)</rs>. KZ would like to acknowledge the support by the <rs type="funder">United States Air Force</rs> under Contract No. <rs type="grantNumber">FA8650-17-C-7715</rs>. We appreciate the comments from <rs type="person">Peter Spirtes</rs> and anonymous reviewers, which greatly helped to improve the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xrDfybm">
					<idno type="grant-number">61876043</idno>
				</org>
				<org type="funding" xml:id="_JaSPEUZ">
					<idno type="grant-number">201902010058</idno>
				</org>
				<org type="funding" xml:id="_9pcGUHn">
					<idno type="grant-number">FA8650-17-C-7715</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The supplementary material contains • Proof of Proposition 1; • Proof of Theorem 1; • Proof of Proposition 2; • Proof of (and remark on) Theorem 2; • Proof of Theorem 3; ⊺ . There are two cases to consider.</p><p>Case 1). Assume that Y is not a causal cluster and show that (X \ Y, Y) violates the GIN condition, leading to the contradiction. Since Y is not a causal cluster, without loss of generality, L(Y) must contain at least two different parental latent variable sets, denoted by L a and L b . Now, we show that there is no non-zero vector ω such that ω ⊺ Y is independent from X \ Ỹ. Because there is no subset Ỹ ⊆ Y such that (X \ Ỹ, Y) follows the GIN condition, the number of elements containing the components of L a in Y is smaller than Dim(L a ) + 1 and the number of elements containing the components of L b in Y is less than Dim(L b ) + 1. Thus, we obtain that there is no ω ≠ 0 such that </p><p>violates the GIN condition, which leads to a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Proof of Theorem 4</head><p>Theorem 4. Let S p and S q be two causal clusters of a LiNGLaM. Assume there is no latent confounder for L(S p ) and L(S q ), and L(S p ) ∩ L(S q ) = ∅. Further suppose that S p contains 2Dim(L(S p )) number of variables with S p = {P 1 , P 2 , ..., P 2Dim(L(S p )) } and that C q contains 2Dim(L(S q )) number of variables with S q = {Q 1 , Q 2 , ..., Q 2Dim(L(S q )) }. Then if ({P Dim(L(S p ))+1 , ...P 2Dim(L(S p )) }, {P 1 , ...., P Dim(L(S p )) , Q 1 , ...Q Dim(L(S q )) }) follows the GIN condition, L(S p ) → L(S q ) holds.</p><p>Proof. For L(S p ) and L(S q ), there are two possible causal relations: L(S p ) → L(S q ) and L(S p ) ← L(S q ). For clarity, let m = Dim(L(S p )) and n = Dim(L(S p )). Further, Let L(S p ) = {L p 1 , ..., L p m } and L(S q ) = {L q 1 , ..., L q n } (note that subscripts denote the causal order). First, we consider case 1: L(S p ) → L(S q ), by leveraging the result of Theorem 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning linear bayesian networks with latent variables</title>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<title level="m">Structural Equations with Latent Variable</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName><surname>Byrne</surname></persName>
		</author>
		<title level="m">Structural equation modeling with amos: Basic concepts, applications, and programming</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12863" to="12872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Random variables and probability distributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cramér</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering unconfounded causal relationships using linear non-gaussian models</title>
		<author>
			<persName><forename type="first">Doris</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JSAI International Symposium on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Aylmer</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Fisher</surname></persName>
		</author>
		<title level="m">Statistical methods for research workers. Statistical methods for research workers</title>
		<imprint>
			<biblScope unit="page">1950</biblScope>
		</imprint>
	</monogr>
	<note>llth ed. revised</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><forename type="middle">J</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causal discovery from heterogeneous/nonstationary data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Characterization problems in mathematical statistics</title>
		<author>
			<persName><forename type="first">Calyampudi Radhakrishna</forename><surname>Abram M Kagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurij</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnik</forename><surname>Vladimirovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causal clustering for 1-factor measurement models</title>
		<author>
			<persName><forename type="first">Erich</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1655" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The seven tools of causal inference, with reflections on machine learning</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of linear non-gaussian acyclic models for latent factors</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="191" to="246" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pearson&apos;s contribution to the theory of two factors</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology. General Section</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="101" />
			<date type="published" when="1928">1928</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Calculation of entailed rank constraints in partially non-linear and cyclic models</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science computer review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied informatics</title>
		<imprint>
			<publisher>SpringerOpen</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automated search for causal relations: Theory and practice</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tillman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trek separation for gaussian graphical models</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Sullivant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelli</forename><surname>Talaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Draisma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1665" to="1685" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ParceLiNGAM: a causal ordering method robust against latent confounders</title>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of ICA for causality discovery in the hong kong stock market</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing (ICONIP)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning causality and causality-related learning</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
