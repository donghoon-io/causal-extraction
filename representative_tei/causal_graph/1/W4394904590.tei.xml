<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning clinical networks from medical records based on information estimates in mixed-type data</title>
				<funder>
					<orgName type="full">PSL Research University and Sorbonne University</orgName>
				</funder>
				<funder>
					<orgName type="full">French Ministry of Higher Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-05-18">May 18, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Cabeliid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis</forename><surname>Verny</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadir</forename><surname>Sellaid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">LIMICS</orgName>
								<orgName type="laboratory" key="lab2">UMRS 1142</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
								<address>
									<addrLine>15 rue de l&apos;e ´cole de me ´decine 4 Ho ˆpital La Pitie ´-Salpêtrière 47-83 boulevard de l&apos;Ho ˆpital</addrLine>
									<postCode>75006 75013</postCode>
									<settlement>Paris Paris Madison</settlement>
									<country>France UNITED STATES</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guido</forename><surname>Uguzzoniid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Vernyid</surname></persName>
							<email>marc.verny@aphp.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Herve</forename><surname>´isambertid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadir</forename><surname>Sella</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">UMR168</orgName>
								<orgName type="institution" key="instit1">PSL Research University</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>26 rue d&apos;Ulm</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Immunity and Cancer</orgName>
								<orgName type="laboratory">Residual Tumor &amp; Response to Treatment Laboratory</orgName>
								<orgName type="institution">INSERM</orgName>
								<address>
									<addrLine>RT2Lab</addrLine>
									<postCode>U932</postCode>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">Laboratoire Physico Chimie Curie</orgName>
								<orgName type="institution">PSL Research University</orgName>
								<address>
									<postCode>CNRS UMR168 75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne-Sophie</forename><surname>Hamy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Universite ´</orgName>
								<address>
									<addrLine>4, place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Immunity and Cancer</orgName>
								<orgName type="laboratory">Residual Tumor &amp; Response to Treatment Laboratory</orgName>
								<orgName type="institution">INSERM</orgName>
								<address>
									<addrLine>RT2Lab</addrLine>
									<postCode>U932</postCode>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Department of Medical Oncology</orgName>
								<orgName type="department" key="dep2">Institut Curie</orgName>
								<address>
									<addrLine>Saint-Cloud</addrLine>
									<postCode>F-92230</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Cabeli</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">LIMICS</orgName>
								<orgName type="laboratory" key="lab2">UMRS 1142</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
								<address>
									<addrLine>15 rue de l&apos;e ´cole de me ´decine 4 Ho ˆpital La Pitie ´-Salpêtrière 47-83 boulevard de l&apos;Ho ˆpital</addrLine>
									<postCode>75006 75013</postCode>
									<settlement>Paris Paris Madison</settlement>
									<country>France UNITED STATES</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">Laboratoire Physico Chimie Curie</orgName>
								<orgName type="institution">PSL Research University</orgName>
								<address>
									<postCode>CNRS UMR168 75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lauren</forename><surname>Darrigues</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marick</forename><surname>Laé</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Department of Tumor Biology</orgName>
								<orgName type="department" key="dep2">Institut Curie</orgName>
								<address>
									<postCode>F-75248</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution">Henri Becquerel Cancer Center</orgName>
								<address>
									<postCode>INSERM U1245</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Reyal</surname></persName>
							<email>fabien.reyal@curie.fr</email>
							<affiliation key="aff3">
								<orgName type="department">Immunity and Cancer</orgName>
								<orgName type="laboratory">Residual Tumor &amp; Response to Treatment Laboratory</orgName>
								<orgName type="institution">INSERM</orgName>
								<address>
									<addrLine>RT2Lab</addrLine>
									<postCode>U932</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Isambert</surname></persName>
							<email>herve.isambert@curie.fr</email>
							<affiliation key="aff5">
								<orgName type="department">Institut Curie</orgName>
								<orgName type="laboratory">Laboratoire Physico Chimie Curie</orgName>
								<orgName type="institution">PSL Research University</orgName>
								<address>
									<postCode>CNRS UMR168 75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institut Curie</orgName>
								<address>
									<postCode>F-75248</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">Department of Surgery</orgName>
								<orgName type="department" key="dep2">Institut Curie</orgName>
								<orgName type="institution">Université Paris</orgName>
								<address>
									<postCode>F-75248</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">UniRouen Normandy University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning clinical networks from medical records based on information estimates in mixed-type data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-18">May 18, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pcbi.1007866</idno>
					<note type="submission">Received: September 9, 2019 Accepted: April 10, 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>PLOS COMPUTATIONAL BIOLOGY Machine learning -data visualization-residual cancer burden -neoadjuvant chemotherapy breast cancer</term>
					<term>5</term>
					<term>2</term>
					<term>NEOREP study on breast cancer patients 121</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Je remercie en premier lieu mon directeur de thèse, Hervé, pour m'avoir donné ma chance dans son équipe. J'ai énormément appris sur ce sujet passionnant, merci pour la supervision tout au long de ces quatres années, pour les discussions, la confiance de me laisser explorer le sujet et la direction quand c'était nécessaire ! Je remercie sincèrement tous ceux qui contribuent à faire fonctionner la recherche, que ce soit le personnel du laboratoire ou les "permanents". Je remercie les membres de mon jury : Simona Cocco, Chloé-Agathe Azencott et Pierre-Henri Wuillemin qui ont aussi eu la gentillesse de constituer mon comité de suivi, et particulièrement mes rapporteurs Simon de Givry et Philippe Leray, pour leur temps et leur considération experte de mon manuscrit.</p><p>Je remercie chaleureusement mes collègues, Nadir pour m'avoir accueilli et bien conseillé la première année, Honghao qui a toujours une idée pertinente et avec qui il est toujours agréable de discuter, Marcel pour les conversations stimulantes sur la science et les cultures française et brésilienne, Franck et Louise pour rendre le travail et les journées plus agréables ces derniers mois et qui vont je suis sûr assurer la relève brillamment. Je remercie également tous les autres dont j'ai croisé le chemin pendant la thèse, qui font de Curie un endroit à la fois stimulant et bienveillant : Kevin, Jason, Tommaso et bien d'autres.</p><p>Je pense aussi à mes camarades de Montréal qui ont été une grande source d'inspiration et m'ont définitivement convaincu de continuer dans la science : Manu, Henry, Alain, Marc-André et les autres. Pour les mêmes raisons, je remercie mes anciens encadrants qui ont entériné cette décision : Alexandre de Brevern, Josselin Noirel et tout le laboratoire (ex-)GBA du CNAM. Merci aussi aux amis qui m'ont accompagné et qui ont rendu la thèse plus facile, rencontrés sur les bancs de la fac (Walid, Quentin !), en selle, ou ailleurs ! Bien sûr, je remercie ma famille qui m'accompagne et me soutient depuis le début. Mes parents, qui m'ont toujours compris et fait confiance pour que je poursuive mes rêves. Mes grand-parents, qui m'ont transmis des leçons encore plus importantes que celles enseignées à l'université. Mimi et Sion, merci ! Mes frères et soeur, sur qui je peux toujours compter. Mon oncle, qui partageait ses magazines La recherche pendant les longs étés, ma tante et mon autre oncle, mes cousines, et Mireille.</p><p>Enfin, un immense merci à celle qui embellit mon quotidien et m'aide à être la meilleure version de moi-même. Merci Kim, ma binôme, pour les rires, les aventures, tous les moments qui rendent notre vie à deux si agréable.</p><p>How could I avoid being brought to certain ideas? From a hundred rabbits you can't make a horse, a hundred suspicions don't make a proof, as the English proverb says, but that's only from the rational point of view-you can't help being partial Fyodor Dostoevsky iv CONTENTS 4.3 Reliable orientations with mutual information supremum .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTENTS v</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé</head><p>La corrélation n'implique pas la causalité, une distinction importante à rappeler alors que les associations statistiques génèrent de plus en plus de discussions dans un monde toujours plus mesuré et documenté. C'est pourtant le but, avoué ou non, de la plupart des domaines scientifiques : définir les mécanismes de notre environnement qui ont produit ces observations. La nouvelle science de la causalité cherche à nous réconcilier avec ce concept en répondant à ces questions : comment formaliser les relations causales, comment nous les représenter, et quand peut-on les découvrir ? Les travaux de cette thèse s'inscrivent dans la théorie principalement développée par Judea Pearl sur les diagrammes causaux; des modèles graphiques qui permettent de dériver toutes les quantités causales d'intérêt (effet du traitement, contrefactuelles...) formellement et intuitivement. Nous traitons le problème de l'inférence de réseau causal à partir uniquement de données d'observation c'est-à-dire sans aucune intervention de la part de l'expérimentateur. En particulier, nous proposons d'améliorer les méthodes existantes pour les rendre plus aptes à analyser des données issues du monde réel, en nous affranchissant le plus possible des contraintes sur les distributions des données, et en les rendant plus interprétables.</p><p>Nous proposons une extension de MIIC, une approche basée sur les contraintes et la théorie de l'information pour retrouver la classe d'équivalence du graphe causal à partir d'observations. Notre contribution est un algorithme de discrétisation optimale pour simultanément estimer la valeur de l'information mutuelle (et multivariée) et évaluer sa significativité entre des échantillons de variables de n'importe quelle nature : continue, catégorique ou mixte. Cette discrétisation optimale est elle-même ancrée dans le principe de longueur de description minimale pour trouver les meilleures représentations des distributions jointes grâce à une estimation du maximum de vraisemblance normalisé. L'évaluation de la significativité de l'information au sens de la complexité stochastique est un dérivé de cette approche, et nous permet de reconstruire des graphes causaux de manière robuste sur des échantillons de taille finie. Nous proposons également des améliorations des algorithmes par contraintes pour s'assurer que le graphe final est plus cohérent avec les données, en modifiant les règles pour choisir les variables de conditionnements. Les outils d'inférence et de visualisation sont mis à disposition de la communauté pour permettre au plus grand nombre d'analyser leurs jeux de données.</p><p>Enfin, nous mettons à profit ces développements pour analyser des jeux de données mixtes, toujours en étroite collaboration avec les équipes responsables de la collecte des données. La première application majeure est l'analyse de dossiers médicaux de patients âgés atteints de troubles cognitifs en collaboration avec l'hôpital La Pitié-Salpêtrière. La seconde concerne les dossiers médicaux de patientes ayant entrepris une chimiothérapie néo-adjuvante contre le cancer du sein, avec le département de chirurgie oncologique de l'hôpital Curie. Enfin, nous présentons des résultats sur l'analyse de gènes métaboliques moteurs de la différentiation hématopoïétique sur des données de profil transcriptomiques de cellules précurseurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 1 Introduction 1.1 Scientific context</head><p>If correlation does not imply causation, then what does ? It is a good thing that this distinction has permeated modern scientific culture, but discovering the causal mechanisms remains the goal of most studies, and correlation their main tool to do so. The new science of causality is trying to reconcile us with this goal, formally defining how to represent causal relations, how to measure them, and most importantly, giving the necessary conditions to discover them.</p><p>The first question on how to represent causal relationships has perhaps found its best answer in the theory of causal diagrams mainly developed by Judea Pearl <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>. A causal diagram is a Bayesian network: a directed acyclic graph that encodes the conditional independences between random variables represented by the nodes; with an added causal dimension transcribed by the direction of the edges. From these graphs, one can derive answers to fundamentally causal questions like "what is the effect of this treatment on this population?", or even "what if this population had received this treatment?". This thesis contributes to the field of causal graph discovery, which aims to reconstruct these graphical models from observational data only. The challenge of causal discovery lies in retaining the direct links that reflect some understanding of nature, the data generating process, and rejecting the spurious interactions that are indirect consequences of the meaningful relationships. In the right conditions, it is known that we can learn the causal graph up to an equivalence graph from only the pattern of dependencies and independencies found in the data, without any intervention.</p><p>In this work, we focus on constraint-based algorithms in general and MIIC specifically, an information-theoretic approach combining elements of both constraint-based and score-based methods. Where classical methods rely on frequentist tests of independence and a parameter α for the p-value threshold, MIIC estimates independence from data with the minimum description length principle and the normalized maximum likelihood distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>The main objective of this thesis is to make MIIC and constraint-based methods more capable of handling real-world data. This class of algorithms relies entirely on conditional independence patterns on sampled data, which is notoriously hard to estimate without making assumption on the distributions.</p><p>We want to be able to use the data that is available to us under any form and making as little assumption as possible on its distribution. Concretely, we want to be able to estimate the conditional independence between two variables X,Y with a conditioning set Z regardless of the nature of the marginal distributions (p(X), p(Y ), p(Z)) and of the joint distributions (p(X,Y )...). This estimate must also be robust to small sample sizes while remaining computable when N is large, and ideally not favor any type of variable or interaction.</p><p>The method presented here is based on the master definition of mutual information:</p><formula xml:id="formula_0">I(X;Y ) = sup P,Q I([X] P ; [Y ] Q )</formula><p>where the supremum is on all finite partitions P and Q <ref type="bibr">[4]</ref>. The developed approach consists in maximizing the value I ([X] ∆ ; [Y ] ∆ ) corrected by the stochastic complexity associated with the discretization [X] ∆ ; [Y ] ∆ to take into account the effects of the finite number of samples. Introducing the complexity also allows us to conclude on the independence on finite samples (for which the information estimate is always positive): I ([X] ∆ ; [Y ] ∆ ) ≤ 0 implies independence between X and Y in the sense of the data complexity <ref type="bibr">[5]</ref>.</p><p>The other contributions of this thesis concern the operation of constraint-based methods. First, we propose an information theoretic test to perform test-wise omission in the case of missing data, avoiding as much as possible the spurious independencies brought by selection bias. In the same idea, we introduce a variant of constraint-based algorithms that guarantees that the conditioning sets used to remove edges are more consistent with the final graph G in f and the data D <ref type="bibr">[6]</ref>.</p><p>We also propose a method to distinguish "genuine" from "putative" causal links returned by MIIC, by excluding the effect of an unobserved common cause for each predicted genuine causal link. It is achieved by evaluating the separate probabilities of the "head" and "tail" of directed links for all directed edges.</p><p>In addition to an open source library on R, we have also developed an online graphical interface to facilitate the exploration of MIIC results, available at <ref type="url" target="https://miic.curie.fr/">https://miic.curie. fr/</ref>.</p><p>Finally, we show different applications of MIIC on real-life mixed datasets.</p><p>The first network is reconstructed from clinical data of the hospital La Pitié-Salpêtrière </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 2</head><p>Causal inference from observational data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal inference and causal structures</head><p>In this section we clarify the concept of using graphs to represent causality. We start by giving an intuitive example of a causal diagram, and we show that such graphs can describe quite naturally common ways of inferring causality, either through experiments or from observation data. Finally, we give the formal notations and definitions of the causal graph framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Intuition of causal graphs</head><p>Let us consider a familiar situation in which our intuition can be represented by a causal diagram (Fig 2 .1). Assume that there are two causes that could be at the origin of a car breakdown, which we try to diagnose before intervening on the car. We consider the factors that could help us make the diagnosis, and decide to draw a diagram to see their relations visually. The two causes considered, low engine oil level or a flat battery, are represented as parents of the "Breakdown" node. There is no reason to think that the two are linked, which we represent by not drawing an edge between them. We include a fourth node that corresponds to another observation : the headlights do not turn on. We know that the headlights do not depend on the oil level but they need battery to run, and the "Lights" node is therefore linked to the "Battery" node only. We also know that usually, it is somehow related to a car breaking down : if the headlights do not turn on, the car will probably not start either. We represent this association with a dotted link. This link reflects a correlation, not a causation : the indirect interaction exists only because of the common ancestor "Battery" but does not inform us about a functional relationship (therefore it would not be included in the causal diagram). It can help us guess the origin of the failure (Battery or Oil), but fixing the headlights will not help to start the car.</p><p>Representing complex systems with a causal graph has two main advantages. First, only the direct links, that correspond to functional relationships, are represented. Spurious correlations between two variables that are not causally linked can always be explained from a path of direct links in the graph. Secondly, the causal order can be read off the graph, via the direction of the edge X → Y . If X is a parent of Y , then intervening on the distribution of X to give it an arbitrary value x will affect the distribution of Y , but the inverse is not true : intervening on Y will not affect its parent X. Such intervention is noted p(Y |do(X = x) and is the basis of do calculus. Remark that p(Y |X) and p(Y |do(X = x)) are not the same : the first is observational while the second is interventional. For example, let X be the reading on a barometer and Y the weather represented in a single variable. If we observe both every day and take note of their values, p(Y |X) would show a strong relationship between the two : if the barometer measures low air pressure the weather is often bad, while if it measures high pressure the weather is better. The distribution p(Y |do(X = x)) however won't actually depend on the value x : we cannot change the weather by setting the reading on a barometer. Even though p(Y |X) can be used for predicting the value of a variable by measuring the other one, it does not inform on the functional relationship between the two. For this, we need to go up a rung on the ladder of causation, by using do-calculus <ref type="bibr">[3]</ref>.</p><p>In the example of Fig 2 .1, we draw the graph from pre-existing knowledge, but what can we do when such knowledge is not available ? This is the domain of causal inference, which aims to uncover causal effects either through experimentation or passive observation of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Causal inference approaches as seen with causal graphs</head><p>The gold standard of causal inference is the randomized controlled trial, where an homogeneous population is randomly attributed either a treatment or a placebo. Let Y be the outcome of the trial for each patient, which can be positive or negative. We want to know the extent to which the outcome depends on the treatment, noted X, as opposed to other external factors which are all grouped in the node Z. Formally, we can answer this question by comparing p(Y |do(X = treatment)) and p(Y |do(X = placebo)). The causal graph of a truly random trial is shown in Figure <ref type="figure" target="#fig_0">2</ref>.2.</p><p>We can see from the causal diagram that the causal effect of X on Y is direct, it is not affected by the rest of the graph. In this setting, random attribution of X is a kind of intervention, and p(Y |do(X)) can be directly observed from the data as p(Y |X). This type of experiment is generally reliable provided the treatment is assigned truly at random, but is has disadvantages. First, it needs to be conducted for each X for which we want to know the effect, and it may be too long or too difficult to enroll enough participants. Secondly, it is unethical when we suspect that the interaction is harmful, think for example of forcing test subjects to be exposed to carcinogens. Finally, it may be simply impossible to intervene on the potential cause, for example we can not randomize the genetic makeup of patients to study the prevalence of certain diseases.</p><p>For those cases, we can still perform causal inference by simply observing the potential cause, the outcome, and all of the confounding factors which affect both the cause and the effect (Fig 2 .3). This graph summarizes the principle behind several approaches, even those that have not adopted the language of causal graphs and do-calculus. Matching procedures for example make exactly the same assumptions to estimate the effect of X on Y by taking the effect of Z into account. Their goal is to reduce the assignment bias for the "treatment" X and mimic a randomized controlled trial by creating samples that were matched on Z, essentially removing the edge Z → X <ref type="bibr">[8,</ref><ref type="bibr">9]</ref>. <ref type="bibr">Fig 2.</ref>3 is also the typical setting where one can simply model Y from X while adjusting for Z. This is the approach taken by genome-wide association studies, which try to measure the effect of thousands of genes X on the apparition of a disease Y adjusting for some principal components Z to model ancestry differences between cases and controls <ref type="bibr">[10]</ref>. All of these methods are theoretically sound but are often criticized for their predisposition to give biased results. According to Pearl, these shortcomings come mainly from the attitude that Z should contain as many covariates as possible, to adjust with all the information that is available <ref type="bibr">[1, p. 350]</ref>. By doing so and ignoring the "strong ignorability conditions" for a variable to be included in Z, we will inevitably end up including variable that are not parents Chapter 2. Causal inference but children of X and Y : X → Z ← Y , violating the assumptions and the graph of Fig 2 .3. Still according to Pearl, this kind of mistake is much less likely to occur when using the causal graph framework, as practitioners are forced to model the interactions first, thinking about the causal relationships between the treatment, the outcome, and the covariates.</p><p>As the last example, we will look at the case where we still cannot intervene on the potential cause X, and we know it is affected by common confounders of Y , but we cannot measure them, nor adjust for them. This was famously the defense of prominent statisticians employed by tobacco companies at the time of the first reports linking cigarettes with lung cancer. The association could not be denied, but they claimed it could be explained by a hidden common cause, some genetic factor for example, which caused a certain population to both want to smoke and develop more cancers than the general population. In 1964 we did not have access to sequencing technology, and since randomized controlled trials were out of the question, this argument was hard to disprove and supposedly delayed anti-smoking legislation <ref type="bibr">[1, p. 83]</ref>. In this setting, we can still measure the effect of X on Y if we measure another "instrumental" variable I that we know to have an effect on X and to be independent of the latent confounders L (Fig 2 <ref type="figure" target="#fig_66">.4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X Y I L</head><note type="other">Figure 2</note><p>.4: When the confounding variables are not observed, the effect for X on Y can be estimated from an instrumental variable I.</p><p>Indeed, we can see from the graph that any association measured between I and Y necessarily goes through X, proving the existence of the directed edge X → Y . In the cigarettes cause lung cancer example, we can take I as the price of cigarette packs. Intuitively, if I is correlated with the number of lung cancers Y we can deduce the existence of a causal link between tobacco consummation and cancer <ref type="bibr">[11]</ref>. Just as with the adjusting variables of the previous example, modelling the interactions in a causal graph is a way of making sure that I corresponds to an instrumental variable. This brief discussion highlights the remarkable adaptability of causal graphs. These intuitive models are able to summarize most if not all approaches that aim to infer causality, federating all causal thinking into a single framework. Once the causal graph is known, one can then derive formal quantities of causal effect and counterfactual thinking using do-calculus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Notations and definitions</head><p>The causal graph framework treats causality as a statistical property, it utilizes the languages of both graph theory and probability. We now take the time to review the definitions that are necessary to define a causal graph.</p><p>• Let D be a dataset comprised of V variables X 1 , ..., X v . For pairwise and conditional cases, we may use X, Y and Z as variables of D instead.</p><p>• Each variable has a distribution p(X i ), and the joint distribution of D is p(V ).</p><p>• We note two variables that are independent as X ⊥ ⊥ Y , conditionally independent on Z as X ⊥ ⊥ Y | Z.</p><p>• D is represented by a graph G where each variable is a node. We note the true causal graph G c and the graph inferred from the data G In f .</p><p>• If variables X and Y of D are adjacent in G, the edge between the two is either unoriented X -Y , oriented X → Y , X ← Y or bi-directed X ↔ Y .</p><p>• The variables that have an edge pointing towards X i are its parents and are noted Pa i . The variables that X i points to are its children, noted Ch i .</p><p>• The skeleton of G is the graph with same adjacencies and no oriented edges.</p><p>• A V-structure is a sub-graph of three nodes where X → Z ← Y where X Y .</p><p>• The complete graph on V variables is the skeleton where all X i , X j are adjacent.</p><p>The true causal graph is, in short, the graph that describes the causal mechanisms that produce the data D, as well as all possible randomized studies on the V variables.</p><p>Definition 2.1. The true causal graph G c of given variables X 1 , ..., X v with distribution p(V ) satisfies :</p><p>• G c is a directed acyclic graph.</p><p>• p(V ) is Markov with respect to G c , i.e. if X and Y are d-separated by Z, then X ⊥ ⊥ Y |Z in p(V ) (see Def 2.3).</p><p>• p(V ) satisfies causal minimality with respect to G c (see Def 2.2).</p><p>• The distribution of the node X i is a function of its parents Pa i and some unique noise ε i :</p><formula xml:id="formula_1">p(X i ) = f (p(Pa i ), ε i )</formula><p>and G c is compatible <ref type="bibr">[1]</ref> with the set of P * of all possible interventional distributions P(V |do(X = x)) for all variable X and value x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Score-based approaches</head><p>The principle behind score-based approaches is fairly intuitive, but difficult to implement. Although there may be preliminary work pre-dating it, in this section we refer here the formal idea introduced by Geiger and Heckerman <ref type="bibr">[13,</ref><ref type="bibr">14]</ref>, and Chickering <ref type="bibr">[15]</ref>. Given the data D from a vector of V variables, find the graph Ĝ that maximizes a likelihood score S(D, G)</p><formula xml:id="formula_2">: Ĝ = argmax G S(D, G) (2.2.1)</formula><p>where G is searched over the space of DAGs.</p><p>We can think of several definitions for the scoring function. If the distribution p(V ) can be described with a parametric model (e.g. discrete multinomial distributions, linear Gaussian relationships), then we can define a set of parameters θ ∈ Θ. The Bayesian definition of S(D, G) is the log posterior with prior beliefs p pr (G) and p pr (θ ) over DAGs and parameters respectively:</p><formula xml:id="formula_3">S(D, G) = log p pr G + log p(D|G)<label>(2.2.2)</label></formula><p>with p(D|G) the marginal likelihood</p><formula xml:id="formula_4">p(D|G) = θ ∈Θ p(D|G, θ )p pr (θ )</formula><p>In this view, Ĝ which maximizes the score is the maximum a posteriori estimator. In <ref type="bibr">[16]</ref>, Heckerman and Geiger discuss how to choose the priors accordingly.</p><p>Another way to define the scoring function is using the maximum likelihood estimator θ from N observed samples, for each graph. We can then define the score function using the Bayesian Information Criterion (BIC) <ref type="bibr">[17]</ref> :</p><formula xml:id="formula_5">S(D, G) = log p(D| θ , G) - d 2 log N (2.2.3)</formula><p>which prevents overfitting by favoring models with fewer parameters d.</p><p>The space of all DAGs grows super-exponentially with V <ref type="bibr">[15]</ref>, so heuristics are needed to search it in practice. Greedy algorithms iterate over the set neighboring graphs, selecting the best candidate at each step and using it as a new reference point. Neighbors are usually defined as all DAGs that differ with at most one missing or extra edge from the reference graph. The greedy equivalence search (GES) <ref type="bibr">[15]</ref> improves on this process by performing two phases : first adding edges up to a local maximum, then simplifying the model by removing edges, returning the pruned graph when a maximum is reached.</p><p>From our perspective, score-based methods suffer from the following drawbacks : <ref type="bibr">(1)</ref> The score function requires simple modeling and parameters, which may destroy subtle Chapter 2. Causal inference causality signals in real data. <ref type="bibr">(2)</ref> The search space is limited to DAGs or their equivalence class, excluding bi-directed edges X ↔ Y . (3) The methods do not scale well with V and typically do not produce good results when V &gt; 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Constraint-based approaches</head><p>Compared to the score-based methods, constraint-based algorithms have more of a local approach to graph reconstruction. They make two assumptions : d-separation in G c implies conditional independence in p(V ) (the Markov condition), and all conditional independences in p(V ) correspond to d-separation in G c (the faithfulness assumption). They will be discussed in more details in Sec 3.1.3, for now we simply assume that they hold for p(V ) and G c . Given both assumptions, constraint-based approaches are able to recover up to the equivalence class of G c from the set of dependencies and conditional dependencies of p(V ).</p><p>We now describe the staple constraint-based approach, the PC algorithm named after Peter Spirtes and Clark Glymour <ref type="bibr">[18]</ref>, which is itself a refinement of the IC algorigthm <ref type="bibr">[12]</ref>. It consists of three phases, as shown in Alg 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The PC Algorithm Require: D</p><p>Step 1: Find the graph skeleton and separating sets of removed edges</p><p>Step 2: Orient V-structures based on separating sets</p><p>Step 3: Propagate orientations of V-structures to as many remaining undirected edges as possible return Output graph G In f</p><p>It was proven to be consistent, returning the correct equivalence class of G c if enough samples are observed. The skeleton reconstruction phase is an iterative process : starting from the complete graph, remove all edges X -Y if X ⊥ ⊥ Y or if X ⊥ ⊥ Y |Z with Z a set of variables in the neighbors of X,Y . Colombo and Maathuis improved the original algorithm by making it order-independent, calling this version "PC-stable" <ref type="bibr">[19]</ref>. This is the version detailed in Alg 2.</p><p>In the second step, we start orienting the edges of the resulting skeleton, V-structure by V-structure. For each triplet X -Z -Y where X Z, orient the edges X → Z ← Y if Z was not in the separating set to remove the edge X -Z. This orientation step also has two other variants, the conservative rule <ref type="bibr">[20]</ref> and the majority rule <ref type="bibr">[19]</ref>. Using the conservative rule, the V-structure is oriented only if Z is in none of the separating sets that satisfy X ⊥ ⊥ Y |{U i } (and accordingly, with majority rule orient if Z is less than 50 percent of those). Both give generally better results than the original scheme, although they require many more conditional independence tests.</p><p>Finally, these orientations are propagated to the rest of the graph following Meek's Algorithm 2 Find skeleton and separating sets (Step 1 of PC-stable algorithm) Require: Conditional independence test between all V variables G In f ← the complete graph on V ← -1 repeat ← + 1 for all vertices X i ∈ G do a(X i ) = adj(G, X i ) end for repeat select a new pair of vertices (X i , X j ) adjacent in G and satisfying |a(</p><formula xml:id="formula_6">X i )\{X j }| ≥ repeat choose new C C C ⊆ a(X i )\{X j }, |C C C| = if (X i ⊥ ⊥ X j |C C C) then Delete edge X i X j from G Sepset(X i , X j | G) = Sepset(X j , X i | G) ← C C C end if</formula><p>until X i and X j are no longer adjacent in G or all C C C ⊆ a(X i )\{X j } with |C C C| = have been considered until all pairs of adjacent vertices (X i , X j ) in G with |a(X i )\{X j }| ≥ have been considered until all pairs of adjacent vertices (X i , X j ) in G satisfy |a(X i )\{X j }| ≤ return G, sepsets rules <ref type="bibr">[21]</ref>. This step does not rely on independencies observed in the data, it is more of a convention to make the result G In f into a graph that is maximally oriented within the equivalence class <ref type="bibr">[1]</ref>. As such, propagated orientations may be considered weaker causality signals than V-structures.</p><p>As opposed to score-based methods, constraint-based methods present many advantages. They can be used to infer larger networks provided the conditional independence test has a good time complexity. They also do not rely on a modeling of p(V ), only a conditional independence test, and are thus applicable to a much wider range of data. Additionally, modifications like the Fast Causal Inference (FCI) Algorithm <ref type="bibr">[22]</ref> (and RFCI <ref type="bibr">[23]</ref>) can make it tolerate and even discover unknown confounding variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">(Conditional) independence tests</head><p>In Section 2.2.2, we purposefully introduced constraint-based methods with an "Oracle" independence test, i.e. able to infer X ⊥ ⊥ Y or X ⊥ ⊥ Y from D with no error. In practice, they also need a parameter α which sets the threshold for significance of a given dependence estimator. Few measures are able to reliably detect dependence between two random variables without being restricted to some type of interaction (see <ref type="bibr">Fig 2.5</ref> for examples of dependencies). In the context of network inference, it is worth reviewing our options as the resulting graph will only contain the interactions in the class of models that are supported by the chosen dependence measure. I now give a brief overview of various dependence measures that can be used in constraint-based methods, going from the simplest to the most general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete independence tests</head><p>The problem of estimating (conditional) independence on discrete data has been studied for a long time. The χ 2 test was developed by Pearson to compare the observed joint frequencies of X and Y with those under the null hypothesis, where X ⊥ ⊥ Y :</p><formula xml:id="formula_7">χ 2 = k ∑ i=1 x 2 i m i -N (2.2.4)</formula><p>where x i is the observed count, m i the expected count and N the number of samples.</p><p>To assess the significance of χ 2 , one can then compare it to the distribution under the null hypothesis, and accept the evidence for dependence if the cumulative density at the estimation (p-value) is lower than the specified al pha.</p><p>One can also compute the closely related G-statistic, of which the χ 2 is an approximation. Since it is related to information theoretic measures, the G-test will be discussed in Section 3.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear and nonlinear correlations</head><p>The most commonly used dependence measure is probably the linear correlation coefficient ρ, or the Pearson correlation coefficient (PCC). It is bounded in [-1, 1], it gives an estimation of both the strength and the direction of the covariance between two variables. As with covariance itself, it is only able to detect linear relationships and ignores many other types of dependencies. For the bivariate normal case, it implies X ⊥ ⊥ Y if and only if ρ X,Y = 0 (with sample size N → ∞). In practice, ρ X,Y is never null and its significance is also assessed by comparing the estimated value to the distribution under the null hypothesis, for which we know the exact form.</p><p>It is generalized to non-linear relationships with Spearman's rank correlation coefficient, which measures the strength and direction of any monotonic function between X and Y . It is more general than the PCC, although still far from the strict equivalence that defines statistical independence : X ⊥ ⊥ Y ⇔ p(x, y) = p(x)p(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance correlation</head><p>Distance correlation or distance covariance is a relatively new measure of dependence between two paired random vectors that is meant to be more universal that the productmoment covariance and correlation <ref type="bibr">[24,</ref><ref type="bibr">25]</ref>.</p><p>The distance correlation R is obtained by normalizing the distance covariance, it behaves like the PCC but generalizes the idea of correlation in at least two fundamental ways. For all distributions with finite first moments, (1) R(X,Y ) is defined for X and Y of arbitrary dimensions (may not be equal) and ( <ref type="formula" target="#formula_3">2</ref></p><formula xml:id="formula_8">) R(X,Y ) = 0 ⇔ X ⊥ ⊥ Y .</formula><p>It is much more powerful than linear and non-linear correlations, for example it is typically able to detect all dependencies of Fig 2 .5 except for the checkerboard pattern. In practice, it is however difficult to judge its significance as no closed form of the distribution under the null hypothesis is known. Instead, the null distribution is estimated by resampling the data many times and significance is assessed by comparing the empirical p-value to α.</p><p>Although they are unrelated, it has comparable power to Hoeffding's much older test of independence <ref type="bibr">[26]</ref>, based on the joint distribution's deviation from independence <ref type="bibr">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel-based tests</head><p>Another popular approach to measure statistical dependence in the non-parametric case is to use kernel transformations of the data, and then measure the covariance in the kernel space <ref type="bibr">[28,</ref><ref type="bibr">29]</ref>. It was also adapted to independence testing in <ref type="bibr">[30]</ref>, giving the Hilbert-Schmidt independence criterion (HSIC). Significance is assessed by estimating the null distribution, either via Monte Carlo resampling or with a gamma distribution approximation.</p><p>Chapter 2. Causal inference <ref type="bibr">[31]</ref> first proposed to adapt the HSIC to the conditional case, and infer causal structures with permutation-based significance testing via constraint-based algorithms. <ref type="bibr">[32]</ref> also implemented kernel-based independence testing with the "Kernel PC" algorithm, although it also suffers from high complexity for independence testing. <ref type="bibr">[33]</ref> improved on this idea and derived the null distribution for the conditional case, making the test less prone to type 1 and type 2 errors, and much more computationally efficient.</p><p>Kernel independence tests are very powerful to detect independencies in the non-parametric case, but they typically do not scale well with the number of samples. Conceptually, this implies that any feature selection technique will in fact learn some local structure of the skeleton of G c , and we can learn the full skeleton by aggregating the results of feature selection for each node in the network. Such a method exists with GENIE 3 <ref type="bibr">[34]</ref>, which treats network reconstruction as several prediction problems, and combines random forests feature importance scores to recreate the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature selection</head><p>However, one must pay particular attention to the process by which feature importance scores are computed. Typically, tree-based models subsample the feature space to avoid overfitting, but this implies that G may be different for each tree. As the simplest example, if the true G c is X 1 → X 2 → Y , the Markov blanket of Y consists of only { X 2 }. By subsampling the feature space for each tree, we potentially remove X 2 and make the interaction X 1 → Y direct, which is not representative of G c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Other graph reconstruction methods</head><p>The original PC algorithm is now more than thirty years old, and although it is consistent and theoretically sound, it has limitations (for example, it is not particularly robust to noise with finite sample size <ref type="bibr">[5,</ref><ref type="bibr">35]</ref>). In this section we give a very brief overview of other recent Bayesian structure learning and causal inference approaches that are neither score-based nor constraint-based.</p><p>A very different approach to causal graph reconstruction was introduced with the linear, non-Gaussian and acyclic model (LiNGAM) of Shimizu et al. <ref type="bibr">[36]</ref>. Instead of examining only the dependencies between variables, in this framework the relationship between two variables X and Y is modeled with a structural equation :</p><formula xml:id="formula_9">Y = bX + ε (2.2.5)</formula><p>with b a factor, ε some noise such that ε ⊥ ⊥ X. The breakthrough of Shimizu et al. is the proof that G c can be recovered from D, in its entirety, whenever at most one ε is Gaussian. The intuition behind LiNGAM is that for non-Gaussian distributions, there is more information in the joint distribution than in the covariance matrix, which can be detected using independent component analysis. The key assumption here is the additive and independent noise model, which can be interpreted as the residual after predicting Y from its parents. DirectLiNGAM introduced another way to find the causal ordering by recursively performing regression and independence test between the predictor and residual <ref type="bibr">[37]</ref>.</p><p>In a similar idea, the Causal Additive Models (CAM) method aims to recover the underlying DAG by modeling the distribution p(V ) with additive structural equation models with Gaussian noise and non-parametric, non-linear relationships <ref type="bibr">[38]</ref>.</p><p>Another different strategy was introduced with NOTEARS, which formulates structure learning as a continuous optimization problem with a smooth function over real matrices <ref type="bibr">[39]</ref>, in opposition to the classical combinatorial score-based learning. The first version of this approach relied heavily on the linear parametrization in the weighted adjacency matrix, and was recently generalized to a larger class of models, that works without assuming any particular form of parametrization <ref type="bibr">[40]</ref>. This general framework essentially makes the score-based method solvable using any of the existing generic solvers, such as multilayer perceptrons.</p><p>In a inspired paper, Wang and Blei proposed to take advantage of the multiplicity of Chapter 2. Causal inference causes for a variable of interest by using unsupervised machine learning to estimate a latent confounding variable <ref type="bibr">[41]</ref>. Having multiple causes is a blessing in this case : it gives more information to estimate a "substitute confounder" which in turn can prove or disprove causal relationships between causes and effect, with weaker assumptions than classical causal inference.</p><p>Finally, it was shown by <ref type="bibr">[42]</ref> that causal structure can be learned by exploiting the invariance of functional relationships under a change of environments. First developed for linear models, <ref type="bibr">[43]</ref> expands the framework of invariant causal prediction to the nonlinear and nonparametric case. This approach in particular will be discussed in Section 3.1.3 in relation with the "stability" or faithfulness assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">MIIC</head><p>MIIC (Multivariate Information-based Inductive Causation), combines constraint-based and information-theoretic frameworks to learn more robust causal graphical models. It was developed on the basis of the 3off2 algorithm by Affeldt and Isambert <ref type="bibr">[5,</ref><ref type="bibr">35]</ref>, which takes advantages of multivariate information to reconstruct the skeleton and orient the edges. Corrected mutual information is described in more details in Section 3.1.1, for now we can simply treat it as a proxy for (conditional) independence between variables :</p><formula xml:id="formula_10">X ⊥ ⊥ Y ⇔ I (X;Y ) &lt; 0 and X ⊥ ⊥ Y |Z ⇔ I (X;Y |Z) &lt; 0.</formula><p>Just like the PC algorithm it starts from a complete graph and prunes the edges to find the skeleton, then orients it (Alg 1) but there is a crucial difference in the way it chooses separating sets to remove edges. Where PC iterates over all the combinations of the neighbors of X and Y in order of increasing cardinality {U i } (until it can conclude conditional independence or it runs out of combinations), MIIC takes off the contributors one by one, using the chain rule of conditional information :</p><formula xml:id="formula_11">I(X;Y |{U i }, Z) = I(X;Y ) -I(X;Y ;U 1 ) -I(X;Y ;U 2 |U 1 ) -• • • -I(X;Y ; Z|{U i }) (2.2.6)</formula><p>This allows to both speed up the process, removing the combinatorial search, and make it more robust to spurious independencies by removing the contributors in order of their information. The full algorithm is given in Alg 3.</p><p>Formally, the score R(X,Y ; Z|{U i }) is the minimum between the two conditions that Z indeed contributes to I(X;</p><formula xml:id="formula_12">Y |{U i }) : R(X,Y ; Z|{U i }) = min (P nv (XY Z|{U i }), P b (XY |Z, { u i })) (2.2.7) Algorithm 3 MIIC network reconstruction Require: D -Skeleton reconstruction G ← the complete graph on V for all edges X -Y ∈ G do if I (X;Y ) &lt; 0 then Delete edge X Y from G Sepset{ X,Y } ← / 0 else Find most contributing node Z ∈ {adj(X) ∪ adj(Y )} which maximizes R(X,Y ; Z| / 0) end if end for while There is a link X -Y with R(X,Y ; Z|{U i }) &gt; 1/2 do for Top link X -Y with highest rank R(X,Y ; Z|{U i }) do Expand contributing set {U i } ← {U i } + Z if I (X;Y |{U i }) &lt; 0 then Delete edge X Y from G Sepset{ X,Y } ← {U i } else Find next most contributing node Z ∈ {adj(X) ∪ adj(Y )} and compute R(X,Y ; Z|{U i }) end if Sort the rank list R(X,Y ; Z|{U i }) end for end while -Skeleton orientation Sort list of unshielded triples L c = { (X, Z,Y ) X Y } in decreasing order of | I (X;Y ; Z|{U i })| repeat Take (X, Z,Y ) X Y ∈ L c with highest | I (X;Y ; Z|{U i })| on which R 0 or R 1 orientation rules can be applied if I (X;Y ; Z|{U i }) &lt; 0 then if (X, Z,Y ) X Y has no diverging orientation, apply R 0 and orient X → Z ← Y else if (X, Z,Y ) X Y has one converging orientation, apply R 1 and orient X → Z → Y end if Update all orientations of (X, Z,Y ) X Y ∈ L c until No additional orientation can be obtained return G Where P nv (XY Z|{U i }) is the probability that X -Z -Y is not a V-structure : P nv (XY Z|{U i }) = 1 1 + e -N I (X;Y ;Z|{U i }) (2.2.8)</formula><p>Chapter 2. Causal inference and P b (XY |Z, {U i }) the probability that the base is X -Y</p><formula xml:id="formula_13">P b (XY |Z, {U i }) = 1 1 + e -N I (X;Z|{U i }) e -N I (X;Y |{U i }) + e -N I (Y ;Z|{U i }) e -N I (X;Y |{U i })</formula><p>(2.2.9)</p><p>The orientation rules are also based on information theoretic measures, and can even be expressed with probabilities (we refer the reader to <ref type="bibr">[35]</ref> for the full derivations). This also makes it more robust than PC, even with majority or conservative rules.</p><p>Much like FCI, MIIC is also able to take into account and discover latent variables <ref type="bibr">[44]</ref>, making it more apt to analyze real-life datasets. It was however limited to discrete data, for which estimating I is rather straightforward.</p><p>One of the main objectives of this thesis was to adapt MIIC to any distribution p(V ), which means developing a mutual information estimator for any type of variable : discrete, continuous or a mixture of both. In the next chapter, we formally define the mutual information and its connection to causal graphs, before introducing our general case estimator developed for general constraint-based reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual information for general constraintbased causal inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mutual information and Conditional mutual information</head><p>The mutual information is a measure of the dependency between two random variables in the most general sense. It is agnostic to the nature of the random variables and of their relationship : noted I(X;Y ), it simply defines the quantity of information one knows about X by knowing Y , and vice-versa. It was introduced by Claude Shannon in 1948 to characterize communication channels <ref type="bibr">[45]</ref> but it has found success in a wide range of applications since. It is still seen by many as the ideal dependency measure, although it is difficult to use in practice as we will see in this chapter.</p><p>In this chapter, I review previous work and present how we developed a new general case (conditional) mutual information estimator for constraint-based causal discovery on mixed variables, introduced in <ref type="bibr">[7]</ref>. The section is organized a follows : I first give the necessary definitions of information theoretic concepts, review the existing estimators for both the discrete and continuous case on finite data, and I introduce the concept of optimal discretization in terms of maximizing a penalized mutual information and detail our implementation. Then, I show qualitative and quantitative results on our discretization scheme to estimate the mutual and conditional information and assess its significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entropy and mutual information</head><p>Before giving the definition of the mutual information between two random variables, it is a good idea to start with the self-information contained in a single variable, called the entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>Chapter 3. Mutual information for constraint-based inference Let X be a discrete random variable with possible values in X and a probability mass function p(x) = Pr{X = x}, x ∈ X. The entropy H(X) of X is defined by :</p><formula xml:id="formula_14">H(X) = -∑ x∈X p(x) log p(x) (3.1.1)</formula><p>It is expressed in bits with a logarithm to the base 2, or nats with the base e, and it denotes the average information or "surprise" that is carried by a random variable. To get a better understanding of this concept, consider a game of chance where you try to predict the result of a coin flip. If the coin is balanced, each realisation has the same "surprise" as both outcomes, heads or tails, are equiprobable. When the coin is biased towards one outcome with probability p, the average surprise decreases as p approaches 0 or 1 at which point it becomes null and your willingness to bet on the outcome increases. Note that the entropy characterises the distribution of a random variable and not the surprise of one realisation. This definition can be naturally extended to a pair of random variables X and Y (which can be thought of a single two-dimensional variable), giving the joint entropy :</p><formula xml:id="formula_15">H(X,Y ) = -∑ x∈X ∑ y∈Y p(x, y) log p(x, y)<label>(3.1.2)</label></formula><p>We can also define the conditional entropy, i.e. the expected "surprise" of the conditional distribution of a variable Y given X :</p><formula xml:id="formula_16">H(Y |X) = -∑ x∈X ∑ y∈Y p(x, y)log p(x, y) p(x) (3.1.3)</formula><p>One desirable property of these information-theoretic values is that they can be combined in an intuitive manner with the "chain rule" :</p><formula xml:id="formula_17">H(X,Y ) = H(X) + H(Y |X) (3.1.4) H(Y |X) = H(X,Y ) -H(X) (3.1.5)</formula><p>Indeed, it comes easily to think of the joint entropy of X and Y as the sum of the information carried by X plus the residual information of Y after "removing" the knowledge of X, as some information may be redundant between the two. So far we have only defined the entropy of discrete variables, but our ideal dependency measure should also include continuous or mixed (part discrete, part continuous) distributions which are present in real-life datasets. Continuous variables are defined by a probability density function f (x) instead of a mass function, which was naturally considered by Shannon to be equivalent in the definition for their entropy. There are however subtle differences with the discrete counterpart, which is why this value is called the differential entropy and is noted h(X) :</p><formula xml:id="formula_18">h(X) = - S f (x) log f (x) dx, (3.1.6)</formula><p>with S as the support set of X where f (x) &gt; 0.</p><p>The source of the differences between differential and discrete entropies becomes evident with our previous example of predicting the value of a random variable : what is the surprise of a realisation of X given that there is an infinite number of possible values in any continuous interval, each with a probability that tends to 0 ?</p><p>It is better to think of the differential entropy as an estimate of the effective volume that a random variable occupies : a very focused distribution will have a low entropy as opposed to a more dispersed distribution with more room for randomness hence higher entropy. Formally, the differential entropy is the logarithm of the length of the smallest interval that contains most of the probability <ref type="bibr">[4]</ref> : for example, the differential entropy of a uniform distribution on the interval [0, a] is log(a).</p><p>Although the differences between entropy and differential entropy go beyond the scope of this thesis, it is still interesting to mention them to understand why discretization has been so popular for so long to estimate the entropy or mutual information between samples of continuous variables. Even though, as we will see in sections 3.1.4 and 3.2, one must be careful to discretize a continuous variable without introducing bias.</p><p>Entropy and differential entropy behave similarly and are interchangeable in the settings that interest us, namely for the joint and conditional differential entropy, the chain rule and especially the relationship to mutual information. For the rest of section 3.1.1, the probability mass function p(x) can be replaced by the density function f (x), and H(X) by h(X) to switch from discrete to continuous random variables. The special case of mixed variables with both continuous and discrete parts will be reviewed at a the end of this section.</p><p>We have established that information theory gives us the necessary tools to define the entropy of a random variable (which can be multidimensional or conditional), i.e. the amount of information needed on average to describe it. Next we show how we can also formalize how much information two variables have in common, giving a measure of how (in)dependent they are. For this we need to introduce the Kullback-Leibler divergence, also called the relative entropy. The relative entropy D KL (p q) is a measure of the difference between two distributions p and q defined on the same space X:</p><formula xml:id="formula_19">D KL (p q) = ∑ x∈X p(x) log p(x) q(x) (3.1.7)</formula><p>Also called the Kullback-Leibler distance (although not a distance in the usual sense as it Chapter 3. Mutual information for constraint-based inference</p><formula xml:id="formula_20">H(X|Y ) H(Y |X) H(X) H(Y ) I(X;Y ) H(X,Y ) Figure 3</formula><p>.1: Relationship between the entropy, conditional entropy, joint entropy and mutual information between two variables is not symmetric), it can thought of as the cost of describing the distribution p when using q as a reference model. As such, it is null if and only if p = q and it is always non-negative.</p><p>Now we get back to our original goal which is to define the dependency between two random variables X and Y . In the most general sense, X and Y are independent if the realization of one does not affect the probability distribution of the other. Formally put, two random variables X and Y with marginal distributions p(x), p(y) and a joint distribution p(x, y) are independent if and only if p(x, y) = p(x)p(y). If these two quantities differ, some information is being shared between X and Y : knowing about X tells us something about Y and vice versa.</p><p>Using the measure of divergence we just introduced, it becomes natural to think of the divergence between the joint distribution and the product of marginals as a direct measure of the dependency. It is in fact the definition of the mutual information I(X;Y ) :</p><formula xml:id="formula_21">I(X;Y ) = D KL (p(x, y) p(x)p(y)) (3.1.8) = ∑ y∈Y ∑ x∈X p(x, y) log p(x, y) p(x) p(y) (3.1.9)</formula><p>In agreement with our interpretation of the relative entropy, assuming the independence model where p(x, y) = p(x)p(y) the mutual information is literally the extra bits that are required to encode the interaction between X and Y . It is always positive, or null if and only if X and Y are independent.</p><p>Just like the other measures, it fits naturally in the "chain rule" and can be expressed intuitively in terms of entropies (Fig 3 .1):</p><formula xml:id="formula_22">I(X;Y ) = H(X) -H(X|Y ) (3.1.10) = H(Y ) -H(Y |X) (3.1.11) = H(X) + H(Y ) -H(X,Y ) (3.1.12)</formula><p>What makes the mutual information a particularly interesting measure is its unique blend of desirable properties.</p><p>First, it satisfies the Data Processing Inequality (DPI) which states that one cannot increase the information content of a signal by processing it. Formally, if n variables form a Markov chain</p><formula xml:id="formula_23">X 0 → X 1 → • • • → X n , then I(X i ; X j ) ≥ I(X i ; X k ) with i &lt; j &lt; k.</formula><p>In relation to the first point, mutual information is also widely considered to be equally sensitive to all types of relationships. This concept was termed "equitablity" by Reshef et al. <ref type="bibr">[46]</ref> (although in a flawed form) and was then formally investigated by Kinney et Atwal <ref type="bibr">[27]</ref>. Kinney et Atwal's "Self-Equitability" is defined to characterize a dependence measure D[X;Y ] if and only if it is symmetric between X and Y , and :</p><formula xml:id="formula_24">D[X;Y ] = D[ f (X);Y ]<label>(3.1.13)</label></formula><p>with f any deterministic function, X ↔ f (X) ↔ Y forming a Markov chain. Put roughly, an equitable measure means that one can measure the strength of the signal (as compared to the noise) between Y and f (X) without having to know the underlying function f .</p><p>Not only is it invariant to invertible transformations of X and Y , it is also invariant under any monotonic (i.e. rank preserving) transformations. Put together, these three properties make the mutual information particularly interesting for general case causal discovery. True causal discovery should make no assumption of the natural mechanisms that produced the observed data, whether on the scale of the unit or shape of the joint distributions. As a simple example, a case can be made to measure the human weight in a logarithmic scale instead of a linear one : for most health related aspects, a difference of 30 kilograms is much more significant between 60 and 90kgs than between 120 and 150kgs. In an experimental context, we can think of the causal diagram as the natural laws that have produced the observations, which are themselves a function of the "observing" process. The self-equitability property and invariance under transformation go some way towards freeing ourselves from this observation process and our own biases.</p><p>Finally, as will be discussed later, these properties hold for any type of variable X and Y , be it continuous, discrete (ordinal or not), or a mixture of discrete and continuous parts.</p><p>A notable disadvantage of mutual information compared to other measures is that the bit, unit of information, is not commonly understood, and the fact that it is unbounded upwards.</p><p>One usually cannot easily derive a p-value from a mutual information estimation on sampled data, which makes it harder to communicate (although the benefits of standardising the p-value have been called into question <ref type="bibr">[47,</ref><ref type="bibr">48]</ref>). Different ideas to evaluate significativity will be presented in Section 3.1.4, but for now we are only interested in the "oracle" value, when the sample size N tends to infinity and the strict equivalence X ⊥ ⊥ Y ↔ I(X;Y ) holds.</p><p>In the discrete case, its value is actually familiar as it is in fact the G-statistic multiplied by a factor of N. With O i the number of observations in a contingency table between two categorical variables X and Y , with i joint levels, and E i the expected counts under the null hypothesis X ⊥ ⊥ Y , the G-statistic is defined as :</p><formula xml:id="formula_25">G = 2 ∑ i O i log O i E i (3.1.14)</formula><p>Recall the definition of the KL divergence (Eq 3.1.7), the same formula as above except for the frequencies (noted o i and e i ) instead of the counts O i , E i . Using the frequencies, the G-statistic becomes :</p><formula xml:id="formula_26">G = 2N ∑ i o i log o i e i = 2N • D KL (o e) = 2N • I(X;Y )<label>(3.1.15)</label></formula><p>It also follows that the mutual information is related to the χ 2 test, as it is itself a second-order Taylor approximation of the G-statistic.</p><p>For two continuous variables, it may be harder to get a good intuition of what the mutual information measures. A first property that may seem odd is that if X is continuous and Y = X, then I(X;Y ) = ∞. This looks as though it contradicts the chain rule (Eq 3.1.10), since it implies that H(X) -H(X|X) = ∞. It is in fact one of the differences between entropy H and differential entropy h which is unbounded in the case of a singularity h(X|X) = -∞, unlike H which is always finite. Thankfully, this theoretical property does not bleed into the real world for several reasons : first, even continuous distributions have always finite differential entropies since we actually treat real numbers up to a finite number of significant digits. Second, much like a correlation coefficient on observed data never reaches 1, we should not ever need to estimate I(X;Y ) where X = Y . The analytical value of the mutual information on a bivariate Gaussian with correlation coefficient ρ is actually known :</p><formula xml:id="formula_27">I(X;Y ) = - 1 2 log(1 -ρ 2 ) (3.1.16)</formula><p>This equivalence is useful for practitioners who are unfamiliar with mutual information and wish to translate it to the better known dependence measure : thanks the self-equitability property if one could transform two variables to a bivariate Gaussian distribution preserving the signal-to-noise ratio, using Eq 3.1. <ref type="bibr">16</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional information and interaction information</head><p>Information theory also allows us to measure the conditional dependence between two variables X and Y given a third, Z. The conditional mutual information is defined as the expected value of the mutual information between X and Y given a third variable Z:</p><formula xml:id="formula_28">I(X;Y |Z) = E z [D KL (p((x, y)|z) p(x|z)p(y|z))] (3.1.17)</formula><p>It is symmetrically decomposable into two-points mutual informations :</p><formula xml:id="formula_29">I(X;Y |Z) = I(X;Y, Z) -I(X; Z) = I(Y ; X, Z) -I(Y ; Z) (3.1.18)</formula><p>where X, Z and Y, Z are joint variables. The conditional mutual information can only be positive, or null if and only if X ⊥ ⊥ Y |Z.</p><p>Finally, the information between more than two variables is called the interaction information. We define it for three variables X,Y, Z and a conditioning set U i : 3: Relationship conditional mutual information and three-point information with three variables. Note that the three-point information can be negative when two variables are pairwise-independent but become dependent when conditioning on the third.</p><formula xml:id="formula_30">I(X;Y ; Z|{U i }) = I(X;Y |{U i }) -I(X;Y |{U i }, Z) = I(X; Z|{U i }) -I(X; Z|{U i },Y ) = I(Y ; Z|{U i }) -I(Y ; Z|{U i }, X) (3.1.19)</formula><p>Unlike the other measures introduced so far, it can be both positive and negative. A positive interaction information indicates that the three variables share some common information. It is negative when there is more information when taking the three variables together than independently. To illustrate this property, we borrow the concept of V-structure from causal diagrams. Consider the 4 possible DAGs with 3 nodes and two edges, shown in As Bayesian networks, the first three graphs encode the same conditional dependencies : X ⊥ ⊥ Y , and X ⊥ ⊥ Y |Z. In terms of informations, I(X;Y ) &gt; 0 and I(X;Y |Z) = 0. They all share some information, either due to a common cause or having a continuous "flow" of information, so I(X;Y ; Z) is also positive. Only the fourth graph on the right shows a different pattern : X and Y are marginally independent (I(X;Y ) = 0), but become dependent when conditioning on Z (I(X;Y |Z) &gt; 0). This is the situation where we "create" information by looking at the interaction of the three variable, and the three-point information I(X;Y ; Z) is negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Mixed variables</head><p>In many real-life datasets, particularly in medical records of patients, we might encounter both discrete and continuous variables and want to measure their interactions, without favoring one type of variable or the other. In other words, the dependency measure should scale with the signal-to-noise ratio the same way for continuous-continuous, discrete-discrete or discrete-continuous combinations. In the context of constraint-based approaches, another layer of difficulty is added as the same applies for the variables of the conditioning set.</p><p>There also exists yet another type of variables that has the characteristics of both continuous and categorical variables, and fits neither definition. For example, think of the height measured in centimeters without decimals : it is not defined on a truly continuous interval as it has non-zero probability to take certain values, but it also has too many unique values, which are potentially infinite (but countable), to be considered discrete. Many estimators depend on one or the other of these properties (continuous density or finite number of levels) to measure the dependency between observations, and will likely struggle to give an unbiased estimation on this type of variable <ref type="bibr">[49,</ref><ref type="bibr">50,</ref><ref type="bibr">51,</ref><ref type="bibr">52]</ref>. Another problematic distribution is the mixture random variable, which is itself a mixture of discrete and continuous parts. A prominent example of this would be a distribution bounded by a minimum value before a certain threshold, and a continuous function of x after it. In such a case the "minimum value" x min can be seen as the discrete part of the distribution as p(x min ) &gt; 0, with the rest behaving like a continuous variable. Real-life examples include the values produced by real-time quantitative polymerase chain reaction (RT-qPCR), used to measure the levels of of messenger RNAs in a cell. One can view the data produced by RT-qPCR as (A(x) if x &gt; threshold, else 0) with x the level of mRNA in the cell and A the amplification process. Another straightforward example is the ReLU activation function f (x) = max(0, x) widely used as an activation function in artificial neural networks (Fig 3 <ref type="figure" target="#fig_67">.5</ref>).</p><p>The ReLU function was actually discussed recently in the context of mutual information, and the problems that classical estimators face with such zero-inflated distribution. Naftali Tishby was a prominent computer scientist and physicist, who also contributed to signal processing and tried to apply information-theoretic concepts to gain intuition on deep learning algorithms. With Pereira and Bialek, he proposed the Information Bottleneck framework, a self-described surprisingly rich framework for discussing a variety of problems in signal processing and learning with information theory <ref type="bibr">[53]</ref>. Put simply, the idea of the Information Bottleneck is to "squeeze" a signal X to a compressed representation T while minimizing the With the Lagrange multiplier β for controlling how much loss we can tolerate when predicting Y from T as opposed to X (recall the DPI principle, since Y → X → T is a Markov chain, I(X;Y ) ≥ I(T ;Y )). By minimizing this difference, we want to reduce X to the only part that is relevant to Y , discarding the rest. This very general framework provides an elegant, if unpractical, solution to the majority of modern machine learning which has to learn which aspects of the input X is useful for predicting Y , and which are noise.</p><p>As deep learning models gathered success faster than a comprehensive theory could definitely explain why they work and how they can be further improved, Shwartz-Ziv and Tishby published new evidence that they claimed could explain the process of training a deep neural network <ref type="bibr">[54]</ref>. In their experiments, they equated the noisy encoding T of the information bottleneck to the hidden layers of a deep neural network (DNN) and measured I(X; T ) and I(T ;Y ) during the training process. Their results showed that the training process acts in two separate phases : first, the fitting phase in which the network maximizes I(T ;Y ), and then a compression phase that minimizes I(X; T ). This was an unprecedented window inside the "black box" of deep learning and could potentially explain how they train, and most importantly how they are able to generalize. Later however, more studies were published and seemed to show that the two phases observed in the original experiment were not in fact an information-theoretic phenomena, but more of an artefact of how the mutual information is estimated between the hidden layers and Y . Saxe et al. could not replicate the two phases in other network architectures from the ones tested in the original study, and in particular no compression phase was observed when training with linear activation functions or ReLU <ref type="bibr">[55]</ref>. In response, Shwartz-Ziv and Tishby claimed that Saxe et al. had used a weak estimator of mutual information, and defended their general claim saying that "when properly done, there are essentially the same fitting and compression phases" on any network. There are however other reasons to believe the compression phase observed in the original study was more a result of geometric operations as the weights of the network are trained, and does not hold so much ground in information theory <ref type="bibr">[56,</ref><ref type="bibr">57]</ref>. Moreover, the simple DNNs are no longer used in practice, they are being replaced by extremely scaled up versions (with too many parameters in hidden layers for mutual information to ever be estimated) or more sophisticated architecture involving different training mechanisms like transfer learning, attention mechanisms etc... diverging from the simple picture of training that was examined.</p><p>We may not know the final word on the information bottleneck for deep learning, but it serves as a cautionary tale when we want to rely on mutual information estimates on big data (as the dimension of X gets large) and the distributions are unfamiliar. It is fortunately not the case for constraint based causal discovery approaches, where X and Y are usually one-dimensional, and the conditioning set Z few-dimensional. Moreover, recent advances were made to better understand mutual information estimators, including on such mixed distributions, as will be discussed in the next section.</p><p>It is not obvious if we are still allowed to swap differential entropy for entropy when considering the mixed case. Crucially, it is not well defined for mixture distribution which are defined neither by a probability density function nor a mass function alone.</p><p>Recent efforts to estimate the mutual information in this general setting have relied on the Radon-Nikodym theorem. With P XY a probability measure on the space X × Y, X and Y being Euclidean spaces. If P XY is absolutely continuous with relation to P X P Y :</p><formula xml:id="formula_31">I(X;Y ) = X×Y log dP XY dP X P Y dP XY ,<label>(3.1.20)</label></formula><p>where dP XY dP X P Y is the Radon-Nikodym derivative. Note the only condition this definition is absolute continuity of P XY , and if true it applies for all cases mentioned so far : X and Y are the same type of variable, X or Y is discrete and the other is continuous, or X, Y or the joint distribution is a mixture itself. Moreover, the Radon-Nikodym derivative is computable in practice <ref type="bibr">[51]</ref>.</p><p>Another way to deal with mixtures is to refer to the master definition of mutual information <ref type="bibr">[4]</ref>. For two random variables X and Y discretized with partitions P and Q :</p><formula xml:id="formula_32">I(X;Y ) = sup P,Q I([X] P ; [Y ] Q ) (3.1.21)</formula><p>where the supremum is over all finite partitions P and Q. It is called the master definition as it always applies, regardless of the nature of the marginal and joint distributions. For discrete Chapter 3. Mutual information for constraint-based inference variables it is simply equivalent to the definition of mutual information (Eq 3.1.9), i.e. the partitions are fixed. For continuous variables, the supremum is obtained by refining P and Q into finer and finer bins, monotonically increasing I([X] P ; [Y ] Q ) . When N → ∞, this quantity tends to the real value of the mutual information (just as the entropy of a discretized variable is approached as the numbers of bins tends to infinity). On a finite sample size however, adding bins to P and Q will inevitably end up overestimating the mutual information, to the limit of having one unique value per bin for which (which results in I(X;Y ) = log(N)).</p><p>In section 3.1.4, we review previous work on choosing the appropriate number of bins to estimate I on continuous data and in section 3.2 we introduce our solution based on the master definition.</p><p>As a general rule, methods that assume a continuous probability density function p(x, y) over the domain of X and Y tend to not work well in the mixed case. Any dependence measure having this assumption will need to be adapted (with more or less difficulty), which may also affect the way we can evaluate its significativity for independence testing. On the other hand, one can still rely on the cumulative distribution function, which is well behaved even for mixture variables (although may not be smooth). For example, decision-tree-based algorithms like random forests and gradient boosting work well with such mixtures (although they are not adapted to all cases, for example they do not deal well with non-ordinal categorical variables with many levels).</p><p>Mutual information is one of the rare measures fit to deal with such distributions, all while keeping its desirable properties. Particularly, its strict equivalence with variable independence (and conditional independence), and its self-equitability property make it ideal for a general case constraint-based algorithm for causal inference. In the next subsection we show the equivalences between information theoretic measures and "constraints" in the causal diagrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Mutual information and causal graphs</head><p>In this part, we make the usual assumptions to bridge the gap between distributions and causal diagrams, namely the Faithfulness and the causal Markov condition. We now take the time to review and discuss these conditions as they are quite relevant to the topic of general-purpose conditional dependence measures.</p><p>The first is the causal Markov condition, defined as follows :</p><p>Definition 3.1. The causal Markov condition states that, given a set of variables V with joint probability P(V ) governed by the true causal graph G c , any given node X is conditionally independent from any other non-descendant node Y given its parents Pa X :</p><formula xml:id="formula_33">X ⊥ ⊥ Y | Pa X .</formula><p>Put roughly, this is equivalent to the Markov condition of Bayesian graphs with extra precautions on ancestor vs descendant nodes to avoid collider bias, which "opens up" information flows by conditioning on a child (see <ref type="bibr">Fig 3.4)</ref>. Additionally, it means that a node is separated from the rest of the network, i.e. statistically independent from every other node, conditionally on its Markov blanket (Fig 2 <ref type="figure" target="#fig_44">.6</ref>). This first assumption may seem specific to causal diagrams but it actually reflects common scientific reasoning : we assume that two variables that are correlated are involved at some point in the same causal mechanism, whether it be due to a common ancestor or one causing the other <ref type="bibr">[1]</ref>.</p><p>The converse to the Markov condition is the faithfulness assumption : Definition 3.2. Given a set of variables V whose true causal graph is G c , their joint probability</p><formula xml:id="formula_34">P(V ) is faithful to G c if all conditional independences in P(V ) correspond to d-separation in G c .</formula><p>It is also called the "stability" property by Pearl <ref type="bibr">[1]</ref>, referring to the process by which P(V ) results from G c . Indeed, in general we can think of two ways independencies are created in the data (recall that with a causal diagram, p(X) = f (p(Pa X ),U X )). The first, intuitive way is to think that two variables are independent only if they can be separated in the causal diagram, either by removing the influence of a common cause or of the intermediate steps. These independencies are called "stable", they hold true under almost any parameterization of the causal mechanisms f (p(Pa X ),U X ). But there is another way to create independencies in P(V ), by carefully choosing mechanisms that either cancel each other statistically, or are undetectable marginally and appear only when looking at interactions between nodes. For example, an XOR gate X → Y ← Z produces an unfaithful distribution as X and Z are marginally independent from Y , but are clearly implicated in the generation function for Y . This type of relation is considered "unstable" as it is strictly dependent on its parameterization, any deviation from this equilibrium would create a different pattern of dependencies. As explained by Pearl, the faithfulness or stability assumption can be thought of as a filter on the type of causal relations that we are considering <ref type="bibr">[1]</ref>:</p><p>Any story that convincingly exemplifies a given pattern of dependencies must sustain that pattern regardless of the numerical values of the story parametersa requirement we called "stability."</p><p>This discussion on stable distributions prompts us to make another aside in deep learning, as an interesting parallel can be made with the recent theory of Causal Invariance advocated by Léon Bottou and Jonas Peters <ref type="bibr">[58,</ref><ref type="bibr">59,</ref><ref type="bibr">42]</ref>. As we know, machine learning in its current form is entirely dependent on statistical dependencies in the training data and while it has found success with many applications it does not generate real knowledge on how the world functions. Invariant causal prediction aims to discover causal knowledge by observing correlations in many different environments (different interventions in a causal context, or different training datasets for machine learning) and positing that the true causal relationships Chapter 3. Mutual information for constraint-based inference are the ones that hold true across the different conditions. It reflects the human thought process of observing our environment and detecting the patterns that stay true even in different situations.</p><p>We can see that this actually echoes the faithfulness assumption in the sense of stable distributions. Indeed, causal mechanisms produce stable distributions because the true relationships between two variables are invariant to external influence on other parts of the system, i.e. to changes in the environment. The main difference is that causal invariance is only interested in discovering the causes of Y as a set of variables, and not the full structure of the causal graph. This matters for example for the XOR relationship, where causal invariance would be able to detect both parents X and Y as causes of Z (provided they are always both included in the varying environments), whereas causal discovery would struggle to find the corresponding graph as the corresponding P(V ) is unfaithful in relation to the graph.</p><p>In other words, with the faithfulness assumption, we restrict our modelling of causal mechanisms to relationships X -Y that stay invariant when modifying any other part of the causal graph. Crucially, this excludes interactions terms (like the XOR gate) and cancelling paths. Causal invariance aims for a larger class of mechanisms by looking for laws that stay invariant when changing the environment of the entire set {Pa Y ,Y }. This difference points to the inherent difficulty to represent interaction terms between nodes in Bayesian networks, as opposed to the multidimensional input in mainstream machine learning.</p><p>The Markov condition is generally considered as the less problematic assumption of the two. It is more of a convention, to limit the task of causal discovery to complete models, including latent variables (when would we want to discover incomplete models?). The faithfulness assumption on the other hand has generated discussion ever since the first constraint based method was published and an extensive body of work has aimed to remove or relax it <ref type="bibr">[1,</ref><ref type="bibr">20,</ref><ref type="bibr">60,</ref><ref type="bibr">61,</ref><ref type="bibr">62]</ref>. With the previous discussion, we hope to have given enough reasons to consider it a fair assumption as we assume both are true going forward.</p><p>These assumptions are actually one step removed from the way constraint-based approaches work, as they have to estimate (conditional) independences from the data. As we have shown in Section 2.2.3, independence testing comes with a set of assumptions of its own, which narrows further the definition of causal mechanisms that most constraint-based approaches actually discover. This is not the case with mutual information, thanks to its strict equivalence with statistical independence and its self-equitability property. In the rest of this section, we show the direct equivalence between causal diagrams and information measures, and in the next section we discuss how they are estimated from finite data. For a collection of variables which we note X, Y , Z..., in a causal graph G c :</p><p>• If X and Y are adjacent in G c , then I(X;Y ) &gt; 0, and I(X;Y |Z) &gt; 0 for any set Z. This follows from the direct equivalence</p><formula xml:id="formula_35">X ⊥ ⊥ Y ⇔ I(X;Y ) = 0 • Similarly, if X and Y are d-separated by Z in G c , then I(X;Y |Z) = 0.</formula><p>• If Y is a direct parent of X and Y 2 an ancestor of X only through Y , then I(X;Y ) ≥ I(X;Y 2 ) (according to the DPI).</p><p>• If X → Z ← Y forms a V-structure and X and Y are d-separated by U i , then I(X;Y ; Z|U i ) &lt; 0. <ref type="bibr">[35,</ref><ref type="bibr">5]</ref> • And the inverse, if X -Y -Z does not form a V-structure and X and Y are d-separated by U i , then I(X;Y ; Z|U i ) &gt; 0</p><p>For constraint-based causal discovery, we can also exploit three-point informations to iteratively take off the best contributors, as was shown in <ref type="bibr">[5,</ref><ref type="bibr">35]</ref> to develop MIIC.</p><formula xml:id="formula_36">I(X;Y |{U i }, Z) = I(X;Y ) -I(X;Y ;U 1 ) -I(X;Y ;U 2 |U 1 ) -• • • -I(X;Y ; Z|{U i }) (3.1.22)</formula><p>Another contribution of Affeldt et al. is the definition of orientation probability from three point information. Arrow head probabilities stem from v-structures like X → Z ← Y , corresponding to a negative conditional 3-point information (Fig 3 .4), I(X;Y ; Z | {U i }) &lt; 0, where {U i } separates X and Y , i.e. X ⊥ ⊥ Y |{U i } <ref type="bibr">[5]</ref>. The head orientation probabilities can then be obtained through the probability decomposition formula as, P(x → z) = P(x → z|z ← y)P(z ← y) + P(x → z|z --y)P(z --y) <ref type="bibr">(3.1.23)</ref> or equivalently, writing the probability of a v-structure as P(x → z, z ← y) = P(x → z|z ← y)P(z ← y) (3.1.24) P →← P →← + P →--+ P --← + P ---- = P →|← P →|← + P --|← P(z ← y) (3.1.25)</p><formula xml:id="formula_37">1 1 + 3e N I(X;Y ;Z | {U i }) = 1 1 + e N I(X;Y ;Z | {U i }) P(z ← y) (3.1.26)</formula><p>which leads (by x/y symmetry) to</p><formula xml:id="formula_38">P(x → z) = P(z ← y) = 1 + e N I(X;Y ;Z | {U i }) 1 + 3e N I(X;Y ;Z | {U i }) (3.1.27)</formula><p>By default, we orient V-structures if the probability is larger than 0.5, but we can also be more or less strict and choose any arbitrary threshold. In Section 4.1.2, we introduce new probabilities that behave better numerically for large N.</p><p>Going beyond the problem of graph inference, Wieczorek et al. also showed how to derive causal effect quantities using only information theoretic terms <ref type="bibr">[63]</ref>. The causal graph alone informs us of the presence or absence of interactions, but we have no way of comparing Chapter 3. Mutual information for constraint-based inference them or even knowing if a given edge is a strong or a weak effect. These questions are answered by causal effect quantification, which has been studied in various forms. In the framework provided by Pearl's diagrams, the the causal effect of X on Y is simply described by the interventional distribution P(Y |do(X)). Other measures of causal strength (notably in Rubin's potential outcome framework) are the average treatment effect and the specific causal effect. The average treatment effect for binary variables is the expected value of the difference between Y |do(X = 1) and Y |do(X = 0) :</p><formula xml:id="formula_39">ACE(X,Y ) = E[Y |do(X = 1) -Y |do(X = 0)] (3.1.28)</formula><p>And the specific causal effect is the average treatment effect conditional on a particular value of other variables Z (provided that Z are non-descendant of X in G)c). The contribution of <ref type="bibr">[63]</ref> is to prove that the combination of conditional mutual information and conditional directed information also give a rigorous framework for causal effect quantification from causal diagrams. The directed information is typically used in time-series analysis, it measures the amount of information that flows from one process to the other. In the context of interventions, Wieczorek et al. define it as :</p><formula xml:id="formula_40">I(X → Y ) = D KL (P(X|Y ) P(X|do(Y ))|P(Y ))<label>(3.1.29)</label></formula><p>This line of work ties up the relationship between mutual information and causal diagrams.</p><p>From graph inference with MIIC to the quantification of causal effects, all of the necessary concepts in causality can be expressed with information theoretic measures.</p><p>But can it be used in practice ? Earlier, we hinted at potential issues and shortcomings when estimating mutual information on sampled data. In the next section we present previous work on existing estimators before introducing our new method that can estimate mutual and conditional mutual information on any type of data and also assess its significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Existing estimators on finite data</head><p>The previous section has established why we would want to estimate information-theoretic quantities from data, and now we will study how and how well it can be done. Several decades of research later, and almost as many different estimators as there were applications, it may come as a surprise that many basic questions remain unanswered (although recent progress has been made, especially in the continuous case). To understand why, recall that mutual information I(X;Y ) is defined for X and Y of any dimensions. For many applications in neuroscience, X may be the activation of hundreds or thousands or neurons, and Y a single-dimensional stimulus or response. Estimating I(X;Y ) from sampled data in this setting is a very different problem than estimating it between two single-dimensional signal! In this section we focus on the use of (conditional) mutual information for constraint-based algorithms, where X and Y are single-dimensional variables and Z may be multidimensional (but rarely very large).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete estimators</head><p>Estimating I(X;Y ) on discrete data is the most straightforward case. We can simply estimate the probability mass functions p(x), p(y) and p(x, y) from independently and identically distributed (i.i.d) data by counting how many times we observe each level. Using the chain rule (Eq 3.1.10), we actually only need an entropy estimator Ĥ to get an estimation Î. Using the observed frequencies pi with i ∈ [1, m], we get what is called the "plug-in" or "naive" estimator :</p><formula xml:id="formula_41">ĤNaive = - m ∑ i=1 p i log p i (3.1.30)</formula><p>Note that it is also the maximum likelihood estimator from the observed data. It is however suboptimal, it has long been known that it is negatively biased everywhere <ref type="bibr" target="#b222">[64]</ref>. The short explanation is that while pi is estimated with symmetric variance on either side of the true frequency p i , the log transformation amplifies more variance towards 0 than towards 1, and the contribution of each pi ends up being underestimated on average. To correct this shortcoming, a common fix is to add the Miller-Madow correction <ref type="bibr" target="#b223">[65]</ref>:</p><formula xml:id="formula_42">ĤMM = ĤNaive + m -1 2N (3.1.31)</formula><p>with m the number of categories with nonzero probability as estimated from the pi . This correction effectively reduces the bias of ĤNaive without adding any complexity, and is preferred in many contexts.</p><p>Another popular idea is to use a jacknife resampling procedure, which trades lower bias for a slightly higher complexity <ref type="bibr" target="#b224">[66]</ref> :</p><formula xml:id="formula_43">ĤJK = N ĤNaive - N -1 N N ∑ j=1 ĤNaive-j (3.1.32)</formula><p>where ĤNaivej is the naive estimator without the jth sample.</p><p>Finally, another way to correct the negative bias of the naive estimator is to act directly on the estimates pi instead of applying a correction a posteriori. The Schurmann-Grassberger estimator does exactly that, by applying prior Bayesian belief that the samples follow a Dirichlet distribution (the multivariate generalization of the Beta distribution) <ref type="bibr" target="#b225">[67]</ref>. It essentially "tricks" the estimator to think that more counts have been observed to compensate for the negative bias of the naive estimator, such that mN becomes the a priori sample size. The result is a less biased estimator, but the choice of the prior end up dominating the Chapter 3. Mutual information for constraint-based inference estimation <ref type="bibr" target="#b226">[68]</ref>.</p><p>All of these improved estimators have been designed for the setting where I(X;Y ) &gt;&gt; 0, as opposed to constraint-based discovery where we are more interested in the independence regime. Importantly, they all share another kind of bias : they overestimate dependencies on finite data. Without knowing the true distributions, any of these estimators will be positive Î(X;Y ) &gt; 0 (resp. Î(X;Y |Z) &gt; 0) almost surely, even when X ⊥ ⊥ Y (resp. X ⊥ ⊥ Y |Z). Several suggestions have been made, mostly based on fixed thresholds as a function of the sample size. A more inspired approach is to also take into account the distributions of the variables : indeed, we do not expect the same bias from sampling simple binary variables with balanced levels, versus more complicated variables with many unbalanced categories. This is the route taken by MIIC, which corrects the naive estimate by subtracting a complexity cost that depends on X, Y and Z. It frames each test of independence in the context of graph reconstruction, favoring simpler models with fewer edges. Namely, it introduces a complexity cost for the edge X -Y potentially separated by separating set U i , noted k X;Y |{U i } . Then, the condition I(X;Y |U i ) &lt; k X;Y |{U i } (N)/N to remove the edge X -Y favors the simpler model compatible with the independencies in the sense of the model complexity, given the observed data. This replaces the strict equivalence I(X;</p><formula xml:id="formula_44">Y |U i ) = 0 ⇔ X ⊥ ⊥ Y |U i which is only valid in the limit N → ∞. The challenge now is to choose the form of k X;Y |{U i }(N) .</formula><p>A common complexity cost used in model selection would be the Bayesian Information Criterion :</p><formula xml:id="formula_45">k BIC X;Y |{U i } (N) = 1 2 (r X -1)(r Y -1) ∏ i r U i log(N) (3.1.33)</formula><p>with r X , r Y , r U i the number of categories of each variable (U i being a joint variable). This complexity cost can be improved by also taking into account the distributions of the variables, not only their number of levels <ref type="bibr">[35]</ref>. Such a score will be discussed in Section 3.2.1 when introducing the new MDL-optimal discretization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous estimators</head><p>Compared to the discrete case, estimating Î on continuous data is notoriously difficult. Historically, one of the most common way to deal with continuous data was to discretize them into bins, the same way we construct histograms. We note [X] and [Y ] the quantized version of X and Y on finite data. This approach is conceptually straightforward, we can simply compute Î([X]; [Y ]) with any discrete estimator and take it as an approximation of I(X;Y ).</p><p>Perhaps because we are used to seeing histograms and picking the correct number of bins visually, surprisingly many applications perform this kind of naive discretization without much justification. In practice however, both the number of bins and their locations dominate the estimation. Even for large N, Î([X], [Y ]) converges on some value that depends on the discretization parameters rather than I(X;Y ), namely the number of bins |∆ X | and |∆ Y |, as well as their size <ref type="bibr" target="#b227">[69]</ref>. This bias was already documented as early as 1989, but was considered manageable if one chose a "reasonable number of cells" <ref type="bibr" target="#b228">[70]</ref>. But the question of what is "reasonable" is more complicated than it appears. For example, Ross et al. note that there is no optimal value of |∆| that works for all distributions : N 0.5 works well for the square wave distribution but N 0.7 is better for a Gaussian distribution <ref type="bibr" target="#b229">[71]</ref>. Similarly, Seok et al. show that even for Gaussian bivariate distributions with the same marginals, the "correct" number of bins that gives the best approximation of I(X;Y ) varies depending on the strength of the correlation ρ <ref type="bibr">[49]</ref>. Note that the same applies for any estimator that takes a number of bins as parameter, regardless of how clever the discretization scheme is (for example, using B-splines <ref type="bibr" target="#b230">[72]</ref>). Instead, it is essential to deduce the number of bins from the observations <ref type="bibr" target="#b231">[73,</ref><ref type="bibr" target="#b232">74]</ref>. Darbellay et al's recursive partitioning scheme <ref type="bibr" target="#b231">[73]</ref> is conceptually one of the closest approach to the novel estimator introduced in Section 3.2, but it is limited in the placement of the bins.</p><p>Another common approach is to compute the mutual information using analytical formulas, having estimated p(X), p(Y ) and p(X,Y ). It is only feasible for few applications with strong a priori on the data distribution, and even if we know the distributions the data is sampled from, only few analytical formulas for the information are known <ref type="bibr" target="#b233">[75]</ref>. Instead of being imposed some priors, the density functions can also be estimated via the usual methods using e.g. kernel functions <ref type="bibr" target="#b234">[76]</ref>. But, related to the problem of choosing the number of bins, one has to choose the type of kernel and its width, which has shown similar bias <ref type="bibr" target="#b228">[70]</ref>. It is also exponentially more complex as the support's dimensions increase, limiting its use for conditional independence testing even with few variable Zs in the conditioning set.</p><p>Undoubtedly, the best results on continuous data are obtained with the "KSG" estimator from Kraskov, Stögbaueur and Grassberger <ref type="bibr" target="#b235">[77]</ref>. We will also refer to this approach as the k-nn approach, as it employs a k-nearest neighbor estimation of the local entropy. It is based on earlier work by Kozachenko and Leonenko, who first derived an estimate of the entropy based on nearest-neighbor distances <ref type="bibr" target="#b236">[78]</ref> :</p><formula xml:id="formula_46">ĤKL (X) = 1 N N ∑ i=1 log Nc d,p ρ k,i k + log(k) -ψ(k) (3.1.34)</formula><p>with ρ k,i the distance from the jth sample to its kth nearest neighbor, c d the volume of the unit ball in d dimensions and ψ(.) the digamma function. The original authors introduced this formula for a fixed k = 1, proving its consistency as N increases, and <ref type="bibr" target="#b237">[79]</ref> proved it later for all k. Additionally, Jiao et al. derived an uniform upper bound on its performance proving its near optimality <ref type="bibr" target="#b238">[80]</ref>, a first for such estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 3. Mutual information for constraint-based inference</head><p>Given this strong estimator, a natural way to get to Î is to use the chain rule :</p><formula xml:id="formula_47">Î3KL = ĤKL (X) + ĤKL (Y ) -ĤKL (X,Y ) (3.1.35)</formula><p>This estimator is also consistent and performs fairly well in practice, but was shown to be uniformly inferior to the KSG estimator in many empirical settings. The KSG estimator is defined as :</p><formula xml:id="formula_48">ÎKSG (X;Y ) = ψ(k) + ψ(N) -ψ(n x,i + 1) + ψ(n y,i + 1) (3.1.36)</formula><p>with n x,i the number of points within an ρ k,i distance on the X dimension, and ψ(n x + 1) + ψ(n y + 1) the average taken on all samples. The ρ k i distance is usually taken with ∞ or 2 norm, see Fig 3 .6. Since its introduction, no other estimator seems to be as performant in most settings and it has become the go-to solution to estimate Î on continuous data. The particularity behind the KSG estimator is to compare H(X), H(Y ) and H(X,Y ) locally to estimate the mutual information directly, instead of having to estimate each of the three terms. Recently, Gao et al. revealed why this choice leads to uniformly better results than the Î3KL estimator. They have shown that the better performance stems from a correlation boosting effect, the bias of the joint entropy is positively correlated to the biases of the marginal entropies, which partly cancel each other when subtracting via the chain rule <ref type="bibr" target="#b239">[81]</ref>. It makes no assumption on either the marginal or joint distributions, and seems to be equitable to all relationships <ref type="bibr">[27]</ref>. Somewhat surprisingly, rank-ordering the variables still gives correct estimates (as it should), although it is not clear whether it should be preferred or not. It was conveniently adapted to the conditional case, also using a direct formula instead of the chain rule <ref type="bibr" target="#b227">[69,</ref><ref type="bibr" target="#b240">82]</ref> :</p><formula xml:id="formula_49">ÎKSG (X;Y |Z) = ψ(k) + ψ(n z,i + 1) -ψ(n xz,i + 1) -ψ(n yz,i + 1) (3.1.37)</formula><p>Still, we note a few disadvantages that discourage its use for general constraint-based algorithms. First, the variance and bias of the estimation are tied to the choice of the parameter k <ref type="bibr" target="#b241">[83]</ref>. The original authors themselves suggest a low k (2-4) for good a estimation ÎKSG , and much larger for independence testing (up to N/2). In general, the trade-off is high variance and low bias for small values of k, and less variance but increased bias for large k <ref type="bibr">[27,</ref><ref type="bibr" target="#b242">84]</ref>. Secondly, as is the case with discrete estimators, the equivalence Î(X;Y ) = 0 ⇔ X ⊥ ⊥ Y is not respected, as variance still exists at independence. Crucially, there are currently no results on the distribution of the estimator, either exact nor asymptotically, and there is no easy way to test for independence <ref type="bibr" target="#b222">[64,</ref><ref type="bibr" target="#b241">83]</ref>. Runge proposed to test for conditional independence using a local permutations scheme, which reliably estimates the null distribution but requires significantly more computation <ref type="bibr" target="#b243">[85]</ref>. Berrett and Samworth improved slightly on this idea, introducing an independence test based on either simulations when marginal distributions are known, or resampling when they are not <ref type="bibr" target="#b244">[86]</ref>.</p><p>Many other estimators exist, involving ensemble methods <ref type="bibr" target="#b245">[87]</ref>, copula transformations <ref type="bibr" target="#b246">[88]</ref>, dependence graphs <ref type="bibr" target="#b247">[89]</ref>, and even deep neural networks <ref type="bibr" target="#b248">[90]</ref>. Overall, the KSG estimator has shown the best performance in the settings that interest us, and is the best understood. It has even been adapted to the mixed case and mixture variables, as we shall see now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed estimators</head><p>Compared to the discrete and the continuous case, relatively little work has been done on estimating mutual information in the mixed case, where X is discrete and Y is continuous, and even less in the case of mixture variables.</p><p>Ross et al. extended the KSG estimator to the mixed case, by counting the number of nearest neighbors in the continuous space Y on the subset of samples that share the same discrete value of X. More specifically, for each sample i, the method first finds the distance to the kth nearest neighbor which also share the same discrete value, and counts the number of neighbors within this distance in the full data, noted m. This estimator is given by :</p><formula xml:id="formula_50">ÎRoss (X;Y ) = ψ(N) + ψ(k) -ψ(N X ) -ψ(m + 1)) (3.1.38)</formula><p>with N X the total number of data points that share the same discrete value on X.</p><p>It was then expanded by Gao et al. to mixture distributions by taking the average of the Radon-Nikodym derivative over all samples <ref type="bibr">[51]</ref>. The way to estimate this derivative depends on each sample : plug-in estimator when the point is discrete (i.e. more than k point share the same value, so ρ i,k = 0), and KSG estimator when there is a locally continuous joint density. The intuition behind this procedure is that the Radon-Nikodym derivative is well defined for all cases, and that it recovers either the plug-in estimator, the KSG estimator, or Ross's estimator depending on the local subspace. By then taking the average of all the Chapter 3. Mutual information for constraint-based inference derivatives, this gives the value Î(X;Y ) for any distributions X and Y . It was proven to be consistent, and has shown better results than binning procedures or noisy KSG on mixture variables. It shares however the same lack of significance test as the other k-nn estimators, which makes it less adapted to constraint-based algorithms.</p><p>Marx et al. also proposed a mixed estimator based on the Radon-Nikodym derivative and adaptive histogram models for the continuous parts of the mixture variables <ref type="bibr" target="#b249">[91]</ref>. Just as our approach introduced in <ref type="bibr">[7]</ref>, they devised an heuristic to find the optimal discretization according to the Minimum Description Length (MDL) principle <ref type="bibr" target="#b250">[92]</ref>. It also comes with easy independence testing with Normalised Maximum Likelihood (NML) correction on discrete data, as introduced in <ref type="bibr">[35]</ref> using the factorized NML criteria <ref type="bibr" target="#b251">[93]</ref> (which was later redefined by Marx et al., proving asymptotic behavior and consistency <ref type="bibr" target="#b252">[94]</ref>). It is well adapted to constraint-based algorithms, however it considers mixture variable in a slightly different way from Gao et al (and <ref type="bibr">[7]</ref>). This difference is best explained through an example. Let (X,Y ) be a mixture of one continuous and one discrete distribution. The continuous distribution is a bivariate Gaussian, with mean µ = 0, marginal variance σ = 1 and correlation ρ. The discrete distribution is two binary variables, with probabilities p</p><formula xml:id="formula_51">(X = 1,Y = 1) = p(X = -1,Y = -1) = β and p(X = 1,Y = -1) = p(X = -1,Y = 1) = β .</formula><p>These two distributions are then mixed with probability p con and p dis respectively. The ground truth as derived by Gao et al. is given by :</p><formula xml:id="formula_52">I(X;Y ) = -p con 2 × log(1 -ρ 2 ) + β 2 × log β /2 p 2 dis + (1 -β ) 2 × log (1 -β )/2 p 2 dis -p con × log p con -p dis × log p dis (3.1.39)</formula><p>Marx et al. used a different ground truth for this distribution, without the last two terms of the sum, -p con × log p con -p dis × log p dis . In their framework, X ⊥ ⊥ Y and I(X;Y ) = 0 if and only if ρ = 0 and β = 0.5. It is justified if one considers that the continuous and discrete parts do not share the same space, acting more like separate dimensions of the joint distribution. On the other hand, if we consider that all parts of X and Y share the same euclidean space, some information is "created" from the structure of the joint distribution, given by -p con × log p con -p dis × log p dis (which equals to log 2 when p con = p dis = 0.5). Indeed, even when ρ = 0 and β = 0.5, the distribution p(x, y) is far from p(x)p(y) due to the constraints imposed by sharing the same space (Fig 3 <ref type="figure" target="#fig_68">.7</ref>).</p><p>The second view is closer to the master definition of mutual information (Eq 3.1.21) which implies that we can use any partitioning to discretize X and Y , potentially combining discrete and continuous parts in a single bin. This also corresponds to the approach taken to develop our own estimator based on optimal binning of X,Y , introduced in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Developing a new general case estimator</head><p>So far, we have rationalized the advantages of using information theoretic measures for causal inference, but our review of available methods showed a lack of a good estimator that would work on any type of data and provide a fair assessment of its significativity. In this section, I first give a brief review of the MDL and NML frameworks, and I introduce the method that was published in <ref type="bibr">[7]</ref> to adapt MIIC to the general case, showing qualitative and quantitative results as a discretization scheme, an estimator of (conditional) mutual information and a test of (conditional) independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">MDL-optimal histograms</head><p>The Minimum description length (MDL) principle is rooted in information theory, it was developed by Jorma Rissanen at the end of the 20th century <ref type="bibr" target="#b250">[92,</ref><ref type="bibr" target="#b253">95]</ref>. The fundamental idea behind it is that there exists a code that describes the data in a more succinct way than copying the data itself, and that the smaller the code, the more we have learned about the data. It provides a general and powerful framework for performing model selection given the available data by separating the signal from the noise. Specifically, the best model is the model with the smallest stochastic complexity, which is the shortest description length of a given data relative to a model class M. This very abstract concept of minimizing the description length actually found an elegant solution via the normalized maximum likelihood (NML) distribution <ref type="bibr" target="#b254">[96,</ref><ref type="bibr" target="#b253">95]</ref>, and found success in various applications, from data clustering <ref type="bibr" target="#b255">[97]</ref> to image denoising <ref type="bibr" target="#b256">[98]</ref>. We now define the NML density and show how it can be used Chapter 3. Mutual information for constraint-based inference to find optimal discretizations on finite data, as developed in <ref type="bibr" target="#b257">[99]</ref>.</p><p>Let x n = (x 1 , ..., x n ) be a data sample of n outcomes in the space X, and θ (x n ) its maximum likelihood estimate. The normalized maximum likelihood density is defined as :</p><formula xml:id="formula_53">f NML (x n |M) = f (x n | θ (x n ), M) C n M (3.2.1)</formula><p>where C n M is the universal normalizing constant, and is given by :</p><formula xml:id="formula_54">C n M = ∑ x n ∈X n f (x n | θ (x n ), M) (3.2.2)</formula><p>The stochastic complexity of the data x n , the quantity to be minimized, is defined via the NML density :</p><formula xml:id="formula_55">SC(x n | M) = -log f NML(x n |M) (3.2.3) = -log f (x n | θ (x n ), M) + log C n M (3.2.4)</formula><p>and log C n M is the parametric complexity. It acts as a normalizing constant, as it is related to the number of essentially different distributions in the model class with regards to x n <ref type="bibr" target="#b257">[99]</ref>.</p><p>The NML distribution has several important properties. It is the unique solution to the minimax problem of <ref type="bibr" target="#b254">[96]</ref>, which essentially means that is is the optimal encoding of any observed x n in the model class. Not only that, but it is also the optimal encoding for any data generating density, even outside the model class <ref type="bibr" target="#b258">[100]</ref>. It automatically prevents any overfitting by learning both the model and the number of parameters of the model, using only the data at hand (as opposed to Bayesian priors). In most applications however, computing the NML density is intractable due to the sum (or integral for continuous data) in Eq 3.2.2.</p><p>Fortunately it not the case for choosing the cutpoints of a discretization where the model class is equivalent to that of multinomial distributions, for which the normalizing constant has a closed form and can actually be computed in linear time via recursion :</p><formula xml:id="formula_56">C r n = ∑ l 1 +l 2 +•••+l r =n n! l 1 !l 2 ! • • • l r ! r ∏ k=1 l k n l k (3.2.5) = C r-1 n + n r -2 C r-2 n (3.2.6)</formula><p>From this result, <ref type="bibr" target="#b257">[99]</ref> developed a dynamic programming scheme to find the MDLoptimal discretization of a sample, giving the best description possible without overfitting <ref type="bibr">(Fig 3.8)</ref>. This method essentially gives a solution to the problem of choosing a discretization for the naive estimator ĤNaive which best describes the features of the sampled distribution, using the most complexity it can justify within the MDL framework. In the next section, we will see how it is adapted to find the two-dimensional discretization, which is needed to estimate the interaction between two variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pairwise mutual information estimation through optimal joint discretization</head><p>The proposed method starts from the master definition of the mutual information (Eq 3.1.21), which we redefine here for convenience. It consists in taking the supremum over all finite partitions, P and Q, of variables, X and</p><formula xml:id="formula_57">Y [4], I(X;Y ) = sup P,Q I([X] P ; [Y ] Q )</formula><p>which can be applied to continuous, discrete or mixture variables.</p><p>By continuing to refine some initial partitions through the addition of further cut points for continuous variable(s), one finds a monotonically increasing sequence <ref type="bibr">[4]</ref></p><formula xml:id="formula_58">, I([X] P ; [Y ] Q ),</formula><p>as shown in Fig. <ref type="figure" target="#fig_24">3</ref>.9. In practice, however, Eq. 3.1.21 cannot be used to estimate Î(X;Y ) from sampled distributions, as the refinement of partitions eventually assigns each of the N different samples into N different bins. This leads to a shift of convergence towards log N instead of the theoretical limit, I(X;Y ), which requires an infinite amount of data (dotted line in Fig. <ref type="figure" target="#fig_24">3</ref>.9).</p><p>In <ref type="bibr">[7]</ref>, we proposed to adapt Eq. 3.1.21 to account for the finite number of samples in Figure <ref type="figure" target="#fig_24">3</ref>.9: Outline of mutual information computation between continuous or mixed-type variables for a finite dataset of N samples. Theoretically, one approaches the true value I(X;Y ) (dotted horizontal line) refining the discretization by adding more bins ∆ (dotted curve). On finite sampling however, one inevitably overestimates I(X;Y ) by adding too many bins for the sample size N, up to a maximum log N (dashed curve). Mutual information is estimated through an optimum partitioning of continuous variable(s) (solid red line and arrow) after introducing a complexity term to account for the finite size of the dataset. actual datasets, I N (X;Y ) = sup</p><formula xml:id="formula_59">P,Q I N ([X] P ; [Y ] Q ) (3.2.7)</formula><p>by introducing a finite size correction to mutual information :</p><formula xml:id="formula_60">I N ([X] P ; [Y ] Q ) = I N ([X] P ; [Y ] Q ) -k P;Q (N) 1 N (3.2.8)</formula><p>where k P;Q (N) corresponds to a complexity term introduced in <ref type="bibr">[5,</ref><ref type="bibr">35]</ref> to discriminate between variable dependence (for</p><formula xml:id="formula_61">I N ([X] P ; [Y ] Q ) &gt; 0) and variable independence (for I N ([X] P ; [Y ] Q ) 0) given N samples.</formula><p>In the present context of finding an optimum discretization for continuous variables, this complexity term introduces a penalty which grows faster than the spurious information gained in refining bin partitions further, when there is not enough data to support such a refined model (Fig. <ref type="figure" target="#fig_24">3</ref>.9). Conceptually, we can also think of this penalty as the uncertainty associated with estimating the frequency p of small bins compared to large bins when N is limited. But how do we choose k P;Q (N) ?</p><p>For discrete variables, typical complexity terms correspond to the Bayesian Information Criterion (BIC), k BIC P;Q (N) = 1/2(r x -1)(r y -1) log N, where r x and r y are the number of bins for X and Y . Within the MDL framework, Roos et al. defined the X-and Y -Normalized Maximum Likelihood (NML) criteria <ref type="bibr" target="#b251">[93,</ref><ref type="bibr">35]</ref> :</p><formula xml:id="formula_62">k X-NML P;Q (N) = r y ∑ y log C r x n y -log C r x N (3.2.9) k Y -NML P;Q (N) = r x ∑ x log C r y n x -log C r y N (3.2.10)</formula><p>where C r x n y is the parametric complexity associated with the yth bin of variable Y containing n y samples, and similarly for C r y n x with the n x -size bin of variable X in Eq. 3.2.10. As mentioned, the parametric complexity or normalizing constant C r n is known for the domain of multinomial distributions. It is defined by summing a multinomial likelihood function over all possible partitions of n data points into a maximum of r bins :</p><formula xml:id="formula_63">C r n = k 0 ∑ 1 + 2 +•••+ r =n n! 1 ! 2 ! • • • r ! r ∏ k=1 k n k (3.2.11)</formula><p>which can in fact be computed recursively in linear-time <ref type="bibr" target="#b259">[101]</ref>. For large n and r, inherent to large datasets with continuous or mixed-type variables, we found that C r n computation can be made numerically stable by implementing the recursion on parametric complexity ratios</p><formula xml:id="formula_64">D r n = C r n /C r-1 n</formula><p>rather than the parametric complexities themselves :</p><formula xml:id="formula_65">D r n = 1 + n (r -2)D r-1 n (3.2.12) log C r n = r ∑ k=2 log D k n (3.2.13) for r 3, with C 1 n = 1 and C 2 n = D 2</formula><p>n , which can be computed directly with the general formula, Eq. 3.2.11, for r = 2,</p><formula xml:id="formula_66">C 2 n = n ∑ h=0 n h h n h n -h n n-h (3.2.14)</formula><p>or its Szpankowski approximation for large n (needed for n &gt; 1000 in practice) <ref type="bibr" target="#b260">[102,</ref><ref type="bibr" target="#b261">103,</ref><ref type="bibr" target="#b262">104]</ref>, For continuous variables, however, the variable categories are not given a priori and need to be specified and thus encoded in the model complexity within the frame of the Minimum Description Length (MDL) principle <ref type="bibr" target="#b257">[99]</ref>. In absence of priors for any specific partition with r bins, the model index should be encoded with a uniform distribution over all partitions with the same number of bins <ref type="bibr" target="#b257">[99]</ref>. As there are N-1 r x -1 ways to choose r x -1 out of N -1 possible cut points, corresponding to a codelength of log N-1 r x -1 for a continuous variable X (and similarly for Y if it is continuous), the model complexity associated with the partitioning of continuous or mixed-type variables becomes,</p><formula xml:id="formula_67">C 2 n = nπ 2 1 + 2 3 2 nπ + 1 12n + O 1 n 3/2 (</formula><formula xml:id="formula_68">k P;Q (N) = k P;Q (N) + log N -1 r x -1 + log N -1 r y -1 (3.2.17) with log N-1 r-1 = (r -1)C N,r</formula><p>, where C N,r corresponds to the encoding cost associated to each of the r -1 cut points with r = r x or r y .</p><p>While finding the supremum of I N ([X] P ; [Y ] Q ) over all possible partitions P and Q according to Eq. 3.2.7 seems intractable, it can be computed rather efficiently in practice.</p><p>The proposed approach is inspired by the computation of the MDL-optimal histogram for a single continuous variable of <ref type="bibr" target="#b257">[99]</ref>, which can be done exactly in O(N 2 × k) steps (with k the maximum number of bins). As the approach cannot be generalized to more than one variable, we implemented a local optimization heuristics, which finds the optimum cut points for a continuous variable X, maximizing its corrected information with a discrete variable I (X; [Y ]). When both X and Y are continuous, we iteratively fix X and Y and compute I ([X]; [Y ]) until a convergence is reached in the limit cycle, as will be detailed below.</p><p>In practice, for two variables variables we start from an initial (or optimized) [Y ] partition with r y bins of various sizes and an estimate of the number of [X] bins, rx (before discretizing X). The sample-scaled mutual information with finite size correction, i.e., n I n (X;Y ), is then optimized iteratively for n = 1, • • • , N samples, over all X partitions, through the following O(N 2 ) dynamic programming scheme, using Eq. 3.2.9 as parametric complexity,</p><formula xml:id="formula_69">n I n (X; [Y ]) = max 0 j&lt;n j I j (X; [Y ]) + r y ∑ y n xy log n xy -n x log n x -log C r y n x -C N, rx (3.2.18)</formula><p>where the last added bin on X, including the jth to nth samples distributed over the r y bins of [Y ] (with ∑ </p><formula xml:id="formula_70">for j = 1 to N in C do I [ j] ← I [0, j] for k = 1 to j in C do I new bin ← I [k] + I [k, j] if I new bin &gt; I [ j] then I [ j] ← I new bin Save Cuts[ j] ← k end if end for end for Reconstruct [X] from trace of best cutpoints, starting from Cuts[N] return [X]</formula><p>In this notation, the array I [] saves the values of I j (X; [Y ]), the partial information taking first j samples corrected the full complexity term k including the combinatorial approximation (Eq 3.2.17). We can significantly speed up the computation at little cost by doing a coarse search of the partitions on C possible cutpoints, instead of all N samples. This allows the algorithm to run in O(C 2 ) instead of O(n 2 ), with C being typically a factor of N 1/3 . Finally, Cuts[ j] corresponds to the location of the last cutpoint giving the best I [ j]. The optimal partition of X can be retraced by following each cutpoint starting from Cuts[N]. Note the special case of independence, when X ⊥ ⊥ [Y ] no multi-bin partitioning creates a positive I , i.e. no information greater than its associated complexity cost can be found. In this case, the output is a single bin from 0 to N (Cuts[N] = 0). Then, adopting this optimized partition for X, one can apply the same dynamic programming scheme for Y using Eq. 3.2.10 as parametric complexity and iterate the optimization of X and Y partitions until a stable two-state limit circle is reached. In practice, we set the initial partitioning over X and Y by testing equal-freq discretizations with k = 2 to N 1/3 bins and choosing the one which gives the highest</p><formula xml:id="formula_71">I N ([X] k e f ;Y k e f</formula><p>). We found that while the convergence speed of the iterative dynamic programming is largely independent of these initial conditions, this scheme does improve it slightly. This leads after only a few iterations to a good estimate of mutual information (averaged over limit circle). The iterative process to compute I(X;Y ) is shown in Alg 5.</p><p>Where r x and r y are the number of levels of [X] and [Y ]. Note that [X] and [Y ] are not updated straight after the call of Opt(), to make the process symmetrical between X and Y ( Î (X;Y ) = Î (Y ; X)).</p><p>We will now analyze this estimator in empirical situations, first qualitatively and then by Chapter 3. Mutual information for constraint-based inference</p><formula xml:id="formula_72">Algorithm 5 Î (X;Y ) heuristic Require: Ranks of X, Y , coarse level c I init = 0 for k = 2 to N 1/3 do if I ([X] k e f ; [Y ] k e f ) &gt; I init then I init ← I ([X] k e f ; [Y ] k e f ) [X] ← [X] k e f , [Y ] ← [Y ] k e f end if end for repeat [X] new ← Opt(I(X; [Y ]), c, r x ) [Y ] new ← Opt(I(Y ; [X]), c, r y ) Update [X] ← [X] new , [Y ] ← [Y ] new I ← I ([X], [Y ]) until Max iteration reached or limit cycle convergence return I , [X], [Y ]</formula><p>quantitatively comparing Î (Y ; X) to other estimators in the discrete and in the mixed case. Perhaps the most noticeable result of this approach is that the optimal discretization [X] of any variable depends on the joint distribution X,Y , no [X] can be MDL-optimal with regards to all joint distributions (Fig 3 .10). This implies that we need to run Alg 5 for each pair of variable to estimate Î (X;Y ) correctly, we cannot reuse the same cutpoints. Importantly, even if all variables are jointly Gaussian, the number of bins still depends on the amount of information, scaling monotonically with the strength of the interaction (Fig 3   This concept of context-dependent discretization perhaps seems fundamentally incompatible with Bayesian networks, for which each node must have a marginal probability distribution defined independently of the rest of the network. In our case, the optimal discretizations [X] and [Y ] must be considered in pairs, they inform us of the edge between X Figure <ref type="figure" target="#fig_24">3</ref>.11: Adaptive information-maximizing partitions depending on interaction strength. 10,000 Gaussian bivairate distributions with N = 1, 000 samples were generated with uniformly distributed correlation coefficients ρ in [-1, 1], and discretized using Alg 5. The real mutual information (RI, shades of blue) of Gaussian bivariate distributions can be computed directly with Eq 3.1.16. and Y , and not of the nodes X and Y . Alternatively, one can think of the optimal discretization scheme only as a proxy to measure Î (X;Y ) (of which [X] and [Y ] are by-products), which is indeed linked to the edge X -Y .</p><p>There are other discretization schemes for mixed Bayesian networks, for example the work done by Neil et al. on dynamic discretization <ref type="bibr" target="#b263">[105,</ref><ref type="bibr" target="#b264">106]</ref>. In this setting, continuous nodes are discretized so as to give the best inference, i.e. the best distribution P(v) as reconstructed from the Bayesian network parameters. From the inference point of view, one needs to find [X] not only in relation to a single variable Y , but to all of its neighbors (as well as all of the other parents of its children, to take into account interaction effects). To re-use the Information Bottleneck terminology, if one is looking for the best inference, one wants to find an encoding [X] of a continuous variable X that maximizes the information between X and all of its neighbors : I([X]; Ad j(X)). On the other hand, for constraint-based graph discovery we want to perform (conditional) independence testing for each edge X -Y , which only requires to look at the (conditional) interaction between two nodes (the conditional estimator is introduced in the next section).</p><p>Next, we assess how good is the estimation of Î (Y ; X) on known distributions. On bivariate Gaussian distributions with correlation ρ ranging from 0.01 to 0.9, our estimator is competitive with the KSG estimator (as implemented in JIDT <ref type="bibr" target="#b265">[107]</ref>). We note a particularly desirable property of the miic estimation : its error and variance tend to zero as the signal Chapter 3. Mutual information for constraint-based inference disappears (ρ → 0) and as the complexity cost is greater than any information coming from the joint discretization of the data. This results in few false positives when doing the graph reconstruction while still having decent power. We also assessed its performance on the mixed case, by conducting the numerical experiments proposed in <ref type="bibr">[51]</ref>. Our approach fared similarly or better compared to a naive equal-frequency discretization with N 1/3 bins, a kernel estimator and a noisy KSG estimator, as well ÎRoss <ref type="bibr" target="#b229">[71]</ref> and ÎGao <ref type="bibr">[51]</ref>. Particularly, our estimator gives the best estimation for the mixture distribution of <ref type="bibr">Fig 3.7</ref>. It converges at the ground truth value given by Eq 3.1.39, in accordance with the master definition of mutual information. For details on benchmark settings and other results, see Supplementary materials of <ref type="bibr">[7]</ref>. analytical values for the mutual information were taken fom <ref type="bibr">[51]</ref> and 50 runs were performed for each sample size N. From left to right, top to bottom, the simulations are devised after experiment I, experiment II, experiment IV with p = 0 and experiment IV with p = 0.15, from <ref type="bibr">[51]</ref>. The top left experiment corresponds to the distribution of Fig 3 .7 with ρ = 0.9 and β = 0.9 and p con = p dis = 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Conditional case</head><p>This optimization scheme, Alg 4, and its iterative dynamic programming computation, Alg 5, can also be adapted to compute mutual information involving joined variables, such as I N (X; {U i }), with corresponding finite size corrections and cut point encoding costs extended from Eqs. 3.2.8-3.2.17. Similarly, the approach can compute the conditional mutual information I N (X;Y |{U i }), involving continuous, discrete or mixture variables. Remark that we do not want to maximize I (X;</p><formula xml:id="formula_73">Y |{U i }) directly, as two dependent variable X ⊥ ⊥ Y that are conditionally independent X ⊥ ⊥ Y |{U i } can always have positive conditional information I(X;Y |[U i ]) if the [U i ] encoding is chosen so that I(X; [U i ]) or I(Y ; [U i ]</formula><p>) is null (e.g. imposing one single bin for all U). Instead, we want an estimation that converges towards 0 for conditional independence, and a positive value otherwise.</p><p>To this end, we can define Î N (X;Y |{U i }) using the chain rule 3.1.19, as the difference between maximized mutual information terms involving either {Y, {A i }} and {A i } (Eq. 3.2. <ref type="bibr">19)</ref> or {X, {A i }} and {A i } (Eq. 3.2.20) as joined variables,</p><formula xml:id="formula_74">Î N (X;Y |{U i }) = Î N (X;Y, {U i }) -Î N (X; {U i }) (3.2.19) = Î N (Y ; X, {U i }) -Î N (Y ; {U i })<label>(3.2.20)</label></formula><p>Starting from an initial (or optimized) partition [Y ], each term of Eq. 3.2.19 is optimized with respect to X and {U i } partitions using Eq. 3.2.9 as parametric complexity extended to multivariate categories, n x,{u i } and n {u i } . Then, in turn, each term of Eq. 3.2.20 is optimized with respect to Y and {U i } partitions using Eq. 3.2.10 as parametric complexity extended to multivariate categories, n y,{u i } and n {u i } . Note, in particular, that {U i } partitions are optimized separately for each of the four terms in Eqs. 3.2.19 &amp; 3.2.20, before taking their differences, as these optimized {U i } partitions might be different in general. This process is detailed by Alg 6.</p><p>The [U] optimization routine is a small loop of 3 iterations maximizing the relevant information, where one U i is discretized while the rest are fixed. As <ref type="bibr" target="#b249">[91]</ref> remarked in their own benchmarks, treating each U i independently gives MIIC a unique advantage compared to other methods as it discards non-informative U i s by discretizing them in a single bin, effectively removing one dimension. This is particularly interesting for constraint-based learning, as it implies that adding irrelevant U i s to the conditioning set does not change the result of the conditional test. In practice, it can sometimes lead to situations where I 1 or I 2 is negative because the [U] optimization gets stuck in a local optimum. This can be fixed by re-using the corresponding [U] cutpoints for the next iteration, ensuring that the differences I (</p><formula xml:id="formula_75">[X], [Y,U]) -I ([X], [U]) and I ([Y ], [X,U]) -I ([Y ], [U]</formula><p>) are positive. This recycling scheme is described in Alg 7.</p><formula xml:id="formula_76">Algorithm 6 Î (X;Y |U) heuristic Require: Ranks of X, Y , U, coarse level c I init = 0 for k = 2 to init_bins max do if I ([X] k e f ; [Y,U] k e f ) &gt; I init then I init ← I ([X] k e f ; [Y,U] k e f ) + I ([Y ] k e f ; [X,U] k e f ) [X] ← [X] k e f , [Y ] ← [Y ] k e f end if end for repeat [U] optimization on I ([Y ]; [X],U) Compute and save I ([Y ]; [X,U]) [X] new ← Opt(I ([Y ], X[U]), c) [U] optimization on I([X]; [Y ],U) Compute and save I ([X]; [Y,U]) [X] new ← Opt(I ([X],Y [U]), c) [U] optimization on I ([X];U) Compute and save I ([X]; [U]) [U] optimization on I ([Y ];U) Compute and save I ([Y ]; [U]) Update [X] ← [X] new , [Y ] ← [Y ] new I 1 ← I ([X], [Y,U]) -I ([X], [U]) I 2 ← I ([Y ], [X,U]) -I ([Y ], [U]) I ← 0.5(I 1 + I 2 ) until Max iteration reached or limit cycle convergence on I return I , [X], [Y ]</formula><p>To benchmark the conditional estimator, four-dimensional normal distributions P(X,Y, Z 1 , Z 2 ) were sampled for N = 100 and 10, 000 samples 100 times for each correlation coefficient ρ = ρ XY between 0.05 and 0.95. The other pairwise correlation coefficients are fixed as</p><formula xml:id="formula_77">ρ XZ 1 = ρ XZ 2 = ρ Y Z 1 = ρ Y Z 2 = λ = 0.7 and ρ Z 1 Z 2 = 0.9.</formula><p>The conditional mutual information I(X;Y |Z 1 , Z 2 ) was then estimated using the proposed optimum partitioning scheme as well as with k-nn conditional information estimates as in Fig 3 .12. In this experiment, ρ values closed to zero, mimick "V-structures" as they correspond to pairwise independence but conditional dependence; by constrast ρ = 2λ 2 /(1 + ρ Z 1 Z 2 ) 0.5158 corresponds to conditional independence, while ρ &gt; 0.5158 implies that X and Y share more information than the indirect flow through Z 1 and Z 2 . The analytical value of the conditional mutual information is derived as follows : given the 4 × 4 covariance matrix Σ Σ Σ and its four 2 × 2 partitions Σ Σ Σ i j , we first compute the conditional covariance matrix</p><formula xml:id="formula_78">Σ Σ Σ = Σ Σ Σ 11 -Σ Σ Σ 12 Σ Σ Σ -1 22 Σ Σ Σ 21 where Σ Σ Σ -1</formula><p>22 is the generalized inverse of Σ Σ Σ 22 . The partial correlation between X and Y is obtained as</p><formula xml:id="formula_79">ρ XY •Z 1 Z 2 = Σ Σ Σ 12 / Σ Σ Σ 11 * Σ Σ Σ 22 , Algorithm 7 Î (X;Y |U) heuristic, re-using cutpoints Require: Ranks of X, Y , U, coarse level c Initialize [X], [Y ]</formula><p>with best equal freq, as Alg 6</p><formula xml:id="formula_80">Reuse_X_cuts ← false Reuse_Y _cuts ← false repeat if Reuse_Y _cuts then [U] ← [U] Y else [U] optimization on I ([Y ]; [X],U) end if Compute and save I ([Y ]; [X,U]) [X] new ← Opt(I ([Y ], X[U]), c) if Reuse_X_cuts then [U] ← [U] X else [U] optimization on I ([X]; [Y ],U) end if Compute and save I ([X]; [Y,U]) [Y ] new ← Opt(I ([X],Y [U]), c) if Reuse_X_cuts then [U] ← [U] X else [U] optimization on I ([X];U) Save [U] X cutpoints ← [U] end if Compute and save I ([X]; [U]) if Reuse_Y _cuts then [U] ← [U] Y else [U] optimization on I ([Y ];U) Save [U] Y cutpoints ← [U] end if Compute and save I ([Y ]; [U]) Update [X] ← [X] new , [Y ] ← [Y ] new I 1 ← I ([X], [Y,U]) -I ([X], [U]) I 2 ← I ([Y ], [X,U]) -I ([Y ], [U]) Assign Reuse_X_cuts ← (I 1 &lt; 0), Reuse_Y _cuts ← (I 2 &lt; 0) I ← 0.5(I 1 + I 2 ) until Max iteration reached or limit cycle convergence on I return I , [X], [Y ]</formula><p>and the analytical conditional mutual information for a multivariate normal distribution is given by that our estimator is also adequate to measure Î(X;Y |{U i }, even though it is a significantly harder problem (even for the KSG estimator). As is the case for the pairwise information, it also seems to converge towards zero at the independence regime, contrary to the k-nn approaches that always give noisy estimates.</p><formula xml:id="formula_81">I(X;Y |Z 1 , Z 2 ) = -log(1 -ρ 2 XY •Z 1 Z 2 )/2.</formula><p>Finally, we tested the sensitivity and power of our estimator to detect (conditional) independence. We reproduced the tests for mixed conditional independence test by <ref type="bibr">[50]</ref> based around the "Local Causal Discovery" algorithm <ref type="bibr" target="#b266">[108]</ref>. In the original article, independences tests are either frequentist or bayesian, and are compared using different detection thresholds to compute the ROC curves and AUCs. Our estimator I N (X;Y ) cannot be readily compared in this way since it is unbounded and it behaves the opposite way of these other tests (dependence implies a large positive value, independence gives a null estimation). For many estimators one can always get an "empirical p-value" without knowing the standard asymptotic distribution by running permutations on the observed data. In our case however, it would not be efficient as the optimal discretization for shuffled data without information is one single bin, and I N (X;Y ) is strictly 0. Instead, to obtain a value between ]0, 1] that behaves the same way as the other tests, we computed the following :</p><formula xml:id="formula_82">I pval (X;Y ) = 1 - I N (X;Y ) min(I N (X; X), I N (Y ;Y )) (3.2.21) I pval (X;Y |Z) = 1 - I N (X;Y |Z) min(I N (X; X), I N (Y ;Y )) (3.2.22)</formula><p>Where min(I N (X; X), I N (Y ;Y )) can be thought of as the maximum value I N (X;Y ) or I N (X;Y |Z) can have in this setting. We can then compare I pval with different marginals X, Y and Z, and compute ROC curves and the area under them by setting different thresholds in ]0, 1]. The results show that our proposed estimator has the best overall AUC when combining the three independence tests C ⊥ ⊥ X, X ⊥ ⊥ Y and C ⊥ ⊥ Y |X on mixed data <ref type="bibr">(Fig 3.15)</ref>. It means that even though the estimator essentially filters out the very weak interactions by setting 1-bin discretization, we are able to compare and rank the estimates Î better than any other test in these settings. This is the closest experiment to causal graph inference, which essentially consists of serial (conditional) independence tests for the skeleton discovery. For details of the different simulations used to benchmark independence testing, I refer the reader to the original study <ref type="bibr">[50]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Publication in PLoS Computation Biology</head><p>Our publication in the Public Library of Science Computational Biology journal <ref type="bibr">[7]</ref> introduced this joint optimal discretization algorithm, showing more examples of discretizations and comparisons with other methods. Importantly, it evaluated the performance of MIIC using this estimator on continuous and mixed data against other state of the art causal discovery approaches. It was shown to have the best overall performance even when testing over the full range of parameters of the other methods (whereas MIIC stays parameter free). Maybe surprisingly, it was even shown to outperform CAM <ref type="bibr">[38]</ref>, which makes explicit assumptions that give it an edge in a simulations setting, and kPC <ref type="bibr">[33]</ref> based on the HSIC, which is known to be one of the most powerful methods for non-parametric conditional independence testing. On mixed datasets, it fared better than either CausalMGM <ref type="bibr" target="#b267">[109]</ref> and MXM <ref type="bibr" target="#b268">[110]</ref> (also on their full range of parameters), the only two known methods that deal with mixed data at the time of writing the article.</p><p>It also presented and analyzed the network inferred by MIIC on a mixed dataset of medical records of elderly patients, which will be introduced in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The precise diagnostics of neurological disorders require to integrate a large amount of information from a variety of biomedical tests and clinical examinations. These diagnostics must also take into account age-related comorbid medical conditions, such as diabetes and cardiovascular diseases, which concern a large fraction of patients, as the incidence of neurodegenerative diseases increases with age. Such comorbid medical conditions influence neuropathology treatment decisions as well as short-and long-term survival of patients but are often overlooked in clinical trials. This situation underlines the need to directly analyze real life medical records to learn clinical networks, that are graphical models highlighting direct, indirect and possibly causal associations between clinically relevant information in patients' medical records.</p><p>Medical records contain, however, mixed types of data from simple binary or nominal variables (i.e., with multiple unordered categories) to ordinal (e.g. neuropsychological test scales) or continuous (e.g. age, body mass index) variables, whose interdependences are not readily assessed within a unified information-theoretic framework. As mutual information is primarily defined between nominal variables, its estimation for continuous or mixed-type variables is notoriously difficult beyond the gaussian approximation of continuous distributions, for which a simple relation exists with correlation coefficients <ref type="bibr">[1]</ref>. In particular, arbitrary discretization of continuous variables tends to underestimate mutual information for small number of bins, while overestimating it for large number of bins due to finite numbers of patients, as sketched in Fig 1 . Moreover, so far, no rationale provides optimum bin partitions to estimate mutual information, for typical cohort size of patients. Alternatively, local metric approaches have been proposed to estimate mutual information <ref type="bibr">[2]</ref> and conditional information <ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref>, including between mixed-type variables <ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref>, based on k-nearest neighbor (kNN) statistics. However, the statistical significance of kNN information estimates remains difficult to assess in practice <ref type="bibr">[2,</ref><ref type="bibr">9]</ref>, thereby limiting their use to uncover (conditional) independences between continuous or mixed-type variables from real-life datasets.</p><p>In this paper, we first develop and implement an optimum binning method to simultaneously compute and assess the significance of mutual information, as well as conditional multivariate information, between any combination of continuous or mixed-type variables. The method is based on minimum description length principles <ref type="bibr">[10,</ref><ref type="bibr">11]</ref> and finds optimum bin </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>Learning clinical networks based on information estimates in mixed-type data partitions, iteratively for each continuous variable, through an efficient dynamic programming scheme with quadratic complexity, OðN 2 Þ, where N is the number of patients in the dataset. This efficient approach is then used to assess direct versus indirectcause-effect relationships between mixed-type data from medical records, by extending a recent network learning method <ref type="bibr">[12,</ref><ref type="bibr">13]</ref> to recontruct graphical models beyond simple categorical datasets.</p><p>The method is shown to outperform existing tools on benchmark mixed-type datasets, before being applied to analyze the medical records of eldery patients with cognitive disorders from La Pitie ´-Salpêtrière Hospital, Paris. The resulting clinical network visually captures the global interdependences in these medical records and some facets of clinical diagnosis practice, without specific hypothesis nor prior knowledge on any clinically relevant information. The reconstructed clinical network recovers well known as well as novel direct and indirect relations between medically relevant variables. In particular, it provides some physiological insights linking the consequence of cerebrovascular accidents to the atrophy of important brain structures associated to cognitive impairment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessing information in continuous or mixed-type data</head><p>Information-maximizing discretization of continous data. While mutual information is usually defined as a discrete summation over nominal variables, i.e., I(X;Y) = ∑ x,y p x,y log(p x,y /p x p y ), its most general definition consists in taking the supremum over all finite partitions, P and Q, of variables, X and Y <ref type="bibr">[1]</ref>,</p><formula xml:id="formula_83">IðX; YÞ ¼ sup P;Q Ið½X� P ; ½Y� Q Þ<label>ð1Þ</label></formula><p>which can be applied to continuous or mixed-type variables. Moreover, by continuing to refine some initial partitions through the addition of further cut points for continuous variable(s), one finds a monotonically increasing sequence <ref type="bibr">[1]</ref>,</p><formula xml:id="formula_84">Ið½X� P ; ½Y� Q Þ, as depicted on Fig 1.</formula><p>In practice, however, Eq 1 cannot be used to estimate I(X; Y) from an actual dataset with finite sample size, as the refinement of partitions eventually assigns each of the N different samples into N different bins. This leads to a shift of convergence towards logN instead of the theoretical limit, I (X; Y), which requires an infinite amount of data (dotted line in <ref type="bibr">Fig 1)</ref>.</p><p>In this paper, we propose to adapt Eq 1 to account for the finite number of samples in actual datasets,</p><formula xml:id="formula_85">I 0 N ðX; YÞ ¼ sup P;Q I 0 N ð½X� P ; ½Y� Q Þ<label>ð2Þ</label></formula><p>by introducing a finite size correction to mutual information,</p><formula xml:id="formula_86">I 0 N ð½X� P ; ½Y� Q Þ ¼ I N ð½X� P ; ½Y� Q Þ À k 0 P;Q ðNÞ 1 N<label>ð3Þ</label></formula><p>where k 0 P;Q ðNÞ corresponds to a complexity term introduced in <ref type="bibr">[14,</ref><ref type="bibr">15]</ref> to discriminate between variable dependence (for I 0 N ð½X� P ; ½Y� Q Þ &gt; 0) and variable independence (for I 0 N ð½X� P ; ½Y� Q Þ⩽0) given a finite dataset of size N. In the present context of finding an optimum discretization for continuous variables, this complexity term introduces a penalty which eventually outweights the information gain in refining bin partitions further, when there is not enough data to support such a refined model, as depicted on Fig 1 . 
For discrete variables, typical complexity terms correspond to the Bayesian Information Criterion (BIC), k BIC P;Q ðNÞ ¼ 1=2ðr x À 1Þðr y À 1Þ log N, where r x and r y are the number of bins</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>for X and Y, or the X-and Y-Normalized Maximum Likelihood (NML) criteria <ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref>, defined as,</p><formula xml:id="formula_87">k XÀ NML P;Q ðNÞ ¼ X r y y log C r x n y À log C r x N ð4Þ k YÀ NML P;Q ðNÞ ¼ X r x x log C r y n x À log C r y N<label>ð5Þ</label></formula><p>where C r x n y is the parametric complexity associated with the yth bin of variable Y containing n y samples, and similarly for C r y n x with the n x -size bin of variable X in Eq 5. Parametric complexities C r n are defined by summing a multinomial likelihood function over all possible partitions of n data points into a maximum of r bins as,</p><formula xml:id="formula_88">C r n ¼ X ' k ⩾0 ' 1 þ' 2 þ���þ' r ¼n n! ' 1 !' 2 ! � � � ' r ! Y r k¼1 ' k n � � ' k<label>ð6Þ</label></formula><p>which can in fact be computed recursively in linear-time <ref type="bibr">[17]</ref>. For large n and r, inherent to large datasets with continuous or mixed-type variables, we found that C r n computation can be made numerically stable by implementing the recursion on parametric complexity ratios D r n ¼ C r n =C rÀ 1 n rather than parametric complexities themselves as,</p><formula xml:id="formula_89">D r n ¼ 1 þ n ðr À 2ÞD rÀ 1 n ð7Þ log C r n ¼ X r k¼2 log D k n ð8Þ for r ⩾ 3, with C 1 n ¼ 1 and C 2 n ¼ D 2</formula><p>n , which can be computed directly with the general formula, Eq 6, for r = 2,</p><formula xml:id="formula_90">C 2 n ¼ X n h¼0 n h ! h n � � h n À h n � � nÀ h<label>ð9Þ</label></formula><p>or its Szpankowski approximation for large n (needed for n &gt; 1000 in practice) <ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref>,</p><formula xml:id="formula_91">C 2 n ¼ ffi ffi ffi ffi ffi ffi np 2 r 1 þ 2 3 ffi ffi ffi ffi ffi ffi 2 np r þ 1 12n þ O 1 n 3=2 � � !<label>ð10Þ</label></formula><formula xml:id="formula_92">' ffi ffi ffi ffi ffi ffi np 2 r exp ffi ffi ffi ffi ffi ffi ffi ffi 8 9np r þ 3p À 16 36np !<label>ð11Þ</label></formula><p>For continuous variables, however, the variable categories are not given a priori and need to be specified and thus encoded in the model complexity within the frame of the Minimum Description Length (MDL) principle <ref type="bibr">[11]</ref>. In absence of priors for any specific partition with r bins, the model index should be encoded with a uniform distribution over all partitions with the same number of bins <ref type="bibr">[11]</ref>. As there are ð NÀ 1</p><formula xml:id="formula_93">r x À 1 Þ ways to choose r x -1 out of N -1 possible cut points, corresponding to a codelength of log ð NÀ 1 r x À 1 Þ for a continuous variable X (and</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>similarly for Y if it is continuous), the model complexity associated with the partitioning of continuous or mixed-type variablesbecomes,</p><formula xml:id="formula_94">k 0 P;Q ðNÞ ¼ k P;Q ðNÞ þ log N À 1 r x À 1 ! þ log N À 1 r y À 1 0 @ 1 A<label>ð12Þ</label></formula><formula xml:id="formula_95">with log ð NÀ 1 rÀ 1 Þ ¼ ðr À 1Þ C N;r</formula><p>, where C N,r corresponds to theencoding cost associated to each of the r -1 cut points with r = r x or r y .</p><p>While finding the supremum of I 0 N ð½X� P ; ½Y� Q Þ over all possible partitions P and Q according to Eq 2 seems intractable, it can be computed rather efficiently in practice.</p><p>The approach is inspired by the computation of an MDL-optimal histogram for a single continuous variable <ref type="bibr">[11]</ref>, which can be done exactly in OðN 3 Þ steps. As the approach cannot be generalized to more than one variable, we implemented a local optimization heuristics, which finds the optimum cut points for each continuous variable, iteratively, keeping the partitions of the other continuous variable(s) fixed. This enables to gain an order of magnitude in the optimization running time at each iteration, which scales as OðN 2 Þ, as detailed below.</p><p>In practice for two variables, we start from an initial (or optimized) X partition with r x bins of various sizes and an estimate of the number of Y bins, r � y . The sample-scaled mutual information with finite size correction, i.e., nI 0 n ðX; YÞ, is then optimized iteratively for n = 1, � � �, N samples, over all Y partitions, through the following OðN 2 Þ dynamic programming scheme, using Eq 4 as parametric complexity,</p><formula xml:id="formula_96">nI 0 n ðX; YÞ ¼ max 0⩽j&lt;n ½jI 0 j ðX; YÞ þ X r x x n xy log n xy À n y log n y À log C r x n y À C N;r � y �<label>ð13Þ</label></formula><p>where the last added Y bin, including n y = n -j samples distributed over the r x bins of X (with P r x x n xy ¼ n y ), comes with an independent mutual information contribution, P r x</p><p>x n xy log n xy À n y log n y , a parametric complexity, log C r x n y , and encoding cost, C N;r � y . The initial condition for j = 0 in ( <ref type="formula" target="#formula_96">13</ref>) is set by convention to include all terms invariant under Y-partitioning, i.e., À</p><formula xml:id="formula_97">P r x x n x log ðn x =NÞ þ log C r x N À ðr x À 1ÞC N;r x þ C N;r � y .</formula><p>Then, adopting this optimized partition for Y, one can apply the same dynamic programming scheme for X using Eq 5 as parametric complexity and iterate the optimization of X and Y partitions until a stable two-state limit circle is reached. In practice, we set the initial partitioning over X and Y by testing equal-freq discretizations with 2 to dN 1/3 e bins and choosing the one which gives thehighest I 0 N ðX; YÞ. We found that while the convergence speed of the iterative dynamic programming is largely independent of these initial conditions, this scheme does improve it slightly. This leads after only a few iterations to a good estimate of mutual information (averaged over limit circle) that is comparable to the existing state of the art, for both continuous and mixed-type variables, as shown below.</p><p>This optimization scheme, Eq 2, and its iterative dynamic programming computation, Eq 13, can also be adapted to compute mutual information involving joined variables, such as I 0 N ðX; fA i gÞ, with corresponding finite size correctionsand cut point encoding costs extended from Eqs 3-12. Similarly, the approach can compute conditional mutual information, such as I 0 N ðX; YjfA i gÞ, involving continuous or mixed-type variables. To this end, I 0 N ðX; YjfA i gÞ needs to be defined, using the chain rule <ref type="bibr">[1]</ref>, as the difference between maximized mutual information terms involving either {Y, {A i }} and {A i } (Eq 14) or {X, {A i }} and {A i } (Eq 15) as joined </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>Thus, starting from an initial (or optimized) partition for X, each term of Eq 14 is optimized with respect to Y and {A i } partitions using Eq 4 as parametric complexity extended to multivariate categories, n y,{ai } and n {ai }. Then, in turn, each term of Eq 15 is optimized with respect to X and {A i } partitions using Eq 5 as parametric complexity extended to multivariate categories, n x,{ai } and n {ai }. Note, in particular, that {A i } partitions are optimized separately for each of the four terms in Eqs 14 &amp; 15, before taking their differences, as these optimized {A i } partitions might be different in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning networks from continuous or mixed-type data</head><p>The above information maximization scheme to estimate (conditional) mutual information between continuous or mixed-type variables can then be used to extend our recent network learning algorithm MIIC <ref type="bibr">[12]</ref> beyond simple categorical datasets.</p><p>Outline of MIIC algorithm. MIIC combines constraint-based approach and information-theoretic framework to robustly learn a broad class of causal or non-causal networks including possible latent variables <ref type="bibr">[12,</ref><ref type="bibr">13]</ref>. MIIC proceeds in three steps: i). Edge pruning. Starting from a fully connected network, MIIC first removes dispensable edges by iteratively subtracting the most significant information contributions from indirect paths between each pair of variables. Significant contributors are collected based on the 3off2 score <ref type="bibr">[14,</ref><ref type="bibr">15]</ref> maximizing conditional three-point information while minimizing conditional two-point (mutual) information, which reliably assesses conditional independence, even in the presence of strongly linked variables <ref type="bibr">[21]</ref>. The residual (conditional) mutual information including finite size corrections, I 0 N ðX; YjfA i gÞ (i.e. after indirect effects of significant contributors, {A i }, have been subtracted from I 0 N ðX; YÞ), is related to the removal probability of each edge, P XY ¼ expðÀ NI 0 N ðX; YjfA i gÞÞ, where NI 0 N ðX; YjfA i gÞ &gt; 0 corresponds to the strength of the retained edge, as visualized by its width in MIIC graphical models <ref type="bibr">[12]</ref>.</p><p>ii). Edge filtering (optional). The remaining edges can be further filtered based on confidence ratio assessment <ref type="bibr">[12]</ref>,C XY ¼ P XY =hP rand XY i, where P rand XY is the average of the probability to remove the XY edge after randomly permutating the dataset for each variable. Hence, the lower C XY , the higher the confidence on the XY edge. In practice, filtering edges with C XY &gt; 0.1 or 0.01 limits the false discovery rates with small datasets, while maintaining satisfactory true positive rates <ref type="bibr">[12]</ref>.</p><p>iii). Edge orientation. Retained edges are then oriented based on the signature of causality in observational data given by the sign of (conditional) three-point information <ref type="bibr">[14,</ref><ref type="bibr">15]</ref>. The final network contains up to three types of edges <ref type="bibr">[12]</ref>: undirected, directed, as well as, bidirected edges, which originate from a latent variable, L, unobserved in the dataset but predicted to be a common cause of X and Y, i.e. X ⤎ (L) ⤏ Y. For clarity, bidirected edges are represented with dashed lines in MIIC networks.</p><p>An important aspect of MIIC algorithm is its ability to take into account datasets with missing values, which are frequent in heterogeneous clinical datasets. In practice, MIIC computes multivariate information estimates (such as I 0 N ðX; YjfA i gÞ) on sub-datasets for which X, Y and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>{A i } do not have missing values. While including iteratively additional conditioning variables A i might further restrict the size of the sub-dataset without missing value, we only consider variables A i if their missing values are missing at random (checking Kullback Leibler divergence between distributions of decreasing supports). If some data is not missing at random, the 3off2 scheme <ref type="bibr">[14,</ref><ref type="bibr">15]</ref>,</p><formula xml:id="formula_99">I(X; Y|{A i } n ) = I(X; Y) -I(X; Y; A 1 ) -I(X; Y; A 2 |A 1 )-� � �-I(X; Y; A n |{A i } n-1</formula><p>), might end without finding conditional independence, ie I(X;Y|{A i } n )&gt;0, and MIIC edge pruning step is conservative by retaining the corresponding edge X-Y due to possible bias in the dataset. MIIC's extension to continuous or mixed-type data has been implemented in MIIC online server and R package, see SI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to benchmark synthetic data</head><p>Optimum discretization and mutual information estimates for continuous or mixedtype data. The multivariate discretization scheme and resulting estimates of (conditional) mutual information were first benchmarked using synthetic data from known mixed or continuous probability distributions for which (conditional) mutual information can be obtained either analytically or through numerical integration. Examples of bivariate information-maximizing discretizations are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>variable Y of interest, the number of X and Y bins are roughly similar(for the chosen test settings), S2A Fig, unlike found with information-maximization discretization methods lacking complexity terms <ref type="bibr">[22]</ref>, S2B Fig.</p><p>Next, we compared our estimation of I N (X; Y) by optimal discretization to the state of the art Kraskov-Sto ¨gbauer-Grassberger (KSG) estimator <ref type="bibr">[2]</ref> for continuous distributions, specifically bivariate Gaussian distributions S4 Fig. <ref type="figure">Like</ref> otherinformation estimators based on kNN statistics, the KSG approach has a tunable parameter k which will typically scale with the sample size N, and has to be chosen depending on the objective: the original authors recommend k = 2 to 4 for the best estimation, and up to N/2 if one is more interested in independence testing. We found that our optimal discretization with the NML complexity does indeed give a correct estimation of I N (X; Y) for all sample sizes and correlation strengths. Our approach also natively deals with categorical and mixed (i.e. part categorical and part continuous) variables, as the master definition of the mutual information, Eq 1, can be applied to variables of any type. Recent efforts were made to extend the KSG estimator to such cases <ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref> which are frequently encountered in real-life data, and specifically in clinical datasets. We compared the mixed-type information estimates of our method to other existing methods for varying sample sizes and found its performance to be similar or superior, S5 Fig. In addition, our informationmaximizing discretization approach facilitates the interpretation of the dependences between continuous or mixed-type variables by returning their most informative categories.</p><p>Information-maximizing discretization and corresponding (conditional) mutual information estimates can be computed for any continuous or mixed-type dataset using the discretizeMutual function from the MIIC R package.</p><p>Optimum discretization as an independence test between continuous or mixed-type variables. Most importantly,our optimum discretization scheme also acts as an independence test by allowing for single bin partitions whenever no multiple-bin partitioning can glean information that is greater than its associated complexity cost. In such cases, our estimator implies variable independence, i.e. I N (X; Y) = 0, with drastically reduced sampling error and variance, S4 Fig, as compared to other direct estimators such as KSG, which always give noisy information estimates even for vanishing mutual information between nearly independent variables and need additional hypothesis testing to be used as independence test.</p><p>Similarly, our approach robustly learns conditional independence,given a set of separating variables, {Z i }, i.e., I N (X; By contrast, spurious dependency between independent variables, X and Y, can be induced, as expected <ref type="bibr">[23]</ref>, by conditioning over a common descendent Z, as in the case of a "vstructure", X ! Z Y, S9 Fig.</p><formula xml:id="formula_100">Y |{Z i }) = 0,</formula><p>Hence, the intrinsic robustness of the present optimum discretization scheme in inferring (conditional) independence and dependency is an important feature of the method as compared to kNN (conditional) information estimates, whose statistical significance remains difficult to assess in practice <ref type="bibr">[2,</ref><ref type="bibr">9]</ref>.</p><p>Reconstruction of benchmark graphical models. We first tested the mixed-type data extension of MIIC network reconstruction method on benchmark mixed-type data. Datasets were generated based on non-linear bayesian rules using the R script provided as Supplementary code; an example of non-Gaussian mixed-type distribution dataset is shown in S10 Fig.</p><p>The resulting reconstructed network F-scores are shown in Fig <ref type="figure" target="#fig_24">3</ref> for an increasing proportion of continuous variables over discrete variables and compared to the recent alternative methods, CausalMGM <ref type="bibr">[24]</ref> and MXM <ref type="bibr">[25]</ref>, also designed to analyze mixed-type data. Precision, Recall and F-scores are shown for both skeleton and CPDAG in S11 and S12 Figs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>Comparisons with fully continuous datasets, S13 Fig, were also performed with additional methods, CAM <ref type="bibr">[26]</ref>, kPC, rank-PC and rank-FCI <ref type="bibr">[27]</ref> algorithms, S14 and S15 Figs, and confirm the better performance of MIIC over alternative continuous or mixed-type network learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to medical records of eldery patients with cognitive disorders</head><p>We applied this information maximization analysis for mixed-type data to reconstruct a clinical network from the medical records of 1,628 eldery patients consulting for cognitive disorders at La Pitie ´-Salpêtrière hospital, Paris. The dataset,provided as S1   • The Mini Mental State (MMS) test assesses cognitive functions related to memory, spacial and temporal orientations but not to executive functions, which require to integrate multiple information sources. MMS is found to be the main hub (with 15 neighbors) of the reconstructed network, as it is directly connected, as expected, to most of the memory test results (forward/backward verbal and visuospatial memory spans, biographic memory and delayed recalls of Taylor or Rey-Osterrieth complex figures). By constrast, MMS is found to be negatively correlated to the Alzheimer's diagnostic, through the MMS 3 word memory test, which is known to be one of the most specific tests for Alzheimer's disease, together with the Free and Cued Selective Reminding (FCSR) test. Interestingly, our network analysis shows that the Alzheimer's disease diagnostic is directly connected to the FCSR test through the low percent reactivity to cueing, which identifies genuine storage deficits (not facilitated by cueing) due to amnesic syndrome of the hippocampal type known to be characteristic of Alzheimer's disease <ref type="bibr">[30]</ref>.</p><p>• The Frontal Assessment Battery (FAB) test is complementary to MMS, as it is entirely focussed on executive functions, centralized in the frontal cortex; it is thus very consistent that FAB is found to be directly connected and negatively correlated to dysexecutive syndrome. Note, however, that patients suffering from dysexecutive syndrome do not typically show poor FCSR scores unlike Alzheimer patients. This confirms the specificity and sensibility of the FCSR test to Alzheimer's disease <ref type="bibr">[31]</ref>.</p><p>• Finally, the Montreal Cognitive Association (MoCA) composite test integrates a variety of other tests such as the clock-drawing test, the phonetic fluency test as well as semantic fluency test (Isaacs Set Test), which is consistent with the direct connections recovered between MoCA and these three individual tests in the inferred network.</p><p>Psychiatric conditions. The third group of nodes concerns variables associated with the psychiatric conditions of patients. It includes their past psychiatric history (Psy_Hist) and present psychiatric conditions, i.e., anxio-depressive or bipolar (BIPO) syndromes, associated treatments (antidepressants, psychotropes, benzodiazepine BZD and neuroleptics NLP) and finally scores used to diagnose depression (GDS_15) and a deterioration in the quality of life (QoL). The analysis of all the links between these variables confirms the overall consistency of this psychiatric cluster: a good quality of life is closely associated with a low GDS_15 score (corresponding to a low probability of depression). Note, however, that psychiatric pathologies are all linked to each other, underlying the difficulty to distinguish them accurately. Yet, our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>Learning clinical networks based on information estimates in mixed-type data PLOS Computational Biology | <ref type="url" target="https://doi.org/10.1371/journal.pcbi.1007866">https://doi.org/10.1371/journal.pcbi.1007866</ref>  <ref type="bibr">May 18, 2020 11 / 19</ref> network analysis shows that patients with bipolar syndrome (BIPO) tend to show better scores at the FCSR recall test.</p><p>Vascular versus mixed forms of dementias. The fourth group of nodes of the clinical network is associated with variables implicated in vascular dementias (VASC_DEM) originating from cerebral vascular accidents (CVA) which damage brain regions essential for cognitive processes. Different types and sizes of vascular accidents are distinguished from microbleeds to ischemic stroke (clot) and lacunae (empty spaces in the deep brain structures). These more severe vascular accidents may also lead to degenerative dementia syndromes, corresponding to a mixed form of dementia (MIXED_FORMS), which is inferred to be directly associated to low MMS scores and poor scores at the FCSR Recall test (i.e., negative direct links). VASC_-DEM and MIXED_FORMS are also found to be connected to the Fazekas scale <ref type="bibr">[32]</ref>, which detects and quantifies white matter hyperintensities in the brain that are the consequence of cerebral small vessel disease including demyelination and axonal loss of neuronal cells. The Fazekas scale is found to be directly associated to low cognitive processing speed (TMTA) and also strongly correlated to the Scheltens scale <ref type="bibr">[33]</ref> quantifying the severity of hippocampal atrophy, in agreement with a recent independent report <ref type="bibr">[34]</ref>. The hippocampus is a brain structure involved in memory and space navigation, which is consistent with our finding of a direct negative association between Scheltens scale and MMS score. Interestingly, this predicted association between the Fazekas and the Scheltens scales, inferred from our unsupervised global network analysis, provides some physiological insights linking the consequence of vascular accidents (Fazekas scale) to the atrophy of important brain structures (Scheltens scale) and, thereby, to cognitive and functional impairments, as reported in clinical studies linking white matter hyperintensities (Fazekas scale) to cognitive impairment <ref type="bibr">[35]</ref>.</p><p>Patient clinical context. The last important group of nodes of the clinical network includes variables associated with the patient clinical context including comorbidities, related examinations and treatments. These are different anterior chronic diseases, such as arterial hypertension (AHT), diabetes, chronic obstructive pulmonary disease (COPD), atrial fibrillation (AFib), that might have an impact on the patient's vital prognosis. All the links within this comorbidity cluster are very consistent, each pathology being directly associated with its known risk and predisposition factors, biological markers, specific examinations and treatments. In particular, diabetes is associated with a high body mass index (BMI), glycated hemoglobin blood test (HbA1c), treatment by oral antidiabetic (OAD) drugs and statin; COPD is associated with sleep apnea syndrome (SAS) and the risk of respiratory failure, the use of bronchiodilator drugs and the necessity to quit smoking; AHT is associated with an increase risk of mixed form dementia and treatments by angiotensin receptor blockers (ARBs), beta-blockers and other anti-hypertension (Anti HT) drugs; Finally, AFib, detected by electrocardiogram (ECG), is associated with an increased risk of heart failure and high levels of thyroid-stimulating hormone (TSH) and treated with vitamine K antagonist (VKA) and direct oral anticoagulants (DOAC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We report in this paper a novel optimal discretization method to simultaneously compute and assess the significance of mutual information, as well as conditional multivariate information, between any combination of continuous or mixed-type variables. The approach is used to reconstruct graphical models from mixed-type datasets by uncovering direct, indirect and possibly causal relationships in complex heterogenous data. The method is shown to outperform state-of-the-art approaches on benchmark mixed-type datasets, before being applied to analyze the medical records of eldery patients with cognitive disorders from La Pitie ´-Salpêtrière Hospital, Paris.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>From a methodological perspective, this information-maximizing discretization approach facilitates the interpretation of either the dependences or the independencies between continuous or mixed-type variables. First, obtaining optimal discretization helps explain the dependences in terms of the most informative categories of continuous variables. Second, and most importantly, optimal discretization also acts as an independence test by allowing for single bin partitions whenever multiple-bin partitioning provides less information than its associated complexity cost.</p><p>From the perspective of clinical applications, the method is able to globally uncover interdependences within complex heterogeneous data from medical records without specific hypothesis nor prior knowledge on any clinically relevant information. The reconstructed clinical network from cognitive disorder patients (Fig <ref type="figure" target="#fig_12">4</ref>) recovers well known as well as novel direct and indirect relations between medically relevant variables.</p><p>In addition, we found that this reconstructed clinical network captures also some facets of the neurologist's reasoning behind the diagnoses of distinct dementias. In particular, diagnosis nodes can be interpreted as "explanatory" variables associated to a number of "explainingaway effects" <ref type="bibr">[23]</ref> in the form of "v-structures", i.e., D 1 ! S/E D 2 , whenever alternative diagnoses, D 1 or D 2 , can independently explain a given syndrome, S, or the result of a specific examination, E. Examples discussed in more details above are PARK_DEM ! PARK_Sd LEWY, VASC_DEM ! Fazekas MIXED_FORMS and VASC_DEM ! Ischemic_Stroke MIXED_FORMS. In addition, anticorrelations between different diagnostic nodes reflect the alternative choices of diagnosis by the neurologist, either in the form of "differential diagnoses" through a reasoning by elimination, in particular, to diagnose Alzheimer's disease, i.e., VASC_DEM a ALZHEIMER, or in the form of a latent variable, visualized as bidirected dotted edges and corresponding to alternative diagnoses by the neurologist, i.e., ALZHEI-MER⤎diagnosis⤏MIXED_FORMS or ALZHEIMER⤎diagnosis⤏BIPO. Latent variables may also represent the clinician's decisions between alternative treatments, e.g., APD⤎clinician_de-cision⤏VKA or a nonrecorded or implicite information in the patient personal or medical history, e.g., active_smoker⤎ever_smoked⤏quit_smoking, Fig <ref type="figure" target="#fig_12">4</ref>.</p><p>The main strengths of our clinical network reconstruction method are three-fold. First, it performs an unbiased check on the database content (expected, yet missing direct links in the reconstructed network hint to likely problems in the database e.g., erroneous or missing data). Second, it does not need any expert-informed hypothesis and provides, without prior knowledge in the field, graphical models complementing analyses by experts. Finally, it can discover novel unexpected direct interdependencies between clinically relevant information, such as the direct connection between Fazekas and Scheltens scales, Fig <ref type="figure" target="#fig_12">4</ref>, which may provide some physiological insights and suggest new research directions for further investigation.</p><p>Hence, beyond the challenge of learning clinical networks from mixed-type data, our method offers a user-friendly global visualisation tool of complex, heterogeneous clinical data which could help other practitioners visualize and analyze direct, indirect and possibly causal effects from patient medical records.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Fig. Adaptive information-maximizing partitions depending on interaction strength.</head><p>To assess the range in bin numbers depending on the strength of interaction between variables, we generated N = 1, 000 independent samples for 10,000 Gaussian bivariate distributions with a uniformly distributed correlation coefficient ρ in [-1, 1]. The real mutual information (RI) of Gaussian bivariate distributions can be computed directly <ref type="bibr">[1]</ref>, as RI(X; Y) = -log(1ρ 2 )/2. For each pair (X, Y), we estimated the mutual information with the proposed optimum bivariate discretization as well as the Maximal Information Coefficient <ref type="bibr">[22]</ref> using the minepy package <ref type="bibr">[36]</ref> (A) The information-maximizing partition proposed in the present paper behaves as expected: the number of bins on each variable is roughly similar and scales monotonically with the strength of the interaction between variables. This implies that additional bins are only introduced when their associated complexity cost is justified by a larger gain in mutual information. Conversely, when the information between X and Y approaches zero, both variables are partitioned into fewer and fewer bins until a single bin is selected for each variable, when they are inferred to be independent, given the available data. (B) The partition chosen to estimate the Maximal Information Coefficient is very different, regardless of the interaction strength, as it systematically corresponds to an unbalanced distribution of bins between the two variables, with one variable usually partitioned into the maximum number of bins(set by default to floor(N 0.6 /2) = 31) while the other is discretized into two levels only. This result is not unexpected, however, as the Maximal Information Coefficient <ref type="bibr">[22]</ref> is defined by maximizing the mutual information of the discretized variables over the grid,</p><formula xml:id="formula_101">Ið½X� D x ; ½Y� D y Þ,</formula><p>normalized by the minimum of log Δ x and log Δ y . Indeed, maximizing the normalized mutual information is done by partitioning as few samples as possible into the maximum number of bins in one dimension (as sketched in Fig 1 ), while simultaneously minimizing the number of bins, and thus log Δ i , in the other dimension. See further discussion in <ref type="bibr">[37]</ref>. (EPS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Fig. Interaction-dependent optimum discretization.</head><p>Optimum bivariate partitions obtained from N = 1, 000 samples of two different joint distributions P(X, Y) sharing the same sampling of X taken from a uniform distribution on [0, 0.3], but with different dependences for Y. (A) Y is defined as log(X) + � 1 , and (B) Y is defined as X 5 + � 2 , where � 1 and � 2 are Gaussian noise terms chosen so that the mutual informations of both examples are PLOS COMPUTATIONAL BIOLOGY comparable,I(X;Y) ' 0.75. This example shows that the optimum partition for X depends on its specific relation with Y and needs to be discretized with finer partitions in (A) at low X values for which Y ' logX varies the most and in (B) at higher X values for Y ' X 5 . (EPS) S4 Fig. <ref type="figure">Mutual</ref> information estimation for Gaussian bivariate distributions. 100 bivariate normal distributions were sampled for varying sample sizes, increasing from top to bottom, and correlation coefficients ρ ranging from 0.01 to 0.9. The mutual information was estimated with the proposed optimum discretization scheme and the KSG estimator with different parameters k. The mean squared error (center graphs) was calculated thanks to the analytical result of the mutual information of the bivariate Gaussian: I(X; Y) = -log(1 -ρ 2 )/2. The standard deviation of each estimator over the 100 replications was also plotted against the correlation coefficient (right). (EPS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Fig. Mutual information estimation of mixed variables.</head><p>Experiment set-ups and analytical values for the mutual information were taken fom <ref type="bibr">[7]</ref> and 50 runs were performed for each sample size N. Our proposed approach is compared to a naive equal-frequency discretization with N 1/3 bins, a kernel and a noisy KSG estimator as implemented in JIDT <ref type="bibr">[38]</ref>, as well as the recent KSG extensions for estimating the mutual informmation between a categorical and a continuous variable (mixed KSG Ross <ref type="bibr">[6]</ref>), and between mixed-type variables (mixed KSG Gao <ref type="bibr">[7]</ref>). For all nearest-neighbour based approaches, the number of nearest neighbours was set to k = 5. From left to right, top to bottom, the simulations are devised after experiment I, experiment II, experiment IV with p = 0 and experiment IV with p = 0.15, from <ref type="bibr">[7]</ref>. (EPS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Fig. Conditional mutual information estimation for multivariate Gaussian distributions.</head><p>Four-dimensional normal distributions P(X, Y, Z 1 , Z 2 ) were sampled for N = 100 to 5, 000 samples 100 times for each correlation coefficient ρ = ρ XY , chosen between 0.05 and 0.95. The other pairwise correlation coefficients were fixed as r XZ 1 ¼ r XZ 2 ¼ r YZ 1 ¼ r YZ 2 ¼ l ¼ 0:7 and r Z 1 Z 2 ¼ 0:9. The conditional mutual information I(X; Y |Z 1 , Z 2 ) was then estimated using the proposed optimum partitioning scheme as well aswith kNN conditional information estimates as in S4 Fig. <ref type="figure">ρ</ref> values closed to zero, mimick "V-structures" as they correspond to pairwise independence but conditional dependence; by constrast r ¼ 2l 2 =ð1 þ r Z 1 Z 2 Þ ' 0:5158 corresponds to conditional independence, while ρ &gt; 0.5158 impliesthat X and Y share more information than the indirect flow through Z 1 and Z 2 . The analytical value of the conditional mutual information is derived as follows; given the 4 × 4 covariance matrix S and its four 2 × 2 partitions S ij , we first compute the conditional covariance matrix � S ¼ S 11 À S 12 S À 1 22 S 21 where S À 1 22 is the generalized inverse of S 22 . The partial correlation between X and Y is obtained as</p><formula xml:id="formula_102">r XY�Z 1 Z 2 ¼ � S 12 =</formula><p>ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi � S 11 � � S 22 p , and the analytical conditional mutual information for a multivariate normal distribution is given by IðX; YjZ</p><formula xml:id="formula_103">1 ; Z 2 Þ ¼ À logð1 À r 2 XY�Z 1 Z 2 Þ=2. (EPS)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7 Fig. Pairwise dependence and conditional independence between X and Y sharing a common cause Z.</head><p>This example illustrates the (conditional) correlation patterns emerging from the presence of a confounding variable, as depicted by the causal diagram X Z ! Y. Z is generated with a uniform law U(0, 1) for N = 1, 000 observations and X, Y are both defined as 2Z + � with independent normal noise � � N ð0; 0:2Þ. (A) optimum discretization maximizing I 0 N ðX; YÞ with a strong pairwise correlation, and (B) optimum discretization which maximizes the conditional mutual information with finite size correction, I 0 N ðX; YjZÞ. In the latter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>case, the optimum discretization scheme results in a single bin on both variables as the flow information between X and Y is blocked by conditioning on the common cause Z.</p><p>(EPS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 Fig. Pairwise dependence and conditional independence between non Gaussian X and Y</head><p>sharing a common categorical cause. Another confounding example, X Z ! Y, taken from <ref type="bibr">[25]</ref> with a uniform categorical Z with three levels, X and Y being continuous, for N = 1, 000 observations. With Z i the binary variable corresponding to the i-th dummy variable of Z, we defined The two variables X and Y being independent, no multi-bin discretization can be found to yield an information estimate that is greater than the corresponding complexity cost. However, (B) conditioning on the common effect Z 'activates' the v-structure path generating a spurious relationship between X and Y. This is reflected in the fact that the induced interaction between X and Y requires a multiple bin optimum discretization to estimate I N (X; Y|Z) = 1.188 (with I 0 N ðX; YjZÞ ¼ 0:745).  <ref type="figure" target="#fig_60">S11</ref>). Performances obtained with our parameter-free informationtheoretic approach MIIC (magenta) are compared to the results obtained with the best parameterization (maximizing the skeleton F-score) of CausalMGM <ref type="bibr">[24]</ref> (blue) and MXM <ref type="bibr">[25]</ref> (green). See Supporting Information. (EPS)</p><formula xml:id="formula_104">X = -Z 1 + Z 2 + 0.2� X which is centered around either -1 if Z = 1, 0 if Z = 3 or 1 if Z = 2; and Y = Z 1 + Z 2 + 0.2� Y , � � N ð0; 1Þ which is centered around either 0 if Z = 3 or 1 if Z = 1 or Z = 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12 Fig. CPDAG assessment of benchmark networks for mixed-type, non-linear, non-</head><p>Gaussian datasets. CPDAG Precision, Recall and F-scores obtained for benchmark random networks with 100 nodes and average degree 3 reconstructed from N = 100-5,000 samples (see histogram example S11 Fig) . Performances obtained with our parameter-free information-theoretic approach MIIC (magenta) are compared to the results obtained with the best parameterization (maximizing the CPDAG F-score) of CausalMGM <ref type="bibr">[24]</ref> (blue) and MXM <ref type="bibr">[25]</ref> (green). See Supporting Information.  <ref type="figure" target="#fig_66">S14</ref>). Results obtained with our parameter-free information-theoretic approach MIIC are compared for optimum non-uniform bin sizes and for equal frequency bin sizes (with N 1/3 bins) as well as to the best results obtained with alternative continuous data methods: PC with Gaussian conditional independence test, rankPC and rankFCI from the pcalg package <ref type="bibr">[27]</ref>, kPC with theHelbert-Schmidt Independence Criterion <ref type="bibr">[39,</ref><ref type="bibr">40]</ref> and CAM <ref type="bibr">[26]</ref> algorithms, after optimizing their respective parameter (α) for each sample size N. See Supporting Information. (EPS)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S15 Fig. CPDAG assessment of benchmark networks for continuous, non-linear, non-</head><p>Gaussian datasets. CPDAG Precision, Recall and F-scores obtained for benchmark random networks with 100 nodes and average degree 3 reconstructed from N = 100 -10, 000 samples (same simulation settings as in Fig. <ref type="figure" target="#fig_67">S15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(EPS)</head><p>In this section we describe the generation of datasets used for the mixed-type (Fig. <ref type="figure" target="#fig_24">3</ref>, Figs. <ref type="figure" target="#fig_60">S11</ref> and<ref type="figure" target="#fig_61">S12</ref>) and continuous (Figs. <ref type="figure" target="#fig_66">S14</ref> and<ref type="figure" target="#fig_67">S15</ref>) benchmarks, and implemented in the R script provided as supplementary material. First, the underlying DAG models were randomly drawn from the space of all possible DAGs <ref type="bibr">[1]</ref>, allowing for a maximum degree of 4 neighbours. Datasets were generated following the causal order of the generated DAG using non-linear structural equations models (SEMs), as outlined below.</p><p>The first nodes in the causal order have no parents, their distributions are sampled either from Gaussian mixtures of 1 to 5 modes (with equal σ) for continuous nodes or with a uniform random sampling of 2 to 4 categorical levels. The distribution of every other node X was generated as a function of its parents Pa(X) plus some Gaussian noise as, X = f (Pa(X)) + . Depending on whether X and its parents are continuous or categorical, different models were used:</p><formula xml:id="formula_105">• Continuous variable X</formula><p>The causal relationship between a continuous node X and its continuous parents Pa c (X) plus their pairwise interaction products I(Pa c (X)) was modeled using polynomials: X = R( Yi∈{Pac(X)∪I(Pac(X))} R(Y i , -1, 1) ci + , 0, 1) with c i chosen in <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>, some Gaussian noise with variance depending on the number of parents and c i , and R(X, min, max) a re-scaling function so that the distribution X is in the range [min, max]. In the case of mixed-type parents, i.e. with some continuous and some discrete parent variables, sets of c i were drawn for each combination of the discrete parents Pa d (X). If all its parents are categorical, a child node is categorical as well. Finally, the distribution of a continuous node has an equal probability to be transformed with a non-linear function, e X , sin(X) or cos(X), or to be retained as is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Discrete variable X</head><p>The continuous parents of a discrete node are first discretized by attributing categorical levels to the distinct peaks if there are any (see Fig. <ref type="figure" target="#fig_65">S13</ref>), or using equal frequency binning with log(N ) bins otherwise. The discrete distribution of the node X is then drawn from random sampling with probability w i for the ith level of X, where each combination of the levels of Pa d (X) are associated to a different set of probabilities {w i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance measures</head><p>For the evaluation, the network reconstruction was treated as a binary classification task and classical performance measures, precision, recall and F-score, were used, based on the numbers of true versus false positive (T P vs F P ) edges and true versus false negative (T N vs F N ) edges. The precision P rec = T P/(T P + F P ) indicates how reliable the edges of the reconstructed network are. This measure does not indicate, however, which fraction of the true edges are detected, which corresponds to the sensitivity or recall of the reconstruction, Rec = T P/(T P + F N ). Finally, the F-score is a global performance measure, which is defined as the harmonic mean of precision and recall measures: F score = 2P rec × Rec/(P rec + Rec). In particular, a Fscore of 1 implies a perfect reconstruction without F P nor F N edges.</p><p>In order to measure how well the orientations of the edges match those of the true DAG, we also define the orientation-dependent counts T P = T P -T P misorient and F P = F P + T P misorient with T P misorient corresponding to all true positive edges of the skeleton with different orientation/non-orientation status as in the true Complete Partially Directed Acyclic Graph (CPDAG). Here, CPDAG refers to the equivalence class of the true DAG, which is taken as the benchmark reference since different DAGs might be equivalent from the data point of view (i.e. if and only if they have the same skeleton and the same v-structures). The CPDAG precision, recall and F-score were then computed with the orientation-dependent T P and F P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark parameter tuning</head><p>The performances of some methods rely on tunable parameters which typically determine the sparsity of the inferred graph. In contrast, miic uses a complexity term derived from the normalised maximum likelihood and is essentially parameter-free. Although in real world applications the best settings cannot be known for certain, meaningful comparisons can only be done after each method has been properly parameterized. Here we detail the steps taken to find the best parameters for each benchmark setting.</p><p>For the mixed-type benchmarks, ranges of parameters for both CausalMGM <ref type="bibr">[2]</ref> and MXM <ref type="bibr">[3]</ref> methods were tested, and their best results (i.e. best F-scores) obtained for a given sample size (N ) and percentage of continuous node (p c ) were compared to miic results. For CausalMGM, the λ sparsity parameter for all edge types (discrete-discrete, continuous-continuous, discrete-continuous) was tested in {0.050, 0.073, 0.108, 0.158, 0.232, 0.341, 0.500}. For MXM, the significance threshold α used for the various independence tests was tested in {0.001, 0.005, 0.01, 0.05, 0.1, 0.2}.</p><p>For the continuous benchmarks, we first optimized each method on separate simulations to find a good approximating function for the best parameter α = f p (N ). The best values for the α N parameter of PC gaussian, PC rank, CAM for sample sizes N spaced evenly on a log scale between 100 and 10, 000 were first found using a zeroth order parameter optimization implemented in dlib <ref type="bibr">[4,</ref><ref type="bibr">5]</ref>. Then, the function f p was fitted as a second order polynomial over all values of N and α N . kPC (using the Hilbert-Schmidt independence criterion with gamma approximation <ref type="bibr">[6,</ref><ref type="bibr">7]</ref>) was not optimized so extensively, due to its much longer execution time, and was only tested for the conservative values of α: 0.05 and 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvements to constraint-based algorithms and MIIC</head><p>In this chapter, we will discuss other contributions to both constraint-based methods and MIIC specifically to be able to deal with imperfect real-life situations.</p><p>In the first section, we briefly discuss the presence of missing data and its implications in causal graph reconstruction with constraint-based methods. We then propose our solution to deal with missing data, based on test-wise deletion and an information-theoretic test to accept or reject potential conditioning nodes. Next, we introduce improved orientation scores for MIIC, adapted for larger sample sizes, and the concept of "putative" versus "genuine" causal orientations, based on the orientation probabilities of both the head and the tail of the edge X → Y . We also mention the advantages of interactive visualisation of the results, by presenting the updated MIIC webserver.</p><p>Finally, we introduce two papers, <ref type="bibr">[6]</ref> published at NeurIPS 2019 in which we aim to make constraint-based methods more interpretable with regard to the choice of separating nodes, making them more consistent with the final graph. Another paper accepted for publication at the Why-21 conference introduces conservative MIIC, which infers more reliable orientations thanks to modifications to the mutual information estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improvements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Handling missing data</head><p>One dimension of observational data that has not been discussed yet is the problem of missing data. Because the data collection was unreliable, or because the variable we are interested is note defined for all cases, some samples may have undefined values. There are typically two ways to deal with those missing values in a dataset. First, one can omit all data affected by missingness and perform the analysis on the subset of complete samples. Using this approach 84 Chapter 4. Other improvements to constraint-based algorithms we lose a lot of partial but valuable information, it is not ideal on e.g. bio-medical datasets, often small or medium size, and for which having a reliable and systematic observation of all variables is difficult.</p><p>The second way to deal with missing data is to impute them using the rest of the observations. If the missing values can be estimated correctly, then one can analyse the data with the same power as the full data analysis would have had. This is the preferred way in many applications, but must make assumptions on the distribution of the missing data P(X Mis ). We can distinguish 3 mechanisms by which the data is missing, identified by Rubin in his seminal work <ref type="bibr" target="#b269">[111]</ref> :</p><p>• Missing Completely at Random (MCAR) : the missingness mechanism is completely random, it does not depend on any other value : P(X Obs ) = P(X Mis ) and P(X Mis ) ⊥ ⊥ P(V ).</p><p>• Missing at Random (MAR) : unlike the name suggests, the missing mechanism can be biased (p(X Mis ) =p(X Obs )) on the condition that it can be explained by the observed data.</p><p>• Missing Not at Random (MNAR) : the probability of the missing data cannot be deduced from observations.</p><p>As Little and Rubin observe in <ref type="bibr" target="#b270">[112]</ref>, essentially all work on multivariate incomplete data, including imputation, makes at least the MAR assumption. It is however often problematic as there is no recognizable criterion for MAR from a dataset alone <ref type="bibr" target="#b271">[113]</ref>. Another issue with data imputation is the prediction of the values itself, which depends on the amount of information I(X;V \X ) and is a difficult problem of its own, especially for continuous distributions. As is often the case when analyzing real data, there is no free lunch when dealing with missing samples. Rubin himself concluded his article <ref type="bibr" target="#b269">[111]</ref> saying that the only correct solution is to act on a case-by-case basis, preferably modeling explicitly the missingness mechanism.</p><p>Interesting work has been done in this direction by Mohan and Pearl <ref type="bibr" target="#b272">[114]</ref> using graphical models called missingness graphs. As suggested by Rubin, the idea is to model the missingness processes explicitly, using Pearl's causal graphs, from which one can easily deduce whether the data is MCAR, MAR or MNAR. Later, the authors also showed that certain properties of the distribution may be recoverable when data is partially observed, including conditional independence relations <ref type="bibr" target="#b271">[113]</ref>.</p><p>But how can we apply those findings to improve graph reconstruction with incomplete data ? This topic has been investigated for some time, for example <ref type="bibr" target="#b273">[115]</ref> introduced a variant of PC using a pseudo-Bayesian test of independence which relies on the local graph and its parameters. However, it needs to represent the data with contingency tables, making it unusable for the mixed case (generally, methods relying on data inference through the Bayesian graph are difficult to implement in the non-parametric setting). Similarly, <ref type="bibr" target="#b274">[116]</ref> uses the expectation maximisation principle to learn both the graph and its parameters with incomplete data.</p><p>These solutions are much harder to implement for the general case, i.e. with as little restriction as possible on the distribution P(V ). We will instead focus on test-wise omission of missing samples, removing only what is necessary to have complete support for each conditional independence test. Strobl et al. developed a scheme combining FCI and test-wise deletion that is still able to recover the PAG from incomplete data, even when MNAR holds, provided that no missingness mechanisms causally affect each other <ref type="bibr" target="#b275">[117]</ref>. This last condition is discussed in more details in <ref type="bibr" target="#b276">[118]</ref>, which focuses on rectifying erroneous edges produced by test-wise deletion PC when MAR or MNAR holds. However, their method is not very well suited to our setting as it needs either the residuals of a linear regression model or estimates of the full data distribution via kernel density estimation.</p><p>What we propose here is a simple information-theoretic rule for rejecting or accepting potential conditioning nodes during the skeleton phase of constraint-based algorithms. Specifically, we want to avoid removing edges because of selection bias as opposed to "explaining away" the direct correlation via other information flow in the causal graph. Our reasoning is that as much as possible, conditional independencies should be read off the inferred graph G in f , i.e. X and Y are not adjacent if and only if X ⊥ ⊥ Y |Z (regardless of missingness). We compare joint the distributions X,Y before and after removing the samples that are missing for a potential separating node Z, respectively noted (X,Y ) and (X,Y |Z Obs ). If (X,Y |Z Obs ) is too different from (X,Y ), then we do not accept Z in the conditioning set of X -Y , as the observed interaction on the reduced support is not representative of the full data. Note that in contrast with <ref type="bibr" target="#b276">[118]</ref>, this scheme can only add back edges in G in f . We argue that it makes the result more interpretable in relation to the way constraint-based methods operate : starting from the complete graph, we remove the edge X -Y only if there is evidence in the data that the link is either non existent (X ⊥ ⊥ Y ) or indirect (X ⊥ ⊥ Y |Z). If there is no such evidence, or if we cannot accept it due to potential selection bias, the edge X -Y stays in G in f . Moreover, it is a simple rule that does not require any assumption about the data distribution, using an information-theoretic measure that fits well with the rest of the MIIC algorithm. This problem is a version of the two-sample test, which aims to determine if two samples come from the same population. It can be naturally approached using the KL divergence (Eq 3.1.7) <ref type="bibr" target="#b277">[119]</ref>, which we redefine here for the discrete and continuous cases :</p><formula xml:id="formula_106">D KL (P Q) = ∑ x∈X p(x) log p(x) q(x) (4.1.1) = x∈X p(x) log p(x) q(x) dx (4.1.2)</formula><p>where P and Q are two distributions defined on the same space X.</p><p>We can then restate our goal using this divergence : during the skeleton phase, test for conditional independence X ⊥ ⊥ Y |Z only if</p><formula xml:id="formula_107">D KL ((X,Y | Z Obs ) (X,Y )) &lt; t KL (4.1.3)</formula><p>where t KL is the threshold for how much divergence we tolerate.</p><p>Unsurprisingly, the approaches to estimate this divergence from samples are similar to the ones introduced for entropy and mutual information. On discrete data, one can simply use the observed frequencies and use a plug-in estimator with Eq 3.1.7 <ref type="bibr" target="#b222">[64]</ref>. It may be tempting to re-use the optimal discretization found by optimizing Î (X;Y ), and simply use the plug-in estimator for Eq 4.1.3. But optimizing Î (X;Y |Z) may give a very different discretization, depending on the interaction with Z which is not represented by optimizing on (X,Y ) alone. Additionally, MIIC takes off the contributors one by one for each edge X -Y , so we do not know the full set U i in advance. For these reasons, Eq 4.1.3 should be computed with the full data, not on reduced discretized versions, to be able to account for future discretizations. However, much like for the mutual information, estimating D KL on continuous and mixture variables is a challenge.</p><p>On continuous data, the best-behaved estimator seems to be a k-nn scheme introduced in <ref type="bibr" target="#b278">[120]</ref> : with P and Q two samples respectively defined on the spaces X and X , of length n and m samples on d dimensions :</p><formula xml:id="formula_108">DKL (P Q) = d log r k (x i ) s k (x i ) n + log m n -1 (4.1.4)</formula><p>where r k (x i ) and s k (x i ) are respectively the distance to the kth nearest neighbor from the point x i in X and X , and log r k (x i )</p><formula xml:id="formula_109">s k (x i ) n</formula><p>is the average taken over all n samples i.</p><p>Our proposed estimator is inspired by mixed estimator of mutual information <ref type="bibr" target="#b229">[71,</ref><ref type="bibr">51]</ref> and treats each case differently. When X and Y are discrete, the plug-in estimator is used with the observed joint counts X,Y . On two continuous samples, we use the k-nn estimator DKL of Eq 4.1.4 with a fixed k = 5. When X is discrete and Y is continuous, the estimator needs to be a bit more involved. Just like <ref type="bibr" target="#b229">[71]</ref>, we sum partial terms over all levels r x of the discrete variable X :</p><formula xml:id="formula_110">DKL (X,Y |Z Obs X,Y ) = d n ∑ r x ∈X n r x log r k (x i ) r x s k (x i ) r x + log m r x n r x -1 n r x</formula><p>+ DKL (X|Z Obs X) (4.1.5) with r k (x i ) r x and s k (x i ) r x respectively the distance to the kth nearest neigbhor in the space of Y in the subsample Z Obs and in the full data, m r x and n r x the total number of points with the same discrete value r x in the full data and in the subsample. Correspondingly, the average is taken over the samples n r x . DKL (Y |Z Obs Y ) is the divergence between the frequencies of the discrete levels X.</p><p>The implementation in MIIC uses an efficient k nearest neighbor scheme using KD-tree index, implemented in nanoflann <ref type="bibr" target="#b279">[121]</ref>. The complexity of finding the nearest neighbor using KD-tree is O(log n) on average and O(n) in the worst case. Finally, to deal with mixture variables, it adds random uniform noise to break up discrete points (just like the noisy KSG estimator). The distances r k and s k are in the 2 norm, which was shown to improve k-nn estimates <ref type="bibr">[51]</ref>.</p><p>Having defined an estimator for the general case, all that remains is to choose divergence threshold t KL in Eq 4.1.3. Of course, the ideal threshold would be adapted to the data in order to control for false-postive and false-negative rates, which can be done for information theoretic values <ref type="bibr" target="#b280">[122]</ref>. We propose a simple heuristic based on the MDL principle and the BIC, and compare |Z Obs | • DKL (X,Y |Z Obs X,Y ) to t KL = log |Z Obs |. Note that it does not behave like a p-value : a smaller, noisier subsample will necessarily create more diverging distributions even under the null hypothesis (MAR), and a fixed p-value will correspond to higher values of D KL ; whereas the threshold log |Z Obs | becomes more stringent as the subsample decreases in size. This reflects the fact that smaller samples are less representative of the full data, and so contain less information to remove the edge X -Y . The choice of a better threshold is left as a perspective for future research in the group, using this heuristic the goal is to avoid the worst cases of selection bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Orientation probability for large N, putative versus genuine orientations</head><p>While constraint-based methods can in principle learn the presence or absence of orientation of individual edges from the available data, the orientation of a V-structure, X → Z ← Y (and X --Y ) corresponds in fact to the discovery of 'putative causality' as one cannot rule out a priori that the edge between X (or Y ) and Z is not due to the effect of a latent common cause, L, unobserved in the dataset, X L Z. In order to discover a genuine cause-effect relations explaining at least part of the association between X and Z, X → Z, one needs to exclude the possibility of such a latent variable, L, between X and Z. We can think of "genuine" orientation in constraint-based methods as setting both the "head" of the edge X → Y and its "tail", excluding latent common causes. Using the MIIC framework, genuine causal edges are then predicted if the head and tail probabilities are statistically significant, while causal edges remain "putative" if their tail probability is not statistically significant or cannot be determined from purely observational data (i.e., undirected links in the G c equivalence class). This gives a better interpretation of constraint methods on real data, for which it is difficult to ensure with certainty that all variables in the system are observed, and Chapter 4. Other improvements to constraint-based algorithms thus that the directed links of G in f are "genuine".</p><p>We now outline the principles to uncover cause-effect relations and distinguish genuine from putative causes through an intuitive toy example of an imaginary dataset of old cars (Fig 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1). (A)</head><p>The signature of causality in such observational datasets corresponds to 3-variable V-structure motifs involving two independent and thus unconnected possible causes, "Broken fuel pump?" and "Discharged battery?", and a resulting effect, "Broken down car?". The converging orientations of this v-structure towards its middle variable, "Broken down car?", stem from the fact that these two edges cannot be undirected, nor can they point towards either "Broken fuel pump?" or "Discharged battery?", as these alternative graphical models would imply correlations contradicting the independence between "Broken fuel pump?" and "Discharged battery?". In practice, such independences between possible causes might in fact be conditional on other variable(s), not considered here. (B) Note, however, that v-structures only identify, "putative" causes, which might not be "genuine" causes; for instance, the variable "Clock stopped?", which can be used as a proxy for the variable "Discharged battery?", also forms a v-structure with the other independent putative cause "Broken fuel pump?". Yet, we know that "Clock stopped?" cannot be a genuine cause of "Broken down car?", as tampering with a car's clock cannot actually cause a car to break down. (C) In absence of background knowledge, showing that "Discharged battery?" is actually a genuine cause of "Broken down car?" (displayed with a green arrowhead) requires to find another v-structure upstream of "Discharged battery?" (e.g. "Lights left on?"→"Discharged battery?"←"Old battery?") or to have prior knowledge about an upstream (putative) cause and to show that the effect of these upstream variables on the downstream variable "Broken down car?" is entirely indirect and mediated (at least in part) by the intermediary variable "Discharged battery?". This requires to find a conditional independence between upstream and downstream variables conditioned on a separating set including the intermediary variable "Discharged battery". These conditions are needed to exclude the possibility of an unobserved common cause between the intermediary variable ("Discharged battery") and the downstream variable ("Broken down car?"), as illustrated in (D). (D) Ruling out a putative cause as genuine cause is done by finding a fourth variable (e.g. "Out-of-order clock?") defining another v-structure sharing the edge between "Broken down car?" and "Clock stopped?" with the v-structure in (B). It implies that the relation between these two variables is actually due to a latent common cause unobserved in the dataset (here "Discharged battery?") and represented with a bidirected edge.</p><p>Formally, we implement the idea of separate likelihood-based estimation of orientation probability. For an edge X --Z, each end point of which is either an arrow head or tail, we denote by p x (p z ) the probability of the end point at X (Z) being an arrowhead X ← Z (X → Z), and by 1 -p x (1 -p z ) the probability of the that end being a tail. Undecided head or tail orientations thus correspond to p = 1 -p = 0.5. With this notation, we predict   <ref type="figure">p</ref> x &lt; 1/2, whereas putative causal edges are predicted for p z &gt; 1/2 and p x = 1/2, that is when p x cannot be decided with the available data. By contrast, undirected (or undecided end) edges are expected for p z 1/2 and p x 1/2, while bidirected edges, corresponding to the presence of a latent common cause, are predicted for p z &gt; 1/2 and p x &gt; 1/2. Orientation probability estimates are computed below, together with the introduction of an orientation confidence threshold, β , enhancing the precision on arrow head prediction. The present probabilistic framework of edge orientation also allows for enforcing prior knowledge about certain orientations, in particular, when a variable is not freely varying like other variables of the dataset as it corresponds to a control parameter or experimental condition. Such contextual variables will have all their edges without incoming arrow head, i.e., p in = 0, by assumption. This expresses our prior knowledge that contextual variables cannot be the consequence of other observed or non-observed variables as they actually correspond to manually set external parameters or experimental conditions.</p><p>Using the orientation threshold 1 &gt; β &gt; 0.5, we can enhance the precision of arrowheads. The condition for predicting genuine causal edges then becomes X → Z, if p z &gt; β and p x &lt; 1β . By contrast, putative causal edges are predicted for p z &gt; β and β p x 1β , while undirected edges are expected for p z β and p x β , and bidirected edges, corresponding to the presence of a latent common cause, for p z &gt; β and p x &gt; β (Fig 4 .2).</p><p>The way to compute the probabilities p x , p y and p z builds on the approach introduced in  [5], deducing tail / head orientation probabilities from an existing arrowhead z ← y :</p><formula xml:id="formula_111">P(x -- * z) = P(x -- * z|z ← y)P(z ← y) + P(x -- * z|z --y)P(z --y) (4.1.6)</formula><p>where * stands for a tail [resp. head] depending on the positivity [resp. negativity] of I (X;Y ;</p><formula xml:id="formula_112">Z|{ A i }) with X ⊥ ⊥ Y |{ A i } ∪ Z [resp. X ⊥ ⊥ Y |{ A i }].</formula><p>However, using the full probability decomposition above can lead to a higher confidence in tail or head induced probabilities than in the head probabilities they derive from, due to the Markov equivalence of non-V-structures. In addition, induced tail / head probabilities can be numerically difficult to compare for large N. To circumvent these issues and capture the rationale that our confidence in induced tail / head orientations can only be lower than our confidence in the arrowhead from which they derive, we propose to redefine the tail / head induced probabilities by retaining only the first term in the probability decomposition above, that is, by assuming that the arrowhead z ← y exists,</p><formula xml:id="formula_113">P(x -- * z) = P(x -- * z|z ← y)P(z ← y) = 1 1 + e -N| I (X;Y ;Z { A i })| P(z ← y) = 1 1 + e -N| I (X;Y ;Z { A i })| × 1 1 + e -score v = 1 1 + e -N| I (X;Y ;Z { A i })| + e -score v + e -N| I (X;Y ;Z { A i })|-score v = 1 1 + e -min + e -max + e -min-max = 1 1 + e -min (1 + e -max+min + e -max ) = 1 1 + e -score i</formula><p>where we introduce score i to enable numerical ordering of orientation probabilities for large N, Hence 0.5 p 1 &lt; p 2 &lt; 1 is equivalent to 0 score 1 &lt; score 2 &lt; ∞, where score 1 and score 2 can be numerically ordered even for very large N, unlike p 1 and p 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Webserver and interactive visualisation</head><p>In  In this example, a violin plot describes the joint distribution between the continuous variable "RCB", and "Death" which is discrete. The horizontal bold lines inform on the optimal discretization found to infer the edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consistent separating sets</head><p>In the same idea as the missing data test, we worked on a variant of constraint-based algorithms that guarantees that the conditioning sets used to remove links are more consistent with the final graph G in f and the data D <ref type="bibr">[6]</ref>. In their original form, these methods rely only on the conditional independences in D and offer no guarantee that the separating sets correspond to d-separations sets in the final graph. In fact, they do not even guarantee that they are still in the same connected component in G in f after the iterative removal of edges. This flaw not only makes the result less interpretable but also makes the performance worse. These inconsistent conditioning sets tend to come from sampling noise rather than from functional realities, and graphs reconstructed on complex data are typically very sparse. The consistent version of constraint-based algorithms produces a G in f graph that is less prone to spurious independencies and from which it is easier to infer the sets of condensations used, making the method more interpretable. This extension is particularly adapted to MIIC which removes the contributors in order starting with the best score, compared to reference methods which try all possible combinations until significance is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Publication at NeurIPS 2019</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraint-based Causal Structure Learning with</head><p>Consistent Separating Sets Honghao Li, Vincent Cabeli, Nadir Sella, Hervé Isambert * Institut Curie, PSL Research University, CNRS UMR168, Paris {honghao.li, vincent.cabeli, nadir.sella, herve.isambert}@curie.fr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We consider constraint-based methods for causal structure learning, such as the PC algorithm or any PC-derived algorithms whose first step consists in pruning a complete graph to obtain an undirected graph skeleton, which is subsequently oriented. All constraint-based methods perform this first step of removing dispensable edges, iteratively, whenever a separating set and corresponding conditional independence can be found. Yet, constraint-based methods lack robustness over sampling noise and are prone to uncover spurious conditional independences in finite datasets. In particular, there is no guarantee that the separating sets identified during the iterative pruning step remain consistent with the final graph. In this paper, we propose a simple modification of PC and PC-derived algorithms so as to ensure that all separating sets identified to remove dispensable edges are consistent with the final graph, thus enhancing the explainability of constraint-based methods. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration. Ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, and is found to increase their validity in terms of actual d-separation. It also significantly improves the sensitivity of constraint-based methods while retaining good overall structure learning performance. Finally and foremost, ensuring sepset consistency improves the interpretability of constraint-based models for real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the oracle versions of constraint-based methods have been demonstrated to be sound and complete <ref type="bibr" target="#b66">(Zhang, 2008;</ref><ref type="bibr">Spirtes, Glymour, and Scheines, 2000;</ref><ref type="bibr">Pearl, 2009)</ref>, a major limitation of these methods is their lack of robustness with respect to sampling noise for finite datasets. This has largely limited their use to analyze real-life data so far, although important advances have been made lately, in particular, to limit the order-dependency of constraint-based methods <ref type="bibr">(Colombo and Maathuis, 2014)</ref> or to improve their robustness to sampling noise by recasting them within a maximum likelihood framework (Affeldt and Isambert, 2015; Affeldt, Verny, and Isambert, 2016). However, it remains that constraint-based methods still lack graph consistency, in practice, as they do not guarantee that the learnt structures belong to their presumed class of graphical models, such as a completed partially directed acyclic graph (CPDAG) model for the PC <ref type="bibr">(Spirtes and Glymour, 1991;</ref><ref type="bibr" target="#b52">Kalisch and Bühlmann, 2008;</ref><ref type="bibr">Kalisch et al., 2012)</ref> or IC <ref type="bibr">(Pearl and Verma, 1991)</ref> algorithms, or a partial ancestral graph (PAG) for FCI or related constraint-based algorithms allowing for unobserved latent variables <ref type="bibr" target="#b63">(Spirtes, Meek, and Richardson, 1999;</ref><ref type="bibr" target="#b57">Richardson and Spirtes, 2002;</ref><ref type="bibr">Colombo et al., 2012;</ref><ref type="bibr">Verny et al., 2017;</ref><ref type="bibr">Sella et al., 2018)</ref>. By contrast, search-and-score structure learning methods <ref type="bibr" target="#b54">(Koller and Friedman, 2009)</ref> inherently enforce graph consistency by searching structures within the assumed class of graphs, e.g., within the class of directed acyclic graphs (DAG). Similarly, hybrid methods such as MMHC <ref type="bibr" target="#b64">(Tsamardinos, Brown, and Aliferis, 2006)</ref> can also ensure graph class consistency by maximizing the likelihood of edge orientation within the class of DAGs. This paper concerns, more specifically, the inconsistency of separating sets used to remove dispensable edges, iteratively, based on conditional independence tests. This inconsistency arises as some separating sets might no longer be compatible with the final graph, if they were not already incompatible with the current skeleton, when testing for conditional independence during the pruning process. It occurs, for instance, when a node in a separating set is not on any indirect path linking the extremities of a removed edge, as noted in <ref type="bibr">(Spirtes, Glymour, and Scheines, 2000)</ref>. Such inconsistencies can be seen as a major shortcoming of constraint-based methods, as the primary motivation to learn and visualize graphical models is arguably to be able to read off conditional independences directly from the graph structure <ref type="bibr">(Spirtes, Glymour, and Scheines, 2000;</ref><ref type="bibr">Pearl, 2009)</ref>.</p><p>In the following, we propose a simple modification of PC or PC-derived algorithms so as to ensure that all conditional independences identified and used to remove dispensable edges are consistent with the final graph. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration, until a limit cycle of successive graphs is reached. The union of the graphs over this limit cycle is then guaranteed to be consistent with the separating sets and corresponding conditional independences used to remove all dispensable edges from the initial complete graph. Enforcing sepset consistency of constraint-based methods is found to limit their tendency to uncover spurious conditional independences early on in the pruning process when the combinatorial space of possible separating sets is still large. As a result, enforcing sepset consistency reduces the large number of false negative edges usually predicted by constraint-based methods (Colombo and Maathuis, 2014) and, thereby, achieve a better balance between their sensitivity and precision. Ensuring the consistency of separating sets is also found to increase their validity in terms of actual d-separation and, therefore, to improve the interpretability of constraint-based models for real-life applications. Moreover, ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, which enables to learn causal structures with consistent separating sets for a few hundred nodes. By contrast, earlier methods aiming at reducing the number of d-separation conflicts or other structural inconsistencies through SAT-based approaches, e.g. <ref type="bibr" target="#b51">(Hyttinen et al., 2013)</ref>, have a much larger complexity burden, which limits their applications to very small networks in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Result</head><p>2.1 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Terminology</head><formula xml:id="formula_114">A graph G(V , E) consists of a vertex set V = {X 1 , • • • , X p }</formula><p>and an edge set E. All graphs considered here have at most one edge between any pair of vertices. A walk is a sequence of edges joining a sequence of vertices. A trail is a walk without repeated edge. A path is a trail without repeated vertices. A cycle is a trail in which the only repeated vertices are the first and last vertices. Vertices are said to be adjacent if there is an edge between them. If all pairs of vertices in a graph are adjacent, it is called a complete graph and is denoted by G c . By constrast, an empty graph, denoted by G ∅ , consists of isolated vertices with no edges. The adjacency set of a vertex X i in a graph G, denoted by adj(G, X i ), is the set of all vertices in V that are adjacent to X i in G. If an edge is directed, as X → Y , X is a parent of Y and Y a child of X. A collider is a triple (X i , X j , X k ) in a graph where the edges are oriented as X i → X k ← X j . A v-structure is a collider for which X i and X j are not adjacent. Given a statistical significance level α, the conditional independence of a pair of variables (X i , X j ) given a set of variables C, is denoted by (X i ⊥ ⊥ X j |C) α , where C is called a separating set or "sepset" for (X i , X j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">The PC and PC-stable Algorithms</head><p>The PC algorithm <ref type="bibr">(Spirtes and Glymour, 1991)</ref>, outlined in algorithm 1, is the archetype of constraintbased structure learning methods <ref type="bibr">(Spirtes, Glymour, and Scheines, 2000;</ref><ref type="bibr">Pearl, 2009)</ref>, as illustrated 2 <ref type="bibr" target="#b252">94</ref> Chapter 4. Other improvements to constraint-based algorithms in Figure <ref type="figure" target="#fig_20">1</ref>. Given a dataset over a set of variables (vertices), it starts from a complete graph G. By a series of statistical tests on each pair of variables, all dispensable edges X Y are removed if a (conditional) independence and separating set C can be found, i.e. (X ⊥ ⊥ Y | C) (step 1). The resulting undirected graph is called the skeleton. V-structures are then identified,</p><formula xml:id="formula_115">X → Z ← Y , if (X ⊥ ⊥ Y | C) and Z /</formula><p>∈ C (step 2). Additional assumptions (e.g., acyclicity) allow for the propagation of v-structure orientations to some of the remaining undirected edges (Zhang, 2008) (step 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The PC Algorithm Require: V , D(V ), significance level α</p><p>Step 1: Find the graph skeleton and separating sets of removed edges</p><p>Step 2: Orient v-structures based on separating sets</p><p>Step 3: Propagate orientations of v-structures to as many remaining undirected edges as possible return Output graph Complete graph Skeleton Identify V-structures Propagation</p><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Figure <ref type="figure" target="#fig_20">1</ref>: General procedure of constraint-based structure learning.</p><p>While the oracle version of the PC-algorithm has been shown to be sound and complete, its application is known to be sensitive to the finite size of real life datasets. In particular, the PC-algorithm in its original implementation <ref type="bibr">(Spirtes, Glymour, and Scheines, 2000)</ref> is known to be order-dependent, in the sense that the output depends on the lexicographic order of the variables. This issue can be circumvented, however, for the first step of algorithm 1 with a simple modification given in algorithm 2 and referred to as Step 1 of PC-stable algorithm <ref type="bibr">(Colombo and Maathuis, 2014)</ref>.</p><p>Algorithm 2 Find skeleton and separating sets (Step 1 of PC-stable algorithm) Require: Conditional independence assessment between all variables V with significance level α</p><formula xml:id="formula_116">G ← G c � ← -1 repeat � ← � + 1 for all vertices X i ∈ G do end for a(X i ) = adj(G, X i ) repeat select a new pair of vertices (X i , X j ) adjacent in G and satisfying |a(X i )\{X j }| ≥ � repeat choose new C ⊆ a(X i )\{X j }, |C| = � if (X i ⊥ ⊥ X j |C) α then Delete edge X i X j from G Sepset(X i , X j | G) = Sepset(X j , X i | G) ← C end if until X i and X j are no longer adjacent in G or all C ⊆ a(X i )\{X j } with |C| = � have been considered</formula><p>until all pairs of adjacent vertices (X i , X j ) in G with |a(X i )\{X j }| ≥ � have been considered until all pairs of adjacent vertices (X i , X j ) in G satisfy |a(X i )\{X j }| ≤ � return G, sepsets 2.2 The Consistent PC Algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Lack of Robustness and Consistency of Constraint-based Methods</head><p>Beyond the order-dependence of the PC Algorithm, the general lack of robustness of constraint-based methods stems from their tendency to uncover spurious conditional independences (false negatives) between variables. This trend originates from the fact that conditioning on other variables amounts to "slicing" the available data into smaller and smaller subsets, corresponding to different combinations of categories or discrete values of the conditioning variables, over which independence tests are essentially "averaged" to assess conditional independence.</p><p>Hence, by making sure that all separating sets are actually consistent with the final graph, one expects to reduce the number of false negative edges due to spurious conditional independences inferred during the edge pruning process and, thereby, to improve the sensitivity (or recall) of the PC or PC-stable algorithms.</p><p>The inconsistency of separating sets can be of different forms, regarding either the skeleton (type I) or the final (partially) oriented graph (type II), as illustrated on Figure <ref type="figure" target="#fig_20">1</ref>.</p><p>A type I inconsistency corresponds to a conditional independence relation such as (2 ⊥ ⊥ 6 | 3) in Figure <ref type="figure" target="#fig_20">1</ref>, for which there is no path between vertex 2 and 6 that passes through 3. This type of inconsistency often involves edges evaluated early on in the pruning process when few edges have been removed, and thus the combinatorial space of possible separating sets is still large. In particular, edge 3 6, which is eventually removed in the final graph, may still exist when the edge 2 6 is under consideration.</p><p>A type II inconsistency is a different kind of incompatibility originating from the orientation of the skeleton. It occurs, in particular, when a conditional independence relation is conditioned on at least one common descendant of the pair of interest in the final graph, e.g. (3 ⊥ ⊥ 6 | 1) in Figure <ref type="figure" target="#fig_20">1</ref>. Since it stems from the orientation of edges (steps 2&amp;3), the origin of type II inconsistencies is generally more complex and results from a cascade of errors in both conditional independence tests and orientation.</p><p>These two types of inconsistency help define the following consistent set for candidate nodes of separating sets in absence of latent variables:</p><formula xml:id="formula_117">Definition 1 (Consistent set). Given a graph G(V , E) and a set of variables { X, Y, Z } ⊆ V , Consist(X, Y | G) = { Z ∈ adj(X) \ { Y } | 1. at least one path γ Z XY exists in G; 2. Z is not a child of X in G }</formula><p>where γ Z XY is a path from X to Y passing through Z. Note that for an undirected graph, the second condition is always satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Consistent PC Pseudocodes</head><formula xml:id="formula_118">Definition 2. NewStep1(G 1 |G 2 ) is a modified version of PC-stable step 1 (algorithm 2) where, 1. G c is replaced by G 1 , and 2. a(X i ) \ {X j } is replaced by a(X i ) \ {X j } ∩ Consist(X i , X j | G 2 )</formula><p>Note that algorithm NewStep1(G c |G c ) corresponds to the unmodified step 1 of original PC-stable algorithm 2. By constrast, algorithm NewStep1(G c |G ∅ ) removes all edges corresponding to independence without conditioning, as no separating set is involved. This unconditional independence search will be noted step 1a, while the subsequent conditional independence search will be referred to as step 1b, thereafter. Then, definition 3 allows to define algorithm 3, which ensures a consistent constraint-based algorithm through an iterative call of S algorithms, (S k ) k∈N � , following an initial step 1a, NewStep1(G c |G ∅ ). As illustrated on Figure <ref type="figure" target="#fig_0">2</ref> and proved below, algorithm 3 achieves separating set consistency by repeating step 1b and step 2&amp;3, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration, until a limit cycle of successive graphs is reached. </p><formula xml:id="formula_119">G c G 0 = NewStep1(G c |G ∅ ) G k = NewStep1(G 0 |G k-1 ) G k = S k (G 0 |G k-1 )</formula><p>Step 1a</p><p>Step 1b</p><p>Step 2&amp;3 Algorithm 4</p><p>Algorithm 3</p><p>Figure <ref type="figure" target="#fig_0">2</ref>: Illustration of the iterative procedure to learn graphical models with orientation-consistent (algorithm 3) or skeleton-consistent (algorithm 4) separating sets. Dashed edges mark the difference between two successive iterations. Proof of separating set consistency is given in theorem 4.</p><p>Algorithm 3 Sepset consistent PC algorithm (1st version, orientation consistency)</p><formula xml:id="formula_120">Require: V , D(V ), significance level α Ensure: G with consistent separating sets G 0 ← NewStep1(G c |G ∅ ) k ← 0 repeat k ← k + 1 G k ← S k (G 0 |G k-1 ) until loop detected, i.e., ∃n &gt; 0, G k-n = G k G ← � (G j )</formula><p>k j=k-n , with discarded conflicting orientations return G and consistent separating sets Alternatively, one may require a separating set consistency at the level of the skeleton only, i.e., before the orientation steps, which corresponds to algorithm 4, below. Indeed, early sepset inconsistencies at the level of the skeleton might cause orientation errors, which in turn can lead to the rejection of valid consistent separating sets in algorithm 3. As outlined in Figure <ref type="figure" target="#fig_0">2</ref>, the modification of algorithm 4 only concerns step 1b, which is called iteratively until a limit cycle is reached. Then, the orientation steps 2&amp;3 are performed as for classical PC or PC-derived algorithms, but using consistent separating sets with respect to the union of skeletons returned by the iterative call of step 1b in algorithm 4. However, as the orientation steps 2&amp;3 might induce additional type II inconsistencies, algorithm 4 requires a final consistency check for all separating sets with respect to the final graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Sepset consistent PC algorithm (2nd version, skeleton consistency)</head><formula xml:id="formula_121">Require: V , D(V ), significance level α Ensure: G with consistent separating sets G 0 ← NewStep1(G c |G ∅ ) k ← 0 repeat k ← k + 1 G k ← NewStep1(G 0 |G k-1 ) until loop detected, i.e., ∃n &gt; 0, G k-n = G k G ← � (G j )</formula><p>k j=k-n and consistent separating sets with respect to the graph skeleton</p><formula xml:id="formula_122">G Step 2 (orientation of v-structures in G) Step 3 (propagation of orientations in G) for all removed edges (X, Y ) in G do Sepset(X, Y | G) ← Sepset(X, Y | G k ) if Sepset(X, Y | G) � Consist(X, Y | G) and Sepset(X, Y | G) � Consist(Y, X | G) then Add undirected edge (X, Y ) to G end if</formula><p>end for return G and consistent separating sets Theorem 4. The separating sets returned by algorithms 3 and 4 are consistent with respect to the final graph G. Proof. Firstly, the limit cycles in algorithms 3 and 4 are warranted to be finite by the deterministic nature of these algorithms and the finite set of graphs G j .</p><p>In algorithm 3, as the union of graphs � (G j ) k j=k-n does not remove any edge from the last graph G k and discards all conflicting orientations with previous graphs G j , j ∈ { kn, k -1 }, taking the union of graphs does not create any new conditional independence relation, nor any inconsistency regarding the final separating sets. More precisely, all removed edges in G k have separating sets consistent with respect to at least one graph in the union (G k-1 ), which is thus also consistent with respect to the union of graphs G.</p><p>In algorithm 4, the consistency of separating sets is guaranteed by similar arguments, but only with respect to the skeleton. As the orientation and propagation steps 2&amp;3 might induce additional type II inconsistencies, algorithm 4 requires a final consistency check for all separating sets. Adding back edges with inconsistent separating sets in the final graph G then guarantees that all the separating sets are consistent with respect to definition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Tests of Consistency</head><p>A unitary operation of algorithms 3 and 4 is to test, for a vertex</p><formula xml:id="formula_123">Z ∈ adj(X) \ { Y } in G, if Z ∈ Consist(X, Y | G)</formula><p>, which requires that 1) at least one path from X to Y passing through Z (i.e. γ Z XY ) exists in G and 2) Z is not a child of X in G (definition 1). To test the first condition, it is conceptually simple to first get all paths between X and Y , then check if Z lies in at least one of them, This is however unfeasible as the complexity of getting all paths between two vertices can be large, depending on the edge density of the graph. Fortunately, it is possible to get directly the set of all Z for which at least on path γ Z XY exists. This can be done very efficiently with the help of biconnected component analysis based on block-cut tree decomposition, as detailed in Supplementary Material.</p><p>The second condition assumes the absence of latent variables, which allows for condition independence tests on adjacent nodes only in algorithm 2. It is thus straightforward to test without additional complexity burden.</p><p>Hence, the overall complexity of the consistency tests of separating sets relies on the block-cut tree decomposition, which can be done beforehand within a single depth first search with complexity O(|V | + |E|). Thus for each pair (X, Y ), the complexity of finding all candidate Z depends on the size of the block-cut tree, which is in the worst case (when the underlying skeleton is a forest) linear in the size of the graph, O(|V | + |E|), see Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Empirical Evaluation</head><p>We conducted a series of benchmark structure learning simulations to study the differences between the original PC-stable algorithm and the proposed modifications ensuring consistent separating sets.</p><p>For each simulation setting, we first quantified the fraction of inconsistent separating sets predicted by the original PC-stable algorithm, Figure <ref type="figure" target="#fig_24">3</ref>. We then compared the performance of the original PC-stable (algorithm 1 and algorithm 2), orientation-consistent PC-stable (algorithm 3) and skeletonconsistent PC-stable (algorithm 4), for different significance levels α, in terms of the precision and recall of the adjacencies found in the inferred graph with respect to the true skeleton, Figures <ref type="figure" target="#fig_66">4</ref> and<ref type="figure" target="#fig_57">5</ref>. Figure <ref type="figure" target="#fig_12">4</ref> highlights situations for which the original PC manages to recover a DAG that is already closely related to the ground truth but produces inconsistent separating sets, as shown in Figure <ref type="figure" target="#fig_24">3</ref>. By constrast, Figure <ref type="figure" target="#fig_57">5</ref> highlights standard benchmarks from the BNlearn repository <ref type="bibr" target="#b59">(Scutari, 2010)</ref> for which the original PC show a poor Recall due to too many spurious conditional independences, and ultimately outputs a graph with only a few obvious edges. Finally, we also measured the fraction of the separating sets used for discarding edges by the three approaches that correspond to true D-separation in the ground-truth DAG, Figure <ref type="figure" target="#fig_38">6</ref>.  Chapter 4. Other improvements to constraint-based algorithms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Data generation and benchmarks</head><p>The data-sets used for the numerical experiments were generated with the following scheme. The underlying DAGs were generated with TETRAD <ref type="bibr" target="#b58">(Scheines et al., 1998)</ref> as scale-free DAGs with 50 nodes (α = 0.05, β = 0.4, average total degree d(G) = 1.6) using a preferential attachment model and orienting its edges based on a random topological ordering of the vertices. Data-sets were simulated with linear structural equation models for three settings : strong, medium and weak interactions (with respective coefficient ranges [0.2, 0.7], [0.1, 0.5], and [0, 0.3] and covariance ranges [0.5, 1.5], [0.5, 1], and [0.2, 0.7]). In addition, we also generated data-sets for the classical benchmarks Insurance (27 nodes, 52 links, 984 paramaters), Hepar2 (70 nodes, 123 links, 1453 paramaters) and Barley (48 nodes, 84 links, 114005 paramaters) networks from the Bayesian Network repository <ref type="bibr" target="#b59">(Scutari, 2010)</ref>.</p><p>Reconstruction benchmarks were performed with pcalg's (Kalisch et al., 2012) PC-stable implementation, modified for enforcing separating set consistency either taking into account orientations (algorithm 3) or at the level of the skeleton (algorithm 4). The (conditional) independence test used in all simulations is a linear (partial) correlation with Fisher's z-transformation. Performances are obtained with relation to the true skeleton by measuring the Precision (positive predictive value), P rec = T P/(T P + F P ) and Recall or Sensitivity (true positive rate), Rec = T P/(T P + F N ) where T P is a correctly predicted adjacency, F P an incorrectly predicted adjacency and F N an incorrectly discarded adjacency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Benchmark Results</head><p>The fraction of inconsistent separating sets that were used to remove edges was first estimated for increasing sample size and varying parent-child interaction strength, using the original PC-stable algorithm for random and scale-free DAGs of 50 nodes, Figure <ref type="figure" target="#fig_24">3</ref>. We note that in typical settings, a significant fraction of the separating sets that were used to remove edges during Step 1 of the PC-stable algorithm cannot be "read off" the returned graph, either because there is no path containing Z that connects X and Y (skeleton inconsistency, green in Figure <ref type="figure" target="#fig_24">3</ref>) or because there is a conditioning on an invalid child node (orientation inconsistency, i.e., difference between blue and green inconsistencies in Figure <ref type="figure" target="#fig_24">3</ref>). Both increasing the sample size and increasing the interaction strength reduces the number of inconsistent sepsets. We attribute this in part to the severity of the PC-stable algorithm which tends to remove to many false negative edges because of spurious inconsistencies. With a larger sample size N and stronger interactions, consistent separating sets are still not guaranteed by the original algorithm but these settings decrease the number of spurious independencies and leads to denser reconstructed graphs, thus making it more likely for potential separating sets to be consistent. Orientation consistency is particularly difficult to obtain with respect to the returned CPDAG, as orientation and propagation steps generally suffer even more from sampling noise and previous mistakes than the skeleton reconstruction (Step 1). Notably, the orientation depends on the order in which separating sets are tested in PC-stable (in pcalg it depends on the ordering of the variables in the data-set).  In each subplot the fraction of inconsistent separating sets with respect to the skeleton (green) or CPDAG (blue) obtained with the original PC-stable algorithm with a fixed α = 0.05 is displayed for increasing sample size N . Data-sets were generated from 100 scale-free graphs of 50 nodes and d(G) = 1.6 with different parent-child interaction strengths : strong (left), medium (middle) and weak (right).</p><p>We then compared the performance of the original PC-stable (algorithm 1 and algorithm 2), orientation-consistent PC-stable (algorithm 3) and skeleton-consistent PC-stable (algorithm 4), for different significance levels α, in terms of the precision and recall of the adjacencies found in the inferred graph with respect to the true skeleton, Figures <ref type="figure" target="#fig_66">4,</ref><ref type="figure" target="#fig_57">5</ref> and S1. Enforcing the sepset consistency is shown to significantly improve the sensitivity of constraint-based methods, for a given α, while achieving equivalent or better overall structure learning performance.</p><p>It is particularly the case for standard benchmark networks from the BNlearn repository (Scutari, 2010), Figure <ref type="figure" target="#fig_57">5</ref>, for which the original PC-stable algorithm shows good precision but poor recall (Rec&lt;0.15-0.35 and Prec&gt;0.65 at maximum Fscore, see iso-Fscore dotted lines in Figure <ref type="figure" target="#fig_57">5</ref>), while consistent PC-stable achieves a better balance between precision and recall (Rec�0.5 and Prec�0.5-0.6 at maximum Fscore, Figure <ref type="figure" target="#fig_57">5</ref>). Finally, we also compared the fraction of valid separating sets used for discarding edges, which entail true d-separation in the ground-truth DAG, Figures <ref type="figure" target="#fig_44">6</ref> and<ref type="figure" target="#fig_61">S2</ref>. Ensuring the consistency of separating sets tends to increase, although not guarantee, their validity in terms of actual d-separation. Consistent sepsets with invalid d-separation are primarily caused by edge mis-orientations rather than skeleton errors. In particular, skeleton-consistent separating sets yield better performance in terms of valid d-separation than orientation-consistent separating sets with the setting of the PC-stable algorithm used here. This is, however, expected to depend on the specific settings for conditional independence test, orientation and propagation rules, used in different constraint-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>In this paper, we propose and implement simple modifications of the PC algorithm also applicable to any PC-derived constraint-based methods, in order to enforce the consistency of the separating sets  proportion of sepsets that correspond to a valid d-separation in the true DAG that were used for removing edges during Step 1 of original, orientation-consistent and skeleton-consistent PC-stable algorithms for all tested α. Bottom row shows the average proportion of valid d-separation for a given average recall over all tested values of α. Data-sets with N =500 were generated from 100 DAGs with linear SEMs with strong (left), medium (middle) and weak (right) interaction (see Figure <ref type="figure" target="#fig_61">S2</ref> for N =100, 1000).</p><p>of discarded edges with respect to the final graph, which is an actual shortcoming of constraint-based approaches, Figure <ref type="figure" target="#fig_24">3</ref>. Enforcing sepset consistency is shown to significantly improve the sensitivity of constraint-based methods, while achieving equivalent or better overall structure learning performance, Figures <ref type="figure" target="#fig_66">4,</ref><ref type="figure" target="#fig_57">5</ref> and S1. In addition, ensuring the consistency of separating sets increases also their validity in terms of actual d-separation, Figures <ref type="figure" target="#fig_44">6</ref> and<ref type="figure" target="#fig_61">S2</ref>.</p><p>The existence of sepset inconsistencies with constraint-based methods originates from their tendency to uncover spurious conditional independences early on in the pruning process when the combinatorial space of possible separating sets is still large, unlike in the final typically sparse skeleton. Such spurious conditional independences are responsible, in particular, for the large number of false negative edges and, therefore, frequently poor sensitivity of constraint-based methods <ref type="bibr">(Colombo and Maathuis, 2014)</ref>. By contrast, enforcing sepset consistency enables to achieve a better balance between sensitivity and precision.</p><p>To circumvent this inconsistency issue during the skeleton step, we have shown that one can either use sepset consistency taking into account orientations to help reject inconsistent sepsets (algorithm 3) or use sepset consistency of the skeleton to help determine the orientations (algorithm 4). The later approach tends to yield slightly better performance with the setting of the PC-stable algorithm used here but this is expected to be dependent on the specific settings used, for conditional independence test, orientation and propagation rules, in different constraint-based methods. Indeed, the methods and algorithmic implementations presented here are not primarily meant to outcompete a specific PC or PC-derived algorithm but rather to improve the explainability of constraintbased methods, by ensuring the consistency of all separating sets in the final causal graphs.</p><p>The approach is very general and applicable to the large variety of constraint-based methods, starting with a complete graph and discarding dispensable edges iteratively based on conditional independence search. Beyond the formal interest of guaranteeing sepset consistency, this is also especially important, in practice, for the interpretability of constraint-based models for real-life applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reliable orientations with mutual information supremum</head><p>Finally, we propose the equivalent of conservative PC for MIIC, which improves the reliability of inferred orientations at only a small cost to sensitivity. It does not rely on an orientation cut β as introduced in Section 4.1.2, but rather on a formulation of information supremum, for both the continuous and the discrete case.</p><p>The publication accepted at the Why21 workshop contextualizes MIIC, proposes the simple change to the orientation rules enforcing non-negative regularized information terms, and compares the performance between the old and the new rules on simulated networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Publication at Why21 workshop, NeurIPS 2021</head><p>Reliable causal discovery based on mutual information supremum principle for finite datasets Anonymous Author(s) Affiliation Address email</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The recent method, MIIC (Multivariate Information-based Inductive Causation), combining constraint-based and information-theoretic frameworks, has been shown to significantly improve causal discovery from purely observational data. Yet, a substantial loss in precision has remained between skeleton and oriented graph predictions for small datasets. Here, we propose and implement a simple modification, named conservative MIIC, based on a general mutual information supremum principle regularized for finite datasets. In practice, conservative MIIC rectifies the negative values of regularized (conditional) mutual information used by MIIC to identify (conditional) independence between discrete, continuous or mixed-type variables. This modification is shown to greatly enhance the reliability of predicted orientations, for all sample sizes, with only a small sensitivity loss compared to MIIC original orientation rules. Conservative MIIC is especially interesting to improve the reliability of causal discovery for real-life observational data applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>Constraint-based structure learning methods can, in principle, discover causal relations in purely observational data <ref type="bibr">(Pearl, 2009;</ref><ref type="bibr">Spirtes, Glymour, and Scheines, 2000)</ref>. This is theoretically feasible up to some independence equivalence classes, as the orientations of certain edges may only be uncovered through perturbative data and remain undetermined if only observational data is available.</p><p>Yet, regardless of this theoretical limitation, it has long been recognized (Ramsey, Spirtes, and Zhang, 2006; Colombo and Maathuis, 2014) that orientations predicted by constraint-based methods are often unreliable, which has largely limited, in practice, the application of constraint-based methods to uncover causal relations in real-life observational data.</p><p>This causal uncertainty originates from the extensive number of steps and conditions that constraintbased methods, such as the original IC <ref type="bibr">(Pearl and Verma, 1991)</ref> and PC <ref type="bibr">(Spirtes and Glymour, 1991)</ref> algorithms, have to meet before they can infer edge orientation. Indeed, they must first learn an undirected skeleton, by uncovering (conditional) independences between all pairs of variables, before inferring the orientation of v-structures and finally propagating these orientations to other undirected edges. This long chain of uncertain computational decisions leads to the accumulation of errors which ultimately limit the accuracy of the final orientation and propagation steps of constraint-based methods. As a result, edge orientations significantly reduce the precision (or positive predicted value) of inferred causal graphs compared to their undirected skeleton. In addition, constraint-based methods are known to suffer from much lower sensitivity or recall (i.e., true positive rate) than precision scores, in general <ref type="bibr">(Colombo and Maathuis, 2014;</ref><ref type="bibr" target="#b79">Li et al., 2019)</ref>. This is related to the fact that separating sets used to remove edges in the (early) steps of constraint-based methods are frequently not consistent with the final skeleton and oriented graphs <ref type="bibr" target="#b79">(Li et al., 2019)</ref>. They correspond to Submitted to WHY-21 workshop @ NeurIPS 2021. Do not distribute. spurious conditional independences responsible for the large number of false negative edges and, therefore, low sensitivity of constraint-based methods.</p><p>While successive refinements of orientation rules, such as conservative rules (Ramsey, Spirtes, and <ref type="bibr" target="#b83">Zhang, 2006</ref>) and majority rules <ref type="bibr">(Colombo and Maathuis, 2014)</ref>, have helped improve the average precision of orientations, they also lead to large precision variance and further aggravate the poor recall of edge orientations at small sample sizes. This is illustrated here for both discrete (Fig. <ref type="figure" target="#fig_20">1</ref>) and continuous (Fig. <ref type="figure" target="#fig_0">2</ref>) benchmark datasets generated by random Bayesian networks using the available codes from <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>, see section on Data generation and benchmarks, below.</p><p>The recently developed method, MIIC, combining constraint-based and maximum likelihood frameworks, has been shown to significantly improve the situation by greatly reducing the imbalance between precision and recall, for all sample sizes <ref type="bibr">(Verny et al., 2017;</ref><ref type="bibr" target="#b70">Cabeli et al., 2020)</ref>. Compared to traditional constraint-based methods, MIIC also significantly reduces the precision gap between skeleton and oriented graphs for large enough datasets, as discussed below. However, a substantial loss in precision remains between skeleton and oriented graphs for smaller datasets. Benchmark datasets are generated from random 100-node DAGs with average degree 2.7 and maximum degree 4 (See Data generation and benchmarks section for details). PC structure learning performance is measured in terms of Precision, Recall and F-scores (±σ) for skeleton (blue), CPDAG (red) and oriented-edge-only subgraph (green).</p><p>In this paper, we propose and implement a simple modification of MIIC algorithm, which is found to greatly improve the precision of predicted orientations even for relatively small datasets. It is achieved at the expense of a small loss of orientation recall but significantly enhances the reliability of predicted orientations for all sample sizes. This simple modification, referred to as conservative MIIC, is especially interesting, in practice, to improve the reliability of causal discovery for real-life observational data applications. </p><formula xml:id="formula_124">I(X; Y |{A i } n ) = I(X; Y ) -I(X; Y ; A 1 ) -I(X; Y ; A 2 |A 1 ) -• • • -I(X; Y ; A n |{A i } n-1 )<label>(1)</label></formula><p>In practice, (conditional) independence is established by comparing mutual information (MI) or conditional mutual information (CMI) to a universal Normalized Maximum Likelihood (NML) complexity term, k NML N (X; Y |{A i })/N , computed over all datasets of the same size N and marginal distributions p(X, {A i }) and p(Y, {A i }) <ref type="bibr" target="#b13">(Affeldt and Isambert, 2015)</ref>. This can be seen as a NMLregularization of MI and CMI for datasets of finite sample size N as,</p><formula xml:id="formula_125">I N (X; Y |{A i }) = I N (X; Y |{A i }) - 1 N k NML N (X; Y |{A i })<label>(2)</label></formula><p>where k NML </p><formula xml:id="formula_126">I N (X; Y ; Z|{A i }) = I N (X; Y |{A i }) -I N (X; Y |{A i }, Z)<label>(3)</label></formula><p>In particular, negative NML-regularized conditional 3-point information terms, I N (X; Y ; Z|{A i }) &lt; 0, ) dynamic programming algorithm <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>. This approach finds optimum partitions, P and Q, specifying the number and positions of cut-points of each continuous variable, X and Y , to maximize the NML-regularized MI between them,</p><formula xml:id="formula_127">I N (X; Y ) = sup P,Q I N ([X] P ; [Y ] Q )<label>(4)</label></formula><p>The NML regularization term, introduced in I N ([X] P ; [Y ] Q ), is necessary for finite datasets and amounts to a model complexity cost, which eventually out-weights the information gain in refining bin partitions further, when there is not enough data to support such a refined model <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>.</p><p>Such optimization-based estimates of MI are at par with alternative distance-based kNN approaches but have also the unique advantage of providing an effective independence test to identify independent continuous or mixed-type variables <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>. This is achieved when partitioning X and Y into single bins maximizes the NML-regularized MI in Eq. 4, which vanishes exactly, in this case, with dramatic reductions in sampling error and variance <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>. By contrast, kNN-MI estimates still need an actual independence test to decide whether some variables are effectively independent or not, as kNN MI estimates are never exactly null.</p><p>MIIC Precision, Recall and F-score on continuous data are comparable to those on discrete data, ). As a result, the approximate NML-regularized CMI estimates between conditionally independent variables can sometime be negative and lead to v-structure orientations contradicting conditional independence, as discussed for discrete data above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Improving MIIC causal discovery by rectifying negative NML-regularized MI &amp; CMI</head><p>The general MI supremum principle <ref type="bibr">(Cover and Thomas, 2006)</ref>, regularized in Eq. 4 for finite datasets, is theoretically valid for any type of variables, not just continuous variables. In particular, it could be applied to small size datasets with discrete or categorical variables with many levels. It would result in the merging of rare levels to better estimate MI and CMI between weakly dependent discrete variables. Ultimately, MI estimates between independent discrete variables should lead to the merging of each variable into a single bin, thereby, resulting in NML-regulatized MI estimates to vanish exactly in this case, as already observed for continuous variables <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>. As a result, optimum NML-regulatized MI should be non-negative as well as, by extension, NML-regulatized CMI, as shown now. The effects on this modification on discrete and continuous benchmark data are show in Fig. <ref type="figure" target="#fig_12">4</ref>.</p><p>While conservative MIIC hardly affects skeleton scores, it clearly has an impact on CPDAG and oriented-edge-only subgraph scores, which exhibit different trends relative to their original MIIC values.</p><p>CPDAG Precision, Recall and, hence, F-scores appear to be slightly lower under conservative MIIC (Fig. <ref type="figure" target="#fig_12">4</ref>) than with original MIIC (Fig. <ref type="figure" target="#fig_24">3</ref>), for discrete data. This illustrates the overall "better" orientation/non-orientation scores of the original MIIC against the theoretical CPDAG objective.</p><p>Indeed, allowing for negative NML-regularized MI enables to infer weakly supported v-structures at small sample sizes. Besides, no significant difference is observed for CPDAG scores on continuous data, as original MIIC already enforces non-negative NML-regularized MI through optimization for continuous data <ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>, suggesting that enforcing also non-negative NML-regularized CMI with conservative MIIC has little impact on the reliability of CPDAG scores for continuous data, at least for the benchmarks tested here.</p><p>By contrast, conservative MIIC is found to greatly improve the precision of oriented-edge-only subgraphs, on discrete datasets, even for relatively small sample sizes, Fig. <ref type="figure" target="#fig_12">4</ref>. This large increase in orientation precision is achieved at the expense of a relatively small loss of orientation recall. Hence, conservative MIIC significantly enhances the reliability and sensitivity of predicted orientations for all sample sizes, as compared to traditional constraint-based methods with conservative orientation rules, Fig. <ref type="figure" target="#fig_12">4</ref>. For instance, conservative MIIC already reaches nearly 90% orientation precision with 25% orientation recall for N 250 (against about 80% orientation precision with only 5% orientation recall for conservative PC). While, by the time conservative PC reaches 90% orientation precision with 25% orientation recall for N 700, conservative MIIC achieves nearly 100% orientation precision with 50% orientation recall, Fig. <ref type="figure" target="#fig_12">4</ref>. In addition, while original MIIC achieves a significantly better 65% orientation recall for N 700, Fig. <ref type="figure" target="#fig_24">3</ref>, its orientation precision simultaneously drops to about 75%, which clearly impacts its reliability for causal discovery.</p><p>On continuous data, conservative MIIC also achieves a large increase in orientation precision, which becomes at par with skeleton precision, even for small datasets, and clearly much better than the corresponding scores obtained with traditional constraint-based methods for large datasets, Fig. <ref type="figure" target="#fig_12">4</ref>.</p><p>For instance, conservative MIIC reaches nearly 75% orientation precision with 50% orientation recall for N 200 (against about 70% orientation precision with 35% orientation recall for conservative PC). While, by the time conservative PC reaches 75% orientation precision with 45% orientation recall for N 1, 000, conservative MIIC achieves more than 90% orientation precision with 80% orientation recall, Fig. <ref type="figure" target="#fig_12">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data generation and benchmarks</head><p>Datasets were simulated using structural equations models (SEMs) following the causal order of randomly generated DAGs. Continuous examples were constructed using linear and non-linear functions, and discrete datasets using unique state probabilities for each of the parents' combinations.</p><p>The DAGs themselves were randomly drawn from the space of all possible 100 node DAGs <ref type="bibr" target="#b80">(Melancon and Philippe, 2004</ref>) allowing for a maximum degree of 4 neighbors, resulting in an average degree of Then, in order to evaluate edge orientations, we also define two orientation-dependent measures.</p><p>The first measure, referred to as the "CPDAG" score, aims to score the overall reconstruction with regards to the equivalence class of the true DAG. Edge types are used to redefine the orientationdependent counts as, T P = T P -T P misorient and F P = F P + T P misorient with T P misorient corresponding to all true positive edges of the skeleton with a different orientation/non-orientation status as in the true CPDAG. The CPDAG precision, recall and F-score were then computed with the orientation-dependent T P and F P . In particular, the CPDAG score equivalently rates as "false positive" the erroneous orientation of an non-oriented edge in the CPDAG and the erroneous nonorientation of an oriented edge in the CPDAG. However, these errors are not equivalent from a causal discovery perspective.</p><p>The second measure, referred to as oriented-edge-only score, uses the same metrics but is restricted to the subgraphs of the CPDAG and the inferred graph containing oriented edges only. It is designed to specifically assess the method performance with regards to causal discovery, that is, on the oriented edges which can in principle be learnt from observational data versus those effectively predicted by the causal structure learning method.</p><p>MIIC was run with default parameters for all settings on the latest version (available at https:// github.com/miicTeam/miic_R_package), and PC with the pcalg package (Kalisch et al., 2012) using bnlearn's (Scutari, 2010) mutual information test for discrete datasets and rank correlation for continuous ones. For PC, the α threshold for significance testing was tuned for each sample size N and network type to produce the best average between skeleton and "CPDAG" F-scores using a zeroth order optimization implemented in dlib <ref type="bibr">(King, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Causal uncertainty and limited sensitivity of traditional constraint-based methods have so far hampered their dissemination for a wide range of possible causal discovery applications on real-life observational datasets. Hence, fulfilling the promise of causal discovery methods in the new data analysis area requires to improve their reliability as well as scalability.</p><p>We propose and implement, in this paper, a simple modification of the recent causal discovery method, MIIC, which greatly enhances the reliability of predicted orientations, for all sample sizes, with only a small sensitivity loss compared to MIIC original orientation rules. This conservative MIIC approach is especially interesting, in practice, to improve the reliability of cause-effect discovery for real-life observational data applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 5 Applications</head><p>In this chapter, we take advantage of the various improvements to MIIC and infer causal graphs from real-life data. In each case, we pre-processed the data in close collaboration with the teams that were responsible for their collection, and benefited from their insight again when analyzing the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning causal graphs from medical records of patients with cognitive disorders</head><p>The first application of MIIC with the mixed case estimator was on medical records of elderly patients from La Pitié-Salpêtrière Hospital. In the final network, 107 variables of 1,628 patients admitted for cognitive disorder were used for the network reconstruction.</p><p>Graphical model reconstruction methods are not commonly used on clinical databases; however, these techniques can be of great help in understanding the structure of the data. Two main benefits emerged from this application on medical records. First, it helps in understanding the relationship between variables : knowing which aspects of cognition are correlated is fairly intuitive for experienced physicians, but it is much harder to conceptualize mediating effects and discerning direct from indirect relations. Analyzing this kind of data also performs quality control in a way : if a link is missing, we can easily understand why by analyzing the detailed information of the network reconstruction, but also by directly observing the dataset. Secondly, it is able to infer previously ignored associations, which could also be a considerable advantage in the initial analysis of this type of varied and complex data set. In this network for example, an unexpected direct edge was inferred between the Fazekas (which measures the amount of hyperintensity in white matter attributed to chronic small vessel ischemia) and Scheltens (medial temporal lobe atrophy) scales, which was indepedently reported recently <ref type="bibr" target="#b281">[123]</ref>.</p><p>The full discussion of the network is published in <ref type="bibr">[7]</ref>, which is included in Section 3.3. 120 Chapter 5. Applications</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NEOREP study on breast cancer patients</head><p>The second application, also on medical records of patients, concerns breast cancer patients that have undergone neoadjuvant (i.e. before the main treatment, usually surgery) chemotherapy.</p><p>In this article, we emphasize the benefits of visually inspecting the dataset after having removed the indirect relationships with MIIC. We note at least three ways in which we benefit from the network approach. First, as it was highlighted in the previous application, it performs an unbiased check of the dataset. For example, it was able to identify differences in clinical practices between the two treatment centers of the cohort which may create instances of Simpson's paradox if not accounted for. By including the node in the dataset, it is a potential contributing node to any relationship and we make sure we remove the effect of the center for each remaining edge. Secondly, MIIC traces the natural course of the disease by having access to patients that are both early in the diagnosis or long after initial treatment. This allows to predict statistically the course of the disease, based on this specific population of patients. Finally, MIIC identifies factors likely to improve prediction or prognosis. Specifically, the direct neighbors of the node reporting the vital status of the patient give some unique information about the outcome that could not be removed due to indirect effects, and merit particular attention. Moreover, the optimal discretization scheme informs us of the most informative cutpoints with regards to the outcome of the disease for the observed population. In this application specifically, we observe that the discretized version of a composite score, which is usually used for prognosis, is less informative of the vital status than its continuous variable. This highlights the fact that many bio-medical discrete variables are discretized using a priori bins and may not be adapted for all situations.</p><p>The manuscript was written in collaboration with the Residual Tumor and Response to Treatment Laboratory and is yet to be submitted, it is included in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Manuscript</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The availability of health data from patient medical records is increasing, and these data constitute, in theory, a rich resource for research purposes. However, despite the unprecedented amount of information now available, health data remain underexploited due to their heterogeneity and complexity. There is, therefore, an urgent need for innovative tools, based on intuitive and interactive graphical interfaces, specifically designed for the exploration of health data by medical practitioners.</p><p>Data visualization is gradually emerging as a new field of research, and graphical representations are used for two main purposes: (i) explanatory illustration, to highlight novel scientific insights graphically and to ensure efficient communication between scientists 1-4 ; and (ii) exploratory analysis, searching for relationships previously overlooked and leading to new discoveries, thereby maximizing the potential of information-rich databases. We present here an exploratory analysis of a global clinical network from a large breast cancer cohort, with a novel interactive graphical interface for the exploration of health data.</p><p>We previously developed an advanced computational method for graphical analyses, including causal relationships, from multivariate data 5 . The underlying MIIC (multivariate information inductive causation) algorithm, which was released as an online server 6 , uses a machine learning method combining constraint-based and information theory approaches to reconstruct causal, non-causal or mixed networks from large datasets. The MIIC algorithm was first developed to analyze categorical genomic data 5, 6 and has recently been extended to the analysis of more challenging heterogeneous datasets, such as medical records, combining both categorical and continuous variables, in which interdependence is notoriously difficult to assess 7 .</p><p>Breast cancer (BC) clinical datasets are particularly suitable for the type of exploratory analysis presented here, as BC is a complex heterogeneous disease highly variable in its aggressiveness and prognosis. BC remains one of the leading causes of cancer-related death among women. The BC patients included in the cohort analyzed here were treated with neoadjuvant (or</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients 123</head><p>preoperative) chemotherapy (NAC). NAC was originally restricted to patients with inflammatory or locally advanced BC, but is now the standard care for aggressive early-stage breast cancers, i.e. triplenegative (TNBC) and HER2-positive BCs 8, 9 . From the patient's viewpoint, the benefits of the neoadjuvant strategy include a greater feasibility of breast-conserving surgery and the prognostic stratification of risk obtained after analyses of the residual tumor burden at surgery. From the research and development standpoint, the neoadjuvant setting makes it possible to monitor the chemosensitivity of the tumor in vivo, and provides an opportunity for the rapid validation of research hypotheses and the acceleration of drug approval.</p><p>The novel interactive graphical interface described here, designed as a front-end for the MIIC server, should enable clinicians to visualize and understand the relationships between clinically relevant variables, such as post-NAC clinical responses and survival. In addition, the MIIC interactive graphical interface should help practitioners to identify actionable nodes and edges in clinical networks, potentially leading to improvements in the patient care pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patients and treatment</head><p>We analyzed a cohort of 1197 patients with non-metastatic BC treated by NAC, with or </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC algorithm</head><p>The functioning of the algorithm has been described in detail elsewhere 5, 7 . Briefly, starting from a fully connected network, the MIIC algorithm first removes dispensable edges by iteratively subtracting the most significant information contributions from indirect paths between each pair of variables. The remaining edges, the underlying effect of which cannot be explained by indirect paths, are then oriented based on the causality signature in the data.</p><p>The original algorithm was restricted to categorical variables 5 , but MIIC has recently been extended to include continuous variables, the values of which are partitioned into optimal bins, maximizing mutual information with another (continuous or categorical) variable of interest, while preventing the overfitting of datasets of finite size due to the use of too many bins 7 . In particular, each continuous variable may have different information-maximizing partitions depending on the associated variable of interest. For instance, MIIC finds three maximally informative bins for the residual cancer burden (RCB) score in association with patient survival status (Fig. <ref type="figure" target="#fig_60">S1A</ref>), whereas 126 Chapter 5. Applications eight RCB bins are required to estimate its mutual information with post-NAC cellularity correctly (Fig. <ref type="figure" target="#fig_60">S1B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC online server</head><p>The MIIC online server is freely accessible at <ref type="url" target="https://miic.curie.fr/index.php">https://miic.curie.fr/index.php</ref>, and can be used with the Google Chrome, Mozilla Firefox, Edge, and Safari browsers. in the dataset table. Once the dataset has been prepared, the user runs the algorithm, and an e-mail is sent when the job is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC output</head><p>The MIIC online server generates a visualization of the global network of the dataset. An example based on the NEOREP dataset is displayed in Fig. <ref type="figure" target="#fig_20">1</ref>, and is accessible as an interactive network at <ref type="url" target="https://miic.curie.fr/job_results_showcase.php?id=NEOREP">https://miic.curie.fr/job_results_showcase.php?id=NEOREP</ref>.</p><p>Each node corresponds to a variable of the dataset, with continuous variables displayed as circles, and categorical variables displayed as squares. The color of the node indicates the group to which the variable belongs, as specified by the user.</p><p>Each edge corresponds to a "direct" association between two variables, that is, a statistical association that cannot be entirely explained by indirect effects involving other variables. Red and blue edges correspond to positive and negative (i.e. anti-correlated) associations, respectively. Four types of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive exploration of the network</head><p>The distributions and neighborhoods of each node and edge of the inferred network can be explored through an interactive interface, through the mouse-over right-or left-click buttons on the browser page, as detailed in the online tutorials. Briefly, any variable can be highlighted by clicking on the network or through the "Search" toolbox (Fig. <ref type="figure" target="#fig_61">S2A</ref>). The corresponding plots can be downloaded as Fig. <ref type="figure" target="#fig_61">S2B-C</ref>) or distribution (continuous variables Fig. <ref type="figure" target="#fig_61">S2D-E</ref>). Each edge can be explored by a right click and the choice of "plot join distribution" or "plot discretization". The resulting plots are (i)</p><p>proportion plots, with the edge representing the association between two categorical variables (Fig. <ref type="figure" target="#fig_61">S2F</ref>); (ii) distribution histograms (Fig. <ref type="figure" target="#fig_61">S2G</ref>) or boxplots (Fig. <ref type="figure" target="#fig_61">S2H</ref>), in which the edge represents the association between a categorical and a continuous variable or (iii) scatter plots (Fig. <ref type="figure" target="#fig_61">S2I</ref>), in which the edge represents the association between two continuous variables. All the plots can be customized by zooming in and out. Additional options include inverting the x and y axes, the choice of frequency or absolute counts, or NA removal (proportion plots), and faceting or superimposing the variables (distribution histograms). All the figures presented here were generated with the MIIC online interactive visualization tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>The global network displayed in Fig. <ref type="figure" target="#fig_20">1</ref> is accessible at <ref type="url" target="https://miic.curie.fr/job_results_showcase.php?id=NEOREP">https://miic.curie.fr/job_results_showcase.php?id=NEOREP</ref>.</p><note type="other">128 Chapter 5. Applications 10</note><p>MIIC then identifies clinical factors known to be epidemiologically related (Fig. <ref type="figure" target="#fig_66">S4A</ref>).</p><p>Menopause, a process occurring in older women, is directly linked to age (Fig. <ref type="figure" target="#fig_66">S4B</ref>) (median age: 43 years for premenopausal, versus 58 years for postmenopausal women). Postmenopausal status is associated with dyslipidemia (Fig. <ref type="figure" target="#fig_66">S4C</ref>) 20 . Consistent with these associations, body mass index (BMI) increases with age (Fig. <ref type="figure" target="#fig_66">S4A</ref>, S4D) and both factors, which have been reported to increase cardiovascular risks, are linked to hypertension (Fig. <ref type="figure" target="#fig_66">S4A,</ref><ref type="figure" target="#fig_66">S4E</ref>). The number of drugs taken by a patient (comedication) increases with the number of comorbidities (Fig. <ref type="figure" target="#fig_66">S4A,</ref><ref type="figure" target="#fig_66">S4F</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC identifies intra-and inter-modality associations</head><p>For the variables derived from pathology records, MIIC found associations between tumor grade, Ki67, and mitotic index (Fig. <ref type="figure" target="#fig_67">S5A-B-C</ref>), all of which are markers of tumor proliferation 21 .</p><p>MIIC can also visualize links between patterns assessed in different ways. Measurements of pre-NAC tumor size evaluated clinically, by mammography and by MRI, were found to be closely related (Fig. <ref type="figure" target="#fig_67">S5C-E</ref>) as previously reported 22, 23 . Similarly, the response to treatment assessed clinically at NAC completion was found to be associated with histological size based on the surgical specimen (Fig. <ref type="figure" target="#fig_67">S5F</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC provides insight into tumor biology and response to treatment</head><p>The presence of lymphovascular invasion (LVI) in the post-NAC specimen is associated with a higher RCB index, consistent with the strong resistance to chemotherapy of these tumors 12 (Fig. <ref type="figure" target="#fig_44">S6A</ref>). TNBCs and HER2-positive tumors have a higher pre-NAC mitotic index and more stromal TIL infiltration (Fig. <ref type="figure" target="#fig_44">S6B-C</ref>) than luminal BCs 24, 25 . Consistently, high TIL levels are significantly associated with histological grade 3 tumors (Fig. <ref type="figure" target="#fig_44">S6D</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC reflects clinical practice</head><note type="other">130 Chapter 5. Applications</note><p>Several associations highlighted in the network reflect clinical practice decisions applied throughout BC centers. For example, the likelihood of performing conservative breast surgery depends on tumor histology (higher rates of mastectomy have been reported for patients with lobular or other histological types of tumor less likely to respond to NAC) 26, 27 (Fig. <ref type="figure" target="#fig_68">S7A</ref>) and is positively associated with the practice of oncoplastic surgery 28 (Fig. <ref type="figure" target="#fig_68">S7B</ref>). Similarly, lumpectomy is more frequently associated with radiation therapy than with mastectomy (Fig. <ref type="figure" target="#fig_68">S7C</ref>) 29-32 . After surgery, the addition of a second line of treatment by adjuvant chemotherapy, to decrease the risk of relapse, is driven by the identification of factors associated with a poor prognosis 33 , such as high levels of lymph node involvement (Fig. <ref type="figure" target="#fig_68">S7D</ref>).</p><p>Beyond these well-established practices, MIIC also identified differences in clinical practices between the two centers of the cohort (Fig. <ref type="figure" target="#fig_65">3A</ref>). For example, oncoplastic surgery and adjuvant chemotherapy were performed at only one of the two centers (Fig. <ref type="figure" target="#fig_65">3B-C</ref>); the NAC regimen also differed between centers, with the Curie St Cloud center using more AC regimens than AC-taxane combinations, resulting in a shorter duration of NAC treatment (Fig. <ref type="figure" target="#fig_65">3D-E</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC traces the natural course of the disease</head><p>The natural course of BC may include local relapse, possibly followed by distant metastases, the trigger events leading to death 34-38 (Fig. <ref type="figure" target="#fig_66">4A-C</ref>). Contralateral BC is often used in composite survival endpoints, such as distant relapse-free survival 39 , but MIIC clearly identifies contralateral BC as an event being independent of other oncologic events and almost totally isolated from the rest of the network (Fig. <ref type="figure" target="#fig_20">1</ref>).</p><p>Luminal BC is known to recur and develop metastases later than HER2-positive BC and TNBC (Fig. <ref type="figure" target="#fig_66">4D</ref>) 24, 25, 40, 41 . The link between has also been found between PR negativity and a higher risk of brain metastasis 42-46 (Fig. <ref type="figure" target="#fig_66">4E</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients 131</head><p>(0, I, II, and III) 53 . Our analysis based on information maximization principles suggested a new unsupervised classification of RCB scores into three categories (Fig. <ref type="figure" target="#fig_67">5E</ref>), with RCB=0 with low RCB values merged, in particular, into a single class associated with a good prognosis. of the US population 54, 55 . By 2016, the National Cancer Database (NCDB) had amassed more than 34 million hospital records from cancer patients (almost four times the size of the SEER database), to become the largest clinical cancer registry in the world 56 . In France, the French administrative health care database, the SNDS (Système National des Données de Santé), is one of the largest administrative databases in the domain of medicine, providing many opportunities for medical research 57, 58 , as it covers 99% of the French population (about 66 million people). The French government is planning to ease access to this almost exhaustive population research resource, through release as part of the "Health data hub" project. Finally, beyond these structured databases, the largest mine of untapped data worldwide remains the content of electronic health records (EHRs), encompassing a full range of data (clinical notes, laboratory results, imaging, genetic data, etc.) relating to patient care. Recent advances in information technology have made it easier for both hospitals and healthcare institutions to collect large amounts of healthcare data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients 14</head><p>Biomedical scientists are now facing new challenges in the management and analysis of massive, heterogeneous datasets 59 . These challenges include the development of tools for exploration and visualization, analytical methods, integration into a comprehensive overview, and translation of the findings into public health impact. The visualization of information makes it possible for users to find profound patterns in clinical data, through visual recognition. Simple charts cannot represent the complexity of big data analyses and fail to support multifaceted tasks effectively 3, 4 . There is, therefore, a need for sophisticated visualization tools dealing with many elements simultaneously and enabling users to perceive the patterns and insight generated by the algorithm 60 . Supplementary Table <ref type="table">1</ref> shows the main data visualization tools used to present medical data. Many of the visual methods have been adopted directly from the field of data mining, but others, specific to the healthcare domain, have also been designed (Supplementary      5.2. NEOREP study on breast cancer patients 143 Recent advances in -omics technologies have provided many valuable insights into complex biological systems. However, the analysis of -omics data is still a challenging frontier, with datasets characterised by high variability, sparsity and technical noise. These complex features make it difficult to discern causal relationships from spurious associations, limiting our ability to obtain novel mechanistic insights, and to optimise the design of resourceintensive downstream experiments. In this application in collaboration with the Perié team, we combine causal network reconstruction, machine learning, and experimental approaches to identify molecular drivers of fate decisions in hematopoietic stem and progenitor cells (HSPCs) which differentiate into all blood cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients 139</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients 141</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Metabolic drivers of hematopoietic differentiation</head><p>We focus on HSPC differentiation towards the erythroid, myeloid, and lymphoid lineages. Recent scRNAseq studies show that HSPCs do not form distinct subgroups but rather a transcriptomic continuum <ref type="bibr">(Fig 5.1)</ref>. This complex feature of the data makes it difficult to predict whether an individual progenitor will give rise to a specialised blood cell subset, and consequently, the molecular drivers of fate decisions in HSPCs are poorly understood.</p><p>Additionally, recently developed metabolomics technologies and small molecule inhibitors have permitted bulk-level analyses of hematopoietic cell-types, showing that metabolism actively regulates hematopoiesis. These studies show that metabolism influences a range of HSC behaviours, modulating not only bioenergetics, but also epigenetic state and signalling pathways <ref type="bibr" target="#b282">[124,</ref><ref type="bibr" target="#b283">125,</ref><ref type="bibr" target="#b284">126]</ref>. Despite this progress, the role of metabolism in other progenitor subsets is poorly understood. To assess metabolic heterogeneity across the hematopoietic system, we first analysed published bulk and single cell transcriptomic datasets of all major hematopoietic cell types. Using a supervised learning approach we first constructed a classifier model capable of accurately predicting mature lineage identity using the expression of metabolic genes that are variably expressed within HSPCs. This result shows that a subset of variably expressed genes in the HSPC compartment are predictive of lineage fate, but this classifier model cannot assess whether they are functionally linked or are merely associated with the differentiation.</p><p>After having established that HSPCs are metabolically heterogeneous, and differences in metabolism can influence cell fate, we wanted to know if targeting HSPC metabolism can be used to regulate myelopoiesis. To predict putative molecular drivers of differentiation we construct a causal gene regulatory network in mature cells from the haemopedia database <ref type="bibr" target="#b285">[127]</ref>, using genes that are variably expressed within the progenitor continuum and including the cell-lineage annotation as an additional variable. There are too many differentially expressed genes for them to be all included in MIIC, so we first select the 200 genes with the highest feature importance when predicting lineage from the full transcriptomic profile, using SHAP scores <ref type="bibr" target="#b286">[128,</ref><ref type="bibr" target="#b287">129]</ref>. We infer the network from the dependencies and independencies between those genes, including the categorical node "lineage" with three values : erythroid, myeloid or neutrophil. We use the resulting network for feature selection, selecting the first order neighborhood of the lineage node as these genes are inferred to have direct relationship to cell differentiation, and paying particular attention to the ancestors of lineage (Fig 5 .2).</p><p>This analysis predicted genes relating to glycolysis (Pkm) and the Pentose Phosphate Pathway (G6pdx) as key drivers of the myeloid metabolic program, while mitochondrial metabolism (Atpif1, Uqcr11) membrane transporters (Slc14a1, Abcb10) were key drivers of the erythroid program. Interestingly, different genes relating to glutathione metabolism were found as high causality markers of both the myeloid (Gsr), and erythroid lineages (Gpx1, Gstm5). Both Slc14a1 and Gsr have been independently reported in the literature as early fate markers, as determined through RNA state-fate analyses of hematopoiesis <ref type="bibr" target="#b288">[130,</ref><ref type="bibr" target="#b289">131]</ref>, supporting the validity of our causal inference predictions.</p><p>We propose that network reconstruction approaches are particularly suited to tackle feature selection problems in the context of -omics analysis. In particular, by modeling genes that drive biological processes rather than simply accompany them, these methods can guide downstream experimental efforts more efficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 6 Conclusion</head><p>The ability to infer causality from observation is extremely powerful in the right conditions. The formalism introduced by causal inference theory is a way to tap into the vast, ever increasing amount of data and gain knowledge about our world without the need for additional costly experiments. It also opens the way to more avenues of research, bringing back causality into the domain of what is attainable when interventions are impossible.</p><p>In this thesis, we contributed to make constraint based methods, and MIIC in particular, more apt to deal with real-life datasets. We devised a general case (conditional) independence test based on the mutual information and the stochastic complexity of the data, binning continuous distributions into MDL-optimal discretizations. Using this estimator, MIIC is able to infer causal graphs from any type of data, which means that there is no restriction on the type of "causality" it is able to discover, besides the faithfulness and Markov condition. We also proposed some modifications to make the inference process more consistent with the resulting graph, ensuring that the separating sets can be read off the graph. This includes both modifications to the skeleton inference algorithm and a rule for test-wise omission of missing data. Other contributions include more reliable orientation of the edges, and the ability to tell apart putative from genuine causes.</p><p>Applications on real data showed the advantages of using this network-oriented approach, with uses ranging from data quality control to decision making for future experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perspectives and future research</head><p>As future research directions, I would like to adapt MIIC to temporal data, which carry intrinsically the causality signature but are harder to analyze using traditional methods. On one hand, information theory is already deeply linked with time-series data, with methods such as Granger causality <ref type="bibr" target="#b290">[132]</ref> and the transfer entropy <ref type="bibr" target="#b291">[133]</ref>. On the other, recent work has shown that the temporal information can be formally included in the theory of causal graphs <ref type="bibr" target="#b292">[134,</ref><ref type="bibr" target="#b293">135]</ref>. A MIIC extension to temporal series would benefit from previous work in both 150 Chapter 6. Conclusion domains, bridging the gap between information theory and causal graph theory for temporal data.</p><p>I would also like to investigate whether the optimal discretization found for a single edge can be used for Bayesian inference, where one tries to reconstruct p(V ) from G In f . The discretization is only optimal in relation to each edge, but there may be a way to combine those results to get an MDL-optimal encoding of the variable in relation to either all of its neighbors or its parents, making it usable for Bayesian inference.</p><p>As mentioned in the introduction, feature selection and the causal structure are closely related. I would want to investigate how we can infer causal structure from the knockoffs framework, which perform feature selection while rigorously controlling the expected fraction of false positives <ref type="bibr" target="#b294">[136,</ref><ref type="bibr" target="#b295">137]</ref>. The biggest drawback of this method is the generation of knockoffs, which could perhaps be simplified using the MDL principle and the NML distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>152</head><p>Appendix A. Résumé long en français une batterie vide, seraient donc représentées comme des parents du noeud "Panne". Après observation, nous disposons d'une autre information : les phares ne s'allument pas. Nous savons que les phares ne dépendent pas du niveau d'huile mais ont besoin de batterie, et sont donc reliés au noeud "Batterie" uniquement. Notez la présence du lien en pointillés entre "Phares" et "Panne", qui traduit l'idée que si les phares ne s'allument pas, la voiture ne va probablement pas démarrer. Ce lien retranscrit une corrélation, et pas une causalité : cette interaction indirecte existe seulement à cause de l'ancêtre commun "Batterie" mais ne nous informe pas sur une relation fonctionnelle. Il peut nous renseigner sur l'origine de la panne (Batterie ou Huile), mais réparer les phares n'aidera pas à faire démarrer la voiture. 1. G c est un réseau dirigé et acyclique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Panne Batterie Huile</head><note type="other">Phares</note><p>2. La distribution de chaque noeud X i peut être exprimé en fonction de celle de ses parents L'algorithme MIIC (multivariate information-based inductive causation) partage le fonctionnement général des algorithmes basés sur les contraintes, avec des modifications qui le rendent plus robustes au bruit et plus efficace <ref type="bibr">[35,</ref><ref type="bibr">44]</ref> :</p><p>• L'indépendance (conditionnelle) est inférée à partir de l'estimation de l'information mutuelle (conditionnelle) corrigée avec la complexité stochastique, qui présente certains avantages par rapport aux test fréquentistes habituels en particulier pour une taille d'échantillon N réduite <ref type="bibr">[5,</ref><ref type="bibr" target="#b296">138]</ref>.</p><p>• La recherche des indépendances conditionnelles se fait itérativement en enlevant les meilleurs contributeurs grâce à la règle de la chaîne de l'information mutuelle conditionelle : • L'orientation des V-structures et leur propagation se basent sur des probabilités calculées avec les informations mutuelles multivariées et ressemble davantage à de l'inférence Bayésienne, donnant en général une meilleure orientation du squelette <ref type="bibr">[44]</ref>.</p><p>Les références <ref type="bibr">[44,</ref><ref type="bibr" target="#b297">139]</ref> montrent des cas d'utilisation à échelles variées : de trajectoires de différentiation à partir de données single-cell à l'étude de la valeur adaptative de différentes caractéristiques de gènes dans un jeu de données phylogénétique. MIIC est cependant limité aux valeurs discrètes pour lesquelles l'information mutuelle et la complexité stochastique peuvent être facilement estimées.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>L'objectif de cette thèse est d'améliorer MIIC pour le rendre plus apte à traiter des données issues du monde réel. Premièrement, s'affranchir le plus possible des conditions imposées sur la distribution P(v) permettrait d'inclure toutes les données à disposition pour reconstruire G in f . Concrètement, nous voulons pouvoir estimer l'indépendance conditionnelle entre deux variables X,Y avec un set de conditionnement Z peu importe la nature des distributions marginales (p(X), p(Y ), p(Z)) et des distributions jointes (p(X,Y ) etc...). Cette estimation doit aussi être robuste à des petite taille d'échantillon tout en restant calculable quand N est grand, et idéalement ne favorise aucun type de variable ou d'interaction. L'information mutuelle est une quantité idéale pour accomplir ces objectifs : elle mesure la dépendance entre deux variables aléatoires au sens le plus général. Elle est définie pour tout type de variables et tout type de relation : notée I(X;Y ), elle donne simplement la quantité d'information que l'on a sur X en connaissant Y , et vice-versa. Introduite par Claude Shannon en 1948 pour caractériser les canaux de communication <ref type="bibr">[45]</ref>, elle a trouvé son succès dans de nombreux domaines grâce à une unique combinaison de propriétés désirables. Premièrement, elle est strictement équivalente à l'indépendance statistique : I(X;Y ) ↔ X ⊥ ⊥ Y , peu importe les distributions p(X), p(Y ) et p(X,Y ). Elle est aussi décomposable grâce à la chain rule, statisfait le principe de data processing inequality, et est invariable aux transformations sur X et Y qui conservent les rangs <ref type="bibr">[4]</ref>. Elle est aussi considérée comme équitable : elle détecte avec la même puissance tout type d'interaction du moment qu'elles ont le même rapport signal sur bruit <ref type="bibr">[27]</ref>.</p><p>La mesure de dépendance idéale en théorie, son estimation sur des échantillons finis est notoirement difficile si X,Y a des composantes continues. Les approches par estimation locale de l'entropie en regardant les k plus proches voisins donnent de bons résultats empiriques <ref type="bibr" target="#b235">[77,</ref><ref type="bibr" target="#b298">140]</ref>, mais leur significativité est difficile à évaluer quand X ⊥ ⊥ Y ou que le signal est très faible <ref type="bibr" target="#b235">[77,</ref><ref type="bibr" target="#b243">85,</ref><ref type="bibr">51]</ref>, ce qui complique leur usage pour les méthodes par contraintes. Une autre façon d'estimer l'information mutuelle sur des variables continues est de les discrétiser dans des partitions, à la manière d'un histogramme. Cependant, le résultat est alors dépendant de la discrétisation de chaque variable X v : l'estimation à partir des versions discrétisées I([X 1 ] ∆ ; [X 2 ] ∆ ) ne converge pas vers la vraie valeur I(X 1 ; X 2 ) mais vers une valeur qui dépend du nombre et de la taille des partitions <ref type="bibr" target="#b227">[69,</ref><ref type="bibr">49]</ref>.</p><p>La méthode présentée ici repose sur la définition maître de l'information mutuelle : I(X;Y ) = sup  </p><formula xml:id="formula_128">I ([X] ∆ ; [Y ] ∆ ) = I([X] ∆ ; [Y ] ∆ ) -k X ∆ ;Y ∆ (N) 1 N (A.0.2)</formula><p>où k X ∆ ;Y ∆ (N) est le terme de complexité, par exemple k BIC X ∆ ;Y ∆ (N) = 1 2 (∆ X -1)(∆ Y -1) log(N) pour le Bayesian Information Criterion. Introduire la complexité permet aussi de conclure sur l'indépendance sur des échantillons finis (pour lesquels l'estimation de l'information est toujours positive) : I ([X] ∆ ; [Y ] ∆ ) ≤ 0 implique l'indépendance entre X et Y au sens de la complexité des données <ref type="bibr">[5]</ref>.</p><p>La maximisation de l'information mutuelle est calculée par programmation dynamique et est inspirée de Kontkanen et al. <ref type="bibr" target="#b257">[99]</ref>. Dans cette étude, les auteurs proposent un algorithme pour trouver la discrétisation optimale d'un échantillon de variable aléatoire en maximisant un score de vraisemblance normalisé dérivé selon le principe de longueur de description minimale. La méthode est adaptée à deux dimensions pour trouver la discrétisation d'une variable X qui maximise l'information corrigée I ([X] ∆ ;Y ) avec une variable discrète Y . Le résultat est un algorithme qui permet simultanément d'estimer la valeur de l'information mutuelle et d'évaluer sa significativité au sens de la complexité stochastique, peu importe la nature des variables étudiées qui peuvent être continues, discrètes ou une mixture des deux. Les partitions trouvées de cette manière satisfont le principe de description minimale, et encodent les données non pas pour décrire les distributions marginales comme dans <ref type="bibr" target="#b257">[99]</ref> mais la distribution jointe (Fig A <ref type="figure" target="#fig_66">.4</ref>). En pratique, il faut donc discrétiser chaque distribution jointe pour conclure sur la dépendance (conditionnelle) entre deux variables : on ne peut pas discrétiser chaque variable une à une et espérer une estimation non biaisée de leurs informations mutuelles. Nous améliorons la complexité de l'approche originale de Kontkanen et al. en limitant le nombre de cutpoints possibles à une valeur c &lt;&lt; N (typiquement, un  La discrétisation optimale est évaluée d'abord comme estimateur de l'information mutuelle (conditionelle) sur variables continues, et se compare favorablement à l'état de l'art en particulier quand le signal se rapproche de l'indépendance X ⊥ ⊥ Y et X ⊥ ⊥ Y | Z <ref type="bibr">[7]</ref>. Elle a également de bon résultats sur les variables mixtes, en accord avec la définition maître de l'information mutuelle, comparée aux autres approches conçues spécialement pour ce cas <ref type="bibr" target="#b229">[71,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b249">91]</ref>. Nous notons plusieurs avantages par rapport aux autres méthodes : le résultat ne dépend pas du choix d'un paramètre (par exemple le nombre de plus proches voisins k, le type de kernel etc...), et la significativité est évaluée de manière strictement identique pour tous les cas de figures : continu-continu, discret-discret ou mixte. L'intégration de cet estimateur à MIIC permet de reconstruire le squelette du graphe en estimant les informations mutuelles conditionnelles, puis d'orienter les liens grâce aux informations mutuelles multivariées; pour tout type de variable. Nous comparons aussi ses performances pour reconstruire des graphes causaux à partir de données simulées, et trouvons des résultats similaires ou supérieurs aux méthodes existantes <ref type="bibr" target="#b267">[109,</ref><ref type="bibr" target="#b268">110]</ref> (Fig A <ref type="figure" target="#fig_67">.5</ref>). En particulier, notre approche est la seule qui semble être non biaisée envers certains types de variable ou d'interaction. Et nous acceptons Z seulement si cette divergence ne dépasse pas un certain seuil, dérivé de la théorie de l'information.</p><p>Dans la même idée, nous avons travaillé sur une variante des algorithmes par contraintes qui garantit que les sets de conditionnement utilisés pour retirer les liens soient plus cohérents avec le graphe final G in f et les données D <ref type="bibr">[6]</ref>. En effet, ces méthodes se basent uniquement sur les indépendances conditionnelles dans D mais n'offrent aucune garantie que les sets de conditionnement utilisés pour retirer les liens correspondent à des sets de d-séparation dans le graphe final. En fait, leur fonctionnement ne garantit même pas qu'elles soient toujours dans la même composante connexe dans G in f . Ce défaut rend non seulement le résultat peu interprétable mais cause aussi des problèmes de performance. Ces sets de conditionnement incohérents ont tendance à venir du bruit d'échantillonage plutôt que de réalités fonctionnelles, et les graphes reconstruits sur des données complexes sont typiquement très peu connectés. La version cohérente des algorithmes par contraintes produit un graphe G in f moins sujet aux indépendances bruitées et duquel il est plus facile de déduire les sets de condionnement utilisés, ce qui rend la méthode plus interprétable. Cette variante est particulièrement adaptée à MIIC qui retire les contributeurs dans l'ordre en commençant par le meilleur score, par rapport aux méthodes de référence qui essayent toutes les combinaisons possibles jusqu'à trouver une significativité.</p><p>Les méthodes traditionnelles basées sur les contraintes (MIIC compris), ne font que découvrir des relations causales "putatives", en découvrant les orientations des V-structures, qui sont en fait compatibles à la fois avec une relation cause-effet réelle et avec un lien bi-directionnel provenant d'une cause commune non observée. Nous contribuons aussi aux méthodes par contraintes en montrant comment distinguer les liens de causalité "authentiques" des liens "putatifs" en excluant l'effet d'une cause commune non observée pour chaque lien de causalité authentique prédit. Nous y parvenons en évaluant les probabilités séparées de la "tête" et de la "queue" des liens dirigés pour toutes les arêtes orientées. Les arêtes causales authentiques sont alors prédites si les probabilités de la tête et de la queue sont statistiquement significatives, tandis que les arêtes causales restent "putatives" si leur probabilité de queue n'est pas statistiquement significative ou ne peut être déterminée à partir de données purement observationnelles (c'est à dire, liens non dirigés dans la classe d'équivalence de G c ). Cela donne une meilleure interprétation des méthodes par contraintes sur des données réelles, pour lesquelles il est difficile d'assurer avec certitude que toutes les variables du système sont observées, et donc que les liens dirigés de G in f soient "authentiques". et indirectes, ou la direction de la causalité. Il permet de vérifier le contenu du jeu de données, en mettant en évidence des liens inattendus ou au contraire des indépendances qui pourraient indiquer des biais dans la collecte des données. Le réseau capture également certaines facettes du raisonnement du neurologue derrière les diagnostics de différentes formes de démences. En particulier, les noeuds de diagnostic peuvent être interprétés comme des variables "explicatives" associées à un certain nombre d'effets "explaining-away" <ref type="bibr">[1]</ref> sous la forme de V-structure. Nous notons aussi des liens directs inattendus entre des informations cliniquement pertinentes, telles que la connexion directe entre les échelles de Fazekas (qui mesure la quantité d'hyperintensité dans la substance blanche, attribuées à l'ischémie chronique des petits vaisseaux) et de Scheltens (atrophie du lobe temporal médian), qui peuvent fournir des informations physiologiques et suggérer de nouvelles directions de recherche <ref type="bibr" target="#b281">[123]</ref>.  a mis en évidence la centralité du noeud "centre de recherche" qui correspond au lieu de prise en charge des patients, Paris ou Saint-Cloud. Il y a au moins deux explications à la présence des liens directes avec le noeud "centre" : les populations de patients traités aux deux lieux sont différentes, ce qui peut causer des biais de sélection ailleurs si ce n'est pas correctement pris en compte; et les deux équipes médicales ont des pratiques et du matériel disponible différents ce qui ne donne pas les mêmes options thérapeutiques. Un autre résultat qui mérite une mention est le voisinage du noeud "Death" qui indique l'état vital de la patiente à l'issue de sa prise en charge. Toutes les variables qui y sont directement liées n'ont pas pu être expliquée par d'autres conditionnements, donc en théorie elles donnent une information unique sur le pronostic final des patients. L'équipe de cliniciens du département de chirurgie est particulièrement intéressée par le lien "Death" -"RCB", Residual Cancer Bruden un indice composite généralement binairisé pour donner la variable "pCR", Pathological Complete Response, qui est utilisé comme facteur de risque pour prédire la mortalité <ref type="bibr" target="#b299">[141]</ref>. Or le résultat de MIIC met en évidence un lien direct avec "RCB" et non "pCR", ce qui indique qu'une partie de l'information est perdue en passant par la dichotomisation.</p><p>Notre collaboration avec l'équipe Périé nous a permis de valider en partie les prédictions de MIIC avec des expériences in vitro. La troisième application de MIIC sur données mixtes porte sur la découverte de gènes qui influencent la différentiation de cellules précurseur hématopoïétiques, et l'inférence du réseau de régulation de ces gènes. L'équipe est particulièrement intéressée par les gènes impliqués dans le métabolisme qui sont moins étudiés que les facteurs de transcription, et a produit des jeux de données d'expression single-cell et bulk pour étudier la différenciation de cellules précurseurs vers les branches myéloïde ou érytroïde. MIIC est utilisé pour trouver les gènes dont l'expression est directement liée avec la lignée cellulaire dans le graphe final, en complément avec d'autres méthodes de sélection de variables. Une attention particulière est aussi donnée aux gènes dont l'orientation les place en amont de la lignée dans l'ordre causal, puisque le but est de trouver les gènes non seulement prédictifs mais qui causent la différenciation. En partant du jeu de données complet, des expériences in vitro ont confirmé le rôle de certaines familles de gènes identifiées par MIIC comme jouant un rôle dans la différenciation : en particulier, des gènes impliqués dans la glycolyse et la voie des pentoses phosphates semblent impliqués dans la spécialisation en cellules myéloïdes, alors que l'expression des gènes du métabolisme mitochondrien dirigent vers le programme érythroïde.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Chapter</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 Chapter 2 Figure 2 . 1 :</head><label>6221</label><figDesc>Figure 2.1: Car breakdown diagnosis with a causal diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2. 1 .Figure 2 . 2 :</head><label>122</label><figDesc>Figure 2.2: Randomized control trial where X is the treatment, Y the outcome and Z external factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 . 3 :</head><label>23</label><figDesc>Figure 2.3: Observational studies require to adjust for the confounding variables Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Chapter 2 .</head><label>2</label><figDesc>Figure 2.5: Various forms of dependencies, X ⊥ ⊥⇔ p(x, y) =p(x)p(y)</figDesc><graphic coords="23,144.85,79.37,305.58,218.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Finally, weFigure 2 . 6 :</head><label>26</label><figDesc>Figure 2.6: All nodes in the light blue shade are in the Markov blanket of the node Y .</figDesc><graphic coords="25,215.42,406.24,159.02,159.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Chapter 3 .Figure 3 .</head><label>33</label><figDesc>Figure 3.3: Relationship conditional mutual information and three-point information with three variables. Note that the three-point information can be negative when two variables are pairwise-independent but become dependent when conditioning on the third.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FigFigure 3 . 4 :</head><label>34</label><figDesc>Figure 3.4: DAGs with 3 nodes and 2 edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Chapter 3 .Figure 3 . 5 :</head><label>335</label><figDesc>Figure 3.5: The ReLU activation function is mixed: it has p(0) &gt; 0 and is continuous for x &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 . 6 :</head><label>36</label><figDesc>Figure 3.6: Choice of the ρ k,i distance with ∞ norm (left) or 2 norm (righ) for the KSG estimator. Figure taken from [81].</figDesc><graphic coords="49,123.02,470.87,349.23,134.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 . 7 :</head><label>37</label><figDesc>Figure 3.7: Two null hypotheses (X ⊥ ⊥ Y ) of the mixture variable given as example, with ρ = 0, β = 0.5, p con = p dis = 0.5. Left : discrete and continuous parts are kept separated, as if in different dimensions. Right : all data points are on the same euclidean space, and the null hypothesis is p(x, y) = p(x)p(y). The joint histogram corresponds to the optimal discretization of [7] for both cases.</figDesc><graphic coords="52,82.24,79.37,213.90,178.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 . 8 :</head><label>38</label><figDesc>Figure3.8: MDL-optimal histograms on a multimodal Gaussian distribution, with N = 300 samples (left) and N = 3000 (right) according to<ref type="bibr" target="#b257">[99]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Algorithm 4</head><label>4</label><figDesc>= n x ), comes with an independent mutual information contribution, ∑ r y y n xy log n xy -n x log n x , a parametric complexity, log C r y n x , and encoding cost, C N, rx . The initial condition for j = 0 in (3.2.18) is set by convention to include all terms invariant under X-partitioning, i.e., -∑ r y y n y log(n y /N) + log C r y N -(r y -1)C N,r y +C N, rx . Equation 3.2.18 is illustrated by the pseudocode of Alg 4. Opt(I(X; [Y ]) : MDL-Optimal discretization of X maximizing I(X; [Y ]) Require: Ranks of X, [Y ], coarse level c, rx Define possible cutpoints C from N and c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 . 10 :</head><label>310</label><figDesc>Figure 3.10: Optimal discretization of three joint distributions with the same marginal X, as found by maximizing Î (X;Y ).</figDesc><graphic coords="59,85.11,497.52,139.70,139.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 3 . 12 :</head><label>312</label><figDesc>Figure3.12: Mutual information estimation for 100 Gaussian bivariate distributions. The mean squared error (center graphs) was calculated thanks to the analytical result of the mutual information of the bivariate Gaussian (Eq 3.1.16). The standard deviation of each estimator over the 100 replications was also plotted against the correlation coefficient (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>3. 2 . 53 Figure 3 . 13 :</head><label>253313</label><figDesc>Figure 3.13: Mutual information estimation of mixed variables. Experiment set-ups and analytical values for the mutual information were taken fom[51]  and 50 runs were performed for each sample size N. From left to right, top to bottom, the simulations are devised after experiment I, experiment II, experiment IV with p = 0 and experiment IV with p = 0.15, from[51]. The top left experiment corresponds to the distribution of Fig 3.7 with ρ = 0.9 and β = 0.9 and p con = p dis = 0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 3 . 14 :</head><label>314</label><figDesc>Figure 3.14: Conditional mutual information estimation for multivariate Gaussian distributions Four-dimensional normal distributions P(X,Y, Z 1 , Z 2 ) were sampled for N = 100 and 10, 000 samples 100 times for each correlation coefficient ρ = ρ XY , chosen between 0.05 and 0.95.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 3 . 15 :</head><label>315</label><figDesc>Figure 3.15: Conditional independence tests on mixed variables. Mean Area Under the Curve of ROC curves from 200 rounds of simulation at each sample size n for the LCD triple [50]. The triple is scored according to a combination of three p-values for three independence tests : C ⊥ ⊥ X, X ⊥ ⊥ Y and C ⊥ ⊥ Y |X, and is given a true 'positive' label if the data is simulated according to the relationship C → X → Y , 'negative' otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig 1 .</head><label>1</label><figDesc>Fig 1. Mutual information computation between continuous or mixed-type variables. Outline of mutual information computation between continuous or mixed-type variables for a finite dataset of N samples. Mutual information is estimated through an optimum partitioning of continuous variable(s) (solid red line and arrow) after introducing a complexity term to account for the finite size of the dataset, see main text. https://doi.org/10.1371/journal.pcbi.1007866.g001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Fig 2 and S1 Fig for increasing sample size. The number of bins increases both with the number of samples, S1 Fig, and the magnitude of mutual information, I N (X; Y), S2A Fig. These tendencies have intuitive explanations: first, more samples means that we can assign smaller bins (width-wise) with more certainty; and second, more information means that more bins are needed to describe the interaction between the variales.We note that no single discretization of a variable X can be optimal with regards to every joint distribution, see S3 Fig.Whilethe precise cut points of variable X actually depend on the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig 2 .</head><label>2</label><figDesc>Fig 2. Optimum bivariate discretization for mutual information estimate. The proposed information-maximizing discretization scheme is illustrated for a joint distribution defined as a Gumbel bivariate copula with parameter θ = 5 and marginal distributions chosen as Gaussian mixtures with three equiprobable peaks and respective means and variances, μ X = {0, 4, 6}, σ X = {1, 2, 0.7} and μ Y = {-3, 6, 9}, σ Y = {2, 0.5, 0.5}. The information-maximizing partition yields (A) I N (X; Y) = 1.04 for N = 500 samples and (B) I N (X; Y) = 1.142 for N = 10, 000 samples, as compared to the exact expected value I(X; Y) = 1.205 computed with numerical integration. See S1 Fig for additional results. Codes are provided at https://github.com/vcabeli/miic_PLoS. https://doi.org/10.1371/journal.pcbi.1007866.g002</figDesc><graphic coords="75,139.99,436.56,420.39,217.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>S6 Fig, as in the case of a single common ancestor Z of X and Y, i.e., X Z ! Y, with concomitant changes in optimum X and Y partitionings from multiple to single bins under conditioning over a continuous (S7 Fig) or categorical (S8 Fig) variable Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. Reconstruction of benchmark networks for mixed-type, non-linear, non-Gaussian datasets. CPDAG F-scores obtained for benchmark random networks with 100 nodes and average degree 3 reconstructed from N = 100-5,000 samples (see histogram example S10 Fig). F-scores obtained with our parameter-free information-theoretic approach MIIC (magenta, upper surface) are compared to the best results obtained with alternative mixed-type data methods, CausalMGM [24] (blue, middle surface) and MXM [25] (green, lower surface), by optimizing CausalMGM regularization parameters (λ) and MXM significance parameter (α), for each sample size N. See additional results in S11-S15 Figs. Codes are provided at https:// github.com/vcabeli/miic_PLoS. https://doi.org/10.1371/journal.pcbi.1007866.g003</figDesc><graphic coords="77,174.08,111.45,386.30,369.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. Network reconstructed from medical records of 1,628 eldery patients with cognitive disorders. Square (resp. circle) nodes correspond to discrete (resp. continuous) variables. Red (resp. blue) edges correspond to correlation (resp. anticorrelation) between variables. Dotted edges reflect latent variables, see Discussion.https://doi.org/10.1371/journal.pcbi.1007866.g004</figDesc><graphic coords="78,139.99,92.79,420.39,300.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>bivariate discretization for mutual information estimation. The proposed information-maximizing discretization scheme is illustrated for a joint distribution defined as a Gumbel bivariate copula with parameter θ = 5 and univariate marginal-distribution functions chosen as Gaussian mixtures with three equiprobable peaks and respective means and variances, μ X = {0, 4, 6}, σ X = {1, 2, 0.7} and μ Y = {-3, 6, 9}, σ Y = {2, 0.5, 0.5}. Information-maximizing partitions are displayed for different sample sizes with corresponding mutual information estimates: (A) N = 100 samples, I N (X; Y) = 0.928 (and I 0 N ðX; YÞ ¼ 0:649); (B) N = 500 samples, I N (X; Y) = 1.040 (and I 0 N ðX; YÞ ¼ 0:866); (C) N = 1, 000 samples, I N (X; Y) = 1.096 (and I 0 N ðX; YÞ ¼ 0:977); (D) N = 10, 000 samples, I N (X; Y) = 1.142 (and I 0 N ðX; YÞ ¼ 1:075). The actual mutual information value was computed through numerical integrationof the marginals and the joint probability distribution and yields, I(X; Y) = 1.205, in good agreement with the obtained estimates for large N. (EPS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>As for continuous common cause in S7 Fig, there is (A) some non-zero mutual information between X and Y corresponding to an optimum discretization, while (B) conditional mutual information vanishes when conditioning on the categorial common cause, Z, with the partitions of both X and Y variables consisting in a single bin. (EPS) S9 Fig. Pairwise independence and conditional dependence with a v-structure. Example of two independent variables X, Y both causing a third variable Z as: X ! Z Y. N = 1, 000 observations are drawn for X; Y � N ð0; 1Þ and Z = X + Y. (A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>of dataset generated for mixed-type, non-linear, non-Gaussian benchmarking with 69 continuous and 31 categorical variables. Each plot represents the observed density or histogram (N = 1, 000) of the continuous or categorical variable X i , constructed by structural equation models given its parents' distributions (see Supporting Information). (EPS) S11 Fig. Skeleton assessment of benchmark networks for mixed-type, non-linear, non-Gaussian datasets. Skeleton Precision, Recall and F-scores obtained for benchmark random networks with 100 nodes and average degree 3 reconstructed from N = 100-5,000 samples (see histogram example Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>of dataset used for continuous, non-linear, non-Gaussian benchmarking with 100 continuous variables. (EPS) S14 Fig. Skeleton assessment of benchmark networks for continuous, non-linear, non-Gaussian datasets. Skeleton Precision, Recall and F-scores obtained for benchmark random networks with 100 nodes and average degree 3 reconstructed from N = 100 -10, 000 samples (see histogram example Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Toy example of putative and genuine causal relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Orientation of putative, genuine or latent edges based on an orientation threshold β .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>4. 1 .</head><label>1</label><figDesc>Improvements 91 score i = minlog 1 + e -max+min + e -max (4.1.7) min = min N| I (X;Y ; Z|{ A i })|, score v (4.1.8) max = max N| I (X;Y ; Z|{ A i })|, score v (4.1.9)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>addition to an open-source R package, we have developed a web interface to analyze visually the results of MIIC. Constraint-based approaches, and MIIC, being entirely nonparametric, it can help to be able to visualize the joint distributions of the inferred direct links. We implemented various plots for the continuous, discrete and mixed case using the D3 JavaScript library and plotly (Fig A.6). When applicable, it also shows the result of the optimal discretization for a given edge. More examples are shown with the application on medical record of breast cancer patients in Section 5.2. Related to the next section, we made it also intuitive to visually control the validity of separating sets found by MIIC. From the inferred graph, one can easily see if they satisfy d-separation or if they violate consistency with respect to G In f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Online network inteface.In this example, a violin plot describes the joint distribution between the continuous variable "RCB", and "Death" which is discrete. The horizontal bold lines inform on the optimal discretization found to infer the edge.</figDesc><graphic coords="100,79.37,460.32,436.53,234.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Definition 3 .</head><label>3</label><figDesc>S(G 1 |G 2 ) is a modified version of the PC-stable algorithm, where step 1 in algorithm 1 is replaced by NewStep1(G 1 |G 2 ) from definition 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>4 96 4 .</head><label>44</label><figDesc>Chapter Other improvements to constraint-based algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>98</head><label>98</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sepset inconsistency of the original PC-stable algorithm. In each subplot the fraction of inconsis-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Precision-recall curves for the original PC-stable (yellow), skeleton-consistent PC-stable (green) and orientation-consistent PC-stable (blue). The mean performances and standard deviations (error bars) obtained over 100 networks are shown for 7 values of the (conditional) independence significance threshold α between 10 -5 and 0.2 Data-sets with N =500 samples were generated from the same graphs as in Figure3with strong (left), medium (middle) and weak (right) interactions. See FigureS1for N =100, 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>8 100 4 .</head><label>84</label><figDesc>Chapter Other improvements to constraint-based algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Proportion of valid d-separation sepsets among edge-removing sepsets. Top row shows the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure S2 : 4 106 4 .</head><label>S244</label><figDesc>Figure S2: Proportion of valid d-separation sepsets among edge-removing sepsets found during reconstruction. Data-sets of N =100 samples (top two rows) or of N =1000 (bottom two rows), with strong (left), medium (middle) and weak (right) interactions. See Figure 6 for more information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: PC original, majority and conservative orientation rules on discrete datasets. Benchmark datasets are generated from random 100-node DAGs with average degree 2.7 and maximum degree 4 (See Data generation and benchmarks section for details). PC structure learning performance is measured in terms of Precision, Recall and F-scores (±σ) for skeleton (blue), CPDAG (red) and oriented-edge-only subgraph (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PC original, majority and conservative orientation rules on continuous datasets.Benchmark datasets are generated from random 100-node DAGs with average degree 2.7 and maximum degree 4 (See Data generation and benchmarks section for details). PC structure learning performance is measured in terms of Precision, Recall and F-scores (±σ) for skeleton (blue), CPDAG (red) and oriented-edge-only subgraph (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>N</head><label></label><figDesc>(X; Y |{A i }) is computed iteratively in linear time<ref type="bibr" target="#b77">(Kontkanen and Myllymäki, 2007;</ref>     Roos et al., 2008)  for increasing numbers of X and Y partitions, r x and r y , starting withk NML N (X; Y |{A i }) = 0 for r x = r y = 1 (Affeldt and Isambert, 2015; Cabeli et al., 2020). Hence, (conditional) independence is established for I N (X; Y |{A i }) 0, whenever sufficient and significant indirect positive contributions could be iteratively collected in Eq. 1 to warrant the removal of the XY edge. This leads to an undirected skeleton, which MIIC then (partially) orients based on the sign and amplitude of the NML-regularized conditional 3-point information terms (Affeldt and Isambert, 2015; Verny et al., 2017), corresponding to the difference between NML-regularized (C)MI terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 3 : 5 112 4 .</head><label>354</label><figDesc>Figure 3: Original MIIC with orientation rules allowing for negative NML-regularized MI &amp; CMI on discrete data (left) and negative NML-regularized CMI on continuous data (right). Benchmark datasets are the same as in Figs. 1 &amp; 2. MIIC structure learning performance is measured in terms of Precision, Recall and F-scores (±σ) for skeleton (blue), CPDAG (red) and oriented-edgeonly subgraph (green). PC average scores for majority orientation rules are shown as dashed lines for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Fig. 3 ,</head><label>3</label><figDesc>Fig.3, and typically much better than the results obtained with traditional constraint-based methods, which, unlike MIIC, need to rely on independence tests, that are notoriously difficult for continuous data.However, by contrast with discrete data, the remaining loss between skeleton and oriented graph precisions appears to differ between the CPDAG score and the oriented-edge-only subgraph score used for the comparison, Fig.3. It indicates that the precision of the oriented-edge-only subgraph is slightly though significantly better than for the overall partially oriented graph, with a small concomitant loss of orientation recall, at small sample sizes, Fig.3. This trend is due to the more stringent condition for v-structure orientation brought by the non-negative NML-regularized MI estimates obtained by MIIC for continuous variables. Yet, the optimum partitioning principle only applies to MI(Cover and Thomas, 2006), not CMI, which need to be estimated through the difference between optimum NML-regularized MI terms, as I N (X; Y |U ) = I N (Y ; {X, U }) -I N (Y ; U ) = I N (X; {Y, U }) -I N (X; U )<ref type="bibr" target="#b70">(Cabeli et al., 2020)</ref>. As a result, the approximate NML-regularized</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Theorem 1 .Proposition 2 .</head><label>12</label><figDesc>Optimum NML-regulatized MI and NML-regulatized CMI are non-negative. Proof. We first address optimum NML-regularized MI, noting that I N (X; Y ) I N ([X] 1 ; [Y ] 1 ) = 0, where [X] 1 and [Y ] 1 are the X and Y variables partitioned into single bins, which leads to a vanishing NML-regularized MI, as both MI and NML complexity cost are null, in this case, as k NML N (X; Y ) = 0 for r x = r y = 1 (Affeldt and Isambert, 2015). Then, NML-regularized CMI is defined as the difference between optimum NML-regularized MI terms as, I N (X; Y |U ) = I N (Y ; {X, U }) -I N (Y ; U ) = I N (X; {Y, U }) -I N (X; U ). However, partitioning X and Y into a single bin leads to I N (Y ; {X, U }) I N (Y ; {[X] 1 , U }) = I N (Y ; U ) and I N (X; {Y, U }) I N (X; {[Y ] 1 , U }) = I N (X; U ) thus implying I N (X; Y |U ) 0 Following these considerations on the negativity of NML-regularized (C)MI with MIIC original orientation implementation, we propose a small modification, based on Theorem 1 and referred to as conservative MIIC, by analogy to the conservative orientation rules of traditional constraint-based methods (Ramsey, Spirtes, and Zhang, 2006), as noted above. Conservative MIIC rectifies negative values of NML-regularized (C)MI, indicating (conditional) independence, to null values instead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>2. 7 . 7 114 4 .Figure 4 :</head><label>7744</label><figDesc>Figure 4: Conservative MIIC with new orientation rules enforcing non-negative NMLregularized MI &amp; CMI on discrete data (left) as well as continuous data (right). Benchmark datasets are the same as in Figs. 1 &amp; 2. Conservative MIIC structure learning performance is measured in terms of Precision, Recall and F-scores (±σ) for skeleton (blue), CPDAG (red) and oriented-edge-only subgraph (green). PC average scores for conservative orientation rules are shown as dashed lines for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figures and TablesFigure 1 :</head><label>1</label><figDesc>Figures and Tables</figDesc><graphic coords="148,70.84,96.10,275.23,510.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The MIIC interactive online interface identifies inherent associations between variables. A) NAC type is directly correlated with NAC duration. NAC=neoadjuvant chemotherapy B) Distribution of neoadjuvant chemotherapy (NAC) duration (in days) according to the NAC regimen administered: anthracyclines (AC), taxanes or sequential AC-taxanes C) The number of axillary nodes in the histological specimen depends on the type of axillary surgery performed D) Boxplot showing the number of axillary nodes removed according to the type of surgery performed: lymph node dissection (LND), sentinel lymph node biopsy (SLN) or both E) Network interactions of the RCB node with the five patterns making up the RCB score.</figDesc><graphic coords="149,70.82,70.81,406.87,541.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. MIIC identifies differences in clinical practices between the two centers of the cohort A) Network interactions around the node "center" of treatment. B) Proportion of patients undergoing oncoplastic surgery, according to treatment center: Paris or St Cloud C) Proportion of patients receiving adjuvant chemotherapy according to treatment center: Paris or St Cloud. D) Proportion of the various NAC regimens according to treatment center. E) Distribution plot for NAC duration in days, according to treatment center.</figDesc><graphic coords="150,70.82,84.62,453.32,498.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. MIIC traces the natural course of the disease A) Network interactions showing links between relapses, metastases and death in breast cancer. B) Proportion of distant metastases according to the occurrence or absence of local relapses. C) Proportion of deaths according to distant metastasis status. D) Distribution plot for relapse-free survival (in months) according to breast cancer subtype. E) Proportion plot displaying the relationship between central nervous system (CNS) metastasis and progesterone receptor (PR) status.</figDesc><graphic coords="151,70.62,82.37,453.57,473.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 5 MIIC</head><label>5</label><figDesc>Figure 5 MIIC identifies factors likely to improve prediction or prognosis. A) Network interaction displaying the link between local relapse occurrence and the number of drugs taken (comedication). B) Proportion plot showing the percentage of different clinical responses according to the presence or absence of pre-NAC lymphovascular invasion. C) Boxplot of relapse-free survival according to the presence or absence of pre-NAC lymphovascular invasion. D) Network interaction displaying the link between death, RCB and post-NAC mitotic index. E) Boxplot of RCB values according to vital status. F) Boxplot of post-NAC mitotic index according to vital status.</figDesc><graphic coords="152,70.42,114.02,453.57,493.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Figure 5.1: Haematopoietic stem cells differentiation.</figDesc><graphic coords="154,188.50,129.33,218.27,173.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>5. 3 .Figure 5 . 2 :</head><label>352</label><figDesc>Figure 5.2: MIIC network inferred from candidate driver genes, centered on the lineage node. Highlighted nodes are directed neighbors of lineage.</figDesc><graphic coords="156,123.02,228.64,349.24,343.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Figure A. 1 :</head><label>1</label><figDesc>Figure A.1: Diagnostique d'une panne de voiture par diagramme causal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Figure A. 2 :</head><label>2</label><figDesc>Figure A.2: Inférence de graphe causal par méthode basée sur les contraintes</figDesc><graphic coords="163,79.37,79.37,436.54,103.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>I</head><label></label><figDesc>(X;Y |{U i }, Z) = I(X;Y ) -I(X;Y ; u 1 ) -I(X;Y ; u 2 |u 1 ) -• • • -I(X;Y ; z|{U i })Dans la méthode de référence, tous les sets de séparation {U i } sont essayés jusqu'à ce qu'une indépendance soit trouvée X ⊥ ⊥ Y | {U i }, et les liens X -Y sont testés dans un ordre arbitraire. En commençant par les meilleurs contributeurs, MIIC est moins sensible aux fausses indépendances dues au bruit d'échantillonage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head></head><label></label><figDesc>X] P ; [Y ] Q ) (A.0.1) où le supremum est sur toutes les partitions finies P et Q [4]. Cette définition est correcte sur les populations X et Y , mais quand la taille de l'échantillon N est finie, augmenter le nombre de partitions dans la discrétisation [X] ∆ ou [Y ] ∆ finit inévitablement par surestimer l'information mutuelle I([X] ∆ ; [Y ] ∆ ) (jusqu'au maximum log(N) avec une partition pour chaque valeur unique observée). L'approche développée consiste à maximiser la valeur I ([X] ∆ ; [Y ] ∆ ) corrigée par la complexité stochastique associée à la discrétisation [X] ∆ ; [Y ] ∆ pour prendre en compte les effets du nombre fini d'échantillons (Fig A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head></head><label></label><figDesc>3) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure A. 3 :</head><label>3</label><figDesc>Figure A.3: Estimation de l'information mutuelle par discrétisation. La vraie valeur I(X;Y ) (pointillés horizontaux) est théoriquement obtenue en raffinant la discrétisation ∆ (courbe pointillée) mais est inévitablement surestimée quand la taille de l'échantillon N est finie (courbe en tirets) jusqu'à un maximum log(N). L'information corrigée (courbe rouge) approche la vraie valeur à son maximum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure A. 4 :</head><label>4</label><figDesc>Figure A.4: Discrétisation optimale de trois distributions jointes X,Yi avec la même variable X et trois Y i différents. La même distribution marginale p(X) a des partitions optimales différentes selon la distribution jointe.</figDesc><graphic coords="166,85.11,432.11,139.70,139.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure A. 5 :</head><label>5</label><figDesc>Figure A.5: Benchmarks de reconstruction de réseaux causaux. Résultat sur 50 simulation de 100 noeuds et un pourcentage de noeuds continus entre 10 et 90%. Le F score est calculé comme F = 2 • precision•recall precision+recall</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head>Figure A. 7 :</head><label>7</label><figDesc>Figure A.7: Réseau reconstruit de dossiers médicaux de patients atteints de troubles cognitifs. Les noeuds carrés (respectivement cercles) correspondent à des variables discrètes (respectivement continues). Les arêtes rouges (bleues) correspondent à la corrélation (anticorrélation) entre les variables. Les liens en pointillés reflètent les variables latentes.</figDesc><graphic coords="170,112.10,305.03,370.93,240.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><head>Figure A. 8 :</head><label>8</label><figDesc>Figure A.8: Reconstruction causale sur les données cliniques de patients atteins du cancer du sein.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="169,79.37,160.77,436.53,234.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Learning clinical networks from medical records based on information estimates in mixed-type data. V Cabeli, L Verny, N Sella, G Uguzzoni, M Verny, H Isambert.Interactive</figDesc><table><row><cell>1.3. Research articles</cell><cell cols="2">Chapter 1. Introduction</cell></row><row><cell>Article</cell><cell></cell><cell>Used in</cell></row><row><cell></cell><cell></cell><cell>Sections 3.3, 5.1</cell></row><row><cell>PLoS computational biology 2020</cell><cell></cell></row><row><cell cols="2">Constraint-based Causal Structure Learning with Consistent Separating Sets.</cell><cell>Section 4.2.1</cell></row><row><cell cols="2">H Li, V Cabeli, N Sella, H Isambert. 33rd Conference on Neural Information</cell></row><row><cell>Processing Systems (NeurIPS 2019)</cell><cell></cell></row><row><cell cols="2">Reliable causal discovery based on mutual information supremum principle for</cell><cell>Section 4.3</cell></row><row><cell cols="2">finite datasets. H Li, V Cabeli, M Ribeiro Dantas, H Isambert. "Why-21"</cell></row><row><cell>workshop at NeurIPS 2021</cell><cell></cell></row><row><cell cols="2">A method to learn interpretable causal networks from very large datasets, appli-</cell><cell>Section 4.1.2</cell></row><row><cell cols="2">cation to 400,000 medical records of breast cancer patients. M Ribeiro Dantas,</cell></row><row><cell>H Li, V Cabeli, H Isambert. In preparation</cell><cell></cell></row></table><note><p>data vizualisation and exploration tool of a global clinical network from a large dataset of breast cancer patients treated with neoadjuvant chemotherapy. N Sella*, A-S Hamy*, V Cabeli*, L Darrigues, B Grandal, M Laé, F Reyal, H Isambert. To be submitted Section 5.2 Metabolic Heterogeneity in Hematopoietic Progenitors Fuels Innate Immunity. In preparation Section 5.3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>one could then get the corresponding correlation coefficient (see Fig 3.2).</figDesc><table><row><cell>I (bits)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>ρ</cell></row><row><cell cols="6">Figure 3.2: Value of the mutual information of a bivariate gaussian with correlation coefficient</cell></row><row><cell>ρ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Learning clinical networks based on information estimates in mixed-type data PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1007866 May 18, 2020 5 / 19</figDesc><table><row><cell>64</cell><cell>Chapter 3. Mutual information for constraint-based inference</cell></row><row><cell></cell><cell>variables,</cell></row><row><cell></cell><cell>I 0 N ðX; YjfA i gÞ ¼ I 0 N ðX; Y; fA i gÞ À I 0 N ðX; fA i gÞ</cell><cell>ð14Þ</cell></row><row><cell></cell><cell>¼ I 0 N ðY; X; fA i gÞ À I 0 N ðY; fA i gÞ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table, contains 107 variables of different types (namely, 19 continuous and 88 categorical variables) and heterogeneous nature (i.e., variables related to previous medical history, comorbidities and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table. Dataset from 1,628 eldery patients with cognitive disorders from La Pitie ´-Salpê- trière hospital, Paris.</head><label></label><figDesc>The dataset, fully deidentified, contains 107 variables of different types PLOS COMPUTATIONAL BIOLOGY Learning clinical networks based on information estimates in mixed-type data PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1007866 May 18, 2020 13 / 19 (namely, 19 continuous and 88 categorical variables) and heterogeneous nature (i.e., variables related to previous medical history, comorbidities and comedications, scores from cognitive tests, clinical, biological or radiological examinations, diagnostics and treatments).</figDesc><table><row><cell>S1 File. Supplementary Materials and Methods. Benchmark data generation (continuous</cell></row><row><cell>and discrete variables). Performance measures. Benchmark parameter tuning. Resource avail-</cell></row><row><cell>ability.</cell></row><row><cell>(PDF)</cell></row><row><cell>S1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>This amounts to progressively uncover the best supported conditional independencies, i.e. I(X; Y |{A i } n ) 0, by iteratively "taking off" the most significant indirect contributions of positive conditional 3-point information, I(X; Y ; A k |{A i } k-1 ) &gt; 0, from every 2-point (mutual) information, I(X; Y ), as,</figDesc><table><row><cell>110</cell><cell>Chapter 4. Other improvements to constraint-based algorithms</cell></row><row><cell>2016).</cell><cell></cell></row><row><cell>2 Results</cell><cell></cell></row><row><cell>2.1 MIIC outline</cell><cell></cell></row><row><cell cols="2">MIIC (Multivariate Information-based Inductive Causation) is a novel structure learning method (Verny et al., 2017; Cabeli et al., 2020) and online server (Sella et al., 2018), combining constraint-based and information-theoretic frameworks. Starting from a fully connected graph, MIIC itera-tively removes dispensable edges, by uncovering significant information contributions from indirect paths based on the "3off2" scheme (Affeldt and Isambert, 2015; Affeldt, Verny, and Isambert,</cell></row><row><cell></cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The care pathway of BC patients eligible for neoadjuvant chemotherapy can be summarized as follows: i) pretreatment biopsy for BC diagnosis; ii) administration of chemotherapy as the first-line treatment; iii) removal of the tumor by surgery; iv) histological analysis of the specimens obtained; v)</figDesc><table><row><cell>124</cell><cell>Chapter 5. Applications</cell></row><row><cell>Variables of interest</cell><cell></cell></row><row><cell cols="2">prescription of adjuvant treatments, if indicated (radiotherapy, hormonotherapy, chemotherapy); (vi)</cell></row><row><cell cols="2">patient follow-up to monitor for relapse or death. We identified 94 clinically relevant variables from</cell></row><row><cell cols="2">clinical, radiological, pathological and outcome data, which we grouped into 14 categories (hospital,</cell></row><row><cell cols="2">history, co-medication, comorbidities, clinical baseline, baseline histology, pre-NAC pathology,</cell></row><row><cell cols="2">treatment response, surgery, treatment, changes during NAC, post-NAC pathology, delayed</cell></row><row><cell cols="2">relapse/survival, metastasis). For composite variables derived from raw variables (e.g. BC subtype,</cell></row><row><cell cols="2">constructed from a combination of ER status, PR status, HER2 status), both the derived and raw</cell></row><row><cell>variables were represented on the network.</cell><cell></cell></row><row><cell cols="2">without trastuzumab, followed by surgery, at either of the two Institut Curie sites (Paris and Saint</cell></row><row><cell cols="2">Cloud) between 2002 and 2012 (NEOREP Cohort, CNIL declaration number 1547270). We included</cell></row><row><cell cols="2">unilateral, non-recurrent, non-inflammatory, non-metastatic tumors, and excluded T4 tumors. This</cell></row><row><cell cols="2">study was conducted in accordance with institutional and ethical rules regarding research on tissue</cell></row><row><cell cols="2">specimens and patients. Information on family history, clinical characteristics (age; menopausal status;</cell></row><row><cell cols="2">body mass index) and tumor characteristics (clinical tumor stage and grade; histology; clinical nodal</cell></row><row><cell cols="2">status; ER, PR and HER2 status; BC subtype; mitotic index; Ki67; lymphovascular invasion) was</cell></row><row><cell cols="2">retrieved from electronic medical records. All the patients of the cohort received NAC, and additional</cell></row><row><cell>treatments were decided in accordance with national guidelines.</cell><cell></cell></row><row><cell>6</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The user guide summarizing the main steps for running the MIIC algorithm is accessible at https://miic.curie.fr/user_guide.php, and an online video tutorial is available from: https://miic.curie.fr/tutorial.php. The workbench is available from https://miic.curie.fr/workbench.php. As input data, the user can upload a dataset formatted as a table with commas, semicolons, tabs, pipes or colons, as eld separators, without sample ID. Each variable can be either categorical or quantitative (discrete or continuous). Variables can be grouped into families, identified with different colors on the network. Missing values are allowed in the dataset and their possible statistical biases are taken into account by MIIC7 . They should be indicated as "NA"</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>5.2. NEOREP study on breast cancer patients 127edge orientations are distinguished by the MIIC online server: i) directed edges with a gray arrowhead represent inferred causal relationships; ii) bidirected edges (drawn with dashed lines) reflect the presence of a latent common cause (L) unobserved in the available dataset, i.e. X←(L)→Y; iii) directed edges with a colored (red or blue) arrowhead consistent with either a causal or a latent common cause relationship; and iv) undirected edges, the orientation of which, if indeed there is one, cannot be inferred from non-perturbative data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2 )</head><label>2</label><figDesc>. For example, Happe and Drezen built the ePEPs toolbox, which displays relevant patterns extracted by eye from patient reimbursement data in the SNDS database, and supporting interactive exploration by researchers61 . CARRE provides web-based components for interactive health data (fitness and biomarkers) visualization and risk analysis for the management of cardiorenal diseases62 . The MITRE Corporation has also developed a web-based solution that provides an overview of an individual's health through graphical representations of EHR data, highlighting abnormal values63 . None of these visualization programs has yet managed to bridge the gap between of the large amounts of clinical data available and the discovery of clinical knowledge or paths for scientific research. By processing large heterogeneous sets of variables inherent to clinical records, MIIC provides physicians with a full picture of BC disease.In conclusion, MIIC, an open-access, interactive, multitask tool, is designed to visualize datasets to help clinicians and researchers to understand the relationships between the variables within them. It opens up promising perspectives for guiding the generation of new hypotheses, helping clinicians to identify actionable nodes and edges in clinical networks, and revealing new clues to relationships of interest for research purposes. Its widespread use in the field of health data could increase the accuracy of prediction for treatment responses and prognosis. This tool has the potential to improve the care pathway and, ultimately, patient survival.</figDesc><table><row><cell>In addition to this use for visualization, the MIIC algorithm presents several other advantages</cell></row><row><cell>for analyses, including its unsupervised nature, overcoming the need for training or human</cell></row><row><cell>involvement. This feature makes it possible to obtain new knowledge through the automatic</cell></row><row><cell>identification of patterns and dependences in the data, highlighting new interactions, and it may be of</cell></row><row><cell>use for feature selection in machine learning models.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1007866May 18, 2020  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Funding: HI received funding from IRIS data science program of PSL university, DIM program from Region Ile-de-France and Labex celtisphybio. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.Competing interests:The authors have declared that no competing interests exist.3.3. Publication in PLoS Computation Biology</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="62" xml:id="foot_2"><p>Chapter 3. Mutual information for constraint-based inference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>3.3. Publication in PLoS Computation Biology</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="66" xml:id="foot_4"><p>Chapter 3. Mutual information for constraint-based inference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1007866 May 18, 2020 9 / 19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="74" xml:id="foot_6"><p>Chapter 3. Mutual information for constraint-based inference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="78" xml:id="foot_7"><p>Chapter 3. Mutual information for constraint-based inference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>April 16, 2020   1/33.3. Publication in PLoS Computation Biology</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>April 16, 2020   2/3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>Chapter 3. Mutual information for constraint-based inference</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p>April 16, 2020   3/33.3. Publication in PLoS Computation Biology</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_12"><p>* corresponding author 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.4.2. Consistent separating sets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_13"><p>4.2. Consistent separating sets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_14"><p>4.3. Reliable orientations with mutual information supremum</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_15"><p>Chapter 4. Other improvements to constraint-based algorithms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="144" xml:id="foot_16"><p>Chapter 5. Applications</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements</head></div>
<div><head>Acknowledgments</head><p>We thank <rs type="person">Etienne Birmele ´, Pierre Charbord</rs>, <rs type="person">Eric Gaussier</rs>, <rs type="person">Gregory Nuel</rs>, <rs type="person">Elisabeth Remy</rs>, <rs type="person">Denis Thieffry</rs> for discussions.</p></div>
<div><head>Acknowledgements</head><p>The authors acknowledge financial support from the <rs type="funder">French Ministry of Higher Education and Research</rs>, <rs type="funder">PSL Research University and Sorbonne University</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource availability</head><p>• MIIC R package for mixed-type data is available at this URL: <ref type="url" target="https://miic.curie.fr/download/miic">https://miic.curie.fr/download/miic</ref> mixed.tar.gz • MIIC online server for mixed-type data is accessible here: <ref type="url" target="https://miic.curie.fr/workbench">https://miic.curie.fr/workbench</ref> mixed.php</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Conceptualization: Herve ´Isambert.</p><p>Data curation: Louis Verny, Nadir Sella, Marc Verny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal analysis: Herve ´Isambert.</head><p>Funding acquisition: Herve ´Isambert.</p><p>Investigation: Vincent Cabeli, Louis Verny, Nadir Sella, Guido Uguzzoni, Marc Verny, Herve Ísambert.</p><p>Methodology: Vincent Cabeli, Guido Uguzzoni, Herve ´Isambert.</p><p>Project administration: Herve ´Isambert.</p><p>Resources: Vincent Cabeli, Louis Verny, Nadir Sella, Marc Verny.</p><p>Software: Vincent Cabeli, Nadir Sella.</p><p>Supervision: Marc Verny, Herve ´Isambert.</p><p>Validation: Vincent Cabeli, Louis Verny, Guido Uguzzoni, Marc Verny.</p><p>Visualization: Vincent Cabeli, Nadir Sella.</p><p>Writing -original draft: Vincent Cabeli, Louis Verny, Herve ´Isambert.</p><p>Writing -review &amp; editing: Herve ´Isambert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLOS COMPUTATIONAL BIOLOGY</head><p>Learning clinical networks based on information estimates in mixed-type data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPORTING INFORMATION</head><p>for manuscript</p><p>Learning clinical networks from medical records based on information estimates in mixed-type data Vincent Cabeli, Louis Verny, Nadir Sella, Guido Uguzzoni, Marc Verny, Hervé Isambert</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials and Methods</head><p>Benchmark data generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL on NIPS 2019 paper</head><p>Constraint-based Causal Structure Learning with Consistent Separating Sets Honghao Li, Vincent Cabeli, Nadir Sella, Hervé Isambert Institut Curie, PSL Research University, CNRS UMR168, Paris {honghao.li, vincent.cabeli, nadir.sella, herve.isambert}@curie.fr An R implementation of the methods in the case of the PC-stable algorithm is available with examples at <ref type="url" target="https://github.com/honghaoli42/consistent_pcalg">https://github.com/honghaoli42/consistent_pcalg</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Test of Consistency A.1 Terminology</head><p>A connected graph G is such that there is a path between each pair of vertices of G. A connected component of a graph is a maximal connected subgraph. An articulation point (or cut point) is a vertex in a connected graph whose removal would disconnect the graph and thus increase its number of connected components. A biconnected graph is a connected graph without articulation point. A biconnected component (or block) is a maximal biconnected subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Biconnected Component Analysis</head><p>For a pair (X, Y ) in a graph G, one of the necessary conditions for its separating set to be consistent, as stated in definition 1, is that for each vertex Z in the separating set, Z lies on a path Z XY between X and Y in the skeleton of G. For one pair of vertices, checking the existence of a path for all Z can already be time consuming if the degrees of the vertices are large. In addition, the complexity will be further multiplied by the number of pairs to be considered. Fortunately, it is possible to avoid this high complexity with the help of the biconnected component analysis based on block-cut tree decomposition, and thus to limit the search of consistent separating vertices within those that are consistent with respect to the skeleton. In the following we establish a relation between biconnected components and the path existence problem. Lemma 6 (Menger's theorem for biconnected graph). Let G(V , E) be a biconnected graph, { X, Y } ✓ V a pair of vertices. There is a cycle in G that contains X and Y .</p><p>XY , and H ⇢ H 0 is not a biconnected component of G as it is not maximal. Therefore we must have Z 2 V H .</p><p>If { X, Y, Z } ✓ V H , then lemma 6 guarantees a cycle that contains Z and Y . Since V H contains at least three vertices, such a cycle contains n 1 vertices other than Z and Y , and can be represented by two edge-distinct paths between Z and Y :</p><p>(1)</p><p>where k 2 Z 0 (with k = 0 indicating a direct edge between Z and Y ), n 2 Z + , k &lt; n and { U i } n i=1 are distinct vertices. Since Y is not an articulation point, there is a path XZ that does not contain Y :</p><p>ZY . As a result, if { X, Y, Z } ✓ V H , then there is always a path Z XY . Corollary 8. Let G(V , E) be a connected graph, T (B, C, Br) the block-cut tree decomposition of G, { X, Y } ✓ V a pair of vertices, n X , n Y the corresponding nodes of X and Y in T , and</p><p>The first case is a direct result of theorem 7. The second case is not difficult to prove once we notice the fact that ⌫ XY is the unique path between n X and n Y in T , and that every XY must contain all the cut points in ⌫ XY , and thus can be decomposed into segments of paths between these cut points.</p><p>Each undirected graph G(V , E) can be decomposed into a set of single vertices and a set of connected subgraphs, where each subgraph can be represented by a block-cut tree. Based on this decomposition, algorithm 5 gives the consistent candidate vertices for separating set for a pair of vertices as described in definition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5 Consistent candidates</head><p>Require: (Partially directed) graph G(V , E), its block-cut tree decomposition for each connected component (with respect to its skeleton)</p><p>if X and Y do not belong to the same block-cut tree T i then return ;</p><p>The block-cut tree decomposition can be done beforehand within a single depth first search with complexity O(|V | + |E|). Thus for each pair (X, Y ), the complexity of finding all candidate Z depends on the size of the block-cut tree. In the worst case where G is a forest with only bridges (edges, the removal of each bridge increases the number of connected components of G), the number of nodes and branches in the block-cut tree T of G is of the same order of |V | and |E|, and for all pair of vertices { X, Y } ✓ V we need to perform a path search in T of complexity O(|V | + |E|) to get S. In the best scenario where G is biconnected, S = V \ { X, Y } for all pairs. Then, an operation of set intersection (Ne(X) \ Child(X)) \ S with linear complexity O(|Ne(X)| + |S|) will give the result.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>With the rapid accumulation of information from medical records in health databases, there is an urgent need for innovative interactive tools specifically designed for the exploration of these data by medical practitioners. Here, we report a novel interactive graphical interface for use as the front end of a machine learning causal inference server (MIIC), to facilitate the visualization and comprehension by clinicians of relationships between clinically relevant variables. We demonstrate the utility of the MIIC interactive interface, by exploring the clinical network of a large cohort of breast cancer patients treated with neoadjuvant chemotherapy (NAC). This example highlights, in particular, the direct and indirect links between post-NAC clinical responses and patient survival. The MIIC interactive graphical interface has the potential to help clinicians to identify actionable nodes and edges in clinical networks, thereby ultimately improving the patient care pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significant statement:</head><p>Despite unprecedented amount of information now available in medical records, health data remain underexploited due to their heterogeneity and complexity. Simple charts and hypothesis-driven statistics can no longer apprehend the content of information-rich clinical data. There is, therefore, a clear need for powerful interactive visualization tools enabling medical practitioners to perceive the patterns and insights gained by state-of-the-art machine learning algorithms. We report here an exploratory analysis of a global clinical network from a large breast cancer cohort, with a novel interactive graphical interface for the exploration of health data. The widespread use of such tools, facilitating the interactive exploration of datasets, is crucial both for data visualization and for the generation of research hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tumor samples and pathological review</head><p>In accordance with the guidelines used in France (Group for Evaluation of Prognostic Factors using Immunohistochemistry in Breast Cancer 10 , cases were considered estrogen receptor (ER)positive or progesterone receptor (PR)-positive if at least 10% of the tumor cells expressed estrogen and/or progesterone receptors (ER/PR). Endocrine therapy was prescribed when this threshold was exceeded. HER2-negative status was defined as a score of 0 or 1+ for the tissue section stained by immunohistochemistry (IHC). Tissue sections with scores of IHC 2+ or IHC 3+ were then analyzed by fluorescence in situ hybridization (FISH) to confirm HER2 positivity. BC tumors were classified into subtypes (TNBC, HER2-positive, and luminal HER2-negative [referred to hereafter as "luminal"]). BC subtypes were defined as follows: luminal, ER + or PR + / HER2 -; TNBC, ER -/PR - /HER2 -; HER2-positive BC, HER2 + . Pretreatment core needle biopsy specimens and/or the corresponding post-NAC surgical specimens were reviewed independently by breast disease experts for research purposes, to assess residual cancer burden index, and the levels of tumor-infiltrating lymphocytes. The pathological reviews of these specimens are described in detail elsewhere 11-13 .</p><p>Pathological complete response (pCR) was defined as the absence of residual invasive cancer cells in the breast and axillary lymph nodes (ypT0/is þ/ypN0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Survival endpoints</head><p>Relapse-free survival (RFS) was defined as the time from surgery to death, locoregional recurrence or distant recurrence, whichever occurred first. Overall survival (OS) was defined as the time from surgery to death. The date of last known contact was retained for patients for whom none of these events were recorded. The cutoff date for survival analysis was March, 13 th , 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients</head><p>We discuss below some of the links inferred in the NEOREP network after grouping according to several clinically relevant concepts identified from published studies on BC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC performs quality control</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC identifies inherent associations between variables</head><p>The duration of neoadjuvant treatment is directly linked to the type of NAC regimen delivered (Fig. <ref type="figure">2A</ref>) reflecting the fact that anthracycline-based (AC) regimens usually include four cycles (median of 106 days, Fig. <ref type="figure">2B</ref>), whereas sequential regimens in which anthracyclines are followed by taxanes are generally administered over six or eight cycles (median of 147 days, Fig. <ref type="figure">2B</ref>). The number of nodes retrieved is associated with the type of axillary surgery (Fig. <ref type="figure">2C</ref>), consistent with the fact that sentinel node (SLN) biopsy procedures were developed to reduce the number of lymph nodes removed during dissection (LND) (Fig. <ref type="figure">2D</ref>) 14 . MIIC correctly represents the direct links between residual cancer burden (RCB) (Fig. <ref type="figure">2E</ref>) and the patterns making up this score, derived from measurements on the primary tumor bed (size, fraction of invasive cancer, cellularity) and the regional lymph nodes (number of positive lymph nodes).</p><p>MIIC first identifies relationships between a disease and the corresponding treatment. ER positivitywhich is predictive of efficacy for anti-hormonal treatment 15 is associated with the use of endocrine therapy (Fig. <ref type="figure">S3A</ref>), and a similar association is observed for HER2-positivity and trastuzumab use (Fig. <ref type="figure">S3B</ref>) 16 . Beyond cancer, significant associations are also found between depression and the use of psycholeptics (Fig. <ref type="figure">S3C</ref>), between thyroid disorders and thyroid hormone use (Fig. <ref type="figure">S3D</ref>), and between hypertension and drugs for the treatment of cardiovascular diseases (Fig. <ref type="figure">S3E</ref>). More generally, comedication use is associated with the type of NAC (Fig. <ref type="figure">S3F</ref>), reflecting the greater likelihood of less toxic regimens being prescribed to fragile patients (patients on other types of medication) than to patients without comedication 17-19 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NEOREP study on breast cancer patients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC identifies factors likely to improve prediction or prognosis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC identifies unexpected associations, leading to new discoveries</head><p>With more than 15 associations involving treatment center (Fig. <ref type="figure">3A</ref>), MIIC unmasked an unexpected "batch" effect relating to the site of BC treatment in this cohort. The observed differences reflect not only differences in therapeutic practice, but also differences in the characteristics of the population (differences in the proportion of women with psychological disorders, difference in incomes), differences in tumor presentation (tumor size), differences in pathological variable scoring (grade, presence of pre-NAC LVI, tumor cellularity, TILs), and differences in time to treatment within the care pathway.</p><p>MIIC also favors new discoveries. For example, comedication appears to protect against local relapse (Fig. <ref type="figure">5A</ref>). Several retrospective studies have reported this association, with the use of statins 47 , NSAIDs 48 , or beta-blockers 49 found to have indirect anticarcinogenic effects. It has recently been suggested that these non-oncological treatments may have immunomodulatory and chemosensitizing effects 50 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIIC suggests relevant combinations of predictive of prognostic biomarkers</head><p>MIIC may provide clues to combinations of new prognostic biomarkers likely to improve the prediction of response to chemotherapy, or post-NAC prognosis. Pre-NAC lymphovascular invasion (LVI) was found to be associated with both lower rates of clinical response (Fig. <ref type="figure">5B</ref>) and shorter relapse-free survival (Fig. <ref type="figure">5C</ref>). Both RCB (Fig. <ref type="figure">5D-E</ref>) and post-NAC mitotic index (Fig. <ref type="figure">5D-F</ref>), a parameter rarely used in practice but nevertheless reported to be a predictor of BC recurrence 51, 52 , appear to be strongly associated with the risk of death. MIIC may, therefore, be an efficient tool for identifying features likely to improve prognosis, by combining gold standard indicators with other parameters, such as post-NAC mitotic index, and post-NAC LVI, for example. Finally, MIIC also makes it possible to optimize the binning of residual cancer burden (RCB). RCB is a post-NAC histological score calculated as an increasing continuous index, and then subdivided into four classes Chapter A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé long en français</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contexte scientifique</head><p>La corrélation n'implique pas la causalité, une distinction importante à se rappeler alors que les associations statistiques génèrent de plus en plus de discussions dans un monde toujours plus mesuré et documenté. C'est pourtant le but, avoué ou non, de la plupart des domaines scientifiques : définir les mécanismes de notre environnement qui ont produit ces observations. La nouvelle science de la causalité cherche à nous réconcilier avec ce concept en répondant à ces questions : comment formaliser les relations causales, comment nous les représenter, et quand peut-on les découvrir ? En particulier, les travaux de cette thèse contribuent aux méthodes d'inférence de causalité à partir uniquement de données d'observation. Si la corrélation seule ne suffit pas à inférer une causalité il est en effet possible d'arriver à ce genre de conclusion sans aucune intervention de la part de l'expérimentateur, en observant les bonnes données dans les bonnes conditions.</p><p>Les travaux de cette thèse s'inscrivent dans la théorie principalement développée par Judea Pearl sur les diagrammes causaux; des modèles graphiques qui permettent de dériver toutes les quantités causales d'intérêt (effet du traitement, contrefactuelles...) formellement et intuitivement <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>. Un diagramme causal est un réseau bayésien : un graphe dirigé et acyclique qui encode les indépendances conditionnelles entre les distributions de variables aléatoires représentées par les noeuds; avec une dimension causale retranscrite par la direction des arêtes. Ainsi, si X est un parent de Y , alors nous savons qu'une intervention sur la variable X pour lui donner une distribution arbitraire p(x), notée do(X = p(x)), changera la distribution p(Y |(do(X = p(x))), mais intervenir sur Y ne changera pas la distribution de son parent.</p><p>Considérons une situation familière dans laquelle notre intuition peut être assez naturellement représentée par un diagramme causal <ref type="bibr">(Fig A.1)</ref>. Admettons qu'il y ait deux causes qui puissent être à l'origine d'une panne de voiture, que nous essayons de diagnostiquer avant d'intervenir sur la voiture. Les deux causes considérées, un niveau d'huile trop bas ou pa i , plus un terme de bruit U i : <ref type="bibr">[1]</ref> avec le set P * de toutes des distributions interventionnelles P(v|do(X = x)), X ⊆ V .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Enfin, nous montrons différentes applications de MIIC sur des données mixtes, en collaboration avec les différentes équipes responsables de la collecte des données.</p><p>Le premier réseau est reconstruit à partir de données cliniques de l'hôpital La Pitié-Salpêtrière de 1628 patients âgés atteints de troubles cognitifs. Après traitement du jeu de données, il contient 107 variables de différents types (à savoir 19 variables continues et 88 variables catégorielles) et de nature hétérogène (c'est-à-dire des variables liées aux antécédents médicaux, aux comorbidités et comédications, aux résultats des tests cognitifs, aux examens cliniques, biologiques ou radiologiques, aux diagnostics et aux traitements). Au-delà des différents types et de la nature hétérogène des données enregistrées, les noeuds du réseau clinique (Fig A <ref type="figure">.7</ref>) peuvent être divisés en groupes associés à des troubles spécifiques de la démence et au contexte clinique du patient, y compris les comorbidités (diabète, hypertension, etc.) et les médicaments associés. Le résultat est un réseau créé sans aucune connaissance préalable sur le domaine, que ce soit sur la distinction entre relations directes</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sto ¨gbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.69.066138</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.69.066138" />
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial Mutual Information for Coupling Analysis of Multivariate Time Series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.99.204101</idno>
		<idno type="PMID">18233144</idno>
		<ptr target="https://doi.org/10.1103/PhysRevLett.99.204101" />
	</analytic>
	<monogr>
		<title level="j">Phys Rev Lett</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">204101</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inferring the directionality of coupling with conditional mutual information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ˇm</forename><surname>Palus</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.77.026214</idno>
		<ptr target="https://doi.org/10.1103/PhysRevE.77.026214" />
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">26214</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nearest neighbor estimate of conditional mutual information in feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsimpiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kugiumtzis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2012.05.014</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2012.05.014" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="12697" to="12708" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutual information between discrete and continuous data sets</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0087357</idno>
		<idno type="PMID">24586270</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0087357" />
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">87357</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating mutual information for discrete-continuous mixtures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5986" to="5997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jackknife approach to the estimation of mutual information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1715593115</idno>
		<ptr target="https://doi.org/10.1073/pnas.1715593115" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="9956" to="9961" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics<address><addrLine>Playa Blanca, Lanzarote, Canary Islands</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="938" to="947" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-1098(78)90005-5</idno>
		<ptr target="https://doi.org/10.1016/0005-1098(78)90005-5" />
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MDL Histogram Density Estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><surname>Myllyma ¨ki P</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="226" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1005662</idno>
		<idno type="PMID">28968390</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1005662" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1005662</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MIIC online: a web server to reconstruct causal or non-causal networks from non-perturbative data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx844</idno>
		<idno type="PMID">29300827</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btx844" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2311" to="2313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3off2: A network reconstruction algorithm based on 2-point and 3-point information statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0856-x</idno>
		<idno type="PMID">26823190</idno>
		<ptr target="https://doi.org/10.1186/s12859-015-0856-x" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian Network Structure Learning using Factorized NML Universal Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myllyma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ITA&apos;08</title>
		<meeting>ITA&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A linear-time algorithm for computing the multinomial stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myllyma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ipl.2007.04.003</idno>
		<ptr target="https://doi.org/10.1016/j.ipl.2007.04.003" />
	</analytic>
	<monogr>
		<title level="j">Inf Process Lett</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Average case analysis of algorithms on sequences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient computation of stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myllyma ¨ki P, Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Bishop</surname></persName>
		</editor>
		<editor>
			<persName><surname>Frey</surname></persName>
		</editor>
		<meeting>the Ninth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Computationally Efficient Methods for MDL-Optimal Density Estimation and Data Clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Part mutual information for quantifying direct associations in networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1522586113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1522586113" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5130" to="5135" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting Novel Associations in Large Data Sets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Finucane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mcvean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Turnbaugh</surname></persName>
		</author>
		<idno type="DOI">10.1126/science</idno>
		<idno type="PMID">22174245</idno>
		<ptr target="https://doi.org/10.1126/science" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">6062</biblScope>
			<biblScope unit="page" from="1518" to="1524" />
			<date type="published" when="2011">2011. 1205438</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference. 2nd ed</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixed graphical models for integrative causal analysis with application to chronic lung disease diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sedgewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buschur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Manatakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery with mixed data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsagris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borboudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<idno type="PMID">30957008</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bu ¨hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="DOI">10.1214/14-AOS1260</idno>
		<ptr target="https://doi.org/10.1214/14-AOS1260" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bu ¨hlmann P. Causal inference using graphical models with the R package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma ¨chler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v047.i11</idno>
		<ptr target="https://doi.org/10.18637/jss.v047.i11" />
	</analytic>
	<monogr>
		<title level="j">J Stat Softw</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prevalence of orthostatic hypotension in Parkinson&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Senard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raï</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapeyre-Mestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rascol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rascol</surname></persName>
		</author>
		<idno type="DOI">10.1136/jnnp.63.5.584</idno>
		<ptr target="https://doi.org/10.1136/jnnp.63.5.584" />
	</analytic>
	<monogr>
		<title level="j">Neurosurgery &amp; Psychiatry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="584" to="589" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Journal of Neurology</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Diagnostic accuracy of 123 I-FP-CIT (DaTS-CAN) in dementia with Lewy bodies: a meta-analysis of published studies. Parkinsonism &amp; related disorders</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Papathanasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boutsiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Bomanji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parkreldis.2011.09.015</idno>
		<ptr target="https://doi.org/10.1016/j.parkreldis.2011.09.015" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="225" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sensitivity to semantic cuing: an index of episodic memory dysfunction in early Alzheimer disease</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tounsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deweer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ergis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Michon</surname></persName>
		</author>
		<idno type="DOI">10.1097/00002093-199903000-00006</idno>
		<idno type="PMID">10192641</idno>
		<ptr target="https://doi.org/10.1097/00002093-199903000-00006" />
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
	<note>Alzheimer Dis Assoc Disord</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Free and Cued Selective Reminding Test-accuracy for the differential diagnosis of Alzheimer&apos;s and neurodegenerative diseases: A large-scale biomarker-characterized monocenter cohort study (ClinAD)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Michon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hampel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jalz.2016.12.014</idno>
		<ptr target="https://doi.org/10.1016/j.jalz.2016.12.014" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="913" to="923" />
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s &amp; Dementia</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MR signal abnormalities at 1.5 T in Alzheimer&apos;s dementia and normal aging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chawluk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hurtig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmerman</surname></persName>
		</author>
		<idno type="DOI">10.2214/ajr</idno>
		<ptr target="https://doi.org/10.2214/ajr" />
	</analytic>
	<monogr>
		<title level="j">Am J Roentgenology</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="356" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Atrophy of medial temporal lobes on MRI in &quot;probable&quot; Alzheimer&apos;s disease and normal ageing: diagnostic value and neuropsychological correlates</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scheltens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Barkhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huglo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vermersch</surname></persName>
		</author>
		<idno type="DOI">10.1136/jnnp.55.10.967</idno>
		<ptr target="https://doi.org/10.1136/jnnp.55.10.967" />
	</analytic>
	<monogr>
		<title level="j">Neurosurgery &amp; Psychiatry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="967" to="972" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>Journal of Neurology</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">White matter hyperintensities are associated with disproportionate progressive hippocampal atrophy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fiford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ridgway</surname></persName>
		</author>
		<idno type="DOI">10.1002/hipo.22690</idno>
		<idno type="PMID">27933676</idno>
		<ptr target="https://doi.org/10.1002/hipo.22690" />
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">White matter hyperintensities, cognitive impairment and dementia: an update</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Prins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheltens</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrneurol.2015.10</idno>
		<idno type="PMID">25686760</idno>
		<ptr target="https://doi.org/10.1038/nrneurol.2015.10" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neurology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">and minepy: a C engine for the MINE suite and its R, Python and MATLAB wrappers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Furlanello</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albanese</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Filosi</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Visintainer</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riccadonna</surname></persName>
			<affiliation>
				<orgName type="collaboration">minerva</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bts707</idno>
		<idno type="PMID">23242262</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/bts707" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="408" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Equitability, mutual information, and the maximal information coefficient</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Atwal</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1309933111</idno>
		<ptr target="https://doi.org/10.1073/pnas.1309933111" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3354" to="3359" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">JIDT: An information-theoretic toolkit for studying the dynamics of complex systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<idno type="DOI">10.3389/frobt.2014.00011</idno>
		<ptr target="https://doi.org/10.3389/frobt.2014.00011" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scho ¨lkopf B. Kernel methods for measuring independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2075" to="2129" />
			<date type="published" when="2005-12">2005. Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1847" to="1855" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating connected acyclic digraphs uniformly at random</title>
		<author>
			<persName><forename type="first">G</forename><surname>Melançon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="209" to="213" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mixed graphical models for integrative causal analysis with application to chronic lung disease diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sedgewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buschur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Manatakis</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty769</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery with mixed data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsagris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borboudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Global optimization of lipschitz functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Malherbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vayatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2314" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dlib-ml: A Machine Learning Toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernel methods for measuring independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2075" to="2129" />
			<date type="published" when="2005-12">2005. Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1847" to="1855" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information</title>
		<author>
			<persName><forename type="first">S</forename><surname>References Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3off2: A network reconstruction algorithm based on 2-point and 3-point information statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models with latent variables: A general sat-based procedure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI&apos;13</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI&apos;13<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robustification of the pc-algorithm for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Of Computational And Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="773" to="789" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Causal inference using graphical models with the R package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic Graphical Models: Principles and Techniques</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Second International Conference on Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ancestral graph markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The tetrad project: Constraint based aids to causal model specification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Bayesian Networks with the bnlearn R Package</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Miic online: a web server to reconstruct causal or non-causal networks from non-perturbative data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2311" to="2313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An algorithm for causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation, Causation, and Discovery</title>
		<meeting><address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Max-Min Hill-Climbing Bayesian Network Structure Learning Algorithm</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1005662</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Other improvements to constraint-based algorithms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information</title>
		<author>
			<persName><forename type="first">S</forename><surname>References Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI 2015)</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3off2: A network reconstruction algorithm based on 2-point and 3-point information statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning clinical networks from medical records based on information estimates in mixed-type data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1007866</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Partial mutual information for coupling analysis of multivariate time series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">204101</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Other improvements to constraint-based algorithms</title>
		<author>
			<orgName type="collaboration">116 Chapter</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Causal inference using graphical models with the R package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A linear-time algorithm for computing the multinomial stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Constraint-based Causal Structure Learning with Consistent Separating Sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. NeurIPS 2019</date>
			<biblScope unit="page" from="14257" to="14266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Melancon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:cs/0403040</idno>
		<idno>arXiv: cs/0403040</idno>
		<title level="m">Generating connected acyclic digraphs uniformly at random</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Representation and Reasoning: Proc. of the Second Int. Conf</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Adjacency-faithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence, UAI</title>
		<meeting>the 22nd Conference on Uncertainty in Artificial Intelligence, UAI<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning using factorized nml universal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2008 Information Theory and Applications Workshop</title>
		<meeting>2008 Information Theory and Applications Workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>ITA-2008</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Learning bayesian networks with the bnlearn r package</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Miic online: a web server to reconstruct causal or non-causal networks from non-perturbative data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2311" to="2313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<idno>1005662. 10</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Health Data Visualization-A review * Seminar Collaborative Data Visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bärtschi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Big Data Application in Biomedical Research and Health Care: A Literature Review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopukumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed Inform Insights</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Beyond simple charts: Design of visualizations for big health data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sedig</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title/>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5302463/" />
	</analytic>
	<monogr>
		<title level="j">Online J Public Health Inform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2016">2016. 2019 Aug 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Improving Healthcare with Interactive Visualization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Hesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">MIIC online: a web server to reconstruct causal or non-causal networks from non-perturbative data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2311" to="2313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Learning clinical networks from medical records based on information estimates in mixed-type data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title/>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7259796/" />
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2020">2020. 2021 Feb 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Neoadjuvant treatment for intermediate/high-risk HER2-positive and triple-negative breast cancers: no longer an &quot;option</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A-S</forename><surname>Hamy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>but an ethical obligation. ESMO Open 4:e000515</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Neoadjuvant treatment: the future of patients with breast cancer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Piccart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESMO Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">371</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Recommendations for the immunohistochemistry of the hormonal receptors on paraffin sections in breast cancer. Update</title>
	</analytic>
	<monogr>
		<title level="m">Group for Evaluation of Prognostic Factors using Immunohistochemistry in Breast Cancer</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>GEFPICS-FNCLCC)</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Ann Pathol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="336" to="343" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Stromal lymphocyte infiltration after neoadjuvant chemotherapy is associated with aggressive residual disease and lower disease-free survival in HER2-positive breast cancer</title>
		<author>
			<persName><forename type="first">A-S</forename><surname>Hamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Pierga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabaila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Oncol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2233" to="2240" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Lymphovascular invasion after neoadjuvant chemotherapy is strongly associated with poor prognosis in breast carcinoma</title>
		<author>
			<persName><forename type="first">A-S</forename><surname>Hamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-T</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Res Treat</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="295" to="304" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Pathological complete response and prognosis after neoadjuvant chemotherapy for HER2-positive breast cancers before and after trastuzumab era: results from a real-life cohort</title>
		<author>
			<persName><forename type="first">A-S</forename><surname>Hamy-Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Belin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bonsang-Kitzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br J Cancer</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Sentinel-lymph-node biopsy as a staging procedure in breast cancer: update of a randomised controlled study</title>
		<author>
			<persName><forename type="first">U</forename><surname>Veronesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paganelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Viale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="983" to="990" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Adjuvant Endocrine Therapy for Women With Hormone Receptor-Positive Breast Cancer: American Society of Clinical Oncology Clinical Practice Guideline Focused Update</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Oncol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Herceptin® (trastuzumab) in HER2-positive early breast cancer: protocol for a systematic review and cumulative network meta-analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wylie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst Rev</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Prognostic factors for the feasibility of chemotherapy and the Geriatric Prognostic Index (GPI) as risk profile for mortality before chemotherapy in the elderly</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Aaldriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maartense</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hjwr</forename><surname>Nortier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Oncol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Potential drug interactions in cancer therapy: a prevalence study using an advanced screening method</title>
		<author>
			<persName><forename type="first">Rwf</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Oncol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2334" to="2341" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Potential drug interactions and chemotoxicity in older patients with cancer receiving chemotherapy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Geriatr Oncol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="307" to="314" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Lipid profile comparison between pre-and post-menopausal women]</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zhonghua Xin Xue Guan Bing Za Zhi</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="799" to="804" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Correlation of Ki-67 antigen expression with mitotic figure index and tumor grade in breast carcinomas using the novel &quot;paraffin&quot;-reactive MIB1 antibody</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weidner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vartanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum Pathol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="337" to="342" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Estimation of tumor size in breast cancer comparing clinical examination, mammography, ultrasound and MRI-correlation with the pathological analysis of the surgical specimen</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cortadellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Argacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Acosta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gland Surg</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="330" to="335" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Diagnostic accuracy of mammography, clinical examination, US, and MR imaging in preoperative assessment of breast cancer</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nessaiver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="page" from="830" to="849" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Impact of breast cancer molecular subtypes on locoregional recurrence in patients treated with neoadjuvant chemotherapy for locally advanced breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Klauber-Demore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Ollila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Surg Oncol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2851" to="2857" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Locoregional recurrence after breast cancer surgery: a systematic review by receptor phenotype</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lowery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Res Treat</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="831" to="841" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Predictors of re-excision among women undergoing breast-conserving surgery for cancer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Waljee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Surg Oncol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1297" to="1303" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Differences in Response and Surgical Management with Neoadjuvant Chemotherapy in Invasive Lobular Versus Ductal Breast Cancer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Truin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vugts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rmh</forename><surname>Roumen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Surg Oncol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Oncoplastic breast surgery: indications, techniques and perspectives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Munhoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Montag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemperli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gland Surg</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="157" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Radiation Therapy for Early-Stage Breast Cancer after Breast-Conserving Surgery</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="63" to="70" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Invasive breast cancer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Allred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Natl Compr Canc Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="136" to="222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">National Institutes of Health Consensus Development Conference Statement: adjuvant therapy for breast cancer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eifel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Axelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="979" to="989" />
			<date type="published" when="2000">November 1-3, 2000. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Conservative surgery and radiation in the treatment of stage I and II carcinoma of the breast. American College of Radiology. ACR Appropriateness Criteria</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Halberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Shank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Haffty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="1193" to="1205" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Adjuvant Capecitabine for Breast Cancer after Preoperative Chemotherapy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ohtani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="2147" to="2159" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Factors associated with breast cancer mortality after local recurrence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Oncol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Ipsilateral breast tumor recurrence postlumpectomy is predictive of subsequent mortality: results from a randomized trial. Investigators of the Ontario Clinical Oncology Group</title>
		<author>
			<persName><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Radiat Oncol Biol Phys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="11" to="16" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">The prognostic significance of late local recurrence after breast-conserving therapy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Spitalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amalric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Radiat Oncol Biol Phys</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">The relationship between local recurrence and death in early-stage breast cancer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sopik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nofech-Mozes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Res Treat</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Survival after Locoregional Recurrence or Second Primary Breast Cancer: Impact of the Disease-Free Interval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Witteveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abg</forename><surname>Kwast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sonke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">120832</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Proposal for standardized definitions for efficacy end points in adjuvant breast cancer trials: the STEEP system</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Costantino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Oncol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2127" to="2132" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Breast Cancer Subtypes and the Risk of Local and Regional Relapse</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Voduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mcu</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyldesley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCO</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1684" to="1691" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Pattern of Local Recurrence and Distant Metastasis in Breast Cancer By Molecular Subtype</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasymjanova</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title/>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5222631/" />
	</analytic>
	<monogr>
		<title level="j">Cureus</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2016">2016. 2021 Feb 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Absent progesterone receptor expression in the lymph node metastases of ER-positive, HER2negative breast cancer is associated with relapse on tamoxifen</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Pathol</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="954" to="960" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Changes in the ER, PgR, HER2, p53 and Ki-67 biological markers between primary and recurrent breast cancer: discordance rates and prognosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Osako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World J Surg Oncol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Evaluation of factors related to late recurrence--later than 10 years after the initial treatment-in primary breast cancer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Osako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="100" to="110" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Hormone receptors status: a strong determinant of the kinetics of brain metastases occurrence compared with HER2 status in breast cancer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darlix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Griguolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thezenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEOREP study on breast cancer patients</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2002">2018 5.2</date>
		</imprint>
	</monogr>
	<note>J Neurooncol</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Progesterone suppresses triple-negative breast cancer growth and metastasis to the brain via membrane progesterone receptor α</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Mol Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="755" to="761" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Statin prescriptions and breast cancer recurrence risk: a Danish nationwide prospective cohort study</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tarp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="1461" to="1468" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">NSAIDs and Breast Cancer Recurrence in a Prospective Cohort Study</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Habel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Causes Control</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Beta-Blocker Drug Therapy Reduces Secondary Cancer Formation in Breast Cancer and Improves Cancer Specific Survival</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Powe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Zänker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncotarget</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="628" to="638" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Comedications influence immune infiltration and pathological response to neoadjuvant chemotherapy in breast cancer</title>
		<author>
			<persName><forename type="first">A-S</forename><surname>Hamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Derosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Valdelièvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OncoImmunology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1677427</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Mitotic index to predict breast cancer recurrence after neoadjuvant systemic therapy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Farrugia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Landmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Diego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCO</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23265" to="e23265" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Value of mitotic index in residual tumors following neoadjuvant therapy for breast cancer: Single institution experience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pattali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Visotcky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCO</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="548" to="548" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Measurement of Residual Breast Cancer Burden to Predict Survival After Neoadjuvant Chemotherapy</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Symmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peintinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Oncology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="4414" to="4422" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">The Surveillance, Epidemiology and End Results (SEER) Program and Pathology: Towards Strengthening the Critical Relationship</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altekruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Surg Pathol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="94" to="e102" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">NCI SEER Public-Use Data: Applications and Limitations in Oncology Research</title>
		<imprint/>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title/>
		<ptr target="https://www.cancernetwork.com/oncology-journal/nci-seer-public-use-data-applications-and-limitations-oncology-research" />
	</analytic>
	<monogr>
		<title level="j">Cancer Network</title>
		<imprint>
			<date type="published" when="2009">2009. 2019 Aug 27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Using the National Cancer Database for Outcomes Research: A Review</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Boffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Oncol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1722" to="1728" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">The national healthcare system claims databases in France, SNIIRAM and EGB: Powerful tools for pharmacoepidemiology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bezin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lassalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pharmacoepidemiol Drug Saf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="954" to="962" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Value of a national administrative database to guide public decisions: From the système national d&apos;information interrégimes de l&apos;Assurance Maladie (SNIIRAM) to the système national des données de santé (SNDS) in France</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tuppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rudant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Constantinou</surname></persName>
		</author>
		<idno>Internet]. /data/revues/03987620/v65sS4/S0398762017304315</idno>
		<ptr target="https://www.em-consulte.com/en/article/1140905" />
		<imprint>
			<date type="published" when="2017">2017. 2019 Aug 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">The National Institutes of Health&apos;s Big Data to Knowledge (BD2K) initiative: capitalizing on biomedical big data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Margolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="957" to="958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-70956-5_7</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-540-70956-5_7" />
		<title level="m">Visual Analytics: Definition, Process, and Challenges</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J-D</forename><surname>Fekete</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2019 Aug 14</date>
			<biblScope unit="page" from="154" to="175" />
		</imprint>
	</monogr>
	<note>Information Visualization</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">A visual approach of care pathways from the French nationwide SNDS database -from population to individual records: the ePEPS toolbox</title>
		<author>
			<persName><forename type="first">A</forename><surname>Happe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Drezen</surname></persName>
		</author>
		<ptr target="https://hal-univ-rennes1.archives-ouvertes.fr/hal-01697626" />
		<imprint>
			<date type="published" when="2018">2018. 2019 Aug 18</date>
		</imprint>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Visual Analytics for Health Monitoring and Risk Management in CARRE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Parvinzamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<publisher>E-Learning and Games</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Revised Selected Papers</title>
		<imprint>
			<biblScope unit="volume">9654</biblScope>
			<biblScope unit="page" from="380" to="391" />
			<date type="published" when="2016-04-14">2016. April 14-16, 2016. 2016</date>
			<pubPlace>Hangzhou, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Health figures: an open source JavaScript library for health data visualization [Internet]</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ledesma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Musawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nieminen</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4802654/Bibliography" />
	</analytic>
	<monogr>
		<title level="j">BMC Med Inform Decis Mak</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2016">2016. 2019 Aug 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl; Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<title level="m">Causal inference in statistics: a primer</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<title level="m">The Book of Why: The New Science of Cause and Effect</title>
		<imprint>
			<publisher>Basic Books</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Robust reconstruction of causal graphical models based on conditional 2-point and 3-point information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UAI 2015 Conference on Advances in Causal Inference</title>
		<meeting>the UAI 2015 Conference on Advances in Causal Inference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1504</biblScope>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
		<title level="m">Constraint-based Causal Structure Learning with Consistent Separating Sets&quot;; in &quot;Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14257" to="14266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Learning clinical networks from medical recordsbased on information estimates in mixed-type data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">The design versus the analysis of observational studies for causal effects: parallels with the design of randomized trials</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1002/sim.2739</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2739" />
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<idno type="ISSN">1097-0258</idno>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/70.1.41</idno>
		<ptr target="https://doi.org/10.1093/biomet/70.1.41" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Principal components analysis corrects for stratification in genome-wide association studies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Plenge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Weinblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Shadick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reich</surname></persName>
		</author>
		<idno type="DOI">10.1038/ng1847</idno>
		<ptr target="https://www.nature.com/articles/ng1847" />
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<idno type="ISSN">1546-1718</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="904" to="909" />
			<date type="published" when="2006">2006</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
	<note>bandiera_abtest: a Cg_type: Nature Research Journals Number: 8 Primary_atype: Research Publisher</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Instrumental variables: application and limitations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Pestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Belitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Klungel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="page" from="260" to="267" />
			<date type="published" when="2006">2006</date>
			<publisher>JSTOR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<date type="published" when="1991">1991</date>
			<pubPlace>Los Angeles, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Publisher: UCLA</note>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman;</surname></persName>
		</author>
		<title level="m">Learning gaussian networks&quot;; in &quot;Uncertainty Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-33486-6_1</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/3-540-33486-6_1" />
		<title level="m">A Bayesian Approach to Causal Discovery&quot;; in &quot;Innovations in Machine Learning</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06241</idno>
		<title level="m">Likelihoods and parameter priors for Bayesian networks&quot;; arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Restricted structural equation models for causal inference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ETH Zurich</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science computer review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6843</idno>
		<title level="m">Adjacency-faithfulness and conservative causal inference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<title level="m">Causal inference and causal explanation with background knowledge&quot;; in &quot;Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="294" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Measuring and testing dependence by correlation of distances</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bakirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2769" to="2794" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Brownian distance covariance</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo;</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-AOAS312</idno>
		<ptr target="https://projecteuclid.org/euclid.aoas/1267453933" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<idno type="ISSN">1932-6157</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1941" to="7330" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">A non-parametric test of independence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<publisher>JSTOR</publisher>
			<date type="published" when="1948">1948</date>
			<biblScope unit="page" from="546" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Equitability, mutual information, and the maximal information coefficient</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Atwal</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1309933111</idno>
		<ptr target="http://www.pnas.org/content/111/9/3354" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424, 1091-6490</idno>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="3354" to="3359" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Kernel methods for measuring independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2075" to="2129" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<title level="m">Measuring Statistical Dependence with Hilbert-Schmidt Norms</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">A kernel statistical test of independence.&quot; in &quot;Nips</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A kernel-based causal learning algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="855" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models&quot;; in &quot;Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tillman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1847" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3775</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Inferring regulatory networks from expression data using tree-based methods</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Irrthum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts;</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Publisher: Public Library of Science</publisher>
			<pubPlace>San Francisco, USA</pubPlace>
		</imprint>
	</monogr>
	<note>PloS one 5, p. e12776</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">3off2: A network reconstruction algorithm based on 2-point and 3-point information statistics&quot;; in</title>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016</date>
			<publisher>BioMed Central Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">CAM: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernest</surname></persName>
		</author>
		<idno type="DOI">10.1214/14-AOS1260</idno>
		<idno type="arXiv">arXiv:1310.1533</idno>
		<ptr target="http://arxiv.org/abs/1310.1533" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01422</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric DAGs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06826</idno>
		<idno>arXiv: 1805.06826</idno>
		<ptr target="http://arxiv.org/abs/1805.06826" />
		<title level="m">The Blessings of Multiple Causes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen;</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssb.12167/abstract</idno>
		<ptr target="http://onlinelibrary.wiley.com/doi/10.1111/rssb.12167/abstract" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<idno type="ISSN">1467-9868</idno>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Invariant Causal Prediction for Nonlinear Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2017-0016</idno>
		<ptr target="https://www.degruyter.com/view/j/jci.2018.6.issue-2/jci-2017-0016/jci-2017-0016.xml" />
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Learning causal networks with latent variables from multivariate information in genomic data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">e1005662</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<title level="m">A mathematical theory of communication&quot;; The Bell system technical journal 27</title>
		<imprint>
			<publisher>Nokia Bell Labs</publisher>
			<date type="published" when="1948">1948</date>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Reshef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Finucane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mcvean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Turnbaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Sabeti</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1205438</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3325791/" />
		<title level="m">Detecting Novel Associations in Large Datasets&quot;; Science</title>
		<meeting><address><addrLine>New York, N.y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="page" from="1518" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Scientists rise up against statistical significance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Amrhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcshane</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-019-00857-9</idno>
		<idno>41586-019-00857-9</idno>
		<ptr target="https://www.nature.com/articles/d" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">567</biblScope>
			<biblScope unit="page" from="305" to="307" />
			<date type="published" when="2019">2019</date>
			<publisher>Nature Publishing Group Subject_term: Research data</publisher>
		</imprint>
	</monogr>
	<note>bandiera_abtest: a Cg_type: Comment Number: 7748 Publisher. Research management</note>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Five ways to fix statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Mcshane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-017-07522-z</idno>
		<ptr target="https://www.nature.com/articles/d41586-017-07522-z;bandiera_abtest" />
	</analytic>
	<monogr>
		<title level="m">a Cg_type: Comment Number: 7682 Publisher: Nature Publishing Group Subject_term: Research data, Lab life, Mathematics and computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">551</biblScope>
			<biblScope unit="page" from="557" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Mutual information between discrete variables with many categories using recursive adaptive partitioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10981</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">A bayesian nonparametric conditional two-sample test with an application to local causal discovery</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Boeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07382</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06212</idno>
		<idno>arXiv: 1709.06212</idno>
		<ptr target="http://arxiv.org/abs/1709.06212" />
		<title level="m">Estimating Mutual Information for Discrete-Continuous Mixtures</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Jackknife approach to the estimation of mutual information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="9956" to="9961" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:physics/0004057</idno>
		<idno>arXiv: physics/0004057</idno>
		<ptr target="http://arxiv.org/abs/physics/0004057" />
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02406</idno>
		<idno>arXiv: 1503.02406</idno>
		<ptr target="http://arxiv.org/abs/1503.02406" />
		<title level="m">Deep Learning and the Information Bottleneck Principle</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry_WPG-A-" />
	</analytic>
	<monogr>
		<title level="j">On the Information Bottleneck Theory of Deep Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Estimating Information Flow in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Goldfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Polyanskiy</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/goldfeld19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09671</idno>
		<idno>arXiv: 2003.09671</idno>
		<ptr target="http://arxiv.org/abs/2003.09671" />
		<title level="m">On Information Plane Analyses of Neural Network Classifiers -A Review</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization&quot;; arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-008-9096-4</idno>
		<ptr target="https://doi.org/10.1007/s11023-008-9096-4" />
		<title level="m">Detection of Unfaithfulness and Robust Causal Inference&quot;; Minds and Machines</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="239" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Causal Discovery with Continuous Additive Noise Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2009" to="2053" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14265</idno>
		<title level="m">A Weaker Faithfulness Assumption based on Triple Interactions&quot;; arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Wieczorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roth;</surname></persName>
		</author>
		<idno type="DOI">10.3390/e21100975</idno>
		<idno>number: 10 Publisher</idno>
		<ptr target="https://www.mdpi.com/1099-4300/21/10/975" />
	</analytic>
	<monogr>
		<title level="j">Information Theoretic Causal Effect Quantification</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">975</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
	<note>Entropy</note>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<title level="m">Estimation of entropy and mutual information&quot;; Neural computation 15</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1191" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">Note on the bias of information estimates&quot;; Information theory in psychology</title>
		<imprint>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176345462.full</idno>
		<ptr target="https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-3/The-Jackknife-Estimate-of-Variance/10.1214/aos/1176345462.full" />
		<title level="m">The Jackknife Estimate of Variance&quot;; The Annals of Statistics</title>
		<imprint>
			<publisher>Institute of Mathematical Statistics</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2168" to="8966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Entropy estimation of symbol sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Chaos</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger;</surname></persName>
			<affiliation>
				<orgName type="collaboration">Chaos</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1063/1.166191</idno>
		<idno type="arXiv">arXiv:cond-mat/0203436</idno>
		<ptr target="http://arxiv.org/abs/cond-mat/0203436" />
	</analytic>
	<monogr>
		<title level="j">An Interdisciplinary Journal of Nonlinear Science</title>
		<idno type="ISSN">1054-1500</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1089" to="7682" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Entropy and information in neural spike trains: Progress on the sampling problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D R</forename><surname>Van Steveninck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">56111</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>APS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Inferring the directionality of coupling with conditional mutual information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">26214</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<monogr>
		<title level="m" type="main">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
	<note>Signal processing 16</note>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357</idno>
		<ptr target="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357" />
		<title level="m">Mutual Information between Discrete and Continuous Data Sets&quot;; PLOS ONE 9</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">87357</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Estimating mutual information using B-spline functions-an improved similarity measure for analysing gene expression data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Daub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Steuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selbig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kloska;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Estimation of the information by an adaptive partitioning of the observation space</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Darbellay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1315" to="1321" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Divergence estimation of continuous distributions based on data-dependent partitions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3064" to="3074" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Entropy expressions for multivariate continuous distributions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Darbellay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.825848</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<idno type="ISSN">0018-9448</idno>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="709" to="712" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Estimation of mutual information using kernel density estimators</title>
		<author>
			<persName><forename type="first">Y.-I</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lall</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.52.2318</idno>
		<ptr target="https://link.aps.org/doi/10.1103/PhysRevE.52.2318" />
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="2318" to="2321" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger;</surname></persName>
		</author>
		<title level="m">Estimating mutual information&quot;; Physical review E 69</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">66138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Sample estimate of the entropy of a random vector</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Kozachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Leonenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publisher: Russian Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="1987">1987</date>
			<publisher>Branch of Informatics, Computer Equipment</publisher>
		</imprint>
	</monogr>
	<note>Problemy Peredachi Informatsii</note>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Nearest neighbor estimates of entropy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hnizdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demchuk;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of mathematical and management sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="301" to="321" />
			<date type="published" when="2003">2003</date>
			<publisher>Publisher: Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08824</idno>
		<idno>arXiv: 1711.08824</idno>
		<ptr target="http://arxiv.org/abs/1711.08824" />
		<title level="m">The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Demystifying Fixed k-Nearest Neighbor Information Estimators</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath;</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03006</idno>
		<idno>arXiv: 1604.03006</idno>
		<ptr target="http://arxiv.org/abs/1604.03006" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Nearest neighbor estimate of conditional mutual information in feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsimpiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kugiumtzis;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="12697" to="12708" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-Cruz</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2008/hash/ccb0989662211" />
		<title level="m">Estimation of Information Theoretic Measures for Continuous Random Variables&quot;; in &quot;Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
	<note>61edae2e26d58ea92f-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">Estimation of mutual information for real-valued data with error bars and controlled bias</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<idno type="DOI">10.1101/589929v2-0</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/589929v2-0" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">589929</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v84/runge18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="938" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Nonparametric independence testing via mutual information</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Berrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asz024</idno>
		<ptr target="https://academic.oup.com/biomet/article/106/3/547/5511208" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">0006-3444</idno>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="547" to="566" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Ensemble estimation of mutual information</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sricharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3030" to="3034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rousselet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Schyns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1541" to="1573" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Noshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero Iii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09125</idno>
		<idno>arXiv: 1801.09125</idno>
		<ptr target="http://arxiv.org/abs/1801.09125" />
		<title level="m">Scalable Mutual Information Estimation using Dependence Graphs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">MINE: mutual information neural estimation&quot;; arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<title level="m">Estimating Conditional Mutual Information for Discrete-Continuous Mixtures using Multi-Dimensional Adaptive Histograms&quot;; in &quot;Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymaki</surname></persName>
		</author>
		<title level="m">Bayesian network structure learning using factorized NML universal models&quot;; in &quot;2008 Information Theory and Applications Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="272" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
		<title level="m">Stochastic Complexity for Testing Conditional Independence on Discrete Data&quot;; in &quot;NeurIPS 2018 Workshop on Causal Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.481776</idno>
	</analytic>
	<monogr>
		<title level="m">Fisher information and stochastic complexity</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Information Theory</note>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Publisher: Russian Academy of Sciences, Branch of Informatics</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Shtar'kov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3" to="17" />
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Computer Equipment and . . .</orgName>
		</respStmt>
	</monogr>
	<note>Universal sequential coding of single messages</note>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">An MDL Framework for Data Clustering&quot;; in &quot;Advances in Minimum Description Length</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The MIT press</publisher>
			<biblScope unit="page">323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/r5/roos05a.html" />
		<title level="m">On the Behavior of MDL Denoising&quot;; in &quot;International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<title level="m">MDL histogram density estimation&quot;; in &quot;Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Strong optimality of the normalized ML models as universal codes and information in data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1712" to="1717" />
			<date type="published" when="2001">2001</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">A linear-time algorithm for computing the multinomial stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">Average case analysis of algorithms on sequences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Efficient computation of stochastic complexity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Ninth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<title level="m" type="main">Computationally efficient methods for MDL-optimal density estimation and data clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Inference in hybrid Bayesian networks using dynamic discretization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marquez;</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-007-9018-y</idno>
		<ptr target="https://doi.org/10.1007/s11222-007-9018-y" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<idno type="ISSN">1573-1375</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="219" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Improved reliability modeling using Bayesian networks and dynamic discretization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenton;</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ress.2009.11.012</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0951832009002646" />
	</analytic>
	<monogr>
		<title level="m">Reliability Engineering &amp; System Safety 95</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="412" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">JIDT: An information-theoretic toolkit for studying the dynamics of complex systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10351</idno>
		<title level="m">Joint causal inference from multiple contexts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Mixed graphical models for integrative causal analysis with application to chronic lung disease diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sedgewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buschur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Manatakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Karoleski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery with mixed data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsagris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borboudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lagani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976">1976</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
		<title level="m" type="main">Statistical analysis with missing data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">793</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Graphical Models for Processing Missing Data&quot;; Forthcoming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">JASA</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<title level="m">Graphical Models for Inference with Missing Data&quot;; Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Publisher: Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1277" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Robust Independence Testing for Constraint-Based Learning of Causal Structure</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UAI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Bayesian network structural learning and incomplete data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Leray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>François</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR 2005)</title>
		<meeting>the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR 2005)<address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Approximate Kernel-Based Conditional Independence Tests for Fast Non-Parametric Causal Discovery</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Visweswaran</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2018-0017</idno>
		<ptr target="https://www.degruyter.com/view/j/jci.2019.7.issue-1/jci-2018-0017/jci-2018-0017.xml?format=INT" />
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<idno type="ISSN">2193-3685</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Bertilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04010</idno>
		<idno>arXiv: 1807.04010</idno>
		<ptr target="http://arxiv.org/abs/1807.04010" />
		<title level="m">Causal Discovery in the Presence of Missing Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="1994">1994</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-Cruz</surname></persName>
		</author>
		<title level="m">Kullback-Leibler divergence estimation of continuous distributions&quot;; in &quot;2008 IEEE international symposium on information theory</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1666" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<monogr>
		<title level="m" type="main">nanoflann: a {C}++ header-only fork of {FLANN}, a library for Nearest Neighbor ({NN}) with KD-trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rai</surname></persName>
		</author>
		<ptr target="https://github.com/jlblancoc/nanoflann" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bhave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6Y05VJfGlFM" />
		<title level="m">Information Theoretic Approaches for Testing Missingness in Predictive Models&quot;; (2020)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">White matter hyperintensities are associated with disproportionate progressive hippocampal atrophy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fiford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Biessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alzheimer's Disease Neu-Roimaging</forename><surname>Initiative</surname></persName>
		</author>
		<idno type="DOI">10.1002/hipo.22690</idno>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<idno type="ISSN">1098-1063</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="249" to="262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<monogr>
		<title level="m" type="main">Hematopoietic stem cell fate through metabolic control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Experimental hematology 64</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Hematopoietic stem cell metabolism during development and aging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura-Ishizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental cell</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="239" to="255" />
			<date type="published" when="2020">2020</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Glucose and glutamine metabolism regulate human hematopoietic stem cell lineage specification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oburoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tardito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>De Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craveiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mamede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cretenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mongellaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell stem cell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2014">2014</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Haemopedia RNA-seq: a database of gene expression during haematopoiesis in mice and humans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bolden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Fairfax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Cor-Coran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>De Graaf</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky1020</idno>
		<ptr target="https://doi.org/10.1093/nar/gky1020" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">785</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Explainable machinelearning predictions for the prevention of hypoxaemia during surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Vavilala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eisses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Liston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim &amp; S.-I</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41551-018-0304-0</idno>
		<ptr target="https://www.nature.com/articles/s41551-018-0304-0" />
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<idno type="ISSN">2157-846</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">749</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Erion &amp; S.-I</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03888</idno>
		<idno>arXiv: 1802.03888</idno>
		<ptr target="http://arxiv.org/abs/1802.03888" />
		<title level="m">Consistent Individualized Feature Attribution for Tree Ensembles</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b288">
	<monogr>
		<title level="m" type="main">Learning dynamics by computational integration of single cell genomic and lineage information</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Klein;</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.05.06.443026v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2021.05.06" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">443026</biblScope>
		</imprint>
		<respStmt>
			<orgName>Cold Spring Harbor Laboratory Distributor: Cold Spring Harbor Laboratory Label: Cold Spring Harbor Laboratory Section</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>New Results Type: article</note>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Lineage tracing on transcriptional landscapes links state to fate during differentiation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weinreb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Fraticelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aaw3381</idno>
		<ptr target="https://science.sciencemag.org/content/367/6479/eaaw3381" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="1095" to="9203" />
			<date type="published" when="2020">2020</date>
			<publisher>American Association for the Advancement of Science Section</publisher>
		</imprint>
	</monogr>
	<note>Research Article</note>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Measuring information transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">461</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<monogr>
		<title level="m" type="main">Necessary and sufficient conditions for causal feature selection in time series with latent common causes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Mastakouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08543</idno>
		<idno>arXiv: 2005.08543</idno>
		<ptr target="http://arxiv.org/abs/2005.08543" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v124/runge20a.html" />
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Panning for gold:&apos;model-X&apos;knockoffs for high dimensional controlled variable selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="551" to="577" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang;</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00434</idno>
		<idno>arXiv: 1903.00434</idno>
		<ptr target="http://arxiv.org/abs/1903.00434" />
	</analytic>
	<monogr>
		<title level="j">Metropolized Knockoff Sampling</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Testing Conditional Independence on Discrete Data using Stochastic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04829</idno>
	</analytic>
	<monogr>
		<title level="j">Complexity</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">MIIC online: a web server to reconstruct causal or non-causal networks from non-perturbative data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uguzzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isambert;</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<title level="m" type="main">Estimation of causal orders in a linear non-Gaussian acyclic model: a method robust against latent confounders&quot;; Artificial Neural Networks and Machine Learning-ICANN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Measurement of residual breast cancer burden to predict survival after neoadjuvant chemotherapy</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Symmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peintinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Assad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poniecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Oncology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="4414" to="4422" />
			<date type="published" when="2007">2007</date>
			<publisher>American Society of Clinical Oncology</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
