<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NESTED MARKOV PROPERTIES FOR ACYCLIC DIRECTED MIXED GRAPHS</title>
				<funder ref="#_QNWrhqf #_6RvH9q3">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_4RXtbn7 #_JpKXyZk #_tH2b2tp #_ErbNeQT">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_GVRVJnt #_But8u2X #_Uz849gT">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-25">25 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington *</orgName>
								<orgName type="institution" key="instit2">University of Oxford †</orgName>
								<orgName type="institution" key="instit3">Harvard University ‡</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington *</orgName>
								<orgName type="institution" key="instit2">University of Oxford †</orgName>
								<orgName type="institution" key="instit3">Harvard University ‡</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington *</orgName>
								<orgName type="institution" key="instit2">University of Oxford †</orgName>
								<orgName type="institution" key="instit3">Harvard University ‡</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington *</orgName>
								<orgName type="institution" key="instit2">University of Oxford †</orgName>
								<orgName type="institution" key="instit3">Harvard University ‡</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NESTED MARKOV PROPERTIES FOR ACYCLIC DIRECTED MIXED GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-25">25 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1214/22-AOS2253</idno>
					<idno type="arXiv">arXiv:1701.06686v6[stat.ME]</idno>
					<note type="submission">Submitted to the Annals of Statistics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graphical models</term>
					<term>hidden variable models</term>
					<term>conditional independence</term>
					<term>causal inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional independence models associated with directed acyclic graphs (DAGs) may be characterized in at least three different ways: via a factorization, the global Markov property (given by the dseparation criterion), and the local Markov property. Marginals of DAG models also imply equality constraints that are not conditional independences; the well-known "Verma constraint" is an example. Constraints of this type are used for testing edges, and in a computationally efficient marginalization scheme via variable elimination.</p><p>We show that equality constraints like the "Verma constraint" can be viewed as conditional independences in kernel objects obtained from joint distributions via a fixing operation that generalizes conditioning and marginalization. We use these constraints to define, via ordered local and global Markov properties, and a factorization, a graphical model associated with acyclic directed mixed graphs (AD-MGs). We prove that marginal distributions of DAG models lie in this model, and that a set of these constraints given by Tian provides an alternative definition of the model. Finally, we show that the fixing operation used to define the model leads to a particularly simple characterization of identifiable causal effects in hidden variable causal DAG models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. Graphical models provide a principled way to take advantage of independence constraints for probabilistic modeling, learning and inference, while giving an intuitive graphical description of qualitative features useful for these tasks. A popular graphical model represents a joint distribution by means of a directed acyclic graph (DAG), where each vertex in the graph corresponds to a random variable. The popularity of DAG models, also known as Bayesian network models, stems from their wellunderstood theory and from the fact that they admit an intuitive causal interpretation (under the assumption that there are no unmeasured common causes; see <ref type="bibr">Spirtes et al. (1993)</ref>). An arrow from a variable A to a variable B in a DAG model can be interpreted, in a way that can be made precise, to mean that A is a "direct cause" of B.</p><p>Starting from a causally interpreted DAG, the consequences of intervention in the system under study can be understood by modifying the graph via removing certain edges, and modifying the corresponding joint probability distribution via reweighting <ref type="bibr" target="#b39">(Strotz and Wold (1960)</ref>, <ref type="bibr">Spirtes et al. (1993)</ref>, <ref type="bibr" target="#b22">Pearl (2000)</ref>). For example, the DAG in Figure <ref type="figure" target="#fig_0">1</ref>(i) represents distributions that factorize as p(x 0 , x 1 , x 2 , x 3 , x 4 ) = p(x 0 )p(x 1 )p(x 2 | x 0 , x 1 )p(x 3 | x 1 , x 2 )p(x 4 | x 0 , x 3 ).</p><p>If the model is interpreted causally, an experiment to externally set the value of X 3 will break the dependence of X 3 on X 1 and X 2 ; however, the dependence of X 4 upon X 3 will be preserved; see Figure <ref type="figure" target="#fig_0">1</ref> <ref type="bibr">(ii)</ref>. This is represented graphically by severing incoming edges to 3, an operation some authors call "mutilation," and probabilistically by removing the factor p(x 3 | x 1 , x 2 ) from the factorization of p to yield a new distribution: p * (x 0 , x 1 , x 2 , x 4 | x 3 ) = p(x 0 )p(x 1 )p(x 2 | x 0 , x 1 )p(x 4 | x 0 , x 3 ).</p><p>(1)</p><p>In certain contexts, we will later call these operations "fixing."</p><p>The functional in (1) is sometimes called the g-formula <ref type="bibr" target="#b28">(Robins, 1986)</ref>, the manipulated distribution <ref type="bibr">(Spirtes et al., 1993)</ref>, or the truncated factorization <ref type="bibr" target="#b22">(Pearl, 2000)</ref>. The distribution p * (x 0 , x 1 , x 2 , x 4 | x 3 ) is commonly denoted by p(x 0 , x 1 , x 2 , x 4 | do(x 3 )). In a causal model given by a DAG where all variables are observed, any interventional probability distribution can be identified by this method.</p><p>Often not all common causes are measured, or there is no way to know a priori whether this is the case. This motivates the study of DAG models containing latent variables <ref type="bibr" target="#b1">(Allman et al., 2015)</ref>, and the constraints they imply. Existing theoretical machinery based on DAGs can be applied to such settings, simply by treating the unobserved variables as missing data. However, this creates a number of problems that are particularly severe when the structure of the underlying DAG model with latents is unknown. First, there are, in general, an infinite number of DAG models with latent variables that imply the (independence) constraints holding in a given observed distribution. Second, assumptions concerning the state-space or distribution of latent variables may have a profound effect on the model. This is problematic if prior knowledge about latent variables is scarce.</p><p>An alternative approach considers a supermodel defined by taking a subset of the constraints implied by a DAG model with latent variables on the observed marginal distribution. More specifically, we consider models defined by equality constraints that are implied by the factorization of a DAG with latents, but that do not depend on assumptions regarding the state-space or distribution of the latent variables. Models defined by these constraints are naturally represented by mixed graphs, that is, graphs containing directed (→) and bidirected (↔) edges, obtained from DAGs via a latent projection operation <ref type="bibr" target="#b42">(Verma and Pearl, 1990)</ref>; see the graph in Figure <ref type="figure" target="#fig_21">3</ref>(i) for the latent projection of the DAG in Figure <ref type="figure" target="#fig_0">1</ref>(i). Much previous work <ref type="bibr" target="#b0">(Ali et al., 2009;</ref><ref type="bibr" target="#b12">Evans and Richardson, 2010;</ref><ref type="bibr">Richardson and Spirtes, 2002;</ref><ref type="bibr">Sadeghi and Lauritzen, 2014;</ref><ref type="bibr">Wermuth, 2011;</ref><ref type="bibr" target="#b45">Wermuth et al., 1994)</ref> has defined Markov models for mixed graphs via independence constraints implied by hidden variable DAGs on the observed margin. It is well known, however, that DAG models with latent variables imply nonparametric constraints that are not conditional independence constraints. For example, consider the DAG shown in Figure <ref type="figure" target="#fig_0">1</ref>(i), and take the vertex 0 as hidden. This DAG implies no conditional independence restrictions on the observed margin p(x 1 , x 2 , x 3 , x 4 ). This is because all vertex sets that d-separate pairs of observed variables-that is, the pairs (x 1 , x 4 ) and (x 2 , x 4 )-include the unobserved variable x 0 . However, it may be shown that the p(x 1 , x 2 , x 3 , x 4 ) margin of any distribution p(x 0 , x 1 , x 2 , x 3 , x 4 ), which factorizes according to the DAG in Figure <ref type="figure" target="#fig_0">1</ref>(i), obeys the constraint that (2)</p><formula xml:id="formula_0">x 2 p(x 4 | x 1 , x 2 , x 3 )p(x 2 | x 1</formula><p>) is a function of only x 3 and x 4 ; see <ref type="bibr" target="#b28">Robins (1986)</ref>; <ref type="bibr" target="#b42">Verma and Pearl (1990)</ref>; <ref type="bibr" target="#b44">Wermuth and Cox (2008)</ref>. In <ref type="bibr" target="#b29">Robins (1999)</ref>, it is shown that this constraint is equivalent to the requirement that X 4 is independent of X 1 given X 3 in the distribution obtained from p(x 1 , x 2 , x 3 , x 4 ) after dividing by the conditional p(x 3 | x 2 , x 1 ). Note that this is the same manipulation performed in (1), but the operation, which we later call "fixing," is purely probabilistic and can be performed without requiring that the model has any causal interpretation. If we interpret the original DAG as causal, then (2) is an (identifiable) dormant independence constraint <ref type="bibr" target="#b33">(Shpitser et al., 2014;</ref><ref type="bibr" target="#b35">Shpitser and Pearl, 2008)</ref>.</p><p>Since, as we have seen, the DAG in Figure <ref type="figure" target="#fig_0">1</ref>(i) implies no conditional independence restrictions on the joint p(x 1 , x 2 , x 3 , x 4 ), the set of distributions obeying these independence relations is (trivially) saturated. Consequently, a structure learning algorithm such as FCI <ref type="bibr">(Spirtes et al., 1993)</ref> that learns a Markov equivalence class of DAGs with latent variables, under the assumption of faithfulness, will return a (maximally uninformative) unoriented complete graph. The assumption of "faithfulness" implies that if X A ⊥ ⊥ X B | X C in the observed distribution then A is d-separated from B given C in the underlying DAG with latent variables.</p><p>Indeed, as originally pointed out by <ref type="bibr" target="#b29">Robins (1999)</ref>, if we assume a certain generalization of faithfulness, if (2) holds, we may rule out the model in Figure <ref type="figure" target="#fig_0">1</ref>(iii). More generally, <ref type="bibr" target="#b36">Shpitser et al. (2009)</ref> used pairwise constraints of this form to test for the presence of certain directed edges (in the context of a specific graph). Further, <ref type="bibr">Tian and Pearl (2002)</ref> presented a general algorithm for finding nonparametric constraints from DAGs with latent variables.</p><p>In this paper, we introduce a (statistical) model called the nested Markov model, defined by these nonparametric constraints, and associated with a mixed graph called an acyclic directed mixed graph (ADMG). We give equivalent characterizations of the model in terms of global and ordered local Markov properties, and a factorization. We next prove that the set of marginal distributions given by a latent DAG model is a submodel of the nested Markov model associated with the corresponding ADMG (obtained by latent projection). We show that our results lead to a particularly simple characterization of identifiable causal effects in hidden variable causal DAG models. Finally, we relate the nested Markov model to a set of constraints obtained by <ref type="bibr">Tian and Pearl (2002)</ref>.</p><p>1.1. Applications of the nested Markov model. Nested models are defined by constraints that correspond to the absence of direct effects <ref type="bibr" target="#b29">(Robins, 1999;</ref><ref type="bibr" target="#b30">Robins and Wasserman, 1997)</ref>. For example, consider the causal graph in Figure <ref type="figure" target="#fig_0">1(i)</ref>, where vertices 1 and 3 correspond to treatments X 1 and X 3 , vertices 2 and 4 are responses X 2 , X 4 and vertex 0 is an unobserved confounding variable X 0 . In this graph, X 1 has no direct effect on X 4 , and as noted above, this implies (2). The nested model shown in Figure <ref type="figure" target="#fig_21">3</ref>(i) is defined by this constraint (see Section 3).</p><p>Such restrictions have been increasingly exploited in the analysis of lon-gitudinal observational data in medicine and public health. For example, consider the nested Markov constraint that a diagnostic or screening test (e.g., a mammogram) has no direct effect on a patient's clinical outcome of interest (death from breast cancer), except through the effect of the test results on the choice of treatment (no treatment if the mammogram was negative and, if positive, biopsy possibly followed by breast surgery and/or chemotherapy). Several recent papers <ref type="bibr" target="#b4">(Caniglia et al., 2019;</ref><ref type="bibr" target="#b16">Kreif et al., 2021;</ref><ref type="bibr" target="#b19">Neugebauer et al., 2017)</ref> have leveraged this no direct effect of screening constraint to construct novel highly efficient estimators of an optimal joint testing and treatment regime. What was surprising and, indeed, unprecedented is that, in actual medical studies, the new estimators have provided a 50-fold increase in efficiency (and thus a 50-fold reduction in the required sample size) compared to estimators that fail to leverage the no direct effect constraint <ref type="bibr" target="#b4">(Caniglia et al., 2019)</ref>. Nested Markov constraints can also be used for structure learning. The review paper <ref type="bibr" target="#b33">(Shpitser et al., 2014)</ref> includes all nested Markov equivalence classes over four variables; classes with nested constraints can be very small, so they are potentially extremely informative about causal structure. This reflects the fact that whereas conditional independences involve three sets, nested constraints are relations among more groups of variables (see Section 2.6). A general theory of nested Markov equivalence and search for ADMGs remains a subject for future work. However, distributions consistent with these constraints have already been observed in applications; see <ref type="bibr" target="#b3">Bhattacharya et al. (2021)</ref>.</p><p>The nested constraints also facilitate the construction of more computationally efficient marginalization methods in some causal graphs <ref type="bibr" target="#b37">(Shpitser et al., 2011)</ref>, for categorical variables. This is achieved via Theorem 48 and the parameterization of <ref type="bibr" target="#b13">Evans and Richardson (2019)</ref>.</p><p>This work is of separate interest to people constructing causal models for quantum mechanics, since it does not impose the inequalities implied by latent variable models <ref type="bibr" target="#b18">(Navascués and Wolfe, 2020)</ref>; see Example 47. <ref type="bibr">Evans (2018)</ref> shows that for categorical variables the nested Markov model is the closest approximation to the margin of a DAG model (ignoring inequalities), in that it incorporates all equality constraints, and in this sense is "complete." 1.2. Overview of nested Markov models. We now outline our strategy for defining the nested Markov model in terms of ordinary conditional independence (in derived distributions) by analogy to a way of defining DAG models in terms of undirected graphical models. We give a specific exam-ple of a nested Markov model, outlining the key concepts, while providing references to the formal definitions within the paper.</p><p>A nested Markov model is represented by an ADMG, a mixed graph naturally derived from DAGs with latent variables via an operation called latent projection. Intuitively, the ADMG does not contain latent variables, but indicates the presence of such variables by the inclusion of bidirected (↔) edges. Earlier work <ref type="bibr">(Richardson (2003)</ref>; <ref type="bibr">Richardson and Spirtes (2002)</ref>) established Markov properties for ordinary independence models defined by ADMGs. Such an independence model is defined by fewer constraints than the nested Markov model represented by the same ADMG, and hence the former is a supermodel of the latter. The global Markov property for these independence models simply corresponds to the natural extension of d-separation <ref type="bibr">(Pearl, 1988)</ref> to ADMGs. This extension, which is sometimes called m-separation, consists of allowing colliders to involve bidirected edges. Latent projection is defined in Section A.3; ADMGs and m-separation in Sections 2.1 and A.2, respectively.</p><p>We also consider conditional ADMGs (CADMGs) where certain "fixed" vertices represent nonrandom variables that index distributions over the random variables. Such vertices are treated similarly to the so-called "strategy nodes" in influence diagrams <ref type="bibr" target="#b7">(Dawid, 2002)</ref>. The Markov property for CADMGs is a simple extension of m-separation that takes into account fixed nodes. CADMGs are defined formally in Section 2.3; the corresponding global Markov property for a CADMG is given in Section 2.8.1. Note that an ADMG is the special case of a CADMG with no fixed vertices.</p><p>CADMGs and their associated Markov models characterize the nested Markov model in much the same way that undirected graphs and their associated Markov models can be used to describe a DAG model. We first briefly review the characterization of DAGs via undirected models.</p><p>The global Markov property for DAGs may be obtained from the (union of the) Markov properties associated with undirected graphs derived from the DAG by the moralization operation ( <ref type="bibr" target="#b17">(Lauritzen, 1996)</ref>, Theorem 3.27); the resulting property is equivalent to d-separation <ref type="bibr">(Pearl, 1988)</ref>. More precisely, the DAG Markov property corresponds to (the union of) the Markov properties associated with undirected graphs representing "ancestral" margins. Likewise, the set of distributions corresponding to the DAG is the intersection of the sets of distributions obeying the factorization properties associated with these undirected graphs; this is equivalent to the characterization that the joint distribution factors into the product of each variable given its parents in the graph. As an example, consider the DAG in <ref type="bibr">Figure 2(i)</ref>. Undirected graphs associated with some ancestral margins, and</p><formula xml:id="formula_1">1 2 3 4 (i) p(x1, x2, x3, x4) 1 2 3 4 (ii) p(x1, x2, x3, x4) = ϕ12(x1, x2)ϕ234(x2, x3, x4) 1 2 3 4 (iii) x 4 p(x1, x2, x3, x4) = ϕ12(x1, x2)ϕ3(x3) 1 2 3 4 (iv) x 2 ,x 4 p(x1, x2, x3, x4) = ϕ1(x1)ϕ3(x3)</formula><p>Fig. <ref type="figure" target="#fig_5">2</ref>: Reduction of a DAG model to a set of undirected models via marginalization and moralization: (i) The original DAG G({1, 2, 3, 4}). Undirected graphs representing the factorization of different ancestral margins: (ii) p(x 1 , x 2 , x 3 , x 4 ); (iii) p(x 1 , x 2 , x 3 ); (iv) p(x 1 , x 3 ). Note that we have also included the marginalized variables on the graph in square nodes (since, as we later show, marginalization is a special case of fixing). The DAG model may be characterized by (the union of) the conditional independence properties implied by the undirected graphs for all ancestral margins. their factorizations, are shown in Figure <ref type="figure" target="#fig_5">2</ref>(ii), (iii), and (iv).</p><p>Likewise, the set of distributions in the nested Markov model associated with an ADMG corresponds to the intersection of the sets of distributions obeying factorization properties encoded by specific CADMGs obtained from the original ADMG. However, whereas the undirected graphs corresponding to a DAG may be seen as representing specific (ancestral) margins, the CADMGs obtained from an ADMG represent "kernel" distributions obtained by sequentially applying a new "fixing" operation on distributions, one that generalizes conditioning and marginalizing. This fixing operation has a natural causal interpretation, as do the kernels that form CADMG factorizations of the nested Markov model. Specifically, in the context of a latent variable causal model whose projection is a given ADMG, kernels can be viewed as (identified) interventional distributions. Not all variables are fixable. From a causal perspective this is natural since in the presence of latent variables, not all interventional distributions are identifiable. The fixing operation and the set of fixable vertices are defined in Section 2.11.</p><p>As a specific example, consider the graph shown in Figure <ref type="figure" target="#fig_21">3</ref>(i). In this ADMG, the vertex 3 may be fixed to give the CADMG shown in Figure <ref type="figure" target="#fig_21">3</ref>(ii), where the corresponding distribution and factorization are also shown, with</p><formula xml:id="formula_2">q 1 (x 1 ) = p(x 1 ), q 24 (x 2 , x 4 | x 1 , x 3 ) = p(x 1 , x 2 , x 3 , x 4 ) p(x 1 )p(x 3 | x 2 , x 1 )</formula><p>.</p><p>Note that, although the original graph implied no conditional independences, the graph in Figure <ref type="figure" target="#fig_21">3</ref>(ii) implies the independence</p><formula xml:id="formula_3">X 1 ⊥ ⊥ X 4 | X 3 via m- separation.</formula><p>Whereas the undirected graphs associated with a DAG correspond to distributions obtained by specific marginalizations (namely those that remove vertices that have no children), CADMGs correspond to certain specific ordered sequences of fixing operations. Not all such sequences are allowed: in some cases a vertex may be fixable only after another vertex has already been fixed. The global nested Markov property corresponds to the (union of the) m-separation relations encoded in the CADMGs derived via allowed sequences of these fixing operations, which we call "valid." Valid fixing sequences are defined in Section 2.13. These fixing sequences are closely related to a particular identification strategy for interventional distributions consisting of recursively applying the g-formula to an already identified intervention distribution to obtain the result of further interventions; this connection is explored further in Section 4.2.</p><p>Returning to the example, given the CADMG in Figure <ref type="figure" target="#fig_21">3</ref>(ii), the vertex 1 may be fixed to give the CADMG in Figure <ref type="figure" target="#fig_21">3</ref>(iii). Further, given the CADMG in Figure <ref type="figure" target="#fig_21">3</ref>(iii), we may fix 2. The kernel in this graph is</p><formula xml:id="formula_4">q 4 (x 4 | x 3 ) = x 2 p(x 4 | x 3 , x 2 , x 1 )p(x 2 | x 1 ). (3)</formula><p>The quantity on the RHS of (3) is a function only of x 3 and x 4 , and not x 1 . This is precisely the constraint (2) implied by the original latent variable DAG model in Figure <ref type="figure" target="#fig_0">1(i)</ref>. This constraint characterizes the nested Markov model corresponding to the ADMG in Figure <ref type="figure" target="#fig_21">3</ref>(i).</p><p>In the original ADMG in Figure <ref type="figure" target="#fig_21">3</ref>(i), we could also have fixed 1 and 4. Had we chosen to fix 1, we could then subsequently have fixed 3, and would have arrived at the same CADMG and distribution as shown in Figure <ref type="figure" target="#fig_21">3(iii)</ref>.</p><formula xml:id="formula_5">1 2 3 4 (i) p(x1, x2, x3, x4) 1 2 3 4 (ii) p(x 1 ,x 2 ,x 3 ,x 4 ) p(x 3 | x 1 ,x 2 ) = q1(x1)q24(x2, x4 | x1, x3) 1 2 3 4 (iii) p(x 1 ,x 2 ,x 3 ,x 4 ) p(x 1 )p(x 3 | x 1 ,x 2 ) = q24(x2, x4 | x1, x3) 1 2 3 4 (iv) x 2 p(x 1 ,x 2 ,x 3 ,x 4 ) p(x 1 )p(x 3 | x 1 ,x 2 ) = x 2 p(x2 | x1)p(x4 | x1, x2, x3) = q4(x4 | x3)</formula><p>Fig. <ref type="figure" target="#fig_21">3</ref>: Reduction of the nested Markov model for an ADMG to a set of ordinary Markov models associated with CADMGs: (i) The ADMG G({1, 2, 3, 4}), which is the latent projection of the graph G from Figure <ref type="figure" target="#fig_0">1</ref>(i). CADMGs, representing the Markov structure of derived distributions, resulting from sequences of fixing operations in G({1, 2, 3, 4}): (ii) 3 ; (iii) 3, 1 or alternatively 1, 3 ; (iv) any of the sequences 1, 3, 2 , 3, 1, 2 , 3, 2, 1 ; it is not valid to fix 2 before 3. The nested Markov model may be defined via the conditional independence properties for all CADMGs and associated kernels obtained (via valid fixing sequences) from the original ADMG and distribution. See also Figure <ref type="figure" target="#fig_5">2</ref> and text.</p><p>Thus, the operations of fixing 3 and 1 commute. However, this is not always the case: for example, 2 may only be fixed after 3.</p><p>Like the DAG model, the nested Markov model may be characterized via a factorization property, as well as by local and global Markov properties described in Section 3. In each case these properties are defined via the set of CADMGs that are "reached" via valid fixing sequences. Just as the factorization property of the DAG model leads naturally to a parameterization in terms of Markov factors, likewise the factorization property of the nested Markov model leads to a parameterization in terms of its factors in multivariate Gaussian and finite categorical cases <ref type="bibr" target="#b13">(Evans and Richardson, 2019;</ref><ref type="bibr">Shpitser et al., 2018)</ref>. The usual DAG parameters are such that the choice of value for one parameter p(x v | x pa(v) ) does not restrict the set of possible values for another, p(x w | x pa(w) ); in other words, they are variation independent, but this is not true for the nested Markov model.</p><p>The rest of the paper is organized as follows. In Section 2.1, we introduce graph theoretic preliminaries. In Section 2.2, we define conditional mixed graphs, which are used to construct our model. In Section 2.3, we introduce a generalization of conditional distributions which we call kernels, and in Section 2.4 we generalize the notion of conditional independence from standard distributions to these kernels. In Sections 2.5-2.7, we provide further properties of kernels, including a proof that conditional independence in kernels satisfies the semigraphoid axioms (Proposition 6). In Section 2.8 and Section 2.9, we give the Markov properties and factorization of kernels associated with CADMGs; we prove they are equivalent in Theorem 16 in Section 2.10. We define the graph-based fixing operation in Section 2.11, give some of its properties in Section 2.12, and define sets that can be reached by successive applications of this operation in Section 2.13.</p><p>We define the nested model in Section 3. Theorem 31 in Section 3.1 shows a key result, that graphs and kernels are invariant to the precise order of fixing in a sequence, as long as each fixing operation is valid. In Section 3.2, Section 3.3, and Section 3.4, respectively, we give definitions of the nested Markov model in terms of the global Markov property, the factorization, and the ordered local Markov property. Corollary 40 in Section 3.5 shows that the nested Markov model corresponding to any complete ADMG gives the saturated model. In Section 4.1, we show in Theorem 46, that any marginal distribution in a hidden variable DAG model lies in the nested Markov model associated with the latent projection. A simple characterization of identifiable causal effects in hidden variable causal DAG models, based on the fixing operation, is given in Theorem 48 of Section 4.3. Finally, a result stating that the nested Markov model may be defined by the set of constraints found by the algorithm in <ref type="bibr">Tian and Pearl (2002)</ref> is given as Theorem 51 in Section 4.4. All proofs of claims are in the Supplementary Material <ref type="bibr" target="#b26">(Richardson et al. (2023)</ref>).</p><p>2. Latent variable DAG models. We assume familiarity with standard graphical definitions relating to DAGs, latent projections, and mixed graphs. See Section A.1 in the Supplementary Material for a review.</p><p>2.1. Basic concepts. The motivation for introducing mixed graphs is twofold. First, by removing latent variables and replacing them with bidirected edges we simplify the representation. For example, to perform a search, instead of considering a potentially infinite class of DAGs with arbitrarily many latent variables, we need only consider a finite set of mixed graphs. Second, although the statistical models that we associate with mixed graphs capture many of the constraints implied by latent variable models, the resulting model will still, in general, be a superset of the set of distributions over the observables that are implied by the original DAG with latents. The use of a mixed graph to represent our model serves to emphasize that, in spite of this connection, the set of distributions we are constructing is nonetheless not a latent variable model (see Example 47).</p><p>We will make use of standard genealogic relations in graphs. The sets of parents, children, ancestors, descendants, and nondescendants of a in G are written pa G (a), ch G (a), an G (a), de G (a), and nd G (a), respectively. By convention, every vertex is its own ancestor and descendant, so an G (a) ∩ de G (a) = {a}. An ordering ≺ of nodes in G is said to be topological if for any pair of vertices a, b, if a ≺ b, then a / ∈ de G <ref type="bibr">(b)</ref>; note that this implies that if a ≺ b then a = b. We define the set pre G,≺ (b) ≡ {a|a ≺ b}. We apply these definitions disjunctively to sets, for example, an</p><formula xml:id="formula_6">G (A) = a∈A an G (a). A set of vertices A in G is called ancestral if an G (a) ⊆ A whenever a ∈ A.</formula><p>Given a DAG G with vertex set V ∪L where L is the set of latent variables, we associate with it a mixed graph, denoted G(V ), via the standard operation of latent projection σ L <ref type="bibr">(Pearl and Verma, 1991)</ref>; see the Supplementary Material, Section A.3 for the definition <ref type="bibr" target="#b26">(Richardson et al., 2023)</ref>. Here and elsewhere, we use standard ∪ notation when we wish to emphasize that the sets in the union are disjoint.</p><p>The latent projection G(V ) = σ L (G(V ∪L)) represents the set of d-separation relations holding among the variables in V in G.</p><p>Proposition 1. Let G be a DAG with vertex set V ∪ L. For disjoint subsets A, B, C ⊆ V (where C may be empty),</p><formula xml:id="formula_7">A is d-separated from B given C in G if and only if A is m-separated from B given C in G(V ).</formula><p>imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023</p><formula xml:id="formula_8">1 2 3 4 (i) 1 2 3 4 (ii)</formula><p>Fig. <ref type="figure" target="#fig_6">4</ref>: (i) A conditional mixed graph G(V = {2, 4}, W = {1, 3}) describing the structure of a kernel q 24 (x 2 , x 4 | x 1 , x 3 ). (ii) The corresponding graph G |W from which the conditional Markov property given by G may be obtained by applying m-separation. The edge 1 ↔ 3 is added.</p><p>However, as we will see later, the latent projection G(V ) captures much more than simply the d-separation relations holding in V . As suggested by Figures <ref type="figure" target="#fig_0">1(i</ref>) and 3(i), G(V ) also represents constraints such as (2), and further all those found by the algorithm in <ref type="bibr">Tian and Pearl (2002)</ref>, as well as others.</p><p>2.2. Conditional ADMGs. We define a conditional acyclic directed mixed graph(CADMG) G(V, W ) to be an ADMG with two disjoint sets of vertices V and W , which we call random and fixed, respectively. We require that for all w ∈ W , pa G (w) = ∅ and there are no bidirected edges involving w.</p><p>Thus, in a CADMG G(V, W ) there are no edges connecting vertices in W , and all edges connecting w ∈ W and v ∈ V take the form w → v. Vertices in V will represent random variables X V , as in a standard graphical model. The rationale for excluding edges between vertices in W or with arrowheads in W is that the CADMG will represent objects where the vertices in W correspond to (nonrandom) values that index distributions over V .</p><p>We note that CADMGs represent kernels that are not, in general, formed by standard conditioning from the original observed distribution. For example, a set of interventional distributions over X V indexed by possible interventions on X W ; note, however, that we do not require a causal interpretation of the graph. We also introduce operators V(G) and W(G) that return, respectively, the sets of random and fixed nodes associated with a CADMG G. We will use circular nodes to indicate the random vertices V(G), and square nodes to indicate the fixed vertices W(G); see, for instance, the CADMGs in Figures 1(ii), 3(ii)-(iv) and 4(i). When the vertex sets are clear from context, we will abbreviate G(V, W ) as G.</p><p>We will apply the standard genealogical definitions to CADMGs. Conversely, since an ADMG G(V ) may be seen as a CADMG in which W = ∅, all subsequent definitions for CADMGs will also apply to ADMGs.</p><p>The induced subgraph of a CADMG G(V, W ) on a set A, denoted G A , is a CADMG with V(G A ) = V ∩ A and W(G A ) = W ∩ A, and precisely those edges from G that have both endpoints in A. Note that in forming G A , the status of the vertices in A with regard to whether they are in V or W is preserved.</p><p>Given a CADMG G(V ∪ L, W ) and a set of latent variables L that are all random, we define the latent projection G(V, W ) to be the CADMG obtained by applying the standard definition of latent projection for ADMGs (see the Supplementary Material, Section A.3).</p><formula xml:id="formula_9">Proposition 2. In a CADMG G(V ∪ L, W ) if V ∪ W is ancestral, then G V ∪W ≡ (G(V ∪ L, W )) V ∪W = G(V, W ).</formula><p>Thus, the induced subgraph on an ancestral set V ∪ W is the same as the latent projection onto V ∪ W . Definition 3. A set of vertices C in a CADMG G is called bidirectedconnected if for every pair of vertices c, d ∈ C there is a bidirected path in G between c and d with every vertex on the path in</p><formula xml:id="formula_10">C. A maximal bidirected- connected set of vertices in V(G) is referred to as a district in G. Let D(G) ≡ {D | D is a district in G}</formula><p>be the set of districts in G. For v ∈ V(G), let dis G (v) be the district containing v in G. We write dis A (v) as a shorthand for dis G A (v), the district of v in the induced subgraph G A . <ref type="bibr">Tian and Pearl (2002)</ref> refer to districts in ADMGs as "c-components." In models associated with ADMGs, districts are used to specify variable partitions that define terms in the factorization of observed random variables. For this reason, districts in CADMGs include only random vertices. In a DAG G(V ), the districts D(G) = {{v} | v ∈ V } are the singleton subsets of V , these are naturally associated with the usual Markov factors p(x v | x pa G (v) ).</p><p>2.3. Kernels. We consider collections of real-valued random variables (X v ) v∈V taking values in (X v ) v∈V . For A ⊆ V , we let X A ≡ × v∈A (X v ), and</p><formula xml:id="formula_11">X A ≡ (X v ) v∈A .</formula><p>Here, v denotes a vertex and X v the corresponding random variable, likewise A denotes a vertex set and X A is the vector (X v : v ∈ A).</p><p>Following <ref type="bibr" target="#b17">Lauritzen (1996)</ref>, p. 46, we define a kernel over X V and indexed by X W to be a nonnegative function q</p><formula xml:id="formula_12">V (x V | x W ) satisfying (4) x V ∈X V q V (x V | x W ) = 1, for all x W ∈ X W .</formula><p>imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023</p><p>We use the term "kernel" and write q V (•|•) (rather than p(•|•)) to emphasize that these functions, though they satisfy (4), and thus most properties of conditional densities, will not, in general, be formed via the usual operation of conditioning on the event X W = x W . Following standard conventions for densities, we will use q V (x V | x W ) to refer either to the function itself, or to its realization under specific values of x V , x W , with the precise meaning being clear from context. To conform with standard notation for densities, we define for every</p><formula xml:id="formula_13">A ⊆ V q V (x A | x W ) ≡ x V \A ∈X V \A q V (x V | x W ); q V (x V \A | x W ∪A ) ≡ q V (x V | x W ) q V (x A | x W ) .</formula><p>(5)</p><p>In general, for R, S ⊆ V and T ⊂ W , the density q V (x R | x S∪T ) may not be defined, since in the absence of a distribution over X W , we cannot integrate out the variables X W \T (though see Definition 4 and subsequent comments below). For disjoint V 1 ∪V 2 = V and W 1 ∪W 2 = W , we will sometimes write q</p><formula xml:id="formula_14">V (x V 1 , x V 2 | x W 1 , x W 2 ) to mean q V (x V 1 ∪V 2 | x W 1 ∪W 2 ).</formula><p>2.4. Independence in kernels. We extend the notion of conditional independence to kernels over X V indexed by X W . A rigorous treatment of conditional independence in settings where not all variables are random was given in <ref type="bibr" target="#b6">Constantinou (2013)</ref>.</p><p>Definition 4. For disjoint subsets A, B, C ⊆ V ∪ W , we define X A to be conditionally independent of X B given X C in a kernel q V , written</p><formula xml:id="formula_15">X A ⊥ ⊥ X B | X C [q V ] if either: (a) A ∩ W = ∅ and q V (x A | x B , x C , x W \(B∪C) ) is a function only of x A and</formula><p>x C (whenever this kernel is defined). In this case, we define q</p><formula xml:id="formula_16">V (x A | x C ) ≡ q V (x A | x B , x C , x W \(B∪C) ), or (b) B ∩ W = ∅ and q V (x B | x A , x C , x W \(A∪C) ) is a function only of x B and</formula><p>x C (whenever this kernel is defined). In this case, we define q</p><formula xml:id="formula_17">V (x B | x C ) ≡ q V (x B | x A , x C , x W \(B∪C) ).</formula><p>See Figure <ref type="figure" target="#fig_1">5</ref> for an illustration of cases; it is necessary for either A or B not to intersect W because there is no joint distribution over elements in W . This definition reduces to ordinary conditional independence in the case W = ∅. The condition that the kernel should be defined simply addresses the situation where the conditioning event has zero probability density. We remark that if (A ∪ B) ∩ W = ∅, then both definitions hold (or fail to hold) at the same time; the conditions become equivalent to saying that the kernel q V (x A , x B | x C , x W \C ) should factorize into a piece that depends only upon x A , x C and a piece that depends only upon x B , x C . Note that the kernels appearing in (a) and <ref type="bibr">(b)</ref> specify values for all of the variables X W , and are defined via conditioning in (a margin of) the original kernel q V . For example, in (a),</p><formula xml:id="formula_18">q V (x A | x B , x C , x W \(B∪C) ) = q V (x V ∩(A∪B∪C) | x W )/q V (x V ∩(B∪C) | x W ). Proposition 5. In a kernel q V (x V | x W ), X A ⊥ ⊥ X B | X C if and only if either X A ⊥ ⊥ X B∪(W \C) | X C or X B ⊥ ⊥ X A∪(W \C) | X C .</formula><p>2.5. Semigraphoid axioms in kernels. Classical conditional independence constraints may logically imply other such constraints. Though no finite axiomatization of these connections is possible <ref type="bibr" target="#b40">(Studený, 1992)</ref>, deductive derivations of conditional independence constraints in DAGs can be restricted to the semigraphoid axioms of symmetry and the "chain rule" <ref type="bibr" target="#b8">(Dawid, 1979)</ref>, which we reproduce here:</p><formula xml:id="formula_19">(X A ⊥ ⊥ X B | X C ) ⇔ (X B ⊥ ⊥ X A | X C ), (X A ⊥ ⊥ X B | X C∪D ) ∧ (X A ⊥ ⊥ X D | X C ) ⇔ (X A ⊥ ⊥ X B∪D | X C ).</formula><p>(The chain rule axiom is sometimes written as the three separate axioms of contraction, decomposition, and weak union.) We now show that, unsurprisingly, conditional independence constraints defined for kernels also obey these axioms. An additional set of axioms called separoids has been shown to apply to versions of conditional independence involving nonstochastic variables like X W <ref type="bibr" target="#b6">(Constantinou, 2013)</ref>.</p><formula xml:id="formula_20">R H T W qV (xV | xW ) R H T W q * V \H (x V \H | xW ∪H )</formula><p>Fig. <ref type="figure">6</ref>: Structure of sets for invariance properties considered in Section 2.6; V = R ∪ H ∪ T ; shaded sets are fixed; see equation ( <ref type="formula" target="#formula_22">6</ref>). Note that here vertices earlier in the ordering are on the right, so R ≻ H ≻ T ≻ W .</p><p>Proposition 6. The semigraphoid axioms are sound for kernel independence.</p><p>2.6. Constructing kernels. We will typically construct new kernels via the operation of dividing either a distribution p(x V ) by p(x H | x T ) or an existing kernel q</p><formula xml:id="formula_21">V (x V | x W ) by q V (x H | x T ∪W )</formula><p>, where H ∪ T ⊆ V . For the results in the remainder of this section, we will consider a kernel q</p><formula xml:id="formula_22">V (x V | x W ) where V = R ∪ H ∪ T , and a new object q * V \H (x V \H | x H , x W ) = q * V \H (x R , x T | x H , x W ) ≡ q V (x R , x H , x T | x W ) q V (x H | x T , x W ) .<label>(6)</label></formula><p>See Figure <ref type="figure">6</ref> for an illustration.</p><formula xml:id="formula_23">Lemma 7. q * V \H (x R , x T | x H , x W ) is a kernel.</formula><p>Lemma 8. For the kernel constructed in (6),</p><formula xml:id="formula_24">q * V \H (x R , x T | x H , x W ) = q V (x R | x H , x T , x W )q V (x T | x W ),<label>(7)</label></formula><p>and hence</p><formula xml:id="formula_25">q * V \H (x R | x H , x T , x W ) = q V (x R | x H , x T , x W ); (8) q * V \H (x T | x H , x W ) = q V (x T | x W ) = q * V \H (x T | x W ). (9) Note that if R = ∅, q * V \H (x V \H | x H , x W ) = x H q V (x V | x W ) = q V (x T | x W ).</formula><p>Proposition 9 (Separation). For the kernel constructed in (6):</p><formula xml:id="formula_26">(i) (X H ⊥ ⊥ X T | X W ) holds in q * V \H (x V \H | x H , x W ). (ii) If X V ⊥ ⊥ X W \W 1 | X W 1 in q V , then X H∪(W \W 1 ) ⊥ ⊥ X T | X W 1 in q * V \H . (iii) If R = ∅, X H ⊥ ⊥ X V \H | X W in q * V \H . A B C R H T W (i) Independence in qV (xT | xW ). R H T W B A C (ii) Independence in qV (xR | xH , xT , xW ).</formula><p>Fig. <ref type="figure" target="#fig_13">7</ref>: Independences that are preserved after fixing in a kernel described in (i) Proposition 10 (where A ⊆ T ), and (ii) Proposition 11.</p><p>2.7. Preservation of existing independences in a kernel. We now state two important properties that transfer conditional independence statements to a new kernel formed via the operation (6).</p><p>Proposition 10 (Ordering). Given disjoint sets A, B, C, where C may be empty, if A, B, C ⊆ T ∪ W , then</p><formula xml:id="formula_27">X A ⊥ ⊥ X B | X C [q V ] ⇔ X A ⊥ ⊥ X B | X C q * V \H .<label>(10)</label></formula><p>This result, which follows directly from ( <ref type="formula" target="#formula_105">9</ref>), states that, given any ordering of variables in which W ∪T precedes H, which in turn precedes R, performing the operation in (6) on X H preserves conditional independence statements among variables that precede H in the ordering; see <ref type="bibr">Figure 7(i)</ref>. Note that, by Definition 4, both independences in (10) imply either A ⊆ T or B ⊆ T .</p><p>Proposition 11 (Modularity). Given disjoint sets A, B, C, where C may be empty, if</p><formula xml:id="formula_28">A ⊆ R and (B ∪ C) ⊇ H ∪ T , then X A ⊥ ⊥ X B | X C [q V ] ⇔ X A ⊥ ⊥ X B | X C q * V \H .</formula><p>This result, which follows directly from <ref type="bibr" target="#b56">(8)</ref>, is illustrated in Figure <ref type="figure" target="#fig_13">7</ref>(ii). In words, it says that if we express a probability distribution as a set of factors via the chain rule of probability, and a conditional independence statement can be stated exclusively in terms of one of the factors, then applying the operation in ( <ref type="formula" target="#formula_22">6</ref>) such that another factor is dropped does not affect this conditional independence statement. In other words, "factors are modular." 2.8. Markov properties for CADMGs. As described earlier, a CADMG G(V, W ) represents the independence structure of a kernel q V (x V | x W ). We now introduce a number of Markov properties for a given kernel, the equivalence of which we will state in Section 2.10. 2.8.1. The CADMG global Markov property. The global Markov property for CADMGs may be derived from m-separation via the following simple construction.</p><p>Definition 12. Given a CADMG G(V, W ), we define G |W to be a mixed graph with vertex set V * = V ∪ W , and edge set</p><formula xml:id="formula_29">E * ≡ E ∪ {w ↔ w ′ | w, w ′ ∈ W }.</formula><p>In words, the graph G |W is formed from G by adding bidirected edges between all pairs of vertices w, w ′ ∈ W , and then eliminating the distinction between vertices in V and W ; see Figure <ref type="figure" target="#fig_6">4</ref>(ii) for an example. Definition 13. A kernel q V satisfies the global Markov property for a CADMG G(V, W ) if for arbitrary disjoint sets A, B, C, (C may be empty),</p><formula xml:id="formula_30">A is m-separated from B given C in G |W =⇒ X A ⊥ ⊥ X B | X C [q V ].</formula><p>We denote this set of kernels P c m (G), where "c" and "m" denote C ADMG and m-separation, respectively. 2.8.2. The CADMG local Markov property. The (ordered) local Markov property for a DAG states that each vertex v is independent of vertices prior to v (under a topological ordering) conditional on the parents of v. In a CADMG, the Markov blanket plays a role analogous to that of the set of parents.</p><p>If t ∈ V , then the Markov blanket of t in G(V, W ) is defined as</p><formula xml:id="formula_31">(11) mb G (t) ≡ pa G dis G (t) ∪ dis G (t) \ {t} . Given a vertex t ∈ V such that ch G (t) = ∅, a kernel q V obeys the local Markov property for G at t if (12) X t ⊥ ⊥ X (V ∪W )\(mb G (t)∪{t}) | X mb G (t) [q V ].</formula><p>If ≺ is a topological total ordering on the vertices in G, then for a subset A define max ≺ (A) to be the ≺-greatest vertex in A.</p><p>We define the set of kernels obeying the ordered local Markov property for the CADMG G(V, W ) under the ordering ≺ as follows:</p><formula xml:id="formula_32">P c l (G, ≺) ≡ q V (x V | x W ) for every ancestral set A, with max ≺ (A) ∈ V, q V (x V ∩A | x W ) obeys the local Markov property for (13) G(V ∩ A, W ) at max ≺ (A) .</formula><p>We will make use of the following extension of the Markov blanket notation: for an ancestral set A in a CADMG G and a vertex t ∈</p><formula xml:id="formula_33">V ∩ A such that ch G (t) ∩ A = ∅, let (14) mb G (t, A) ≡ mb G A (t).</formula><p>Proposition 14. Given a CADMG G, an ancestral set A and a random vertex t ∈ A such that ch G (t) ∩ A = ∅:</p><formula xml:id="formula_34">(i) mb G (t, A) ⊆ mb G (t) ⊆ D ∪ pa G (D), where t ∈ D ∈ D(G); (ii) if A * is an ancestral set and t ∈ A * ⊆ A, then mb G (t, A * ) ⊆ mb G (t, A).</formula><p>2.8.3. The CADMG augmented Markov property. The following is the analog of moralization in DAGs for CADMGs. For a CADMG G(V, W ), the augmented graph derived from G, denoted (G) a , is an undirected graph with vertex set V ∪W such that c -d in (G) a if and only if c and d are connected by a path in G |W containing only colliders (or a single edge).</p><p>A kernel q V obeys the augmented Markov property for a CADMG G(V, W ) if X A ⊥ ⊥ X B | X C in q V whenever: for arbitrary disjoint sets A, B, C (C may be empty), every path in (G an(A∪B∪C) ) a from any a ∈ A to any b ∈ B passes through a vertex in C. We denote the set of such kernels by P c a (G).</p><p>2.9. Tian factorization for CADMGs. The joint distribution under a DAG model may be factorized into univariate densities. In DAG models, these factors take the form p(x a | x pa G (a) ). This factor is a conditional distribution for a singleton variable X a , given the set of variables corresponding to parents of a in the graph. The factorization property may be generalized to CADMGs by requiring factorization of q V into kernels over districts.</p><p>We define the set of kernels that Tian factorize with respect to a CADMG:</p><formula xml:id="formula_35">P c f (G) ≡ q V for every ancestral set A, there exist kernels f A D (•|•) s.t. q V (x V ∩A | x W ) = D∈D(G A ) f A D (x D | x pa G (D)\D ) .<label>(15)</label></formula><p>In the next lemma, we show that the terms <ref type="formula" target="#formula_35">15</ref>) are equal to products of univariate conditional densities, that is, instances of the gformula of <ref type="bibr" target="#b28">Robins (1986)</ref>, and thus f A D (• | •) does not depend on A other than through D.</p><formula xml:id="formula_36">f A D (• | •) arising in (</formula><p>Lemma 15. Let G be a CADMG with topological ordering ≺. If q V ∈ P c f (G), then for every ancestral set A and every D ∈ D(G A ), we have</p><formula xml:id="formula_37">(16) f A D (x D | x pa(D)\D ) = d∈D q V (x d | x T (d,D) ≺ ),</formula><p>where</p><formula xml:id="formula_38">T (d,D) ≺ ≡ mb G (d, an G (D) ∩ (pre G,≺ (d) ∪ {d})), so that (17) q V (x d | x T (d,D) ≺ ) = q V (x d | x A∩pre G,≺ (d) , x W ).</formula><p>Conversely, if (17) holds for all d ∈ A and ancestral sets A, and the f A D functions are defined by ( <ref type="formula">16</ref>), then q V ∈ P c f (G).</p><p>Note that by Proposition 14, mb</p><formula xml:id="formula_39">G (d, an G (D) ∩ pre G,≺ (d)) ⊆ D ∪ pa G (D)</formula><p>. Lemma 15 has the following important consequence:</p><formula xml:id="formula_40">P c f (G) = q V for every ancestral set A, q V (x V ∩A | x W ) = D∈D(G A ) q D (x D | x pa(D)\D ) ,<label>(18)</label></formula><p>where q D (x D | x pa(D)\D ) is defined via the right-hand side of ( <ref type="formula">16</ref>) under any topological ordering. In a context where we refer to q V and q D for D ∈ D(G), it is implicit that q D is derived from q V in this way. In Section 3 we will extend this notation to include other "reachable" sets.</p><p>2.10. Equivalence of factorizations and Markov properties for CADMGs. The above definitions describe the same set of kernels due to the following result.</p><formula xml:id="formula_41">Theorem 16. P c f (G) = P c l (G, ≺) = P c m (G) = P c a (G).</formula><p>We thus use P c (G) to denote the set of such kernels, and simply refer to a kernel q V ∈ P c (G) as being Markov with respect to a CADMG G.</p><p>2.11. The fixing operation and fixable vertices. We now introduce a "fixing" operation on an ADMG or CADMG that has the effect of transforming a random vertex into a fixed vertex, thereby changing the graph. However, we define this operation only for a subset of the vertices in the graph, which we term the set of (potentially) fixable vertices.</p><formula xml:id="formula_42">Definition 17. Given a CADMG G(V, W ), the set of fixable vertices is F(G) ≡ v ∈ V | dis G (v) ∩ de G (v) = {v} .</formula><p>In words, a vertex v is fixable in G if there is no other vertex t that is both a descendant of v and in the same district as v in G. For details on the causal interpretation of fixing, see Section 4.</p><formula xml:id="formula_43">Proposition 18. For any v ∈ V , we have de G (v) ∩ dis G (v) ∩ F(G) = ∅.</formula><p>Thus, if a vertex in a district is not fixable then there is a descendant of the vertex within the district that is fixable. In particular, every district contains at least one vertex that is fixable.</p><p>Recall that mb G (t), defined in (11), is the set of vertices v ∈ (V \{t})∪ W , which can be reached via paths of the form: <ref type="formula">12</ref>) follows by the CADMG local Markov property. More generally, if t is fixable then</p><formula xml:id="formula_44">t ↔ • • • ↔ v or t ↔ • • • ↔← v or t ← v. Suppose that q V is Markov with respect to the CADMG G; if ch G (t) = ∅, then (</formula><formula xml:id="formula_45">X t ⊥ ⊥ X nd G (t)\mb G (t) | X mb G (t) [q V ] (19) follows from m-separation in G |W ; thus, in addition, X t ⊥ ⊥ X W \mb G (t) | X mb G (t) in q V . These hold even if ch G (t) = ∅.</formula><p>Definition 19. Given a CADMG G(V, W ), and a kernel q V (x V | x W ), for every r ∈ F(G), we associate a fixing transformation φ r on the graph G defined as follows:</p><formula xml:id="formula_46">φ r (G) ≡ G * V \ {r}, W ∪ {r} ,</formula><p>where G * (V \ {r}, W ∪ {r}) has precisely the subset of edges in G(V, W ) that do not have arrowheads at r. With slight abuse of notation, we define a fixing transformation φ r on the pair (q</p><formula xml:id="formula_47">V (x V | x W ), G): (20) φ r q V (x V | x W ); G ≡ q V (x V | x W ) q V (x r | x mb G (r) )</formula><p>.</p><formula xml:id="formula_48">Note that V(φ r (G)) = V(G) \ {r} and W(φ r (G)) = W(G) ∪ {r}, so that φ r (G) is a new CADMG in</formula><p>which the status of r changes from random to fixed, while φ r (q V ; G) forms a new kernel, per <ref type="bibr">Lemma 7.</ref> Although the CADMG φ r (G) is determined solely by the graph G given as input, the transformation φ r (q V (x V | x W ); G) on the kernel q V (x V | x W ) is a function of both the graph and the kernel itself.</p><p>Proposition 20. If q V is Markov with respect to a CADMG G(V, W ) and ch G (r) = ∅ (and hence r ∈ F(G)), then</p><formula xml:id="formula_49">φ r q V (x V | x W ); G = xr q V (x V | x W ) = q V (x V \{r} | x W ).</formula><p>Thus, if ch G (r) = ∅, then φ r simply marginalizes over X r : the conditioning on X r in φ r (q V (x V | x W ); G) is vacuous in the sense that the resulting kernel does not depend on the value of X r . Though it may at first appear unnatural, it greatly simplifies our subsequent analyses to unify marginalization and fixing in this way.</p><formula xml:id="formula_50">Proposition 21. If q V is Markov w.r.t. G(V, W ) and r ∈ F(G), then φ r q V (x V | x W ); G = q V (x V | x W )/q V (x r | x nd G (r) ). (21) Lemma 22. If r ∈ F(G), then F(G) \ {r} ⊆ F(φ r (G)).</formula><p>That is, any vertex s that was fixable before r was fixed is still fixable after r has been fixed (with the obvious exception of r itself). Thus, when fixing vertices, although our choices may be limited at various stages, we are never faced with a choice between fixing r and r ′ , whereby choosing r precludes subsequently fixing r ′ .</p><p>Proposition 23. If G is a subgraph of G * with the same random and fixed vertex sets, then F(G * ) ⊆ F(G).</p><p>For conciseness, we use D r to denote dis G (r) when the graph G is clear from the context.</p><formula xml:id="formula_51">Proposition 24. Let G be a CADMG, with r ∈ F(G). If r ∈ D r ∈ D(G), then D φ r (G) = D(G) \ D r ∪ D(G D r \{r} ),</formula><p>where the sets on the right are disjoint. Thus, if</p><formula xml:id="formula_52">D ∈ D(φ r (G)) then D ⊆ D * for some D * ∈ D(G); further if D = D * , then r ∈ D * .</formula><p>In words, the set of districts in φ r (G), the graph obtained by fixing r, consists of the districts in G that do not contain r, together with new districts that are subsets of D r , the district in G that contains r. The new districts are bidirected-connected subsets of D r after removing r.</p><p>2.12. Fixing and factorization.</p><p>Proposition 25. Take a CADMG G(V, W ) with kernel q V ∈ P c (G) with associated district factorization:</p><formula xml:id="formula_53">(22) q V (x V | x W ) = D∈D(G) q D (x D | x pa G (D)\D ),</formula><p>where the kernels q D (x D | x pa G (D)\D ) are defined via the right-hand side of (16</p><formula xml:id="formula_54">). If r ∈ F(G) and D r ∈ D(G) is the district containing r, then φ r q V (x V | x W ); G = q D r (x D r \{r} | x pa G (D r )\D r ) D∈D(G)\{D r } q D (x D | x pa G (D)\D ).</formula><p>The proof is found in the Supplementary Material, Section A.6 in <ref type="bibr" target="#b26">Richardson et al. (2023)</ref>. In words, the result of a fixing operation is solely to marginalize the variable X r from the density q D r associated with the district D r in which the vertex r occurs, while leaving unchanged all of the other terms q D in the factorization.</p><p>2.13. Reachable graphs derived from an ADMG. A sequence of distinct vertices w is said to be valid in G if either w = , or w = w 1 , w 2 , . . . , w k , w 1 ∈ F(G) and w 2 , . . . , w k is valid in φ w 1 (G). Given a valid w, we define the fixing operator φ w (G) on graphs, and the fixing operator φ w (q V ; G) on kernels inductively as follows:</p><formula xml:id="formula_55">φ (G) ≡ G; φ w 1 ,...,w k (G) ≡ φ w 2 ,...,w k φ w 1 (G) ; φ (q V ; G) ≡ q V ; φ w 1 ,...,w k (q V ; G) ≡ φ w 2 ,...,w k φ w 1 (q V ; G); φ w 1 (G) . Definition 26. A CADMG G(V, W ) is reachable from an ADMG G * (V ∪ W ) if there exists a valid fixing sequence w of the vertices in W such that G = φ w (G * ).</formula><p>In words, a graph is reachable from G * if there exists an ordering on vertices in W such that the first element w 1 in the ordering may be fixed in G * , the second element w 2 in φ w 1 (G * ), the third element w 3 in φ w 1 ,w 2 (G * ), and so on. Note that by definition</p><formula xml:id="formula_56">G is reachable from itself. If a CADMG G(V, W ) is reachable from G * (V ∪ W ), we say that V is reachable in G * .</formula><p>A key result, which we will show later as Theorem 31, is that under the nested Markov model reachable CADMGs and their associated kernels are invariant with respect to any valid fixing sequence. It is not hard to see that if there are two valid fixing sequences w and u for W then φ w (G) = φ u (G). However, it requires more work to show that φ w (q V ; G) = φ u (q V ; G).</p><p>3. Nested Markov models. In this section, we define a set of recursive Markov properties and a factorization, and show their equivalence, in Proposition 29 and Theorem 38. The models, which obey these properties, will be called "nested" Markov models. For the rest of this section, we will fix an ADMG G(V ) and a density p(x V ).</p><p>Definition 27. We say that a distribution p(x V ) is globally nested Markov with respect to G(V ) if for all fixing sequences w valid in G, the kernel φ w (p(x V ); G) obeys the global Markov property for φ w (G).</p><p>Note that the same graph may be reached by more than one sequence, that is, φ w 1 (G) = φ w 2 (G) for two distinct valid sequences w 1 , w 2 .</p><p>Definition 28. We say that a distribution p(x V ) nested Markov factorizes with respect to G(V ) if, for all valid fixing sequences w in G, there exist kernels</p><formula xml:id="formula_57">{f w D (x D | x pa G (D)\D ) : D ∈ D(φ w (G))} such that (23) φ w p(x V ); G = D∈D(φw(G)) f w D (x D | x pa G (D)\D ).</formula><p>Proposition 29. With respect to an ADMG G(V ), a distribution p(x V ) is globally nested Markov if and only if it nested Markov factorizes.</p><p>3.1. Invariance to the order of fixing in an ADMG. In this section, we show that, given a distribution that obeys the nested Markov property with respect to an ADMG, any two valid fixing sequences that fix the same vertices will lead to the same reachable graph and kernel. For marginal distributions obtained from a hidden variable DAG, this claim follows by results in <ref type="bibr">Tian and Pearl (2002)</ref> on identification of causal effects in hidden variable DAG models. However, for distributions which obey the nested Markov property for an ADMG, but which are not derived from any hidden variable DAG, the claim is far less obvious; see Example 47 below. For instance in the ADMG in Figure <ref type="figure">8</ref>, the fixing sequence 4, 3, 1 , which leads to the kernel</p><formula xml:id="formula_58">q 1 2,5 (x 2 , x 5 | x 4 , x 3 , x 1 ) ≡ x 3 p(x 5 | x 4 , x 3 , x 2 , x 1 )p(x 3 , x 2 , x 1 ) x 3 ,x 2 ,x 5 p(x 5 | x 4 , x 3 , x 2 , x 1 )p(x 3 , x 2 , x 1 ) 1 2 3 4 5</formula><p>Fig. <ref type="figure">8</ref>: A graph where 4, 3, 1 and 3, 4, 1 are valid fixing sequences.</p><p>and the fixing sequence 3, 4, 1 , which leads to the kernel</p><formula xml:id="formula_59">q 2 2,5 (x 2 , x 5 | x 4 , x 3 , x 1 ) ≡ p(x 5 | x 4 , x 3 , x 2 , x 1 )p(x 2 , x 1 ) x 5 ,x 2 p(x 5 | x 4 , x 3 , x 2 , x 1 )p(x 2 , x 1 )</formula><p>are both valid, and these two kernels are therefore the same, in the context of our model. This is not entirely obvious from inspecting these expressions.</p><p>In addition, q 1 2,5 and q 2 2,5 are not functions of x 3 in our model; this is clear for q 1 2,5 since x 3 is summed out, but not so obvious for q 2 2,5 .</p><p>Lemma 30. Let G(V, W ) be a CADMG with r, s ∈ F(G) and let q V be a kernel Markov w.r.t. G. Then</p><formula xml:id="formula_60">φ r,s (G) = φ s,r (G) and φ r,s (q V ; G) = φ s,r (q V ; G).</formula><p>In words, if we have a choice to fix two vertices in G then the order in which we do this does not affect the resulting graph, or kernel, provided that the original kernel is Markov with respect to G.</p><p>Theorem 31. Let p(x V ) be a distribution that is nested Markov with respect to an ADMG G (in either the sense of Definitions 27 or 28). Let u, w be different valid fixing sequences for the same set</p><formula xml:id="formula_61">W ⊂ V . Then φ u (G) = φ w (G) and (24) φ u p(x V ); G = φ w p(x V ); G .</formula><p>Due to this theorem, our fixing operations φ w , which were defined for a specific fixing sequence w, can, under the model, be defined purely in terms of the set W of nodes that were fixed; the order does not matter (provided that at least one valid fixing sequence exists). Consequently, we will subscript the fixing operator φ by a set rather than a sequence. That is, we write φ V \R (G) and φ V \R (p(x V ); G) to mean "apply the fixing operator φ w , for any valid sequence w of elements in V \ R, to G and the pair (p(x V ); G), respectively." For conciseness, and consistency with notation in <ref type="bibr">Tian and Pearl (2002)</ref>, we will also denote φ</p><formula xml:id="formula_62">V \R (G) by G[R].</formula><p>imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023</p><p>Corollary 32. If a distribution p(x V ) obeys the global nested Markov property for G(V ), then for all reachable sets R, the kernel</p><formula xml:id="formula_63">q R (x R | x pa(R)\R ) ≡ φ V \R p(x V ); G obeys the global Markov property for φ V \R (G).</formula><p>We will subsequently see that if we assume the existence of a latent variable DAG model (with observed variables V ∪ W ) that has latent projection G, then if W is fixable, the kernel φ W (p(x V ∪W ); G) can be interpreted as the intervention distribution p(x V | do G (x W )); see <ref type="bibr">Lemma D.3</ref> in the Supplementary Material. In this context, a valid fixing sequence corresponds to a sequence of steps in the ID algorithm of <ref type="bibr">Tian and Pearl (2002)</ref> that identify this intervention distribution; see Section 4.3. Consequently, were we to assume the existence of a DAG with latent variables, then the soundness of the ID algorithm would directly imply the equality (24). However, since we are not assuming such a DAG exists, φ W (p(x V ∪W ); G) may not correspond to an intervention distribution, and hence a separate proof is required; see Example 47 for an inequality constraint that is implied by the existence of a latent variable, but does not follow from the nested Markov property applied to the latent projection.</p><p>3.2. Intrinsic sets. We introduce the following two definitions that will prove useful.</p><formula xml:id="formula_64">Definition 33. A set C is intrinsic if it is a district in a reachable graph derived from G. The set of intrinsic sets in an ADMG G is denoted by I(G). Definition 34. For a set R reachable in G and a vertex v ∈ R, with ch φ V \R (G) (v) = ∅, we define the Markov blanket of v in R to be (25) mb G (v, R) ≡ mb φ V \R (G) (v).</formula><p>Since every ancestral set A is reachable in G, this is a natural extension of our previous definition ( <ref type="formula">14</ref>). The next two subsections give different characterizations of the nested Markov model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The simplified nested factorization property.</head><p>Theorem 35. If p(x V ) nested Markov factorizes with respect to G, then for every reachable R in G,</p><formula xml:id="formula_65">φ V \R p(x V ); G = D∈D(φ V \R (G)) φ V \D p(x V ); G . (26)</formula><p>Since all the sets D quantified in the product are districts in a reachable graph derived from G, it follows that in a nested Markov model every kernel corresponding to a reachable set can be constructed by combining kernels corresponding to intrinsic sets. We call (26) the simplified nested factorization.</p><p>3.4. The ordered local nested property. Notwithstanding Theorem 31, in general we may not know a priori that a distribution is Markov with respect to a graph. It is therefore helpful to have a canonical order in which variables should be fixed for the purposes of verifying that a distribution is in the nested Markov model.</p><p>Definition 36. We define the intrinsic power DAG for an ADMG G and topological ordering to be the graph I(G) whose vertices are intrinsic sets I(G), and such that there is an edge from D to C if one can obtain C as the district of the maximal vertex in D by fixing some other vertex in D.</p><p>To simplify the next definition, we set fam G (C) := C ∪ pa G (C).</p><p>Definition 37. Let G be an ADMG with arbitrary topological order ≺; define T v = {w : w ≺ v} ∪ {v} as the initial segment for v. A distribution p(V ) is ordered local nested Markov with respect to ≺, if:</p><p>(i) for each v ∈ V , we have the independences</p><formula xml:id="formula_66">X v ⊥ ⊥ X Tv\fam(C) | X fam(C)\{v} [p], (<label>27</label></formula><formula xml:id="formula_67">)</formula><p>where C = dis Tv (v); (ii) for every edge D → C in I(G) (obtained by fixing w ∈ D \ {v}) and v being maximal under ≺ in D, we have</p><formula xml:id="formula_68">X v ⊥ ⊥ X fam(D)\(fam(C)∪{w}) | X fam(C)\{v} φ w q D ; G[D] ,<label>(28)</label></formula><p>where q D is the unique kernel resulting from some valid fixing sequence for D.</p><p>Note that in this definition, we assume that we have reached each node D by traversals from a root node (i.e., a set C in (i) for some v). In this way, crossing the edge D → C will (potentially) introduce a new conditional independence, as well as defining q C . Theorem 38. p(x V ) is globally nested Markov with respect to G if and only if p(x V ) is ordered local nested Markov with respect to G for any topological ordering ≺.</p><p>Proposition 29 and Theorem 38 thus yield three equivalent ways of defining the set P n (G) of distributions in the nested Markov model of G.</p><p>3.5. Nested Markov models for complete graphs are saturated. It is known that any distribution is Markov relative to a complete DAG or ADMG (i.e., a graph with at least one edge between every pair of vertices). This also holds in the nested Markov case.</p><p>Theorem 39. Let G be an ADMG. The model P n (G) is saturated if and only if for every valid fixing sequence r 1 , . . . , r k , and every i, j ∈ 1, . . . , k, either</p><formula xml:id="formula_69">r i ∈ mb G (j) (r j ) or r j ∈ mb G (i) (r i ). Here, G (1) ≡ G and G (ℓ+1) ≡ φ r ℓ (G (ℓ) ).</formula><p>Corollary 40. Let G be a complete ADMG; then P n (G) is saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Connections with causal inference. The fixing operation is closely related to the identification of intervention distributions.</p><p>4.1. Latent variable DAG models are in the nested Markov model. We first show that if p(x L∪V ) is Markov relative to a DAG G(L ∪ V ), then p(x V ) is in the nested Markov model of G(V ). We extend the latent projection operation in the natural way from ADMGs to CADMGs, and denote the operation of creating a latent projection G(V, W ) of a CADMG G(L ∪ V, W ) onto the subset V as σ L . That is, σ L (G(L∪V, W )) = G(V, W ), and G(V, W ) |W encodes the m-separation relations holding among V ∪ W in G(L ∪ V, W ) |W . Precise statements appear in the Supplementary Material, Section A.3.</p><p>We will call a CADMG, which does not contain bidirected arrows, a conditional acyclic directed graph (CDAG). It is a corollary of the definition of</p><formula xml:id="formula_70">P c f that if G(V, W ) is a CDAG, then q V (x V | x W ) ∈ P c f (G) if q V (x V | x W ) = a∈V q V (x a | x pa G (a) ).</formula><p>Lemma 41. Let G be a DAG with a vertex set V . Then every nonempty subset S of V is reachable, and if p(x V ) is Markov with respect to G,</p><formula xml:id="formula_71">φ V \S p(x V ); G = q S (x S | x pa G (S)\S ) = a∈S p(x a | x pa G (a) ). In other words, φ V \S (p(x V ); G) ∈ P c (φ V \S (G)).</formula><p>For a DAG G, let P d (G) denote the set of distributions Markov with respect to G (see the Supplementary Material, Section A.1, for details). That P d (G) ⊆ P n (G) is a special case of Theorem 46 below when L is empty.</p><formula xml:id="formula_72">G(L ∪ V, W ) G((L ∪ V ) \ {v}, W ∪ {v}) G(V, W ) G(V \ {v}, W ∪ {v}) φv σL σL (a) φv qL∪V (xL∪V | xW ) q (L∪V )\{v} (x (L∪V )\{v} | x W ∪{v} ) qL∪V (xV | xW ) q V \{v} (x V \{v} | x W ∪{v} ) φv(.; G(L ∪ V, W )) x L x L (b) φv(.; G(V, W ))</formula><formula xml:id="formula_73">Lemma 43. Let G(L ∪ V, W ) be a CDAG. Assume v ∈ V is fixable in G(V, W ) = σ L (G(L ∪ V, W )). Then σ L (φ v (G(L ∪ V, W ))) = φ v (σ L (G(L ∪ V, W ))).</formula><p>That is, the commutative diagram in Figure <ref type="figure" target="#fig_2">9</ref>(a) holds.</p><p>Note that v ∈ V is fixable in G(V, W ) by assumption, while the fact that every element of V is fixable in G(L ∪ V, W ) follows since G(L ∪ V, W ) is a CDAG, and has no bidirected edges. In fact, an inspection of the proof of this lemma (found in the Supplementary Material) shows it does not rely on the vertex v being fixable in G(V, W ), only on the specific way edges are removed by φ. In Lemma 45, we give a more general version of this result, that is useful for deriving properties of causal models, which we discuss later in Section 4.2.</p><p>Given a DAG G(V ∪L), there is always a well-defined intervention distribution associated with any vertex v ∈ V ; however, if v is not fixable in G(V ), then this distribution may not be identifiable from p(x V ); see <ref type="bibr">(29)</ref> imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023 below. We use φ * v to denote the graphical operation that corresponds to this intervention, and note that this applies to all vertices in G(V ), as opposed to φ v , which presupposes identifiability from p(x V ), and hence applies only to elements in F(G(V )). The resulting graph φ * v (G(V )), without distinguishing fixed and random vertices, was denoted G v by <ref type="bibr" target="#b22">Pearl (2000)</ref>. We use φ * v to make the connection to fixing more explicit.</p><p>For any r, s ∈ V , φ * r,s (G) = φ * s,r (G), and so for any Z ⊆ V , we define φ * Z (G) inductively under any order as in Lemma 30.</p><formula xml:id="formula_74">Corollary 44. Let G(L ∪ V, W ) be a CDAG. Then for any v ∈ V , σ L (φ * v (G(L ∪ V, W ))) = φ * v (σ L (G(L ∪ V, W ))).</formula><p>Corollary 44 is essentially equivalent to Proposition 8 of Evans (2016).</p><p>Lemma 45. Let G(L∪V, W ) be a CDAG, and assume</p><formula xml:id="formula_75">q L∪V (x L∪V | x W ) ∈ P c f (G(L ∪ V, W )). Assume v ∈ V is fixable in G(V, W ) = σ L (G(L ∪ V, W )). Then x L φ v q L∪V (x L∪V | x W ); G(L∪V, W ) = φ v q L∪V (x V | x W ); σ L G(L∪V, W )</formula><p>That is, the commutative diagram in Figure <ref type="figure" target="#fig_2">9</ref>(b) holds. <ref type="bibr" target="#b28">Robins (1986)</ref> proves a similar result that he calls the "collapse of the g-formula." We now have enough to prove the main result of this section.</p><p>Theorem 46. Let G(V ∪ L) be a DAG. Then</p><formula xml:id="formula_76">p(x V ∪L ) ∈ P d G(V ∪ L) ⇒ p(x V ) ∈ P n G(V ) .</formula><p>Thus, the constraints implied by the nested model for the latent projection of G(V ∪ L) also hold in the hidden variable CDAG model. Note that the converse is not true in general. There are distributions p(x V ) ∈ P n (G(V )) for which there exists no joint distribution p(x V ∪L ) ∈ P d (G(V ∪ L)). This is because marginals of hidden variable DAGs may induce additional inequality constraints, which are not satisfied by all elements of the associated nested Markov model. The best known of these are the instrumental inequalities of <ref type="bibr" target="#b21">Pearl (1995)</ref>, which were generalized by <ref type="bibr" target="#b4">Bonet (2001)</ref>, <ref type="bibr" target="#b9">Evans (2012)</ref> and <ref type="bibr" target="#b15">Kédagni and Mourifié (2020)</ref>. Building on the above result, <ref type="bibr">Evans (2018)</ref> showed further that the discrete latent variable DAG model does not imply any additional equality constraints not implied by the discrete nested Markov model.</p><p>Example 47. Consider variables X 0 , . . . , X 4 under a distribution, which is Markov with respect to the graph in Figure <ref type="figure" target="#fig_0">1(i)</ref>. Then the marginal distribution over X 1 , . . . , X 4 satisfies the nested Markov property with respect to the graph in Figure <ref type="figure" target="#fig_21">3</ref>(i). However, if the observed variables are binary (and regardless of the state-space of X 0 ) their marginal distribution also satisfies the following inequality constraints not implied by the nested Markov property:</p><formula xml:id="formula_77">0 ≤ q 24 (0 2 | x 1 ) + q 24 (0 4 | x 3 ) + q 24 (0 2 , 0 4 | 1 -x 1 , 1 -x 3 ) -q 24 (0 2 , 0 4 | x 1 , x 3 ) -q 24 (0 2 , 0 4 | x 1 , 1 -x 3 ) -q 24 (0 2 , 0 4 | 1 -x 1 , x 3 ) ≤ 1</formula><p>for each x 1 , x 3 ∈ {0, 1}; here, for example, 0 2 is a shorthand for the event {X 2 = 0}. These are related to the CHSH inequalities of <ref type="bibr" target="#b5">Clauser et al. (1969)</ref>, and are sometimes referred to as Bell inequalities after <ref type="bibr" target="#b2">Bell (1964)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.</head><p>Causal model of a DAG. The statistical model of a DAG G with vertices V , described earlier, is a set of distributions p(x V ) defined by the factorization (A.1) formulated on G. We define a causal model of a DAG G by a set of similar factorizations that yield joint distributions under an intervention operation, which corresponds to setting values of variables from outside the system.</p><p>Specifically, for a DAG G with vertices V , and any A ⊆ V , the causal model for G defines the kernel resulting from intervention on x A to be</p><formula xml:id="formula_78">p x V \A | do G (x A ) ≡ v∈V \A p(x v | x pa G (v) ). (<label>29</label></formula><formula xml:id="formula_79">)</formula><p>This is known as the g-formula, truncated factorization or manipulated distribution. Note that since for any DAG G, P d (G) = P n (G), we have</p><formula xml:id="formula_80">p x V \A | do G (x A ) ≡ v∈V \A p(x v | x pa G (v) ) = φ A p(x V ); G . (<label>30</label></formula><formula xml:id="formula_81">)</formula><p>We will drop the G subscript from do(•) when the graph is obvious. Thus, (30) provides a causal interpretation of the fixing operation in a DAG.</p><p>In this light the earlier propositions relating to the preservation of independence in kernels may also be reinterpreted. Specifically, Proposition 9 (Separation) states that a variable that is intervened on no longer depends on its direct causes. Proposition 10 (Ordering) states that interventions in the future cannot causally affect the past. While Proposition 11 (Modularity) says that in the setting given, intervening and conditioning are interchangeable.</p><p>4.3. Reformulation of the ID algorithm via fixing. Identification of causal effects is a more difficult problem in causal DAGs where some variables are unobserved. In particular, not every distribution p(x Y | do(x A )) is identified in every G(L ∪ V ). Given a causal model G(L ∪ V ), the goal is to find an identifying functional for p(x Y | do(x A )) in terms of the observed marginal distribution p(x V ) or to show that no such functional exists.</p><p>The problem may be formalized by considering a latent projection ADMG G(V ) in place of the original causal DAG with hidden variables, G(L ∪ V ). A well-known "folklore" result in causal inference states that any two hidden variable DAGs G 1 (L 1 ∪ V ) and G 2 (L 2 ∪ V ) with the same latent projection G(V ) will share all identifying functionals, and so there is no loss of generality in reasoning on G(V ). We prove this folklore result later in this section as Corollary 49.</p><p>The ID algorithm, introduced by Tian and Pearl ( <ref type="formula">2002</ref>) solves the identification problem; the algorithm, which applies to ADMGs, was proved to be complete by <ref type="bibr">Shpitser and Pearl (2006)</ref> and also independently by <ref type="bibr" target="#b14">Huang and Valtorta (2006)</ref>. Here, "complete" means that whenever the algorithm fails to find an expression for p(x Y | do(x A )) in terms of p(x V ) in the causal model given by G(L ∪ V ), no other algorithm is able to do so without making more assumptions. In this section, we use the fixing operation to give a simple constructive characterization (via an algorithm) of all causal effects identifiable by the ID algorithm, and thus of all causal effects identifiable in any hidden variable causal DAG G(L∪V ). We can view this characterization as using the fixing operation to simplify the ID algorithm from its original two-page formulation down to the single formula (31).</p><p>Theorem 48. Let G(L∪V ) be a causal DAG with latent projection G(V ).</p><formula xml:id="formula_82">For A ∪ Y ⊆ V , let Y * = an G(V ) V \A (Y ). Then if D(G(V ) Y * ) ⊆ I(G(V )), p x Y | do G(L∪V ) (x A ) = x Y * \Y D∈D(G(V ) Y * ) p x D | do G(L∪V ) (x pa G(V ) (D)\D ) = x Y * \Y D∈D(G(V ) Y * ) φ V \D p(x V ); G(V ) . (31) If not, there exists D ∈ D(G(V ) Y * ) that is not intrinsic in G(V ), and p(x Y | do G(L∪V ) (x A )) is not identifiable in G(L ∪ V ).</formula><p>Note that Y * is the set of vertices v ∈ V \ A for which, for some y ∈ Y , there is a directed path v → • • • → y, with no vertex on the path in A; such paths are called "proper causal paths" in <ref type="bibr" target="#b24">Perković et al. (2015)</ref>. Also note that since, by Theorem 46,</p><formula xml:id="formula_83">X D ⊥ ⊥ X V \(D∪pa G(V ) (D)) | X pa G(V ) (D)\D in φ V \D (p(x V ); G(V )), it follows that φ V \D (p(x V ); G(V )</formula><p>) is a function solely of x D and x pa G(V ) (D)\D . Thus, the product on the RHS of ( <ref type="formula">31</ref>) is a function of the "bound" variables x Y * \Y present in the sum and (a subset of) the "input" variables on the LHS, x Y , x A * where A</p><formula xml:id="formula_84">* = A ∩ D∈D(G(V ) Y * ) pa G(V ) (D). Corollary 49. Let G 1 (L 1 ∪ V ) and G 2 (L 2 ∪ V ) be two causal DAGs, with the same latent projection, so G 1 (V ) = G 2 (V ). Then for any A ∪Y ⊆ V : (i) p(Y | do G 1 (A)) is identified if and only if p(Y | do G 2 (A)) is identified; (ii) if p(Y | do G 1 (A)) is identified, then p(Y | do G 1 (A)) = p(Y | do G 2 (A)).</formula><p>Example 50. Given some hidden variable DAG G(V ∪ L), where V = {x 1 , . . . , x 4 } with latent projection G(V ) given by the ADMG in Figure <ref type="figure" target="#fig_21">3(i)</ref>, consider the problem of identifying p(x 4 | do G (x 2 )). Mapping this problem to the notation of Theorem 48, we have Y = {4}, A = {2}, Y * = {4, 3, 1}. The districts of G Y * are {4}, {3} and {1}. In fact, these three sets are intrinsic in G, and thus a fixing sequence exists for each corresponding kernel:</p><formula xml:id="formula_85">φ 2,3,4 p(x 1 , x 2 , x 3 , x 4 ); G = φ 2,3 p(x 1 , x 2 , x 3 , x 4 ) p(x 4 | x 3 , x 2 , x 1 ) ; G {1,2,3} = φ 2 p(x 1 , x 2 , x 3 ) p(x 3 | x 2 , x 1 ) ; G {1,2} = p(x 1 , x 2 ) p(x 2 | x 1 ) = p(x 1 ), φ 1,2,4 p(x 1 , x 2 , x 3 , x 4 ); G = φ 1,2 p(x 1 , x 2 , x 3 , x 4 ) p(x 4 | x 3 , x 2 , x 1 ) ; G {1,2,3} = φ 1 p(x 3 , x 2 , x 1 ) p(x 2 | x 1 ) ; φ 2 (G {1,2,3} ) = p(x 3 | x 2 , x 1 )p(x 1 ) p(x 1 ) = p(x 3 | x 2 , x 1 ), φ 2,3,1 p(x 1 , x 2 , x 3 , x 4 ); G = φ 2,3 p(x 1 , x 2 , x 3 , x 4 ) p(x 1 ) = p(x 2 , x 3 , x 4 | x 1 ); φ 1 (G) = φ 2 p(x 2 , x 3 , x 4 | x 1 ) p(x 3 | x 2 , x 1 ) ≡ q 24 (x 2 , x 4 | x 1 , x 3 ); φ 31 (G) = q 24 (x 2 , x 4 | x 1 , x 3 ) q 24 (x 2 | x 1 , x 3 , x 4 ) = x 2 p(x 4 | x 3 , x 2 , x 1 )p(x 2 | x 1 ).</formula><p>imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023</p><p>The last step here follows because q 24 (x 2 , x 4 | x 1 , x 3 ) = p(x 2 | x 1 )p(x 4 | x 1 , x 2 , x 3 ), and q 24 (x 2 | x 1 , x 3 , x 4 ) = q 24 (x 2 , x 4 | x 1 , x 3 )/( x 2 q 24 (x 2 , x 4 | x 1 , x 3 )). Combining these kernels as in Theorem 48 yields the same identifying functional as the one obtained by the ID algorithm applied to G:</p><formula xml:id="formula_86">p x 4 | do G (x 2 ) = x 3 ,x 1 p(x 1 )p(x 3 | x 2 , x 1 ) x ′ 2 p x 4 | x 3 , x ′ 2 , x 1 p x ′ 2 | x 1 ,</formula><p>where we relabel x 2 as x ′ 2 in the last kernel to avoid confusion between free and summation quantifier-captured versions of the variable x 2 in the final expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.</head><p>Connections with Tian's constraint algorithm. An algorithm for enumerating constraints on kernels in marginals of DAG models was given in <ref type="bibr">Tian and Pearl (2002)</ref>. Tian's algorithm may be viewed as implementing fixing for both graphs and kernels, with three important differences from our formalization. First, unlike CADMGs, subgraphs obtained by fixing in <ref type="bibr">Tian and Pearl (2002)</ref> do not show fixed nodes explicitly. This makes it difficult to graphically represent constraints that may involve fixed nodes. Second, the kernel objects obtained in intermediate steps of the algorithm, called "q-factors" and written as Q[V ] where V is the set of nodes not yet fixed, do not explicitly show the dependence on nodes already fixed. This makes it hard to explicitly write down restrictions on q-factors, since these restrictions often state that dependence on some already fixed nodes does not exist. Third, there is no unified fixing operation on kernels, instead the algorithm in <ref type="bibr">Tian and Pearl (2002)</ref> alternates between steps corresponding to the application of the g-formula (division by a conditional density), and steps corresponding to marginalization.</p><p>For a given DAG G(V ∪ L) and a density p(x V ∪L ) Markov relative to G(V ∪ L), a subset of observable nodes V , and a topological order ≺ on G, Tian's constraint algorithm gives a list of constraints of the form "a kernel corresponding to a q-factor Q[C] obtained by some set of applications of the g-formula and marginalization on p(x V ) does not functionally depend on a set X D , for some D ⊆ V ."</p><p>The algorithm in <ref type="bibr">Tian and Pearl (2002)</ref> was developed in the context of hidden variable DAG models only. We have reformulated this algorithm, using the language of CADMGs and kernels, to yield an algorithmic specification of a set of generalized independence constraints implied by the larger nested model, which does not rely on the existence of an underlying hidden variable DAG; see Algorithm 1 in the Supplementary Material, Section E. As noted before, this is not a trivial reformulation, since the issue of invariance to choice of valid fixing sequences arises if we do not assume the existence of an underlying hidden variable DAG. We now state a key result relating this algorithm and the nested Markov model.</p><p>Theorem 51. For an ADMG G(V ), let P t (G, V, ≺) be the set of densities p(x V ) in which the list of constraints found by Algorithm 1 holds. Then P t (G, V, ≺) = P n (G(V )).</p><p>Thus, the set of constraints given by Algorithm 1 implicitly defines the nested Markov model.  <ref type="formula">2011</ref>) used constraints in causal DAG models with latent variables to construct a variable elimination (VE) algorithm for evaluating causal queries p(x Y | do(x A )) in a computationally efficient manner. This algorithm used an older definition called the "r-factorization property." The nested Markov model r-factorizes, which implies that the VE algorithm applies to these models as well.</p><formula xml:id="formula_87">Theorem 52. If p(x V ) ∈ P n (G(V )), then p(x V ) r-factorizes with respect to G and {φ V \C (p(x V ); G) | C ∈ I(G)}.</formula><p>5. Summary. We have introduced a novel statistical model defined by the equality constraints holding in marginals of DAG models, such as the Verma constraint. Though this model represents constraints found in marginal distributions, it does not itself model latent variables explicitly; indeed the existence of a latent variable may imply additional inequality constraints not captured by our model; see Example 47. We call this model the nested Markov model, and it is represented by means of an acyclic directed mixed graph (ADMG). Our model is "nested" because it is recursively defined. Specifically, just as a DAG model links a graph and a distribution, the nested model links sets of graphs derived from the original ADMG by a graphical fixing operation with sets of kernels obtained from the original distribution by an analogous fixing operation. This operation unifies certain marginalizations, conditioning operations and applications of the g-formula. Central to our model definition is the fact that any two valid sequences of fixing operations that fix the same set of nodes give the same result. We have characterized our model via Markov properties and a factorization. We have also shown a close connection between our model and a constraint enumeration algorithm for marginals of causal DAG models given in <ref type="bibr">Tian and Pearl (2002)</ref>, and used the fixing operation to characterize all identifiable causal effects in hidden variable DAG models using a one line formula (31).</p><p>if either there is a directed path a → • • • → d from a to d, or a = d; similarly d is said to be a descendant of a. If this is not the case we say that d is a non-descendant of a.</p><p>If a ↔ b we say that a is a sibling of b (and vice versa). The district of a is the set of vertices that are connected to a by a bidirected path (including a itself). The set of parents, children, ancestors, descendants, non-descendants and siblings of a in G are written pa G (a), ch G (a), an G (a), de G (a), nd G (a) and sib G (a) respectively; the district of a is denoted dis G (a). We apply these definitions disjunctively to sets, e.g. an</p><formula xml:id="formula_88">G (A) = a∈A an G (a). A set of vertices A in G is called ancestral if an G (a) ⊆ A whenever a ∈ A.</formula><p>An ordering ≺ of nodes in G is said to be topological if for any vertex pair a, b ∈ G, if a ≺ b, then a ∈ de G (b); note that this definition is the same as that for a DAG. We define</p><formula xml:id="formula_89">the set pre G,≺ (b) ≡ {a | a ≺ b}. Definition A.1. A distribution p(x V ) is said to be Markov relative to a DAG G if p(x V ) = v∈V p(x v | x pa G (v) ). (A.1)</formula><p>We denote the set of distributions that are Markov relative to a DAG G by P d (G).</p><p>A directed cycle is a path of the form v → • • • → w along with an edge w → v. An acyclic directed mixed graph (ADMG) is a mixed graph containing no directed cycles.</p><p>For any T ⊂ V , the induced subgraph G T of G contains the vertex set T , and the subset of edges in E that have both endpoints in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The m-separation criterion</head><p>We introduce the natural extension of d-separation to mixed graphs. A non-endpoint vertex z on a path is a collider on the path if the edges preceding and succeeding z on the path both have an arrowhead at z, i.e. → z ←, ↔ z ↔, ↔ z ←, → z ↔. A non-endpoint vertex z on a path which is not a collider is a non-collider on the path, i.e. C. Note that if G is a DAG then the above definition is identical to Pearl's d-separation criterion; see <ref type="bibr">(Pearl, 1988)</ref>.</p><formula xml:id="formula_90">← z →, ← z ←, → z →, ↔ z →, ← z ↔. A path</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Latent Projections</head><p>Given a DAG with latent variables we associate a mixed graph via the following operation;</p><p>see <ref type="bibr">(Pearl and Verma, 1991)</ref>. Generalizations of this construction are considered by <ref type="bibr">Wermuth (2011)</ref>, <ref type="bibr" target="#b60">Koster (2002)</ref> and <ref type="bibr">Sadeghi and Lauritzen (2014)</ref> in the context of marginalizing and conditioning. As an example, the mixed graph in Figure <ref type="figure" target="#fig_21">3</ref>(i) is the latent projection of the DAG shown in Figure <ref type="figure" target="#fig_0">1</ref>(i).</p><formula xml:id="formula_91">Proposition A.3. If G is an ADMG with vertex set V ∪ L then G(V ) is also an ADMG. Proof. It follows directly from the construction that if v → v ′ in G(V ) then v ∈ an G (v ′ ).</formula><p>The presence of a directed cycle in G(V ) would imply a directed cycle in G, which is a contradiction. (ii) G(V, W ) contains an edge a ↔ b if there exists a path between a and b such that the non-endpoints are all non-colliders in L, and such that the edge adjacent to a and the edge adjacent to b both have arrowheads at those vertices. For example,</p><formula xml:id="formula_92">a ↔ • • • → b. Proposition A.5. If G(V ∪ L, W ) is a CADMG then G(V, W ) is also a CADMG. In particular, for all w ∈ W , pa G(V,W ) (w) = sib G(V,W ) (w) = ∅.</formula><p>Proof. The absence of directed cycles in G(V, W ) follows from the proof of Proposition A.3. Since vertices in W only have children in G(V ∪L, W ), and no vertex in W is latent, it follows from Definition A.4 that there are no additional edges introduced between vertices in W . Further, any additional edge with a vertex w ∈ W as an endpoint takes the form w → v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Results On Graphs And Kernels</head><p>Proposition 1 Let G be a DAG with vertex set V ∪ L. For disjoint subsets A, B, C ⊆ V (where C may be empty),</p><formula xml:id="formula_93">A is d-separated from B given C in G if and only if A is m-separated from B given C in G(V ).</formula><p>Proof. For every path π in G, by Definition 3 there is a corresponding path π * in G(V ) consisting of a subsequence of the vertices on π, such that if a vertex v is a collider Proposition 5 In a set of kernels q</p><formula xml:id="formula_94">V (x V | x W ), X A ⊥ ⊥ X B | X C if and only if either X A ⊥ ⊥ X B∪(W \C) | X C or X B ⊥ ⊥ X A∪(W \C) | X C .</formula><p>Proof. This follows directly from Definition 4.</p><p>Proposition 6 The semi-graphoid axioms are sound for kernel independence.</p><p>Proof. Symmetry follows directly from Definition 4.</p><p>Let q V (x V | x W ) be a kernel for which (X A ⊥ ⊥ X B∪D | X C ) holds. Assume condition (a) for this independence holds, that is A∩W = ∅, and assume q</p><formula xml:id="formula_95">V (x A | x B , x C , x D , x W \(B∪C∪D) )</formula><p>is only a function of x A and x C . Then it immediately follows that condition (a) for</p><formula xml:id="formula_96">(X A ⊥ ⊥ X B | X C∪D ) also holds. To see that (X A ⊥ ⊥ X D | X C ) holds, consider the following derivation. q V (x A | x C , x D , x W \(C∪D) ) = xB∩V q V (x A , x B∩V , x C∩V , x D∩V | x W ) xB∩V q V (x B∩V , x C∩V , x D∩V | x W ) = xB∩V q V (x A | x B , x C , x D , x W \(B∪C∪D) ) • q V (x B∩V , x C∩V , x D∩V | x W ) xB∩V q V (x B∩V , x C∩V , x D∩V | x W ) = q V (x A | x B , x C , x D , x W \(B∪C∪D) ) • xB∩V q V (x B∩V , x C∩V , x D∩V | x W ) xB∩V q V (x B∩V , x C∩V , x D∩V | x W ) = q V (x A | x B , x C , x D , x W \(B∪C∪D) ).</formula><p>Here the first equality follows by ( <ref type="formula">5</ref>), the second by the chain rule of probability, which applies to kernels also by ( <ref type="formula">5</ref>), the third since we established above that (</p><formula xml:id="formula_97">X A ⊥ ⊥ X B | X C∪D ) holds in q V (x V | x W )</formula><p>, and the last by cancellation. Since (</p><formula xml:id="formula_98">X A ⊥ ⊥ X B∪D | X C ),</formula><p>the final term does not depend upon x B or x D , so the independence (</p><formula xml:id="formula_99">X A ⊥ ⊥ X D | X C ) follows. Now assume (X A ⊥ ⊥ X B∪D | X C ) holds due to condition (b), that is q V (x B , x D | x A , x C , x W \(A∪C) ) is only a function of x B , x C and x D . Then q V (x B | x A , x C , x D , x W \(A∪C∪D) ) is also only a function of x B , x C and x D , which in turn implies (X A ⊥ ⊥ X B | X C∪D ). To see that (X A ⊥ ⊥ X D | X C ) holds, simply sum over x B , to see that q V (x D | x A , x C , x W \(A∪C) )</formula><p>is only a function of x C and x D .</p><p>To show the converse, let q</p><formula xml:id="formula_100">V (x V | x W ) be a kernel in which (X A ⊥ ⊥ X B | X C∪D ) and (X A ⊥ ⊥ X D | X C ) hold. If this is due to condition (a) (where A ∩ W = ∅), then (X A ⊥ ⊥ X B∪D | X C</formula><p>) follows by the above derivation, and the two assumed independences.</p><p>If this is due to condition (b) (where</p><formula xml:id="formula_101">(B ∪ D) ∩ W = ∅), then (X A ⊥ ⊥ X B∪D | X C ) follows</formula><p>by the above derivation where x A is swapped with x B and x D respectively, and the two assumed independences.</p><formula xml:id="formula_102">Lemma 7 q * V \H (x R , x T | x H , x W ) is a kernel. Proof. Since q V (x R | x H , x T , x W ) and q V (x T | x W ) are derived from q V (x V | x W )</formula><p>, they are kernels. Thus, for every value</p><formula xml:id="formula_103">x H , x W , q * V \H (x R , x T | x H , x W ) ≥ 0. In addition, xR,xT q * V \H (x R , x T | x H , x W ) = xR,xT q V (x R | x H , x T , x W ) q V (x T | x W ) = xT q V (x T | x W ) • xR q V (x R | x H , x T , x W ) = xT q V (x T | x W ) • 1 = 1.</formula><p>Lemma 8 For the kernel constructed in (6):</p><formula xml:id="formula_104">q * V \H (x R , x T | x H , x W ) = q V (x R | x H , x T , x W )q V (x T | x W ),<label>(7)</label></formula><p>and hence</p><formula xml:id="formula_105">q * V \H (x R | x H , x T , x W ) = q V (x R | x H , x T , x W ); (8) q * V \H (x T | x H , x W ) = q V (x T | x W ) = q * V \H (x T | x W ). (<label>9</label></formula><formula xml:id="formula_106">)</formula><p>Proof. By the chain rule of probability:</p><formula xml:id="formula_107">q V (x R , x H , x T | x W ) = q V (x R | x H , x T , x W ) q V (x H | x T , x W ) q V (x T | x W ).</formula><p>Hence <ref type="bibr" target="#b55">(7)</ref> follows directly from (6). Since</p><formula xml:id="formula_108">q * V \H (x R , x T | x H , x W ) = q * V \H (x R | x H , x T , x W ) q * V \H (x T | x H , x W ), (A.2)</formula><p>the first equality in (9) follows by summing the right-hand sides of ( <ref type="formula" target="#formula_24">7</ref>) and (A.2) over</p><p>x R . The second then follows directly since q V (x T | x W ) is not a function of x H . Finally <ref type="bibr" target="#b56">(8)</ref> follows from ( <ref type="formula" target="#formula_105">9</ref>) by canceling q * V \H (x T | x H , x W ) and q V (x T | x W ) from the right-hand sides of ( <ref type="formula" target="#formula_24">7</ref>) and (A.2).</p><p>Proposition 9 For the kernel constructed in (6):</p><formula xml:id="formula_109">(i) (X H ⊥ ⊥ X T | X W ) in q * V \H (x V \H | x H , x W ). (ii) If X V ⊥ ⊥ X W \W1 | X W1 in q V , then X H∪(W \W1) ⊥ ⊥ X T | X W1 in q * V \H . (iii) If R = ∅, X H ⊥ ⊥ X V \H | X W in q * V \H .</formula><p>Proof. From ( <ref type="formula" target="#formula_105">9</ref>), the kernel q *</p><formula xml:id="formula_110">V \H (x T | x H , x W ) is a function only of x T and x W , which implies (i). Further, if X V ⊥ ⊥ X W \W1 | X W1 [q V ] then q V (x V | x W ) is not a function of</formula><p>x W \W1 , and therefore neither is q V (x T | x W ). Hence the previous argument gives (ii).</p><p>(iii) is implied by definition of marginalization.</p><p>Lemma 15 Let G be a CADMG with topological ordering ≺. If q V ∈ P c f (G), then for every ancestral set A and every D ∈ D(G A ), we have</p><formula xml:id="formula_111">f A D (x D | x pa(D)\D ) = d∈D q V (x d | x T (d,D) ≺ ), (A.3)</formula><p>where</p><formula xml:id="formula_112">T (d,D) ≺ ≡ mb G (d, an G (D) ∩ pre G,≺ (d)), so that q V (x d | x T (d,D) ≺ ) = q V (x d | x A∩pre G,≺ (d) , x W ). (A.4)</formula><p>Conversely, if (A.4) holds for all d ∈ A and ancestral sets A, and the f A D functions are defined by (A.3), then q V ∈ P c f (G).</p><p>Proof. (Cf. proof of Lemma 1 in <ref type="bibr">(Tian and Pearl, 2002a)</ref>):</p><p>(⇒) The proof is by induction on the size of the set A in (15). If |A| = 1, the claim is trivial. Suppose that the claim holds for sets A of size less than n. Specifically, we assume that all factors f A D (•|•) occurring in (15) for sets A such that |A| &lt; n, obey (A.3) and (A.4). Now suppose that A contains n variables and that A ⊆ {t} ∪ pre G,≺ (t) for some vertex t ∈ A. Let D t ≡ dis GA (t) be the district containing t in G A , so that by hypothesis:</p><formula xml:id="formula_113">q V (x A∩V | x W ) = f A D t (x D t | x pa(D t )\D t ) D∈D(GA)\{D t } f A D (x D | x pa(D)\D ). (A.5) Since A \ {t} ⊆ pre G,≺ (t), for all D ∈ D(G A ) \ {D t }, t / ∈ pa G (D) \ D.</formula><p>Thus summing both sides of (A.5) over x t leads to: </p><formula xml:id="formula_114">q V (x (A∩V )\{t} | x W ) = xt f A D t (x t , x D t \{t} | x pa(D t )\D t ) × D∈D(GA)\{D t } f A D (x D | x pa(D)\D ). (A.6) Now since ≺ is a topological ordering, A\{t} is an ancestral set in G; further every district in D(G A ) \ {D t } is also a district in G A\{t} hence,</formula><formula xml:id="formula_115">f A D t (x D t | x pa(D t )\D t ) = q V (x A∩V | x W ) D∈D(GA)\{D t } f A D (x D | x pa(D)\D )</formula><p>.</p><p>By the chain rule of probability,</p><formula xml:id="formula_116">q V (x A∩V | x W ) = a∈A∩V q V (x a | x A∩pre G,≺ (a) , x W ). Since for every D ∈ D(G) \ {D t }, f A D (•|•) obeys (A.</formula><p>3) and (A.4) so</p><formula xml:id="formula_117">f A D t (x D t | x pa(D t )\D t ) = d∈D t q V (x d | x A∩pre G,≺ (d) , x W ). (A.7)</formula><p>By the inductive hypothesis applied to A \ {t}, we have that (A.3) holds for all D * ∈ D(G A\{t} ) and (A.4) holds for all d ∈ D t \ {t} ⊆ A \ {t}. Consequently:</p><formula xml:id="formula_118">f A D t (x D t | x pa(D t )\D t ) = q V (x t | x A∩pre G,≺ (t) , x W ) d∈D t \{t} q V (x d | x T (d,A) ≺ ). (A.8)</formula><p>Rearranging (A.8) we obtain:</p><formula xml:id="formula_119">q V (x t | x A∩pre G,≺ (t) , x W ) = f A D t (x D t | x pa(D t )\D t ) d∈D t \{t} q V (x d | x T (d,A) ≺ )</formula><p>.</p><p>(A.9) By Proposition 14, for all d ∈ D t \ {t} we have T</p><formula xml:id="formula_120">(d,A) ≺ ⊆ (D t \ {t}) ∪ pa G (D t ), so the RHS is a function of x D t ∪pa(D t )</formula><p>. Hence:</p><formula xml:id="formula_121">X t ⊥ ⊥ X (A∪W )\(D t ∪pa(D t )) | X (D t \{t})∪pa(D t ) [q V ]</formula><p>from which (A.4) holds for t and D t ; (A.3) follows from (A.8).</p><p>(⇐) Follows from construction of the kernels f A D (•|•) via (A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Results On Fixing and Fixability</head><p>Proposition 18 For any v ∈ V , we have de</p><formula xml:id="formula_122">G (v) ∩ dis G (v) ∩ F(G) = ∅.</formula><p>Proof. Let ≺ be any topological ordering of G, and consider the maximal vertex in the</p><formula xml:id="formula_123">set de G (v) ∩ dis G (v) ⊇ {v}. Proposition 20 If q V is Markov with respect to a CADMG G(V, W ) and ch G (r) = ∅ (and hence r ∈ F(G)), then φ r (q V (x V | x W ); G) = xr q V (x V | x W ) = q V (x V \{r} | x W ).</formula><p>Proof. This follows by definition of F(G), φ r and ( <ref type="formula">19</ref>).</p><p>Note that, in this Appendix, we will sometimes write simply φ r (q V ) rather than φ r (q V ; G), provided that the graph with respect to which the fixing is being performed is clear.</p><formula xml:id="formula_124">Proposition 21 If q V is Markov with respect to G(V, W ) and r ∈ F(G), then φ r (q V (x V | x W ); G) = q V (x V | x W )/q V (x r | x nd G (r) ). (<label>21</label></formula><formula xml:id="formula_125">)</formula><p>Proof. This follows from Theorem 16 and ( <ref type="formula">19</ref>) with t = r.</p><formula xml:id="formula_126">Lemma 22 If r ∈ F(G), then F(G) \ {r} ⊆ F(φ r (G)).</formula><p>That is, any vertex s that was fixable before r was fixed is still fixable after r has been fixed (with the obvious exception of r itself). Thus when fixing vertices, although our choices may be limited at various stages, we are never faced with a choice between fixing r and r ′ , whereby choosing r precludes subsequently fixing r ′ .</p><p>Proof. This follows from the definition of F(G) and φ r (G). Since the set of edges in φ r (G) is a subset of the set of edges in G,</p><formula xml:id="formula_127">any vertex t ∈ V \ {r} that is in F(G) is also in F(φ r (G)).</formula><p>Proposition 23 If G is a subgraph of G * with the same random and fixed vertex sets, then F(G * ) ⊆ F(G).</p><p>Proof. If r has no descendant within the district containing it in G * then this also holds in G.</p><p>Proposition 24 Let G be a CADMG, with r ∈ F(G). If r ∈ D r ∈ D(G), then</p><formula xml:id="formula_128">D(φ r (G)) = (D(G) \ {D r }) ∪ D(G D r \{r} ),</formula><p>where the sets on the right are disjoint. Thus, if D ∈ D(φ r (G)) then D ⊆ D * for some</p><formula xml:id="formula_129">D * ∈ D(G); further if D = D * , then r ∈ D * .</formula><p>Proof. Fixing r does not affect bidirected edges that are not adjacent to r, so it is clear that districts other than D r will be preserved. The districts that replace D r are just those in the induced subgraph over D r \ {r}, which gives the result.</p><formula xml:id="formula_130">Proposition A.6. Let t ∈ F(G) and v ∈ V ∪W ; then de φt(G) (v) ⊆ de G (v) and (for v = t) pa φt(G) (v) = pa G (v).</formula><p>Proof. The directed edges removed by forming φ t (G) are precisely the edges into t (and none are added), from which both results follow.</p><p>This result implies, in particular, that the fixing operation preserves the set of parents of all vertices other than the one being fixed, meaning that the set of parents of any random vertex in any CADMG resulting from a valid fixing sequence is equal to the set of parents in the original ADMG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Fixing and factorization</head><p>Proposition 25 Take a CADMG G(V, W ) with kernel q V ∈ P c (G) with the associated district factorization:</p><formula xml:id="formula_131">q V (x V | x W ) = D∈D(G) q D (x D | x pa(D)\D ),<label>(22)</label></formula><p>where the kernels q D (x D | x pa(D)\D ) are defined via the right-hand side of</p><formula xml:id="formula_132">(A.3). If r ∈ F(G) and D r ∈ D(G) is the district containing r, then φ r (q V (x V | x W ); G) = q D r (x D r \{r} | x pa G (D r )\D r ) D∈D(G)\{D r } q D (x D | x pa G (D)\D ). (A.10)</formula><p>Further, by (A.10), the resulting kernel is given by:</p><formula xml:id="formula_133">φ r (q V (x V | x W ); G) = q D r (x D r \{r} | x pa G (D r )\D r ) D∈D(G)\{D r } q D (x D | x pa G (D)\D ). (A.13)</formula><p>Here, and in the remainder of the proof, we use q D , with D ∈ D(G), to refer to terms in the decomposition ( <ref type="formula" target="#formula_131">22</ref>) associated with G.</p><formula xml:id="formula_134">Set G = φ r (G). If r, s ∈ F(G) then either (i) dis G (r) = dis G (s), but r / ∈ de G (s) and s / ∈ de G (r), or (ii) dis G (r) = dis G (s)</formula><p>. We now consider each case in turn:</p><p>(i) In this case, s ∈ D r since r and s are in the same district in G. By definition, the divisor when fixing s, having already fixed r, is given by:</p><formula xml:id="formula_135">(φ r (q V ))(x s | x mbG(s) ). Now, {s} ∪ mb G(s) is a subset of {s} ∪ mb G (s) = D r ∪ pa G (D r ), because fixing removes edges.</formula><p>It follows from applying m-separation to G |W by Definition 27 (or alternatively the factorization given in Definition 28) that the independence</p><formula xml:id="formula_136">X s ⊥ ⊥ X (mb G (s)\{r})\mbG(s) |</formula><p>X mb G(s) holds in q V . Since all the sets in this independence are subsets of mb G (r), it follows from Proposition 10 that this independence also holds in φ r (q V ). Hence</p><formula xml:id="formula_137">φ r (q V )(x s | x mbG(s) ) = φ r (q V )(x s | x mb G (s)\{r} ) = q V (x s | x mb G (s)\{r} ),</formula><p>where we have used (9) in the second equality. The product of the two divisors is then</p><formula xml:id="formula_138">q V (x r | x mb G (r) ) • φ r (q V )(x s | x mbG(s) ) = q V (x r | x mb G (r) ) • q V (x s | x mb G (s)\{r} ) = q V (x r , x s | x mb G (r)\{s} ).</formula><p>Note that since, by hypothesis, r and s are in the same district in G, this last expression is symmetric in r and s.</p><p>(ii) Let D s be the district in D(G) that contains s. Since, by assumption, D s = D r , by</p><p>Proposition 24 it follows that s ∈ D s ∈ D( G). It then follows from (A.13) that</p><formula xml:id="formula_139">(φ r (q V ))(x s | x mbG(s) ) = q D s (x s | x mb G (s) ). (A.14)</formula><p>Thus the product of divisors is given by</p><formula xml:id="formula_140">q D r (x r | x mb G (r) ) • q D s (x s | x mb G (s) ).</formula><p>Hence in both cases, the product of the divisors is symmetric in r and s, and a symmetric argument shows that the same divisor is obtained when fixing s first, and r second.</p><p>Theorem 31 Let p(x V ) be a distribution that is nested Markov with respect to an ADMG G (in either the sense of Definitions 27 or 28). Let u, w be different valid fixing sequences for the same set W ⊂ V . Then φ u (G) = φ w (G) and</p><formula xml:id="formula_141">φ u (p(x V ); G) = φ w (p(x V ); G). (A.15)</formula><p>Proof. We perform a direct proof. Let u i , w i denote the ith vertices in sequences u, w respectively. Further, let k the smallest i such that u i = w i , and let v ≡ u k , so that u and w agree in the first k -1 fixing operations. By definition of k,</p><formula xml:id="formula_142">φ u1,...,u k-1 (G) = φ w1,...,w k-1 (G).</formula><p>Since u, w both contain the same vertices, there exists l &gt; k such that w l = v. Since, by hypothesis, u, w are both valid fixing sequences, it follows that v ∈ F(φ w1,...,w k-1 (G)).</p><p>It further follows by Lemma 22 that v ∈ F(φ w1,...,wi-1 (G)), for k -1 ≤ i ≤ l.</p><p>Then by Lemma 30, we have that:</p><formula xml:id="formula_143">φ w1,...,w l-1 ,v=w l (G) = φ w1,...,w l-2 ,v,w l-1 (G) φ w1,...,w l-1 ,v=w l (p(x V ); G) = φ w1,...,w l-2 ,v,w l-1 (p(x V ); G).</formula><p>By further applications of Lemma 30, we may show that both the graphs and kernels resulting from the fixing sequences w 1 , . . . , w l-1 , v = w l and w 1 , . . . , w k-1 , v, w k , . . . , w l-1 .</p><p>are the same. It further follows that the whole sequence w leads to the same graph and kernel as w 1 , . . . , w k-1 , v, w k , . . . , w l-1 , w l+1 , . . . , w |W | . This latter sequence now agrees with u in the first k fixing operations. By repeating the argument we may thus show that u and w lead to the same graph and kernel.</p><p>As discussed in the main body of the paper, the above result implies that, under the model, if there are multiple fixing sequences for V \ R valid in G(V ), we can define φ V \R (p(V ); G(V )) without loss of generality to be the result of fixing elements in V \ R in any valid sequence. Furthermore if (S 1 , S 2 ) is a partition of V \ R, and there exists a fixing sequence for V \ R valid in G(V ) that fixes all elements in S 1 before any element in S 2 , we define φ S1,S2 (G(V )) to be equal to φ S2 (φ S1 (G(V ))), and</p><formula xml:id="formula_144">φ S1,S2 (p(V ); G(V )) to be equal to φ S2 (φ S1 (p(V ); G(V )), φ S1 (G(V ))).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Equivalence Of CADMG Markov Properties and Factorization</head><p>Theorem 16</p><formula xml:id="formula_145">P c f (G) = P c l (G, ≺) = P c m (G) = P c a (G).</formula><p>To prove this result we will need a number of intermediate results. The argument for the last two equalities follows that given by <ref type="bibr">Richardson (2003)</ref>.</p><p>Lemma B.1. For a CADMG G(V, W ), suppose µ is a path which m-connects t and y given Z in G |W . Then the sequence of non-colliders on µ form a path connecting t and y in (G an({t,y}∪Z) ) a .</p><p>This proof follows that of <ref type="bibr">Lemma 3 in Richardson (2003)</ref>.</p><p>Proof. Every vertex on an m-connecting path is either an ancestor of a collider, and hence of some element of Z, or an ancestor of an endpoint. Thus all the vertices on µ are in G an({t,y}∪Z) . Suppose that v i and v i+1 (1 ≤ i ≤ k -1) are the successive noncolliders on µ. The subpath µ(v i , v i+1 ) consists entirely of colliders, hence v i and v i+1 are adjacent in (G an({t,y}∪Z) ) a . Similarly v 1 and v k are adjacent to t and y, respectively,</p><formula xml:id="formula_146">in (G an({t,y}∪Z) ) a . Theorem B.2. If G is a CADMG then P c m (G) = P c a (G).</formula><p>This proof follows that of Theorem 1 in <ref type="bibr">Richardson (2003)</ref>. there are vertices t ∈ T , y ∈ Y such that there is a minimal path π between t and y in (G an(T ∪Y ∪Z) ) a on which no vertex is in Z. Our strategy is to replace each augmented edge on π with a corresponding collider path in G |W and replace the other edges on π with the corresponding edges in G (choosing arbitrarily if there is more than one). It follows from Lemma 2 in <ref type="bibr">Richardson (2003)</ref> that the resulting sequence of edges forms a path from t to y in G |W , which we denote ν. Further, any non-collider on ν is a vertex on π and hence not in Z. Finally, since all vertices in ν are in G an(T ∪Y ∪Z) it follows that every collider is in an G (T ∪ Y ∪ Z). Thus by Lemma 1 in <ref type="bibr">Richardson (2003)</ref>  </p><formula xml:id="formula_147">(i) Either T ∩ W = ∅ or Y ∩ W = ∅ (or both); (ii) Either T is m-separated from Y ∪ (W \ Z) given Z in G |W , or Y is m-separated from T ∪ (W \ Z) given Z in G |W (or both).</formula><p>Proof. (i) Suppose that there are vertices w, w * ∈ W such that w ∈ T , w * ∈ Y . Since w ↔ w * in G |W , it follows that T and Y are m-connected given Z in G |W , which is a contradiction.</p><p>(ii) On similar lines, suppose that there exist vertices w, w * ∈ W \ Z such that in G |W some vertex t ∈ T is m-connected to w given Z by a path ν tw , but, in addition, some y ∈ Y is m-connected to w * given Z by a path ν w * y . In this case a path m-connecting t and y given Z may be constructed by concatenating ν tw , the edge w ↔ w * and ν w * y .</p><formula xml:id="formula_148">Lemma B.4. If G(V, W ) is a CADMG, t ∈ V is a vertex in an ancestral set A ⊆ V ∪ W ,</formula><p>and ch G (t) ∩ A = ∅, then the induced subgraph of the augmented graph (G A ) a on the set {t} ∪ mb G (t, A) is always a clique. In addition, if y t in (G A ) a then y ∈ mb G (t, A).</p><p>Proof. (Cf. proof of Theorem 4 in <ref type="bibr">Richardson (2003)</ref>) By construction, y t in (G A ) a if and only if t is collider-connected to y in (G A ) |W ∩A . Since t ∈ V and ch G (t) ∩ A = ∅, the vertex adjacent to t on any collider path is in sib GA (t) ∪ pa GA (t). Consequently, a collider path to t in (G A ) |W ∩A takes one of three forms:</p><formula xml:id="formula_149">(a) y → t ⇔ y ∈ pa GA (t); (b) y ↔ u ↔ • • • ↔ t ⇔ y ∈ dis GA (t) \ {t}; (c) y → u ↔ • • • ↔ t ⇔ y ∈ pa GA (dis GA (t) \ {t}).</formula><p>It then follows from the definition of a Markov blanket and</p><formula xml:id="formula_150">G |W that if t ∈ V then y is collider-connected to t in (G A ) |W ∩A if and only if y ∈ mb G (t, A).</formula><p>Suppose that u, v ∈ mb G (t, A), with u = v. Then there are collider paths ν ut , ν vt in (G A ) |W ∩A that do not contain any bidirected edge between vertices in W . Traversing the path ν ut from u to t, let s be the first vertex which is also on ν vt ; such a vertex is guaranteed to exist since t is common to both paths. Concatenating the subpaths ν ut (u, s) and ν vt (v, s) forms a collider path connecting u and s in (G A ) |W ∩A . (If s = t, this follows from ch G (t) ∩ A = ∅.) Hence u v in (G A ) a , proving the first claim.</p><p>Theorem B.5. If G(V, W ) is a CADMG and ≺ is a topological ordering then</p><formula xml:id="formula_151">P c l (G, ≺) = P c f (G).</formula><p>Proof. First we show that P c l (G, ≺) ⊆ P c f (G). Fix an ancestral set A ⊆ V ∪ W . We have:</p><formula xml:id="formula_152">q V (x A∩V | x W ) = d∈D∈D(GA) q V (x d | x A∩pre G,≺ (d) , x W ) = d∈D∈D(GA) q V (x d | x mb(d,A∩pre G,≺ (d)) ).</formula><p>The first line is by the chain rule of probabilities and the fact that</p><formula xml:id="formula_153">D(G A ) = D(G A∪W )</formula><p>is a partition of random nodes in G A , the second by the ordered local Markov property. This is sufficient for the conclusion.</p><p>Now we show that P c f (G) ⊆ P c l (G, ≺). Let V = {v 1 , . . . , v n } be a numbering of the vertices in V such that v i ≺ v j if and only if i &lt; j, so pre G,≺ (v k ) ∩ V = {v 1 , . . . , v k-1 };</p><p>the proof is by induction on this sequence of vertices.</p><formula xml:id="formula_154">For k = 1, if A is an ancestral set in which v 1 is the maximal vertex in A ∩ V then A ∪ W = {v 1 } ∪ W . The assumed Markov factorization for the graph G({v 1 }, W ) then implies q V (x v1 | x W ) = q V (x v1 | x pa(v1) ) = q V (x v1 | x mb(v1,A)</formula><p>), as required by the ordered local Markov property. Now assume the inductive hypothesis holds for all j &lt; k, so that for all ancestral sets A * in which v j is maximal, q V obeys the ordered local Markov property for G(V ∩A * , W ) at v j . Now fix an ancestral set A with max ≺ (A ∩ V ) = v k . By the chain rule of probabilities and the fact that D(G A ) is a partition of the random nodes in G A , we have:</p><formula xml:id="formula_155">q V (x A∩V | x W ) = d∈D∈D(GA) q V (x d | x A∩pre G,≺ (d) , x W ).</formula><p>However, by the assumed Markov factorization we also have:</p><formula xml:id="formula_156">q V (x A∩V | x W ) = d∈D∈D(GA) q V (x d | x mb(d, A∩pre G,≺ (d)) ).</formula><p>Thus:</p><formula xml:id="formula_157">D∈D(GA) d∈D q V (x d | x A∩pre G,≺ (d) , x W ) = D∈D(GA) d∈D q V (x d | x mb(d, A∩pre G,≺ (d)) ).</formula><p>As the inductive hypothesis holds for all vertices d ≺ v k , we can cancel all terms from the above equality, except for those involving v k : q</p><formula xml:id="formula_158">V (x v k | x A∩pre G,≺ (v k ) , x W ) and q V (x v k | x mb(v k , A∩pre G,≺ (v k )) ). Since v k is the maximal vertex in A ∩ V , it follows that mb v k , A ∩ pre G,≺ (v k ) = mb(v k , A), which establishes the ordered local property for A in G A at v k . Theorem B.6. If G(V, W ) is a CADMG and ≺ is a topological ordering then P c a (G) = P c l (G, ≺).</formula><p>Proof. We first show that P c l (G, ≺) ⊆ P c a (G). The proof is similar to Proposition 5 in <ref type="bibr" target="#b61">Lauritzen et al. (1990)</ref>, and Theorem 2 in <ref type="bibr">Richardson (2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number the vertices in</head><formula xml:id="formula_159">V = {v 1 , . . . , v n } such that v i ≺ v j if and only if i &lt; j, so pre G,≺ (v k+1 ) = {v 1 , . . . , v k }. Let q V ∈ P c l (G, ≺).</formula><p>The proof is by induction on the sequence of ordered vertices in V . The inductive hypothesis is that if </p><formula xml:id="formula_160">T ∪Y ∪Z ⊆ {v 1 , . . . , v k } ∪ W and T is separated from Y by Z in (G an(T ∪Y ∪Z) ) a then X T ⊥ ⊥X Y | X Z in q V . By Theorem B.</formula><formula xml:id="formula_161">= W \ Z. Let A ≡ T ∪ Y ∪ Z = {v 1 } ∪ W . Note that A = an G (A) and mb G (v 1 , A) = pa G (v 1 ) ⊆ Z ∩ W .</formula><p>By the ordered local Markov property for A we have:</p><formula xml:id="formula_162">X v1 ⊥ ⊥ X W \(pa(v1)∪{v1}) | X pa(v1) [q V ]. (B.1) Since Y ⊆ W \ Z and pa G (v 1 ) ⊆ Z, (B.1) implies X v1 ⊥ ⊥ X Y | X Z [q V ].</formula><p>(Inductive case) Suppose that the induction hypothesis holds for j &lt; k. Let H ≡</p><formula xml:id="formula_163">(G an(T ∪Y ∪Z) ) a . If T is separated from Y by Z in H and v ∈ an G (T ∪ Y ∪ Z) \ (T ∪ Y ∪ Z)</formula><p>then in H either v is separated from Y by Z, or v is separated from T by Z (or both).</p><p>Hence we may always extend T and Y , so that an G (T ∪ Y ∪ Z) = T ∪ Y ∪ Z, and thus need only consider this case.</p><formula xml:id="formula_164">If v k / ∈ (T ∪ Y ∪ Z) then T ∪ Y ∪ Z ⊆ {v j } ∪ pre G,≺ (v j ) ∪ W for some v j ≺ v k hence the</formula><p>required independence follows directly from the induction hypothesis. Thus we suppose</p><formula xml:id="formula_165">that v k ∈ (T ∪ Y ∪ Z) ⊆ {v k } ∪ pre G,≺ (v k ) ∪ W . As before, let A ≡ T ∪ Y ∪ Z. Since A is ancestral and v k has no children in A, the local Markov property for A = A ∪ W implies that X v k ⊥ ⊥ X (A∪W )\({v k }∪mb(v k ,A)) | X mb(v k ,A) [q V ]. (B.2)</formula><p>There are now three cases to consider:</p><formula xml:id="formula_166">(i) v k ∈ T ; (ii) v k ∈ Y ; (iii) v k ∈ Z. (i) Note that (G an(Y ∪Z∪(T \{v k })) ) a contains a subset of the edges in H. Thus (if non-empty) T \ {v k } is separated from Y by Z in (G an(Y ∪Z∪(T \{v k })) ) a , hence X T \{v k } ⊥ ⊥X Y | X Z in q V by the induction hypothesis. It is thus sufficient to prove that X v k ⊥ ⊥X Y | X Z∪(T \{v k })</formula><p>in q V ; this also covers the case where T = {v k }.</p><p>Since, by <ref type="bibr">Lemma B.4</ref>, the vertices in</p><formula xml:id="formula_167">{v k } ∪ mb G (v k , A) form a clique in H it follows that {v k } ∪ mb G (v k , A) ⊆ T ∪ Z, so Y ⊆ A \ ({v k } ∪ mb G (v k , A)).</formula><p>Thus by the local</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markov property for</head><formula xml:id="formula_168">A = A ∪ W , X v k ⊥ ⊥ X A\({v k }∪mb(v k ,A)) | X mb(v k ,A) [q V ] ⇒ X v k ⊥ ⊥X Y | X Z∪(T \{v k }) [q V ].</formula><p>(ii) Similar to case (i).</p><p>(iii) By hypothesis,</p><formula xml:id="formula_169">A = an G (A) ⊆ {v k }∪pre G,≺ (v k )∪W , so v k / ∈ an G (T ∪Y ∪(Z \{v k })).</formula><p>Thus the vertex v k is not in (G an(T ∪Y ∪(Z\{v k })) ) a , and this graph contains a subset of the edges in H. Hence T is separated from</p><formula xml:id="formula_170">Y given Z \{v k } in (G an(T ∪Y ∪(Z\{v k })) ) a .</formula><p>The induction then implies</p><formula xml:id="formula_171">X T ⊥ ⊥X Y | X Z\{v k } . It is then sufficient to prove that either X v k ⊥ ⊥X Y | X T ∪(Z\{v k }) or X v k ⊥ ⊥X T | X Y ∪(Z\{v k }) in q V . Since by Lemma B.4, {v k } ∪ mb G (v k , A) forms a clique in (G A ) a it follows that either {v k } ∪ mb G (v k , A) ⊆ T ∪ Z or {v k } ∪ mb G (v k , A) ⊆ Y ∪ Z.</formula><p>Suppose the former. In this case, by the ordered local Markov property,</p><formula xml:id="formula_172">X v k ⊥ ⊥ X A\({v k }∪mb(v k ,A)) | X mb(v k ,A) [q V ] ⇒ X v k ⊥ ⊥ X Y | X T ∪(Z\{v k }) [q V ].</formula><p>If the latter then</p><formula xml:id="formula_173">X v k ⊥ ⊥X T | X Y ∪(Z\{v k }) [q V ].</formula><p>Now we show that P c a (G) ⊆ P c l (G, ≺). Let A be an ancestral set with t = max ≺ (A) ∈ V . Note that A ∪ W is ancestral. By <ref type="bibr">Lemma B.4,</ref><ref type="bibr">every vertex</ref> </p><formula xml:id="formula_174">adjacent to t in (G A∪W ) a is in mb G (t, A ∪ W ). Thus t is separated from (A ∪ W ) \ (mb G (t, A) ∪ {t}) by mb G (t, A) in (G A∪W ) a . Hence if q V ∈ P c a (G) then q V ∈ P c l (G, ≺).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Results On Nested Markov Models</head><p>We begin with a proof of the equivalence of the global nested Markov property and district factorization.</p><p>Proposition 29 With respect to an ADMG G(V ), a distribution p(x V ) is globally nested Markov if and only if it nested Markov factorizes.</p><p>Proof. This follows immediately by equivalence of the CADMG global property and CADMG factorization.</p><p>Theorem 35 If p(x V ) nested Markov factorizes with respect to G then for every reachable</p><formula xml:id="formula_175">R in G, φ V \R (p(x V ); G) = D∈D(φ V \R (G)) φ V \D (p(x V ); G).</formula><p>Proof. By Theorem 16 we know that, for each reachable set R (and valid fixing sequence w for R), φ w (p) will be a member of </p><formula xml:id="formula_176">P c m (G[R]) if</formula><formula xml:id="formula_177">R reachable in G, q R (x R | x V \R ) ≡ φ V \R (p(x V ); G) ∈ P c f (G[R]), so φ V \R (p(x V ); G) = D∈D(φ V \R (G)) f w D (x D | x pa(D)\D ).</formula><p>By Lemma 15, f w D = q D , where</p><formula xml:id="formula_178">q D = φ R\D (q ; G[R]) = φ R\D (φ V \R (p; G); φ V \R (G)</formula><p>). However, by Theorem 31 we know that this is equivalent to φ V \D (p; G). This gives the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ordered Local Nested Markov Property</head><p>The following example shows that we must take care in our definition of the ordered local nested Markov property if we do not yet know that the distribution is Markov with respect to the graph.</p><p>Example C.1. It is natural to construct a nested local property in terms of kernels; although under the model all valid fixing sequences for a given set lead to the same kernel, in terms of defining the model the choice of fixing sequences becomes important. To see this in a simple example consider the three node graph shown in Figure <ref type="figure" target="#fig_0">10</ref>(i) and the independence X 2 ⊥ ⊥ X 3 . That this constraint holds in the kernel that results from fixing 1 and then 2 is equivalent to saying that this independence holds in p. However, if we first fix 2 this corresponds to marginalization, and the independence will hold trivially in any kernels derived from it. The 'moral' of this example is that we must be careful which fixing sequence we choose to describe the kernels used to define the local nested Markov property.</p><formula xml:id="formula_179">1 3 (i) 2 1 3 (ii) 2 1 3 (iii)</formula><p>Figure <ref type="figure" target="#fig_0">10</ref>: (i) An ADMG, and (ii, iii) the ADMG from (i) after fixing 1 and 2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Definition of Local Property</head><p>Definition C.2. Suppose that we have an ADMG with a topological ordering ≺. We C.3 Proof that ordered local nested property implies the global</p><formula xml:id="formula_180">property Definition C.6. Let v be a fixable vertex in a CADMG G(V, W ) such that q V (x v | x mb(v) ) = q V (x v | x V \{v} , x W ).</formula><p>Then we say that v is fixed by marginalization in q V (and G).</p><p>The terminology is used because, in this case,</p><formula xml:id="formula_181">φ v (q V ; G) = q V (x V | x W ) q V (x v | x V \{v} , x W ) = xv q V (x V | x W ),</formula><p>and it follows that φ v (q V ; G) does not depend upon x v . This may occur because nd G (v) = (V \ {v}) ∪ W and q V is Markov with respect to G, or equally because mb G (v) = (V \ {v}) ∪ W even if q V is not Markov with respect to G. This idea will be particularly useful in proofs where the fact of q V being Markov with respect to a graph has not yet been established.</p><p>Recall that pre ℓ denotes {ℓ} ∪ {k : k ≺ ℓ} under a topological ordering ≺. </p><formula xml:id="formula_182">v = k, which becomes X k ⊥ ⊥ X pre(k)\mb(k,pre k ) | X mb(k,pre k ) [φ pre k \C (p pre k )]. (C.3)</formula><p>We now proceed to prove that the hypothesis also holds for pre k . Note that an application of Lemma C.5 shows that it is equivalent to observe the independence (C.3) in p pre k , the marginal distribution over pre k .</p><p>Using all the independences (C.1) we obtain</p><formula xml:id="formula_183">p(x V ) = p(x k | x mb(k,pre k ) ) • i:i≺k q {i} (x i | x mb(i,pre i ) ).</formula><p>Note that this gives us the district factorization of p(x V ) w.r.t. G since, by definition, the Markov blanket within the set of predecessors only ever includes vertices in the same district (and its parents). So in particular, by Lemma 15, we also have</p><formula xml:id="formula_184">V ) = D∈D(G[V ]) q D (x D | x pa(D)\D ).</formula><p>Now, any reachable set R containing k can be obtained by iteratively fixing one vertex at a time. We now proceed to show that q R district factorizes by an inner induction on reachable sets whose maximal vertex is k. As a base case, we already have the result for R = V , so now assume we have the required factorization for some set R ⊆ V . We will prove that it also holds for sets R ′ = R \ {ℓ} obtained by fixing a vertex ℓ ∈ F(R) \ {k}.</p><p>Let D ≡ dis G[R] (k); there are two possibilities: either ℓ is in D, or it is not. If it is not, then by application of Proposition 25 the fixing will only affect the factors that correspond to vertices in the same district. We therefore know, from the original (outer)</p><p>inductive hypothesis for pre G,≺ (k), that if we only fix vertices not in D we will obtain that the appropriate district factorization for the reachable graph G[R ′ ] also holds.</p><p>If ℓ ∈ D, then there will be a transition D → C in I(G) corresponding to fixing ℓ from D, so by (C.2) that yields the independence</p><formula xml:id="formula_185">X k ⊥ ⊥ X (fam(D)\{ℓ})\fam(C) | X fam(C)\{k} [φ {ℓ},D\(C∪{ℓ}) (q D ; G)].</formula><p>Note that, from q D the vertex ℓ is fixed by marginalization by the district factorization; the new independence shows that if k is fixed after ℓ, then k is also fixed by marginalization.</p><p>For the reverse order, note that by the inner inductive hypothesis we already have the district factorization for R, so k is clearly also fixed from D by marginalization. Then we can apply the outer induction hypothesis on pre G,≺ (k) to show that ℓ is also fixed by marginalization from its new district in D \ {k}. It follows that φ k and φ ℓ commute when applied to q D , and hence q C is well defined for every district C of D \ {ℓ}.</p><p>Hence (letting D * ≡ D \ {ℓ}) we obtain</p><formula xml:id="formula_186">q D * (x D * | x pa(D * )\D * ) = q D (x k | x (C\{k})∪pa(C) ) C∈D(G[D * \{k}]) q C (x C | x pa( C)\ C ) = C∈D(G[D * ]) q C (x C | x pa( C)\ C ).</formula><p>Consequently we have that for the reachable set</p><formula xml:id="formula_187">R ′ = R \ {ℓ}, the distribution q R ′ = D ′ ∈D(G[R ′ ]) q D ′ district factorizes according to G[R ′ ].</formula><p>Hence, by the combination of the two inductions, the same is true for every reachable set R in G. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Illustrative Examples</head><p>Here we give some examples of the ordered local nested Markov property associated with a given vertex, so as to illustrate how it works.</p><p>Example C.8. Consider again the bidirected graph in Figure <ref type="figure" target="#fig_0">10</ref>(i), under the topological ordering 1, 2, 3. In this case there are three intrinsic sets containing the maximal vertex 3, and these are {3}, {2, 3} and {1, 2, 3}. Then the power DAG for the initial segment {1, 2, 3} is just the complete DAG on these three vertices, and the only transition that gives a constraint is {1, 2, 3} → {3} when fixing 1, which tells us that X</p><formula xml:id="formula_188">3 ⊥ ⊥ X 2 [φ 1 (p)],</formula><p>or equivalently that X 3 and X 2 are marginally independent. Example C.9. Consider the ADMG in Figure <ref type="figure" target="#fig_0">11</ref>(i), and consider the topological ordering 1, 2, 3, 4, 5, 6. Then the relevant power DAG restricted to descendants of V = pre 6 is shown in Figure <ref type="figure" target="#fig_16">12</ref>.</p><p>There are two highlighted arrows, one from {1, 4, 5, 6} to {6}, and the other from {1, 3, 5, 6} to {6}. These are both maximal, in the sense that there is no superset of either of these sets from which one can transition directly to the set {6}.</p><p>Note that it is necessary for us to use two different fixing sequences to get to the intrinsic set 6, because one of them gives the ordinary independence X 6 ⊥ ⊥ X 4 , X 5 , and the other gives a nested independence X 6 ⊥ ⊥ X 3 , X 5 | X 2 . There is no way to obtain these both from the same fixing sequence, and there is no joint independence involving all of these variables.</p><p>Example C.10. Now consider the graph in Figure <ref type="figure" target="#fig_21">13</ref>(a) under a numerical topological ordering, and the part of its intrinsic power DAG in Figure <ref type="figure" target="#fig_21">13</ref> <ref type="bibr">(b)</ref>. Again, note that there are two distinct edges into {6}, from {2, 4, 6} and {3, 4, 6}, that give two distinct independences. However, there is no intrinsic set containing {2, 3, 4, 6} from which we could obtain both these independences.</p><p>A moral of the examples above is that it is also not possible to have a local Markov property where there is at most one independence statement corresponding to each intrinsic set. Our local property associates independences with initial segment districts D, and with pairs (D, w) where D is an intrinsic set in which w is fixable. Now suppose we are fixing w ∈ {v} ∪ sib G (v) in some reachable graph φ V \R (G). Every random vertex is joined to w, and hence is a parent or sibling of w. Note that, (if w = v) any parents of v are also in mb G (w; R). The only variables that might be outside mb G (w; R) are those fixed vertices that were not parents of w (or v) in G; i.e. they were also siblings of v. The first such vertex in the sequence, say a, is fixed by marginalization because its Markov blanket includes all variables.</p><p>For subsequent vertices, the Markov blanket includes all vertices other than those previously fixed by marginalization; since, by definition, the kernel does not depend upon these marginalized vertices, then these subsequent vertices are also fixed by marginalization. It follows by a simple induction that w is also fixed by marginalization.</p><p>Starting with a complete ADMG, fixing operations remove edges and may lead to a CADMG that is no longer complete. Consequently, the following result is by no means trivial.</p><p>Lemma C.13. Let G be a complete ADMG and p an arbitrary distribution over X V (not necessarily Markov with respect to G). If v is a vertex with no children in G, then for any reachable set R ⊆ V \ {v} and any w fixable in φ V \R (G), we have φ v,w (q R ; φ V \R (G)) = φ w,v (q R ; φ V \R (G)) where q R = φ V \R (p; G).</p><p>Proof. By Lemma C.12, v is fixed by marginalization in G * ≡ φ V \R (G). If w is a sibling of v then (also by Lemma C.12) it is also fixed by marginalization, and the operations φ v and φ w amount to sums that clearly commute.</p><p>Otherwise w is a parent of v and-since w is fixable-is in a different district to v in</p><formula xml:id="formula_189">G * . Therefore mb G (w; R) = mb G (w; R \ {v}) and also v / ∈ mb G (w; R). It then follows that φ v (φ w (q R ; G * ); φ w (G * )) = xv q R (x R | x V \R ) q R (x w | x mb(w,R) ) = xv q R (x R | x V \R ) q R\{v} (x w | x mb(w,R\{v}) ) = q R\{v} (x R\{v} | x V \R ) q R\{v} (x w | x mb(w,R\{v}) ) = φ w (φ v (q R ; G * ); φ v (G * )) as required. G(L ∪ V, W ) G((L ∪ V ) \ {v}, W ∪ {v}) G(V, W ) G(V \ {v}, W ∪ {v}) φ v σ L σ L φ v Proof. Both σ L (φ v (G(L ∪ V, W ))) and φ v (σ L (G(L ∪ V, W ))) have the same set of random vertices V \ {v} and fixed vertices W ∪ {v}. Consider the set of edges E in σ L (G(L ∪ V, W )) = G(V, W ). The set of edges E ′ in φ v (σ L (G(L ∪ V, W ))</formula><p>) is a subset of E containing all edges not having an arrowhead at v. Now let π be the set of paths in G(L ∪ V, W ), where both endpoints are in V ∪ W and all non-endpoints are non-colliders in L. These paths d-connect marginally (i.e. given ∅).</p><p>Similarly, let π ′ be the set of paths in φ</p><formula xml:id="formula_190">v (G(L ∪ V, W )) = G((L ∪ V ) \ {v}, W ∪ {v}),</formula><p>where both endpoints are in V ∪ W and all non-endpoints are non-colliders in L. π ′ is the subset of π formed by removing paths containing an edge with an arrowhead at v (note that since v / ∈ L, v can only occur as an endpoint).</p><p>By definition of latent projections, there is a bijection that associates each edge e in E, with a subset of paths in π with the same endpoints as e, and the same starting and ending orientations as e. These subsets partition π. Applying φ v to G(L ∪ V, W ) means that only those paths in π ′ are left in the resulting graph. Paths in π ′ are only in subsets of π associated with edges in E ′ (by the bijection). Applying σ L to the graph then results in the edge set E ′ . This establishes our conclusion.</p><formula xml:id="formula_191">Lemma D.2. Assume q L∪V (x L∪V | x W ) is in P c (G(L ∪ V, W )) for a CDAG G(L ∪ V, W ). Then q L∪V (x V | x W ) = D∈D(G(V,W ))   xL D a∈D∪LD q L∪V (x a | x pa G(L∪V,W ) (a) )   = D∈D(G(V,W )) a∈D q L∪V (x a | x pre ≺,G(V,W ) (a) )</formula><p>where L D = an G(L∪V,W )D∪L (D) ∩ L, and ≺ is any topological ordering for G(V, W ).</p><p>In other words, the margin over the observed variables factorizes into separate kernels for each district of the latent projection.</p><p>Proof. Simple extension of the proof for W = ∅ found in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref>.</p><p>Lemma 45 Let G(L ∪ V, W ) be a CDAG, and assume q L∪V (x</p><formula xml:id="formula_192">L∪V | x W ) ∈ P c f (G(L ∪ V, W )). Assume v ∈ V is fixable in G(V, W ) = σ L (G(L ∪ V, W )). Then xL φ v (q L∪V (x L∪V | x W ); G(L ∪ V, W )) = φ v (q L∪V (x V | x W ); σ L (G(L ∪ V, W ))).</formula><p>That is, the following commutative diagram holds:  is not identifiable in G(L ∪ V ).</p><formula xml:id="formula_193">q L∪V (x L∪V | x W ) q (L∪V )\{v} (x (L∪V )\{v} | x W ∪{v} ) q L∪V (x V | x W ) q V \{v} (x V \{v} | x W ∪{v} ) φ v (.; G(L ∪ V, W ))</formula><p>Proof. We first prove (31). Algorithm 1 The constraint-finding algorithm in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref> expressed in the CADMG and kernel notation used in this manuscript. v is an element of V .</p><p>Require: G(V ) : an ADMG over a vertex set V , p(x V ) : a density over x V , ≺ : a total topological ordering on V .</p><p>Ensure: A list of constraints on p(x V ) implied by G(V ).</p><p>1: procedure Find-Constraints(G(V ), p(x V ), ≺)</p><p>2:</p><p>L ← {} Ensure: A list of constraints on q(x S | x W ).</p><p>1: procedure Node-Constraints(v, G(S, W ), q S ) 2:</p><p>L ← {}; </p><formula xml:id="formula_194">input subgraph G ′ (L ′ ∪ S) G(S, V \ S) = G(S, W ) = φ V \S (G(V )) input kernel Q[S] q S (x S | x V \S ) = q S (x S | x W ) = φ V \S (p(x V ); G(V ))</formula><p>ancestral margin kernel</p><formula xml:id="formula_195">Q[D ′ ] = Q[S \D] = D Q[S] q D ′ (x D ′ | x V \D ′ ) = φ S\D ′ (q S (x S | x W ); G(S, W )) = φ V \D ′ (p(x V ); G(V ))</formula><p>kernel for subsequent call</p><formula xml:id="formula_196">Q[E] = z∈E Q[D ′ z ] z Q[D ′ z ] q E (x E | x V \E )</formula><p>= φ D ′ \E (q D ′ ; φ D (G(S, W )))</p><p>= φ V \E (p(x V ); G(V ))</p><p>variables Q[S] or q S (•) depends on Pa + (S) ≡ S ∪ pa G(V ) (S) S ∪ pa φ V \S (G(V )) (S)</p><p>Table <ref type="table">1</ref>: Differences between the notation used in Algorithms 1 and 2, and steps (A1) and (A2) of Tian's algorithm. The input to Algorithm 1 is a latent projection G(V ), and the corresponding marginal distribution p(x V ). The input to (A1) is a hidden variable DAG G(V ∪ L), and the corresponding marginal distribution p(x V ) = xL p(x V , x L ).</p><p>edge in I(G) constraint kernel 123456 → 1246 X 6 ⊥ ⊥ X 3 | X 1,2,4 φ 5 (q V ; G)</p><p>12456 → 156 X 6 ⊥ ⊥ X 2 | X 1,5 φ 3,4 (q V ; G)</p><p>1356 → 156 X 6 ⊥ ⊥ X 2 | X 1,5 φ 4,3 (q V ; G)</p><p>1246 → 16 X 6 ⊥ ⊥ X 2 | X 1 φ 5,3,4 (q V ; G)</p><p>1356 → 16 X 6 ⊥ ⊥ X 2,3 | X 1 φ 4,2,5 (q V ; G)</p><p>1356 → 6 X 6 ⊥ ⊥ X 2,3,5 φ 4,2,1 (q V ; G)</p><p>1456 → 6 X 6 ⊥ ⊥ X 4,5 φ 3,2,1 (q V ; G)</p><p>146 → 6 X 6 ⊥ ⊥ X 4 φ 5,3,2,1 (q V ; G)</p><p>156 → 6 X 6 ⊥ X 5 φ 4,3,2,1 (q V ; G)</p><p>Table <ref type="table">2</ref>: The constraints implied by the power DAG in Figure <ref type="figure" target="#fig_6">14</ref>.</p><p>that are not a priori the same. In contrast, our algorithm gives many fewer syntactically equivalent independences in different kernels.</p><p>We also note that Tian's algorithm under the ordering where 3 is placed after 6 does not, for example, give the constraint 6 ⊥ ⊥ 2, 3, 5 directly, but rather it has to be obtained by applying our graphoid axioms to the independences provided.</p><p>It follows that Tian's algorithm corresponds neither to a global property, since it does not give all possible independences, nor a local property, as it repeats many equivalent constraints in different subgraphs.</p><p>Indeed, we can note that when considering an edge in the power DAG, it will only possibly lead to a constraint if the intrinsic set we move to and its parents lose more than one vertex. The subgraph of the power DAG in Figure <ref type="figure" target="#fig_16">12</ref> that corresponds to having removed the edges that do not imply a constraint is shown in Figure <ref type="figure" target="#fig_6">14</ref>; it only has nine edges, corresponding to the constraints in Table <ref type="table">2</ref>.</p><p>Although there is some syntactic redundancy between the constraints corresponding to the edges 12456 → 156 and 1356 → 156, this only holds because we already know that the marginal distribution over the vertices prior to 6 is Markov with respect to the corresponding induced subgraph; in particular, because X 5 ⊥ ⊥ X 2 | X 1 in φ 6,4,3 (q V ; G).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (i) A DAG on five variables and (ii) a DAG representing the model after an experiment to externally fix X 3 . (iii) A DAG on five variables representing a statistical model distinguishable from the model represented by the DAG in (i) by a generalized independence constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Illustration of cases in Definition 4:(a) A ∩ W = ∅; (b) B ∩ W = ∅.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Commutativity diagrams for: (a) Lemma 43; (b) Lemma 45.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 5 .</head><label>5</label><figDesc>Connections with r-factorization. Shpitser et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>between vertices a and b in a mixed graph G is said to be m-connecting given a set C in G if every non-collider on the path is not in C, and every collider on the path is an ancestor of C in G. If there is no path m-connecting a and b given C, then a and b are said to be m-separated given C. Sets A and B are said to be m-separated given C, if for all a, b, with a ∈ A and b ∈ B, a and b are m-separated given</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition A. 2</head><label>2</label><figDesc>(latent projection). Let G be an ADMG with vertex set V ∪ L where the vertices in V are observed, those in L are latent and ∪ indicates a disjoint union. The latent projection G(V ) is a directed mixed graph with vertex set V , where for every pair of distinct vertices a, b ∈ V :(i) G(V ) contains an edge a → b if there is a directed path a → • • • → b on which every non-endpoint vertex is in L. (ii) G(V )contains an edge a ↔ b if there exists a path between a and b such that the non-endpoints are all non-colliders in L, and such that the edge adjacent to a and the edge adjacent to b both have arrowheads at those vertices. For example, a ↔ • • • → b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition A. 4</head><label>4</label><figDesc>(latent projection for CADMGs). Let G(V ∪ L, W ) be a CADMG where W is a set of fixed vertices, the random vertices are V ∪ L and those in L are latent. The latent projection G(V, W ) is a directed mixed graph with random vertex set V , where for every pair of distinct vertices a, b ∈ V ∪ W :(i) G(V, W ) contains an edge a → b if there is a directed path a → • • • → b on which every non-endpoint vertex is in L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>non-collider) on π * then it is a collider (non-collider) on π. It follows from this that d-connection in G implies m-connection in G(V ). Conversely, by Definition 3 for each edge ǫ * with endpoints e and f on π * in G(V ) there is a unique path µ ǫ * with endpoints e and f in G such that there is an arrowhead at e (f ) on ǫ * if and only if the edge on µ ǫ * with e (f ) as an endpoint has an arrowhead at e (f ). It then follows from Lemma 3.3.1 in(Spirtes et al., 1993)  that if there is a path m-connecting a and b given C in G(V ) then there is a path d-connecting a and b given C in G. The result then follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>by the induction hypothesis, all of the corresponding densities f A D (•|•) in (A.6) obey (A.3) and (A.4). Rearranging (A.5) gives:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>showing that if T and Y are m-connected given Z in G |W then T and Y are not separated by Z in (G an(T ∪Y ∪Z) ) a . If T and Y are m-connected given Z in G |W then there are vertices t ∈ T , y ∈ Y such that there is a path µ which m-connects t and y given Z in G |W . By Lemma B.1 the non-colliders on µ form a path µ * connecting t and y in (G an(T ∪Y ∪Z) ) a . Since µ is m-connecting, no non-collider is in Z hence no vertex on µ * is in Z. Thus T and Y are not separated by Z in (G an(T ∪Y ∪Z) ) a . (P c a (G) ⊆ P c m (G)) We show that if T and Y are not separated by Z in (G an(T ∪Y ∪Z) ) a then T and Y are m-connected given Z in G |W . If T and Y are not separated by Z in (G an(T ∪Y ∪Z) ) a then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>there exist vertices t * ∈ T and y * ∈ Y which are m-connected given Z in G |W , hence T and Y are m-connected given Z. Lemma B.3. If G(V, W ) is a CADMG and T and Y are m-separated given Z in G |W then:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>2 and Lemma B.3 either T ∩ W = ∅ and T is m-separated from W \ Z given Z or Y ∩ W = ∅ and Y is m-separated from W \ Z given Z in G |W . Without loss of generality we suppose the former; the other case is symmetric. It follows that we may extend Y such that W ⊆ Y ∪ Z, so that T is m-separated from Y by Z in G |W , and thus the corresponding separation holds in (G an(T ∪Y ∪Z) ) a by Theorem B.2. (Base case) For k = 1, it follows that we have T = {v 1 } and Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>define the initial segments to be the |V | sets pre v ≡ pre G,≺ (v) ∪ {v} = {w : w ≺ v} ∪ {v} consisting of the first k vertices in the ordering, for each k = 1, . . . , |V |. The associated initial segment district is the district of the maximal vertex in each initial segment. Let G be an ADMG with a topological order ≺. Let (C, D) be two intrinsic sets, such that 1. max ≺ (C) = v = max ≺ (D); 2. C = dis G[D\{w}] (v) for some w ∈ D, such that w ∈ F(G[D]). We define I(G) to be the DAG whose vertices are the intrinsic sets of G, and edges D → C if and only if the pair (C, D) satisfies the two conditions above. We call I(G) the intrinsic power DAG for G. For any transition D → C in I(G) obtained by fixing (say) w ∈ D; then C = dis G[D\{w}] (v), where v = max ≺ (D). Note that the power DAG consists of |V | separate connected components, each with an initial segment district as its root node. We are now ready to define the ordered local nested Markov property, which we do by using the transitions represented in the power DAG. To simplify subscripts we use the notation fam G (C) := C ∪ pa G (C). Definition C.3. Let G be an ADMG with an arbitrary topological order ≺. A distribution p obeys the ordered local nested Markov property with respect to (G, ≺), if: also notice that, since none of the variables in pre v \ C are descendants of v and they are all in districts that do not contain v, by Proposition 11 fixing them will have no effect on the status of the conditional independence (C.1). For the second claim, note that C becomes a district in G[D] immediately after fixing w. Then note that any later fixings are within districts other than C, and therefore the Markov blankets of those vertices do not include the maximal vertex v. Hence, again by Proposition 11, the status of the independence involving the conditional distribution of v given D \ {w, v} is unaffected by all these fixings, and so it also holds in φ w (q D ; G[D]) if and only if it holds in φ {w},D\({w}∪C) (q D ; G[D]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Proposition C. 7 .</head><label>7</label><figDesc>Let p be a distribution that is ordered local nested Markov with respect to G and the topological order ≺. Then given any valid fixing sequence for a reachable set R (say s), the kernel φ s (p; G) also district factorizes according to G[R]. Proof. Note first that the ordered local nested Markov property for G implies the same property for G pre k , so we can use the trivial base case with |V | = 1 and an inductive argument to assume that the global Markov property holds for the margin p(x pre G,≺ (k) ) = φ k (p(x pre k ); G); this identity follows from the independence (C.1) with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Theorem 38 p(x V ) is globally nested Markov with respect to G if and only if p(x V ) is ordered local nested Markov with respect to G for any topological ordering ≺. Proof. That the global property implies the ordered local nested Markov property (for any ordering) is obvious from the definitions, since (C.1) and (C.2) correspond to mseparations in reachable subgraphs. For the converse, we simply apply Proposition C.7 and Theorem 35 to obtain the global property.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure 11: (i) An ADMG G; (ii) G[{4, 5, 6}]; (iii) G[{1, 3, 5, 6}].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The connected component of the power DAG for G in Figure 11 with intrinsic sets in which 6 is the maximal vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>xL xL φ v (.; G(V, W )) Proof. φ v → Σ xL direction: Since q L∪V (x L∪V | x W ) ∈ P c f (G(L ∪ V, W)), we haveq L∪V (x L∪V | x W ) = a∈L∪V q L∪V (x a | x pa G (a) ).This implies by Lemma 41 thatφ v (q L∪V (x L∪V | x W ); G(L ∪ V, W )) = a∈(L∪V )\{v} q L∪V (x a | x pa G (a) ), which implies φ v (q L∪V (x L∪V | x W ); G(L ∪ V, W )) ∈ P c f (φ v (G(L ∪ V, W ))). Then by Lemma D.2, xL φ v (q L∪V (x L∪V | x W ); G(L ∪ V, W )) = D∈D(φv(G(V,W ))) a∈D q L∪V (x a | x pre ≺,G (a) ) . (D.1) Σ xL → φ v direction: Similarly, by Lemma D.2, xL q L∪V (x L∪V | x W ) = q L∪V (x V | x W ) = D∈D(G(V,W )) a∈D q L∪V (x a | x pre ≺,G (a) ) Now let D v be the element of D(G(V, W )) such that v ∈ D v . Then by Proposition 25, φ v (q L∪V (x V | x W ); G(V, W )) =   a∈D v \{v} q L∪V (x a | x pre ≺,G (a) )   • D∈D(G(V,W ))\{D v } a∈D q L∪V (x a | x pre ≺,G (a) ) (D.2) Since, by Proposition 24, D(φ v (G(V, W ))) = (D(G(V, W ))\{D v }) ∪ D(G(V, W ) D v \{v} ),the right hand sides of (D.1) and (D.2) are equal.Theorem 46 Let G(V ∪ L) be a DAG. Thenp(x V ∪L ) ∈ P d (G(V ∪ L)) ⇒ p(x V ) ∈ P n (G(V )). Proof. Assume p(x V ∪L ) ∈ P d (G(V ∪ L)), and for a set R reachable in G(V ) with A ⊆ R and B, C ⊆ V (C possibly empty), suppose that A is m-separated from B given C in φ V \R (G(V )) |V \R .By an inductive application of Lemma 43,φ V \R (G(V )) is a latent projection of φ (L∪V )\R (G(L ∪ V )). Therefore, by Lemma D.1, A is m-separated from B given C in φ (L∪V )\R (G(L ∪ V )) |(L∪V )\R .Our assumption, and Corollary 42 then implyX A ⊥ ⊥ X B | X C holds in the kernel φ (L∪V )\R (p(x L∪V ); G(L ∪ V )). By an inductive application of Lemma 45,xL φ (L∪V )\R (p(x L∪V ); G(L ∪ V )) = φ V \R (p(x V ); G(V ))and thusX A ⊥ ⊥ X B | X C holds in φ V \R (p(x V ); G(V )). Our conclusion follows. Lemma D.3. Let G(L ∪ V ) be a hidden variable causal DAG. For any set S reachable from G(V ), the interventional distributions p(x S | do G(L∪V ) (x V \S )) are identifiable from p(x V ) by the kernel φ V \S (p(x V ); G(V )), which depends only on x S and x pa G(V ) (S)\S . Proof. Our conclusion follows by (30) and an inductive application of Lemma 45. That the kernel φ V \S (p(x V ); G) only depends on x S and x pa(S)\S follows by the global nested Markov property, and Theorem 46. Lemma D.4. Let G(L ∪ V ) be a hidden variable causal DAG. For any Y ⊆ V \ A, let A Y = an φ * A (G) (Y ) ∩ A. Then p(x Y | do G (x A )) = p(x Y | do G (x AY )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Proof.</head><label></label><figDesc>Follows by (29) and the global Markov property for CDAGs.Theorem 48 Let G(L∪V ) be a causal DAG with latent projection G(V ).For A ∪Y ⊆ V , let Y * = an G(V ) V \A (Y ). Then if D(G(V ) Y * ) ⊆ I(G(V )), p(x Y | do G(L∪V ) (x A )) = x Y * \Y D∈D(G(V ) Y * ) p(x D | do G(L∪V ) (x pa G(V ) (D)\D )) = x Y * \Y D∈D(G(V ) Y * ) φ V \D (p(x V ); G(V )). (31)If not, there exists D ∈ D(G(V ) Y * ) that is not intrinsic in G(V ), and p(x Y | do G(L∪V ) (x A ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Let A * = V \ Y * ⊇ A. By Lemma D.4 we have p(x Y * | do G(L∪V ) (x A )) = p(x Y * | do G(L∪V ) (x A * )). Let G * (L ∪ (V \ A * ), A * ) = φ A * (G(L ∪ V )); note that since G(L ∪ V ) is a DAG, A * is fixable in G(L ∪ V ), and G * (L ∪ (V \ A * ), A * ) is a CDAG. By Corollary 44, σ L (φ * A * (G(L ∪ V ))) = φ * A * (σ L (G(L ∪ V ))), where σ L is the latent projection operation, that is σ L (G(L ∪ V )) = G(V ). Since G * (Y * , A * ) = σ L (φ * A * (G(L ∪ V ))) = φ * A * (G(V )).Then, by definition of induced subgraphs andY * , G(V ) Y * = (φ * A * (G(V ))) Y * , and G(V ) Y * = G * (Y * , A * ) Y * . Consequently, D(G(V ) Y * ) = D(G * (Y * , A * )).For every D ∈ D(G * (Y * , A * )), define L D ≡ L ∩ an G(L∪V )D∪L (D), and let L * = D∈D(G * (Y * ,A * )) L D . Thus L D is the set of variables h ∈ L, for which there exists a vertex d ∈ D and a directed path h → • • • → d in G(L ∪ V ) on which, excepting d, all vertices are in L.It follows from the construction that:(a) if D, D ′ ∈ D(G * (Y * , A * )), and D = D ′ then L D ∩ L D ′ = ∅; (b) for each D ∈ D(G * (Y * , A * )) we have pa G(L∪V ) (D ∪ L D ) ∩ L * = L D ; (c) Y * ∪ L * is ancestral in G(L ∪ V ) (L∪V )\A , so if v ∈ Y * ∪ L * , pa G(L∪V ) (v) ∩ L ⊆ L * .We now have:p(x Y * | do G(L∪V ) (x A * )) = xL v∈L∪Y * p(x v | x pa G(L∪V ) (v) ) = x L * v∈L * ∪Y * p(x v | x pa G(L∪V ) (v) ) x L\L * v∈L\L * p(x v | x pa G(L∪V ) (v) ) =1 = x L * D∈D(G * (Y * ,A * )) v∈D∪LD p(x v | x pa G(L∪V ) (v) ) = D∈D(G * (Y * ,A * ))   xL D v∈D∪LD p(x v | x pa G(L∪V ) (v)) first equality follows from (30), the second follows from (c), the third from (a), and the fourth from(b). Now, for any givenD ∈ D(G * (Y * , A * )), xL D v∈D∪LD p(x v | x pa G(L∪V ) (v) ) = xL D v∈D∪LD p(x v | x pa G(L∪V ) (v) ) x L\L D v∈L\LD p(x v | x pa G(L∪V ) (v) ) v | x pa G(L∪V ) (v) ) = xL φ V \D (p(x L∪V ); G(L ∪ V )).(D.4) Here the second line uses that pa G(L∪V ) (D ∪ L D ) ∩ (L \ L D ) = ∅, which follows (b), (c) and the definition of L D . Since, by hypothesis, D ∈ D(G(V ) Y * ) = D(G * (Y * , A * )) ⊆ I(G(V )), it follows from Lemma 45 that xL φ V \D (p(x L∪V ); G(L ∪ V )) = φ V \D (p(x V ); G(V )). (D.5) Hence by (D.3), (D.4) and (D.5), p(x Y * | do G(L∪V ) (x A * )) = D∈D(G * (Y * ,A * )) φ V \D (p(x V ); G(V )). The conclusion, (31), then follows since p(x Y | do G(L∪V ) (x A )) = x Y * \Y p(x Y * | do G(L∪V ) (x A * )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>L</head><label></label><figDesc>pre v ← pre G(V ),≺ (v) ∪ {v}; 5: let S ∈ D(G[pre v ]) be the unique district s.t. v ∈ S; 6: if pre v \(mb(v, pre v ) ∪ {v}) = ∅ then 7: L ← L ∪ "(X v ⊥ ⊥ X pre v \(mb(v,pre v )∪{v}) | X mb(v,pre v ) ) [p(x pre v )]"; ← L ∪ Node-Constraints(v, φ pre v \S (G[pre v ]), φ pre v \S (p(x pre v ); G[pre v ])); subroutine of Algorithm 1 which recursively finds constraints associated with a particular vertex v. Require: G(S, W ) : a CADMG, v a vertex in S with no children in G, q S (x S | x W ) : a kernel associated with G,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>3 :L</head><label>3</label><figDesc>for every ∅ ⊂ D ⊂ S closed under descendants in G, s.t. v ∈ D do 4: let D ′ ← S \ D; q D ′ ← φ D (q S ; G(S, W )); 5: if (pa G (S) \ S) \ pa G (D ′ ) = ∅ then 6: L ← L ∪ "(X D ′ ⊥ ⊥ X (pa G (S)\S)\pa G (D ′ ) | X pa G (D ′ )\D ′ ) [q D ′ ]"; let E ∈ D(φ D (G(S, W ))), be the unique district s.t. v ∈ E; 9: if |D(φ D (G(S, W )))| &gt; 1 and (D ′ ∪ pa G (D ′ )) \ (mb G (v, E) ∪ {v}) = ∅ then 10: L ← L ∪ "(X v ⊥ ⊥ X (D ′ ∪pa G (D ′ ))\(mb G (v,E)∪{v}) | X mb G (v,E) ) [q D ′ ]"; ← L ∪ Node-Constraints(v, φ S\E (G), φ D ′ \E (q D ′ ; φ D (G(S, W ))));</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and only if it is a member of P c f (G[R]). If p nested Markov factorizes w.r.t. G then, by Definition 28 and Corollary 32, for every</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>imsart-aos ver. 2007/12/10 file: main.tex date: September 27, 2023</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>See Definition C.6.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="person">Zhongyi Hu</rs> for pointing out an error in the proof of Proposition 6, and the Associate Editor and referees for their helpful comments. The authors completed work on this paper while visiting the <rs type="institution">American Institute for Mathematics</rs> and the <rs type="institution">Simons Institute, Berkeley, California</rs>. The first author was supported in part by <rs type="funder">ONR</rs> Grants <rs type="grantNumber">N00014-19-1-2446</rs> and <rs type="grantNumber">N00014-15-1-2672</rs>, and <rs type="funder">NIH</rs> Grant <rs type="grantNumber">R01 AI032475</rs>.</p><p>The third author was supported in part by <rs type="funder">ONR</rs> Grant <rs type="grantNumber">N00014-19-1-2446</rs>, and <rs type="funder">NIH</rs> Grant <rs type="grantNumber">R01 AI032475</rs>.</p><p>The fourth author was supported in part by <rs type="funder">ONR</rs> Grant <rs type="grantNumber">N00014-21-1-2820</rs>, <rs type="funder">NSF</rs> Grants <rs type="grantNumber">2040804</rs> and <rs type="grantNumber">1942239</rs>, and <rs type="funder">NIH</rs> Grant <rs type="grantNumber">R01 AI127271-01A1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4RXtbn7">
					<idno type="grant-number">N00014-19-1-2446</idno>
				</org>
				<org type="funding" xml:id="_JpKXyZk">
					<idno type="grant-number">N00014-15-1-2672</idno>
				</org>
				<org type="funding" xml:id="_GVRVJnt">
					<idno type="grant-number">R01 AI032475</idno>
				</org>
				<org type="funding" xml:id="_tH2b2tp">
					<idno type="grant-number">N00014-19-1-2446</idno>
				</org>
				<org type="funding" xml:id="_But8u2X">
					<idno type="grant-number">R01 AI032475</idno>
				</org>
				<org type="funding" xml:id="_ErbNeQT">
					<idno type="grant-number">N00014-21-1-2820</idno>
				</org>
				<org type="funding" xml:id="_QNWrhqf">
					<idno type="grant-number">2040804</idno>
				</org>
				<org type="funding" xml:id="_6RvH9q3">
					<idno type="grant-number">1942239</idno>
				</org>
				<org type="funding" xml:id="_Uz849gT">
					<idno type="grant-number">R01 AI127271-01A1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials for</head><p>Nested Markov Properties for Acyclic Directed Mixed Graphs September 27, 2023</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Graphical Model Definitions And Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Graphical Definitions</head><p>An undirected graph G(V ) is a pair consisting of a finite set of vertices V and a finite set of unordered pairs of vertices corresponding to undirected edges, which we denote by E ⊆ {{a, b} ⊆ V : a = b}. We denote undirected edges {a, b} by ab.</p><p>A directed graph G(V ) is similar but the edge set consists of ordered pairs: E ⊆ {(a, b) ∈ V × V : a = b}; if (a, b) ∈ E we write a → b. A directed acyclic graph (DAG) is subject to the restriction that there are no directed cycles v → • • • → v.</p><p>A directed mixed graph G(V ) is a graph with a set of vertices V , and a set of edges E which are each either directed (→) or bidirected (↔). A path in any of our graphs G is a sequence of distinct, adjacent edges, of any type or orientation, between distinct vertices.</p><p>The first and last vertices on the path are the endpoints. In mixed graphs it is necessary to specify a path as a sequence of edges rather than vertices because it is possible that there is both a directed and a bidirected edge between the same pair of vertices. is a parent of a, and a is a child of b. A vertex a is said to be an ancestor of a vertex d Proof.</p><p>Now consider a topological ordering ≺ on the vertices in G under which r is the last vertex in D r , so that D r \ {r} ⊆ pre G,≺ (r); since r ∈ F(G), such an ordering exists. By (A.3), and taking A = V , we have that:</p><p>), <ref type="bibr">(A.11)</ref> where</p><p>) by Proposition 14. Finally,</p><p>), by the local Markov property and (A.3). Hence these terms cancel as required.</p><p>A.7 Invariance to the order of fixing in an ADMG <ref type="bibr">Lemma 30</ref> Let G(V, W ) be a CADMG with r, s ∈ F(G) and let q V be a kernel Markov w.r.t. G. Then φ r,s (G) = φ s,r (G) and φ r,s (q V ; G) = φ s,r (q V ; G).</p><p>Proof. That the resulting graphs are the same is immediate since φ r removes edges into r, while φ s removes edges into s.</p><p>To show that the resulting kernels are the same we will show that if r, s ∈ F(G) then the product of the two divisors arising in (20</p><p>are the same as the product of the divisors in φ s (q</p><p>Let D r ∈ D(G) be the district containing r in G. The divisor when first fixing r is given by:</p><p>where q D r is given by (A.3) and (A.4).</p><p>(i) for each v ∈ V we have that the independences</p><p>hold, where C = dis pre v (v) and φ pre v \C is implemented via some valid fixing sequence;</p><p>(ii) for every transition D → C in I(G) (obtained by fixing w ∈ D) with v being maximal under ≺ in C, we have:</p><p>where q D is the unique kernel resulting from some valid fixing sequence for D, and φ D\(C∪{w}) is implemented via some valid fixing sequence.</p><p>Remark C.4. Note that there is one independence associated with each initial segment district, and one associated with each transition in the power DAG. Any of these independences may be null, in that the right hand side of the independence might not contain any variables.</p><p>We will show as part of our proof of Proposition C.7 that the kernel q D in (C.2) is invariant to the choice of valid fixing sequences in (C.1) and (C.2); hence, the local nested Markov property itself remains invariant to any choice of valid sequence for any q D that appears in the definition.</p><p>Note that the above definition is not the same as that given in the main body of the paper, but it follows from the next result that these two sets of independences are equivalent. Proof. For the first claim, note that this independence will clearly hold under p(x pre v ) if and only if it holds under p, since all the variables involved are contained in pre v . Then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Saturated Nested Models</head><p>In this subsection, we assume a fixing sequence 1, . . . , k for V and denote G (1) := G and</p><p>Our first lemma will be useful in showing the existence of distributions that are not Markov with respect to a particular graph.</p><p>Lemma C.11. Let G be an ADMG and 1, . . . , i, . . . , j be a valid fixing sequence in which</p><p>Then:</p><p>) for all ℓ ≤ j. In particular, if X i ⊥ ⊥ X j in p, then this also holds for each q V (ℓ) .</p><p>Proof. For ℓ = i, j, probabilistic fixing involves dividing by q</p><p>. By the independence, this is the same as both q</p><p>and hence by Lemma 22 both the conditional distribution of X i , X j given X V \{i,j} and the marginal distribution of X i , X j is preserved in q V (ℓ+1) .</p><p>When fixing i, we divide by q</p><p>) and, by hypothesis, j / ∈ mb G (i) (i).</p><p>Hence, again by <ref type="bibr">Lemma 8</ref>, the distribution of X j conditional on X V \{j} is preserved.</p><p>Then since X j ⊥ ⊥ X V \{i,j} | X i , this means that the distribution of X j conditional on X i is preserved.</p><p>Lemma C.12. Let v be a childless vertex in a complete ADMG G. Then, for any distribution p and valid fixing sequence, v is fixed by marginalization. 1 In addition, if any w ∈ sib G (v) is fixed before v, it is also fixed by marginalization.</p><p>Proof. Note that every other random vertex is a parent or sibling of v, and hence contained in mb G (v). Certainly, then, if v is fixed from G then it is fixed by marginalization. Let w be a sibling of v, and note that if w is fixable in φ V \R (G) then it is also childless in φ V \R (G). To see this, suppose for contradiction that w → t; by completeness, either t → v or t ↔ v, but in either case w is not fixable and we reach our contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5.1 Maximal Arid Graphs</head><p>Let A ⊆ V be a set of vertices, and define the intrinsic closure of A, denoted A G , to be the smallest reachable set containing A. Note that this set is well-defined by Theorem 31. We say that two vertices v, w are densely connected if either:</p><p>Lemma C.14. The condition in Theorem 39 is equivalent to every pair of vertices being densely connected.</p><p>Proof. First suppose i and j are densely connected. If i ∈ pa G ( j G ), then for any fixing</p><p>Now suppose that i and j are not densely connected. Consider any fixing sequence</p><p>G) has a childless vertex that is not in {i, j}. Since childless vertices are always fixable, we have a contradiction, since the only vertices fixable in</p><p>Without loss of generality assume i does not have children in φ V \ {i,j} (G). Then, since {i, j} G is not bidirected-connected, i is not in the Markov blanket of j in G (j) for a fixing sequence of V \ {i, j} G , followed by j, followed by any remaining vertices in</p><p>so by assumption j / ∈ mb G (i) (i). Thus, we have constructed a fixing sequence for which the condition in Theorem 39 fails.</p><p>In <ref type="bibr">Shpitser et al. (2018, Theorem 30)</ref> it is shown that every graph G is nested Markov equivalent to a graph G † , called its maximal arid projection, for which {v} G † = {v}. The graph G † is simple (at most one edge between each pair of vertices), and is obtained from G by joining pairs of vertices that are densely connected. It follows that the condition in Theorem 39 is satisfied only by graphs that are nested Markov equivalent to a complete, simple graph.</p><p>Theorem 39 Let G be an ADMG. The model P n (G) is saturated if and only if for every valid fixing sequence r 1 , . . . , r k , and every i, j ∈ 1, . . . , k, either r i ∈ mb G (j) (r j ) or</p><p>Proof. Choose G such that the condition is false for a fixing sequence 1, . . . , k, and a pair of vertices i, j with i &lt; j. Pick a distribution p in which X V \{i,j} ⊥ ⊥ X i , X j but with X i ⊥ ⊥ X j ; we claim p is not nested Markov with respect to G. By Lemma C.11 we have</p><p>.</p><p>By ordinary graphoids we then obtain</p><p>But the nested local Markov property implies that</p><p>. This contradicts our construction above, and hence the distribution p is not nested Markov with respect to G, and the model is not saturated. Now, since the condition in the statement of the theorem is equivalent to the resulting maximal arid projection G † being complete, we will proceed to show that if a graph is simple and complete, then the resulting nested Markov model is saturated. Then since every graph G for which the original condition holds is nested Markov equivalent to a complete simple graph G † , the result will follow.</p><p>Suppose all vertex pairs are adjacent in G, take any topological ordering ≺, pick any S ∈ I(G) and let i be the ≺-greatest element of S. We will show that q S does not depend upon any x j for j / ∈ S ∪ pa G (S), and hence that the local Markov property holds.</p><p>First, note Lemma C.13 implies that we may assume any vertices that are not ancestors of S have already been fixed by marginalization by reordering reverse topologically. To see this, note that if there is some childless vertex not in S, then Lemma C.13 states that we can fix it first without consequence, and that this corresponds to marginalization.</p><p>Hence, if j is a child of i, then q S does not depend upon x j . Hereafter we assume without loss of generality that all vertices in G are ancestors of S, and in particular i does not have any children.</p><p>Since the graph is complete, this leaves only two possibilities. If j ∈ pa G (i), then j ∈ pa G (S), and there is nothing to show. Otherwise j ↔ i, and there is nothing to prove unless j / ∈ S ∪ pa G (S). In this case, by Lemma C.12, j fixed by marginalization.</p><p>We have shown that for any distribution p and any intrinsic set S, the kernel q S resulting from fixing does not depend upon any x j for j / ∈ S ∪ pa G (S); hence the local Markov property holds.</p><p>Corollary 40 Let G be a complete ADMG; then P n (G) is saturated.</p><p>Proof. Note that if i ↔ j, then certainly i and j are in each other's Markov blankets whenever one of these is fixed. If i → j, then i is in j's Markov blanket, and will remain so even if i is fixed. Hence the condition in Theorem 39 holds. </p><p>3 That is, the following commutative diagram holds:</p><p>2 The authors have been made aware of an independent proof by Bhattacharya and Nabi of the result that the nested model associated with a complete ADMG is nonparametric saturated <ref type="bibr" target="#b58">(Bhattacharya et al., 2022)</ref>.</p><p>3 Note that v is fixable in G(L ∪ V, W ) since this graph has no bidirected edges, and thus all random vertices are fixable.</p><p>To establish the last claim, fix D ∈ D(G(V ) Y * ) \ I(G(V )), and let D * ≡ D be the minimal intrinsic superset of D. Assume, for contradiction, that D * does not intersect A * .</p><p>Then D * ⊆ Y * . But since D * is intrinsic, it must be a subset of some</p><p>But this is impossible since D D * , and  <ref type="bibr">(Shpitser and Pearl, 2006)</ref>. <ref type="bibr">(Shpitser and Pearl, 2006)</ref></p><p>, where L † consists of a hidden variable for every bidirected arc in G(V ) (see, e.g. <ref type="bibr">Richardson and Spirtes (2002)</ref> §6 for a formal definition of G(L † ∪ V )). Counterexamples witnessing this non-identification can be easily extended to counterexamples witnessing non-identification of p</p><p>in G(L ∪ V ) as follows. First, we restrict attention to a submodel of the causal model represented by G(L † ∪ V ) where</p><p>We then note that it follows from Theorem 2 in <ref type="bibr">(Evans, 2018)</ref> that the model associated with G(L † ∪ V ) is a submodel of that associated with G(L ∪ V ). This immediately implies</p><p>Proof. Follows directly by Theorem 48, since Y * , D(G(V ) Y * ), I(G(V )) and the terms on the RHS of (31) are defined solely in terms of the latent projection.</p><p>E Connections with Tian's Constraint Algorithm <ref type="bibr" target="#b71">Tian and Pearl (2002b)</ref> gave an algorithm for deriving constraints from a latent variable model. In this section we show that a translation of this algorithm into the kernel framework used in this paper may be used to derive a set of constraints that define the nested Markov model. However, as we illustrate below in §E.1 this set of constraints is longer and hence more redundant than the ordered local nested property. At the same time there are also constraints arising from the global nested property that are not given by Tian's algorithm. Below, we will call the algorithm in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b</ref>) "Tian's algorithm," and our reformulation "Algorithm 1." Whereas Tian's algorithm takes a latent variable DAG G(L ∪ V ) as input, Algorithm 1 takes an ADMG G(V ) (without necessarily presupposing the existence of an underlying hidden variable DAG corresponding to this ADMG G(V )); both algorithms require as inputs a topological order ≺ on V , and the observed marginal distribution p(V ). Given these inputs, both Tian's algorithm, and Algorithm 1 inductively construct a list of constraints that is initially empty.</p><p>The first set of steps of Tian's algorithm lists constraints for every v ∈ V among the set pre v ≡ {v} ∪ pre G(V ),≺ (v). The constraints enumerated in the step (A1) are that each v is independent of variables outside its Markov blanket in G(pre v ), conditional on its Markov blanket. These steps correspond exactly to those in Algorithm 1, which enumerates the same list of constraints.</p><p>The second part (A2) of Tian's algorithm is recursive, taking as input a subgraph G ′ (L ′ ∪ S) obtained from the original hidden variable graph G(L ∪ V ), as well a q-factor Q[S] obtained from the original marginal distribution p(V ). Here L ′ is a subset of L consisting of hidden variables relevant for S, defined as ancestors of S in the subgraph G ′ (L ∪ V ) S∪L . Each step (A2) considers constraints in certain q-factors obtained from the q-factor Q[S].</p><p>(A2) iterates over all observable subsets D of S closed under descendants, 4 and possibly adds a constraint on the margin of Q[S] involving variables D ′ ≡ S \ D; i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q[D</head><p>. This constraint is based on the observation that some observed parents of D in the graph G(V ), which <ref type="bibr" target="#b71">Tian and Pearl (2002b)</ref> call "effective par- ents" may not be parents of D ′ , which implies that the q-factor q D</p><p>is independent of these missing parents. This step, outlined in the first paragraph of the description of (A2) in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref>, corresponds to lines 3-7 of Algorithm 2.</p><p>Note that in our reformulation the marginalization operation above is instead represented by the fixing operator φ D (q S ; G(S, W )).</p><p>Every call to (A2) of Tian's algorithm potentially adds another constraint associated with the set D ′ . This constraint is based on examining the latent projection</p><p>where E is the district in G ′ (D ′ ) containing v. This step, outlined in the second paragraph of the description of (A2) in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref>, corresponds to lines 8-11 of Algorithm 2.</p><p>Finally, Tian's algorithm is called recursively with v,</p><p>,</p><p>where</p><p>and the corresponding subgraph of G(L ∪ V ). The final recursive call is outlined in the third paragraph of the description of (A2) in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref>, and corresponds to line 12 of Algorithm 2. Note that the above formula expressing</p><p>Table <ref type="table">1</ref> displays the mapping between objects used in steps (A1) and (A2) of Tian's algorithm, and our reformulation via Algorithms 1 and 2.</p><p>While the graph input to the recursive part of Tian's algorithm is always some subgraph G ′ (L ′ ∪ S) of the original hidden variable graph G(L ∪ V ), the graph input to Algorithm 2 is the CADMG φ V \S (G(V )). Here, L ′ is the subset of latent variables relevant to S i , which are ancestors of S i in G Si∪L . Lemma E.1 below shows that this reformulation preserves the structure of Tian's algorithm.</p><p>Lemma E.1. Let G ′ (L ′ ∪ S) be a DAG obtained during some call to step (A2) of Tian's algorithm from the original DAG G(L ∪ V ), where V and S are the sets of observable vertices in G and G ′ , respectively. Then:</p><p>In other words, G ′ (S) is equal to the graph obtained from G(V ) by fixing all vertices outside of S and removing all fixed vertices from the resulting CADMG.</p><p>Proof. We first note that (e) holds for every intrinsic set S by definition of "effective parents" in <ref type="bibr" target="#b71">(Tian and Pearl, 2002b)</ref> and definition of latent projections, and the fixing operator on ADMGs. Thus, establishing (a) for an input set S establishes (e) for S.</p><p>We prove the claim by induction on the recursive structure of Tian's algorithm.</p><p>We first establish that (a), <ref type="bibr">(b)</ref>, <ref type="bibr">(c)</ref>, and <ref type="bibr">(d)</ref> hold in the base case, which consists of the initial recursive calls to step (A2) in step (A1). Note that when Tian's algorithm calls step (A2) from step (A1), it is with sets S i ≡ S which are districts in G[T ], where each T is a subset of V that is ancestral in G(V ). Thus, these sets S are intrinsic by definition, which establishes (a).</p><p>The graph G ′ (L ′ ∪ S i ) given as input to the initial recursive call of Tian's algorithm is <ref type="bibr">Lemma 43</ref>. This fact, coupled with the fact that the operations of taking latent projections and restricting the graph to random vertices commute in CADMGs establishes <ref type="bibr">(b)</ref>. The link between graphs used in recursive calls of Algorithm 2, and step (A2) of Tian's algorithm provided by <ref type="bibr">(b)</ref> immediately implies <ref type="bibr">(c)</ref> and <ref type="bibr">(d)</ref>.</p><p>Assume we are in step (A2) with a set S, where (a), <ref type="bibr">(b)</ref>, <ref type="bibr">(c), and (d)</ref> hold by the inductive hypothesis.</p><p>Since <ref type="bibr">(c)</ref> and <ref type="bibr">(d)</ref> hold for S, (a) holds for E. Since (a) holds for E, we can repeat the above argument for the base case to establish <ref type="bibr">(b)</ref> for E. This, in turn, establishes <ref type="bibr">(c)</ref> and <ref type="bibr">(d)</ref> for E.</p><p>This establishes the claim.</p><p>Step (A2) of Tian's algorithm takes q-factors as inputs. The analogous inputs for our reformulated Algorithm 2 are kernels q S (x S | x W ) obtained from the original margin p(x V ) by the fixing operator. Lemma E.1 above ensures that every such kernel q S (x S | x W ) has the same form as the corresponding q-factor Q[S]. The original formulation of Tian's algorithm did not fully describe the order of fixing operations used to construct q-factors.</p><p>Instead, as described above, any particular call with a set S as input first marginalized out a subset D ⊆ S, and then constructed the q-factor Q[E] for a subset E ⊆ S \D from the q-factor S\D Q[S]. Thus, in our translation of Tian's procedure into Algorithms 1 and 2, the order of fixing operations is only partly specified, to be consistent with the specified sequence of q-factor constructions and marginalizations. Specifically, for every set S that serves as input to Algorithm 2, the variables in S \ D, for some set closed under descendants in G(S, W ), are fixed first before variables in (S \ E) \ (S \ D) are fixed, and further variables in V \ S are fixed before anything in S.</p><p>Lemma E.1 establishes that Tian's algorithm, and our reformulation via Algorithms 1 and 2 are structurally isomorphic. We now turn to the connection between our reformulated algorithm and the local nested Markov property.</p><p>Consider an intrinsic set S, and a path S 0 , S 1 , . . . , S in the intrinsic power DAG I(G(V )) starting from the root node S 0 corresponding to S. Then there exists a sequence of nested recursive calls to Algorithm 2 which use the sets in the above path as inputs in the specified order. Further, there exists at least one fixing sequence w on V \ S with the order of operations consistent with these calls, in the sense that all vertices in S i \ S i+1 are fixed before vertices in S i+1 \ S i+2 , and vertices in S i \ S i+1 are fixed according to the restriction described above.</p><p>Lemma E.2. Fix an arbitrary ADMG G(V ). Then for every intrinsic set S k in G(V ), and every sequence of intrinsic sets S 0 , . . . , S k-1 , S k forming a path to S k from the corresponding root S 0 in I(G(V )), there exists a sequence of successive recursive calls to Algorithm 2 with a sequence of vertex set inputs equal to S 0 , . . . , S k-1 , S k .</p><p>Note that there are many sequences of successive recursive calls to Algorithm 2 that do not correspond to sequences of intrinsic sets forming a path in the intrinsic power Proof. This is an immediate corollary of Lemma E.2, and an induction on the recursive structure of Algorithm 2.</p><p>Let P t (G, V, ≺) be the set of distributions obeying restrictions in the list returned by Algorithms 1 and 2.</p><p>Proposition E.4. Let G(V ) be an ADMG over vertex set V . Then</p><p>Proof. It suffices to show that every constraint found by Algorithms 1 and 2, if given a graph G(V ) as one of the inputs, is implied by some constraint given by the global nested Markov property for G(V ).</p><p>All constraints found in Algorithm 1 on line 6 are ordinary conditional independence constraints. Moreover, they are easily seen to follow from the m-separation criterion, which forms a part of the global nested Markov property (since the sets of nodes T in which these constraint are found are all reachable in G(V )).</p><p>Consider some D ′ obtained during a recursive call of Algorithm 2. By Lemma E.1, D ′ is a set of random vertices in a set ancestral in a CADMG corresponding to a set reachable in G(V ). Therefore D ′ is itself reachable, so the global nested Markov property implies that the kernel q</p><p>which is precisely the constraint on line 6.</p><p>Similarly, if the preconditions on line 9 hold, v is m-separated from a non-empty</p><p>This directly implies the constraint on line 10. Proposition E.5. Let G(V ) be an ADMG with a vertex set V . Then</p><p>Proof. Throughout the proof we will ignore trivial independences of the form X A ⊥ ⊥</p><p>Consider the list of restrictions arising from (C.1) and (C.2) under an ordering ≺, where each kernel q S is constructed according to some sequence w that corresponds to a path from a root node to S in I(G(V )). We will show that this list is implied by the list of restrictions output by Algorithms 1 and 2. That this list of restrictions exists follows by Lemma E.3.</p><p>That Algorithm 1 implies all restrictions in (C.1) follows immediately by line 7.</p><p>By Lemma E.3, every with a restriction in (C.2) serves as an input to some call of Algorithm 2. For such a kernel q S , we obtain the following (possibly trivial) independence restrictions:</p><p>(E.1)</p><p>It is sufficient to show that these restrictions imply: By the weak union graphoid axiom and (E.1), since v ∈ D ′ , we conclude that:</p><p>Combining (E.2) with this independence using the contraction graphoid axiom ((Q ⊥ ⊥ Thus, the above independence is equivalent to</p><p>That this independence also holds in</p><p>follows by the inductive application of the axiom of modularity (Proposition 11). This establishes (E.3).</p><p>Theorem 51 For an ADMG G(V ), let P t (G, V, ≺) be the set of densities p(x V ) in which the list of constraints found by Algorithm 1 holds. Then P t (G, V, ≺) = P n (G(V )).</p><p>Proof. This is an immediate corollary of Propositions E.4, E.5, and Theorem 38. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Markov equivalence for ancestral graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2808" to="2837" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parameter identifiability of discrete Bayesian networks with hidden variables</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stanghellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valtorta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Einstein-Podolsky-Rosen paradox</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 22nd International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emulating a trial of joint dynamic strategies: an application to monitoring and treatment of HIV-positive individuals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Caniglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abgrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mugavero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hernández-Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Seventeenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001. 2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2428" to="2446" />
		</imprint>
	</monogr>
	<note>Instrumentality tests revisited</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proposed experiment to test local hidden-variable theories</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Clauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shimony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">880</biblScope>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Constantinou</surname></persName>
		</author>
		<title level="m">Conditional Independence and Applications in Statistical Causality</title>
		<imprint>
			<publisher>University of Cambridge</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Department of Pure Mathematics and Mathematical Statistics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Influence diagrams for causal modelling and inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="161" to="189" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional independence in statistical theory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Ser. B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphical methods for inequality constraints in marginalized DAGs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphs for margins of Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="625" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Margins of discrete Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="2623" to="2656" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood fitting of acyclic directed mixed graphs to binary data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Sixth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty Sixth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smooth, identifiable supermodels of discrete DAG models with latent variables</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="848" to="876" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pearl&apos;s calculus of interventions is complete</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valtorta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty Second Conference On Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized instrumental inequalities: testing the instrumental variable independence assumption</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kédagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mourifié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="661" to="675" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting nonsystematic covariate monitoring to broaden the scope of evidence about the causal effects of adaptive treatment strategies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kreif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sofrygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schmittdiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="329" to="342" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graphical Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon</publisher>
			<pubPlace>Oxford, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The inflation technique completely solves the causal compatibility problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Navascués</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="91" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identification of the joint effect of a dynamic treatment intervention and a stochastic monitoring intervention under the no direct effect assumption</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schmittdiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of causal inference</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan and Kaufmann</publisher>
			<pubPlace>San Mateo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the testability of causal models with latent and instrumental variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 11th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A complete generalized adjustment criterion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Textor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-first Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Thirty-first Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scand. J. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Supplementary materials for nested markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ancestral graph Markov models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new approach to causal inference in mortality studies with sustained exposure periods -application to control of the healthy worker survivor effect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Modeling</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1393" to="1512" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Testing and estimation of direct effects by reparameterizing directed acyclic graphs with structural nested models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation, Causation, and Discovery</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="349" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimation of effects of sequential treatments by reparameterizing directed acyclic graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 13th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="309" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Markov properties for mixed graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="676" to="696" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acyclic linear SEMs obey the nested Markov property</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fourth Conference on Uncertainty in Artificial Intelligence (UAI-18)</title>
		<meeting>the Thirty-fourth Conference on Uncertainty in Artificial Intelligence (UAI-18)</meeting>
		<imprint>
			<date type="published" when="2018-09-27">2018. September 27, 2023</date>
		</imprint>
	</monogr>
	<note>imsart-aos ver. 2007/12/10 file: main.tex date</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An introduction to nested Markov models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-Markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-First National Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dormant independence</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third Conference on Artificial Intelligence (AAAI 2008)</title>
		<meeting>the Twenty Third Conference on Artificial Intelligence (AAAI 2008)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1081" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Testing edges by truncations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1957" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient algorithm for computing interventional distributions in latent variable causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Uncertainty in Artificial Intelligence (UAI-11)</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recursive versus non-recursive systems: An attempt at synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="417" to="427" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conditional independence relations have no finite complete characterization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studený</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory, Statistical Decision Functions and Random Processes. Transactions of the 11th Prague Conference</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Dordrecht -Boston -London (also Academia</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">B</biblScope>
			<biblScope unit="page" from="377" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the testable implications of causal models with hidden variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI-02</title>
		<meeting>UAI-02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno>R-150</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Los Angeles</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Probability distributions with summary graph structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="845" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distortion of effects caused by indirect confounding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="17" to="33" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explanations for multivariate structures derived from univariate recursive regressions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ber. Stoch. Verw. Geb., Univ. Mainz 94. Department of Statistics</title>
		<imprint>
			<biblScope unit="page">98195</biblScope>
			<date type="published" when="1994">1994</date>
			<pubPlace>University of Washington B313 Padelford Hall Northeast Stevens Way Seattle, WA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">University of Oxford 24-29 St Giles&apos; Oxford OX1 3LB, UK E-mail: evans@stats.ox.ac.uk Department of Biostatistics</title>
		<author>
			<persName><forename type="first">E-Mail</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">tsr@stat.washington.edu Department of Statistics</title>
		<meeting><address><addrLine>Harvard; Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2115</biblScope>
		</imprint>
		<respStmt>
			<orgName>Chan School of Public Health Harvard University 677 Huntington Avenue Kresge Building</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E-Mail</forename></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>robins@hsph.harvard.edu Department of Computer Science, Johns Hopkins University Baltimore</orgName>
		</respStmt>
	</monogr>
	<note>MD 21218 E-mail: ilyas@cs.jhu.edu</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">In particular, only recursive calls that marginalize a singleton vertex take part in such sequences of intrinsic sets</title>
		<author>
			<persName><surname>Dag I(g(v ))</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">But then the only fixable vertices in S ′ are also in S; this contradicts the assumption that S is intrinsic. Next, fix a sequence S 0 , . . . , S k-1 , S k forming a path to S k in I(G(V )), with the ≺-maximal vertex v in S k . Note that the set S 0 is a district in the ancestral set pre i for some i, and thus is one of the inputs to Algorithm 2. For every pair of sets S i , S i+1 adjacent in the sequence, there exists an element w fixable in G[S i ] (and thus in φ V \Si (G(V ))) such that S i+1 is a district in G[S i \ {w}] (and thus in φ V \(Si\{w}) (G(V ))). The conclusion follows since {w} is closed under descendants in G</title>
		<author>
			<persName><forename type="first">V</forename><surname>S ∈ I(g(v ) ; G(v )) ; G(d</surname></persName>
		</author>
		<author>
			<persName><surname>\ S)) = Φ V \si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Since S is bidirected-connected, it is contained in some element in D</title>
		<imprint/>
	</monogr>
	<note>Since Algorithm 2 is called from Algorithm 1 with every element in D(G(V )), there exists a smallest S ′ ⊃ S called by Algorithm 2. However, by hypothesis there is no strict ancestral subset D ′ of S ′ in G(S ′ , W ) which contains S, where W = V \ S ′ . Since S ′ was called from Algorithm 2, S ′ is a single district in G(S ′ , W ), and since no ancestral subset contains S it is the case that every d ∈ S ′ is an ancestor of some element of S in G(S ′ , W ). Thus, since, by induction, S i serves as an input to Algorithm 2, so does S i+1 . Given an intrinsic S k in G(V ), we call a fixing sequence for S k consistent with I(G(V )</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">S k-1 , S k forming a path to S from the appropriate root node in I(G(V )), every vertex in S i \ S j is fixed before every vertex in S j , if i &lt; j, and further for each S i , the vertex w i associated with the power DAG</title>
		<title level="s">sequence of intrinsic sets S 0</title>
		<imprint/>
	</monogr>
	<note>transition from S i to S i+1 is fixed before any other element in S i . Lemma E.2 implies the following result</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">For any kernel q S mentioned in (C.1) and (C.2) for an ADMG G(V ), obtained by some fixing sequence w consistent with I(G(V )), there exists an implementation of Algorithms 1 and 2 with a recursive call that has q S as input, and q S is implemented by Algorithm 2 by fixing from p(V ) in the order specified by w</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lemma</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>{1, 2, 3, 4, 5, 6}: no constraint. Consider ancestral subgraphs: 1. {1, 2, 3, 5, 6}: no constraint. new c-component of 6 is {1, 3, 5, 6}, so consider ancestral subgraphs: (a) {3, 5, 6}: 6 ⊥ ⊥ 2, 3, 5 [φ 4,2,1 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><surname>{1</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3, 6}: 1, 6 ⊥ ⊥ 2, 3 [φ 4,2,5 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">}: 6 ⊥ ⊥ 5</title>
		<imprint/>
	</monogr>
	<note>φ 4,2,3,1 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">}: 6 ⊥ ⊥ 4</title>
		<imprint/>
	</monogr>
	<note>φ 5,3,2,1 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">}: 6 ⊥ ⊥ 5</title>
		<imprint/>
	</monogr>
	<note>φ 4,3,2,1 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">}: 6 ⊥ ⊥ 4</title>
		<imprint/>
	</monogr>
	<note>φ 5,3,2,1 (q V ; G)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Note that, although many of these constraints are redundant under the model, this is not immediately apparent without our Theorem 31 that shows the order of fixing is unimportant</title>
		<imprint/>
	</monogr>
	<note>In particular, we highlight that Tian&apos;s algorithm gives the constraints 1(d), 3(a) and 7 all of which give a marginal constraint between X 5 and X 6 under kernels References</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semiparametric inference for causal effects in graphical models with hidden variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">295</biblScope>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Margins of discrete Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="2623" to="2656" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Marginalizing and conditioning in graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="817" to="840" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Independence properties of directed markov fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Leimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="491" to="505" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan and Kaufmann</publisher>
			<pubPlace>San Mateo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A theory of inferred causation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Markov properties for acyclic directed mixed graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scand. J. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ancestral graph Markov models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Markov properties for mixed graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="676" to="696" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Acyclic linear SEMs obey the nested Markov property</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fourth Conference on Uncertainty in Artificial Intelligence (UAI-18)</title>
		<meeting>the Thirty-fourth Conference on Uncertainty in Artificial Intelligence (UAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Identification of joint interventional distributions in recursive semi-Markovian causal models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-First National Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1219" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A general identification condition for causal effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighteenth National Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On the testable implications of causal models with hidden variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI-02</title>
		<meeting>UAI-02</meeting>
		<imprint>
			<date type="published" when="2002">2002b</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Probability distributions with summary graph structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wermuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="845" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
