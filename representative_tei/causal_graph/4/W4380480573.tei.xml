<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_RTFvXrr #_JHrUkqf">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_5uXhsuw">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-06-13">13 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">C</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="laboratory">S. Pan is with</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Wang Are With Fujian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="laboratory">S. Pan is with</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="laboratory">S. Pan is with</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@cs.uic.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Computer Science and Technology Department, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing</orgName>
								<orgName type="department" key="dep3">Ministry of Education of China</orgName>
								<orgName type="laboratory">Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution" key="instit1">Spatio-Temporal Joint Graph Convolutional Networks for Traffic Forecasting</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">FEIT</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-13">13 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.13684v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatio-temporal</term>
					<term>graph convolutional network</term>
					<term>traffic forecasting Spatial connection Temporal connection Spatio-temporal connection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shifted their focus towards formulating traffic forecasting as a spatio-temporal graph modeling problem. Typically, they constructed a static spatial graph at each time step and then connected each node with itself between adjacent time steps to create a spatio-temporal graph. However, this approach failed to explicitly reflect the correlations between different nodes at different time steps, thus limiting the learning capability of graph neural networks. Additionally, those models overlooked the dynamic spatio-temporal correlations among nodes by using the same adjacency matrix across different time steps. To address these limitations, we propose a novel approach called Spatio-Temporal Joint Graph Convolutional Networks (STJGCN) for accurate traffic forecasting on road networks over multiple future time steps. Specifically, our method encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which represent comprehensive and dynamic spatio-temporal correlations. We further introduce dilated causal spatio-temporal joint graph convolution layers on the STJG to capture spatio-temporal dependencies from distinct perspectives with multiple ranges. To aggregate information from different ranges, we propose a multi-range attention mechanism. Finally, we evaluate our approach on five public traffic datasets and experimental results demonstrate that STJGCN is not only computationally efficient but also outperforms 11 state-of-the-art baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S PATIO-TEMPORAL data forecasting has received increas- ing attention from the deep learning community in recent years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. It plays a vital role in a wide range of applications, such as traffic speed prediction <ref type="bibr" target="#b3">[4]</ref> and air quality inference <ref type="bibr" target="#b4">[5]</ref>. In this paper, we study the problem of forecasting the future traffic conditions given historical observations on a road network.</p><p>Recent studies formulate traffic forecasting as a spatiotemporal graph modeling problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The basic assumption is that the state of each node is conditioned on its neighboring node information. Based on this, they construct a spatial graph with a pre-defined <ref type="bibr" target="#b3">[4]</ref> or data-adaptive <ref type="bibr" target="#b6">[7]</ref> adjacency matrix. In such a graph, each node corresponds to a location of interest (e.g., traffic sensor). The graph neural network <ref type="bibr" target="#b11">[12]</ref> is applied on that graph to model the correlations among spatial neighboring (Corresponding author: Xiaoliang Fan) nodes at each time step. To leverage the information from temporal neighboring nodes, they further connect each node with itself between adjacent time steps, which results in a spatio-temporal graph, as shown in Figure <ref type="figure">1</ref>(a). The 1D convolutional neural network <ref type="bibr" target="#b5">[6]</ref> or recurrent neural network <ref type="bibr" target="#b3">[4]</ref> is commonly used to model the correlations at each node between different time steps. By combining the spatial and temporal features, they are able to update the state of each node.</p><p>However, those spatio-temporal graphs do not explicitly reflect the correlations between different nodes at different time steps (e.g., the red dash lines in Figure <ref type="figure">1(b)</ref>). In such a graph, the information of spatial and temporal neighborhoods is captured through the spatial and temporal connections respectively, while the information of neighboring nodes across both spatial and temporal dimensions are not considered, which may restrict the learning ability of graph neural networks. For example, a traffic jam occurred at an intersection may affect not only current nearby roads (spatial neighborhoods) and its local future traffic condition (temporal neighborhoods), but also the downstream roads in next few hours (spatio-temporal neighborhoods). Thus, we argue that it is necessary to model the comprehensive correlations in the spatio-temporal data.</p><p>Another limitation of previous works is that they ignore the dynamic correlations among nodes at different time steps, as shown in Figure <ref type="figure">1(c</ref>). The road network distances among sensors (nodes) are commonly used to define the spatial graph <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. This pre-defined graph is usually static. Some researchers <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> propose to learn a dataadaptive adjacency matrix, which is also unchanged over time steps. However, the traffic data exhibits strong dynamic correlations in the spatial and temporal dimensions, those Fig. <ref type="figure">1</ref>. The comprehensive and dynamic connections among nodes in graph-structured spatio-temporal data. There are three common scenarios: (a) Spatio-Temporal Graph: The node 2 at time step t can be influenced by nodes 1 and 3 at time step t through spatial connections, and node 2 at time step t -1 through the temporal connection. (b) Pre-defined Spatio-Temporal Joint Graph: The node 2 at time step t may also be affected by nodes 1 and 3 at time step t -1 through spatio-temporal connections. (c) Adaptive Spatio-Temporal Joint Graph: Compared with time step t -1, the connections among nodes 1, 2 and 3 exhibit strong dynamic characteristics at the time step t. For instance, the connection between nodes 1 and 3 gets weakened, while the connection between nodes 2 and 3 becomes stronger. Both (b) and (c) scenarios have not been comprehensively explored in existing studies.</p><p>static graphs are unable to reflect the dynamic characteristics of correlations among nodes. For example, the residence region is highly correlated to the office area during workday morning rush hours, while the correlation would be relatively weakened in the evening because some people might prefer to dining out before going home. Thus, it is crucial to model the dynamic spatio-temporal correlations for traffic forecasting.</p><p>This paper addresses these limitations from the following perspectives. First, besides the spatial and temporal connections, we further add the spatio-temporal connections between two time steps according to the spatio-temporal distances to define the spatio-temporal joint graph (STJG). In this way, the pre-defined STJG preserves comprehensive spatio-temporal correlations between any two time steps. Second, in order to adapt to the dynamic correlations among nodes, we suggest to explore an adaptive STJG, which is time-variant by encoding the time features. The adjacency matrix in this adaptive STJG is dynamic, changing over time steps. By constructing both the pre-defined and adaptive STJGs, we are able to preserve comprehensive and dynamic spatio-temporal correlations.</p><p>On these basis, we then develop the spatio-temporal joint graph convolution (STJGC) operations on both pre-defined and adaptive STJGs to simultaneously capture the spatiotemporal dependencies in a unified operation. We further design the dilated causal STJGC layers to extract multiple spatio-temporal ranges of information. Next, a multi-range attention mechanism is proposed to aggregate the information of different ranges. Finally, we apply independent fully-connected layers to produce the multi-step ahead prediction results. The whole framework is named as spatiotemporal joint graph convolutional networks (STJGCN), which can be learned end-to-end. To evaluate the efficiency and effectiveness of STJGCN, we conduct extensive experiments on five public traffic datasets. The experimental results demonstrate that our STJGCN is computationally efficient and achieves the best performance against 11 state-of-the-art baseline methods. Our main contributions are summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We construct both pre-defined and adaptive spatio-temporal joint graphs (STJGs), which reflect comprehensive and dynamic spatio-temporal correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We design dilated causal spatio-temporal joint graph convolution layers on both types of STJG to model multiple ranges of spatio-temporal correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a multi-range attention mechanism to aggregate the information of different ranges.</p><p>• We evaluate our model on five public traffic datasets, and experimental results demonstrate that STJGCN has high computation efficiency and outperforms 11 state-of-the-art baseline methods.</p><p>The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents the preliminary of this work. Section 4 details the method of STJGCN. Section 5 compares STJGCN with state-of-the-art methods on five datasets. Finally, section 6 concludes this paper and draws future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Networks</head><p>Graph convolutional networks (GCNs) are successfully applied on various tasks (e.g., node classification <ref type="bibr" target="#b12">[13]</ref>, link prediction <ref type="bibr" target="#b13">[14]</ref>) due to their superior abilities of handling graph-structured data <ref type="bibr" target="#b11">[12]</ref>. There are mainly two types of GCN <ref type="bibr" target="#b14">[15]</ref>: spatial GCN and spectral GCN. The spatial GCN performs convolution filters on neighborhoods of each node. Researchers in <ref type="bibr" target="#b15">[16]</ref> propose a heuristic linear method for neighborhood selecting. GraphSAGE <ref type="bibr" target="#b16">[17]</ref> samples a fixed number of neighbors for each node and aggregates their features. GAT <ref type="bibr" target="#b17">[18]</ref> learns the weights among nodes via attention mechanisms. Researchers in <ref type="bibr" target="#b18">[19]</ref> improve graph neural network architecture by exploiting correlation structure in the regression residuals. The spectral GCN defines the convolution in the spectral domain <ref type="bibr" target="#b19">[20]</ref>, which is firstly introduced in <ref type="bibr" target="#b20">[21]</ref>. ChebNet <ref type="bibr" target="#b21">[22]</ref> reduces the computational complexity with fast localized convolution filters. In <ref type="bibr" target="#b12">[13]</ref>, researchers further simplify the ChebNet to a simpler form and achieve state-of-the-art performances on various tasks. Recently, a range of studies apply the GCN on time-series data and construct spatio-temporal graphs for traffic forecasting <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, human action recognition <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatio-Temporal Forecasting</head><p>Spatio-temporal forecasting is an important research topic, which has been extensively studied for decades <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Recurrent neural networks (RNNs), especially the long short-term memory (LSTM) and gated recurrent unit (GRU) are successfully applied for modeling temporal correlations <ref type="bibr" target="#b30">[31]</ref>. To capture the spatial dependencies, convolutional neural networks (CNNs) are introduced, which are restricted to process regular grid structures <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Recently, researchers apply graph neural networks to model the non-Euclidean spatial correlations <ref type="bibr" target="#b36">[37]</ref>. DCRNN <ref type="bibr" target="#b3">[4]</ref> employs diffusion convolution to capture the spatial dependency and applies GRU to model the temporal dependency. STGCN <ref type="bibr" target="#b5">[6]</ref> uses graph convolution and 1D convolution to model the spatial and temporal dependencies, respectively. Researchers in <ref type="bibr" target="#b37">[38]</ref> study the effect of the order of spatial layers and temporal layers on STGCN model performance. Several works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> introduce the attention mechanisms <ref type="bibr">[41]</ref> into the spatiotemporal graph modeling to improve the prediction accuracy. AGSTN <ref type="bibr">[42]</ref> proposes an attention adjustment mechanism to realize fluctuation modulation for learning timeevolving spatio-temporal correlation. Some studies consider more kinds of connections (e.g., semantic connection <ref type="bibr">[43]</ref>, edge interaction patterns <ref type="bibr">[44]</ref>) to construct the spatial graph. The adjacency matrices in these models are usually predefined according to some prior knowledge (e.g., distances among nodes). Some researchers <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> argue that the pre-defined adjacency matrix does not necessarily reflect the underlying dependencies among nodes, and propose to learn an adaptive adjacency matrix for graph modeling. However, both the pre-defined and adaptive adjacency matrices assume static correlations among nodes, which cannot adapt to the evolving systems (e.g., traffic networks). Moreover, these graph-based methods do not explicitly model the correlations between different nodes at different time steps, which may restrict the learning ability of graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>Problem definition. Suppose there are N sensors (nodes) on a road network, and each sensor records C traffic measurements (e.g., volume, speed) at each time step. Thus, the traffic conditions at time step t can be represented as X t ∈ R N ×C . The traffic forecasting problem aims to learn a function f that maps the traffic conditions of historical P time steps to next Q time steps:</p><formula xml:id="formula_0">[X t-P +1 , X t-P +2 , • • • , X t ] f -→ [X t+1 , X t+2 , • • • , X t+Q ].</formula><p>(1) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">STJG Construction Module</head><p>In this module, we first pre-define the spatio-temporal joint graph (STJG) according to the spatio-temporal distances among nodes. While, the pre-defined graph may not reflect the underlying correlations among nodes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we further propose to learn adaptive STJG. By constructing both types of STJG, we are able to represent comprehensive and dynamic spatio-temporal correlations among nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pre-defined Spatio-Temporal Joint Graph</head><p>Previous studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> for traffic forecasting on graphs usually define the spatial adjacency matrix based on pairwise road network distances:</p><formula xml:id="formula_1">A i,j = exp(- dist(v i , v j ) 2 σ 2 ),<label>(2)</label></formula><p>where dist(v i , v j ) represents the road network distance from node v i to node v j , σ is the standard deviation of distances, and A i,j denotes the edge weight between node v i and node v j . They construct the spatial graph at each time step, and then connect each node with itself between adjacent time steps to define the spatio-temporal graph. In such a graph, the connections between different nodes at different time steps are not incorporated, which may restrict its representation ability.</p><p>We propose to construct a spatio-temporal joint graph (STJG), which preserves comprehensive spatio-temporal correlations. The intuitive idea is to further connect different nodes between two time steps, as shown in Figure <ref type="figure">1(b)</ref>. Thus, we modify Equation 2 to be the STJG adjacency matrix, as:</p><formula xml:id="formula_2">A i,t-k;j,t = exp(- ((k + 1) • dist(v i , v j )) 2 σ 2 ),<label>(3)</label></formula><p>where k is the time difference between two time steps. A i,t-k;j,t defines the edge weight between node v i at time step t -k and node v j at time step t, which decreases with the increase of spatio-temporal distance. When k = 0, Equation 3 degenerates to Equation 2, which represents the spatial connections. If i = j, the STJG adjacency matrix defines the temporal connections at each node between two time steps. Otherwise, it represents the spatiotemporal connections between different nodes at different time steps. Thus, we are able to define a comprehensive spatio-temporal graph according to Equation <ref type="formula" target="#formula_2">3</ref>. Note that the STJG could be constructed between any two time steps, which makes it flexible to reveal multiple time-ranges of spatio-temporal correlations.</p><p>We filter the values smaller than a threshold δ pdf in the STJG adjacency matrix to eliminate weak connections and control the sparsity. As this adjacency matrix is conditioned on the time difference k, but irrelevant to a specific time step, we denote it as A (k) ∈ R N ×N in following discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Adaptive Spatio-Temporal Joint Graph</head><p>Previous studies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> demonstrate that the pre-defined adjacency matrix may not reflect the underlying correlations among nodes, and propose adaptive ones. However, they only define the spatial graph, and it is unchanged over time steps. We propose to learn adaptive STJG adjacency matrices that could represent comprehensive and dynamic spatiotemporal correlations based on the latent space modeling algorithm <ref type="bibr">[45]</ref>.</p><p>4.2.2.1 Latent space modeling: Given a graph, we assume each node resides in a latent space with various attributes. The attributes of nodes and how these attributes interact with each other jointly determine the underlying relations among nodes. The nodes which are close to each other in the latent space are more likely to form a link. Mathematically, we aim to learn two matrices U and B. Here, U ∈ R N ×d denotes the d latent attributes of the N nodes, and B ∈ R d×d represents the attributes interaction patterns, which could be an asymmetric matrix for directed graph or symmetric matrix for undirected graph. The product of U BU ⊤ could represent the connections among nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.2">Spatio-temporal embedding:</head><p>We propose a spatio-temporal embedding to form the latent node attributes. We first randomly initialize a spatial embedding for each of the N nodes, and then transform it to d dimensions via fully-connected layers. To obtain time-varying node attributes and take periodic patterns in historical input data (i.e., morning rush hour) into account, we further encode the time information as the temporal embedding. At each time step, we consider two time features, i.e., time-of-day and day-of-week, which are encoded by one-hot coding and then be projected to d dimensions using fully-connected layers. We then add the spatial and temporal embeddings together to generate the spatio-temporal embedding at each time step t, represented as U t ∈ R N ×d , which can be updated during the training stage. The spatio-temporal embedding encodes both the node-specific and time-varying information, and it could mine periodic spatio-temporal patterns of historical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.3">Adaptive STJG adjacency matrix:</head><p>Based on the spatio-temporal embedding, we define the STJG adjacency matrix at time step t according to the latent space modeling algorithm, as:</p><formula xml:id="formula_3">Lt = sof tmax(ψ(U t BU ⊤ t )),<label>(4)</label></formula><p>with</p><formula xml:id="formula_4">ψ(x) = x, if x ≥ δ adt 0, otherwise ,<label>(5)</label></formula><p>where U t ∈ R N ×d is the spatio-temporal embedding of N nodes at time step t, ψ(x) is used to eliminate the weights smaller than a threshold δ adt , and the softmax function is applied for normalization. Lt ∈ R N ×N defines the spatial connections among N nodes at time step t, which is dynamic, changing over time steps. In order to construct the connections between different time steps, we modify Equation 4 as:</p><formula xml:id="formula_5">Lt-k;t = sof tmax(ψ(U t-k BU ⊤ t )),<label>(6)</label></formula><p>where Lt-k;t ∈ R N ×N is the normalized STJG adjacency matrix between time steps t -k and t. When k = 0, Equation 6 degenerates to Equation <ref type="formula" target="#formula_3">4</ref>, which describes the spatial graph at time step t. Thus, Equation 6 is able to define the spatio-temporal joint graph between time steps t -k and t with comprehensive and dynamic spatio-temporal connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dilated Causal STJGC Module</head><p>The standard graph convolution performs on spatial graphs to model spatial correlations only, we thus propose the spatio-temporal joint graph convolution (STJGC) on both types of STJG to model spatio-temporal correlations in a unified operation. We further design dilated causal STJGC layers to capture multiple ranges of spatio-temporal dependencies, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. In the following discussion, we first describe the STJGC operation in section 4.3.1, and then introduce the dilated causal STJGC layers in section 4.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Spatio-Temporal Joint Graph Convolution (STJGC)</head><p>Graph convolution is an effective operation for learning node information from spatial neighborhoods according to the graph structure, while the standard graph convolution performs on the spatial graph to model the spatial correlations only. In order to model the comprehensive and dynamic spatio-temporal correlations on the STJG, we propose the spatio-temporal joint graph convolution (STJGC) operations on both types of STJG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.1">Graph Convolution:</head><p>The graph convolution is defined as <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_6">Z = ϕ( ÃXW + b).<label>(7)</label></formula><p>Here, X ∈ R N ×d1 and Z ∈ R N ×d2 denote the input and output graph signals, W ∈ R d1×d2 and b ∈ R d2 are learnable parameters, ϕ(•) is an activation function (e.g., ReLU <ref type="bibr">[46]</ref>),</p><formula xml:id="formula_7">Ã = D -1/2 AD -1/2 ∈ R N ×N</formula><p>is the normalized adjacency matrix, where A is the adjacency matrix with self-loops, and D = j A i,j is the degree matrix. 4.3.1.2 STJGC on pre-defined STJG: Consider the STJG between time steps t -k and t, the information of each node at time step t comes from its spatial, temporal, and spatio-temporal neighborhoods:</p><formula xml:id="formula_8">Z pdf t = ϕ( Ã(k) X t-k W pdf 1 + Ã(0) X t W pdf 2 + b pdf ),<label>(8)</label></formula><p>where Ã(k) is the normalized pre-defined STJG adjacency matrix between time steps t -k and t (see Equation <ref type="formula" target="#formula_2">3</ref>). In Equation <ref type="formula" target="#formula_8">8</ref>, Ã(k) X t-k W pdf 1 means we aggregate neighborhoods (both temporal and spatio-temporal) information from time step t -k, and Ã(0) X t W pdf 2 means we aggregate the information from spatial neighborhoods at time step t. Thus, by performing Equation <ref type="formula" target="#formula_8">8</ref>, we are able to model comprehensive spatio-temporal correlations between two time steps.</p><p>Furthermore, at time step t, we propose to incorporate K (denoted as kernel size) time step information (e.g., t, t -1, • • • , t -K + 1) to update the node features. Specifically, we modify Equation 8 as:</p><formula xml:id="formula_9">Z pdf t = K-1 k=0 ϕ( Ã(k) X t-k W pdf k + b pdf ).<label>(9)</label></formula><p>In the case of a directed graph, we consider two directions of information propagation (i.e., forward and backward), corresponding to two normalized adjacency matrices:</p><formula xml:id="formula_10">Ã(k) f w = D (k) O -1/2 A (k) D (k) O -1/2 and Ã(k) bw = D (k) I -1/2 A (k) ⊤ D (k) I -1/2</formula><p>, where D</p><formula xml:id="formula_11">(k) O = j A (k) i,j<label>and</label></formula><formula xml:id="formula_12">D (k) I = i A (k)</formula><p>i,j represent the out-degree and in-degree matrices, respectively. Thus, we transform Equation <ref type="formula" target="#formula_9">9</ref>to:</p><formula xml:id="formula_13">Z pdf t = K-1 k=0 ϕ( Ã(k) f w X t-k W pdf k,1 + Ã(k) bw X t-k W pdf k,2 + b pdf ),<label>(10)</label></formula><p>where X t-k ∈ R N ×d and X t ∈ R N ×d are the input graph signals at time steps t-k and t respectively, Z pdf t denotes the updated feature at time step t, W pdf k,1 ∈ R d×d , W pdf k,2 ∈ R d×d , and b pdf ∈ R d are learnable parameters.</p><p>By this design, our STJGC simultaneously models the information propagation from three kinds of connections (i.e., spatial, temporal, and spatio-temporal) in a unified operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.3">STJGC on adaptive STJG:</head><p>As the predefined STJG may not reflect the underlying correlations among nodes, we further propose STJGC on adaptive STJG. The computation is similar as that on pre-defined STJG:</p><formula xml:id="formula_14">Z adt t = K-1 k=0 ϕ( Lt-k;t X t-k W adt k + b adt ),<label>(11)</label></formula><p>where Lt-k;t is the normalized adaptive STJG adjacency matrix between time steps t-k and t (defined in Equation <ref type="formula" target="#formula_5">6</ref>).</p><p>Inspired by the bi-directional RNN <ref type="bibr">[47]</ref>, we consider both time directions of the information flow. Specifically, we compute two adaptive STJG adjacency matrices: Lt-k;t and Lt;t-k , and modify Equation 11 accordingly, as:</p><formula xml:id="formula_15">Z adt t = K-1 k=0 ϕ( Lt-k;t X t-k W adt k,1 + Lt;t-k X t-k W adt k,2 + b adt ),<label>(12)</label></formula><p>where Z adt t is the updated feature at time step t, which encodes the comprehensive and dynamic spatio-temporal correlations, W adt k,1 ∈ R d×d , W adt k,2 ∈ R d×d , and b pdf ∈ R d are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.4">Gating fusion:</head><p>The pre-defined and adaptive STJGs represent the spatio-temporal correlations from distinct perspectives. To enhance the representation ability, we use a gating mechanism to fuse the features extracted on two types of STJG. Specifically, we define a gate to control the importance of two features as:</p><formula xml:id="formula_16">G = sigmoid(W g [Z pdf t , Z adt t ] + b g ),<label>(13)</label></formula><p>where [•, •] denotes the concatenation operation, the sigmoid function is used to control the output lies in range [0, 1], W g ∈ R 2d×d and b g ∈ R d are learnable parameters. The gate G ∈ R N ×d controls the information flow between predefined and adaptive STJGs in both node-wise and channelwise. Based on the gate, we fuse two features as:</p><formula xml:id="formula_17">Z t = G ⊙ Z pdf t + (1 -G) ⊙ Z adt t ,<label>(14)</label></formula><p>where ⊙ denotes the element-wise product. As a result, Z t ∈ R N ×d represents the updated representation of N nodes at time step t, which aggregates the information from their spatial, temporal, and spatio-temporal neighborhoods on both types of STJG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Dilated Causal STJGC Layers</head><p>The STJGC operation is able to model the correlations in different time ranges by controlling the time difference k.</p><p>In addition, different STJGC layers aggregate information within diverse neighborhood ranges. This makes it flexible to model the spatio-temporal correlations in multiple neighborhood and time ranges. The information in different ranges reveals distinct traffic properties. A small range uncovers the local dependency and a large range indicates the global dependency. Inspired by the dilated causal convolution <ref type="bibr">[48]</ref>, <ref type="bibr">[49]</ref>, which is able to capture diverse time-ranges of dependencies in different layers, we propose dilated causal STJGC layers to capture multiple ranges of spatio-temporal dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.1">Dilated causal convolution:</head><p>The dilated causal convolution operation slides over the input sequence by skipping elements with a certain time step (i.e., dilation factor γ), and it involves only historical information at each time step to satisfy the causal constraint. In this way, it models diverse time-ranges of dependencies in different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.2">Dilated causal STJGC:</head><p>As illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, we first transform the inputs into d dimension space using fully-connected layers. Then we stack a couple of STJGC layers upon it in the dilated causal way. Different to the standard dilated causal convolution using 1D CNN, we use the STJGC in each layer to model the dynamic and comprehensive spatio-temporal correlations. Suppose the length of input graph signals is P = 12, we could stack four STJGC layers with kernel size K = 2 and dilation factor γ = {2, 4, 4, 4} in each layer, respectively. The residual connections <ref type="bibr">[50]</ref> are also applied in each STJGC layer at the corresponding output time steps. The number of STJGC layers, dilation factors and kernel size could be re-designed according to the length of input graph signals, in order to ensure that the output of the last STJGC layer covers the information from all input time steps.</p><p>In these dilated causal STJGC layers, each STJGC layer captures different ranges of spatio-temporal dependencies. For example, as shown in Figure <ref type="figure" target="#fig_1">3</ref>, in the first STJGC layer, the hidden state at time step t aggregates information from 1-hop neighborhoods at time steps t -1 and t. With the layer goes deeper, it could extract features from higher order neighborhoods at longer time-ranges. In particular, in the last STJGC layer, each node at time step t captures the information within 4-hop neighborhoods from total P time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction Module</head><p>In this module, we first propose a multi-range attention mechanism to aggregate the information of different ranges extracted by the dilated causal STJGC layers, and then apply independent fully-connected layers to produce the multistep ahead prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Multi-Range Attention</head><p>As introduced in section 4.3.2, each STJGC layer captures different spatio-temporal ranges of dependencies. A small range uncovers the local dependency and a large range indicates the global dependency, e.g., the correlations between distant nodes at distant time steps. Thus, It is essential to combine the multi-range information. In addition, the importance of different ranges could be diverse. We propose a multi-range attention mechanism to aggregate the information of different ranges. Mathematically, we denote the hidden state of node v i at time step t in m-th STJGC layer as z (m) i ∈ R d , the attention score is computed as:</p><formula xml:id="formula_18">s m i = v ⊤ tanh(W a z (m) i + b a ),<label>(15)</label></formula><formula xml:id="formula_19">α m i = exp(s m i ) M m=1 exp(s m i ) ,<label>(16)</label></formula><p>where . Based on the attention scores, the multi-range information can be aggregated as:</p><formula xml:id="formula_20">W a ∈ R d×d , b a ∈ R d ,</formula><formula xml:id="formula_21">y i = M m=1 α m i z (m) i ,<label>(17)</label></formula><p>where y i is the updated feature of node v i , which aggregates the information from multiple spatio-temporal ranges. The attention mechanism is conducted on all of the N nodes in parallel with shared learnable parameters, and produces an output as Y ∈ R N ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Independent Fully-Connected Layers</head><p>As the traffic of different time steps may exhibit different properties, it would be better to use different networks to generate the predictions at different forecasting horizons. We thus apply Q independent two-fully-connected layers upon Y to produce the Q time steps ahead prediction results:</p><formula xml:id="formula_22">Xt+i = ϕ(Y W i 1 + b i 1 )W i 2 + b i 2 ,<label>(18)</label></formula><p>where Xt+i denotes the prediction result of time step t + i</p><formula xml:id="formula_23">(i = 1, 2, • • • , Q), W i 1 ∈ R d×d , b i 1 ∈ R d , W i 2 ∈ R d×1</formula><p>, and b i 2 ∈ R are the corresponding learnable parameters, ϕ(•) is an activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Loss Function</head><p>The mean absolute error (MAE) loss is commonly used in the traffic forecasting problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In practice, the MAE loss optimizes all prediction values equally regardless of the value size, which leads to relatively non-ideal predictions for small values compared to the predictions of large values. The mean absolute percentage error (MAPE) loss is more relevant to the predictions of small values. Thus, we propose to combine the MAE loss and MAPE loss as our loss function:</p><formula xml:id="formula_24">L( Xt+i ; Θ) = 1 Q ( Q i=1 (| Xt+i -X t+i |+β• | Xt+i -X t+i | X t+i •100)),<label>(19)</label></formula><p>where β is used to balance MAE loss and MAPE loss, Θ denotes all learnable parameters in STJGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Complexity Analysis</head><p>We further analyze the time complexity of the main components in each module in our STJGCN.</p><p>In the STJG construction module, the computation mainly comes from the learning of adaptive STJG adjacency matrix (Equation <ref type="formula" target="#formula_5">6</ref>). The time complexity is O(N d 2 + N 2 d), where N denotes the number of nodes, d is the dimension of the spatio-temporal embedding. Regarding d as a constant, the time complexity turns to O(N 2 ), which is attributed to the pairwise computation of the N nodes' embeddings. One concern is that the large-scaled node would result in a more expensive cost. To mitigate the scale problem, we suggest to only calculate the connected edges in adaptive STJG adjacency matrix according to a priori knowledge (i.e., pre-defined STJG).</p><p>In the dilated casual STJGC module, the time complexity mainly depends on the computation of each STJGC operation (Equations 10 and 12), which incurs O(K(|E|d + N d 2 )) time complexity. Here, K is the kernel size, |E| denotes the number of edges in the graph, and d is the dimension of hidden states. The time complexity of STJGC mainly depends on |E|, as each node aggregates information from its neighborhoods, whose number is equal to the edge number.</p><p>In the prediction module, the time complexities of multirange attention mechanism (Equations 15, 16, and 17) and independent fully-connected layers (Equation <ref type="formula" target="#formula_22">18</ref>) are O(N (M d + d 2 )) and O(QN d 2 ), respectively. Thus, the total time complexity of the prediction module is O(N (M d + Qd 2 )), where M is the number of STJGC layers and Q is the number of time steps to be predicted. The time complexity is highly related to Q, as we use Q independent fully-connected layers to produce the multi-step prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our STJGCN on five highway traffic datasets: PeMSD3, PeMSD4, PeMSD7, PeMSD8 and Seattle-Loop. The previous four datasets are released in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These datasets are collected by the Caltrans Performance Measurement System (PeMS) from 4 districts in real time every 30 seconds. The raw traffic data is aggregated into 5-minute time interval. There are three kinds of traffic measurements in PeMSD4 and PeMSD8 datasets, including total flow, average speed, and average occupancy. In PeMSD3 and PeMSD7 datasets, only the traffic flow is recorded. Seattle-Loop is released in <ref type="bibr">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, which is a highway speed dataset collected from 323 loop detectors in the Greater Seattle Area. The dataset contains 5-minute resolution traffic speed data for the entirety of 2015. Following previous studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b52">[53]</ref>, we predict the traffic flow in first four datasets, and traffic speed in last dataset. The summary statistics of five datasets are presented in Table <ref type="table" target="#tab_2">1</ref>.</p><p>All datasets are normalized using the Z-Score method, and be split in chronological order with 60% for training, 20% for validation, and 20% for testing. The pair-wise road network distances are provided in the datasets, and we use them to construct the pre-defined STJG according to Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation Metrics</head><p>We adopt three widely used metrics for evaluation, i.e., mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE), which are defined as: </p><formula xml:id="formula_25">M AE = 1 N Q N i=1 Q j=1 | Xi,t+j -X i,t+j |, (<label>20</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">RM SE = 1 N Q N i=1 Q j=1 ( Xi,t+j -X i,t+j ) 2 , (<label>21</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">M AP E = 1 N Q N i=1 Q j=1 | Xi,t+j -X i,t+j | X i,t+j ,<label>(22)</label></formula><p>where Xi,t+j and X i,t+j denote the prediction result and ground truth of node v i at time step t + j, respectively, N is the number of nodes, and Q is the number of time steps to be predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experimental Settings</head><p>The PeMSD3 and PeMSD7 datasets contain one traffic measurement (i.e., traffic flow). Thus, the dimensions of the input and output are C = 1 and 1, respectively. The PeMSD4 and PeMSD8 datasets contain three traffic measurements (i.e., traffic flow, average speed, and average occupancy), and only the traffic flow is predicted in the experiments <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Thus, the dimensions of the input and output are C = 3 and 1, respectively. The Seattle-Loop dataset contains one traffic measurement (i.e., traffic speed). Thus, the dimensions of the input and output are C = 1 and 1. Following previous studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b52">[53]</ref>, we use the traffic data of historical 12 time steps (P = 12) to forecast the next 12 time steps (Q = 12). The core hyperparameters in STJGCN include the thresholds δ pdf and δ adt in pre-defined and adaptive STJG adjacency matrices respectively, the dimension d of hidden states, the kernel size K of each STJGC layer, and the threshold β in the loss function. We tune these hyperparameters on the validation set that achieve the best validation performance. We provide a parameter study in section 5.3.3. The detailed hyperparameter settings of STJGCN on five datasets are presented in Table <ref type="table" target="#tab_3">2</ref>.</p><p>The nonlinear activation function ϕ(•) in our STJGCN refers to the ReLU activation <ref type="bibr">[46]</ref>, and we also add a Batch Normalization <ref type="bibr" target="#b53">[54]</ref> layer before each ReLU activation function.</p><p>We train our model using the Adam optimizer <ref type="bibr" target="#b54">[55]</ref> with an initial learning rate 0.001 and batch size 64 on a NVIDIA Tesla V100 GPU card. We run the experiments for 200 epochs and save the best model that evaluated on the validation set. We run each experiment 5 times, and report the mean errors and standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Baseline Methods</head><p>We compare STJGCN with 11 baseline methods, which could be divided into two categories. The first category is the time-series prediction models, including:</p><p>• VAR <ref type="bibr" target="#b55">[56]</ref>: Vector Auto-Regressive is a traditional time-series model, which can capture pairwise relationships among all traffic series.</p><p>• FC-LSTM <ref type="bibr" target="#b56">[57]</ref>: an encoder-decoder framework using long short-term memory (LSTM) with peephole for multi-step time-series prediction.</p><p>• SVR <ref type="bibr" target="#b57">[58]</ref>: Support Vector Regression utilizes a linear support vector machine to perform regression.</p><p>The second category refers to the spatio-temporal graph neural networks, which are detailed as follows:</p><p>• DCRNN <ref type="bibr" target="#b3">[4]</ref>: Diffusion Convolutional Recurrent Neural Network, which models the traffic as a diffusion process, and integrates diffusion convolution with recurrent neural network (RNN) into the encoder-decoder architecture.</p><p>• STGCN <ref type="bibr" target="#b5">[6]</ref>: Spatio-Temporal Graph Convolutional Network, which employs graph convolutional network (GCN) to capture spatial dependencies and 1D convolutional neural network (CNN) for temporal correlations modeling.</p><p>• ASTGCN <ref type="bibr" target="#b7">[8]</ref>: Attention based Spatio-Temporal Graph Convolutional Network that designs spatial and temporal attention mechanisms to capture spatial and temporal patterns, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Graph WaveNet <ref type="bibr" target="#b6">[7]</ref>: a graph neural network that performs diffusion convolution with both pre-defined and self-adaptive adjacency matrices to capture spatial dependencies, and applies 1D dilated causal convolution to capture temporal dependencies.</p><p>• STSGCN <ref type="bibr" target="#b8">[9]</ref>: Spatio-Temporal Synchronous Graph Convolutional Network that designs spatio-temporal synchronous modeling mechanism to capture localized spatio-temporal correlations.</p><p>• AGCRN <ref type="bibr" target="#b9">[10]</ref>: Adaptive Graph Convolutional Recurrent Network that learns data-adaptive adjacency matrix for graph convolution to model spatial correlations and uses gated recurrent unit (GRU) to model temporal correlations.</p><p>• GMAN <ref type="bibr" target="#b38">[39]</ref>: Graph Multi-Attention Network is an encoder-decoder framework, which designs multiple spatial and temporal attention mechanisms in the encoder and decoder to model spatio-temporal correlations, and a transform attention mechanism to transform information from encoder to decoder.</p><p>• Z-GCNETs <ref type="bibr" target="#b10">[11]</ref>: Time Zigzags at Graph Convolutional Networks that introduce the concept of zigzag persistence <ref type="bibr" target="#b58">[59]</ref> into the graph convolutional networks for modeling the spatial correlations and use the GRU networks to capture the temporal dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Overall Comparison</head><p>Table <ref type="table" target="#tab_4">3</ref> presents the forecasting performance comparison of our STJGCN with 11 baseline methods. We observe that:</p><p>(1) the time-series prediction models, including traditional approach (i.e., VAR), machine learning based method (i.e., SVR), and deep neural network (i.e., FC-LSTM) perform poorly as they only consider the temporal correlations. (2) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ablation Study</head><p>To better understand the effectiveness of different components in STJGCN, we conduct ablation studies on PeMSD4 and PeMSD8 datasets. 5.3.2.1 Effect of spatio-temporal connections: One difference between our STJG with normal spatio-temporal graph is that we explicitly add the spatio-temporal connections between different nodes at different time steps. To evaluate the effectiveness of this approach, we drop them separately/simultaneously from the pre-defined or/and adaptive STJG. These three variants of STJGCN are named as "w/o STC-pdf" (drop in pre-defined STJG), "w/o STCadt" (drop in adaptive STJG), and "w/o STC" (drop in both types of STJG), respectively. The results in Table <ref type="table" target="#tab_5">4</ref> demonstrate that the introduction of spatio-temporal connections improves the performance as it helps the model to explicitly capture comprehensive spatio-temporal correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.2">Effect of dynamic graph modeling:</head><p>To evaluate the effect of dynamic graph modeling, we conduct experiments of learning static adjacency matrices. Specifically, we design a variant of STJGCN (i.e., "w/o dgm") that only uses the node embedding to generate the adaptive STJG adjacency matrix without using the time feature. The results in Table <ref type="table" target="#tab_5">4</ref> validate the effectiveness of modeling dynamic correlations among nodes at different time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.3">Effect of multi-range information:</head><p>To verify the effect of multi-range information, we design a variant of STJGCN, namely "w/o mr", in which we do not combine multiple ranges of information but directly use the output of the last STJGC layer to produce the predictions. The results in Table <ref type="table" target="#tab_5">4</ref> indicate the necessity of leveraging multirange information. We further design a variant "w/o att" that directly adds the outputs of each STJGC layer together without using the multi-range attention mechanism, and it performs worse than STJGCN, showing that it is beneficial to distinguish the importance of different ranges of information.</p><p>5.3.2.4 Effect of independent fully-connected layers: In the prediction module, we use Q independent fullyconnected layers to produce the multi-step predictions. To evaluate the effectiveness of this, we conduct experiments of using shared fully-connected layers with Q units in the output layer to produce the Q time steps predictions. We name this variant of STJGCN as "w/o idp", and present the experimental results in Table <ref type="table" target="#tab_5">4</ref>. We observe that STJGCN improves the performances by introducing independent learning parameters for multi-step prediction. A potential reason is that the traffic of different time steps may exhibit different properties, and using different networks to generate the predictions at different forecasting horizons could be beneficial.</p><p>5.3.2.5 Effect of different STJG adjacency matrix configurations: We further conduct experiments of using different STJG adjacency matrix configurations to evaluate their effectiveness. As shown in Table <ref type="table" target="#tab_6">5</ref>, the models with only pre-defined STJG adjacency matrices (lines 3-4) achieve poor performances as they do not capture the underlying dependencies in the data. We observe that the models with only adaptive STJG adjacency matrices (lines 5-6) could realize promising performances, which indicates that our model can also be used even if the graph structure is unavailable. By using both pre-defined and adaptive STJG adjacency matrices (line 7), we could achieve better results. We further apply a gating fusion approach (section 4.3.1.4) in STJGCN (line 8) and observe consistent improvement of the predictive performances, as the gate is able to control the information flow between pre-defined and adaptive STJGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Parameter Study</head><p>We conduct a parameter study on five core hyperparameters in STJGCN on the PeMSD4 and PeMSD8 datasets, including the thresholds δ pdf and δ adt in the pre-defined and adaptive STJG adjacency matrices, respectively, the dimension d of hidden states, the kernel size K in the STJGC operation, and the threshold β in the loss function. We change the parameter under investigation and fix other parameters in each experiment. Figures <ref type="figure" target="#fig_2">4</ref> and<ref type="figure">5</ref> show the experimental results on the PeMSD4 and PeMSD8 datasets, respectively.   As shown in Figures <ref type="figure" target="#fig_2">4(a</ref>), 4(b), 5(a), and 5(b), the performance is not strongly sensitive to the sparsity of the STJG adjacency matrices, which we think is because the adaptive STJG adjacency matrix could adjust itself for aggregating the neighboring information during the training stage. While, in general, a more sparse adjacency matrix is beneficial to select the most related nodes for each node, and leads to better results. However, a too sparse graph may lose the connections between interrelated nodes, and thus degrades the performances. According to the validation loss, we set δ pdf = δ adt = 0.5 in the PeMSD4 dataset, and δ pdf = 0.5, δ adt = 0.3 in the PeMSD8 dataset.</p><p>As shown in Figures <ref type="figure" target="#fig_2">4(c</ref>) and 5(c), increasing the number of hidden units could enhance the model's expressive capacity. However, when it is larger than 64, the performance degrades significantly, as the model needs to learn more parameters and may suffer from the over-fitting problem.</p><p>Figures <ref type="figure" target="#fig_2">4(d</ref>) and 5(d) show that the model performs poorly when the kernel size equals to 1, as it captures only the spatial dependencies and does not consider the correlations in the temporal dimension. We can further observe that it is enough to aggregate the information from neighboring 2 or 3 time steps at each time step. When K = 4, the model's performance degrades. It is possibly because that a node's information at a time step may only correlated to the nodes at a limited number of neighboring time steps, and a large  K would introduce noises into the model. Thus, according to the validation loss, we set K = 3 and K = 2 on the PeMSD4 and PeMSD8 datasets, respectively.</p><p>In the parameter study of the threshold β in the loss function, we report the validation MAE, RMSE, and MAPE instead of reporting the loss value, as the size of β directly impacts the size of the loss value. As shown in Figures 4(e), 4(g), 5(e), and 5(g), a larger β means the model optimizes more on the MAPE loss and less on the MAE loss, and thus leads to smaller MAPE and larger MAE. The RMSE can also be influenced, as shown in Figures <ref type="figure" target="#fig_2">4(f</ref>) and 5(f). Through a comprehensive consideration of the validation MAE, RMSE, MAPE and their standard deviations, we choose to use β = 1.0 and β = 1.5 in the PeMSD4 and PeMSD8 datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Performance Comparison at Each Horizon</head><p>Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> present the forecasting performance comparison of our STJGCN with five representative baseline methods (i.e., Graph WaveNet, STSGCN, AGCRN, GMAN, and Z-GCNETs) at each prediction time step on the PeMSD4 and PeMSD8 datasets, respectively. We exclude other baseline methods due to their poorer performances, as shown in Table <ref type="table" target="#tab_4">3</ref>. We can observe that Graph WaveNet performs well in the short-term (one or two time steps ahead) prediction. However, its performance degrades quickly with the increase of the forecasting horizon. The performance of GMAN degrades slowly when the predictions are made further into the future, and it performs well in the longterm (e.g., 12 time steps ahead) prediction, while still worse than STJGCN. In general, our model achieves the best performances at almost all horizons in terms of all three metrics on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Model Size and Computation Time</head><p>We present the comparison of model size and computation time of our STJGCN with graph-based baseline methods in Table <ref type="table" target="#tab_8">6</ref>.</p><p>The results in four PeMS datasets demonstrate the high computation efficiency of our model. In terms of the model size, STJGCN has fewer parameters than most of the baseline models. In the training phase, our model runs faster than other methods except for STGCN. In the inference stage, STGCN runs very slowly as it adopts an iterative way to generate multi-step predictions, while STJGCN and Graph WaveNet are the most efficient. By further considering the prediction accuracy (see Table <ref type="table" target="#tab_4">3</ref>), our model shows superior ability in balancing predictive performances and time consumption as well as parameter settings.</p><p>The results in Seattle-Loop dataset show that out STJGCN compares favorably to baseline methods. In terms of the model size, STJGCN has not been affected by the larger amount of data, and still has fewer parameters than most of the baseline models. In the training phase, our STJGCN is faster than GMAN and Z-GCNETs. Other 6 baselines are more efficient than STJGCN but they show poor prediction performance (see Table <ref type="table" target="#tab_4">3</ref>). In the inference stage, STJGCN is only slower than STSGCN and AGCRN, while both of which have worse prediction accuracy than our model (see Table <ref type="table" target="#tab_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed STJGCN, which models comprehensive and dynamic spatio-temporal correlations and aggregates multiple ranges of information to forecast the traffic conditions over several time steps ahead on a road network. When evaluated on five public traffic datasets, STJGCN showed high computation efficiency and outperformed 11 state-ofthe-art baseline methods. Our model could be potentially applied to other spatio-temporal data forecasting tasks, such as air quality inference and taxi demand prediction. We plan to investigate this as future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FullyFig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of Spatio-Temporal Joint Graph Convolutional Networks (STJGCN). It consists of three modules: (i) the STJG construction module (detailed in section 4.2) constructs both pre-defined and adaptive spatio-temporal joint graphs (STJGs); (ii) the dilated causal STJGC module (detailed in section 4.3) stacks dilated causal spatio-temporal joint graph convolution (STJGC) layers to capture multiple ranges of spatiotemporal dependencies, where each STJGC layer performs convolution operation based on both types of STJG; (iii) the prediction module (detailed in section 4.4) aggregates the information of different ranges via a multi-range attention mechanism and produces the prediction results using fully-connected layers.</figDesc><graphic coords="4,240.12,43.92,131.48,153.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The illustration of the dilated causal STJGC module (middle part in the figure) and the prediction module (right part in the figure) in STJGCN.In the dilated csusal STJGC module, the inputs are first transformed by fully-connected layers and then be passed to the dilated causal STJGC layers, which pick inputs every γ (dilation factor, γ = {1, 2, 4, 4} for each STJGC layer in the figure) step and apply STJGC (left part in the figure) to the selected inputs. The prediction module first aggregates the outputs of each STJGC layer via the multi-range attention mechanism and then uses fully-connected layers to produce the prediction results.</figDesc><graphic coords="6,85.61,102.76,116.50,118.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Parameter study on the PeMSD4 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Fig. 5. Parameter study on the PeMSD8 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>4.1 Framework Overview</head><label></label><figDesc></figDesc><table><row><cell>methods generally ignore the spatio-temporal connections</cell></row><row><cell>and the dynamic correlations among nodes, we thus pro-</cell></row><row><cell>pose the spatio-temporal joint graph (STJG) construction module</cell></row><row><cell>to construct both pre-defined and adaptive STJGs, which</cell></row><row><cell>preserve comprehensive and dynamic spatio-temporal cor-</cell></row><row><cell>relations. Second, as the standard graph convolution opera-</cell></row><row><cell>tion models spatial correlations only, we propose the spatio-</cell></row><row><cell>temporal joint graph convolution (STJGC) operation on both</cell></row><row><cell>types of STJG to model the comprehensive and dynamic</cell></row><row><cell>spatio-temporal correlations in a unified operation. Based on</cell></row><row><cell>the STJGC, we further propose the dilated casual STJGC mod-</cell></row><row><cell>ule to capture spatio-temporal dependencies within multiple</cell></row><row><cell>neighborhood and time ranges. Finally, in the prediction</cell></row><row><cell>module, we propose a multi-range attention mechanism to</cell></row><row><cell>aggregate the information of different ranges, and apply</cell></row><row><cell>fully-connected layers to produce the prediction results. We</cell></row><row><cell>detail each module in the following subsections.</cell></row><row><cell>Figure 2 depicts the framework of our proposed Spatio-</cell></row><row><cell>Temporal Joint Graph Convolutional Networks (STJGCN),</cell></row><row><cell>which includes three modules. First, previous graph-based</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Summary statistics of five datasets.</figDesc><table><row><cell>Dataset</cell><cell>Time range</cell><cell cols="2">Time interval # Nodes</cell></row><row><cell>PeMSD3</cell><cell>1/Sep/2018 -30/Nov/2018</cell><cell>5-minute</cell><cell>358</cell></row><row><cell>PeMSD4</cell><cell>1/Jan/2018 -28/Feb/2018</cell><cell>5-minute</cell><cell>307</cell></row><row><cell>PeMSD7</cell><cell>1/May/2017 -31/Aug/2017</cell><cell>5-minute</cell><cell>883</cell></row><row><cell>PeMSD8</cell><cell>1/Jul/2016 -31/Aug/2016</cell><cell>5-minute</cell><cell>170</cell></row><row><cell>Seattle-Loop</cell><cell>1/Jan/2015 -31/Dec/2015</cell><cell>5-minute</cell><cell>323</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Hyperparameter settings of STJGCN on five datasets.</figDesc><table><row><cell>Dataset</cell><cell>δ pdf</cell><cell>δ adt</cell><cell>d</cell><cell>K</cell><cell>β</cell></row><row><cell>PeMSD3</cell><cell>0.5</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>0.1</cell></row><row><cell>PeMSD4</cell><cell>0.5</cell><cell>0.5</cell><cell>64</cell><cell>3</cell><cell>1.0</cell></row><row><cell>PeMSD7</cell><cell>0.9</cell><cell>0.7</cell><cell>64</cell><cell>2</cell><cell>0.5</cell></row><row><cell>PeMSD8</cell><cell>0.5</cell><cell>0.3</cell><cell>64</cell><cell>2</cell><cell>1.5</cell></row><row><cell>Seattle-Loop</cell><cell>0.5</cell><cell>0.3</cell><cell>64</cell><cell>2</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Forecasting performance comparison of different models on five datasets.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell>VAR</cell><cell>SVR</cell><cell>FC-LSTM</cell><cell>DCRNN</cell><cell>STGCN</cell><cell>ASTGCN</cell><cell>Graph WaveNet</cell><cell>STSGCN</cell><cell>AGCRN</cell><cell>GMAN</cell><cell>Z-GCNETs</cell><cell>STJGCN</cell></row><row><cell></cell><cell>MAE</cell><cell cols="6">19.72 19.77 19.56±0.32 17.62±0.13 19.76±0.67 18.67±0.42</cell><cell>15.67±0.06</cell><cell cols="3">15.74±0.09 16.10±0.16 15.52±0.09</cell><cell>15.90±0.77</cell><cell>14.92±0.10</cell></row><row><cell>PeMSD3</cell><cell>RMSE</cell><cell cols="6">32.38 32.78 33.38±0.46 29.86±0.47 33.87±1.18 30.71±1.02</cell><cell>26.42±0.14</cell><cell cols="3">26.39±0.36 28.55±0.28 26.53±0.19</cell><cell>27.90±0.86</cell><cell>25.70±0.41</cell></row><row><cell></cell><cell cols="7">MAPE (%) 20.50 23.04 19.56±0.51 16.83±0.13 17.33±0.94 19.85±1.06</cell><cell>15.72±0.23</cell><cell cols="3">15.40±0.07 15.02±0.26 15.19±0.25</cell><cell>15.51±1.67</cell><cell>14.81±0.16</cell></row><row><cell></cell><cell>MAE</cell><cell cols="6">24.44 26.18 23.60±0.52 24.42±0.06 23.90±0.17 22.90±0.20</cell><cell>19.91±0.10</cell><cell cols="3">19.62±0.16 19.74±0.09 19.25±0.06</cell><cell>19.54±0.07</cell><cell>18.81±0.06</cell></row><row><cell>PeMSD4</cell><cell>RMSE</cell><cell cols="6">37.76 38.91 37.11±0.50 37.48±0.10 36.43±0.22 35.59±0.35</cell><cell>31.06±0.17</cell><cell cols="3">31.02±0.29 32.01±0.17 30.85±0.21</cell><cell>31.33±0.11</cell><cell>30.35±0.09</cell></row><row><cell></cell><cell cols="7">MAPE (%) 17.27 22.84 16.17±0.13 16.86±0.09 13.67±0.14 16.75±0.59</cell><cell>13.62±0.22</cell><cell cols="3">13.13±0.11 12.98±0.21 13.00±0.26</cell><cell>12.87±0.05</cell><cell>11.92±0.04</cell></row><row><cell></cell><cell>MAE</cell><cell cols="6">27.96 28.45 34.05±0.51 24.45±0.85 26.22±0.37 28.13±0.70</cell><cell>20.83±0.18</cell><cell cols="3">21.64±0.11 21.22±0.17 20.68±0.08</cell><cell>21.26±0.28</cell><cell>19.95±0.04</cell></row><row><cell>PeMSD7</cell><cell>RMSE</cell><cell cols="6">41.31 42.67 55.70±0.60 37.61±1.18 39.18±0.42 43.67±1.33</cell><cell>33.64±0.22</cell><cell cols="3">34.87±0.27 35.05±0.13 33.56±0.12</cell><cell>34.53±0.28</cell><cell>33.01±0.07</cell></row><row><cell></cell><cell cols="7">MAPE (%) 12.11 14.00 15.31±0.31 10.67±0.53 10.74±0.16 13.31±0.55</cell><cell>9.10±0.27</cell><cell>9.09±0.05</cell><cell>9.00±0.12</cell><cell>9.31±0.12</cell><cell>9.04±0.11</cell><cell>8.31±0.11</cell></row><row><cell></cell><cell>MAE</cell><cell cols="6">19.83 20.92 21.18±0.27 18.49±0.16 18.79±0.49 18.72±0.16</cell><cell>15.57±0.12</cell><cell cols="3">16.12±0.25 15.92±0.19 14.87±0.15</cell><cell>16.12±0.08</cell><cell>14.53±0.17</cell></row><row><cell>PeMSD8</cell><cell>RMSE</cell><cell cols="6">29.24 31.23 31.88±0.43 27.30±0.22 28.23±0.36 28.99±0.11</cell><cell>24.32±0.21</cell><cell cols="3">24.89±0.52 25.31±0.25 24.06±0.16</cell><cell>25.74±0.13</cell><cell>23.74±0.20</cell></row><row><cell></cell><cell cols="7">MAPE (%) 13.08 14.24 13.72±0.27 11.69±0.06 10.55±0.30 12.53±0.48</cell><cell>10.32±0.79</cell><cell cols="2">10.50±0.22 10.30±0.13</cell><cell>9.77±0.07</cell><cell>10.35±0.09</cell><cell>9.15±0.09</cell></row><row><cell></cell><cell>MAE</cell><cell>3.77</cell><cell>4.86</cell><cell>3.94±0.06</cell><cell>3.54±0.04</cell><cell>3.55±0.09</cell><cell>3.37±0.04</cell><cell>3.81±0.03</cell><cell>3.52±0.05</cell><cell>3.33±0.04</cell><cell>3.22±0.03</cell><cell>3.29±0.02</cell><cell>3.19±0.03</cell></row><row><cell>Seattle-Loop</cell><cell>RMSE</cell><cell>5.86</cell><cell>8.96</cell><cell>7.42±0.09</cell><cell>6.22±0.04</cell><cell>5.95±0.07</cell><cell>5.69±0.03</cell><cell>6.81±0.04</cell><cell>6.32±0.11</cell><cell>5.99±0.05</cell><cell>5.70±0.03</cell><cell>5.85±0.02</cell><cell>5.61±0.04</cell></row><row><cell></cell><cell cols="5">MAPE (%) 11.12 15.38 11.74±0.05 10.63±0.01</cell><cell>9.43±0.06</cell><cell>10.14±0.09</cell><cell>10.73±0.14</cell><cell>10.14±0.04</cell><cell>9.74±0.03</cell><cell>9.27±0.01</cell><cell>9.40±0.02</cell><cell>8.92±0.02</cell></row><row><cell cols="8">Spatio-temporal graph neural networks generally achieve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">better performances as they further model the spatial cor-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">relations using graph neural networks. (3) Our STJGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">performs the best in terms of all metrics on all datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">(1.4%˜7.7% improvement against the second best results).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Compared with other graph-based methods, the advantages</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">of our STJGCN are three-fold. First, STJGCN models com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">prehensive spatio-temporal correlations. Second, STJGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">is able to capture dynamic dependencies at different time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">steps. Third, STJGCN leverages the information of multiple</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">spatio-temporal ranges.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Effect of spatio-temporal connections, dynamic graph modeling, multi-range information, and independent fully-connected layers.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell>STJGCN</cell><cell cols="2">w/o STC-pdf w/o STC-adt</cell><cell>w/o STC</cell><cell>w/o dgm</cell><cell>w/o mr</cell><cell>w/o att</cell><cell>w/o idp</cell></row><row><cell></cell><cell>MAE</cell><cell>18.81±0.06</cell><cell>18.99±0.14</cell><cell>19.07±0.10</cell><cell cols="5">19.36±0.09 19.70±0.06 19.03±0.04 18.97±0.09 18.89±0.08</cell></row><row><cell>PeMSD4</cell><cell>RMSE</cell><cell>30.35±0.09</cell><cell>30.63±0.23</cell><cell>30.71±0.13</cell><cell cols="5">30.80±0.10 31.47±0.05 30.79±0.08 30.56±0.12 30.46±0.10</cell></row><row><cell></cell><cell cols="2">MAPE (%) 11.92±0.04</cell><cell>12.00±0.07</cell><cell>12.07±0.06</cell><cell cols="5">12.27±0.08 12.39±0.07 11.98±0.03 11.96±0.02 11.95±0.02</cell></row><row><cell></cell><cell>MAE</cell><cell>14.53±0.17</cell><cell>14.63±0.23</cell><cell>14.82±0.09</cell><cell cols="5">15.07±0.07 15.49±0.22 15.11±0.57 14.67±0.11 14.60±0.11</cell></row><row><cell>PeMSD8</cell><cell>RMSE</cell><cell>23.74±0.20</cell><cell>24.01±0.22</cell><cell>24.11±0.14</cell><cell cols="5">24.22±0.14 24.49±0.23 24.49±0.55 24.03±0.30 23.96±0.21</cell></row><row><cell></cell><cell>MAPE (%)</cell><cell>9.15±0.09</cell><cell>9.18±0.19</cell><cell>9.26±0.08</cell><cell>9.48±0.06</cell><cell>9.55±0.16</cell><cell>9.39±0.22</cell><cell>9.16±0.09</cell><cell>9.16±0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Effect of different STJG adjacency matrix configurations. The term "gf" in the last line denotes the gating fusion approach.</figDesc><table><row><cell cols="3">STJG adjacency matrix configuration</cell><cell></cell><cell>PeMSD4</cell><cell></cell><cell>PeMSD8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAPE (%)</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAPE (%)</cell></row><row><cell>[A</cell><cell>(k) f w ]</cell><cell></cell><cell cols="4">24.64±0.05 38.21±0.02 15.70±0.08 18.52±0.10 29.24±0.18 11.35±0.08</cell></row><row><cell cols="2">[A [ Lt-k;t ] (k) f w , A</cell><cell>(k) bw ]</cell><cell cols="4">24.40±0.06 38.03±0.23 15.47±0.03 18.12±0.07 28.49±0.16 11.19±0.11 19.39±0.12 31.60±0.23 12.38±0.08 15.93±0.15 25.87±0.23 9.98±0.07</cell></row><row><cell cols="3">[ Lt-k;t , Lt;t-k ]</cell><cell cols="4">19.35±0.13 31.47±0.16 12.34±0.14 15.42±0.15 24.80±0.32</cell><cell>9.85±0.14</cell></row><row><cell>[A</cell><cell>(k) f w , A</cell><cell>(k) bw , Lt-k;t , Lt;t-k ]</cell><cell cols="4">18.93±0.09 30.48±0.13 11.97±0.04 14.65±0.08 23.93±0.14</cell><cell>9.23±0.08</cell></row><row><cell>[A</cell><cell>(k) f w , A</cell><cell>(k)</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>bw , Lt-k;t , Lt;t-k ] + gf (ours) 18.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>81±0.06 30.35±0.09 11.92±0.04 14.53±0.17 23.74±0.20 9.15±0.09</head><label></label><figDesc></figDesc><table><row><cell>30.1</cell></row><row><cell>30.0</cell></row><row><cell>29.9</cell></row><row><cell>29.8</cell></row><row><cell>29.7</cell></row><row><cell>0.1 0.3 0.5 0.7 0.9</cell></row><row><cell>pdf</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Comparisons of parameter number and computation time. The training time is the time cost per epoch in the training phase, and the inference time is the total time cost on the validation set.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="9">DCRNN STGCN Graph WaveNet ASTGCN STSGCN AGCRN GMAN Z-GCNETs STJGCN</cell></row><row><cell></cell><cell># Parameter (M)</cell><cell>0.37</cell><cell>0.42</cell><cell>0.31</cell><cell>0.59</cell><cell>3.50</cell><cell>0.75</cell><cell>0.57</cell><cell>0.52</cell><cell>0.32</cell></row><row><cell>PeMSD3</cell><cell>Training time (s/epoch)</cell><cell>118.06</cell><cell>12.20</cell><cell>59.73</cell><cell>78.69</cell><cell>127.86</cell><cell>55.45</cell><cell>168.77</cell><cell>208.55</cell><cell>49.82</cell></row><row><cell></cell><cell>Inference time (s)</cell><cell>18.70</cell><cell>19.10</cell><cell>5.16</cell><cell>26.80</cell><cell>15.41</cell><cell>8.44</cell><cell>17.45</cell><cell>25.79</cell><cell>5.22</cell></row><row><cell></cell><cell># Parameter (M)</cell><cell>0.37</cell><cell>0.38</cell><cell>0.31</cell><cell>0.45</cell><cell>2.87</cell><cell>0.75</cell><cell>0.57</cell><cell>0.52</cell><cell>0.31</cell></row><row><cell>PeMSD4</cell><cell>Training time (s/epoch)</cell><cell>69.55</cell><cell>6.54</cell><cell>32.40</cell><cell>53.51</cell><cell>56.18</cell><cell>37.05</cell><cell>82.40</cell><cell>88.41</cell><cell>25.64</cell></row><row><cell></cell><cell>Inference time (s)</cell><cell>11.97</cell><cell>13.44</cell><cell>2.60</cell><cell>14.67</cell><cell>6.03</cell><cell>5.55</cell><cell>9.16</cell><cell>11.84</cell><cell>2.87</cell></row><row><cell></cell><cell># Parameter (M)</cell><cell>0.37</cell><cell>0.75</cell><cell>0.31</cell><cell>3.24</cell><cell>15.36</cell><cell>0.75</cell><cell>0.57</cell><cell>0.52</cell><cell>0.36</cell></row><row><cell>PeMSD7</cell><cell>Training time (s/epoch)</cell><cell>306.66</cell><cell>33.59</cell><cell>173.85</cell><cell>213.30</cell><cell>465.12</cell><cell>189.48</cell><cell>779.12</cell><cell>624.32</cell><cell>158.64</cell></row><row><cell></cell><cell>Inference time (s)</cell><cell>45.13</cell><cell>71.17</cell><cell>16.17</cell><cell>64.81</cell><cell>54.60</cell><cell>26.31</cell><cell>83.2</cell><cell>89.99</cell><cell>16.30</cell></row><row><cell></cell><cell># Parameter (M)</cell><cell>0.37</cell><cell>0.30</cell><cell>0.31</cell><cell>0.18</cell><cell>1.66</cell><cell>0.75</cell><cell>0.57</cell><cell>0.52</cell><cell>0.31</cell></row><row><cell>PeMSD8</cell><cell>Training time (s/epoch)</cell><cell>46.41</cell><cell>4.24</cell><cell>20.48</cell><cell>47.07</cell><cell>31.23</cell><cell>21.74</cell><cell>32.27</cell><cell>52.51</cell><cell>17.60</cell></row><row><cell></cell><cell>Inference time (s)</cell><cell>8.81</cell><cell>9.37</cell><cell>1.72</cell><cell>14.01</cell><cell>3.09</cell><cell>3.04</cell><cell>4.06</cell><cell>7.36</cell><cell>1.67</cell></row><row><cell></cell><cell># Parameter (M)</cell><cell>0.37</cell><cell>0.39</cell><cell>0.31</cell><cell>0.49</cell><cell>3.50</cell><cell>0.75</cell><cell>0.57</cell><cell>0.52</cell><cell>0.32</cell></row><row><cell>Seattle-Loop</cell><cell>Training time (s/epoch)</cell><cell>378.16</cell><cell>59.83</cell><cell>100.82</cell><cell>249.34</cell><cell>120.66</cell><cell>161.44</cell><cell>1901.50</cell><cell>809.35</cell><cell>516.69</cell></row><row><cell></cell><cell>nference time (s)</cell><cell>106.47</cell><cell>124.36</cell><cell>120.20</cell><cell>100.46</cell><cell>36.45</cell><cell>16.36</cell><cell>170.7</cell><cell>66.83</cell><cell>45.01</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The research was supported by <rs type="funder">Natural Science Foundation of China</rs> (<rs type="grantNumber">62272403</rs>, <rs type="grantNumber">61872306</rs>), and <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (<rs type="grantNumber">20720200031</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RTFvXrr">
					<idno type="grant-number">62272403</idno>
				</org>
				<org type="funding" xml:id="_JHrUkqf">
					<idno type="grant-number">61872306</idno>
				</org>
				<org type="funding" xml:id="_5uXhsuw">
					<idno type="grant-number">20720200031</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for spatio-temporal data mining: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on modern deep neural network for traffic prediction: Trends, methods and challenges</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Tedjopurnomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stpc-net: Learn massive geo-sensory data as spatio-temporal point clouds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural attention model for urban air quality inference: learning the weights of monitoring stations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2151" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3634" to="3640" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph wavenet for deep spatial-temporal graph modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyuwan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyuwan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adaptive graph convolutional recurrent network for traffic forecasting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Z-gcnets: Time zigzags at graph convolutional networks for time series forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Segovia-Dominguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Gel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual correlation in graph neural network regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A graphbased temporal attention framework for multi-sensor traffic flow forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08555</idno>
		<title level="m">A comprehensive survey on traffic prediction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting citywide crowd flows in irregular regions using multi-view graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting interpretable patterns for flow prediction in dockless bike sharing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepcrowd: A deep model for largescale citywide crowd density and flow prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsubouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning dynamics and heterogeneity of spatial-temporal graph data for traffic forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory neural network for traffic speed prediction using remote microwave sensor data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep multi-view spatial-temporal network for taxi demand prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="2588" to="2595" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting spatialtemporal similarity: A deep learning framework for traffic prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepstd: Mining spatio-temporal disturbances of multiple context factors for citywide traffic flow prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3744" to="3755" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flow prediction in spatiotemporal networks based on multitask deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How to build a graph-based deep learning architecture in traffic domain: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11691</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks for traffic forecasting: Spatial layers first or temporal layers first</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 29th International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="427" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gman: A graph multiattention network for traffic prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Traffic flow prediction via spatial temporal graph neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1082" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Agstn: Learning attention-adjusted graph spatio-temporal networks for short-term urban sensor value forecasting</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1148" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Origindestination matrix prediction via graph convolution: a new perspective of passenger demand modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1227" to="1235" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multirange attentive bicomponent graph convolutional network for traffic forecasting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent space model for road networks to predict time-varying traffic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep bidirectional and unidirectional lstm recurrent neural network for network-wide traffic speed prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02143</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pgcn: Progressive graph convolutional networks for spatial-temporal traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08982</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Time series analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Princeton university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Support vector regression machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapoik</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Zigzag persistence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of computational mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="367" to="405" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ATRank: An attention-based user behavior modeling framework for recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4564" to="4571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">TrustSVD: Collaborative filtering with both the explicit and implicit influence of user trust and of item ratings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="123" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SoRec: Social recommendation using probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th ACM Conf. Inf. Knowl. Mining (CIKM)</title>
		<meeting>17th ACM Conf. Inf. Knowl. Mining (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Finding the bias and prestige of nodes in networks based on trust scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. World Wide Web (WWW)</title>
		<meeting>20th Int. Conf. World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SIDE: Representation learning in signed directed networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. World Wide Web (WWW)</title>
		<meeting>27th Int. Conf. World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (ROC) curve</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">SIGNet: Scalable embeddings for signed networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAKDD</title>
		<imprint>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Joint Conf. Artif. Intell. (IJCAI)</title>
		<meeting>26th Int. Joint Conf. Artif. Intell. (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
