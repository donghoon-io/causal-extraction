<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Graph Neural Networks on Out-Of-Distribution Graphs</title>
				<funder ref="#_tngeVzH #_by5TtSb">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-10">10 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaohua</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE, Peng Cui, Senior Member, IEEE</roleName><forename type="first">Bai</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Generalizing Graph Neural Networks on Out-Of-Distribution Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-10">10 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.10657v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Out-Of-Distribution Generalization</term>
					<term>Causal Representation Learning</term>
					<term>Stable Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are proposed without considering the agnostic distribution shifts between training graphs and testing graphs, inducing the degeneration of the generalization ability of GNNs in Out-Of-Distribution (OOD) settings. The fundamental reason for such degeneration is that most GNNs are developed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. This learning mechanism inherits from the common characteristics of machine learning approaches. However, such spurious correlations may change in the wild testing environments, leading to the failure of GNNs. Therefore, eliminating the impact of spurious correlations is crucial for stable GNN models. To this end, in this paper, we argue that the spurious correlation exists among subgraph-level units and analyze the degeneration of GNN in causal view. Based on the causal view analysis, we propose a general causal representation framework for stable GNN, called StableGNN. The main idea of this framework is to extract high-level representations from raw graph data first and resort to the distinguishing ability of causal inference to help the model get rid of spurious correlations. Particularly, to extract meaningful high-level representations, we exploit a differentiable graph pooling layer to extract subgraph-based representations by an end-to-end manner. Furthermore, inspired by the confounder balancing techniques from causal inference, based on the learned high-level representations, we propose a causal variable distinguishing regularizer to correct the biased training distribution by learning a set of sample weights. Hence, GNNs would concentrate more on the true connection between discriminative substructures and labels. Extensive experiments are conducted on both synthetic datasets with various distribution shift degrees and eight real-world OOD graph datasets. The results well verify that the proposed model StableGNN not only outperforms the state-of-the-arts but also provides a flexible framework to enhance existing GNNs. In addition, the interpretability experiments validate that StableGNN could leverage causal structures for predictions. The source code is available at <ref type="url" target="https://github.com/googlebaba/StableGNN">https://github.com/googlebaba/StableGNN</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>G Raph Neural Networks (GNNs) are powerful deep learning algorithms on graphs with various applications <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. One major category of applications is the predictive task over entire graphs, i.e., graph-level tasks, such as molecular graph property prediction <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, scene graph classification <ref type="bibr" target="#b8">[9]</ref>, and social network category classification <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, etc. The success could be attributed to the non-linear modeling capability of GNNs, which extracts useful information from raw graph data and encodes them into the graph representation in a data-driven fashion.</p><p>The basic learning diagram of existing GNNs is to learn the parameters of GNNs from the training graphs, and then make predictions on unseen testing graphs. The most fundamental assumption to guarantee the success of such a learning diagram is the I.I.D. hypothesis, i.e., training and testing graphs are independently sampled from † Corresponding author.</p><p>• S. <ref type="bibr">Fan</ref>  the identical distribution <ref type="bibr" target="#b10">[11]</ref>. However, in reality, such a hypothesis is hardly satisfied due to the uncontrollable generation mechanism of real data, such as data selection biases, confounder factors or other peculiarities <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The testing distribution may incur uncontrolled and unknown shifts from the training distribution, called Out-Of-Distribution (OOD) shifts <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which makes most GNN models fail to make stable predictions. As reported by OGB benchmark <ref type="bibr" target="#b5">[6]</ref>, GNN methods will occur a degeneration of 5.66% to 20% points when splitting the datasets according to the OOD settings, i.e., splitting structurally different graphs into training and testing sets. Essentially, for general machine learning models, when there incurs a distribution shift, the main reason for the degeneration of accuracy is the spurious correlation between the irrelevant features and the category labels. This kind of spurious correlations is intrinsically caused by the unexpected correlations between irrelevant features and relevant features for a given category <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. For graphlevel tasks that we focus on in this paper, as the predicted properties of graphs are usually determined by the subgraph units (e.g., groups of atoms and chemical bonds representing functional units in a molecule) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, we define one subgraph unit could be one relevant feature or irrelevant feature with graph label. Taking the classification task of "house" motif as an example, as depicted in Figure <ref type="figure">1</ref>, where the graph is labeled by whether the graph has a "house" motif and the first column represents the testing graphs. The GCN Fig. <ref type="figure">1</ref>: Visualization of subgraph importance for "house" motif classification task, produced by the vanilla GCN model and StableGNN when most of training graphs containing "house" motifs with "star" motifs. The red subgraph indicates the most important subgraph used by the model for prediction (generated by GNNExplainer <ref type="bibr" target="#b0">[1]</ref>). Due to the spurious correlation, the GCN model tends to focus more on "star" motifs while our model focuses mostly on "house" motifs.</p><p>For more cases of testing graphs, please refer to Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>model is trained on the dataset where "house" motifs coexist with "star" motifs in most training graphs. With this dataset, the structural features of "house" motifs and "star" motifs would be strongly correlated. This unexpected correlation leads to spurious correlations between structural features of "star" motifs with the label "house". And the second column of Figure <ref type="figure">1</ref> shows the visualization of the most important subgraph used by the GCN for prediction (shown with red color and generated by GNNExplainer <ref type="bibr" target="#b0">[1]</ref>). As a result, the GCN model tends to use such spurious correlation, i.e., "star" motif, for prediction. When encountering graphs without "star" motif, or other motifs (e.g., "diamond" motifs) with "star" motifs, the model is prone to make false predictions (See Section 5.1).</p><p>To improve the OOD generalization ability of GNNs, one important way is to make GNNs get rid of such subgraphlevel spurious correlation. However, it is not a trivial task, which will face the two following challenges: (1) How to explicitly encode the subgraph-level information into graph representation? As the spurious correlation usually exists between subgraphs, it is necessary to encode such subgraph information into the graph representation, so that we can develop a decorrelation method based on the subgraph-level representation. <ref type="bibr" target="#b1">(2)</ref> How to remove the spurious correlation among the subgraph-level representations? The nodes in the same type of subgraph units may exist correlation, for example, the information of 'N' atom and 'O' atom in 'NO 2 ' groups of molecular graphs may be encoded into different dimensions of learned embedding, but they act as an integrated whole and such correlations are stable across unseen testing distributions. Hence, we should not remove the correlation between the interior variables of one kind of subgraphs and only need to remove the spurious correlation between subgraph-level variables.</p><p>To address these two challenges, we propose a novel causal representation framework for graph, called Sta-bleGNN, which takes advantage of both the flexible representation ability of GNNs and the distinguishing ability for spurious correlations of causal inference methods. In terms of the first challenge, we propose a graph high-level variable learning module that employs a graph pooling layer to map nearby nodes to a set of clusters, where each cluster will be one densely-connected subgraph unit of original graph. Moreover, we theoretically prove that the semantic meanings of clusters would be aligned across graphs by an ordered concatenation operation. Given the aligned highlevel variables, to overcome the second challenge, we analyze the degeneration of GNNs in causal view and propose a novel causal variable distinguishing regularizer to decorrelate each high-level variable pair by learning a set of sample weights. These two modules are jointly optimized in our framework. Furthermore, as shown in Figure <ref type="figure">1</ref>, StableGNN can effectively partial out the irrelevant subgraphs (i.e., "star" motif) and leverage truly relevant subgraphs (i.e., "house" motif) for predictions.</p><p>In summary, the major contributions of the paper are as follows:</p><p>• To our best knowledge, we are one of the pioneer works studying the OOD generalization problem on GNNs for graph-level tasks, which is a key direction to apply GNNs to wild non-stationary environments.</p><p>• We propose a general causal representation learning framework for GNNs, jointly learning the high-level representation with the causal variable distinguisher, which could learn an invariant relationship between the graph causal variables with the labels. And our framework is general to be adopted for various base GNN models to help them get rid of spurious correlations.</p><p>• Comprehensive experiments are conducted on both synthetic datasets and real-world OOD graph datasets. The effectiveness, flexibility and interpretability of the proposed framework have been well-validated with convincing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss three main categories closely related to our work: graph neural networks, causal representation learning and stable learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>Graph neural networks are powerful deep neural networks that could perform on graph data directly <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. GNNs have been applied to a wide variety of tasks, including node classification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, link prediction <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, graph clustering <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and graph classification <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For graph classification, the task we majorly focus on here, the major challenge in applying GNNs is how to generate the representation of the entire graph. Common approaches to this problem are simply summing up or averaging all the node embedding in the final layer. Several literatures argue that such simple operation will greatly ignore high-level structure that might be presented in the graph, and then propose to learn the hierarchical structure of graph in an end-to-end manner <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Although these methods have achieved remarkable results in I.I.D setting, most of them largely ignore the generalization ability in OOD setting, which is crucial for real applications. During the review process, we notice several works claiming the importance of OOD generalization on graph classification tasks <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Unlike the framework of OOD-GNN <ref type="bibr" target="#b31">[32]</ref>, which learns a single embedding for each graph and decorrelates each dimension of embeddings, our framework emphasizes the need to learn meaningful high-level representations for each subgraph and decorrelate those representations. DIR <ref type="bibr" target="#b32">[33]</ref> divides a graph into causal-and non-causal part by an edge threshold. However, the threshold is set as the same for all graphs and is hard to select a good threshold for all graphs. CIGA <ref type="bibr" target="#b33">[34]</ref> proposes to maximize the agreement between the invariant part of graphs with the same labels. For example, the functional groups NO 2 and NH 2 could both determine the mutagenicity of a molecule. However, subgraphs with the same labels may not always be identical. DisC <ref type="bibr" target="#b35">[36]</ref> studies how to learn causal substructure in severe bias scenarios. Other methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref> are based on the environmental inference framework, in which these methods iteratively infer the environment labels of graphs and learn the invariant information based on the environment labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Representation Learning</head><p>Recently, Schölkopf et al. <ref type="bibr" target="#b37">[38]</ref>  labeled hinders the IRM methods from real applications. To overcome such a dilemma, some methods are proposed to implicitly learn domains from data <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, but they implicitly assume the training data is formed by balanced sampling from latent domains. However, all these methods are mainly designed for tabular or image data, thus they cannot capture the intrinsic properties of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stable Learning</head><p>To improve the feasibility of IRM-based methods, a series of researches on stable learning are proposed <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. These methods mainly bring the ideas from the causal effect estimation <ref type="bibr" target="#b46">[47]</ref> into machine learning models. Particularly, Shen et al. <ref type="bibr" target="#b47">[48]</ref> propose a global confounding balancing regularizer that helps the logistic regression model to identify causal features, whose causal effect on outcomes are stable across domains. To make the confounder balancing much easier in high-dimensional scenarios, Kuang et al. <ref type="bibr" target="#b44">[45]</ref> utilize the autoencoder to encode the high-dimensional features into low-dimensional representation. Furthermore, <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b48">[49]</ref> demonstrate that decorrelating relevant and irrelevant features can make a linear model produce stable predictions under distribution shifts. Nevertheless, they are all developed based on the linear framework. Recently, Zhang et al. <ref type="bibr" target="#b17">[18]</ref> extend decorrelation into a deep CNN model to tackle more complicated data types like images. This method pays much attention to eliminating the non-linear dependence among features, but the feature learning part is largely ignored. We argue that it is important to develop an effective high-level representation learning method to extract variables with appropriate granularity for the targeted task, so that the causal variable distinguishing part could develop based on meaningful causal variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Notations. In this paper, for any vector</p><formula xml:id="formula_0">v = (v 1 , v 2 , ⋯, v n ), let v i the i-th element of v.</formula><p>For any matrix X, let X i, and X ,j represent the i-th row and the j-th column in X, respectively. X ,j∶k denotes the submatrix of X from j-th column to (k -1)th column. And for any italic uppercase letter, such as X, it will represent a random variable. We summarize the key notions used in the paper in Table <ref type="table" target="#tab_4">1</ref>.  We utilize a causal graph of the data generation process, as shown in Figure <ref type="figure">2</ref>, to illustrate the fundamental reason to cause the distribution shifts on graphs. As illustrated in the figure, the observed graph data G is generated by the unobserved latent cause Z and M . Z is a set of relevant variables and the label Y is mainly determined by Z. M is a set of irrelevant variables which does not decisive for label Y . During the unseen testing process in the real world, the variable M could change, but the causal relationship P(Y |Z) is invariant across environments. Taking the classifying mutagenic property of a molecular graph <ref type="bibr" target="#b49">[50]</ref> as an example, G is a molecular graph where the nodes are atoms and the edges are the chemical bonds between atoms, and Y is the class label, e.g., whether the molecule is mutagenic or not. The whole molecular graph G is an effect of relevant latent causes Z such as the factors to generate nitrogen dioxide (NO 2 ) group, which has a determinative effect on the mutagenicity of molecule, and the effect of irrelevant variable M , such as the carbon ring which exists more frequently in mutagenic molecules but not determinative <ref type="bibr" target="#b49">[50]</ref>. If we aim to learn a GNN model that is robust to unseen change on M , such as carbon exists in the non-mutagenic molecule, one possible way is to develop a representation function f (⋅) to recover Z and M from G, and learn a classifier g(⋅) based on Z, so that the invariant causal relationship P (Y |Z) could be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 1. OOD Generalization Problem on Graphs. Given the training graphs</head><formula xml:id="formula_1">G train = {(G 1 , Y 1 ), ⋯, (G n , Y n )},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The basic idea of our framework is to design a causal representation learning method that could extract meaningful graph high-level variables and estimate their true causal effects for graph-level tasks. As depicted in Figure <ref type="figure" target="#fig_0">3</ref>, the proposed framework mainly consists of two components: the graph high-level variable learning component and causal variable distinguishing component. The graph high-level variable learning component first employs a graph pooling layer that learns the node embedding and maps nearby nodes into a set of clusters. Then we get the cluster-level embeddings through aggregating the node embeddings in the same cluster, and align the cluster semantic space across graphs through an ordered concatenation operation. The cluster-level embeddings act as high-level variables for graphs. After obtaining the high-level variables, we develop a sample reweighting component based on Hilbert-Schmidt Independence Criterion (HSIC) measure to learn a set of sample weights that could remove the non-linear dependencies among multiple multi-dimensional embeddings. As the learned sample weights could generate a pseudo-distribution with less spurious correlation among cluster-level variables, we utilize the weights to reweight the GNN loss. Thus the GNN model trained on this less biased pseudo-data could estimate the causal effect of each high-level variable on the label more precisely, resulting in better generalization ability in wild environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph High-level Variable Learning</head><p>As the goal of this component is to learn a representation that could represent the original subgraph unit explicitly, we adopt a differentiable graph pooling layer that could map densely-connected subgraphs into clusters in an end-to-end manner. And then we theoretically prove that the semantic meaning of the learned high-level representations could be aligned by a simple ordered concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level Variable Pooling.</head><p>To learn node embedding as well as map densely connected subgraphs into clusters, we exploit the DiffPool layer <ref type="bibr" target="#b7">[8]</ref> to achieve this goal by learning a cluster assignment matrix based on the learned embedding in each pooling layer. Particularly, as GNNs could smooth the node representations and make the representation more discriminative, given the input adjacency matrix A, also denoted as A (0) in this context, and the node features F of a graph, we firstly use an embedding GNN module to get the smoothed embeddings F (0) :</p><formula xml:id="formula_2">F (0) = GNN (0) embed (A (0) , F),<label>(1)</label></formula><p>where GNN (0) embed (⋅, ⋅) is a three-layer GNN module, and the GNN layer could be GCN <ref type="bibr" target="#b2">[3]</ref>, GraphSAGE <ref type="bibr" target="#b4">[5]</ref>, or others. Then we develop a pooling layer based on the smoothed representation F (0) . In particular, we first generate node representation at layer 1 as follows:</p><formula xml:id="formula_3">Z (1) = GNN (1) embed (A (0) , F (0) ).<label>(2)</label></formula><p>As nodes in the same subgraph would have similar node features and neighbor structure and GNN could map the nodes with similar features and structural information into similar representation, we also take the node embeddings F (0) and adjacency matrix A (0) into a pooling GNN module to generate a cluster assignment matrix at layer 1, i.e., the clusters of original graph:</p><formula xml:id="formula_4">S (1) = softmax(GNN<label>(1)</label></formula><formula xml:id="formula_5">pool (A (0) , F (0) )),<label>(3)</label></formula><p>where S (1) ∈ R n 0 ×n 1 , and n 0 is the number of nodes of the input graph and n 1 is the number of the clusters at the layer 1, and S</p><p>i, represents the cluster assignment vector of i-th node, and S</p><p>,j corresponds to all nodes' assignment probabilities on j-th cluster at the layer 1. GNN </p><p>pool (⋅, ⋅) is a pre-defined maximum number of clusters at layer 1 and is a hyperparameter. And the appropriate number of clusters could be learned in an end-to-end manner. The maximum number of clusters in the last pooling layer should be the number of high-level variables we aim to extract. The softmax function is applied in a row-wise fashion to generate the cluster assignment probabilities for each node at layer 1.</p><p>After obtaining the assignment matrix S (1) , we could know the assignment probability of each node on the predefined clusters. Hence, based on the assignment matrix S (1) and the learned node embedding matrix Z (1) , we could get a new coarsened graph, where the nodes are the clusters learned by this layer and edges are the connectivity strength between each pair of clusters. Particularly, the new matrix of embeddings is calculated by the following equation:</p><formula xml:id="formula_9">F (1) = S (1) T Z (1) ∈ R n 1 ×d , (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>where d is the dimension of the embedding. This equation aggregates the node embedding Z (1) according to the cluster assignment S (1) , generating embeddings for each of the n 1 clusters. Similarly, we generate a coarsened adjacency matrix as follows.</p><formula xml:id="formula_11">A (1) = S (1) T A (0) S (1) ∈ R n 1 ×n 1 .<label>(5)</label></formula><p>For all the operations with superscript (1), it is one DiffPool unit and it is denoted as (A (1) , F (1 ) = DiffPool(A (0) , F (0) ). For any DiffPool layer l, it could be denoted as (A (l) , F (l) ) = DiffPool(A (l-1) , F (l-1) ). In particular, we could stack multiple DiffPool layers to extract the deep hierarchical structure of the graph.</p><p>High-level Representation Alignment. After stacking L graph pooling layers, we could get the most high-level cluster embedding</p><formula xml:id="formula_12">F (L) ∈ R n L ×d , where F (L) i,</formula><p>represents the i-th high-level representation of the corresponding subgraph in the original graph and n L is the number of highlevel representation. As our target is to encode subgraph information into graph representation and F (L) has explicitly encoded each densely-connected subgraph information into each row of F (L) , we propose to utilize the embedding matrix F (L) to represent the high-level variables. However, due to the Non-Euclidean property of graph data, for the i-th learned high-level representations F(L) i, and F(L) i, from two graphs Ĝ and G, respectively, their semantic meaning may not be matched, e.g., F(L) i, and F(L) i, may represent scaffold substructure (e.g., carbon ring) and functional group (e.g., NO 2 ) in two molecular graphs, respectively. To match the semantic meaning of learned high-level representation across graphs, we propose to concatenate the high-level variables by the order of row index of high-level embedding matrix for each graph:</p><formula xml:id="formula_13">h = concat(F (L) 1, , F (L) 2, , ⋯, F (L) n L , ),<label>(6)</label></formula><p>where h ∈ R n L d and concat(⋅) means concatenation operation by the row axis. Moreover, we stack m high-level representations for a mini-batch with m graphs to obtain the embedding matrix H ∈ R m×n L d : (i.e., the GNN method used is permutation equivariant), then DiffPool(A, F) = DiffPool(PAP T , PF).</p><formula xml:id="formula_14">H = stack(h 1 , h 2 , ⋯, h m ),<label>(7)</label></formula><formula xml:id="formula_15">Proof. Following [51], a function f ∶ R n×d → R n×l : is invariant if f (F, X) = f (PFP T , PF), i.</formula><p>e, the permutation will not change node representation and the node order in the learned matrix.</p><formula xml:id="formula_16">A function f ∶ R n×d → R n×l : is equivariant if f (A, F) = P ⋅ f (PAP T , PF), i.</formula><p>e., the permutation will not change the node representation but the order of nodes in matrix will be permuted. The permutation invariant aggregator functions such as mean pooling and max pooling ensure that the basic model can be trained and applied to arbitrarily ordered node neighborhood feature sets, i.e, permutation equivariant. Therefore, the Eq. ( <ref type="formula" target="#formula_3">2</ref>) and ( <ref type="formula" target="#formula_5">3</ref>) are the permutation equivariant by the assumption that the GNN module is permutation equivariant. And since any permutation matrix is orthogonal, i.e., P T P = I, applying this into Eq. ( <ref type="formula" target="#formula_9">4</ref>) and ( <ref type="formula" target="#formula_11">5</ref>), we have: 1) .</p><formula xml:id="formula_17">F (1) = S (1) T P T PZ (1) ,<label>(8)</label></formula><formula xml:id="formula_18">A (1) = S (1) T P T PA (1) PP T S<label>(</label></formula><p>Hence, DiffPool layer is permutation invariant.</p><p>After proving the permutation invariant property of a single graph, we then illustrate the theoretical results for the semantic alignment of learned high-level representation across graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Semantic Alignment of High-level Variables.</head><p>Given any two graphs G i = (A i , F i ) and G j = (A j , F j ), the semantic space of high-level representations H i, and H j, learned by a series of shared DiffPool layers is aligned.</p><p>Proof. WL-test <ref type="bibr" target="#b51">[52]</ref> is widely used in graph isomorphism checking: if two graphs are isomorphic, they will have the same multiset of WL colors at any iteration. GNNs could be viewed as a continuous approximation to the WL test, where they both aggregate signal from neighbors and the trainable neural network aggregators is an analogy to the hash function in WL-test <ref type="bibr" target="#b9">[10]</ref>. Therefore, the outputs of GNN module are exactly the continuous WL colors. The outputs of GNN layers or the WL color of nodes would represent their structural roles <ref type="bibr" target="#b9">[10]</ref>. For example, the output assignment matrix of the pooling GNN module in Diffpool layer S (1) ∈ R n 0 ×n 1 could be interpreted as the structural roles of n 0 input nodes with respect to the n 1 clusters of layer 1. Through the assignment matrix S (1) , it will aggregate the nodes with similar structural roles into the clusters with the same index. Hence, the semantic meaning of each column of the learned assignment matrices S (l) i and S (l) j of graphs G i and G j would be aligned, e.g., the k-th column would represent carbon ring structure in all molecule graphs, which act as the scaffold structural role in molecule graphs. Due to the permutation invariant property of DiffPool layer, according to Eq. ( <ref type="formula" target="#formula_17">8</ref>), the input graph with any node order will map its carbon ring structure signal into the k-th high-level representation, i.e., the k-th high-level representation of H i,(k-1)d∶kd and H j,(k-1)d∶kd . Hence, the semantic meaning of H i,. and H j,. is aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Causal Variable Distinguishing Regularizer</head><p>So far the variable learning part extracts all the high-level representations for all the densely-connected subgraphs regardless of whether it is relevant with the graph label due to causal relation or spurious correlation. In this section, we first analyze the reason leading to the degeneration of GNNs on OOD scenarios in a causal view and then propose the Causal Variable Distinguishing (CVD) regularizer with sample reweighting technique.  Revisiting on GNNs in Causal View. As described in Section 3, our target is to learn a classifier g(⋅) based on the relevant variable Z. To this end, we need to distinguish which variable of learned high-level representation H belongs to Z or M . The major difference between Z and M is whether they have a causal effect on Y . For a graph-level classification task, after learning the graph representation, it will be fed into a classifier to predict its label. The prediction process could be represented by the causal graph with three variables and their relationships, as shown in Figure <ref type="figure" target="#fig_2">4</ref>(a), where T is a treatment variable, Y is the prediction/outcome variable 1 , and X is the confounder variable, which has effects both on the treatment variable and the outcome variable. The path T → Y represents the target of GNNs that aims to estimate the causal effect of the one learned variable T (e.g., i-th high-level representation H ,(i-1)d∶id ) on Y . Meanwhile, other learned variables (e.g., j-th high-level representation H ,(j-1)d∶jd ) will act as confounder X. Due to the existence of spurious correlations of subgraphs, there are spurious correlations between their learned embeddings. Hence, there incurs a path between X and T . 2 And because GNNs also employ the confounder (e.g., the representation of carbon ring) for prediction, there exists a path X → Y . Hence, these two paths form a backdoor path between X and Y (i.e., T ← X → Y ) and induce the spurious correlation between T and Y . And the spurious correlation will amplify the true correlation between treatment variable and label, and may change in testing environments. Under this scenario, existing GNNs cannot estimate the causal effect of subgraphs accurately, so the performance of GNNs will degenerate when the spurious correlation change in the testing phase.</p><p>1. We use variable Y for both the ground-truth labels and prediction, as they are optimized to be the same.</p><p>2. Note that the direction of arrow means that the assignment of treatment value will dependent on the confounder. However, if the arrow is reversed, it will not affect the theoretical results in our paper.</p><p>Confounding balancing techniques <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> correct the nonrandom assignment of treatment variable by balancing the distributions of confounder across different treatment levels. Because moments can uniquely determine a distribution, confounder balancing methods directly balance the confounder moments by adjusting weights of samples <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b54">[55]</ref>. The sample weights w for binary treatment scenario are learnt by:</p><formula xml:id="formula_20">w = arg min w || ∑ i∶T i =1 w i ⋅ X i, ∑ i∶T i =1 w i - ∑ j∶T j =0 w j ⋅ X j, ∑ j∶T j =0 w j || 2 2 , (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>where X is the confounder matrix and X i, is the confounder vector for i-th graph. Given a binary treatment feature T ,</p><formula xml:id="formula_22">∑ i∶T i =1 w i ⋅X i, ∑ i∶T i =1 w i and ∑ j∶T j =0 w j ⋅X j,</formula><p>∑ j∶T j =0 w j refer to the mean value of confounders on samples with and without treatment, respectively. After confounder balancing, the dependence between X and T (i.e., T ← X) would be eliminated, thus the correlation between the treatment variable and the output variable will represent the causal effect (i.e., X → Y ). Moreover, for a GNN model, we have little prior knowledge on causal relationships between the learned high-level variables {H ,0∶d , ⋯, H ,(n L -1)d∶n L d }, thus we have to set each learned high-level variable as treatment variable one by one, and the remaining high-level variables are viewed as confounding variables, e.g, H ,0∶d is set as treatment variable and {H ,d∶2d , ⋯, H ,(n L -1)d∶n L d } are set as confounders. Note that for a particular treatment variable, previous confounder balancing techniques are mainly designed for a singledimensional treatment feature as well as the confounder usually consists of multiple variables where each variable is a single-dimensional feature. In our scenario, however, as depicted in Figure <ref type="figure" target="#fig_2">4</ref>(b), we should deal with the confounders which are composed of multiple multi-dimensional confounder variables {X (1) , ⋯, X (p) }, where each multidimensional confounder variable corresponds to one of learned high-level representations {H ,0∶d , ⋯, H ,(n L -1)d∶n L d }.</p><p>The multi-dimensional variable unit usually has integrated meaning such as representing one subgraph, so it is unnecessary to remove the correlation between the treatment variable with each of feature H ,i in one multi-dimensional feature unit, e.g., H ,0∶d . And we should only remove the subgraphlevel correlation between treatment variable T with multiple multi-dimensional variable units {X (1) , ⋯, X (p) }. One possible way to achieve this goal is to learn a set of sample weights that balance the distributions of all confounding variables for the targeted treatment variable, as illustrated in Figure <ref type="figure" target="#fig_2">4</ref>(b), i.e., randomizing the assignment of treatment T 3 with confounders {X (1) , ⋯, X (p) }. The sample weights w could be learnt by the following multiple multi-dimensional confounder balancing objective:</p><formula xml:id="formula_23">w = arg min w p ∑ k=1 || ∑ i∶T i =1 w i ⋅ X (k) i, ∑ i∶T i =1 w i - ∑ j∶T j =0 w j ⋅ X (k) j, ∑ j∶T j =0 w j || 2 2 ,<label>(11)</label></formula><p>where X (k) is the embedding matrix for k-th confounding variable X (k) .</p><p>3. Here, we still assume the treatment is a binary variable. In the following part, we will consider the treatment as a multi-dimensional variable.</p><p>Weighted Hilbert-Schmidt Independence Criterion. Since the above confounder balancing method is mainly designed for binary treatment variable, which needs the treatment value to divide the samples into treated or control group, it is hard to apply to the continuous multi-dimensional variables learned by GNNs. Inspired by the intuition of confounder balancing which is to remove the dependence between the treatment with the corresponding confounding variables, we propose to remove dependence between continuous multidimensional random variable T with each of confounder in {X (1) , ⋯, X (p) }. Moreover, as the relationship between representations learned by the representation module is highly non-linear, it is necessary to measure the nonlinear dependence between them. And it is feasible to resort to HSIC measure <ref type="bibr" target="#b55">[56]</ref>. For two random variables U and V and kernel k and l, HSIC is defined as</p><formula xml:id="formula_24">HSIC k,l (U, V ) ∶= ||C k,l U V ||<label>2</label></formula><p>HS , where C k,l is a cross-covariance operator in the Reproducing Kernel Hilbert Spaces (RKHS) of k and l <ref type="bibr" target="#b56">[57]</ref>, an RKHS analogue of covariance matrices. || ⋅ || HS is the Hilbert-Schmidt norm, a Hilbert-space analogue of the Frobenius norm. For two random variables U and V and radial basis function (RBF) kernels k and l, HSIC k,l (U, V ) = 0 if and only if U ⊥ V . To estimate HSIC k,l (U, V ) with finite sample, we employ a widely used estimator HSIC k,l 0 (U, V ) <ref type="bibr" target="#b56">[57]</ref> with m samples, defined as:</p><formula xml:id="formula_25">HSIC k,l 0 (U, V ) = (m -1) -2 tr(KPLP),<label>(12)</label></formula><p>where K, L ∈ R m×m are RBF kernel matrices containing entities</p><formula xml:id="formula_26">K ij = k(U i , U j ) and L ij = l(V i , V j ). P = I -m -1 11 T ∈ R</formula><p>m×m is a centering matrix, where I is an identity matrix and 1 is an all-one column vector. P is used to center the RBF kernel matrices to have zero mean in the feature space.</p><p>To eliminate the dependence between the high-level treatment representation with the corresponding confounders, sample reweighting techniques could generate a pseudodistribution that has less dependence between variables <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b57">[58]</ref>. We propose a sample reweighting method to eliminate the dependence between high-level variables, where the nonlinear dependence is measured by HSIC.</p><p>We use w ∈ R m + to denote a set of sample weights. For any two random variables U and V , we first utilize the random initialized weights to reweight these two variables:</p><formula xml:id="formula_27">Û = (w ⋅ 1 T ) ⊙ U,<label>(13)</label></formula><formula xml:id="formula_28">V = (w ⋅ 1 T ) ⊙ V,<label>(14)</label></formula><p>where '⊙' is the Hadamard product. Substituting Û and V into Eq. ( <ref type="formula" target="#formula_25">12</ref>), we obtain the weighted HSIC value:</p><formula xml:id="formula_29">ĤSIC k,l 0 (U, V, w) = (m -1) -2 tr( KP LP),<label>(15)</label></formula><p>where K, L ∈ R m×m are weighted RBF kernel matrices containing entries Kij = k( Ûi , Ûj ) and Lij = l( Vi , Vj ). Specif- ically, for treatment variable H ,0∶d and its corresponding multiple confounder variables {H ,d∶2d , ⋯, H ,(n L -1)d∶n L d }, we share the sample weights w across multiple confounders and propose to optimize w by:</p><formula xml:id="formula_30">w * = arg min w∈∆ m ∑ 1&lt;p&lt;n L ĤSIC k,l 0 (H ,0∶d , H ,(p-1)d∶pd , w), (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>where</p><formula xml:id="formula_32">∆ m = {w ∈ R m + | ∑ m i=1</formula><p>w i = m} is used to control the overall loss of each batch almost unchange, and we utilize w = softmax(w) to satisfy this constraint. Hence, reweighting training samples with the optimal w * can mitigate the dependence between high-level treatment variable with confounders to the greatest extent.</p><p>Global Multi-dimensional Variable Decorrelation. Note that the above method is to remove the correlation between a single treatment variable H ,0∶d with the confounders {H ,d∶2d , ⋯, H ,(n L -1)d∶n L d }. However, we need to estimate the causal effect of all the learned high-level representations {H ,0∶d , H ,d∶2d , ⋯, H ,(n L -1)d∶n L d }. As mentioned above, we need to set each high-level representation as a treatment variable and the remaining high-level representations as confounders, and remove the dependence between each treatment variable with the corresponding confounders. One effective way to achieve this goal is to remove all the dependence between variables. Specifically, we learn a set of sample weights that globally remove the dependence between each pair of high-level representations, defined as follows:</p><formula xml:id="formula_33">w * = arg min w∈∆ m ∑ 1≤i&lt;j≤n L ĤSIC k,l</formula><p>0 (H ,(i-1)d∶id , H ,(j-1)d∶jd , w).</p><p>(17) As we can see from Eq. ( <ref type="formula">17</ref>), the global sample weights w simultaneously reduce the dependence among all high-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weighted Graph Neural Networks</head><p>In the traditional GNN model, the parameters of the model are learned on the original graph dataset G = {G 1 , ⋯, G m }. Because the sample weights w learned by the causal variable distinguishing regularizer are capable of globally decorrelating the high-level variables, we propose to use the sample weights to reweight the GNN loss, and iteratively optimize sample weights w and the parameters of weighted GNN model as follows:</p><formula xml:id="formula_34">f (t+1) , g (t+1) = arg min f,g m ∑ i=1 w (t) i L(g(f (A i , F i )), y i ),<label>(18)</label></formula><formula xml:id="formula_35">w (t+1) = arg min w (t+1) ∈∆ m ∑ 1≤i&lt;j≤n L ĤSIC k,l 0 (H<label>(t+1)</label></formula><p>,(i-1)d∶id , H</p><p>,(j-1)d∶jd , w</p><formula xml:id="formula_37">(t) ),<label>(19)</label></formula><p>where f (⋅) is the representation part of our model and its output is the high-level representation H, H (t+1) = f (t+1) (A, F), t represents the iteration number, g(⋅) is a linear prediction layer, and L(⋅, ⋅) represents the loss function depends on which task we target. When updating the sample weights and the parameters of GNN model being fixed, we need to optimize the objective function Eq. <ref type="bibr" target="#b18">(19)</ref>. We update the sample weights by mature optimization technique, i.e., Adam.</p><p>After obtaining the sample weight of each graph in the batch, we optimize the objective function of weighted GNNs Eq. ( <ref type="formula" target="#formula_34">18</ref>) by an Adam optimizer <ref type="bibr" target="#b58">[59]</ref>. For the classification task, cross-entropy loss is used, and for the regression task, least squared loss is used. Initially, w (0) = (1, 1, ⋯, 1) T in each mini-batch. In the training phase, we iteratively optimize sample weights and model parameters with Eq. ( <ref type="formula" target="#formula_34">18</ref>) and <ref type="bibr" target="#b18">(19)</ref>.</p><p>During the inference phase, the predictive model directly conducts prediction based on the GNN model without any calculation of sample weights. The detailed procedure of our model is shown in Algorithm 1.</p><p>Although StableGNN still performs on dataset G, the weight w i of each graph is no longer same. This weight adjusts the contribution of each graph in the mini-batch loss, so that the GNN parameters are learned on the dataset that each high-level features are decorrelated 4 which can better learn the true correlation between relevant features and labels. Discussions. Our proposed framework, StableGNN, aims to relieve the distribution shifts problem by causal representation learning diagram in a general way. Specifically, a new causal representation learning for graphs that seamless integrates the power of representation learning and causal inference is proposed. For the representation learning part, we could utilize various state-of-the-art graph pooling layer <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> to extract high-level representations, nevertheless, the main intuition of our work is that we should learn high-level meaningful representation rather than meaningless mixed representation in our proposed learning diagram. This point is key for meaningful and effective causal learning, which is validated in Section 5.2.</p><p>Limitations. In our model, we assume a general causal variable relationships in Figure <ref type="figure" target="#fig_2">4</ref>. Nevertheless, for some datasets or tasks, there may exist more complicated causal relationships among high-level variables, hence discovering causal structure for these high-level variables may be useful for reconstructing the latent data generation process and achieving better generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Complexity Analysis.</head><p>As the proposed framework consists of two parts, we analyze them separately. For the graph high-level representation learning part, to cluster nodes, although it requires the additional computation of an assignment matrix, we observed that the Diffpool layer did not incur substantial additional running time in practice. The reason is that each DiffPool layer reduces the size of graphs 4. In this paper, we slightly abuse the term "decorrelate/decorrelation", which means removing both the linear and non-linear dependence among features unless specified.</p><p>by extracting a coarser high-level representation of the graph, which speeds up the graph convolution operation in the next layer. For CVD regularizer, given n L learned high-level representation, the complexity of computing HSIC value of each pair of high-level variables is O(m 2 ) <ref type="bibr" target="#b59">[60]</ref>, where m is the batch size. Hence, for each batch, the computation complexity of CVD regularizer is O(tn L (n L -1)m 2 ), where t is the number of epochs to optimize w and n L is a very small number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate our algorithm on both synthetic and real-world datasets, comparing with state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configurations.</head><p>In our experiments, the GNN layer used for DiffPool layer is built on top of the GraphSAGE <ref type="bibr" target="#b4">[5]</ref> or GCN <ref type="bibr" target="#b2">[3]</ref> layer, to demonstrate that our framework can be applied on top of different GNN models. We use the "max pooling" variant of GraphSAGE. One DiffPool layer is used for all the datasets and more sophisticated hierarchical layers could be learned by stacking multiple DiffPool layers. For our model, StableSAGE/StableGCN refers to using GraphSAGE/GCN as base model, respectively. All the models are trained with same learning rate mode and the model for prediction is selected based on the epoch with best validation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on Synthetic Data</head><p>To better verify the advantages of StableGNN on datasets with different degrees of distribution shifts between training set and testing set, we generate the synthetic datasets with a clear generation process so that the bias degree of datasets is controllable.</p><p>Dataset. We aim to generate a graph classification dataset that has a variety of distribution shifts from training dataset to testing dataset. Inspired by recent studies on GNN explanation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b60">[61]</ref>, we utilize motif as a subgraph of graphs. We first generate a base subgraph for each graph, that is, each positive graph has a "house"-structured network motif and each negative graph has a motif that is randomly drawn from 4 candidate motifs (i.e., star, clique, diamond and grid motifs). Hence, the "house" motif is the causal structure that causally determines the label. To inject spurious correlation, µ * 100% of positive graphs will be added "star" motif and the remaining positive and negative graphs will randomly add a non-causal motif from 4 candidate motifs. The node features are drawn from the same uniform distribution for all nodes. We set µ as {0.6, 0.7, 0.8, 0.9} to get four spurious correlation degrees for the training set. And we set µ = 0.5 to generate OOD validation set and µ = 0.25 to generate an unbiased testing dataset. The larger µ for the training set means there incurs a larger distribution shift between the training and testing sets. The resulting graphs are further perturbed by adding edges from the vertices of the first motif to one of the vertices of the second motif in each graph with the probability 0.25. The number of training samples is 2000, and for validation and testing set is 1000.</p><p>Experimental Settings and Metrics. As the synthetic data has a known generating mechanism and our model is based on the GraphSAGE/GCN, to clearly validate and explain the effectiveness of our framework helping base GNN get rid of spurious correlation, in this subsection, we only compare with the base models. The number of layers of GraphSAGE and GCN is set as 5. The dropout rate for all the methods is set as 0.0. For all the GNN models, an initial learning rate is set as 1 × 10 -3 , the reduce factor is 0.5, and the patience value is 5. And the learning rate of CVD regularizer is selected from {0.1, 0.3, ⋯, 1.3}. To baselines, the training epoch is set as 50. For Stable-SAGE/GCN, we set 20 epochs to warm up the parameters, i.e., training without the CVD regularizer, and 30 epochs to train the whole model. The decorrelation epoch to learn sample weights is set as 50. For all the methods, we take the model of the epoch with the best validation performance for prediction. The batch size is set as 250. The maximum number of clusters (i.e., high-level representations) is set as 7 for StableSAGE and 8 for StableGCN. For all the baseline models, if not mentioned specially, we aggregate node embeddings by mean pooling readout function to get the graph-level representation. Following <ref type="bibr" target="#b14">[15]</ref>, we augment each GNN layer with batch normalization (BN) <ref type="bibr" target="#b61">[62]</ref> and residual connection <ref type="bibr" target="#b62">[63]</ref>. We evaluate the methods with three widely used metrics for binary classification, i.e., Accuracy, F1 score and ROC-AUC <ref type="bibr" target="#b63">[64]</ref>. For all the experiments, we run 4 times with different random seeds and report their mean and standard error of prediction value with the corresponding metric on the test set in percent.</p><p>Results on Synthetic Data. The results are given in Table <ref type="table" target="#tab_7">2</ref>, and we have the following observations. First, both the GraphSAGE and GCN suffer from serious performance decrease with the increase of spurious correlation degree, e.g., for F1 score, GraphSAGE drops from 67.83 to 54.24, and GCN drops from 67.06 to 62.28, indicating that spurious correlation greatly affects the GNNs' generalization performance and the heavier distribution shifts will cause a larger performance decrease. Second, compared with the base model, our proposed models achieve up to 19.64% performance improvements, and gain larger improvements under heavier distribution shifts. As we know the "house" motif is decisive for the label, the only way to improve the performance is to utilize this causal subgraph, demonstrating that our models could significantly reduce the influence of spurious correlation among subgraphs and reveal the true relationship between causal subgraphs with labels. Third, when building our framework both on GraphSAGE and GCN, our framework could achieve consistent improvements, and it indicates that StableGNN is a general framework and has the potential to adapt to various GNN architectures. Explanation Analysis. An intuitive type of explanation for GNN models is to identify subgraphs that have a strong influence on final decisions <ref type="bibr" target="#b0">[1]</ref>. To demonstrate whether the model focuses on the relevant or irrelevant subgraphs while conducting prediction, we utilize GNNExplainer <ref type="bibr" target="#b0">[1]</ref> to calculate the most important subgraph with respect to GNN's prediction and visualize it with red color. As GN-NExplainer needs to compute an edge mask for explanation and GraphSAGE cannot be aware of the edge weights, we explain the GCN-based model, i.e., GCN and StableGCN. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, we find the following interesting cases contributing to the success of our model, where each case represents a kind of failure easily made by existing methods.</p><p>• Case 1. As we can see, GCN assigns the higher weights to "star" motif, however, StableGCN concentrates more on "house" motif. Although GCN could make correct predictions based on the "star" motif, which is highly correlated with "house" motif, this prediction is unstable and it means that if there incurs spurious correlations in the training set, the model could learn the spurious correlation and rely on this clue for predictions. This unstable prediction is undesirable, as the unstable structure may change during the wild testing environments.</p><p>• Case 2. In this case, there is a "grid" motif connecting with "house" motif. As we can see, GCN pays more attention on "grid" motif and StableGCN still concentrates on "house" motif. Due to the existence of spurious correlated subgraphs, it will reduce the confidence of the true causal subgraph for prediction. When there appears another irrelevant subgraph, GCN may also pay attention on this irrelevant subgraph, leading to incorrect prediction results. However, our model could focus on the true causal subgraphs regardless of which kind of subgraphs are associated with them.</p><p>• Case 3. This is a case for negative samples. The spurious correlation leads to the GCN model focusing on the "star" motif. As the "star" motif is correlated with the positive label, GCN model will predict this graph as a positive graph. In contrast, due to the decorrelation of subgraphs in our model, we find that the "star" motif may not discriminate to the label decision and "diamond" motif may attribute more to the negative labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Real-world Datasets</head><p>In this section, we apply our StableGNN algorithm on eight real-world datasets for out-of-distribution graph property prediction. Datasets. We adopt seven molecular property prediction datasets from OGB Datasets <ref type="bibr" target="#b5">[6]</ref>. All the molecules are pre-processed using RDKit <ref type="bibr" target="#b64">[65]</ref>. Each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. We use the 9-dimensional node features and 3dimensional edges features provided by <ref type="bibr" target="#b5">[6]</ref>, which has better generalization performance. The task of these datasets is to predict the target molecular properties, e.g., whether a molecule inhibits HIV virus replication or not. Input edge features are 3-dimensional, containing bond type, bond stereochemistry as well as an additional bond feature indicating whether the bond is conjugated. Depending on the properties of molecular, these datasets can be categorized into three kinds of tasks: binary classification, multi-label classification and regression. Different from commonly used random splitting, these datasets adopt a scaffold splitting <ref type="bibr" target="#b65">[66]</ref> procedure that splits the molecules with different scaffolds into training or testing sets. All the molecules with the same scaffold could be treated as an environment, and the scaffold splitting attempts to separate molecules with different scaffolds into different subsets. For example, two molecules, Cyclopropanol (C3H6O) and 1,4-Cyclohexanediol (C6H12O2), contain different scaffold patterns: the former scaffold is 3C-ring and the latter is 6C-ring. Although sampled with different distributions, they are both readily soluble in water due to the invariant subgraph hydroxy (-OH) attached to different scaffolds <ref type="bibr" target="#b66">[67]</ref>. The environments of training and testing sets are different, resulting in different graph distributions. Therefore, this kind of data could be used to test whether the model could leverage the causal subgraph to make predictions, i.e., the generalization ability of GNNs on graphs with different distributions. Moreover, we also conduct the experiments on a commonly used graph classification dataset, MUTAG <ref type="bibr" target="#b67">[68]</ref>, as we could explain the results based on the knowledge used in <ref type="bibr" target="#b49">[50]</ref>. It consists of 4,337 molecule graphs. Each graph is assigned to one of 2 classes based on its mutagenic effect. Note that this dataset cannot adopt the scaffold splitting, as the +4 valence N atom, which commonly exists in this dataset, is illegal in the RDKit <ref type="bibr" target="#b68">[69]</ref> tool used for scaffold splitting. We just use the random splitting for this dataset, however, we still believe there are some OOD cases in the testing set. The splitting ratio for all the datasets is 80/10/10. The detailed statistics are shown in Table <ref type="table" target="#tab_9">3</ref>.</p><p>Baselines. As the superiority of GNNs against traditional methods on graph-level tasks, like kernel-based methods <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, has been demonstrated by previous literature <ref type="bibr" target="#b7">[8]</ref>, here, we mainly consider baselines based upon several related and state-of-the-art GNNs.</p><p>• Base models: GraphSAGE <ref type="bibr" target="#b4">[5]</ref> and GCN <ref type="bibr" target="#b2">[3]</ref> are classical GNN methods. We utilize them as base models in our framework, so they are the most related baselines to validate the effectiveness of the proposed framework.</p><p>• DIFFPOOL <ref type="bibr" target="#b7">[8]</ref>: It is a hierarchical graph pooling method in an end-to-end fashion. It adopts the same model architecture and hyperparamter setting with high-level variable learning components in our framework, except that DIFFPOOL model aggregates the clusters' representation by summation operation rather than an ordered concatenation operation to generate the final high-level representations.</p><p>• GAT <ref type="bibr" target="#b3">[4]</ref>: It is an attention-based GNN method, which learns the edge weights by an attention mechanism.</p><p>• GIN <ref type="bibr" target="#b71">[72]</ref>: It is a graph isomorphism network that is as powerful as the WL graph isomorphism test. We compare with two variants of GIN, i.e., whether the ϵ is a learnable parameter or a fixed 0 scalar, denoted as GIN and GIN0, respectively.</p><p>• MoNet <ref type="bibr" target="#b72">[73]</ref>: It is a general architecture to learn on graphs and manifolds using the bayesian gaussian mixture model.</p><p>• SGC <ref type="bibr" target="#b73">[74]</ref>: It is a simplified GCN-based method, which reduces the excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers.</p><p>• JKNet <ref type="bibr" target="#b74">[75]</ref>: It is a GNN framework that selectively combines different aggregations at the last layer.</p><p>• DIR <ref type="bibr" target="#b32">[33]</ref>: It is a GNN method designed for the distribution shift problem, which disentangles the casual and non-causal subgraphs.</p><p>• CIGA <ref type="bibr" target="#b33">[34]</ref>: It is a GNN method that learns causally invariant representations for OOD generalization on graphs.</p><p>Experimental Settings and Metrics. Here, we only describe the experimental settings that are different from Section 5.1. For all GNN baselines, we follow <ref type="bibr" target="#b5">[6]</ref> which set the number of layers as 5. The dropout rate after each layer for all the methods on the datasets for binary classification and multilabel classification is set as 0.5, and for regression datasets, the dropout rate is set as 0.0. The number of batch size for binary classification and multi-label classification is set as 128, and for regression datasets, the batch size is set as 64.</p><p>The training epoch for all the baselines is set as 200. And for our model, we utilize 100 epochs to warm up and 100 epochs to train the whole model. The maximum number of clusters is set as 7 for our models and DIFFPOOL. For DIR and CIGA, the hyperparameter s c the selection ratio of casual subgraph is chosen from {0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, and the hyperparameters α and β for contrastive loss and hinge loss of CIGA are both chosen from {0.5, 1, 2, 4, 8, 16, 32} according to the validation performances. Moreover, as the loss of DIR and CIGA are specifically designed for the multi-classification task, they cannot perform on the datasets with multi-label classification and regression tasks. As these datasets usually treat chemical bond type as their edge type, to include edge features, we follow <ref type="bibr" target="#b5">[6]</ref> and add transformed edge features into the incoming node features.</p><p>For the datasets from OGB, we use the metrics recommended by the original paper for each task <ref type="bibr" target="#b5">[6]</ref>. And following <ref type="bibr" target="#b0">[1]</ref>, we adopt Accuracy for MUTAG dataset.     Comparison with Different Decorrelation Methods. As there may be other ways to decorrelate the variables in GNNs by sample reweighting methods, one question arises naturally: is our proposed decorrelation framework a more suitable strategy for graphs? To answer this question, we compare the following alternatives:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on</head><p>• GraphSAGE-Decorr: This method directly decorrelates each dimension of graph-level representations learned by GraphSAGE. The results are shown in Table <ref type="table" target="#tab_11">5</ref>. Compared with these potential decorrelation methods, StableGNN achieves better results consistently, demonstrating that decorrelating the representations by the cluster-level granularity is a more suitable strategy for graph data. Moreover, we find that GraphSAGE-Decorr/StableGNN-NoCVD-Decorr shows worse performance than GraphSAGE/StableGNN-NoCVD, and the reason is that if we aggressively decorrelate single dimension of embeddings, it will inevitably break the intrinsic semantic meaning of original data. Furthermore, StableGNN-NoCVD-Distent forces the high-level representations to be disentangled, which changes the semantic implication of features, while StableGNN learns sample weights to adjust the data structure while the semantics of features are not affected. Overall, StableGNN is a general and effective framework compared with all the potential ways.</p><p>Convergence Rate and Parameter Sensitivity Analysis. We first analyze the convergence rate of our proposed causal variable distinguishing regularizer. We report the summation of HSIC value of all high-level representation pairs and the difference of learned weights between two consecutive epochs during the weights learning procedure in one batch on three relatively smaller datasets in Figure <ref type="figure" target="#fig_5">7</ref>. As we can see, the weights learned by CVD regularizer could achieve convergence very fast while reducing the HSIC value significantly. In addition, we study the sensitivity of the number of high-level representations and report the performance of StableGNN-NoCVD and StableGNN based on the same pre-defined number of clusters in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>StableGNN outperforms StableGNN-NoCVD on almost all cases, which well demonstrates the robustness of our methods with the number of pre-defined clusters and the effectiveness of the proposed CVD regularizer. Note that our framework could learn the appropriate number of clusters in an end-to-end way, i.e., some clusters might not be used   by the assignment matrix.</p><p>Interpretability Analysis. In Figure <ref type="figure" target="#fig_9">9</ref>, we investigate explanations for graph classification tasks on MUTAG dataset. In the MUTAG example, colors indicate node features, which represent atoms. StableGNN correctly identifies chemical NO 2 and NH 2 , which are known to be mutagenic <ref type="bibr" target="#b49">[50]</ref> while baselines fail in. These cases demonstrate that our model could utilize more interpretable structures for prediction. Moreover, the difference of StableGCN-NoCVD between StableGCN indicates the necessity of integrating two components in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we study a practical but seldom studied problem: generalizing GNNs on out-of-distribution graph data. We analyze the problem in a causal view that the generalization of GNNs will be hindered by the spurious correlation among subgraphs. To improve the stability of existing methods, we propose a general causal representation learning framework, called StableGNN, which integrates graph high-level variable representation learning and causal effect estimation in a unified framework. Extensive experiments well demonstrate the effectiveness, flexibility, and interpretability of the StableGNN.</p><p>In addition, we believe that this paper just opens a direction for causal representation learning on graphs. As the most important contribution, we propose a general framework for causal graph representation learning: graph highlevel variable representation learning and causal variable distinguishing, which can be flexibly adjusted for specific tasks. For example, besides molecules, we could adjust our framework to learn the causal substructures of proteins. The substructures of proteins play a crucial causal role in determining their properties and functions. Different substructures can confer distinct functionalities and characteristics on the proteins. To have a broader impact, we believe the idea could also spur causal representation learning in other areas, like object recognition <ref type="bibr" target="#b75">[76]</ref>, multi-modal data fusion <ref type="bibr" target="#b76">[77]</ref>, and automatic driving in wild environments <ref type="bibr" target="#b77">[78]</ref>.</p><p>62322203, 62172052, 62002029, 62141607, U1936219). This work was also supported in part by National Key R&amp;D Program of China (No. 2018AAA0102004).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The overall architecture of the proposed StableGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Multiple multi-dimensional confounder balancing framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Causal view on GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Explanation cases of GCN and StableGCN. Red nodes are the important subgraph calculated by the GNNExplainer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Ablation study on three real-world tasks.</figDesc><graphic coords="13,85.42,48.68,141.72,74.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Convergence rate analysis of CVD regularizer.</figDesc><graphic coords="13,64.16,267.78,155.90,103.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><label></label><figDesc>StableGNN-NoCVD-Decorr: This method decorrelates each dimension of concatenated high-level representation H learned by StableGNN-NoCVD. • StableGNN-NoCVD-Distent: This method forces the high-level representation learned by StableGNN-NoCVD to be disentangled by adding a HSIC regularizer to the overall loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Sensitivity of the pre-defined maximum number of clusters.</figDesc><graphic coords="14,295.90,215.64,128.23,96.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Explanation instance for MUTAG dataset. The blue, green, red and yellow colors represent N, H, O, C atoms, respectively. The top important subgraphs selected by GNNExplainer are viewed in black color (keep the top 6 important edges). The picture is best viewed in color.</figDesc><graphic coords="14,420.66,305.14,129.75,97.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with the Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing 100876, China, and the Department of Computer Science and Technology in Tsinghua University, Beijing 100084, China.</figDesc><table /><note><p><p><p>E-mail: fanshaohua@bupt.cn • X.</p>Wang  </p>is with the School of Software, Beihang University, Beijing, 100191, China. Email: xiao_wang@buaa.edu.cn • C. Shi, and B. Wang are with the Department of Computer Science, Beijing University of Posts and Telecommunications, Beijing 100876, China. Email: {shichuan,wangbai}@bupt.edu.cn. • P. Cui is with the Department of Computer Science and Technology in Tsinghua University, Beijing 100084, China. E-mail: cuip@tsinghua.edu.cn. Manuscript received Nov 17, 2021; revised May 1, 2023; revised Sep 15, 2023;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>Glossary of notations.</figDesc><table><row><cell cols="3">Notations Description</cell></row><row><cell cols="2">G train /G test</cell><cell>Training/Testing graphs</cell></row><row><cell></cell><cell>A</cell><cell>Adjacency matrix</cell></row><row><cell></cell><cell cols="2">F Feature matrix</cell></row><row><cell></cell><cell cols="2">S Cluster assignment matrix</cell></row><row><cell></cell><cell>Z</cell><cell>Node representation matrix</cell></row><row><cell></cell><cell>H</cell><cell>High-level variable representation matrix</cell></row><row><cell></cell><cell>P</cell><cell>Permutation matrix</cell></row><row><cell></cell><cell>T</cell><cell>Treatment variable</cell></row><row><cell></cell><cell>Y</cell><cell>Label/prediction variable</cell></row><row><cell></cell><cell>X</cell><cell>Confounder variable</cell></row><row><cell>X</cell><cell>(p)</cell><cell>The p-th high-level confounder variable</cell></row><row><cell>X</cell><cell>(p)</cell><cell>The p-th high-level confounder matrix</cell></row><row><cell></cell><cell>w</cell><cell>Sample/graph weights</cell></row><row><cell cols="3">corresponding label, the task is to learn a GNN model h θ (⋅) with</cell></row><row><cell cols="3">parameter θ to precisely predict the label of testing graphs G test ,</cell></row><row><cell cols="3">where the distribution Ψ(G train ) ≠ Ψ(G test ). And in the OOD</cell></row><row><cell cols="3">setting, we do not know the distribution shifts from training graphs</cell></row><row><cell cols="3">to unseen testing graphs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>where h i is the concatenated high-level representation of sample i, stack(⋅) means the stacking operation by the row axis, and H i, means the high-level representation for the i-th graph in the mini-batch. H</figDesc><table /><note><p><p><p><p><p>i,(k-1)d∶kd means k-th high-level representation of i-th graph. Hence, to prove the semantic alignment of any two high-level representations H i, and H j, , we need to demonstrate that the semantic meanings of H i,(k-1)d∶kd and H j,(k-1)d∶kd are aligned for all k ∈ [1, n L ].</p>To this end, we first prove the permutation invariant property of DiffPool layer.</p>Lemma 1. Permutation Invariance</p><ref type="bibr" target="#b7">[8]</ref></p>. Given any permutation matrix P ∈ {0, 1} n×n , if P ⋅ GNN(A, F) = GNN(PAP T , PF)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Optimize sample weights w via Eq. (19); Back propagate with weighted GNN loss to update f and g via Eq. (18);</figDesc><table><row><cell cols="2">Algorithm 1: Training process of StableGNN</cell></row><row><cell></cell><cell>Input: Training graphs</cell></row><row><cell></cell><cell>G train = {(G 1 , y 1 ), ⋯, (G N , y N )};</cell></row><row><cell></cell><cell>Training Epoch:Epoch;</cell></row><row><cell></cell><cell>Decorrelation Epoch: DecorEpoch;</cell></row><row><cell></cell><cell>Output: Learned GNN model;</cell></row><row><cell cols="2">1 while t &lt; Epoch do</cell></row><row><cell>2</cell><cell>for 1 to BatchNumber do</cell></row><row><cell>3</cell><cell>Forward propagation to generate H;</cell></row><row><cell>6</cell><cell>end</cell></row><row><cell>7</cell><cell></cell></row><row><cell>8</cell><cell>end</cell></row><row><cell cols="2">9 end</cell></row></table><note><p><p><p>4</p>for 1 to DecorEpoch do</p><ref type="bibr" target="#b4">5</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 :</head><label>2</label><figDesc>Results on synthetic datasets in different settings. The 'improvements' means the improvement percent of StableSAGE/StableGCN against GraphSAGE/GCN.</figDesc><table><row><cell>Correlation Degree (µ)</cell><cell>Method</cell><cell>Accuracy</cell><cell>F1 score</cell><cell>ROC-AUC</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>69.68±0.91</cell><cell>67.83±1.41</cell><cell>77.49±0.71</cell></row><row><cell>0.6</cell><cell>StableSAGE</cell><cell>73.93±0.66</cell><cell>73.05±1.21</cell><cell>79.22±1.07</cell></row><row><cell></cell><cell>Improvements</cell><cell>6.10%</cell><cell>7.70%</cell><cell>2.23%</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>67.85±0.76</cell><cell>63.76±1.32</cell><cell>75.55±1.01</cell></row><row><cell>0.7</cell><cell>StableSAGE</cell><cell>73.9±1.78</cell><cell>71.23±1.88</cell><cell>81.48±4.13</cell></row><row><cell></cell><cell>Improvements</cell><cell>8.92%</cell><cell>11.71%</cell><cell>7.85%</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>65.67±1.22</cell><cell>60.23±0.81</cell><cell>72.70±1.45</cell></row><row><cell>0.8</cell><cell>StableSAGE</cell><cell>72.15±1.26</cell><cell>68.56±0.87</cell><cell>81.35±2.12</cell></row><row><cell></cell><cell>Improvements</cell><cell>9.86%</cell><cell>13.83%</cell><cell>9.98%</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>65.2±0.94</cell><cell>54.24±1.98</cell><cell>72.89±0.67</cell></row><row><cell>0.9</cell><cell>StableSAGE</cell><cell>70.35±1.66</cell><cell>64.84±2.54</cell><cell>80.31±1.78</cell></row><row><cell></cell><cell>Improvements</cell><cell>7.90%</cell><cell>19.54%</cell><cell>10.18%</cell></row><row><cell></cell><cell>GCN</cell><cell>70.98±0.93</cell><cell>67.06±2.95</cell><cell>77.55±0.55</cell></row><row><cell>0.6</cell><cell>StableGCN</cell><cell>74.92±1.91</cell><cell>73.91±2.49</cell><cell>81.79±2.42</cell></row><row><cell></cell><cell>Improvements</cell><cell>5.56%</cell><cell>10.21%</cell><cell>5.47%</cell></row><row><cell></cell><cell>GCN</cell><cell>70.9±1.45</cell><cell>65.57±3.69</cell><cell>78.27±1.53</cell></row><row><cell>0.7</cell><cell>StableGCN</cell><cell>73.15±2.62</cell><cell>70.29±2.76</cell><cell>79.77±3.42</cell></row><row><cell></cell><cell>Improvements</cell><cell>3.17%</cell><cell>7.198%</cell><cell>1.92%</cell></row><row><cell></cell><cell>GCN</cell><cell>70.35±0.50</cell><cell>65.41±0.85</cell><cell>75.76±1.09</cell></row><row><cell>0.8</cell><cell>StableGCN</cell><cell>74.5±1.03</cell><cell>70.94±1.69</cell><cell>81.53±0.86</cell></row><row><cell></cell><cell>Improvements</cell><cell>5.90%</cell><cell>8.45%</cell><cell>7.62%</cell></row><row><cell></cell><cell>GCN</cell><cell>69.68±0.56</cell><cell>62.28±1.38</cell><cell>76.61±0.66</cell></row><row><cell>0.9</cell><cell>StableGCN</cell><cell>76.35±1.37</cell><cell>72.64±2.62</cell><cell>83.24±0.58</cell></row><row><cell></cell><cell>Improvements</cell><cell>9.57%</cell><cell>16.63%</cell><cell>8.65%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The experimental results on eight datasets are presented in Table4, and we have the following observations. First, comparing with these competitive GNN models, Stable-SAGE/GCN achieves 6 rank one and 2 rank two on all eight datasets. And the average rank of StableSAGE and StableGCN are 1.75 and 2.87, respectively, which is much higher than the third place, i.e., 4.38 for MoNet.</figDesc><table><row><cell>It means that most existing GNN models cannot perform well</cell></row><row><cell>on OOD datasets and our model significantly outperforms</cell></row><row><cell>existing methods, which well demonstrates the effectiveness</cell></row><row><cell>of the proposed causal representation learning framework.</cell></row><row><cell>Second, compared with the base models, i.e., GraphSAGE</cell></row><row><cell>and GCN, our models achieve consistent improvements</cell></row><row><cell>on all datasets, validating that our framework could boost</cell></row><row><cell>the existing GNN architectures. Third, Stable-SAGE/GCN</cell></row><row><cell>also outperforms DIFFPOOL method by a large margin.</cell></row><row><cell>Although we utilize the DiffPool layer to extract high-level</cell></row><row><cell>representations in our framework, the seamless integration</cell></row></table><note><p>Real-world Datasets. of representation learning and causal learning is the key to the improvements of our model. Fourth, Stable-SAGE/GCN outperforms DIR and CIGA, showing the effectiveness of our model over them on OOD generalization problem on graphs. And DIR and CIGA need to set a hyperparameter for the selection ratio of casual subgraph for all graphs, where the selection ratio is fixed and the same for all graphs and it</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 :</head><label>3</label><figDesc>Summary of real-world datasets.</figDesc><table><row><cell>Dataset</cell><cell>Task Type</cell><cell>#Graphs</cell><cell>Average #Nodes</cell><cell>Average #Edges</cell><cell>#Task</cell><cell>Splitting Type</cell><cell>Metric</cell></row><row><cell>Molbace</cell><cell>Binary classification</cell><cell>1,513</cell><cell>34.1</cell><cell>36.9</cell><cell>1</cell><cell>Scaffold splitting</cell><cell>ROC-AUC</cell></row><row><cell>Molbbbp</cell><cell>Binary classification</cell><cell>2,039</cell><cell>24.1</cell><cell>26.0</cell><cell>1</cell><cell>Scaffold splitting</cell><cell>ROC-AUC</cell></row><row><cell>Molhiv</cell><cell>Binary classification</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>1</cell><cell>Scaffold splitting</cell><cell>ROC-AUC</cell></row><row><cell>MUTAG</cell><cell>Binary classification</cell><cell>4,337</cell><cell>30.32</cell><cell>30.77</cell><cell>1</cell><cell>Random splitting</cell><cell>Accuracy</cell></row><row><cell>Molclintox</cell><cell>Multi-label classification</cell><cell>1,477</cell><cell>26.2</cell><cell>27.9</cell><cell>2</cell><cell>Scaffold splitting</cell><cell>ROC-AUC</cell></row><row><cell>Moltox21</cell><cell>Multi-label classification</cell><cell>7,831</cell><cell>18.6</cell><cell>19.3</cell><cell>12</cell><cell>Scaffold splitting</cell><cell>ROC-AUC</cell></row><row><cell>Molesol</cell><cell>Regression</cell><cell>1,128</cell><cell>13.3</cell><cell>13.7</cell><cell>1</cell><cell>Scaffold splitting</cell><cell>RMSE</cell></row><row><cell>Mollipo</cell><cell>Regression</cell><cell>4,200</cell><cell>27.0</cell><cell>29.5</cell><cell>1</cell><cell>Scaffold splitting</cell><cell>RMSE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc>Performance of real-world graph datasets. The number in the (⋅) along with each performance number means the rank of the method among all the methods on this dataset. Because the losses of DIR and CIGA are designed for binary/multi-classifcation task, they cannot perform on the datasets with multilabel classification and regression tasks. Hence, we only show the results of them on binary classification task and do not rank them. "↑" means that for this metric, the larger value means better performance. "↓" means that for this metric, the smaller value means better performance. Best results of all methods are indicated in bold.</figDesc><table><row><cell>results or better results with DIFFPOOL method. The only</cell></row><row><cell>difference between them is that DIFFPOOL model averages</cell></row><row><cell>the learned clusters' representations and StableGNN-NoCVD</cell></row><row><cell>concatenates them by a consistent order, and then the aggre-</cell></row><row><cell>gated embeddings are fed into a classifier. As the traditional</cell></row><row><cell>MLP classifier needs the features of all samples should in</cell></row><row><cell>a consistent order, the superior performance of StableGNN-</cell></row><row><cell>NoCVD well validates that the learned representations</cell></row><row><cell>H are in a consistent order across graphs. Moreover, we</cell></row><row><cell>find that StableGNN consistently outperforms StableGNN-</cell></row><row><cell>NoCVD. As the only difference between the two models is</cell></row><row><cell>the CVD term, we can safely attribute the improvements</cell></row><row><cell>to the distinguishing of causal variables by our proposed</cell></row></table><note><p>regularizer. Note that on some datasets, we could find that StableGNN-NoDVD cannot achieve satisfying results, however, when combined with the regularizer, it makes clear improvements. This phenomenon further validates the necessity of each component that we should conduct</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5 :</head><label>5</label><figDesc>Performance of different decorrelation methods.</figDesc><table><row><cell>Method</cell><cell cols="6">Molhiv (↑) Molbace (↑) Molbbbp (↑) MUTAG (↑) Molclintox (↑) Moltox21 (↑)</cell><cell>Molesol (↓)</cell><cell>Mollipo (↓)</cell></row><row><cell>GraphSAGE</cell><cell>75.78±2.19</cell><cell>78.51±1.72</cell><cell>66.16±0.97</cell><cell>79.78±0.7</cell><cell>88.60±2.44</cell><cell>68.88±0.59</cell><cell cols="2">1.8098±0.1220 0.7911±0.0147</cell></row><row><cell>GraphSAGE-Decorr</cell><cell>76.52±0.69</cell><cell>77.34±5.63</cell><cell>66.28±0.89</cell><cell>80.76±1.32</cell><cell>86.51±0.82</cell><cell>68.77±0.66</cell><cell cols="2">1.7889±0.1234 0.8024±0.0165</cell></row><row><cell>StableGNN-NoCVD</cell><cell>76.47±1.01</cell><cell>79.65±0.86</cell><cell>66.73±1.87</cell><cell>80.13±1.80</cell><cell>87.56±1.91</cell><cell>68.63±0.63</cell><cell>1.0819±0.0219</cell><cell>0.6971±0.017</cell></row><row><cell>StableGNN-NoCVD-Decorr</cell><cell>75.32±0.52</cell><cell>78.71±2.34</cell><cell>67.02±1.55</cell><cell>79.17±1.07</cell><cell>88.89±2.58</cell><cell>69.05±0.66</cell><cell cols="2">1.0377±0.0389 0.7171±0.0378</cell></row><row><cell cols="2">StableGNN-NoCVD-Distent 75.76±0.64</cell><cell>79.04±1.24</cell><cell>64.21±1.34</cell><cell>80.92+0.50</cell><cell>90.44±0.79</cell><cell>68.87±1.28</cell><cell cols="2">1.0729±0.0256 0.7171±0.0378</cell></row><row><cell>StableGNN</cell><cell>77.63±0.79</cell><cell>80.73±3.98</cell><cell>68.47±2.47</cell><cell>82.13±0.32</cell><cell>90.96±1.93</cell><cell>69.14±0.24</cell><cell>1.022±0.0039</cell><cell>0.6971±0.0297</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>Note that, for simplicity, in the following studies we mainly conduct analysis on StableSAGE, and StableGCN will get similar results. StableGNN will refer to StableeSAGE unless mentioned specifically.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U20B2045</rs>, <rs type="grantNumber">62192784</rs>,</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tngeVzH">
					<idno type="grant-number">U20B2045</idno>
				</org>
				<org type="funding" xml:id="_by5TtSb">
					<idno type="grant-number">62192784</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>is hard to set in practice. Fifth, Stable-SAGE/GCN achieves superior performance on datasets with three different tasks and a wide range of dataset scales, indicating that our proposed framework is general enough to be applied to datasets with a variety of properties.</p><p>Ablation Study. Note that our framework naturally incorporates high-level representation learning and causal effect estimation in a unified framework. Here we conduct ablation studies to investigate the effect of each component. For our framework without the CVD regularizer, we term it as StableGNN-NoCVD. 5 The results are presented in Figure <ref type="figure">6</ref>.</p><p>We first find that StableGNN-NoCVD outperforms Graph-SAGE on most datasets, indicating that learning hierarchical structure by our model for graph-level tasks is necessary and effective. Second, StableGNN-NoCVD achieves competitive </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explainability methods for graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">781</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A pac-bayesian approach to generalization bounds for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TEC</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Test-time training with self-supervision for generalization under distribution shifts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. PMLR</title>
		<meeting>Int. Conf. Mach. Learn. PMLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Le</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep stable learning for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5372" to="5382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical generation of molecular graphs using structural motifs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4839" to="4848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal domain adaptive network embedding for node classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4022" to="4030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metapathguided heterogeneous graph neural network for intent recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2478" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond homophily: Reconstructing structure for graph-agnostic clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">One2multi graph autoencoder for multi-view graph clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="3070" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-ofdistribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for outof-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24" to="934" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The risks of invariant risk minimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Does invariant risk minimization capture invariance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tangella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4069" to="4077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to learn single domain generalization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">565</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="11" to="749" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06256</idno>
		<title level="m">Learning robust representations by projecting superficial statistics out</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stable prediction across unknown environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4485" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Identification and estimation of local average treatment effects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Angrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="467" to="475" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causally regularized learning with agnostic data selection bias</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="411" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stable learning via sample reweighting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5692" to="5699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7092" to="7101" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsiya</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hainmueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="46" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stable weights that balance covariates for estimation with incomplete outcome data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zubizarreta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">511</biblScope>
			<biblScope unit="page" from="910" to="922" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Approximate residual balancing: De-biased inference of average treatment effects in high dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07125</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Supervised feature selection via dependence estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="823" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with hilbert-schmidt norms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Counterfactual prediction for bundle treatment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Feature selection via dependence maximization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S B</forename><surname>Normalization</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Using auc and accuracy in evaluating learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">RDKit: Open-source cheminformatics</title>
		<ptr target="https://www.rdkit.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graph neural networks with multiple feature extraction paths for chemical property estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecules</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3125</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A software suite for cheminformatics, computational chemistry, and predictive modeling</title>
		<author>
			<persName><surname>Rdkit</surname></persName>
		</author>
		<ptr target="http://www.rdkit.org/RDKit_Overview.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Models of object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1199" to="1204" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Survey on deep multi-modal data analytics: Collaboration, rivalry, and fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
