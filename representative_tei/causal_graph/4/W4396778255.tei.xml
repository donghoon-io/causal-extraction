<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ALCM: Autonomous LLM-Augmented Causal Discovery Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-04-16">16 Apr 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elahe</forename><surname>Khatibi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mahyar</forename><surname>Abbasian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongqi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iman</forename><surname>Azimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><forename type="middle">M</forename><surname>Rahmani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Nursing</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ALCM: Autonomous LLM-Augmented Causal Discovery Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-16">16 Apr 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.01744v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large Language Models</term>
					<term>Causal Reasoning</term>
					<term>Causal Graph</term>
					<term>Causal Discovery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NPhard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The process of causal discovery, essential in various domains and scientific discoveries, seeks to reveal complex causal relationships in observational data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>. For instance, in healthcare, this process is crucial and instrumental for pinpointing disease etiologies, devising effective interventions, and prevention strategies <ref type="bibr" target="#b48">[49]</ref>. Subsequently, causal inference allows for the quantification of the influence exerted by different variables on one another, once a causal structure is identified. This phase, often referred to as causal estimation, relies on the construction of a preliminary causal Furthermore, we implement an automatic pipeline for making the causal discovery an automatic task. Our contributions are as follows:</p><p>Our contributions in this work are as follows:</p><p>• Unified Framework for Enhanced Causal Discovery: We introduce the ALCM framework that synergistically integrates the strengths of conventional data-driven causal discovery (CCD) methods and Large Language Models (LLMs) to overcome the limitations of individual approaches by generating accurate, interpretable, and comprehensive causal graphs.</p><p>• Dynamic, Scalable, and Autonomous Operations: ALCM demonstrates adaptability to dynamic data environments, autonomous operation without domain expertise dependency, and scalability across unseen datasets.</p><p>• Improved Predictive Precision and Reliability: Leveraging the contextual reasoning capabilities of LLMs, the framework refines causal relationships with algorithmic rigor.</p><p>• Comprehensive Graph Representation and Explainability: ALCM provides a fully automated pipeline for constructing and refining causal graphs. It ensures interpretability and explainability of results.</p><p>• Benchmarking and Performance Validation: We extensively evaluate ALCM across multiple benchmark datasets, demonstrating its superior performance compared to the related works.</p><p>This work advances the field of causal discovery by demonstrating the transformative potential of a unified framework that synthesizes the complementary capabilities of CCD and LLM-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this section, we outline the existing research on causal structure learning within the literature, delineating it into two primary groups: 1) Conventional data-driven causal discovery algorithms; and 2) Using LLMs for causal discovery.</p><p>1) Conventional data-driven causal discovery algorithms: conventional data-driven causal discovery algorithms are broadly classified into five categories as follows:</p><p>• Score-Based Algorithms: They operate on scores and engage in a comprehensive exploration of the entire space of potential Directed Acyclic Graphs (DAGs) to identify the most suitable graph for explaining the underlying data. Typically, such score-based approaches consist of two integral components: (i) a systematic search strategy tasked with navigating through the potential search states or the space of candidate graphs, denoted as G', and (ii) a score function designed to evaluate the viability of these candidate causal graphs. The synergy between the search strategy and the score function is instrumental in optimizing the exploration of all conceivable DAGs. A widely employed score function in the selection of causal models is the Bayesian Information Criterion (BIC) <ref type="bibr" target="#b13">[14]</ref>. Some examples of scorebased algorithms are Greedy Equivalence Search (GES) <ref type="bibr" target="#b10">[11]</ref>, Fast Greedy Search (FGS) <ref type="bibr" target="#b32">[33]</ref>, and A* Search <ref type="bibr" target="#b47">[48]</ref>.</p><p>• Constraint-Based Algorithms: This category, exemplified by Peter-Clark (PC) <ref type="bibr" target="#b37">[38]</ref> algorithm, employs conditional independence (CI) tests to reveal the graph's skeleton and vstructures, ultimately returning the Directed Acyclic Graph (DAG) of the functional causal model while considering v-structures and doing edge-orientations <ref type="bibr" target="#b13">[14]</ref>. Other constraint-bsaed algorithms are like Fast Causal Inference (FCI), Anytime FCI, RFCI, PC-stable, and so forth.</p><p>• Hybrid Algorithms: Hybrid approaches are founded on the integration of various causal discovery methods, combining constraint-based, score-based, Functional Causal Model (FCM)based, gradient-based, and other techniques. This amalgamation reflects a comprehensive strategy that leverages the strengths of different methodologies to enhance the robustness and effectiveness of causal discovery in complex systems. Max-Min Hill Climbing (MMHC) <ref type="bibr" target="#b39">[40]</ref>belonging to this category-stands out as a hybrid causal discovery technique that seamlessly integrates principles from both score-based and constraint-based algorithms. This hybrid approach combines the advantages of scoring methods and constraint-based strategies, offering a comprehensive and effective framework for uncovering causal relationships in complex systems.</p><p>• Function-Based Algorithms: Approaches grounded in Functional Causal Models (FCM) delineate the causal connections between variables within a defined functional structure. In FCMs, variables are expressed as functions of their direct causes (parents), augmented by an independent noise term denoted as E. The distinguishing feature of FCM-based methodologies lies in their capacity to differentiate between various Directed Acyclic Graphs (DAGs) within the same equivalence class. This discrimination is achieved by introducing supplementary assumptions concerning data distributions and/or function classes. Several notable FCM-based causal discovery methodologies are introduced, including Linear Non-Gaussian Acyclic Model (LiNGAM) <ref type="bibr" target="#b36">[37]</ref> and Structural Agnostic Modeling (SAM) <ref type="bibr" target="#b18">[19]</ref>. SAM employs an adversarial learning methodology for causal graph identification. Specifically, SAM utilizes Generative Adversarial Neural Networks (GANs) to seek a Functional Causal Model (FCM) while ensuring the detection of sparse causal graphs through the incorporation of appropriate regularization terms. The optimization process involves a learning criterion that integrates distribution estimation, sparsity considerations, and acyclicity constraints. This holistic criterion facilitates end-to-end optimization of both the graph structure and associated parameters, accomplished through stochastic gradient descent.</p><p>The previous three-mentioned categories may be limited to the Markov equivalence class, posing constraints. Function-based algorithms like LiNGAM <ref type="bibr" target="#b43">[44]</ref> aim to uniquely identify causal DAGs by exploiting data generative process asymmetries or causal footprints.</p><p>• Optimization-Based Algorithms: Recent investigations in causal discovery have approached the structure learning problem by casting it as a continuous optimization task, employing the least squares objective and an algebraic representation of Directed Acyclic Graphs (DAGs). Notably, this transformation converts the combinatorial nature of the structure learning problem into a continuous framework, and solutions are obtained through the application of gradient-based optimization techniques. These methods exploit the gradients of an objective function concerning the parameterization of a DAG matrix to achieve effective structure learning. NOTEARS <ref type="bibr" target="#b50">[51]</ref> is among the causal discovery algorithms that formulate the structure learning problem as a purely continuous constrained optimization task.</p><p>2) Using LLM for causal discovery task: Leveraging recent advancements in LLMs and Natural Language Processing (NLP) presents an opportunity to offer enhanced capabilities in capturing causal concepts and relations while handling large-scale datasets more effectively <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>. This proficiency is rooted in the extensive training LLMs undergo on vast, high-quality datasets <ref type="bibr" target="#b17">[18]</ref>. LLMs possess the ability to establish a comprehensive knowledge base across diverse domains, facilitating language understanding, ensuring generalizability, automating the causal reasoning pipeline, and enabling plausible reasoning. In this regard, the second group, namely using LLMs for causal discovery, is introduced. This group is classified into three major groups as follows:</p><p>• Fine-tuning: This category mainly focuses on fine-tuning LLMs to empower LLMs with causal-and-effect knowledge and address the causal reasoning challenges <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref>. For instance, Jin et al. <ref type="bibr" target="#b16">[17]</ref> introduce the CORR2CAUSE benchmark dataset on which they fine-tune their model. This is done to both asses and empower LLMs with causal reasoning ability. In fact, CORR2CAUSE dataset serves as a tool to evaluate the proficiency of LLMs in discerning causal relationships, particularly when the LLMs are fine-tuned to distinguish causation from correlational statements in the context of NLP.</p><p>• Performance Evaluation: The second category focuses on using LLM for causal discovery and delves into emerging research that explores the causal analysis capabilities of Large Language Models. In contrast to causal discovery algorithms relying on statistical patterns in the data, this group utilizes LLMs to discover causal structures from variables. A majority of these methods solely utilize LLMs to predict pairwise causal relationships among a given set of variables <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>• Prior or Posterior Knowledge: In the third category, focused on employing LLMs, the objective is either to assign direction to undirected edges generated by causal discovery algorithms or to impose constraints on the edge orientation and functionality of these algorithms. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Despite these efforts from conventional data-driven causal discovery algorithms to propose robust, precise, adaptable, efficient, and scalable causal discovery algorithms, encountered limitations and inefficiencies persist. These challenges are as follows. 1) Real-world data, often sparse and insufficient for accurately capture authentic probability distributions <ref type="bibr" target="#b5">[6]</ref>. 2) Sole reliance on precollected static data introduces accuracy risks, particularly when models must adapt to dynamic real-world data and unforeseen factors. 3) Inferring complete edge orientations from observed data is hindered by the existence of equivalent Directed Acyclic Graphs (DAGs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>. 4) Algorithm dependence on domain knowledge experts, who may be scarce, time/resource-intensive, or exhibit variable quality across domains <ref type="bibr" target="#b11">[12]</ref>. 5) Traditional causal discovery algorithms fall short in answering user-submitted causal questions due to a lack of proficiency in language understanding and processing. These challenges collectively contribute to diminished accuracy, incompleteness, and unreliability in the estimated causal graph.</p><p>On the other hand, significant advances have been made in utilizing LLMs for causal tasks. However, their inherent limitations in precision and complexity handling remain evident. These challenges are highlighted as follows. 1) LLMs inherently lack the precision necessary for accurately responding to complex, user-generated causal queries <ref type="bibr" target="#b40">[41]</ref>. 2) LLMs are limited in their ability to dissect and comprehend nuanced causal concepts without additional data-driven causal reasoning algorithms. 3) There is a challenge in constructing complete causal graphs and unraveling intricate causal relations due to the oversimplified understanding of LLMs. 4) LLMs struggle with handling extensive datasets, often failing to capture the depth and variability within them. These issues collectively hinder the effectiveness of LLMs in accurately and reliably determining causal relationships. Consequently, data-driven causal reasoning algorithms assume a critical role in mitigating the limitations of LLMs in causal tasks, offering nuanced comprehension of causal concepts, unraveling intricate causal relations, constructing complete causal graphs, and handling extensive datasets.</p><p>In light of these considerations, a unified, comprehensive causal framework that integrates LLMs with data-driven conventional causal discovery algorithms is required. To address this need, we propose the development of ALCM. ALCM aims to enhance the robustness and accuracy of causal discoveries by leveraging the conventional causal discovery algorithms and LLMs.</p><p>Table <ref type="table" target="#tab_0">1</ref> indicates the capabilities of two distinct causal discovery methods-Conventional datadriven Causal Discovery (CCD), LLMs-based approaches, and ALCM framework-across essential functional attributes. Dynamic Data Adaptability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">50]</ref> is the capability of a method to adjust to changing data, while Detection of Hidden Variables <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref> refers to identifying unobserved influencers within the dataset. Comprehensive Graph Model Representation <ref type="bibr" target="#b5">[6]</ref> assesses the completeness of the depicted causal structure, and Predictive Accuracy <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> measures the success in forecasting the correct causal relations. CCD methods are limited by their reliance on pre-defined statistical models as well as domain knowledge expert validation, lacking adaptability to dynamic data, generalizability <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref> to unseen data, autonomy, and lack of accuracy. Similarly, while LLMs are adept at dynamicity of data, generalizability, and detecting hidden variables, they fall short in providing comprehensive graph model representations, interpretability, explainability, autonomy, and precision for causal discovery task. ALCM combining these strengths while enhancing user independence from expert validation <ref type="bibr" target="#b19">[20]</ref> and interpretability <ref type="bibr" target="#b7">[8]</ref> in causal discovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Framework</head><p>In this section, we present ALCM, an advanced causal discovery framework aimed to leverage the combined strengths of traditional causal discovery algorithms and LLMs. ALCM provide an automated pipeline constructing a comprehensive causal graph, refining it, and incorporating previously overlooked insights to enrich the resulting causal model. This integration aims to utilize the precision of conventional causal discovery algorithms in identifying data relationships, while also enhancing and validating these findings with insights from LLMs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Structure Learning</head><p>The Causal Structure Learning component serves as the foundational data-driven module of the ALCM framework, responsible for generating the initial causal graph from observational datasets. This component identifies causal relationships among variables by analyzing probabilistic dependencies and independencies in the data, leveraging well-established conventional causal discovery methods. These methods typically infer causal graphs by estimating relationships between variables (nodes) and their potential causal links (edges), using statistical tests to determine conditional independencies and distinguish direct relationships from those mediated by other variables. Additionally, orientation rules are applied to establish causal directions, guided by assumptions such as </p><formula xml:id="formula_0">G i ← G i ∪ ∅ ▷ If valid, retain the edge. 6: end if 7:</formula><p>if z orientation is revised by LLM-Driven Refiner then ▷ Step 4: Adjust the edge direction if required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><formula xml:id="formula_1">G i ← z ′ ∪ G i 9: end if 10:</formula><p>if z is removed by LLM-Driven Refiner then ▷ Step 5: Remove the edge if deemed invalid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><formula xml:id="formula_2">G i ← G i -z ′ 12: end if 13:</formula><p>if a new edge z ′′ is added by LLM-Driven Refiner then ▷ Step 6: Add a new edge identified by the LLM.</p><p>14:</p><formula xml:id="formula_3">G i ← z ′′ ∪ G i 15:</formula><p>end if 16: end for 17: Return G i acyclicity or specific properties of the data distribution. Optimization techniques, including scoring functions-based methods (e.g., Bayesian Information Criterion (BIC) or likelihood-based scores), might be used to refine graph structures and select the most plausible causal model. To enhance robustness, the causal structure learning component integrates complementary methods to account for diverse data characteristics, such as linearity, non-linearity, or Gaussianity. The output of this component is an initial causal graph encapsulating key variables (nodes) and their inferred causal relationships (edges). The causal structure learning component directly influences the accuracy and reliability of both the final causal graph and future causal inferences drawn from the data.</p><p>For the implementation, we utilize three established causal discovery algorithms-Peter-Clark (PC) <ref type="bibr" target="#b37">[38]</ref>, Linear Non-Gaussian Acyclic Model (LiNGAM) <ref type="bibr" target="#b36">[37]</ref>, and Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning (NOTEARS) <ref type="bibr" target="#b50">[51]</ref>-each chosen for their distinct strengths and complementary characteristics.</p><p>The PC algorithm leverages conditional independence (CI) tests to construct a graph's skeleton and identify v-structures, offering a robust framework for causal inference when the underlying relationships can be uncovered through probabilistic dependencies. Its ability to handle discrete and continuous variables effectively makes it a reliable choice for datasets where independence testing plays a central role. LiNGAM, on the other hand, excels at uncovering linear causal relationships in datasets with non-Gaussian distributions. By utilizing Independent Component Analysis (ICA) for causal ordering, LiNGAM demonstrates superior performance in disentangling complex linear interactions. Its focus on exploiting the statistical properties of non-Gaussianity ensures that causal directions are accurately inferred, even in the presence of latent confounders. Complementing these approaches, NOTEARS offers a novel optimization-based framework that reformulates the combinatorial problem of DAG discovery into a continuous optimization task. By incorporating an acyclicity constraint into its objective function, NOTEARS efficiently learns causal structures while maintaining scalability to larger datasets. Its gradient-based methodology makes it particularly adept at handling high-dimensional data with intricate causal dependencies.</p><p>Building on the unique strengths of these three algorithms, we propose a hybrid method that combines PC, LiNGAM, and NOTEARS within a unified framework. This hybrid approach employs a majority-weighted voting mechanism to leverage the individual advantages of each algorithm dynamically. The weighting is determined based on their relative performance on specific datasets, ensuring that the final causal graph benefits from their collective expertise. This integration enhances the robustness and reliability of the causal discovery process, allowing the hybrid method to adapt to diverse data characteristics.</p><p>The causal structure learning component synthesizes an initial causal graph by combining the outputs of these algorithms, encapsulating the potential causal linkages identified from the dataset. This graph, which represents the key variables (nodes) and their inferred causal relationships (edges), is subsequently passed to the Causal Wrapper component for further contextualization and refinement, enabling downstream tasks to operate on a well-defined and accurate causal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Wrapper</head><p>The Causal Wrapper component serves as a critical intermediary or bridge between the causal structure learning and LLM-driven refiner components. This component encapsulates and translates the raw, initial causal graph into a series of contextual, causal-aware prompts (i.e., causal prompts). These prompts are fed to the LLM-driven refiner. The primary aim of these causal prompts is to act as guides for the LLM-driven refiner, aiding it in comprehending the initial causal graph. Furthermore, these causal prompts direct the LLM-driven refiner to identify and integrate the relevant and updated causal background knowledge to make the solution more suited to the specific causal discovery problem at hand. Given these reasons, this prompting strategy ensures that the final causal graph is not only precise, but also robust and reflective of the underlying causal mechanisms within the dataset.</p><p>Equation 1 shows our causal-aware prompting strategy by infusing the context of problem and metadata information into the prompts. This prompting strategy was inspired by an effort by Kim et al. <ref type="bibr" target="#b20">[21]</ref>. They demonstrated that contextual information is important in boosting the overall performance of LLMs' responses.</p><formula xml:id="formula_4">Causalprompt = Instruction + Causal Context + Metadata + Question + Output format (1)</formula><p>This enhancement is accomplished by incorporating explicit elements into the prompt, with each edge being transformed into a causal prompt structured as follows: Instructions: This section clarifies the role of LLMs, their objectives, and the expected behavior. Causal Context: It includes details about the selected causal discovery algorithm, such as its name and output. Metadata: This section outlines the dataset domain or variable names along with their descriptions. Question: It specifies the precise query, for example, whether A causes B. Output format: This delineates the desired format for the output. Figure <ref type="figure" target="#fig_2">2</ref> illustrates an example of the causal wrapper's functionality. The causal structure learning component generates the initial causal graph by applying conventional causal discovery algorithms, such as Peter-Clark (PC), LiNGAM, or NOTEARS. These algorithms analyze the input observational dataset to uncover key variables (nodes) and their probabilistic dependencies, forming the skeleton of the initial graph. The nodes in this graph represent significant variables derived from the dataset, while the directed edges illustrate potential causal relationships. This foundational graph is then passed to the causal wrapper and subsequently refined by the LLM-driven refiner. Notably, the output can incorporate supplementary reasoning and confidence metrics to enhance interpretability. For instance, a simple instruction can prompt the LLM-driven refiner to engage in step-by-step reasoning using a Chain-of-Thought (CoT) approach <ref type="bibr" target="#b45">[46]</ref>. Additionally, it can request the LLM to quantify its confidence level or provide a likelihood estimate for the generated outputs, using log-likelihood values or confidence percentages.</p><p>These prompts are critical for ensuring that the LLM comprehends the initial graph and integrates relevant causal knowledge effectively. For example, they direct the LLM to identify hidden causal relationships or validate existing edges by leveraging its contextual reasoning capabilities. This interaction facilitates the enhancement of the initial graph into a more accurate and robust representation of the underlying causal mechanisms. Once these causal prompts are generated, they are dispatched to the LLM-driven refiner component. This method ensures that the ALCM framework optimally utilizes LLMs for uncovering, refining, and validating causal relationships, thereby advancing the field of causal discovery with a high level of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LLM-driven Refiner</head><p>The LLM-driven Refiner leverages advanced language models in the refinement and evaluation of causal graphs. This component receives a series of intricately designed, contextual causal prompts from the causal wrapper component, which serve as a nuanced guide for its operations.  The LLM-driven Refiner evaluates each edge and node in the graph by applying advanced reasoning capabilities of LLMs (e.g., GPT-4). The process involves:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Causal Graph</head><note type="other">Cancer</note><p>1. Assessing the validity of existing causal relationships using contextual knowledge.</p><p>2. Detecting and integrating hidden causal relationships by reasoning over unobserved variables.</p><p>3. Reorienting or removing edges that do not align with domain knowledge or probabilistic dependencies.</p><p>4. Assigning confidence scores or likelihood estimates to refined relationships, ensuring interpretability and reliability.</p><p>The LLM-driven Refiner verifies hidden causal relationships by leveraging advanced capabilities of LLMs to assess, validate, and refine the initial causal graph. This process begins with the causal prompts generated by the Causal Wrapper, which provide the LLM with explicit instructions, contextual metadata, and domain-specific information about the causal relationships. The LLM evaluates each edge in the graph based on its internal knowledge base and the provided context, determining whether the relationship is valid, needs reorientation, or should be removed. Furthermore, the LLM identifies potential hidden relationships by reasoning over unobserved variables and interactions that conventional algorithms may overlook. To ensure accuracy, the refined relationships are accompanied by confidence scores or likelihood estimates, enabling a structured and interpretable refinement process. Finally, these refined graphs are validated through crossreferencing with up-to-date domain knowledge, ensuring that the final output is both accurate and comprehensive.</p><p>The significance of the LLM-driven Refiner lies in its capacity to address and alleviate inherent limitations present in both the causal discovery algorithms and the datasets themselves. This component plays a pivotal role in uncovering and assimilating previously overlooked or concealed causal information, thereby elevating the accuracy and comprehensiveness of the causal graph. The identification and integration of hidden causal relationships into the graph are essential, as they can reveal causal connections or nodes that traditional causal discovery methods might miss or that dataset constraints could obscure. Upon completion of the refinement process, the results are saved, and various post-processing techniques are applied to generate the final graph. These techniques involve leveraging natural language processing (NLP) to parse and extract causal relationships from textual responses provided by LLMs. Subsequently, these extracted relationships undergo validation and structuring to form a coherent causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Interactions Between Components</head><p>The Causal Structure Learning component generates the initial graph, providing a data-driven foundation. The Causal Wrapper transforms this graph into contextualized prompts, enabling the LLM to reason about relationships in a guided and structured manner. The LLM-driven Refiner refines and validates the graph, identifying hidden relationships and ensuring alignment with external knowledge. This iterative feedback loop ensures that the final causal graph is both accurate and interpretable, addressing limitations in traditional causal discovery methods and leveraging the strengths of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We elucidate the technical underpinnings and strategic choices behind the deployment of the ALCM framework. We provide two demonstrations of implementation of our framework to show that our framework can enhance the accuracy and generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation 1 (ALCM-PC)</head><p>For the first implementation, we select the PC causal discovery algorithm for its robustness in handling large datasets and its efficiency in inferring causal structures through conditional independence (CI) tests. The algorithm constructs an undirected graph and iteratively removes edges by testing CI between variable pairs, conditioned on subsets of other variables, a process known as skeleton discovery. It then orients edges by detecting v-structures (triplets of nodes with specific dependency patterns) and ensuring acyclicity, resulting in a DAG that represents the causal relationships. The PC algorithm's ability to prune unnecessary connections makes it particularly effective for high-dimensional datasets, balancing computational efficiency with accuracy. For the causal wrapper component, we utilize causal prompt. We illustrate one example of our prompt in Figure <ref type="figure" target="#fig_3">3</ref>. For LLM-driven refiner, we exploit OpenAI GPT-4 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref> in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer:</head><p>Given the context of neuropathic pain and causal discovery, the output from the PC (Peter and Clark) algorithm suggesting that 'L Wrist pain' causes 'R Shoulder pain' warrants careful consideration.</p><p>... Based on the standard understanding of neuropathic pain pathways and without additional context justifying this causal relationship, the answer to the correctness of the algorithm's output would be: False.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt:</head><p>Presuming your expertise lies in diagnosing neuropathic pain and the realm of causal discovery, consider the scenario where you are presented with the findings from a causal discovery algorithm (PC) that has been applied to a "neuropathic dataset". The algorithm deduces that 'L Wrist pain' (indicating left wrist pain) causes 'R Shoulder pain' (denoting right shoulder pain), with "R" and "L" symbolizing the right and left sides of the body, respectively. Leveraging your current, in-depth understanding of this field, you are asked to evaluate and amend this conclusion if necessary. Additionally, you should classify the algorithm's output as True or False to indicate whether it is correct or incorrect based on your assessment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALCM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation 2 (ALCM-Hybrid)</head><p>For the second implementation, we present a hybrid causal discovery approach that integrates the strengths of three leading methods: Peter-Clark (PC) <ref type="bibr" target="#b37">[38]</ref>, LiNGAM <ref type="bibr" target="#b36">[37]</ref>, and NOTEARS <ref type="bibr" target="#b50">[51]</ref>. These methods address different aspects of causal structure learning, and their combination provides a robust and accurate framework for causal discovery.</p><p>The PC method employs conditional independence (CI) tests to iteratively construct a causal graph by building its skeleton and identifying v-structures. This method is particularly effective for datasets with mixed discrete and continuous variables and excels in capturing probabilistic dependencies. Its iterative and constraint-based nature ensures computational efficiency, even in high-dimensional settings. In contrast, LiNGAM is specifically designed to uncover linear causal relationships in datasets with non-Gaussian distributions. By leveraging Independent Component Analysis (ICA), LiNGAM accurately identifies causal ordering and orients edges with high precision, even in the presence of latent confounders and linear dependencies. NOTEARS complements these approaches by reformulating causal discovery into a continuous optimization problem. By incorporating a differentiable acyclicity constraint, NOTEARS transforms the combinatorial problem of DAG discovery into a solvable optimization task, making it highly effective for datasets with intricate causal dependencies and scalable to high-dimensional data.</p><p>To leverage the unique strengths of these methods, we propose a hybrid approach that combines their outputs using dynamically assigned weights. These weights are determined based on a composite score for each method, which captures its performance on a given dataset. The composite score is defined as the difference between the Accuracy and NHD, balancing edge-specific performance and structural alignment with the ground truth. Formally, the composite score for a method is given by:</p><formula xml:id="formula_5">Composite method = Accuracy method -NHD method<label>(2)</label></formula><p>This score accounts for both the overall correctness of edge identification (via Accuracy) and the structural similarity of the causal graph (via NHD), ensuring that methods achieving both accurate and well-aligned graphs are given higher importance. The weights are derived by normalizing the composite scores across all methods:</p><formula xml:id="formula_6">W method = Composite method all methods Composite method<label>(3)</label></formula><p>where W method represents the weight assigned to a method, ensuring that the sum of all weights equals one.</p><p>To further enhance the adaptability of the hybrid approach, we introduce a neural networkbased architecture to dynamically learn these weights based on both method performance metrics and dataset-specific features. The neural network is designed to take as input the composite scores of the methods, along with features such as graph density, node degree distribution, and sparsity. Graph density quantifies how connected the graph is and is defined as the ratio of the number of edges to the maximum possible edges. Node degree distribution describes the variability in the number of connections per node, while sparsity measures the proportion of missing edges compared to a fully connected graph.</p><p>The architecture of the neural network consists of three layers: 1. An input layer with nine features, including the composite scores of the methods (Composite PC , Composite LiNGAM , Composite NOTEARS ) and six dataset-specific features such as graph density, average node degree, and sparsity. 2. Two hidden layers with 64 and 32 neurons, respectively, each using the Rectified Linear Unit (ReLU) activation function to capture non-linear relationships among the features. 3. An output layer with three neurons (one for each method), using a softmax activation to produce normalized weights for the methods. Formally, the neural network outputs the weights as follows:</p><formula xml:id="formula_7">W = Softmax(H 2 • W o + b o ) (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where H 2 represents the outputs from the second hidden layer, W o and b o are the weights and biases of the output layer, and Softmax ensures the weights sum to one. The neural network is trained on a dataset comprising simulated graphs with varying densities, node degrees, and sparsity levels. For each graph, the outputs of PC, LiNGAM, and NOTEARS are evaluated using Accuracy and NHD, and the composite scores are computed. The ground-truth weights for training are derived by normalizing these composite scores as described in Equation ( <ref type="formula" target="#formula_6">3</ref>). The training objective minimizes the mean squared error (MSE) between the predicted weights and the ground-truth weights. Using the dynamically learned weights, the hybrid approach synthesizes a causal graph by aggregating the outputs of PC, LiNGAM, and NOTEARS. For each edge e, the final score is computed as:</p><formula xml:id="formula_9">Score e = method W method • I method(e)<label>(5)</label></formula><p>Here, I method(e) is an indicator function that equals 1 if the edge e is identified by the method and 0 otherwise. Edges with scores exceeding a predefined threshold are retained in the hybrid causal graph. For edges uniquely identified by only one method, one LLM is employed as a decisive layer. The LLM evaluates these edges based on contextual knowledge and causal reasoning to ensure that only plausible causal links are included. The validated edges are then added to the hybrid graph, enhancing its comprehensiveness and accuracy.</p><p>The resulting hybrid causal graph is passed to the Causal Wrapper component, where it is further contextualized and refined using domain-specific templates and LLM-driven reasoning. This ensures that the final causal graph is robust, accurate, and adaptable to diverse data characteristics. By combining the strengths of traditional causal discovery methods with the adaptability of neural networks and the reasoning capabilities of LLMs, the hybrid approach achieves superior performance in causal discovery, setting a new benchmark for accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first present benchmark datasets used in our expermients. Next, we outline the evaluation metrics selected to measure the framework's performance in terms of accuracy, robustness, and reliability. Finally, we summarize the experimental results, demonstrating the effectiveness of the ALCM framework in generating and refining causal graphs, and its ability to reveal latent causal relationships, showcasing its advancement over existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Datasets</head><p>We utilize six benchmark datasets and their ground truth causal graphs from the BN repository: Asia, Cancer, Child, Insurance, Sachs, Sangiovese <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref>, and also the well-known Neuropathetic dataset <ref type="bibr" target="#b41">[42]</ref> to evaluate the efficacy of the ALCM framework. These datasets are chosen for their diverse origins and complexities, covering a range of scenarios from medical studies to insurance modeling and genetic pathways. The importance of utilizing these benchmark datasets lies in their ability to provide a standardized basis for comparison, enabling the assessment of the ALCM framework's performance across varied domains and conditions. Table <ref type="table" target="#tab_2">2</ref> indicates a summary of these datasets. To ensure these datasets are compatible with the input requirements of causal discovery algorithms within the ALCM framework, we implement a series of preprocessing techniques as part of the causal structure learning component. This preprocessing involves cleaning the data, handling missing values, and normalizing data formats, among other adjustments, to tailor the datasets for optimal processing. By meticulously preparing these datasets, we facilitate their effective use as inputs for the causal discovery algorithms, ensuring that the initial causal graphs generated are as accurate and informative as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We select five metrics to assess the effectiveness and precision of the ALCM framework's causal discovery capabilities. The evaluation of the predicted causal graphs against the ground truth is paramount to validate the accuracy and reliability of our methodology. To this end, we employ five key metrics: precision, recall, F1-score, accuracy, and Normalized Hamming Distance (NHD), each selected for its ability to provide a comprehensive understanding of the framework's performance from different perspectives <ref type="bibr" target="#b48">[49]</ref>.</p><p>• Precision: measures the proportion of correctly identified causal relationships out of all relationships identified by the algorithm. This metric is crucial for ensuring that the causal links proposed by our framework are indeed valid, minimizing false positives.</p><p>• Recall: assesses the fraction of true causal relationships that have been correctly identified by the algorithm, highlighting the framework's ability to uncover the full extent of causal connections present within the data.</p><p>• F1-score: serves as a harmonic mean of precision and recall, offering a single metric that balances both the accuracy and completeness of the identified causal relationships. This is particularly useful for comparing the overall performance of different causal discovery approaches.</p><p>• Accuracy: evaluates the overall correctness of the causal graph, including both the presence of true causal connections and the absence of false ones. This metric provides a straightforward assessment of the model's overall predictive performance.</p><p>• Normalized Hamming Distance (NHD): quantifies the difference between the predicted causal graph and the ground truth by measuring the proportion of mismatched edges, adjusted for the size of the graph. NHD is instrumental in assessing the structural similarity of the causal graphs, offering insights into the nuanced differences that may not be captured by other metrics. In the context of a graph with m nodes, the NHD between the predicted graph G p and the ground-truth graph G is determined by calculating the number of edges that exist in one graph but not the other. This count is then divided by the total number of all possible edges-this formula is defined in Equation <ref type="formula" target="#formula_10">6</ref>. In essence, the NHD provides a normalized measure of dissimilarity, offering insights into the accuracy of the predicted graph compared to the ground-truth graph, accounting for the total potential edges in the graph with m nodes.</p><formula xml:id="formula_10">N HD = m i=1 m j=1 1 m 2 • 1, where G ij ̸ = G pij .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>In this section, we present the experimental results and a comprehensive analysis of the ALCM framework's performance compared to various causal discovery methods. These evaluations were conducted using seven benchmark datasets and five evaluation metrics: precision, recall, F1-score, accuracy, and Normalized Hamming Distance (NHD). The comparison encompasses traditional causal discovery methods (PC, LiNGAM, NOTEARS), a hybrid method, and LLM-based approaches. Additionally, the ALCM-PC and ALCM-Hybrid implementations, described in Sections 4.1 and 4.2, are included to demonstrate the benefits of integrating conventional algorithms with advanced refinement mechanisms. Table <ref type="table" target="#tab_3">3</ref> highlights the significant improvements achieved by the ALCM framework across all datasets. ALCM-PC and ALCM-Hybrid consistently outperform other methods in precision, recall, F1-score, and accuracy, while also achieving the lowest NHD values, which indicate a closer alignment with the ground truth causal graph. ALCM-Hybrid demonstrates the highest accuracy and F1-scores across all datasets, outperforming ALCM-PC and other methods due to its ability to integrate multiple causal discovery paradigms and incorporate LLM-driven contextual refinement. ALCM-PC, which builds upon the PC method as its backbone, also performs robustly by leveraging LLM-based refinements to improve accuracy and reduce structural mismatches in the resulting causal graphs.</p><p>The experimental results show that LLM-based approaches, while capable of identifying novel causal relationships, tend to exhibit lower precision and higher NHD values. This highlights a tendency of such approaches to overgeneralize causal relationships from input data, which may result in the identification of spurious edges. Conversely, traditional methods like PC, LiNGAM, and NOTEARS demonstrate varying levels of performance. PC is effective for simpler datasets, such as Asia, where probabilistic dependencies are more straightforward to infer. However, it struggles with more complex datasets, such as Neuropathic and Sachs, which involve intricate causal dependencies. LiNGAM, tailored for linear, non-Gaussian relationships, performs well on datasets adhering to its assumptions but exhibits higher NHD values and lower precision on datasets with more diverse causal structures. NOTEARS provides scalable and efficient causal discovery but also faces limitations in capturing complex interactions in highly nonlinear datasets.</p><p>The hybrid approach (ALCM-Hybrid) capitalizes on the unique strengths of PC, LiNGAM, and NOTEARS by combining them in a weighted majority voting framework. This method dynamically assigns weights to the contributions of each algorithm based on dataset-specific characteristics, enabling it to adapt to varying causal structures. The integration of these dynamically learned weights ensures that ALCM-Hybrid achieves robust and reliable causal graph construction, as evidenced by its superior performance in metrics such as precision (up to 0.95) and accuracy (up to 98.18%). Furthermore, the incorporation of LLMs in the ALCM framework provides an additional layer of contextual reasoning and domain-specific validation. This is particularly beneficial for datasets with intricate causal dependencies, such as Sachs and Neuropathic, where conventional algorithms alone may fail to capture the nuanced relationships between variables. By blending algorithmic rigor with AI-driven insights, the ALCM framework establishes a new benchmark for accuracy and robustness in causal discovery.</p><p>These results indicate the transformative potential of combining traditional causal discovery algorithms with LLM-driven enhancements. The ALCM framework not only addresses the limitations of existing methodologies but also demonstrates its capacity to provide reliable, interpretable, and accurate causal graphs for diverse and complex datasets. The results presented in Table <ref type="table" target="#tab_4">4</ref> provide a detailed evaluation of the ALCM-Hybrid framework when integrated with five prominent LLMs, namely GPT-4, Llama3.1-8B, Llama3.1-70B, Gemma2-9B, and Ministral-7B, across seven benchmark datasets. This analysis complements the broader comparison shown in Table <ref type="table" target="#tab_3">3</ref>, focusing specifically on the metrics of accuracy and Normalized Hamming Distance (NHD) to evaluate predictive reliability and structural alignment of causal graphs with the ground truth. As expected, GPT-4 consistently achieves the highest accuracy and lowest NHD values across all datasets, demonstrating its ability to leverage extensive pre-trained knowledge for accurate causal inference. For instance, in the Asia dataset, GPT-4 achieves an accuracy of 96.55% with an NHD of 0.0179, significantly outperforming smaller models. The performance trends reveal that as model complexity and size decrease, there is a gradual decline in accuracy and an increase in NHD. Smaller models, such as Ministral-7B, while computationally efficient, exhibit limitations in capturing intricate causal dependencies, particularly in complex datasets like Cancer and Sachs. For example, in the Cancer dataset, Ministral-7B achieves an accuracy of 87.50</p><p>Llama3.1-70B and Gemma2-9B demonstrate competitive performance, approaching GPT-4's accuracy while maintaining slightly higher NHD values, indicating room for improvement in structural fidelity. The Insurance dataset, in particular, showcases consistent performance trends across all models, suggesting that even less complex LLMs can achieve reasonable results when the dataset complexity is moderate. Despite these observations, GPT-4's superior performance across all datasets highlights the benefits of advanced reasoning capabilities and large-scale pre-training in enhancing causal discovery tasks.</p><p>Overall, the results show the adaptability and robustness of the ALCM-Hybrid framework when paired with varying LLMs. While smaller models like Ministral-7B offer computational efficiency, they trade off precision and structural fidelity, making them less suitable for tasks requiring high accuracy. The synergy between ALCM-Hybrid and LLMs ensures robust causal inference, even when using less powerful models. These findings provide valuable insights into selecting the appropriate LLM for specific causal discovery tasks, balancing resource constraints and performance requirements. Table <ref type="table" target="#tab_4">4</ref> should be placed immediately after Section 5.3 to maintain a logical flow of causal reasoning and validation that is absent in traditional approaches, while the hybrid model further capitalizes on this by combining algorithmic precision with AI's contextual insights. This strategic amalgamation ensures that our framework is at the forefront of causal discovery, setting a new benchmark for accuracy, comprehensiveness, and applicability in the field. We also visualize the additive contributions of each causal discovery framework in Figures <ref type="figure">5</ref> and<ref type="figure">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results for Adding New Nodes or Edges</head><p>The extensive updated knowledge and expert supervision provided by LLMs significantly facilitate the identification of elusive variables (Markov blanket) and causal connections. These might remain undetected or in the dataset or overlooked by causal discovery algorithms. Figure7a and 7b show these capability of unmasking these hidden aspects. As Figure <ref type="figure">7a</ref> demonstrates, the causal discovery algorithm (PC) fails to detect all of the true nodes and edges, but ALCM can provide us with new nodes or edges that not present in the output set of causal discovery algorithm as illustrated in 7b. We prompted LLMs to provide us the confidence level for its responses as well. The validity of ALCM answer is also confirmed by the up-to-the-date medical articles, including <ref type="bibr" target="#b27">[28]</ref>.</p><p>The traditional causal discovery depends on the structured dataset and their quality which are curated and annotated by human experts. However, these dataset are neither available in a wide range of domains or can be generalize to the new tasks. Hence, we empower ALCM by viture of LLMs component with this capability to uncover hidden variables and causal connections. Figure <ref type="figure">8</ref> indicates the ALCM capability to entangle the hidden variables and causal relations which are not present in the dataset.</p><p>Figure <ref type="figure">8</ref> illustrates the confidence levels associated with the causal relationships refined or discovered by the ALCM framework. These confidence levels are provided by the LLM-driven refiner and represent the degree of certainty for each modification or addition to the causal graph. For instance, in Figure <ref type="figure">8</ref>, confidence scores are categorized as "Very High," "High," or "Moderate," reflecting the LLM's assessment based on domain knowledge and available data. These scores play  a critical role in validating the plausibility of newly added nodes and edges, such as "Smoking → Lung Cancer" and "Environmental Factors → Tuberculosis and Lung Cancer," which traditional causal discovery algorithms might overlook. The use of confidence levels ensures that the refined causal graph not only aligns with existing domain knowledge but also achieves a higher level of interpretability and reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>While the ALCM framework demonstrates significant advancements in leveraging LLMs for causal discovery, several limitations need to be addressed. First, the framework's reliance on LLMs introduces challenges related to the accuracy and reliability of the generated causal refinements, as LLMs can occasionally generate incorrect or hallucinated outputs. This limitation highlights the importance of robust validation mechanisms, such as cross-referencing with domain-specific knowledge and incorporating human oversight where necessary.</p><p>Second, the scalability of the ALCM framework to large, high-dimensional datasets remains constrained by the computational costs associated with running LLMs and the underlying causal discovery algorithms. Optimizing the framework for computational efficiency is essential for broader applicability in real-world scenarios.</p><p>Third, the framework assumes the availability of high-quality input datasets and metadata, which might not always be accessible in certain domains. In cases of sparse or biased data, the effectiveness of ALCM's causal graph generation may be compromised.</p><p>Last, the framework currently lacks a mechanism to quantify uncertainties in the generated causal graphs beyond the confidence scores provided by the LLMs. Future iterations of the framework could integrate statistical measures to provide more comprehensive uncertainty quantification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>In the subsequent phases of our research, we aim to develop a more sophisticated causal-aware framework. This framework will leverage the power of knowledge graphs, which are instrumental in augmenting the accuracy of our models. Furthermore, we plan to explore the integration of our framework with Monte Carlo Tree Search (MCTS). This integration is envisioned to evolve our system into a more dynamic and adaptive problem-solving agent.</p><p>Additionally, to advance the ALCM framework's capabilities and address the issue of LLM hallucination, we propose integrating ALCM with the Retrieval-Augmented Generation (RAG) system and openCHA <ref type="bibr" target="#b0">[1]</ref>. This integration aims to harness RAG's ability to augment LLMs' generative processes with data retrieval, ensuring that causal discovery are grounded in relevant and factual information. openCHA sophisticated dialogue capabilities will further enhance ALCM by enabling dynamic, interactive validation of causal hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This study showed the transformative potential of combining LLMs with data-driven causal discovery algorithms through the introduction of the Autonomous LLM-Augmented Causal Discovery Framework (ALCM). The ALCM emerges as a groundbreaking solution, aiming to enhance the generation of causal graphs by leveraging the sophisticated capabilities of LLMs alongside conventional causal discovery techniques. By integrating causal structure learning, a causal wrapper, and an LLM-driven causal refiner, ALCM facilitated an autonomous approach to causal discovery, significantly outperforming existing methodologies in both accuracy and interpretability. The empirical validation of ALCM not only attests to its superior efficacy over prevailing LLM methods and conventional causal reasoning mechanisms but also illuminates new pathways for leveraging LLMs in uncovering intricate causal relationships across a myriad of domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 Figure 1 :</head><label>11</label><figDesc>Figure 1: ALCM Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Assume you are an expert on Cancer Risk Factors, Genetic Cancer Relation [Metadata], along with the domain of causal discovery.[Instruction] Consider yo have received the results from a causal discovery algorithm (PC) executed on a "Cancer dataset." [Causal Context] The algorithm suggests a causal link where 'pollution' causes 'cancer'. [Question] Based on your current comprehensive understanding of this field, please evaluate, and adjust this conclusion as necessary. You may modify, delete, or add nodes/ edges, or change the orientation of the edges. Ensure that your modifications are grounded in actual data and avoid making unfounded assumptions.[Instruction]  In terms of the output format, denote the correctness of the causal discovery algorithm's output as True or False, represent the causal relationship in the form "('', '')", and include your confidence level for each pair you propose [Output format].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal Prompt Demonstration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompt Template</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Causal graphs for demonstrating new nodes or edges</figDesc><graphic coords="24,91.80,275.25,428.40,387.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparative Analysis of CCD, LLMs, and ALCM across Key Functional Attributes</figDesc><table><row><cell>Descriptive Attribute</cell><cell cols="3">CCD 1 LLMs ALCM</cell></row><row><cell>Dynamic Data Adaptability</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Detection of Hidden Variables</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Comprehensive Graph Model Representation</cell><cell>✓</cell><cell>×</cell><cell>✓</cell></row><row><cell>Predictive Accuracy</cell><cell>×</cell><cell>×</cell><cell>✓</cell></row><row><cell>Autonomous Operation</cell><cell>×</cell><cell>×</cell><cell>✓</cell></row><row><cell>Generalizability to Unseen Data</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Autonomous Expert Validation</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Interpretability and Explainability</cell><cell>✓</cell><cell>×</cell><cell>✓</cell></row><row><cell cols="4">1 CCD methods often rely on pre-defined statistical models and assumptions</cell></row><row><cell>about the data generation process.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2 LLMs-based methods may utilize vast amounts of data and natural lan-</cell></row><row><cell cols="4">guage processing to infer causal relationships, potentially incorporating do-</cell></row><row><cell>main expertise.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">3 ALCM synthesizes the strengths of both CCDs and LLMs to uncover causal</cell></row><row><cell>connections.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of Datasets</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell cols="2">#Nodes #Edges</cell></row><row><cell>Asia</cell><cell>Social Science</cell><cell>8</cell><cell>8</cell></row><row><cell>Cancer</cell><cell>Medical</cell><cell>11</cell><cell>18</cell></row><row><cell>Child</cell><cell>Social Science</cell><cell>20</cell><cell>31</cell></row><row><cell>Insurance</cell><cell>Finance</cell><cell>27</cell><cell>43</cell></row><row><cell cols="2">Neuropathic Medical</cell><cell>221</cell><cell>475</cell></row><row><cell>Sachs</cell><cell>Biological</cell><cell>11</cell><cell>18</cell></row><row><cell>Sangiovese</cell><cell>Social Science</cell><cell>36</cell><cell>47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation Results for Various Causal Discovery Methods</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell></cell><cell></cell><cell>Metrics</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Precision Recall F1-Score Accuracy NHD</cell></row><row><cell></cell><cell>PC</cell><cell>0.75</cell><cell>0.375</cell><cell>0.5</cell><cell>33.33</cell><cell>0.1429</cell></row><row><cell></cell><cell>LiNGAM</cell><cell>0.1818</cell><cell>0.25</cell><cell>0.2105</cell><cell>25.00</cell><cell>0.8824</cell></row><row><cell></cell><cell>NOTEARS</cell><cell>0.1786</cell><cell>0.625</cell><cell>0.2778</cell><cell>53.57</cell><cell>0.4643</cell></row><row><cell>Asia</cell><cell>Hybrid</cell><cell>0.452</cell><cell>0.483</cell><cell>0.466</cell><cell>47.00</cell><cell>0.193</cell></row><row><cell></cell><cell>LLMs</cell><cell>0.1428</cell><cell cols="2">0.2174 0.1742</cell><cell>16.00</cell><cell>0.75</cell></row><row><cell></cell><cell>ALCM-PC</cell><cell>1.0</cell><cell cols="2">0.5945 0.746</cell><cell>87.00</cell><cell>0.0893</cell></row><row><cell></cell><cell cols="2">ALCM-Hybrid 0.89</cell><cell>1.0</cell><cell>0.942</cell><cell>96.6</cell><cell>0.0179</cell></row><row><cell></cell><cell>PC</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>33.33</cell><cell>0.2</cell></row><row><cell></cell><cell>LiNGAM</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>50.00</cell><cell>0.6667</cell></row><row><cell></cell><cell>NOTEARS</cell><cell>0.2</cell><cell>0.5</cell><cell>0.2857</cell><cell>50.00</cell><cell>0.5</cell></row><row><cell>Cancer</cell><cell>Hybrid</cell><cell>0.4</cell><cell>0.46</cell><cell>0.4286</cell><cell>42.00</cell><cell>0.2111</cell></row><row><cell></cell><cell>LLMs</cell><cell>0.158</cell><cell>0.75</cell><cell>0.261</cell><cell>21.4</cell><cell>0.85</cell></row><row><cell></cell><cell>ALCM-PC</cell><cell>0.667</cell><cell>1.00</cell><cell>0.8</cell><cell>85.71</cell><cell>0.1</cell></row><row><cell></cell><cell cols="2">ALCM-Hybrid 0.9</cell><cell>0.95</cell><cell>0.924</cell><cell>90.32</cell><cell>0.0333</cell></row><row><cell></cell><cell>PC</cell><cell>0.2</cell><cell>0.28</cell><cell>0.233</cell><cell>27.00</cell><cell>0.121</cell></row><row><cell></cell><cell>LiNGAM</cell><cell>0.14</cell><cell>0.56</cell><cell>0.224</cell><cell>56.00</cell><cell>0.8739</cell></row><row><cell></cell><cell>NOTEARS</cell><cell>0.0474</cell><cell>0.36</cell><cell>0.0837</cell><cell>48.16</cell><cell>0.5184</cell></row><row><cell>Child</cell><cell>Hybrid</cell><cell>0.3</cell><cell>0.35</cell><cell>0.3231</cell><cell>34.00</cell><cell>0.2875</cell></row><row><cell></cell><cell>LLMs</cell><cell>0.0657</cell><cell>0.48</cell><cell>0.1156</cell><cell>29.21</cell><cell>0.8765</cell></row><row><cell></cell><cell>ALCM-PC</cell><cell>1.0</cell><cell cols="2">0.6185 0.764</cell><cell>78.89</cell><cell>0.047</cell></row><row><cell></cell><cell cols="2">ALCM-Hybrid 0.92</cell><cell>0.85</cell><cell>0.8839</cell><cell>98.04</cell><cell>0.016</cell></row><row><cell></cell><cell>PC</cell><cell>0.2153</cell><cell cols="2">0.2692 0.2393</cell><cell>13.59</cell><cell>0.864</cell></row><row><cell></cell><cell>LiNGAM</cell><cell>0.12</cell><cell cols="2">0.3462 0.1782</cell><cell>34.62</cell><cell>0.9022</cell></row><row><cell></cell><cell>NOTEARS</cell><cell>0.0843</cell><cell cols="2">0.5577 0.1465</cell><cell>51.85</cell><cell>0.4815</cell></row><row><cell>Insurance</cell><cell>Hybrid</cell><cell>0.25</cell><cell>0.32</cell><cell>0.28</cell><cell>30.00</cell><cell>0.315</cell></row><row><cell></cell><cell>LLMs</cell><cell>0.069</cell><cell cols="2">0.5833 0.1234</cell><cell>22.9</cell><cell>0.862</cell></row><row><cell></cell><cell>ALCM-PC</cell><cell>1.0</cell><cell>0.857</cell><cell>0.923</cell><cell>94.8</cell><cell>0.054</cell></row><row><cell></cell><cell cols="2">ALCM-Hybrid 0.95</cell><cell>0.91</cell><cell>0.9294</cell><cell>95.2</cell><cell>0.045</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy and NHD Metrics for ALCM-Hybrid Across Five LLM Models</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>NHD</cell></row><row><cell></cell><cell>GPT-4</cell><cell>96.6</cell><cell>0.0179</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>94.32</cell><cell>0.0210</cell></row><row><cell>Asia</cell><cell>Llama3.1-70B</cell><cell>95.10</cell><cell>0.0192</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>94.80</cell><cell>0.0205</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>93.90</cell><cell>0.0223</cell></row><row><cell></cell><cell>GPT-4</cell><cell>90.32</cell><cell>0.0333</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>88.40</cell><cell>0.0378</cell></row><row><cell>Cancer</cell><cell>Llama3.1-70B</cell><cell>89.20</cell><cell>0.0355</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>88.60</cell><cell>0.0369</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>87.50</cell><cell>0.0385</cell></row><row><cell></cell><cell>GPT-4</cell><cell>98.04</cell><cell>0.016</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>96.70</cell><cell>0.0205</cell></row><row><cell>Child</cell><cell>Llama3.1-70B</cell><cell>97.20</cell><cell>0.0193</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>96.80</cell><cell>0.0201</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>96.00</cell><cell>0.0210</cell></row><row><cell></cell><cell>GPT-4</cell><cell>95.2</cell><cell>0.045</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>94.80</cell><cell>0.0402</cell></row><row><cell>Insurance</cell><cell>Llama3.1-70B</cell><cell>95.50</cell><cell>0.0389</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>94.90</cell><cell>0.0398</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>93.70</cell><cell>0.0415</cell></row><row><cell></cell><cell>GPT-4</cell><cell>90.00</cell><cell>0.174</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>89.20</cell><cell>0.1790</cell></row><row><cell>Sachs</cell><cell>Llama3.1-70B</cell><cell>89.70</cell><cell>0.1755</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>89.30</cell><cell>0.1780</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>88.50</cell><cell>0.1805</cell></row><row><cell></cell><cell>GPT-4</cell><cell>98.18</cell><cell>0.0122</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>96.90</cell><cell>0.0188</cell></row><row><cell>Neuropathic</cell><cell>Llama3.1-70B</cell><cell>97.40</cell><cell>0.0175</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>97.00</cell><cell>0.0182</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>96.20</cell><cell>0.0193</cell></row><row><cell></cell><cell>GPT-4</cell><cell>93.5</cell><cell>0.065</cell></row><row><cell></cell><cell>Llama3.1-8B</cell><cell>91.80</cell><cell>0.0701</cell></row><row><cell>Sangiovese</cell><cell>Llama3.1-70B</cell><cell>92.30</cell><cell>0.0685</cell></row><row><cell></cell><cell>Gemma2-9B</cell><cell>91.90</cell><cell>0.0699</cell></row><row><cell></cell><cell>Ministral-7B</cell><cell>90.70</cell><cell>0.0715</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>on two benchmarksneuropathetic and sachs.</figDesc><table><row><cell>PC</cell><cell>ALCM -PC PC + LLM</cell><cell cols="2">ALCM -Hybrid PC + LLM + LINGAM + NOTEARS</cell></row><row><cell>Causal Discovery</cell><cell>51.7</cell><cell>89.26</cell><cell>98.18</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Figure 5: Additive Contribution on Causal Discovery Accuracy on Neuropathetic Pain</cell></row><row><cell>PC</cell><cell>ALCM -PC PC + LLM</cell><cell cols="2">ALCM -Hybrid PC + LLM + LINGAM + NOTEARS</cell></row><row><cell>Causal Discovery</cell><cell>80.91</cell><cell>87.50</cell><cell>90.00</cell></row><row><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Figure 6: Additive Contribution on Causal Discovery Accuracy on Sachs</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and provide a detailed breakdown of the experimental results.</p><p>We depict the causal graphs obtained by a couple of causal discovery methods on Sachs dataset in Figure <ref type="figure">4</ref>. The Sachs dataset <ref type="bibr" target="#b35">[36]</ref> includes data on 11 phosphorylated proteins and phospholipids from human immune cells, providing a basis for analyzing protein signaling pathways and constructing causal networks. It is especially valuable for causal discovery research, with data collected from cells under different experimental conditions, making it an excellent benchmark for testing causal discovery algorithms. Graph of ground truth, LLMs-based approach, PC, ALCM, ALCM-Hybrid are shown in Figures <ref type="figure">4a,</ref><ref type="figure">4b,</ref><ref type="figure">4c,</ref><ref type="figure">4d,</ref><ref type="figure">4e</ref>, respectively.  The enhanced performance across all metrics for both ALCM-PC and ALCM-Hybrid variants can be directly linked to their innovative methodologies. ALCM's use of LLMs introduces a layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mek</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Iman</forename><surname>Mahyar Abbasian</surname></persName>
		</author>
		<author>
			<persName><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02374</idno>
		<title level="m">Conversational health agents: A personalized llm-powered agent framework</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causal modelling agents: Causal graph discovery through synergising metadata-and data-driven reasoning</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdulaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Montana-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiantian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayodeji</forename><surname>Ijishakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Drobnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><surname>Openai -Openai</surname></persName>
		</author>
		<idno>04-03-2024</idno>
		<ptr target="https://openai.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Zero-shot causal graph extrapolation from text via llms</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Antonucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregorio</forename><surname>Piqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Zaffalon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14670</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Swagata</forename><surname>Ashwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshiteesh</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishith</forename><surname>Reddy Mannuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Singh Sengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Chaitanya Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dishant</forename><surname>Kathala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinija</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Chadha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18139</idno>
		<title level="m">Cause and effect: Can large language models truly understand causality? arXiv preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Causal structure learning supervised by large language model</title>
		<author>
			<persName><forename type="first">Taiyu</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyuzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derui</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.11689</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">From query tools to causal architects: Harnessing large language models for advanced causal discovery from data</title>
		<author>
			<persName><forename type="first">Taiyu</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyvzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16902</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards llm-guided causal explainability for black-box text classifiers</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raha</forename><surname>Moraffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>researchgate.net</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.00139</idno>
		<title level="m">Is knowledge all large language models needed for causal reasoning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mitigating prior errors in causal structure learning</title>
		<author>
			<persName><forename type="first">Lyuzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiyu</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derui</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07032</idno>
	</analytic>
	<monogr>
		<title level="m">Towards llm driven prior knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Selective greedy equivalence search: Finding optimal bayesian networks using a polynomial number of score evaluations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02113</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of prior knowledge on causal structure learning</title>
		<author>
			<persName><forename type="first">Zhigao</forename><surname>Anthony C Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neville</forename><forename type="middle">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Kitson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on causal discovery methods for iid and time series data</title>
		<author>
			<persName><forename type="first">Uzma</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emam</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gani</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can large language models truly understand prompts? a case study with negated prompts</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonghyeon</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Transfer Learning for Natural Language Processing Workshop</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large language model for causal decision making</title>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhe</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.17122</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05836</idno>
		<title level="m">Can large language models infer causation from correlation? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient causal graph discovery using large language models</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Jiralerspong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01207</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural agnostic modeling: Adversarial learning of causal graphs</title>
		<author>
			<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9831" to="9892" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kıcıman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00050</idno>
		<title level="m">Causal reasoning and large language models: Opening a new frontier for causality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cynthia Breazeal, and Hae Won Park. Healthllm: Large language models for health prediction via wearable sensor data</title>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06866</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kyle Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16183</idno>
		<title level="m">Passive learning of active causal strategies in agents and language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03941</idno>
		<title level="m">Discovery of the hidden world with large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03439</idno>
		<title level="m">Evaluating the logical reasoning ability of chatgpt and gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Causal discovery with language models as imperfect experts</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Piché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tibor</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02390</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Can large language models build causal graphs</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tibor</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Piché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Servicenow</forename><surname>Research</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05279</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13375</idno>
		<title level="m">Capabilities of gpt-4 on medical challenge problems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">on the Evaluation of Carcinogenic Risks to Humans, World Health Organization, and International Agency for Research on Cancer. Tobacco smoke and involuntary smoking</title>
		<author>
			<orgName type="collaboration">IARC Working Group</orgName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Iarc</publisher>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report. arxiv 2303.08774. View in Article</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Answering causal questions with augmented llms. openreview.net</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Survey</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The causal mediation formula-a guide to the assessment of pathways and mechanisms</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prevention science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="426" to="436" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling up greedy causal search for continuous variables</title>
		<author>
			<persName><forename type="first">Ramsey</forename><surname>Joseph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07749</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beware of the simulated dag! causal discovery benchmarks may be easy to game</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Reisach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Weichwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27772" to="27784" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A meta-reinforcement learning algorithm for causal discovery</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erman</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent François-Lavet</forename><surname>Acar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="602" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">bnlearn -Bayesian Network Repository -bnlearn</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Scutari</surname></persName>
		</author>
		<idno>13-02-2024</idno>
		<ptr target="https://www.bnlearn.com/bnrepository/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Integrating large language models in causal discovery: A statistical causal approach</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadahisa</forename><surname>Okuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thong</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuyoshi</forename><surname>Ikenoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Fukuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiyoshi</forename><surname>Sannai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01454</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-01">jan 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation</title>
		<author>
			<persName><forename type="first">Ruibo</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bertilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedvig</forename><surname>Kjellstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Vashishtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Abbavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saketh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Bachu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.15117</idno>
		<title level="m">Causal inference using llm-guided discovery</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bridging causal discovery and large language models: A comprehensive survey of integrative approaches and future directions</title>
		<author>
			<persName><forename type="first">Guangya</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11068</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiteng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06805</idno>
		<title level="m">Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Can foundation models talk causality</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Moritz Willig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Zečević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Singh Dhami</surname></persName>
		</author>
		<author>
			<persName><surname>Kersting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10591</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A* lasso for learning a sparse bayesian network structure for continuous variables</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyoung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey on causal discovery: theory and practice</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Zanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Ozkirimli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Stella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="101" to="129" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Causal graph discovery with retrieval-augmented generation based large language models</title>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15301</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
