<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dually Enhanced Propensity Score Estimation in Sequential Recommendation</title>
				<funder ref="#_T8qbUCC #_7QDUdWS #_9dyvQnK">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_jwTVR3S">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_Cyg2pRu">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-03-15">15 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Xu</surname></persName>
							<email>xc_chen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<email>junxu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
							<email>xu.chen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenghua</forename><surname>Dong</surname></persName>
							<email>dongzhenhua@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><forename type="middle">2022</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Dually</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China</orgName>
								<orgName type="institution" key="instit3">Renmin University of China</orgName>
								<orgName type="institution" key="instit4">Renmin University of China</orgName>
								<address>
									<addrLine>CIKM &apos;22 October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dually Enhanced Propensity Score Estimation in Sequential Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-15">15 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3511808.3557299</idno>
					<idno type="arXiv">arXiv:2303.08722v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>sequential recommendation, propensity score estimation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequential recommender systems train their models based on a large amount of implicit user feedback data and may be subject to biases when users are systematically under/over-exposed to certain items. Unbiased learning based on inverse propensity scores (IPS), which estimate the probability of observing a user-item pair given the historical information, has been proposed to address the issue. In these methods, propensity score estimation is usually limited to the view of item, that is, treating the feedback data as sequences of items that interacted with the users. However, the feedback data can also be treated from the view of user, as the sequences of users that interact with the items. Moreover, the two views can jointly enhance the propensity score estimation. Inspired by the observation, we propose to estimate the propensity scores from the views of user and item, called Dually Enhanced Propensity Score Estimation (DEPS). Specifically, given a target user-item pair and the corresponding item and user interaction sequences, DEPS firstly constructs a time-aware causal graph to represent the user-item observational probability. According to the graph, two complementary propensity scores are estimated from the views of item and user, respectively, based on the same set of user feedback data. Finally, two transformers are designed to make the final preference prediction. Theoretical analysis showed the unbiasedness and variance of DEPS. Experimental results on three publicly available and an industrial datasets demonstrated that DEPS can significantly outperform the state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Sequential recommendation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> has attracted increasing attention from both industry and academic communities. Basically, the key advantage of sequential recommender models lies in the explicit modeling of item chronological correlations. To capture such information accurately, recent years have witnessed lots of efforts based on either Markov chains or recurrent neural networks. While these models have achieved remarkable successes, the observed item correlations can be skewed due to the exposure or selection bias <ref type="bibr" target="#b2">[3]</ref>. As exampled in Figure <ref type="figure" target="#fig_0">1</ref>(a), given a user behavior sequence, the observed next item is a coffeepot cleaner. By building models based on the observational data, one can learn the correlations between cleaner and coffeepot. However, from the user preference perspective, the next item can also be the ink-boxes. But the model has no opportunities to capture the correlations between printer and ink-box because they are not recommended and observed in the data. The bias makes the recommendation less effective, especially when testing environment is more related with the office products.</p><p>In order to alleviate the above problem, previous models are mostly based on the technique of inverse propensity score (IPS) <ref type="bibr" target="#b29">[30]</ref>, where if a training sample is more likely to appear in the dataset, then it should have lower weight in the optimization process. In this research line, the key is to accurately approximate the probability of observing a user-item pair (ğ‘¢, ğ‘–) given the historical information ğ» , i.e., ğ‘ƒ (ğ‘¢, ğ‘– |ğ» ). To this end, previous methods usually decompose ğ‘ƒ (ğ‘¢, ğ‘– |ğ» ) as ğ‘ƒ (ğ‘– |ğ‘¢, ğ» )ğ‘ƒ (ğ‘¢ |ğ» ), and focus on parameterizing ğ‘ƒ (ğ‘– |ğ‘¢, ğ» ) (i.e., estimating ğ‘ƒ (ğ‘¢, ğ‘– |ğ» ) from view of item <ref type="bibr" target="#b33">[34]</ref>) to predict which item the user will interact in the next given the previous items. One reason is that estimating propensity scores from the view of item matches well with the online process of sequential recommendation: the users come to the system randomly and the system aims to provide the recommended items immediately.</p><p>While these methods are effective, we argue that the probability of observing a user-item pair can also be considered from a dual perspective, that is, for an item, predicting the next interaction user given the ones who have previously interacted with it. In principle, this is equal to decomposing ğ‘ƒ (ğ‘¢, ğ‘– |ğ» ) in another manner by ğ‘ƒ (ğ‘¢ |ğ‘–, ğ» )ğ‘ƒ (ğ‘– |ğ» ), where ğ‘ƒ (ğ‘¢ |ğ‘–, ğ» ) exactly aims to predict the user given an item and the history users (i.e. estimating ğ‘ƒ (ğ‘¢, ğ‘– |ğ» ) from view of user). Intuitively, for the same item, if two users interact with it for a short time, they should share some similarities at that time. As a result, the previous users may provide useful signals <ref type="bibr" target="#b11">[12]</ref> for predicting the next user and the observation of the user-item pair. We believe such a user-oriented method can provide complementary information to the previous item-oriented models.</p><p>For example, in Figure <ref type="figure" target="#fig_0">1</ref>(b), from the item prediction perspective, the tripod can be observed as the next item for both sequences A and B, since the historical information is similar. However, from the user prediction perspective, we may infer that sequence A should be more likely to be observed, since recently, the tripod is more frequently interacted with by female users, for example, due to the reasons like the promotion sales for the Women's Day. This example suggests that the temporal user correlation signals may well compensate for traditional item-oriented IPS methods, which should be taken seriously for debiasing sequential recommender.</p><p>This paper proposes to build an unbiased sequential recommender model with dually enhanced IPS estimation (called DEPS). The major challenges lie in three aspects: To begin with, the itemand user-oriented IPS are useful, how do we estimate them from the same set of the user feedback data? Secondly, how to combine them is still not clear, especially, since we have to consider the chronological information. At last, how to theoretically ensure that the proposed objective is still unbiased also needs our careful design.</p><p>To solve these challenges, we use two GRUs to estimate the propensity scores, one from the view of item and another from the view of user. Also, to make our model DEPS practical, two transformers are used to make the final recommendation, one encodes the historical interacted item sequence of the target user, and the other encodes the user sequence that interacted with the target item. The encoded sequences' embeddings, as well as the target item and user embeddings, are jointly used to predict the final recommendation score. Moreover, a two-stage learning procedure is designed to estimate the parameters from the modules of propensity score estimation and the item recommendation.</p><p>Major contributions of this paper can be concluded as follows:</p><p>(1) We highlighted the importance of propensity score estimation from two views and proposed a dually enhanced IPS method for debiasing sequential recommender models.</p><p>(2) To achieve the above idea, we implement a double GRU architecture to consider both user-and item-oriented IPS estimation, and theoretically proof the unbiasedness of our objective.</p><p>(3) We conduct extensive experiments to demonstrate the effectiveness of our model by comparing it with the state-of-the-art methods based on three publicly available benchmarks and an industrial-scale commercial dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A lot of research efforts have been made to develop models for sequential recommendation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Compared to traditional recommendation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>, sequential recommendation tries to capture the item chronological correlations. Models based on either Markov chains or recurrent neural networks have been proposed. For example, GRU4Rec+ <ref type="bibr" target="#b31">[32]</ref> introduces an RNN to encode the historical item sequences as the user preference. BERT4Rec <ref type="bibr" target="#b30">[31]</ref> proposes an attention-based way <ref type="bibr" target="#b19">[20]</ref> to model user behavior sequences practically. BST <ref type="bibr" target="#b6">[7]</ref> utilizes the transformer <ref type="bibr" target="#b19">[20]</ref> to capture the user preference from the interaction sequences. LightSANs <ref type="bibr" target="#b10">[11]</ref> introduce a low-rank decomposed self-attention to the model context of the item. As for model training, S3-Rec <ref type="bibr" target="#b43">[44]</ref> incorporates self-supervised and adapts the Pre-train/fine-tune paradigm.</p><p>Modern recommender systems have to face variant biases, including selection bias <ref type="bibr" target="#b18">[19]</ref>, position bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>, popularity bias <ref type="bibr" target="#b37">[38]</ref>, and exposure bias <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>. Biases usually happen on multi sides <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. For example, item exposure is affected by both the user's previous behaviors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> and the user's background <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Wang et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, Zhang et al. <ref type="bibr" target="#b36">[37]</ref> pointed out that sequential scenarios are different and more studies are needed.</p><p>One common way to remedy the bias is through inverse propensity score (IPS) <ref type="bibr" target="#b29">[30]</ref>. Devooght et al. <ref type="bibr" target="#b9">[10]</ref>, Hu et al. <ref type="bibr" target="#b13">[14]</ref> used the prior experience as propensity score to uniformly re-weight the samples. UIR <ref type="bibr" target="#b27">[28]</ref> and UBPR <ref type="bibr" target="#b26">[27]</ref> propose to utilize the latent probabilistic model to estimate propensity score. Agarwal et al. <ref type="bibr" target="#b1">[2]</ref>, Fang et al. <ref type="bibr" target="#b12">[13]</ref> utilized the intervention harvesting to learn the propensity. Joachims et al. <ref type="bibr" target="#b14">[15]</ref>, Qin et al. <ref type="bibr" target="#b22">[23]</ref> learns propensity model with EM algorithms.USR <ref type="bibr" target="#b33">[34]</ref> proposed a network to estimate propensity scores from the view of item in the sequential recommendation. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> pointed out that it is useful to carefully consider the user's perspective when estimating propensity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION 3.1 Sequential Recommendation</head><p>Suppose that a sequential recommender system manages a set of user-item historical interactions D = {(ğ‘¢, ğ‘–, ğ‘ ğ‘¡ )} were each tuple (ğ‘¢, ğ‘–, ğ‘ ğ‘¡ ) records that at time stamp ğ‘¡, a user ğ‘¢ âˆˆ U accessed the system and interacted with an item ğ‘– âˆˆ I, and the user's feedback is ğ‘ ğ‘¡ âˆˆ {0, 1}, where U and I respectively denote the set of users and items in the system, and ğ‘ ğ‘¡ = 1 means that the user ğ‘¢ clicked the item ğ‘– and 0 otherwise at time ğ‘¡. Moreover, the context information of (ğ‘¢, ğ‘–) (e.g. user profile and item attribute) collected from the system is often represented as real-valued vectors (embeddings) e(ğ‘¢), e(ğ‘–) âˆˆ R ğ‘‘ , where ğ‘‘ denotes the dimensions of the embeddings. At a specific time ğ‘¡ and given a target user-item pair (ğ‘¢, ğ‘–), two types interaction sequences can be derived from D: (1) the sequence of items that the user ğ‘¢ previously interacted before time ğ‘¡:</p><formula xml:id="formula_0">h &lt;ğ‘¡ ğ‘¢ = [ğ‘– 1 , ğ‘– 2 , â€¢ â€¢ â€¢ , ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) ],</formula><p>where ğ‘™ (ğ‘¢, ğ‘¡) denotes the number of items the user ğ‘¢ interacted before time ğ‘¡; (2) the sequence of the users that item ğ‘– was previously interacted with before time ğ‘¡:</p><formula xml:id="formula_1">h &lt;ğ‘¡ ğ‘– = [ğ‘¢ 1 , ğ‘¢ 2 , â€¢ â€¢ â€¢ , ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ ) ],</formula><p>where ğ‘™ (ğ‘–, ğ‘¡) denotes the number of users the item ğ‘– was interacted before time ğ‘¡.</p><p>Figure <ref type="figure" target="#fig_1">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Biases in Sequential Recommendation</head><p>In sequential recommendation, bias happens when the user ğ‘¢ is systematically under/over-exposed to certain items. As shown in Figure <ref type="figure">3</ref>(a) and from a causal sense, a user clicks an item at time ğ‘¡ (ğ‘ ğ‘¡ = 1) only if the item ğ‘– is relevant to the user ğ‘¢ (ğ‘Ÿ ğ‘¡ = 1) and the ğ‘– is exposed to ğ‘¢ (ğ‘œ ğ‘¢ğ‘–ğ‘¡ = 1), where ğ‘œ ğ‘¢ğ‘–ğ‘¡ âˆˆ {0, 1} and ğ‘Ÿ ğ‘¡ âˆˆ {0, 1} respectively denote whether the user ğ‘¢ is aware of item ğ‘– and is relevant to ğ‘–, or formally ğ‘ ğ‘¡ = ğ‘Ÿ ğ‘¡ â€¢ ğ‘œ ğ‘¢ğ‘–ğ‘¡ . Further suppose that the two interaction sequences h &lt;ğ‘¡ ğ‘¢ and h &lt;ğ‘¡ ğ‘– will also influence whether ğ‘¢ is aware of the ğ‘–. Since the model predicts the user preference ğ‘Ÿ ğ‘¡ with the observed clicks ğ‘ ğ‘¡ , the prediction is inevitably biased by the item exposure ğ‘œ ğ‘¢ğ‘–ğ‘¡ . This is because the ğ‘œ ğ‘¢ğ‘–ğ‘¡ becomes a confounder after observing the click in the causal graph (i.e., click as a collider <ref type="bibr" target="#b21">[22]</ref>).</p><p>Formally, the probability of a user ğ‘¢ clicks an item ğ‘– at time ğ‘¡ can be factorized as the probability that ğ‘– is observable to the user (i.e., ğ‘œ ğ‘¢ğ‘–ğ‘¡ = 1) and the probability that (ğ‘¢, ğ‘–) is relevant ğ‘Ÿ ğ‘¡ = 1 1 : <ref type="formula">1</ref>) As have shown in Figure <ref type="figure" target="#fig_0">1</ref>, existing studies usually estimate ğ‘ƒ (ğ‘œ ğ‘¢ğ‘–ğ‘¡ = 1) only from the view of item, ignoring the dual view of user. Also, the estimation need to take the chronological information (i.e., h &lt;ğ‘¡ ğ‘¢ and h &lt;ğ‘¡ ğ‘– ) into consideration.</p><formula xml:id="formula_2">ğ‘ƒ (ğ‘ ğ‘¡ = 1|ğ‘¢, ğ‘–, ğ‘¡) = ğ‘ƒ (ğ‘Ÿ ğ‘¡ = 1|ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– ) â€¢ ğ‘ƒ (ğ‘œ ğ‘¢ğ‘–ğ‘¡ = 1). (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unbiased Objective for Recommendation</head><p>Ideally, the learning objective for recommendation (including the sequential and non-sequential scenarios) should be constructed based on the correlations between the true preference and the predicted score by the recommendation model:</p><formula xml:id="formula_3">L ideal ğ‘› = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘– âˆˆI ğ›¿ (ğ‘Ÿ, r (ğ‘¢, ğ‘–)),<label>(2)</label></formula><p>where r (ğ‘¢, ğ‘–) is the prediction by the recommendation model, ğ‘Ÿ is the true preference, and ğ›¿ (â€¢, â€¢) is the loss function defined over each user-item pair. Note that in non-sequential recommendation, time ğ‘¡ and historical information h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– are not considered. In real world, however, L ideal ğ‘› cannot be directly optimized because the true preference ğ‘Ÿ cannot be observed.</p><p>Traditional RS regard the observed clicks ğ‘ as the labels to learn the models, which is inevitably influenced by the exposure or selfselection bias as shown in Figure <ref type="figure">3(b)</ref>. A practical solution is to remedy the biases through the propensity score of the confounder <ref type="bibr" target="#b2">[3]</ref>. A typical approach developed under the non-sequential scenarios is utilizing the propensity score ğ‘ƒ (ğ‘œ = 1) to weigh each observed interaction:</p><formula xml:id="formula_4">L unbiased ğ‘› = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘– âˆˆI I(ğ‘œ ğ‘¢ğ‘– = 1) ğ›¿ (ğ‘, r (ğ‘¢, ğ‘–)) ğ‘ƒ (ğ‘œ ğ‘¢ğ‘– = 1) ,<label>(3)</label></formula><p>where I(â€¢) is the indicator function, and Eq. ( <ref type="formula" target="#formula_4">3</ref>) is an unbiased estimation of the ideal objective Eq. ( <ref type="formula" target="#formula_3">2</ref>), i.e., E ğ‘œ L unbiased ğ‘› = L ideal ğ‘› . Please refer to <ref type="bibr" target="#b2">[3]</ref> for more details.</p><p>Unbiased recommendation models have been developed under the framework. Generalizing these methods to sequential recommendation is a non-trial task. In this paper, we presented an approach to utilizing the user-item interaction sequences to estimate the propensity scores framework called DEPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR APPROACH: DEPS</head><p>In this section, we proposed an unbiased objective for the sequential recommendation. After that, a dually enhanced IPS estimation model called DEPS is developed to estimate the propensity scores in the objective of sequential recommendations. Finally, two transformers are proposed to adapt our framework in a practical way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unbiased Loss for Sequential Recommendation</head><p>Given a user-item historical interactions D, we define the ideal learning objective of sequential recommendation as evaluating the preference at each time that the users access the system: 1 We suppose that the observational probability ğ‘ƒ (ğ‘œ = 1) is only based on the user and item's historical interaction sequences. In real tasks, item observable probability is also influenced by other factors such as the ranking position, as shown in Figure <ref type="figure">3(b)</ref>. Considering these factors in sequential recommendation will be the future work.</p><formula xml:id="formula_5">L ideal ğ‘  = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘¡ :(ğ‘¢,ğ‘– â€² ,ğ‘ ğ‘¡ ) âˆˆD âˆ‘ï¸ ğ‘– âˆˆI ğ›¿ (ğ‘Ÿ ğ‘¡ , rğ‘¡ (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘– , h &lt;ğ‘¡ ğ‘¢ )),<label>(4)</label></formula><p>where r (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘– , h &lt;ğ‘¡ ğ‘¢ ) is the prediction by the sequential recommendation model, and ğ‘Ÿ ğ‘¡ is the true while un-observable preference at time ğ‘¡. Different from the non-sequential unbiased recommender models, the propensity score in sequential recommendation is related to the time, as shown in the causal graph in Figure <ref type="figure">3(a)</ref>. One way to achieve the unbiased sequential recommendation learning objective is estimating the propensity score ğ‘ƒ (ğ‘œ ğ‘¢ğ‘–ğ‘¡ = 1) corresponds to (ğ‘¢, ğ‘–) at time ğ‘¡, based on the historical interaction sequences of h &lt;ğ‘¡ ğ‘– and h &lt;ğ‘¡ ğ‘¢ , as shown in the following theorem. Theorem 1 (Time-aware Unbiased Learning Objective). Given user-item interactions D = {(ğ‘¢, ğ‘–, ğ‘ ğ‘¡ )}, we have</p><formula xml:id="formula_6">E ğ‘œ L unbiased ğ‘  = E ğ‘œ [ğ›¼ L ğ‘¢ + (1 -ğ›¼)L ğ‘– ] = L ideal ğ‘  ,<label>(5)</label></formula><p>where ğ›¼ âˆˆ [0, 1] is the co-efficient that balances the two objectives:</p><formula xml:id="formula_7">L ğ‘¢ = âˆ‘ï¸ (ğ‘¢,ğ‘–,ğ‘ ğ‘¡ ) âˆˆD ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ (ğ‘–,ğ‘ ğ‘¡ ) âˆˆD ğ‘¢ ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ )</formula><p>,</p><formula xml:id="formula_8">L ğ‘– = âˆ‘ï¸ (ğ‘¢,ğ‘–,ğ‘ ğ‘¡ ) âˆˆD ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) = âˆ‘ï¸ ğ‘– âˆˆI âˆ‘ï¸ (ğ‘¢,ğ‘ ğ‘¡ ) âˆˆD ğ‘– ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– )</formula><p>,</p><p>where</p><formula xml:id="formula_9">D ğ‘¢ = {(ğ‘–, ğ‘ ğ‘¡ ) : (ğ‘¢, ğ‘–, ğ‘ ğ‘¡ ) âˆˆ D}, D ğ‘– = {(ğ‘¢, ğ‘ ğ‘¡ ) : (ğ‘¢, ğ‘–, ğ‘ ğ‘¡ ) âˆˆ D}, ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ )</formula><p>is the probability that ğ‘– and h &lt;ğ‘¡ ğ‘– appear, and ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) is the probability that ğ‘¢ and h &lt;ğ‘¡ ğ‘– appear. Proof of Theorem 1 can be found in the Appendix A.1. From the theorem, we can see that an unbiased learning objective for sequential recommendation can be achieved either from the view of user L ğ‘¢ or the view of item L ğ‘– . Moreover, it is easy to know that the average of the two unbiased losses, i.e., L unbiased ğ‘  defined in Eq. ( <ref type="formula" target="#formula_6">5</ref>), is still an unbiased objective.</p><p>Considering that the propensity scores play as the denominators in L ğ‘– and L ğ‘¢ . To enhance the estimation stability, clip technique <ref type="bibr" target="#b28">[29]</ref> is applied to the estimated probabilities in Eq. ( <ref type="formula" target="#formula_16">8</ref>) and Eq. ( <ref type="formula" target="#formula_20">9</ref>), achieving</p><formula xml:id="formula_10">ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) = max{ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ), ğ‘€ },<label>(6)</label></formula><formula xml:id="formula_11">ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) = max{ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ), ğ‘€ },<label>(7)</label></formula><p>where ğ‘€ âˆˆ (0, 1) is the clip value. We show that with the clipped propensity scores the estimation variance can be bounded:</p><formula xml:id="formula_12">Theorem 2 (Estimation Variance). Let ğ¿ ğ‘¡ ğ‘¢ = ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘¢,h &lt;ğ‘¡ ğ‘– )</formula><p>and</p><formula xml:id="formula_13">ğ¿ ğ‘¡ ğ‘– = ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘–,h &lt;ğ‘¡ ğ‘¢ )</formula><p>are two random variables w.r.t. loss on a single training sample (ğ‘–, ğ‘¢, ğ‘ ğ‘¡ ) âˆˆ D, ğ›¼ğ¿ ğ‘¡ ğ‘¢ + (1 -ğ›¼)ğ¿ ğ‘¡ ğ‘– 's estimation variance satisfies:</p><formula xml:id="formula_14">V ğ›¼ğ¿ ğ‘¡ ğ‘¢ + (1 -ğ›¼)ğ¿ ğ‘¡ ğ‘– â‰¤ max{V ğ¿ ğ‘¡ ğ‘¢ , V ğ¿ ğ‘¡ ğ‘– } â‰¤ 1 ğ‘€ -1 ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ).</formula><p>Proof of Theorem 2 can be found in the Appendix A.2. We conclude that the averaged loss would not bring additional variance. Intuitively, the clip value ğ‘€ is a trade-off between the unbiasedness and the variance and provides a mechanism to control the variance. A larger ğ‘€ leads to lower variance and more bias. We show that the clip technique in Saito et al. <ref type="bibr" target="#b28">[29]</ref> still works in a dual perspective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimating Propensity Scores with GRUs</head><p>In the real world, sequences h &lt;ğ‘¡ ğ‘¢ and h &lt;ğ‘¡ ğ‘– are very sparse and short compared to the whole sets of items and users. In this paper, we resort to the neural language model of GRU <ref type="bibr" target="#b7">[8]</ref> for estimating the propensity scores. Also, according to Theorem 2, the clip technique is applied to the estimated propensity scores.</p><p>Specifically, two GRUs are respectively used to estimate the ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) and ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ), as shown in Figure <ref type="figure" target="#fig_1">2</ref>(c). Specifically, given a tuple (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– ), its propensity score from the view of item is estimated as the maximum value of ğ‘€ and ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ), where ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) is proportional to the dot product of ğ‘–'s embedding e(ğ‘–), and the output of a GRU which takes the sequence h &lt;ğ‘¡ ğ‘¢ = [ğ‘– 1 , ğ‘– 2 , â€¢ â€¢ â€¢ , ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) ] as input. By applying the clip technique in Eq. ( <ref type="formula" target="#formula_10">6</ref>), we write the estimated propensity score from view of item as:</p><formula xml:id="formula_15">ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) = max ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³</formula><p>exp e(ğ‘–) ğ‘‡ y(ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) )</p><formula xml:id="formula_16">ğ‘– â€² âˆˆI exp e(ğ‘– â€² ) ğ‘‡ y(ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) ) , ğ‘€ ï£¼ ï£´ ï£´ ï£½ ï£´ ï£´ ï£¾ ,<label>(8)</label></formula><p>where y(ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) ) âˆˆ R ğ‘‘ is the GRU output from its last layer (i.e., corresponds to the ğ‘™ (ğ‘¢, ğ‘¡)-th input). The GRU scans the items in h &lt;ğ‘¡ ğ‘¢ as follows: at the ğ‘˜-th (ğ‘˜ = 1, â€¢ â€¢ â€¢ , ğ‘™ (ğ‘¢, ğ‘¡)) layer, it takes the embedding of the ğ‘˜-th item e(ğ‘– ğ‘˜ ) as input, and outputs y(ğ‘– ğ‘˜ ) which is the representation for the scanned sub-sequence</p><formula xml:id="formula_17">[ğ‘– 1 , ğ‘– 2 , â€¢ â€¢ â€¢ , ğ‘– ğ‘˜ ]: y(ğ‘– ğ‘˜ ), z ğ‘˜ = GRU 1 (e(ğ‘– ğ‘˜ ), z ğ‘˜-1 ),</formula><p>where z ğ‘˜ and z ğ‘˜-1 are the hidden vectors of ğ‘˜-th and (ğ‘˜ -1)-th steps, and GRU 1 is the GRU cell that processes the sequence from the view of item.</p><p>Similarly, given a tuple (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– ), its propensity score can also be estimated from the view of user: the maximum of ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) and ğ‘€, where ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) is proportional to the dot product of the user embedding e(ğ‘¢) and the representation of sequence</p><formula xml:id="formula_18">h &lt;ğ‘¡ ğ‘– = [ğ‘¢ 1 , ğ‘¢ 2 , â€¢ â€¢ â€¢ , ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ ) ]</formula><p>. By applying the clip technique in Eq. ( <ref type="formula" target="#formula_11">7</ref>), we write the estimated propensity score from view of user as:</p><formula xml:id="formula_19">ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) = max ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³</formula><p>exp e(ğ‘¢) ğ‘‡ y(ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ ) )</p><p>ğ‘¢ â€² âˆˆU exp e(ğ‘¢ â€² ) ğ‘‡ y(ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ ) )</p><p>, ğ‘€</p><formula xml:id="formula_20">ï£¼ ï£´ ï£´ ï£½ ï£´ ï£´ ï£¾ ,<label>(9)</label></formula><p>where y(ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) ) âˆˆ R ğ‘‘ is the output of another GRU which scans h &lt;ğ‘¡ ğ‘– as follows: at the ğ‘˜-th layer, it takes the embedding of the ğ‘˜-th user e(ğ‘¢ ğ‘˜ ) as input, and output y(ğ‘¢ ğ‘˜ ) representation for the scanned sub-sequence [ğ‘¢ 1 , ğ‘¢ 2 , â€¢ â€¢ â€¢ , ğ‘¢ ğ‘˜ ]:</p><formula xml:id="formula_21">y(ğ‘¢ ğ‘˜ ), z ğ‘˜ = GRU 2 (e(ğ‘¢ ğ‘˜ ), z ğ‘˜-1 ),</formula><p>where z ğ‘˜ and z ğ‘˜-1 are the hidden vectors, and GRU 2 is another GRU that processes the interaction sequence from the view of user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Backbone: Transformer-based Recommender</head><p>As shown Figure <ref type="figure" target="#fig_2">4</ref>, the implementation of the sequential recommendation model consists of a Transformer Layer and a Prediction Layer. The Transformer Layer consists of two transformers <ref type="bibr" target="#b19">[20]</ref>. One converts the sequence h &lt;ğ‘¡ ğ‘¢ and the target item ğ‘– into representation vector, and another converts the sequence h &lt;ğ‘¡ ğ‘– and the target user ğ‘¢ into another representation vector. The Prediction Layer concatenates the vectors and makes the prediction with an MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Transformer</head><p>Layer. The overall item representation of the input tuple (ğ‘¢, ğ‘–, ğ‘¡) can be represented as the concat of item id embedding and user historical sequence embeddings:</p><formula xml:id="formula_22">e iv (ğ‘¢, ğ‘–, ğ‘¡) = e(h &lt;ğ‘¡ ğ‘¢ )âˆ¥e(ğ‘–) ,<label>(10)</label></formula><p>where operator 'âˆ¥' concatenates two vectors, e(ğ‘–) is the embedding of the items, ğ‘¡ ) ] is the user sequence related to the target user ğ‘¢, and e(h &lt;ğ‘¡ ğ‘¢ ) is the vector that encodes the sequence, defined as the mean of the vectors outputted by a transformer:</p><formula xml:id="formula_23">h &lt;ğ‘¡ ğ‘¢ = [ğ‘– 1 , ğ‘– 2 , â€¢ â€¢ â€¢ , ğ‘– ğ‘™ (ğ‘¢,</formula><formula xml:id="formula_24">e(h &lt;ğ‘¡ ğ‘¢ ) = Mean Transformer 1 [e(ğ‘– 1 ), â€¢ â€¢ â€¢ , e(ğ‘– ğ‘™ (ğ‘¢,ğ‘¡ ) )] ,<label>(11)</label></formula><p>where 'Mean' is the mean pooling operation for all the input vectors, and Transformer 1 (â€¢) is a transformer <ref type="bibr" target="#b19">[20]</ref> architecture. Similarly, the overall item representation of the input tuple (ğ‘¢, ğ‘–, ğ‘¡) can be represented as the concat of user id embedding and item historical sequence embeddings:</p><formula xml:id="formula_25">e uv (ğ‘¢, ğ‘–, ğ‘¡) = e(h &lt;ğ‘¡ ğ‘– )âˆ¥e(ğ‘¢) ,<label>(12)</label></formula><p>e(ğ‘¢) is the embedding of the target user ğ‘–, ğ‘¡ ) ] is item sequence interacted by the target item ğ‘–, and e(h &lt;ğ‘¡ ğ‘– ) is the mean of the output of another transformer:</p><formula xml:id="formula_26">h &lt;ğ‘¡ ğ‘– = [ğ‘¢ 1 , ğ‘¢ 2 , â€¢ â€¢ â€¢ , ğ‘¢ ğ‘™ (ğ‘–,</formula><formula xml:id="formula_27">e(h &lt;ğ‘¡ ğ‘– ) = Mean Transformer 2 [e(ğ‘¢ 1 ), â€¢ â€¢ â€¢ , e(ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ ) )] ,<label>(13)</label></formula><p>where Transformer 2 (â€¢) is another transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Prediction Layer.</head><p>Finally, an Multi-Layer Perception (MLP) is applied which takes e iv (ğ‘¢, ğ‘–, ğ‘¡) and e uv (ğ‘¢, ğ‘–, ğ‘¡) as inputs, and outputs the predicted preference r : r = ğœ (MLP(e iv (ğ‘¢, ğ‘–, ğ‘¡)||e uv (ğ‘¢, ğ‘–, ğ‘¡))),</p><p>where 'ğœ' is the sigmoid function operation and MLP is a twolayer fully connected neural network that takes both e iv (ğ‘¢, ğ‘–, ğ‘¡) and Calculate preference score r = ğ‘“ (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– ) (Eq. ( <ref type="formula" target="#formula_28">14</ref>))</p><p>13:</p><p>Calculate propensity score ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ), ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) (Eq. ( <ref type="formula" target="#formula_10">6</ref>), ( <ref type="formula" target="#formula_11">7</ref>))</p><p>14:</p><p>Update ğœƒ ğ‘’ , ğœƒ ğ‘¡ , ğœƒ ğ‘š by minimizing the loss L unbiased ğ‘  . 15: end for e uv (ğ‘¢, ğ‘–, ğ‘¡) as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning with Estimated Propensity Scores</head><p>The proposed model has a set of parameters to learn, denoted as Î˜ = {ğœƒ ğ‘’ , ğœƒ ğ‘ , ğœƒ ğ‘¡ , ğœƒ ğ‘š }, where parameters ğœƒ ğ‘’ denotes the parameters in the embedding models which output the user and item embeddings, the parameters ğœƒ ğ‘ in GRU 1 and GRU 2 for estimating propensity scores, the parameters ğœƒ ğ‘¡ in Transformer 1 , Transformer 2 , and the parameters ğœƒ ğ‘š in MLP for making the final recommendation.</p><p>Inspired by the pre-train and then fine-tune paradigm, we also design a two-stage learning procedure to learn the model parameters. In the first stage, the parameters of {ğœƒ ğ‘’ , ğœƒ ğ‘ , ğœƒ ğ‘¡ } are trained in an unsupervised learning manner, achieving a relatively good initialization. Then, the second stage learns all of the parameters with the aforementioned unbiased learning objectives. Adam <ref type="bibr" target="#b15">[16]</ref> optimizer is used for conducting the optimization.</p><p>For stage-1, we apply ğ‘› ğ‘ epochs to optimize the L stage-1 . For stage-2, we apply ğ‘› ğ‘¢ epochs to alternative train, where in each epoch, ğ‘› ğ‘ epochs to optimize the L AR ğ‘¢ + L AR ğ‘– and 1 epoch is set to optimize the L unbiased ğ‘  , respectively. The overall algorithm process can be seen in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">First</head><p>Stage: Unsupervised Learning. In the first stage, the two views of all user-system interaction sequences, i.e., h &lt;ğ‘¡ ğ‘– 's and h &lt;ğ‘¡ ğ‘¢ 's for all ğ‘¢ âˆˆ U and ğ‘– âˆˆ I, are utilized as the unsupervised training instances. Inspired by the success of the Autoregressive language models and the masked language models, two learning tasks are designed which respectively apply these two languages models to the user sequences and item sequences, resulting in a total loss L stage-1 that consists of four parts:</p><formula xml:id="formula_29">L stage-1 = (L AR ğ‘¢ + L AR ğ‘– ) + ğœ† ğ‘ (L MLM ğ‘¢ + L MLM ğ‘– ),<label>(15)</label></formula><p>where ğœ† ğ‘ &gt; 0 is the a trade-off coefficient, L AR ğ‘¢ and L AR ğ‘– are the losses correspond to respectively apply the Autoregressive language models to the sequences of L AR ğ‘¢ and L ğ´ğ‘… ğ‘– . Specifically, L AR ğ‘¢ is defined as:</p><formula xml:id="formula_30">L AR ğ‘¢ = âˆ‘ï¸ ğ‘¢ âˆˆU AR(h &lt;ğ‘¡ ğ‘¢ ) = âˆ‘ï¸ ğ‘¢ âˆˆU ğ‘™ (ğ‘¢,ğ‘¡ ) âˆ‘ï¸ ğ‘š=1 -log ğ‘ƒ ğ‘– ğ‘š | ğ‘– 1 , â€¢ â€¢ â€¢ , ğ‘– ğ‘š-1 ,</formula><p>where ğ‘– ğ‘š is the ğ‘š-th item in sequence h &lt;ğ‘¡ ğ‘¢ and ğ‘– 1 , â€¢ â€¢ â€¢ , ğ‘– ğ‘š-1 is the (ğ‘š -1)-length prefix of h &lt;ğ‘¡ ğ‘¢ , and the probability ğ‘ƒ (â€¢|â€¢) is calculated according to Eq. ( <ref type="formula" target="#formula_16">8</ref>). Similarly, L ğ´ğ‘… ğ‘– is defined as:</p><formula xml:id="formula_31">L AR ğ‘– = âˆ‘ï¸ ğ‘– âˆˆI AR(h &lt;ğ‘¡ ğ‘– ) = âˆ‘ï¸ ğ‘– âˆˆI ğ‘™ (ğ‘–,ğ‘¡ ) âˆ‘ï¸ ğ‘š=1 -log ğ‘ƒ ğ‘¢ ğ‘š | ğ‘¢ 1 , â€¢ â€¢ â€¢ , ğ‘¢ ğ‘š-1 ,</formula><p>probability ğ‘ƒ (â€¢|â€¢) is calculated according to Eq. ( <ref type="formula" target="#formula_20">9</ref>). As for L MLM ğ‘¢ and L MLM ğ‘– , following the practice in BERT4Rec <ref type="bibr" target="#b30">[31]</ref>, we respectively apply the masked language models to the sequences of h &lt;ğ‘¡ ğ‘¢ and h &lt;ğ‘¡ ğ‘– , achieving:</p><formula xml:id="formula_32">L MLM ğ‘¢ = âˆ‘ï¸ ğ‘¢ âˆˆU MLM h &lt;ğ‘¡ ğ‘¢ ; L MLM ğ‘– = âˆ‘ï¸ ğ‘– âˆˆI MLM h &lt;ğ‘¡ ğ‘– ,</formula><p>where MLM(â€¢) calculates the masked language model loss on the inputted sequence. The MLM task will make our training phase of the second stage more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Second</head><p>Stage: Unbiased Learning. In the second stage training, given D = {(ğ‘¢, ğ‘–, ğ‘ ğ‘¡ )}, the estimated propensity scores is used to re-weight the original biased loss ğ›¿ (ğ‘, r ), achieving the unbiased loss L stage-<ref type="foot" target="#foot_0">foot_0</ref> :</p><formula xml:id="formula_33">L stage-2 = L unbiased ğ‘  ,<label>(16)</label></formula><p>where unbiased objective L unbiased ğ‘  is constructed based on the Eq. ( <ref type="formula" target="#formula_6">5</ref>) in Theorem 1, by substituting the estimated propensity scores in Eq. ( <ref type="formula" target="#formula_10">6</ref>) and Eq. ( <ref type="formula" target="#formula_11">7</ref>) to Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><p>Note that the second-stage also empirically involves L ğ´ğ‘… ğ‘¢ and L ğ´ğ‘… ğ‘– for avoiding the high variance of propensity score estimation for alternate training. In all of the experiments of this paper, he original loss ğ›¿ (ğ‘, r ) was set to the binary cross entropy:</p><formula xml:id="formula_34">ğ›¿ (ğ‘, r ) = ğ‘ â€¢ log( r ) + (1 -ğ‘) log(1 -r ),</formula><p>where r is predicted by Eq. ( <ref type="formula" target="#formula_28">14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conducted experiments to verify the effectiveness of DEPS. 2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>The experiments were conducted on four large scale publicly available sequential recommendation benchmarks:</p><p>MIND<ref type="foot" target="#foot_1">foot_1</ref> : a large scale news recommendation dataset. Users/items interacted with less than 5 items/users were removed for avoiding extremely sparse cases.</p><p>Amazon-Beauty/Amazon-Digital-Music: Two subsets (beauty and digital music domains) of Amazon Product dataset <ref type="foot" target="#foot_2">4</ref> . Similarly, users/items interacted with less than 5 items/users were removed. We treated the 4-5 star ratings of Amazon dataset made by users as positive feedback (labeled with 1), and others as negative feedback (labeled with 0). Huawei Dataset: To verify the effectiveness of our method on production data, we collect 1 month traffic log from the Huawei music service system, with about 245K interactions after sampling.</p><p>Table <ref type="table" target="#tab_2">1</ref> lists statistics of the four datasets. Following the practices in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, Debiased recommender models need to be evaluated based on unbiased testing sets <ref type="bibr" target="#b28">[29]</ref>. Following the practice of <ref type="bibr" target="#b25">[26]</ref>, we utilized the first 50% interactions sorted by interaction times for training and re-sample other 50% data for evaluation and test. Specifically, suppose item ğ‘– were clicked ğ‘š ğ‘– times, we used the inverse probability ğ‘š ğ‘– /max ğ‘— âˆˆI ğ‘š ğ‘— to sample. Then we utilized 20% and 30% sorted data for validation and test, respectively.</p><p>The following representative sequential recommendation models were chosen as the baselines: STAMP <ref type="bibr" target="#b17">[18]</ref> which models the long-and short-term preference of users; GRU4Rec+ <ref type="bibr" target="#b31">[32]</ref> is an improved version of GRU4Rec with data augmentation and accounting for the shifts in the inputs; BERT4Rec <ref type="bibr" target="#b30">[31]</ref> employs an attention module to model user behaviors and trains with unsupervised style; FPMC <ref type="bibr" target="#b24">[25]</ref> captures users' preference by combing matrix factorization with first-order Markov chains; DIN <ref type="bibr" target="#b42">[43]</ref> applies an attention module to adaptively learn the user interests from their historical behaviors; BST <ref type="bibr" target="#b6">[7]</ref> applies the transformer architecture to adaptively learn user interests from historical behaviors and the side information of users and items; LightSANs <ref type="bibr" target="#b10">[11]</ref> is a lowrank decomposed SANs-based recommender model. We also chose the following unbiased recommendation models as the baselines: UIR <ref type="bibr" target="#b28">[29]</ref> is an unbiased recommendation model that estimates the propensity score using heuristics; CPR <ref type="bibr" target="#b32">[33]</ref> is a pairwise debiasing approach for exposure bias; UBPR <ref type="bibr" target="#b26">[27]</ref>is an IPS method for nonnegative pair-wise loss. DICE <ref type="bibr" target="#b40">[41]</ref>: A debiasing model focused on the user communities. USR <ref type="bibr" target="#b33">[34]</ref>: A debiasing sequential model that aims to alleviate bias raised by latent confounders.</p><p>To evaluate the performances of DEPS and baselines, we utilized two types of metrics: the accuracy of recommendation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> in terms of NDCG@K and HR@K. Following the practices in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43]</ref>, for representing the users and items in the sequences (i.e., users in h &lt;ğ‘¡ ğ‘– and items in h &lt;ğ‘¡ ğ‘¢ ), sequence position embedding were added to the original embedding.</p><p>As for the hyper parameters in all models, the learning rate was tuned among [1ğ‘’ -3, 1ğ‘’ -4] and the propensity score estimation clip coefficient ğ‘€ was tuned among [0.01, 0.2]. The trade-off coefficients in the first-stage ğœ† ğ‘ was set to 0.5. The trade-off coefficients ğ›¼ of two views was tuned among [0.4, 0.6]. The hidden dimensions of the neural networks ğ‘‘ was tuned among {64, 128, 256}, the dropout rate was tuned among {0.1, 0.2, 0.3, 0.4, 0.5}, and number of transformer layers was tuned among {2, 3, 4}.</p><p>All the baselines and the experiments were developed and conducted under the Recbole recommender tools <ref type="bibr" target="#b38">[39]</ref> and Pytorch <ref type="bibr" target="#b20">[21]</ref>. All the models were trained on a single NVIDIA GeForce RTX 3090, NCCG@5 NCCG@10 NCCG@20 HR@5 HR@10 HR@20 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Table <ref type="table" target="#tab_3">2</ref> reports the experimental results of DEPS and the baselines on all of the four datasets, in terms of NDCG@K and HR@K which measure the recommendation accuracy. ' * ' means the improvements over the best baseline are statistical significant (t-tests and ğ‘-value &lt; 0.05). Underlines indicate the best-performed methods.</p><p>From the reported results, we can see that DEPS significantly outperformed nearly all of the baselines in terms of NDCG and HR expect NDCG@5 on Huawei commercial data, verified the effectiveness of DEPS in terms of improving the sequential recommendation accuracy. Moreover, DEPS significantly outperformed the unbiased models, demonstrating the importance of estimating propensity scores from the views of item and user sequential recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Analysis</head><p>We conducted more experiments to analyze DEPS, based on the Amazon-Digital-Music test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Ablation Study.</head><p>To further show the importance of estimating propensity scores with the two types of sequences from the view of user and the view of item, we also studied their unbiased performance in the second stage of training when the L ğ‘¢ğ‘›ğ‘ğ‘–ğ‘ğ‘ ğ‘’ğ‘‘ is optimized. Specifically, we showed the NDCG@K and HR@K of several DEPS variations. These variations include learning the recommendation model with no propensity score estimation (denoted as "w/o IPS"), estimating propensity scores with view of item sequences only ("w/o user-oriented IPS"), with the view of user sequences only ("w/o item-oriented IPS"). From the performances shown in Figure <ref type="figure" target="#fig_3">5</ref>, we found that (1) "w/o propensity" performed the worst, indicating the importance of propensity scores are in unbiased sequence recommendation; (2) "w/o user-oriented IPS" and "w/o item-oriented IPS" performed much better, indicating that the propensity scores estimated from either of the two views are effective; (3) DEPS with dual propensity scores performed best, verified the effectiveness of DEPS by using both views to conduct the propensity scores estimation.</p><p>Dual-transformer depends on several important mechanisms for estimating propensity scores and learning model parameters, including estimating with the interaction sequences from both h &lt;ğ‘¡ ğ‘¢ 's and h &lt;ğ‘¡ ğ‘¢ 's, and using the first stage unsupervised learning for initializing the parameters. Based on the Amazon-Digital-Music  According to the results reported in Table <ref type="table" target="#tab_4">3</ref>, we found that compared with the original DEPS, the performances of all DEPS variations dropped, indicating the importance of these mechanisms. Specifically, we found that the performances dropped a lot when either sequence of h &lt;ğ‘¡ ğ‘¢ 's or h &lt;ğ‘¡ ğ‘– 's were removed from the model, verified the importance of estimating propensity scores from both views simultaneously in sequence recommendation. The results also indicate that the first stage of unsupervised learning did enhance recommendation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Influence of Sequential IPS Estimation Methods.</head><p>In this section, we study the influence of sequential IPS estimation methods compared to non-sequential IPS estimation methods. In our model, the user-oriented and item-oriented IPS are estimated through GRU. We compared it with the non-sequential IPS estimation methods (frequency-based propensity score). The user-oriented IPS are calculated as ğ‘ ğ‘¢, * = ğ‘š ğ‘¢ /max ğ‘¢ â€² âˆˆI ğ‘š ğ‘¢ â€² and item-oriented IPS are calculated as ğ‘ * ,ğ‘– = ğ‘š ğ‘– /max ğ‘– â€² âˆˆI ğ‘š ğ‘– â€² , where ğ‘š ğ‘¢ and ğ‘š ğ‘– denotes the interaction numbers of user ğ‘¢ and item ğ‘–.</p><p>We studied their unbiased performance in the second stage of training when the unbiased loss function L unbiased is optimized. Specifically, we showed the NDCG@K and HR@K of several DEPS variations, including learning the recommendation model with a single propensity score ğ‘ * ,ğ‘– (denoted as "Item-Pro"), with a single propensity score ğ‘ ğ‘¢, * (denoted as "User-Pro"), and with dual propensity scores ğ‘ * ,ğ‘– , ğ‘ ğ‘¢, * (i.e., replacing estimated propensity score ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ), ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) in DEPS as ğ‘ * ,ğ‘– , ğ‘ ğ‘¢, * respectively, and denoted as "Item-User-Pro").</p><p>From the performances shown in Table <ref type="table" target="#tab_5">4</ref>, we found that (1) "DEPS" outperformed "Item-User-Pro" by a large margin, indicating the importance of estimating the propensity scores sequentially; (2) "Item-Pro" and "User-Pro" performed worse than "Item-User-Pro", indicating that the propensity scores estimated from both views (user or item) are effective and complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">DEPS as a</head><p>Model-Agnostic Framework. Though DEPS designs a transformer-based model for conducting the recommendation, it can also be used as a model-agnostic framework, by replacing the underlying model (the transformers and the MLP shown in Section 4.3) with other sequential recommendation models. In the experiments, we replaced it with GRU4Rec+ <ref type="bibr" target="#b31">[32]</ref> and FPMC <ref type="bibr" target="#b24">[25]</ref>,  achieving two new models, denoted as "DEPS (GRU4Rec+)" or "DEPS (FPMC)", respectively. Please note that the sequential recommendation models of GRU4Rec and FPCM cannot be trained on MLM tasks. Therefore, the loss functions in the first stage training of DEPS (GRU4Rec+) and DEPS (FPMC) degenerates to L AR ğ‘¢ + L AR ğ‘– . From the results reported in Table <ref type="table" target="#tab_6">5</ref>, we found that the DEPS (GRU4Rec+) and DEPS (FPMC) respectively achieved improvements over their underlying models of GRU4Rec+ and FPMC. The results indicate that the propensity scores estimated by DEPS is general. They can be used to improve other sequential recommendation models in a model-agnostic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.4</head><p>Impact of the Clipping Value M. According to Theorem 2, the clip value ğ‘€ balances the unbiasedness and variance in DEPS. In this experiment, we studied how NDCG@K and HR@K changed when the clip value ğ‘€ was set to different values from [0.05.0.2]. From the curves shown in Figure <ref type="figure" target="#fig_4">6</ref>, we found the performance improved when ğ‘€ âˆˆ [0.01, 0.05] and then dropped between [0.05, 0.2]. The results verified the theoretical analysis that too small ğ‘€ (e.g., ğ‘€ = 0.01) results in large variance estimation while too large ğ‘€ (e.g., ğ‘€ = 0.2) results in large bias. It is important to balance the unbiasedness and variance in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposes a novel IPS estimation method called Dually Enhanced Propensity Score Estimation (DEPS) to remedy the exposure or selection bias in the sequential recommendation. DEPS estimates the propensity scores from the views of item and user and offers several advantages: theoretical soundness, model-agnostic nature, and end2end learning. Extensive experimental results on four real datasets demonstrated that DEPS can significantly outperform the state-of-the-art baselines under the unbiased test settings. â–¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOF OF THEOREMS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Motivating example of unbiased sequential recommendation. (b) A toy example on the complementary roles of the user prediction problem for existing IPS methods.</figDesc><graphic coords="2,73.04,83.68,201.76,167.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Propensity score estimation in sequential recommendation. (a) Representing historical user-item interactions as a data cube, where item and user can be placed on the axis in any order; (b) Construction of two interaction sequences correspond to a target tuple (ğ‘¢, ğ‘–, ğ‘¡), from the views of item and user, respectively; (c) Using two GRUs to estimate the propensity scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two transformers based sequential recommender model with sequences from both users and items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Empirical analysis based on Amazon-Digital Music: DEPS variation performances of NDCG@K and HR@K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: NDCG and HR curves of DEPS w.r.t. clip value ğ‘€</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Two sequences correspond to (u, i) at time t ğ‘Ÿğ‘Ÿ (a) Biases in sequential recommendation (b) Other biases in recommendation</head><label></label><figDesc>It is expected that the predicted preference is close the true while un-observable user preference ğ‘Ÿ ğ‘¡ âˆˆ {0, 1} at time ğ‘¡, where ğ‘Ÿ ğ‘¡ = 1 means that ğ‘– is preferred by ğ‘¢ at time ğ‘¡, and 0 otherwise.</figDesc><table><row><cell>ğ‘–ğ‘– 1 ğ‘–ğ‘– 1</cell><cell></cell><cell></cell><cell>u 1</cell><cell></cell></row><row><cell>ğ‘–ğ‘– 2 ğ‘–ğ‘– 2 â‹¯</cell><cell>ğ’‰ğ’‰ ğ’–ğ’– &lt;ğ’•ğ’•</cell><cell>ğ’‰ğ’‰ ğ’Šğ’Š &lt;ğ’•ğ’•</cell><cell>ğ‘¢ğ‘¢ 2 â‹¯</cell><cell>ğ‘œğ‘œ ğ‘¢ğ‘¢ğ‘¢ğ‘¢</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘ğ‘</cell></row><row><cell></cell><cell>ğ‘œğ‘œ ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¡ğ‘¡</cell><cell></cell><cell>ğ‘¢ğ‘¢, ğ‘–ğ‘–</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ‘ğ‘ ğ‘¡ğ‘¡</cell><cell></cell><cell></cell></row><row><cell>ğ‘¢ğ‘¢, ğ‘–ğ‘–</cell><cell>ğ‘Ÿğ‘Ÿ ğ‘¡ğ‘¡</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 3: Causal graphs of bias in (sequential) recommendation.</cell></row><row><cell>time ğ‘¡.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a,b) illustrate that h &lt;ğ‘¡ ğ‘¢ and h &lt;ğ‘¡ ğ‘– are actually dual views</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of the data cube derived from D. Specifically, by considering the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>user, item, and time as three axes, D can be represented as a sparse</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data cube where the (ğ‘¢, ğ‘–, ğ‘¡)-th element is 1 if ğ‘¢ clicked ğ‘– at time ğ‘¡, 0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>if observed but not clicked, and NULL if not-interacted. It is obvious</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>that h &lt;ğ‘¡ ğ‘– and h &lt;ğ‘¡ ğ‘¢ are two views of the data cube: (1) view of item:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>given a user ğ‘¢, her/his historical interactions before ğ‘¡ are stored in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the matrix sliced by ğ‘¢. Since ğ‘¢ can only interact with one item at</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>a time, we can remove the non-clicked items, sort the remaining</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>items according to the time, and achieve the list h &lt;ğ‘¡ ğ‘¢ , where ğ‘™ (ğ‘¢, ğ‘¡)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>is the number of nonzero elements in the sliced matrix; (2) view of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>user: given an item ğ‘– and its interaction history, the matrix sliced</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>by ğ‘– can also be aggregated into another list h &lt;ğ‘¡ ğ‘– .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>The task of sequential recommendation becomes, based on the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>user-item interactions and users' feedback in D, learning a function</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rğ‘¡ = ğ‘“ (ğ‘¢, ğ‘–, h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– ) that predicts user ğ‘¢'s preference on item ğ‘– at</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: Learning Algorithm of DEPS Input: Training set D = {(ğ‘¢, ğ‘–, ğ‘ ğ‘¡ )}, iteration numbers ğ‘› ğ‘ , ğ‘› ğ‘¢ , ğ‘› ğ‘ , coefficients ğœ† ğ‘ Output: Î˜ = {ğœƒ ğ‘’ , ğœƒ ğ‘ , ğœƒ ğ‘¡ , ğœƒ ğ‘š } 1: Î˜ â† random values 2: H â† h &lt;ğ‘¡ ğ‘¢ |ğ‘¢ âˆˆ U âˆª h &lt;ğ‘¡ ğ‘– |ğ‘– âˆˆ I {Extract seq. from D} 3: for ğ‘› = 1, â€¢ â€¢ â€¢ , ğ‘› ğ‘ do</figDesc><table><row><cell>4:</cell><cell>Update ğœƒ ğ‘ by minimizing L AR ğ‘¢ + L AR ğ‘–</cell><cell></cell><cell></cell></row><row><cell>5:</cell><cell>Update ğœƒ ğ‘’ , ğœƒ ğ‘¡ by minimizing ğœ† ğ‘ (L MLM ğ‘¢</cell><cell>+ L MLM ğ‘–</cell><cell>)</cell></row><row><cell cols="2">6: end for</cell><cell></cell><cell></cell></row><row><cell cols="2">7: for ğ‘› = 1, â€¢ â€¢ â€¢ , ğ‘› ğ‘¢ do</cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell cols="2">Extract the sequence h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– from (ğ‘¢, ğ‘–, ğ‘¡)</cell><cell></cell></row><row><cell>9:</cell><cell>for ğ‘˜ = 1, â€¢ â€¢ â€¢ , ğ‘› ğ‘ do</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="3">Update ğœƒ ğ‘ by minimizing the loss L AR ğ‘¢ + L AR ğ‘– .</cell></row><row><cell>11:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>12:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#User #Item #Interaction Sparsity</cell></row><row><cell>MIND</cell><cell cols="2">13863 2464</cell><cell>59228</cell><cell>99.82%</cell></row><row><cell>Amazon-Beauty</cell><cell cols="2">24411 32371</cell><cell>94641</cell><cell>99.98%</cell></row><row><cell cols="2">Amazon-Digital-Music 4424</cell><cell>5365</cell><cell>32314</cell><cell>99.86%</cell></row><row><cell>Huawei</cell><cell cols="2">1997 17490</cell><cell>245564</cell><cell>99.29%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons between DEPS and the baselines on MIND, Beauty, Music, and Huawei datasets. ' * ' means the improvements over the best baseline (the underlined number) are statistical significant (t-tests and ğ‘-value &lt; 0.05).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Sequential recommender baselines</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Unbiased recommender baselines</cell><cell>Our approach</cell></row><row><cell cols="2">Dataset Metric</cell><cell>STAMP</cell><cell>DIN</cell><cell cols="3">BERT4Rec FPMC GRU4Rec+</cell><cell>BST</cell><cell>LightSANs</cell><cell>UIR</cell><cell>CPR</cell><cell>UBPR</cell><cell>DICE</cell><cell>USR</cell><cell>DEPS</cell><cell>Improv.</cell></row><row><cell></cell><cell cols="2">NDCG@5 0.0471</cell><cell>0.1149</cell><cell>0.0900</cell><cell>0.0670</cell><cell>0.0865</cell><cell>0.0865</cell><cell>0.1148</cell><cell cols="5">0.0594 0.0582 0.0588 0.0612 0.0658 0.1197*</cell><cell>4.2%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.0669</cell><cell>0.1548</cell><cell>0.1277</cell><cell>0.1006</cell><cell>0.1306</cell><cell>0.1233</cell><cell>0.1650</cell><cell cols="5">0.0823 0.0847 0.0863 0.0861 0.0955 0.1728*</cell><cell>4.7%</cell></row><row><cell>MIND</cell><cell cols="2">NDCG@20 0.0997 HR@5 0.0861</cell><cell>0.1948 0.2090</cell><cell>0.1817 0.1671</cell><cell>0.1400 0.1229</cell><cell>0.1819 0.1504</cell><cell>0.1753 0.1607</cell><cell>0.2159 0.2024</cell><cell cols="5">0.1233 0.1204 0.1237 0.1235 0.1339 0.2249* 0.1141 0.1037 0.1048 0.1201 0.1207 0.2200*</cell><cell>4.2% 8.7%</cell></row><row><cell></cell><cell>HR@10</cell><cell>0.1519</cell><cell>0.3379</cell><cell>0.2922</cell><cell>0.2359</cell><cell>0.2918</cell><cell>0.2825</cell><cell>0.3692</cell><cell cols="5">0.1909 0.1898 0.1942 0.2030 0,2194 0.3961*</cell><cell>7.3%</cell></row><row><cell></cell><cell>HR@20</cell><cell>0.2863</cell><cell>0.5019</cell><cell>0.5145</cell><cell>0.3999</cell><cell>0.5052</cell><cell>0.4948</cell><cell>0.5760</cell><cell cols="5">0.3577 0.3379 0.3511 0.3571 0.3807 0.6078*</cell><cell>5.5%</cell></row><row><cell></cell><cell cols="2">NDCG@5 0.0985</cell><cell>0.1139</cell><cell>0.1008</cell><cell>0.1225</cell><cell>0.1050</cell><cell>0.1156</cell><cell>0.1312</cell><cell cols="5">0.1188 0.1172 0.0918 0.1238 0.1046 0.1362*</cell><cell>3.8%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.1330</cell><cell>0.1407</cell><cell>0.1333</cell><cell>0.1563</cell><cell>0.1438</cell><cell>0.1548</cell><cell>0.1661</cell><cell cols="5">0.1603 0.1465 0.1214 0.1645 0.1429 0.1830*</cell><cell>10.2%</cell></row><row><cell cols="3">Amazon-NDCG@20 0.1780</cell><cell>0.1699</cell><cell>0.1722</cell><cell>0.1962</cell><cell>0.1919</cell><cell>0.1983</cell><cell>0.2078</cell><cell cols="5">0.2109 0.1823 0.1580 0.2127 0.1903 0.2302*</cell><cell>8.2%</cell></row><row><cell>Beauty</cell><cell>HR@5</cell><cell>0.1945</cell><cell>0.2160</cell><cell>0.1964</cell><cell>0.2292</cell><cell>0.2132</cell><cell>0.2134</cell><cell>0.2430</cell><cell cols="5">0.2338 0.2119 0.1780 0.2365 0.2064 0.2557*</cell><cell>8.1%</cell></row><row><cell></cell><cell>HR@10</cell><cell>0.3349</cell><cell>0.3174</cell><cell>0.3166</cell><cell>0.3530</cell><cell>0.3555</cell><cell>0.3679</cell><cell>0.3696</cell><cell cols="5">0.3836 0.3176 0.2879 0.3834 0.3435 0.4175*</cell><cell>8.8%</cell></row><row><cell></cell><cell>HR@20</cell><cell>0.5364</cell><cell>0.4434</cell><cell>0.4864</cell><cell>0.5233</cell><cell>0.5656</cell><cell>0.5524</cell><cell>0.5424</cell><cell cols="5">0.5980 0.4756 0.4465 0.5890 0.5470 0.6130*</cell><cell>2.5%</cell></row><row><cell></cell><cell cols="2">NDCG@5 0.0727</cell><cell>0.1220</cell><cell>0.1419</cell><cell>0.1355</cell><cell>0.0651</cell><cell>0.1556</cell><cell>0.1940</cell><cell cols="5">0.0691 0.1664 0.1353 0.1170 0.0921 0.2256*</cell><cell>16.3%</cell></row><row><cell cols="3">Amazon-NDCG@10 0.0961</cell><cell>0.1513</cell><cell>0.1771</cell><cell>0.1799</cell><cell>0.0961</cell><cell>0.1976</cell><cell>0.2192</cell><cell cols="5">0.0912 0.2121 0.1643 0.1432 0.1222 0.2766*</cell><cell>30.4%</cell></row><row><cell cols="3">Digital-NDCG@20 0.1305</cell><cell>0.1890</cell><cell>0.2231</cell><cell>0.2153</cell><cell>0.1385</cell><cell>0.2454</cell><cell>0.2586</cell><cell cols="5">0.1317 0.2558 0.2060 0.1789 0.1643 0.3273*</cell><cell>26.6%</cell></row><row><cell>Music</cell><cell>HR@5</cell><cell>0.1373</cell><cell>0.2745</cell><cell>0.2606</cell><cell>0.2941</cell><cell>0.1709</cell><cell>0.3249</cell><cell>0.3613</cell><cell cols="5">0.1541 0.3473 0.2521 0.2185 0.2017 0.4093*</cell><cell>13.3%</cell></row><row><cell></cell><cell>HR@10</cell><cell>0.2437</cell><cell>0.3950</cell><cell>0.4249</cell><cell>0.4391</cell><cell>0.3025</cell><cell>0.4902</cell><cell>0.4737</cell><cell cols="5">0.2437 0.5014 0.3754 0.3361 0.3305 0.5852*</cell><cell>16.7%</cell></row><row><cell></cell><cell>HR@20</cell><cell>0.4034</cell><cell>0.5518</cell><cell>0.5949</cell><cell>0.6091</cell><cell>0.4874</cell><cell>0.6779</cell><cell>0.6162</cell><cell cols="5">0.4314 0.6415 0.5546 0.4902 0.5098 0.7757*</cell><cell>14.4%</cell></row><row><cell></cell><cell cols="2">NDCG@5 0.0919</cell><cell>0.1081</cell><cell>0.1247</cell><cell>0.1079</cell><cell>0.1090</cell><cell>0.1016</cell><cell>0.1335</cell><cell cols="5">0.0554 0.1414 0.1033 0.0670 0.0922</cell><cell>0.1400</cell><cell>-1.0%</cell></row><row><cell></cell><cell cols="2">NDCG@10 0.1024</cell><cell>0.1195</cell><cell>0.1352</cell><cell>0.1193</cell><cell>0.1163</cell><cell>0.1125</cell><cell>0.1323</cell><cell cols="5">0.0634 0.1489 0.1192 0.0767 0.1042 0.1503*</cell><cell>1.0%</cell></row><row><cell>Huawei</cell><cell cols="2">NDCG@20 0.1202 HR@5 0.3569</cell><cell>0.1421 0.3874</cell><cell>0.1561 0.4192</cell><cell>0.1416 0.3954</cell><cell>0.1357 0.3941</cell><cell>0.1344 0.3594</cell><cell>0.1454 0.4576</cell><cell cols="5">0.0793 0.1711 0.1409 0.0926 0.1234 0.1755* 0.2129 0.4726 0.3880 0.2453 0.3435 0.4765*</cell><cell>2.5% 0.9%</cell></row><row><cell></cell><cell>HR@10</cell><cell>0.5540</cell><cell>0.6138</cell><cell>0.6242</cell><cell>0.6174</cell><cell>0.5833</cell><cell>0.5625</cell><cell>0.6315</cell><cell cols="5">0.3569 0.6730 0.6132 0.4155 0.5552 0.6852*</cell><cell>1.8%</cell></row><row><cell></cell><cell>HR@20</cell><cell>0.7404</cell><cell>0.8011</cell><cell>0.8090</cell><cell>0.8188</cell><cell>0.7890</cell><cell>0.7785</cell><cell>0.7889</cell><cell cols="5">0.5625 0.8603 0.8115 0.6107 0.7529 0.8713*</cell><cell>1.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on Amazon-Digital-Music test set.</figDesc><table><row><cell>Metric</cell><cell></cell><cell>NDCG@K</cell><cell></cell><cell></cell><cell>HR@K</cell><cell></cell></row><row><cell>K</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>w/o h &lt;ğ‘¡ ğ‘¢</cell><cell>0.2161</cell><cell>0.2720</cell><cell>0.3192</cell><cell>0.3599</cell><cell>0.5467</cell><cell>0.7115</cell></row><row><cell>ğ‘– w/o h &lt;ğ‘¡</cell><cell>0.2034</cell><cell>0.2541</cell><cell>0.3035</cell><cell>0.3819</cell><cell>0.5383</cell><cell>0.7253</cell></row><row><cell cols="2">w/o stage-1 0.1953</cell><cell>0.2423</cell><cell>0.2869</cell><cell>0.3489</cell><cell>0.5082</cell><cell>0.6869</cell></row><row><cell>DEPS</cell><cell cols="6">0.2256 0.2766 0.3273 0.4093 0.5852 0.7747</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons between DEPS and nonsequential IPS methods on Amazon-Digital-Music test set.</figDesc><table><row><cell>Metric</cell><cell></cell><cell>NDCG@K</cell><cell></cell><cell></cell><cell>HR@K</cell><cell></cell></row><row><cell>K</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Item-Pro</cell><cell cols="6">0.1984 0.2385 0.1928 0.3819 0.5220 0.7170</cell></row><row><cell>User-Pro</cell><cell cols="6">0.2044 0.2494 0.2983 0.3681 0.5275 0.7088</cell></row><row><cell cols="7">Item-User-Pro 0.2133 0.2665 0.3156 0.3736 0.5495 0.7273</cell></row><row><cell>DEPS</cell><cell cols="6">0.2256 0.2766 0.3273 0.4093 0.5852 0.7747</cell></row><row><cell cols="7">dataset, we conducted ablation studies to test the performances of</cell></row><row><cell cols="7">DEPS variations by removing these components shown in Table 3.</cell></row><row><cell cols="7">These DEPS variations include: estimating the propensity scores</cell></row><row><cell cols="7">and conducting recommendation without using h &lt;ğ‘¡ ğ‘¢ (denoted as</cell></row><row><cell cols="7">"w/o h &lt;ğ‘¡ ğ‘¢ "), without using h &lt;ğ‘¡ ğ‘– (denoted as "w/o h &lt;ğ‘¡ ğ‘– "), and training</cell></row><row><cell cols="6">by skipping the first stage tuning (denoted "w/o stage-1").</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons between DEPS variationswhere the underlying recommendation model is replaced with GRU4Rec+ or FPMC. Experiments were conducted on Amazon-Digital-Music and ' * ' means the improvements over the underlying models are statistical significant (t-test and ğ‘-value &lt; 0.05).</figDesc><table><row><cell cols="2">Metric</cell><cell></cell><cell></cell><cell></cell><cell cols="2">NDCG@K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HR@K</cell></row><row><cell cols="2">TopK</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell cols="2">20</cell><cell>5</cell><cell></cell><cell>10</cell><cell>20</cell></row><row><cell cols="2">GRU4Rec+</cell><cell></cell><cell></cell><cell>0.0651</cell><cell></cell><cell>0.0961</cell><cell cols="2">0.1385</cell><cell cols="2">0.1709</cell><cell>0.3025</cell><cell>0.4874</cell></row><row><cell cols="13">DEPS(GRU4Rec+) 0.0861* 0.1219* 0.1608* 0.1905* 0.3272* 0.5182*</cell></row><row><cell cols="2">FPMC</cell><cell></cell><cell></cell><cell>0.1355</cell><cell></cell><cell>0.1799</cell><cell cols="2">0.2153</cell><cell cols="2">0.2941</cell><cell>0.4391</cell><cell>0.6091</cell></row><row><cell cols="3">DEPS(FPMC)</cell><cell cols="10">0.1396* 0.1858* 0.2189* 0.3025* 0.4566* 0.6426*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR@5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG@5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR@10 HR@20</cell><cell></cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell>NDCG@10 NDCG@20</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HR@K</cell><cell>0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG@K</cell><cell>0.24 0.26 0.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.22</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell>0.02</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell></cell><cell>0.01</cell><cell>0.02</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">clip M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">clip M</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) HR@K curve</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) NDCG@K curve</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>A.1 Proof of Theorem 1 Proof. Let ğ‘ h &lt;ğ‘¡ ğ‘¢ ,ğ‘– âˆˆ {0, 1} where ğ‘ h &lt;ğ‘¡ ğ‘¢ ,ğ‘– = 1 indicates item ğ‘– is observed in the historical interaction sentence h &lt;ğ‘¡ ğ‘¢ , otherwise ğ‘ h &lt;ğ‘¡ ğ‘¢ ,ğ‘– = 0. According to the definition, ğ‘ƒ (ğ‘ h &lt;ğ‘¡ ğ‘¢ ,ğ‘– = 1) = ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) in sequential recommendation. Abbreviate historical information h &lt;ğ‘¡ ğ‘¢ , h &lt;ğ‘¡ ğ‘– to ğ» . Therefore, E ğ‘œ [L ğ‘¢ ] = E ğ‘œ ğ‘ ğ‘¡ ) âˆˆD ğ‘¢ ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ (ğ‘¢, ğ‘– ğ‘¡ , ğ» )) ğ‘ƒ (ğ‘– ğ‘¡ , h &lt;ğ‘¡ ğ‘¢ ) &lt;ğ‘¡ ğ‘– ,ğ‘¢ âˆˆ {0, 1} where ğ‘ h &lt;ğ‘¡ ğ‘– ,ğ‘¢ = 1 indicates the user ğ‘¢ appeared in the historical interaction sentence h &lt;ğ‘¡ ğ‘– . We have ğ‘ƒ (ğ‘ h &lt;ğ‘¡ ğ‘– ,ğ‘¢ = 1) = ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ), and E ğ‘œ [L ğ‘– ] = E ğ‘œ Proof. Following the notations defined Theorem 1, we can write ğ¿ ğ‘¡ ğ‘¢ = ğ›¿ (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) ğ‘ (ğ‘¢,h &lt;ğ‘¡ ğ‘– ) ğ‘ h &lt;ğ‘¡ ğ‘– ,ğ‘¢ , and according to the clip operation shown in Eq. (7) we have ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) â‰¥ ğ‘€. Therefore, ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) â‰¤ 1 ğ‘€ -1)ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) . Applying the Cauchy-Schwarz' inequality, we can bound the variance of average loss L unbiased ğ‘  = ğ›¼ğ¿ ğ‘¡ ğ‘– + (1 -ğ›¼)ğ¿ ğ‘¡ ğ‘¢ as: V L unbiased ğ‘  = ğ›¼ 2 V ğ¿ ğ‘¡ ğ‘– + (1 -ğ›¼) 2 V ğ¿ ğ‘¡ ğ‘¢ + 2ğ›¼ (1 -ğ›¼)Cov(ğ¿ ğ‘¡ ğ‘– , ğ¿ ğ‘¡ ğ‘¢ ) â‰¤ ğ›¼ 2 V ğ¿ ğ‘¡ ğ‘– + (1 -ğ›¼) 2 V ğ¿ ğ‘¡ ğ‘¢ + 2ğ›¼ (1 -ğ›¼)</figDesc><table><row><cell>â‰¤ max V ğ¿ ğ‘¡ ğ‘– , V ğ¿ ğ‘¡ ğ‘¢</cell><cell>â‰¤ (</cell><cell>ğ‘€ 1</cell><cell>ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° ğ‘¢ âˆˆU âˆ‘ï¸ âˆ‘ï¸ ğ‘¢ âˆˆU ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° âˆ‘ï¸ ğ‘– âˆˆI = = âˆ‘ï¸ ğ‘– âˆˆI ğ‘¢ = E ğ¿ ğ‘¡ V ğ¿ ğ‘¡ ğ‘¢ 2 -E ğ¿ ğ‘¡ âˆ‘ï¸ (ğ‘–,ï£¹ ï£º ï£º ï£º ï£º ï£» âˆ‘ï¸ âˆ‘ï¸ âˆ‘ï¸ ğ‘¢ 2 = ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) -ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) ï£¹ ï£º ï£º ï£º ï£º ï£» ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) = 1 -1 âˆšï¸ƒ V ğ¿ ğ‘¡ ğ‘– V ğ¿ ğ‘¡ ğ‘¢ -1)ğ›¿ 2 (ğ‘Ÿ, r ).</cell></row></table><note><p>ğ‘¡ :(ğ‘¢,ğ‘– â€² ,ğ‘ ğ‘¡ ) âˆˆD âˆ‘ï¸ ğ‘– âˆˆI E ğ‘ ğ‘ h &lt;ğ‘¡ ğ‘¢ ,ğ‘– â€¢ ğ›¿ (ğ‘Ÿ ğ‘¡ , rğ‘¡ (ğ‘¢, ğ‘–, ğ» )) ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘¡ :(ğ‘¢,ğ‘– â€² ,ğ‘ ğ‘¡ ) âˆˆD âˆ‘ï¸ ğ‘– âˆˆI ğ›¿ (ğ‘Ÿ ğ‘¡ , rğ‘¡ (ğ‘¢, ğ‘–, ğ» )) = L ideal ğ‘  Similarly, let ğ‘ h (ğ‘–,ğ‘ ğ‘¡ ) âˆˆD ğ‘– ğ›¿ (ğ‘ ğ‘¡ , rğ‘¡ (ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ +1) , ğ‘–, ğ» )) ğ‘ƒ (ğ‘¢ ğ‘™ (ğ‘–,ğ‘¡ +1) , h &lt;ğ‘¡ ğ‘– ) ğ‘¡ :(ğ‘¢,ğ‘– â€² ,ğ‘ ğ‘¡ ) âˆˆD âˆ‘ï¸ ğ‘¢ âˆˆU E ğ‘ ğ‘ h &lt;ğ‘¡ ğ‘– ,ğ‘¢ ğ›¿ (ğ‘Ÿ, r (ğ‘¢, ğ‘–, ğ» )) ğ‘ƒ (ğ‘¢, h &lt;ğ‘¡ ğ‘– ) = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ‘ï¸ ğ‘¡ :(ğ‘¢,ğ‘– â€² ,ğ‘ ğ‘¡ ) âˆˆD âˆ‘ï¸ ğ‘– âˆˆI ğ›¿ (ğ‘Ÿ ğ‘¡ , rğ‘¡ (ğ‘¢, ğ‘–, ğ» )) = L ideal ğ‘  . Therefore, L ideal ğ‘  = E L unbiased ğ‘  = E [ğ›¼ L ğ‘¢ + (1 -ğ›¼)L ğ‘– ] â–¡ A.2 Proof of Theorem 2 Similarly, we can bound the variance of objective functions ğ¿ ğ‘¡ ğ‘– as, V ğ¿ ğ‘¡ ğ‘– = 1 ğ‘ƒ (ğ‘–, h &lt;ğ‘¡ ğ‘¢ ) -1 ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ) â‰¤ 1 ğ‘€ -1 ğ›¿ 2 (ğ‘Ÿ ğ‘¡ , rğ‘¡ ).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The source code is shared at https://github.com/XuChen0427/Dually-Enhanced-Propensity-Score-Estimation-in-Sequential-Recommendation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://msnews.github.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://jmcauley.ucsd.edu/data/amazon/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was funded by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2019YFE0198200</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61872338</rs>, <rs type="grantNumber">62102420</rs>, <rs type="grantNumber">61832017</rs>), <rs type="programName">Beijing Outstanding Young Scientist Program</rs> <rs type="grantNumber">NO. BJJWZYJH012019100020098</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jwTVR3S">
					<idno type="grant-number">2019YFE0198200</idno>
				</org>
				<org type="funding" xml:id="_T8qbUCC">
					<idno type="grant-number">61872338</idno>
				</org>
				<org type="funding" xml:id="_7QDUdWS">
					<idno type="grant-number">62102420</idno>
				</org>
				<org type="funding" xml:id="_9dyvQnK">
					<idno type="grant-number">61832017</idno>
					<orgName type="program" subtype="full">Beijing Outstanding Young Scientist Program</orgName>
				</org>
				<org type="funding" xml:id="_Cyg2pRu">
					<idno type="grant-number">NO. BJJWZYJH012019100020098</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-sided Exposure Bias in Recommendation</title>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Mansoury</surname></persName>
		</author>
		<idno>CoRR abs/2006.15772</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating position bias without intrusive interventions</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/2010.03240</idno>
		<title level="m">Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling Users&apos; Exposure with Social Knowledge Influence and Consumption Influence for Recommendation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="953" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast adaptively weighted matrix factorization for recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3470" to="3477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Samwalker: Social recommendation with informative sampling strategy</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="228" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Behavior sequence transformer for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</title>
		<meeting>the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã‡aglar</forename><surname>GÃ¼lÃ§ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP. ACL</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP. ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominika</forename><surname>Tkaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joeran</forename><surname>Beel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06565</idno>
		<title level="m">A study of position bias in digital library recommender systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic matrix factorization with priors on unknown values</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Devooght</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Kourtellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Mantrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD Conference</title>
		<meeting>the 21th ACM SIGKDD Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lighter and better: low-rank decomposed self-attention networks for next-item recommendation</title>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1733" to="1737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous-time sequential recommendation with temporal graph collaborative transformer</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM CIKM Conference</title>
		<meeting>the 30th ACM CIKM Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intervention harvesting for context-dependent examination-bias estimation</title>
		<author>
			<persName><forename type="first">Zhichong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="825" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unbiased learning-to-rank with biased feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A general knowledge distillation framework for counterfactual recommendation via uniform data</title>
		<author>
			<persName><forename type="first">Dugang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference</title>
		<meeting>the 43rd International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="831" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">STAMP: shortterm attention/memory priority model for session-based recommendation</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD</title>
		<meeting>the 24th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5267</idno>
		<title level="m">Collaborative filtering and the missing at random assumption</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Decomposable Attention Model for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on EMNLP</title>
		<meeting>the 2016 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PyTorch</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attribute-based propensity for unbiased learning in recommender systems: Algorithm and case studies</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwoo</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzheng</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asymmetric Tri-training for Debiasing Missing-Not-At-Random Explicit Feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased pairwise learning from biased implicit feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbiased recommender learning from missing-not-at-random implicit feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suguru</forename><surname>Yaginuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Sakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Nakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unbiased recommender learning from missing-not-at-random implicit feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suguru</forename><surname>Yaginuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Sakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Nakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved recurrent neural networks for session-based recommendations</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Kiam Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross Pairwise Ranking for Unbiased Item Recommendation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2370" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unbiased Sequential Recommendation with Latent Confounders</title>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2195" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Counterfactual data-augmented sequential recommendation</title>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Matrix Factorization Models for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hong-Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causerec: Counterfactual user sequence synthesis for sequential recommendation</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06067</idno>
		<title level="m">Causal Intervention for Leveraging Popularity Bias in Recommendation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Shanlei</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushuo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01731[cs.IR]</idno>
		<title level="m">RecBole: Towards a Unified, Comprehensive and Efficient Framework for Recommendation Algorithms</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11011</idno>
		<title level="m">Disentangling user interest and popularity bias for recommendation with causal embedding</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disentangling user interest and conformity for recommendation with causal embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2980" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep interest evolution network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5941" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
