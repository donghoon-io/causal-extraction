<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation</title>
				<funder ref="#_rtq584Z">
					<orgName type="full">Australian Research Council Discovery Project</orgName>
				</funder>
				<funder ref="#_56NcgWE #_VzzczJt">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-13">13 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-13">13 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475266</idno>
					<idno type="arXiv">arXiv:2107.02390v3[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visually-aware</term>
					<term>causal inference</term>
					<term>debiased recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visually-aware recommendation on E-commerce platforms aims to leverage visual information of items to predict a user's preference for these items in addition to the historical user-item interaction records. It is commonly observed that user's attention to visual features does not always reflect the real preference. Although a user may click and view an item in light of a visual satisfaction of their expectations, a real purchase does not always occur due to the unsatisfaction of other essential features (e.g., brand, material, price). We refer to the reason for such a visually related interaction deviating from the real preference as a visual bias. Existing visually-aware models make use of the visual features as a separate collaborative signal similarly to other features to directly predict the user's preference without considering a potential bias, which gives rise to a visually biased recommendation. In this paper, we derive a causal graph to identify and analyze the visual bias of these existing methods. In this causal graph, the visual feature of an item acts as a mediator, which could introduce a spurious relationship between the user and the item. To eliminate this spurious relationship that misleads the prediction of the user's real preference, an intervention and a counterfactual inference are developed over the mediator. Particularly, the Total Indirect Effect is applied for a debiased prediction during the testing phase of the model. This causal inference framework is model agnostic such that it can be integrated into the existing methods. Furthermore, we propose a debiased visuallyaware recommender system, denoted as CausalRec to effectively retain the supportive significance of the visual information and remove the visual bias. Extensive experiments are conducted on eight benchmark datasets, which shows the state-of-the-art performance of CausalRec and the efficacy of debiasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visually-aware recommendation on E-commerce platforms takes the visual information of items into account in predicting a user's preference of these items in addition to the historical user-item interactions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55]</ref>. Compared with traditional recommender systems, visually-aware methods improve the recommendation performance in many scenarios, e.g., shopping garments, where the users' preference is largely related to the appearance of items.</p><p>Although the visual feature is commonly used along with other features (e.g., brand, material, price) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>, the widelyused collaborative signal modeling of the visual feature is actually performing a biased learning scheme due to the visual feature itself. How could the visual feature give rise to bias while playing an important role in the recommendation? An example of the bias from the visual feature in buying white t-shirts is presented in Figure <ref type="figure">1</ref>. Imagine that a user is looking for a white t-shirt made of cotton. When white t-shirts made of fabric or polyester are shown to the user, it is very likely for the user to click them because their appearance perfectly fits the user's need yet it will not lead to a purchase. These interaction records are imprecise for training the model for this user since these clicks do not reflect the real preference for the clicked items. Unfortunately, in most cases, all the clicks will be logged by the platform without discrimination. We refer to this mismatch between the interaction records and the real preference resulted by the visual feature as visual bias. Existing visually-aware recommender systems are mainly trained on visually biased records without debiasing procedure <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>There exist various biases in recommendations, e.g., position bias, selection bias and popularity, which trigger the emergence of a few debiasing approaches correspondingly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. However, these approaches can hardly be applied to eliminate the visual bias, which originates from the item itself rather than the external bias mentioned above.</p><p>Recently, causal inference <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> has shown a great potential in removing the bias embedded in the data itself for vision-language tasks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. Generally, in these methods, a causal graph is built to indicate a causal effect between different components for their tasks, where the causal effect quantifies the impact of a certain component on another one. To analyze the causal effect, intervention and counterfactual inference are collectively common tools to provide debiased calculation results.</p><p>Figure <ref type="figure">1</ref>: Example of visual bias on buying white t-shirts. A user is looking for a white t-shirt made of the suitable material. The user would click all of these white t-shirts of different materials since they look exactly like the target. However, the user's real preference relies on the material as well. With implicit feedback, it is difficult to tell if an interaction represents a real preference or just a spurious relationship purely between the visual feature and the interaction.</p><p>In light of the promising ability in removing bias of causal inference, we explore the way to adopt it for eliminating the visual bias in visually-aware recommendations. As a crucial step, we first identify the important factors in the recommendation: the user ID, the item ID, the visual feature of the item, the user-item preference match, the user's visual notice and the interaction. We introduce the user's notice of the visual feature of an item to indicate the user's pure visual preference without the influence by any other features. In many real-world shopping scenarios, the user's visual notice would strongly lead to user-item interactions when lacking other information (e.g., materials, brand, etc) for users' consideration at first glance of items. Therefore, it is expected to remove the causal effect of the visual notice in predicting the preference based on the biased interaction records. Interventions and the counterfactual inference are leveraged in this paper to pursue an unbiased prediction. The main idea is by asking the following question:</p><p>If a user had seen other items with the same visual feature, would this user still interact with these items?</p><p>The counterfactual thinking is shown by comparing the fact that the user has already interacted with an item and the imagination that the user "had seen other items with the same visual feature". After the comparison between these two situations, the direct visual effect is naturally identified since the visual feature is the only thing remaining unchanged. When this direct visual effect is eliminated, the prediction of the preference is expected to be visually debiased.</p><p>Specifically, in this paper, a causal graph is developed to analyze the visual bias in existing visually-aware recommendation methods. To perform the debiased recommendation for these methods, we propose to make use of the Total Indirect Effect (TIE) in the inference phase of these methods. Furthermore, a causal inference-based novel recommender model (CausalRec) is proposed to retain the supportive visual information and perform visual debiasing. The contributions of this paper are as follows:</p><p>â€¢ A causal inference-based framework is derived to identify, analyze and remove the visual bias in existing visually-aware recommender systems. To the best of our knowledge, this is the first attempt in this research area.</p><p>â€¢ A novel CausalRec model is proposed to unbiasedly make use of the visual feature whilst remove the visual bias in the visually-aware recommendation.</p><p>â€¢ Extensive experiments are conducted on eight datasets and the results demonstrate the efficacy of the causal inference module and the state-of-the-art performance of CausalRec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, the basic concepts of causal inference <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> are provided. In the following, capital letters are used for random variables and lowercase letters for an observation of random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Graph</head><p>A causal graph is a directed acyclic graph that represents the causal relationship between random variables. The causal graph is denoted as G = (V, E), where V stands for a set of random variables (nodes) in the graph and E denotes the cause-and-effect relationships (edges) between those variables. Figure <ref type="figure" target="#fig_1">2</ref> (a) demonstrates an example of a causal graph, which includes three random variables ğ´, ğµ and ğ¶. In this figure, a few causal relationships can be identified. Since the variable ğ´ has a direct effect on another variable ğµ, the causal path ğ´ â†’ ğµ indicates that ğ´ is a cause of ğµ. Meanwhile, both ğ´ and ğµ are the causes of ğ¶. If the causal effect of ğ´ towards ğ¶ is to be investigated, there two corresponding causal paths linking ğ´ and ğ¶ together: ğ´ â†’ ğ¶ and ğ´ â†’ ğµ â†’ ğ¶, accounting for the direct effect and the indirect effect respectively. When there is a treatment ğ‘ for node ğ´, it will has a causal effect on ğµ and ğ¶ so that they become ğµ ğ‘ and ğ¶ ğ‘,ğµ ğ‘ as in Figure <ref type="figure" target="#fig_1">2 (a)</ref>. If ğ´ is assigned to the value ğ‘ * , which stands for no-treatment in this paper with a null value or an average value for this variable <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, then ğµ and ğ¶ will become ğµ ğ‘ * and ğ¶ ğ‘ * ,ğµ ğ‘ * under this no-treatment. The shadowed nodes in Figure <ref type="figure" target="#fig_1">2</ref> (b) stand for the no-treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intervention</head><p>In causal inference, an intervention is an operation to cut off the incoming edges towards certain nodes. For example, in the causal graph in Figure <ref type="figure" target="#fig_1">2</ref> (a), if the direct effect of ğ´ on ğ¶ is of interest to investigate, the causal path ğ´ â†’ ğµ â†’ ğ¶ will become a spurious relationship since it introduces bias in the estimation of ğ‘ƒ (ğ¶ | ğ´) according to Bayes rule: ğ‘ƒ (ğ¶ | ğ´) = ğ‘ ğ‘ƒ (ğ¶ | ğ´, ğ‘)ğ‘ƒ (ğ‘ | ğ´), where we slightly abuse the notation ğ‘ƒ (ğµ = ğ‘) = ğ‘ƒ (ğ‘) and the mediator ğµ introduces an observation bias through ğ‘ƒ (ğ‘ | ğ´). If an intervention is exerted the node ğµ to set a certain value ğ‘, i.e., ğ‘‘ğ‘œ (ğµ = ğ‘) (simplified to ğ‘‘ğ‘œ (ğµ)), the causal path between ğ´ and ğµ is cutoff. This omission of the edge is shown in Figure <ref type="figure" target="#fig_1">2 (c)</ref> and<ref type="figure">(d)</ref>. Since this intervention has eliminated the relationship between ğ´ and ğµ, applying Bayes rule: ğ‘ƒ (ğ¶ | ğ‘‘ğ‘œ (ğ´)) = ğ‘ ğ‘ƒ (ğ¶ | ğ´, ğ‘)ğ‘ƒ (ğ‘). Here, ğµ is not affected by ğ´ anymore, and vice versa, which requires to calculate the condition on ğ‘ fairly. Note that Figure <ref type="figure" target="#fig_1">2</ref>    The edges in the graph indicate the causal relationship between nodes. For example, the causal path ğ´ â†’ ğµ represents that ğ´ is the cause of ğµ. And since there are two causal paths (ğ´ â†’ ğ¶ and ğµ â†’ ğ¶) directing to ğ¶, both ğ´ and ğµ are the causes of ğ¶. When there is an observation of ğ´ = ğ‘, ğµ becomes ğµ ğ‘ because it is based on ğ´. Similarly, ğ¶ becomes ğ¶ ğ‘,ğµ ğ‘ . (b) A no-treatment assigns ğ´ = ğ‘ * and this no-treatment results in ğ¶ ğ‘ * ,ğµ * . (c) An intervention is operated on node ğµ with a no-treatment to assign ğ´ = ğ‘ * while leaving ğµ unchanged. In this situation, the causal path of ğ´ â†’ ğµ is removed. The result of this no-treatment is ğ¶ ğ‘ * ,ğµ . (d) An intervention is operated on node ğµ and a no-treatment ğ´ = ğ‘ * affects the causal path ğ´ â†’ ğµ â†’ ğ¶ instead of ğ´ â†’ ğ¶.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Counterfactual Notations</head><p>Counterfactual notations are used to translate the causal effect assumption from the causal graph to formulas. In the counterfactual situation, ğ´ is set to a different value ğ‘ * for different causal paths as in Section 2.2 above. Under this situation, ğ¶ will become either ğ¶ ğ‘ * ,ğµ ğ‘ or ğ¶ ğ‘,ğµ ğ‘ * . These situations are called counterfactual because they do not really happen in the real world. It is imagined to investigate how a certain factor would affect the final outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Causal Effect</head><p>Comparing the potential outcome after the counterfactual inference with the real outcome from an observation, the causal effect of the treatment used for the counterfact can be evaluated <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref>. Assume that Figure <ref type="figure" target="#fig_1">2</ref>  (</p><formula xml:id="formula_0">)<label>1</label></formula><p>If the intervention is exerted according to Figure <ref type="figure" target="#fig_1">2</ref> (d), the Natural Direct Effect (NDE) can be derived as:</p><formula xml:id="formula_1">NDE = ğ¶ ğ‘,ğµ ğ‘ * -ğ¶ ğ‘ * ,ğµ ğ‘ * .<label>(2)</label></formula><p>Based on TE and NDE, total indirect effect (TIE) is defined as:</p><formula xml:id="formula_2">TIE = TE -NDE = ğ¶ ğ‘,ğµ ğ‘ -ğ¶ ğ‘,ğµ ğ‘ * .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUAL BIAS IN VISUALLY-AWARE RECOMMENDATION</head><p>In this section, the causal view of existing models of visually-aware recommendation is discussed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation and Task Definition</head><p>In the following, lowercase letters are slightly overloaded to represent scalar, e.g., ğ‘¢ for user ID and ğ‘– for item ID. Bold lowercase letters are used to represent vectors, e.g., ğœ¸ ğ‘¢ for latent embedding of user ğ‘¢ and ğœ¸ ğ‘– for latent embedding of item ğ‘–. Bold capital letters are used to represent matrices and higher dimensional tensors, e.g., ğ‘½ ğ‘– for the image corresponding to item ğ‘–. In the following causal graphs, the node set will include: item ğ¼ , the corresponding visual feature ğ‘‰ of the item, user ğ‘ˆ , the match ğ‘€ representing the real preference between the user and the item, the visual notice ğ‘ of the user on the visual feature and the interaction ğ‘Œ .</p><p>The recommendation task considered in this paper only contains implicit feedback such as clicks and views instead of rating scores indicating the explicit preference. Within a recommendation scenario, there are a user set U and an item set I. For each user ğ‘¢, the ID information is provided as well as the feedback to an item set I + ğ‘¢ . For each item ğ‘–, besides the ID information, an image ğ‘½ ğ‘– of the item is also available to help with the prediction of user's preference. The objective of visually-aware recommendation is to generate a personalized item ranking for each user ğ‘¢ over I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-visual Example: Matrix Factorization</head><p>Matrix Factorization (MF) has shown the state-of-the-art performance in recommendation tasks with the implicit feedback <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>. The method is to develop a statistical model for the conditional probability ğ‘ƒ (ğ‘Œ | ğ¼, ğ‘ˆ ). A common usage of MF to predict the preference of a user ğ‘¢ on an item ğ‘– is formulated as follows:</p><formula xml:id="formula_3">ğ‘¦ ğ‘–,ğ‘¢ = ğ›¼ + ğ›½ ğ‘¢ + ğ›½ ğ‘– + ğœ¸ ğ‘‡ ğ‘¢ ğœ¸ ğ‘– ,<label>(4)</label></formula><p>where ğ›¼ is an offset term, ğ›½ ğ‘¢ and ğ›½ ğ‘– are the bias terms of user and item respectively. ğœ¸ ğ‘¢ and ğœ¸ ğ‘– are the latent embedding factors of user ğ‘¢ and item ğ‘– respectively. The offset and bias terms are considered as mean effects of users and items. Latent embedding factors are performing a match between user's preference and item's properties in the form of dot product of dense vectors. The causal graph of MF is presented in Figure <ref type="figure" target="#fig_3">3</ref> (a), where it is clear that the user, the item and the match of the real preference are all the cause of an interaction with the causal paths: ğ‘ˆ â†’ ğ‘Œ , ğ¼ â†’ ğ‘Œ and ğ‘€ â†’ ğ‘Œ . Intuitively, the causal path ğ‘€ â†’ ğ‘Œ is the wanted relationship to predict an interaction because it is based on the real preference. Similar to the analysis in Section 2.1, both ğ¼ â†’ ğ‘Œ and ğ‘ˆ â†’ ğ‘Œ are considered as backdoor paths of the causal path ğ‘€ â†’ ğ‘Œ . Therefore, ğ¼ and ğ‘ˆ both are the confounders, which is also observed by Wei et al. <ref type="bibr" target="#b53">[54]</ref>. Given that these two nodes have direct causal effect to the interaction, common situations account for them are the popularity bias of items and the active user bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visual Bayesian Personalized Ranking</head><p>Visual Bayesian Personalized Ranking (VBPR) <ref type="bibr" target="#b13">[14]</ref> is a strong baseline. The method targets at ğ‘ƒ (ğ‘Œ | ğ¼, ğ‘‰ , ğ‘ˆ ) via extending the basic MF <ref type="bibr" target="#b41">[42]</ref>, which exploits the visual feature of the item similarly: where ğ‘¬ is a transform matrix, ğœ™ is a backbone network (e.g., ResNet <ref type="bibr" target="#b11">[12]</ref> and VGG <ref type="bibr" target="#b46">[47]</ref>) to extract the visual feature representation from the item image ğ‘½ ğ‘– and ğœ½ ğ‘¢ stands for a specific latent vector of the user towards the visual feature. The corresponding causal graph is shown in Figure <ref type="figure" target="#fig_3">3 (b)</ref>. Compared with the causal graph of MF, there are two extra nodes of the visual feature ğ‘‰ and the visual notice ğ‘ of the user towards the visual feature, where ğ‘ can be thought of as ğœ½ ğ‘‡ ğ‘¢ (ğ‘¬ğœ™ (ğ‘½ ğ‘– )) in Equation ( <ref type="formula" target="#formula_4">5</ref>). Furthermore, there is an extra direct cause of the interaction, ğ‘ â†’ ğ‘Œ .</p><formula xml:id="formula_4">ğ‘¦ ğ‘–,ğ‘£,ğ‘¢ = ğ›¼ + ğ›½ ğ‘¢ + ğ›½ ğ‘– + ğœ¸ ğ‘‡ ğ‘¢ ğœ¸ ğ‘– + ğœ½ ğ‘‡ ğ‘¢ (ğ‘¬ğœ™ (ğ‘½ ğ‘– )) ,<label>(5)</label></formula><p>Within the causal graph, it can be concluded that the node ğ‘‰ is the mediator and accountable for the visual bias. Such a visual bias is introduced by the pure visual notice as depicted as node ğ‘ , which also lies on the backdoor path ğ‘€ â† ğ¼ â†’ ğ‘‰ â†’ ğ‘ â†’ ğ‘Œ . VBPR also shares the same biases related to the item and the user itself as MF due to their direct causal effects towards the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DeepStyle and Adversarial Multimedia Recommendation</head><p>DeepStyle <ref type="bibr" target="#b27">[28]</ref> and Adversarial Multimedia Recommendation <ref type="bibr" target="#b47">[48]</ref> (AMR) are two follow-up methods of VBPR sharing the same causal graph with different techniques to improve the performance. They both remove the direct causal effects of ğ¼ and ğ‘ˆ on ğ‘Œ . The formulation of the prediction of DeepStyle is as follows:</p><formula xml:id="formula_5">ğ‘¦ ğ‘–,ğ‘£,ğ‘¢ = ğœ¸ ğ‘‡ ğ‘¢ ğ‘¬ğœ™ (ğ‘½ ğ‘– ) -ğ’„ ğ‘– + ğœ¸ ğ‘– ,<label>(6)</label></formula><p>where ğ’„ ğ‘– represents the categorical information of the item image and subtracting this term from the visual feature is assumed to extract the more important style information. Furthermore, this method applies the same latent user vector ğœ¸ ğ‘¢ to interact with both the visual feature and the item latent vector. Similarly, AMR follows the same prediction paradigm while introducing a noise term to increase the robustness of the model:</p><formula xml:id="formula_6">ğ‘¦ ğ‘–,ğ‘£,ğ‘¢ = ğœ¸ ğ‘‡ ğ‘¢ ğ‘¬ğœ™ (ğ‘½ ğ‘– ) + ğš« ğ‘– + ğœ¸ ğ‘– ,<label>(7)</label></formula><p>where ğš« ğ‘– denotes the noise added on the visual feature by as an adversary, which is trained in an adversarial learning style. The causal graph of these two models is presented in Figure <ref type="figure" target="#fig_3">3  (c</ref>). Compared with the causal graph of VBPR, it has the same set of nodes and removes two direct causal paths towards ğ‘Œ , ğ¼ â†’ ğ‘Œ and ğ‘ˆ â†’ ğ‘Œ . Different from VBPR, these two methods only have the backdoor path related to the visual notice, which indicates that DeepStyle and AMR are visually biased models rather than popularity biased and active user biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Deep Visual Bayesian Personalized Ranking</head><p>Deep Visual Bayesian Personalized Ranking (DVBPR) <ref type="bibr" target="#b18">[19]</ref> is also based on VBPR. Although the visual feature is incorporated in DVBPR, the latent item vector is omitted, which is more related to outfit compatibility. Its prediction procedure is defined as:</p><formula xml:id="formula_7">ğ‘¦ ğ‘–,ğ‘£,ğ‘¢ = ğ›¼ + ğ›½ ğ‘¢ + ğœ½ ğ‘‡ ğ‘¢ (ğ‘¬ğœ™ (ğ‘½ ğ‘– )) .<label>(8)</label></formula><p>According to this equation, the only information related to the item is the visual feature. Therefore, in the causal graph of DVBPR in Figure <ref type="figure" target="#fig_3">3 (d)</ref>, there is no match node ğ‘€. And the causes of the interaction consist of two causal paths, ğ‘ˆ â†’ ğ‘Œ and ğ‘ â†’ ğ‘Œ .</p><p>In terms of the outfit compatibility, the causal path ğ‘ â†’ ğ‘Œ is the base of prediction. Since ğ‘ˆ has a direct causal effect on ğ‘Œ as well, ğ‘ˆ is the mediator and it will introduce the active user bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL DEBIASING</head><p>In this section, the debiasing method based on causal inference and the CausalRec model are proposed. The main idea is to follow this question: If a user had seen other items with the same visual feature, would this user still interact with these items?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Counterfactual Inference in</head><p>Visually-Aware Recommendation</p><p>In visually-aware recommendation, it is important to predict the match between user and item based on the real preference for the features of the item including the visual feature. The match is the criteria whether to recommend this item to the user. According to the previous analysis, there is visual bias resulted from a spurious relationship between the interaction and the user-item pair due to direct effect of the visual feature. Therefore, it is expected to remove this direct effect on the interaction. To further analyze the causes of the interaction, we describe the form of the interaction ğ‘Œ based on a user ğ‘¢ and an item ğ‘– with the visual feature ğ‘£ as:</p><formula xml:id="formula_8">ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ = ğ‘Œ (ğ¼ = ğ‘–, ğ‘‰ = ğ‘£, ğ‘ˆ = ğ‘¢) = ğ‘Œ ğ‘€ ğ‘–,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ ,<label>(9)</label></formula><p>where ğ‘€ denotes the match and ğ‘ stands for the visual notice in the causal graphs shown in Figure <ref type="figure" target="#fig_3">3</ref> (b) and (c). If a no-treatment ğ¼ = ğ‘– * is applied on both the direct and indirect effects, then the total effect (TE) of ğ¼ = ğ‘– as defined in Equation (1) in Section 2.4 is:</p><formula xml:id="formula_9">TE = ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ -ğ‘Œ ğ‘– * ,ğ‘£ * ,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘–,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ -ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘ ğ‘£ * ,ğ‘¢ .<label>(10)</label></formula><p>4.1.1 Intervention. To eliminate the visual bias, it is expected to remove the direct effect of the visual feature on the interaction. In the causal graph, there are both direct and indirect effects of the visual feature. The direct effect lies on the causal path ğ¼ â†’ ğ‘‰ â†’ ğ‘ â†’ ğ‘Œ . In the contrast, the visual feature can impact the match as well, which is the indirect effect in the causal path ğ¼ â†’ ğ‘€ â†’ ğ‘Œ . According to our counterfactual thinking: "If a user had seen other items with the same visual feature, would this user still interact with these items?", the visual feature for the direct effect should be removed while stays in the indirect effect.</p><p>To investigate the direct effect of the visual feature, an intervention is conducted. The main purpose is to let the original visual feature affect the interaction with the direct effect while change the item for the indirect effect. Therefore, a no-treatment ğ¼ = ğ‘– * is exerted on the causal path of indirect effect as shown in Figure <ref type="figure" target="#fig_3">3</ref> (e) and (f). In this situation, the interaction is represented as:</p><formula xml:id="formula_10">ğ‘Œ ğ‘– * ,ğ‘£,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ .<label>(11)</label></formula><p>Based on Equation ( <ref type="formula" target="#formula_1">2</ref>), the natural direct effect of the visual feature of the treatment ğ¼ = ğ‘– is: </p><formula xml:id="formula_11">NDE = ğ‘Œ ğ‘– * ,ğ‘£,ğ‘¢ -ğ‘Œ ğ‘– * ,ğ‘£ * ,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ -ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘ ğ‘£ * ,ğ‘¢ .<label>(12</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CausalRec Model</head><p>With the causal inference analysis for visual debiasing in the last section, after performing the debiasing procedure, the visual information will be fully removed. To effectively retain the supportive visual information and remove the visual bias, the CausalRec is proposed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Base Model.</head><p>In light of the analysis of VBPR, DeepStyle and AMR, they can be unified as:</p><formula xml:id="formula_12">ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ = F (ğ‘€ ğ‘–,ğ‘¢ , ğ‘ ğ‘£,ğ‘¢ ),<label>(16)</label></formula><p>where F represents a fusion function of the match ğ‘€ and the visual notice ğ‘ , for example, a summation. Usually, both ğ‘€ and ğ‘ are calculated with a dot product:</p><formula xml:id="formula_13">ğ‘€ ğ‘–,ğ‘¢ = ğœ¸ ğ‘‡ ğ‘¢ ğœ¸ ğ‘– ,<label>(17)</label></formula><formula xml:id="formula_14">ğ‘ ğ‘£,ğ‘¢ = ğœ½ ğ‘‡ ğ‘¢ ğ‘¬ğœ™ (ğ‘½ ğ‘– ),<label>(18)</label></formula><p>where ğœ½ ğ‘¢ could be the same as ğœ¸ ğ‘¢ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">CausalRec Model.</head><p>In addition to the base model, the visual indirect effect is included in the match node in the proposed Causal-Rec model as shown in Figure <ref type="figure" target="#fig_4">4</ref> (a). The detailed model is as follows:</p><formula xml:id="formula_15">ğ‘€ ğ‘–,ğ‘¢ = ğœ (ğœ¸ ğ‘‡ ğ‘¢ ğœ¸ ğ‘– ),<label>(19)</label></formula><formula xml:id="formula_16">ğ‘€ ğ‘–,ğ‘£,ğ‘¢ = ğœ (ğœ¸ ğ‘‡ ğ‘¢ (ğœ¸ ğ‘– â€¢ ğ‘¬ğœ™ (ğ‘½ ğ‘– )))), (<label>20</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">ğ‘ ğ‘£,ğ‘¢ = ğœ (ğœ½ ğ‘‡ ğ‘¢ ğ‘¬ğœ™ (ğ‘½ ğ‘– )),<label>(21)</label></formula><p>ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ = F (ğ‘€ ğ‘–,ğ‘¢ , ğ‘€ ğ‘–,ğ‘£,ğ‘¢ , ğ‘ ğ‘£,ğ‘¢ )</p><formula xml:id="formula_19">= ğ‘€ ğ‘–,ğ‘¢ â€¢ ğ‘€ ğ‘–,ğ‘£,ğ‘¢ â€¢ ğ‘ ğ‘£,ğ‘¢ ,<label>(22)</label></formula><p>where â€¢ denotes the Hadamard product for the element-wise multiplication of vectors and ğœ denotes the Sigmoid function. In the choice of F , a simple scalar multiplication is employed.</p><p>To train the model, we use the multitask learning framework to simultaneously train the CausalRec model with the following multi-tasking learning objective function:</p><formula xml:id="formula_20">â„“ = â„“ rec (ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ ) + â„“ rec (ğ‘ ğ‘£,ğ‘¢ ) + â„“ rec (ğ‘€ ğ‘–,ğ‘¢ ğ‘€ ğ‘–,ğ‘£,ğ‘¢ ), (<label>23</label></formula><formula xml:id="formula_21">)</formula><p>where â„“ rec is the BPR loss <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_22">â„“ rec ( Å¶ ) = âˆ‘ï¸ (ğ‘¢,ğ‘–,ğ‘—) âˆˆ O -ln ğœ Å·ğ‘¢ğ‘– -Å·ğ‘¢ ğ‘— + ğœ† 1 âˆ¥Î˜âˆ¥ 2 2 , (<label>24</label></formula><formula xml:id="formula_23">)</formula><p>where Å¶ is the prediction of interaction and O denotes the pairwise training dataset with ğ‘– being the positive item and ğ‘— being the negative item for user ğ‘¢. Î˜ represents all the trainable parameters and ğœ† indicates the weight of this â„“ 2 regularization. </p><p>Similarly, the no-treatment situation of ğ¼ = ğ‘– * is represented as:</p><formula xml:id="formula_25">ğ‘Œ ğ‘– * ,ğ‘£ * ,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘€ ğ‘– * ,ğ‘£ * ,ğ‘¢ ,ğ‘ ğ‘£ * ,ğ‘¢ .<label>(26)</label></formula><p>The final inference via TIE is as follows:</p><formula xml:id="formula_26">TIE = ğ‘Œ ğ‘€ ğ‘–,ğ‘¢ ,ğ‘€ ğ‘–,ğ‘£,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ -ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘€ ğ‘– * ,ğ‘£ * ,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ .<label>(27)</label></formula><p>To enhance the representation ability and retain a certain amount of the benevolent visual bias, a hyper-parameter ğœ† 2 is used to control the scale of visual bias to be removed:</p><formula xml:id="formula_27">Å·ğ‘–,ğ‘£,ğ‘¢ = ğ‘€ ğ‘–,ğ‘¢ â€¢ ğ‘€ ğ‘–,ğ‘£,ğ‘¢ â€¢ ğ‘ ğ‘£,ğ‘¢ -ğœ† 2 â€¢ ğ‘€ ğ‘– * ,ğ‘¢ â€¢ ğ‘€ ğ‘– * ,ğ‘£ * ,ğ‘¢ â€¢ ğ‘ ğ‘£,ğ‘¢ .<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, extensive experiments will be conducted to evaluate the CausalRec model and the debiasing method. Four main research questions will be discussed:</p><p>â€¢ RQ1: Does CausalRec outperform the existing methods?</p><p>â€¢ RQ2: Does the causal inference-based debiasing method help with the existing methods? â€¢ RQ3: How does different choices of implementation of the causal inference module help with removing the visual bias? â€¢ RQ4: What is the sensitivity of hyper-parameters?  <ref type="formula" target="#formula_3">4</ref>) Grocery and Gourmet Food (short for Grocery), (5) Office Products (short for Office), ( <ref type="formula" target="#formula_5">6</ref>) Sports and Outdoors (short for Sports), ( <ref type="formula" target="#formula_6">7</ref>) Tools and Home Improvement (short for Tools) and ( <ref type="formula" target="#formula_7">8</ref>) Toys on Amazon.com 1 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>, which are widely used for visually-aware recommendation with available images for items <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>. The statistics of these datasets are presented in Table <ref type="table" target="#tab_2">1</ref>. The visual feature is extracted by a pretrained convolutional neural network following VBPR <ref type="bibr" target="#b13">[14]</ref>. For all datasets, we consider the implicit feedback scenario 2 . Users and items that 1 <ref type="url" target="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</ref> 2 As one of the reviewers points out that it is relatively unfair to evaluate on a biased dataset. Yet, it is impractical to construct a debiased test set. Possible future solutions would include relying on the explicit ratings and A/B test. occur less than five times will be filtered out as well as the items without visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Metrics.</head><p>To evaluate the performance of the recommender models, Mean Reciprocal Ranking (MRR), top-50 Normalized Discounted Cumulative Gain (NDCG) and top-50 Hit Ratio (HR) are used with a ranking of the whole item set for fair comparisons <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation.</head><p>There are a few hyper-parameters in the model. In the implementation of the model, we set the embedding size to 32 for the fairness of comparison. We use Adam <ref type="bibr" target="#b19">[20]</ref> with a learning rate from {0.01,0.001,0.0001} and set the batch size as 100. ğœ† 1 and ğœ† 2 are chosen from {0.1,0.05,0.01,0.005} and {0,0.2,0.4,0.6,0.8,1,1.2} respectively. Our implementation is based on Cornac framework <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Baselines.</head><p>The following baselines are used for comparisons and all of them have been discussed in detail in Section 3:</p><p>â€¢ BPR <ref type="bibr" target="#b41">[42]</ref> is a baseline used only ID information of items and users. The visual feature is not included in this method. â€¢ VBPR <ref type="bibr" target="#b13">[14]</ref> is one of the earliest and the strongest baseline visually-aware recommender model. It extends the BPR method with a visual term multiplied with a user embedding to help with the collaborative signal. â€¢ AMR <ref type="bibr" target="#b47">[48]</ref> further simplifies the VBPR model by omitting the user and item bias terms. DeepStyle <ref type="bibr" target="#b27">[28]</ref> shares a large similarity with AMR and thus be omitted here.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RQ1: Overall Comparisons</head><p>The overall performance of baselines and the proposed CausalRec is presented in Table <ref type="table" target="#tab_3">2</ref>. In this experiment, we conduct the experiments on eight datasets and evaluate them with the following metrics: MRR, NDCG@50 and HR@50. According to the table, it is clear that the proposed CausalRec can achieve the best performance compared with all the baseline methods. The relative improvements are presented with the maximum one up to 70%.</p><p>As a collaborative filtering model using only ID information of users and items, BPR serves as a strong baseline in all datasets. Generally, VBPR can achieve a steady improvement compared with BPR. It is reasonable that the visual feature is important for these categories of items on the E-commerce platforms. Yet there are still some datasets, e.g., Baby and Tools, where the direct incorporation of the visual information will harm the recommendation performance. In addition to VBPR, AMR is a simplified version of VBPR. However, there is no significant improvement for AMR over VBPR.</p><p>Our proposed CausalRec model has higher performance for all datasets with the presented metrics. Compared with BPR, Causal-Rec explicitly includes a visual term to exploit the visual feature for recommendations, which is behaving similarly with the visual term in VBPR. While compared with VBPR and AMR, the most important difference lies in the causal inference module using the counterfactual thinking with the TIE quantification. With this module, the CausalRec model can perform a visual debiasing procedure while keeping the benevolent visual impact in the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RQ2: Visual Debiasing with Causal Inference</head><p>In this experiment, the proposed causal inference-based visual debiasing method is integrated into existing visually-aware recommendation models as well as the CausalRec model. This setting is designed to verify the generality of our debiasing method. VBPR and AMR are extended with the causal inference (CI) from Equation ( <ref type="formula">14</ref>) and <ref type="bibr" target="#b14">(15)</ref>, denoted as VBPR w/ CI and AMR w/ CI. The biased version of CausalRec is shown here by not performing the CI in Equation ( <ref type="formula" target="#formula_27">28</ref>), denoted as CausalRec w/o CI. The result is presented in Figure <ref type="figure" target="#fig_7">5</ref>. The results are evaluated on Baby and Sports datasets with the MRR metric. Similar trends are observed on other datasets. According to the figures, it is clear that our debiasing method can improve steadily for these existing methods as well as the CausalRec. This is mainly because the existing models are trained   to simply maximize the interaction prediction probability. With our analysis in Section 3, their training methods are visually biased. Using a proper debiasing method, it is expected to eliminate the visual bias in the originally biased recommendation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">RQ3: Different Causal Modules</head><p>In this experiment, we substitute the design of the multiplicationbased fusing function F in Equation <ref type="bibr" target="#b21">(22)</ref> with the original addition function used in CausalRec. The following variants named with the suffix A, M, AML and MML denote the addition fusing function, the addition fusing function with multi-tasking learning, the multiplication fusing function and the multiplication fusing function with multi-tasking learning respectively. The result is presented in Figure <ref type="figure" target="#fig_9">6</ref> for the CausalRec on Tools and Office datasets with NDCG and HR metrics. Similar phenomena are observed on other datasets.</p><p>In Figure <ref type="figure" target="#fig_9">6</ref>, we can see that the choice of the fusing function will not affect too much of the performance. Either the proposed multiplication approach, M, or the addition based approach, A, are having comparable results. While the multitasking learning helps in a more general way for both the MML and AML. This could be due to the debiasing procedure is conducted by subtracting the causal effect of one of the branches in the model. If a branch is forced to learn more information about the recommendation task, subtracting this branch would give a more effective debiasing procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">RQ4: Parameter Sensitivity</head><p>In the CausalRec model, there are a few hyper-parameters as discussed in Section 5.1.3. Among all of them, the coefficient ğœ† 2 in Equation ( <ref type="formula" target="#formula_27">28</ref>) is the one having direct control of the causal effect of the visual bias. The parameter sensitivity of ğœ† 2 is tested in this section. We set the value of ğœ† 2 in {0, 0.2, 0.4, 0.6, 0.8, 1, 1.2}. When ğœ† 2 = 0, the model is not debiasing any visual feature. When ğœ† 2 = 1, the direct causal effect of the visual feature is completely removed. The result is presented in Figure <ref type="figure" target="#fig_10">7</ref> with the evaluation on Beauty, Clothing, Grocery and Toys datasets over MRR and NDCG.</p><p>From the figures, it is clear that removing a certain scale of the direct causal effect of the visual feature can improve the recommendation result. As ğœ† 2 increases, the recommendation performance will improve until completely removing the visual bias. If the visual bias is removed with a large scale factor, then the recommendation performance will be greatly harmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we review the related work of the visually-aware recommendation and causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visually-Aware Recommendation</head><p>Visually-aware recommendations incorporates the visual feature of items into the prediction of the user's preference. Before the deep learning era, most methods depend on image retrieval for the recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. These methods assume that the user's preference for the similar visual feature would be similar. Kalantidis et al. <ref type="bibr" target="#b17">[18]</ref> propose a method to firstly conduct a segmentation of the query image and retrieve items based on each of the predicted classes. In this work, the retrieval is conducted within the same class. In the following work, Jagadeesh et al. <ref type="bibr" target="#b16">[17]</ref> find out that the semantic information of images is important and useful in the retrieval procedure. In their customized dataset setting, the semantic information is included with a large amount of annotations.</p><p>With more deep learning-based recommendation models being developed <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>, recent methods can provide a more complicated modeling of the user-item interaction with the visual feature in addition to the simple retrieval-based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>. These methods mainly rely on pretrained deep learning framework to incorporate the visual knowledge, e.g., ResNet <ref type="bibr" target="#b11">[12]</ref> and VGG <ref type="bibr" target="#b46">[47]</ref>. IBR <ref type="bibr" target="#b28">[29]</ref> recommends the complementary items based on the styles of item's visual feature. More generally, VBPR <ref type="bibr" target="#b13">[14]</ref>, AMR <ref type="bibr" target="#b47">[48]</ref> and Fashion DNA <ref type="bibr" target="#b6">[7]</ref> leverage the visual feature to support the collaborative filtering computation. With the help of the visual feature, these methods can improve the performance of the recommender systems in the sparse situation and the cold-start problem. DeepStyle <ref type="bibr" target="#b27">[28]</ref> argues that the existing methods use the visual feature in an inappropriate way, in which the pretrained visual feature is majorly related to the class or category information. DeepStyle focuses more on the style of the visual feature rather than the categorical information. Besides passively using the existing visual features, DVBPR <ref type="bibr" target="#b18">[19]</ref> applies an end-to-end trained CNN instead of the pretrained backbone for visual feature extraction. ImRec <ref type="bibr" target="#b30">[31]</ref> proposes to use the reciprocal information between user groups with the aid of the image features.</p><p>There are a few existing methods focusing solely on the fashion recommendation task, which is more related to the compatibility of outfits <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b49">50]</ref>. These models focus on recommending a suitable outfit and evaluating the compatibility of the outfit. The visually-aware recommendation has a broader scope than just the outfit. Many products on E-commerce platforms have important visual feature as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Causal Inference for Debiasing</head><p>In recent applications of machine learning to different tasks, causal inference has been used for debiasing towards different biases. In recommendation research area, the causal inference is mainly used to remove the interaction bias <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b45">46]</ref>, especially the popularity bias <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. The most widely used causal inference tool for these methods is Inverse Propensity Weighting (IPW) <ref type="bibr" target="#b43">[44]</ref>, which will conduct a reweighting on the interaction. A more recent work MACR <ref type="bibr" target="#b53">[54]</ref> applies a causal graph <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> to analyze the causal effect of the popularity of items.</p><p>In multi-modal tasks, more and more methods make use of the causal inference to remove the bias in the data or in the model <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. For example, Tang et al. <ref type="bibr" target="#b48">[49]</ref> use counterfactual inference in scene graph generation to remove the bias introduced by the image content. Qi et al. <ref type="bibr" target="#b36">[37]</ref> argue that using intervention can remove the language bias in the historical language bias in the visual dialog. A more recent work investigates the clickbait issue via a causal graph method with the exposure feature being the source of the bias and the content feature is different from the exposure feature <ref type="bibr" target="#b51">[52]</ref>. While in CausalRec, the visual feature serves as both the source of the visual bias and the content feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, the visual bias problem is identified and analyzed in the visually-aware recommendation. A novel causal inference framework is developed to investigate the direct and indirect causal effect of the visual feature of items on the interaction. To perform a debiased recommendation, the intervention and the counterfactual inference are applied after the biased training process. We further propose the CausalRec model to effectively make use of the visual feature and in the meanwhile to remove the visual bias. Extensive experiments are conducted on eight benchmark datasets, which demonstrates the state-of-the-art performance of the CausalRec model and the efficacy of the proposed visual debiased approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(c) and Figure 2 (d) represent different meanings whether the treatment is on the direct or the indirect causal paths. For Figure 2 (c), the no-treatment ğ´ = ğ‘ * is on the path ğ´ â†’ ğ¶, which results in ğ¶ ğ‘ * ,ğµ ğ‘ . While in Figure 2 (d), the treatment ğ´ = ğ‘ * is on the path ğ´ â†’ ğµ â†’ ğ¶, which results in ğ¶ ğ‘,ğµ ğ‘ * . The half shadowed node indicates the causes of it consist of both treatment and no-treatment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of intervention on causal graph. (a) The causal graph includes three random variables (nodes), ğ´, ğµ and ğ¶. The edges in the graph indicate the causal relationship between nodes. For example, the causal path ğ´ â†’ ğµ represents that ğ´ is the cause of ğµ. And since there are two causal paths (ğ´ â†’ ğ¶ and ğµ â†’ ğ¶) directing to ğ¶, both ğ´ and ğµ are the causes of ğ¶. When there is an observation of ğ´ = ğ‘, ğµ becomes ğµ ğ‘ because it is based on ğ´. Similarly, ğ¶ becomes ğ¶ ğ‘,ğµ ğ‘ . (b) A no-treatment assigns ğ´ = ğ‘ * and this no-treatment results in ğ¶ ğ‘ * ,ğµ * . (c) An intervention is operated on node ğµ with a no-treatment to assign ğ´ = ğ‘ * while leaving ğµ unchanged. In this situation, the causal path of ğ´ â†’ ğµ is removed. The result of this no-treatment is ğ¶ ğ‘ * ,ğµ . (d) An intervention is operated on node ğµ and a no-treatment ğ´ = ğ‘ * affects the causal path ğ´ â†’ ğµ â†’ ğ¶ instead of ğ´ â†’ ğ¶.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) stands for under treatment ğ´ = ğ‘ and Figure 2 (b) stands for under no-treatment ğ´ = ğ‘ * . The Total Effect (TE) of the treatment ğ´ = ğ‘ on ğ¶ is denoted as: TE = ğ¶ ğ‘,ğµ ğ‘ -ğ¶ ğ‘ * ,ğµ ğ‘ * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variants of the causal graph of existing visually-aware recommender models and two examples of intervention. (a) MF leverages all of ğ‘ˆ , ğ¼ and ğ‘€ to predict the interaction. (b) VBPR makes use of the direct effect of every component except the visual feature to predict the interaction. (c) DeepStyle and AMR remove the direct causal effect of ğ‘ˆ and ğ¼ on the prediction of ğ‘Œ . (d) DVBPR further removes the direct effect of ğ‘€, the match between the user and the item. Such a paradigm is applied more often on fashion compatibility. (e) The intervention of VBPR is conducted by setting a no-treatment for ğ¼ = ğ‘– * . (f) The intervention of DeepStyle and AMR is conducted by setting a no-treatment for ğ¼ = ğ‘– * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The causal graph of CausalRec and the corresponding intervention for visual debiasing. (a) The visual feature is used in both ğ‘€ and ğ‘ . (b) In the intervention, both ğ¼ and ğ‘‰ are set to no-treatment for ğ‘€.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5. 1 . 1</head><label>11</label><figDesc>Datasets. The experiments are conducted on eight benchmark datasets: (1) Baby, (2) Beauty, (3) Clothing, Shoes and Jewelry (short for Clothing), (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of causal inference-based debiasing methods for visually-aware models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of different implementations of the causal inference module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The parameter sensitivity of the ğœ† 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Therefore, with Equation (5) of VBPR, the counterfactual prediction based on TIE becomes: Å·ğ‘–,ğ‘£,ğ‘¢ = ğ›½ ğ‘– -ğ›½ ğ‘– * + ğœ¸ ğ‘‡ ğ‘¢ (ğœ¸ ğ‘– -ğœ¸ ğ‘– * ).</figDesc><table><row><cell></cell><cell>(14)</cell></row><row><cell cols="2">For DeepStyle and AMR, the counterfactual prediction becomes:</cell></row><row><cell>Å·ğ‘–,ğ‘£,ğ‘¢ = ğœ¸ ğ‘‡ ğ‘¢ (ğœ¸ ğ‘– -ğœ¸ ğ‘–  *  ).</cell><cell>(15)</cell></row><row><cell>)</cell><cell></cell></row><row><cell>4.1.2 Counterfactual Inference. Since the intervention is already</cell><cell></cell></row><row><cell>exerted on the causal graph, it is naturally to answer our counterfac-</cell><cell></cell></row><row><cell>tual question by removing the direct effect of the visual feature on</cell><cell></cell></row><row><cell>the interaction using TIE. According to Equation (3) in Section 2.4,</cell><cell></cell></row><row><cell>the natural way is to use the Total Indirect Effect (TIE), i.e., minus</cell><cell></cell></row><row><cell>the Natural Direct Effect (NDE) from the Total Effect (TE):</cell><cell></cell></row></table><note><p>TIE = TE -NDE = ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ -ğ‘Œ ğ‘– * ,ğ‘£,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘–,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ -ğ‘Œ ğ‘€ ğ‘– * ,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ . (13) Using the maximum TIE for inference is different from the existing methods based on the conditional probability ğ‘ƒ (ğ‘Œ | ğ¼, ğ‘‰ , ğ‘ˆ ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets CausalRec Inference. During the test phase of the CausalRec model, the counterfactual inference is applied with the intervention on the item. The intervention is detailed in Figure4 (b). For CausalRec, the prediction can be elaborated as: ğ‘Œ ğ‘–,ğ‘£,ğ‘¢ = ğ‘Œ ğ‘€ ğ‘–,ğ‘¢ ,ğ‘€ ğ‘–,ğ‘£,ğ‘¢ ,ğ‘ ğ‘£,ğ‘¢ .</figDesc><table><row><cell></cell><cell cols="4">â™¯ Users â™¯ Items â™¯ Interactions Sparsity</cell></row><row><cell>Baby</cell><cell>19,822</cell><cell>7,776</cell><cell>163,856</cell><cell>99.89%</cell></row><row><cell>Beauty</cell><cell>25,837</cell><cell>16,893</cell><cell>227,920</cell><cell>99.95%</cell></row><row><cell cols="2">Clothing 58,197</cell><cell>44,310</cell><cell>422,474</cell><cell>99.98%</cell></row><row><cell>Grocery</cell><cell>16,318</cell><cell>11,581</cell><cell>165,893</cell><cell>99.91%</cell></row><row><cell>Office</cell><cell>6,913</cell><cell>4,775</cell><cell>68,306</cell><cell>99.79%</cell></row><row><cell>Sports</cell><cell>40,358</cell><cell>24,766</cell><cell>334,238</cell><cell>99.97%</cell></row><row><cell>Tools</cell><cell>20,134</cell><cell>14,774</cell><cell>163,451</cell><cell>99.95%</cell></row><row><cell>Toys</cell><cell>24,314</cell><cell>18,906</cell><cell>209,281</cell><cell>99.95%</cell></row><row><cell>4.2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall performance.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>BPR</cell><cell>VBPR</cell><cell cols="2">AMR CausalRec Improve</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0146 0.0112 0.0070</cell><cell>0.0172</cell><cell>17.81%</cell></row><row><cell>Baby</cell><cell cols="4">NDCG 0.0271 0.0215 0.0135</cell><cell>0.0320</cell><cell>18.08%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0983 0.0726 0.0462</cell><cell>0.1047</cell><cell>6.51%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0071 0.0160 0.0102</cell><cell>0.0192</cell><cell>20.00%</cell></row><row><cell>Beauty</cell><cell cols="4">NDCG 0.0137 0.0315 0.0201</cell><cell>0.0384</cell><cell>21.90%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0476 0.1048 0.0690</cell><cell>0.1254</cell><cell>19.66%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0038 0.0065 0.0036</cell><cell>0.0088</cell><cell>35.38%</cell></row><row><cell>Clothing</cell><cell cols="4">NDCG 0.0065 0.0125 0.0068</cell><cell>0.0172</cell><cell>37.60%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0216 0.0417 0.0235</cell><cell>0.0528</cell><cell>26.62%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0140 0.0209 0.0147</cell><cell>0.0250</cell><cell>19.61%</cell></row><row><cell>Grocery</cell><cell cols="4">NDCG 0.0281 0.0414 0.0307</cell><cell>0.0475</cell><cell>14.73%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0981 0.1376 0.1068</cell><cell>0.1541</cell><cell>11.99%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0152 0.0164 0.0145</cell><cell>0.0223</cell><cell>35.98%</cell></row><row><cell>Office</cell><cell cols="4">NDCG 0.0296 0.0340 0.0291</cell><cell>0.0445</cell><cell>30.88%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.1040 0.1222 0.1021</cell><cell>0.1537</cell><cell>25.78%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0078 0.0086 0.0047</cell><cell>0.0147</cell><cell>70.93%</cell></row><row><cell>Sports</cell><cell cols="4">NDCG 0.0140 0.0168 0.0087</cell><cell>0.0258</cell><cell>53.57%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0460 0.0563 0.0291</cell><cell>0.0792</cell><cell>40.67%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0110 0.0085 0.0048</cell><cell>0.0134</cell><cell>21.82%</cell></row><row><cell>Tools</cell><cell cols="4">NDCG 0.0181 0.0162 0.0089</cell><cell>0.0244</cell><cell>34.81%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0537 0.0556 0.0324</cell><cell>0.0775</cell><cell>39.39%</cell></row><row><cell></cell><cell>MRR</cell><cell cols="3">0.0056 0.0106 0.0074</cell><cell>0.0153</cell><cell>44.34%</cell></row><row><cell>Toys</cell><cell cols="4">NDCG 0.0106 0.0200 0.0141</cell><cell>0.0305</cell><cell>52.50%</cell></row><row><cell></cell><cell>HR</cell><cell cols="3">0.0370 0.0663 0.0478</cell><cell>0.1024</cell><cell>54.45%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">ACKNOWLEDGMENTS</head><p>The work was supported by <rs type="funder">Australian Research Council Discovery Project</rs> (<rs type="grantNumber">ARC DP190102353</rs>, <rs type="grantNumber">DP190101985</rs>, <rs type="grantNumber">CE200100025</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rtq584Z">
					<idno type="grant-number">ARC DP190102353</idno>
				</org>
				<org type="funding" xml:id="_56NcgWE">
					<idno type="grant-number">DP190101985</idno>
				</org>
				<org type="funding" xml:id="_VzzczJt">
					<idno type="grant-number">CE200100025</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Managing Popularity Bias in Recommender Systems with Personalized Re-Ranking</title>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In FLAIRS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Unfairness of Popularity Bias in Recommendation</title>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Mansoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RMSE@RecSys</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A General Framework for Counterfactual Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical biases in Information Retrieval metrics for recommender systems</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>BellogÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">IvÃ¡n</forename><surname>Cantador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr. J</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Causal embeddings for recommendation</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: the example of computational advertising</title>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><forename type="middle">QuiÃ±onero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Xavier Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bracher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dressing as a Whole: Outfit Compatibility Learning Based on Node-wise Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personalized Video Recommendation Using Rich Contents from Videos</title>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PAL: a position-bias aware learning framework for CTR prediction in live recommender systems</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Fashion Compatibility with Bidirectional LSTMs</title>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effects of Position Bias on Click-Based Recommender Evaluation</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>BellogÃ­n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian Inference for Causal Effects in Randomized Experiments with Noncompliance</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale visual recommendations from street fashion images</title>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyndon</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visually-Aware Fashion Recommendation and Design with Generative Image Models</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advances in Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Unfairness of Popularity Bias in Music Recommendation: A Reproducibility Study</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Kowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Lex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Sampled Metrics for Item Recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD, year = 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining Fashion Outfit Composition Using an End-to-End Deep Learning Approach on Set Data</title>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multim</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attribute-aware Explainable Complementary Clothing Recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fashion Recommendation with Multirelational Representation Learning</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In PAKDD</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">OutfitNet: Fashion Outfit Recommendation with Attention-Based Multiple Instance Learning</title>
		<author>
			<persName><forename type="first">Yusan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Moosaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepStyle: Learning User Preferences for Visual Recommendation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Controlling Fairness and Bias in Dynamic Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ImRec: Learning Reciprocal Preferences Using Images</title>
		<author>
			<persName><forename type="first">James</forename><surname>Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Correcting for Selection Bias in Learning-to-rank Systems</title>
		<author>
			<persName><forename type="first">Zohreh</forename><surname>Ovaisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragib</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Vasilaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Zheleva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direct and Indirect Effects</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning and Inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Book of Why: The New Science of Cause and Effect</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two Causal Principles for Improving Visual Dialog</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting Positional Information for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting Crosssession Information for Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A new approach to causal inference in mortality studies with a sustained exposure period-application to control of the healthy worker survivor effect</title>
		<author>
			<persName><forename type="first">James</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Modelling</title>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cornac: A Comparative Framework for Multimodal Recommender Systems</title>
		<author>
			<persName><forename type="first">Aghiles</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc-Tuan</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recommendations as Treatments: Debiasing Learning and Evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial Training Towards Robust Multimedia Recommender System</title>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unbiased Scene Graph Generation From Biased Training</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Visual Clothing Style with Heterogeneous Dyadic Co-Occurrences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balazs</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual Commonsense R-CNN</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Causal Inference for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System</title>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chufeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling Product&apos;s Visual and Functional Characteristics for Recommender Systems</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Counterfactual Zero-Shot and Open-Set Visual Recognition</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Discrete Deep Learning for Fast Content-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In WSDM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
