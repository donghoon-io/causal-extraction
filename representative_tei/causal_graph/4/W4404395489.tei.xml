<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Agnostic Causal Bayesian Optimisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sumantrak</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Science and its Applications</orgName>
								<orgName type="department" key="dep2">German Research Centre for Artificial Intelligence(DFKI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengyan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><forename type="middle">J</forename><surname>Vollmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Data Science and its Applications</orgName>
								<orgName type="department" key="dep2">German Research Centre for Artificial Intelligence(DFKI)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kaiserslautern-Landau</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Agnostic Causal Bayesian Optimisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of globally optimising a target variable of an unknown causal graph on which a sequence of soft or hard interventions can be performed. The problem of optimising the target variable associated with a causal graph is formalised as Causal Bayesian Optimisation (CBO), which has various applications in biology, manufacturing, and healthcare. However, in many real-world applications, the true data-generating process is often unknown or partially known. We study the CBO problem with unknown causal graphs for two settings, namely structural causal models with hard interventions and function networks with soft interventions. We propose Graph Agnostic Causal Bayesian Optimisation (GACBO), an algorithm that actively discovers the causal structure that contributes to achieving optimal rewards. GACBO seeks to balance exploiting the actions that give the best rewards against exploring the causal structures and functions. We show our proposed algorithm outperforms baselines in simulated experiments and real-world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bayesian Optimisation (BO) is a powerful framework for sequentially optimising black-box functions, with various applications in fields such as drug and material discovery, robotics, agriculture, and automated machine learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16]</ref>. Although most conventional Bayesian optimisation methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b15">16]</ref> treat functions as black boxes, the actual data-generating process usually exhibits some structural patterns, such as a network structure. Causal Bayesian Optimisation methods have been proposed to leverage underlying structures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref> and enable us to improve sample efficiency.</p><p>It is unrealistic to assume that the structural patterns are known in practice. In many real-world applications causal graphs are often unknown or wrongly specified (refer to Figure <ref type="figure" target="#fig_2">2 (c-w1), (c-w2</ref>) as examples). To solve this problem, we consider the Causal Bayesian Optimisation problem with an unknown causal graph. One solution is to perform causal discovery (structure learning) <ref type="bibr" target="#b18">[19]</ref> as a prior step to employing such techniques. However, learning the complete true graph and all associated mechanisms globally is not essential and possibly wasteful when the goal is finding the global optima. Previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46]</ref> proposed methods that integrate causal discovery and causal reasoning by jointly modelling the causal graphs and associated mechanisms for active learning tasks or minimising simple regret, under hard intervention settings. Different from their objective, we consider the Bayesian optimisation task with both soft and hard interventions, where our objective is to maximise the cumulative reward with the help of learning causal structures at the same time.</p><p>To leverage graph structure for BO without wasting additional samples by learning inessential details of the graph structure, we need to balance exploitation and exploration, where exploitation is in terms of selecting actions with the maximum possible outcome, and exploration is in terms of either selecting actions with high uncertainty in function space or causal structure learning. We propose </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward</head><p>Apply BO on all plausible graphs g1 g2 g3 g4 g5 …. Graph Agnostic Causal Bayesian Optimisation (GACBO, Algorithm 1 and 2) with a causal subgraph discovery in Algorithm 3. GACBO optimises the target variable and learns the causal structure only when it serves the purpose of finding a possible optimal action. We show the workflow of our proposed method in Figure <ref type="figure" target="#fig_1">1</ref>. Starting from a uniform prior over all possible acyclic graph structures, we model surrogate functions for all ancestral nodes of the target for all possible graphs with Gaussian processes which take the parents and actions affecting the node as inputs. The probability of possible graphs is modelled using the Bayesian Score <ref type="bibr" target="#b14">[15]</ref>. In each round, we maintain the set of functions and graphs that are within specified confidence intervals with high probability. We select interventions using an Upper Confidence Bound (UCB) based acquisition function utilising the reparametrization trick <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select actions corresponding to best possible graphs</head><p>Our contributions are: 1) To the best of our knowledge, we are the first work studying causal Bayesian optimisation with cumulative objective when the graph is unknown or partially known.</p><p>2) We consider both soft and hard interventions and propose a novel algorithm Graph Agnostic Causal Bayesian Optimisation (GACBO) to address the new setting. 3) We introduce an Upper Confidence Bound based acquisition function that makes causal discovery a subtask only relevant when distinguishing between graphs yields better results, balancing the exploitation and exploration for causal structure learning. 4) We demonstrate on synthetic and real-world causal graphs that our algorithm demonstrates competitive performance compared with baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Problem Setting</head><p>In this paper, we address the novel and challenging setting where an agent interacting with a Structural Causal Model (SCM) or a Noisy Function Network (NFN), with unknown graph structure and unknown associated functions, to maximise the cumulative rewards over T rounds. We provide the background of SCM and NFN, and define our problem setting. We use uppercase letters to denote random variables and lowercase letters to denote the realisation of random variables. We overload the notation of random variables to denote nodes in the causal graph. Bold letters denote vectors or sets. We summarise our notation in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structural Causal Models and Hard Interventions</head><p>An SCM <ref type="bibr" target="#b36">[37]</ref> is expressed as a tuple ⟨g * , Y, V , f g * , Ω⟩ with the following elements: g * is the true but unknown. Denote [m] as the set of integers {0 . . . m}. Directed Acyclic Graph (DAG), Y denotes the reward variable, V = {V i } m-1 i=0 denotes a set of observed random variables (nodes in g * ), where </p><formula xml:id="formula_0">A2 X2 Y A1 (a) X2 A2 Y X2 Y X1 A1 A2 Y X2 Y X2 A1 (c-w1) X1 A1 Y X2 Y X2 A2 X1 Y X2 Y X2 (c) (b) (c-w2)</formula><formula xml:id="formula_1">all nodes i ∈ [m] belong to a compact V i ∈ R and f g * = {f g * i } m i=0</formula><p>is the set of respective unknown functions associated with g * with a set of independent noises Ω = {Ω i } m i=0 with zero mean and known distribution.</p><p>Denote the indices of parent nodes of any node as pa g * (i) ⊂ [m] is defined for the DAG g * . Z g * i = {V j } j∈pa g * (i) are the parents of the i th node in the true graph g * . Node V i ∈ V are generated according to the function f g * i : Z i,g * -→ V i . We observe the noisy output of each function</p><formula xml:id="formula_2">v i = f g * i (z g * i ) + ω i .</formula><p>The functions are evaluated topologically starting from root to leaf nodes according to g * . We consider a realistic setting where not all observable variables are intervenable <ref type="bibr" target="#b28">[29]</ref>. Denote I ⊂ {0, . . . , m -1} as the indices of variables that allow to be intervened. The set of observable variables can be then decomposed into two mutually exclusive sets V = {C, X}, where X = {V j } j∈I are intervenable variables and C = {V j } j / ∈I are non-intervenable variables. We assume the target variable Y = V m is non-intervenable.</p><p>For SCM, we consider hard interventions also referred to as do-interventions. For hard interventions we are required to pick the indices we intervene on I ∈ P(I) from the powerset of possible indices. We are also required to choose the corresponding values a I = {a i } i∈I we intervene with. In our setting the space of action values possible for a particular node i ∈ I t is given by the compact space A i ∈ R. For a particular set of indices I the intervention space is denoted using A I = {A i } i∈I and therefore the total space of interventions is given by A I = {A I } I∈P(I) , At round t, we select indices I t ∈ P(I) and set the corresponding nodes to specific values {do(x i,t = a i,t ) ∀i ∈ I t , a i,t ∈ A i } We then observe the values of all nodes, for i ∈ [m],</p><formula xml:id="formula_3">v i,t = f g * i (z g * i,t ) + ω i,t if i / ∈ I t a i,t if i ∈ I t<label>(1)</label></formula><p>Note f i is dependent only on its ancestors in the mutated graph g * I where the intervened nodes have no dependence on their parents. We define the optimal intervention as the indices and values that maximise the target reward variable, i.e.</p><formula xml:id="formula_4">I * , a * I * = argmax I∈P(I),a∈A I E[Y |do(x I = a)]<label>(2)</label></formula><p>For brevity we denote a * = (I * , a * I * ) ∈ A I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Function Networks and Soft Interventions</head><p>The key difference between an NFN and SCM is that, for NFN <ref type="bibr" target="#b5">[6]</ref> one cannot directly set the value of a particular node in the function network. We consider a soft intervention model <ref type="bibr" target="#b11">[12]</ref>, within this framework, interventions are described by controllable action variables. These actions a i belonging to a compact space A i ∈ R appear as additional nodes in the NFN with edges directed to the variable they affect (see Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>Since these actions affect nodes in conjugations with their existing inputs, we treat them as additional inputs, each node can be affected by multiple action variables, and the action affecting node V i is defined as a g * i = {a j } j∈I g * i where I g * i is the set of actions affecting node i in g * . A g * i ∈ R q denotes the compact space of an input a g * i where (q = arg max i |I g * i | denotes the maximum number of actions affecting any node in the NFN). We define the total space of actions as a ∈ A where a = {a g * i } m i=0 . Considering any node V i is dependent on the value of its parents Z g * i as well as the actions a g * i affecting that node we define the set of functions as</p><formula xml:id="formula_5">f g * = {f g * i } m-1 i=0 , with f g * i : Z g * i × A g * i - → V i , with v g * i = f g * i (z g * i , a g * i )</formula><p>+ ω i .However, the functional relationships relating the actions to the nodes are unknown to us apriori. We overload notations for SCM and NFN cases for simplified notations, we will refer to different settings whenever needed.</p><p>At round t, we select actions a :,t = {a g * i,t } m-1 i=0 and observe nodes v :,t = {v i,t } m i=0 , where v :,t are generated by the oracle topologically according to the graph g * ,</p><formula xml:id="formula_6">v i,t = f g * i (z g * i,t , a g * i,t ) + ω i,t , ∀i ∈ [m]<label>(3)</label></formula><p>The root nodes in the graph have no observable parents but may still be affected by actions. The target node has a m = {0} j∈I g * m since interventions on the target node are not allowed. The value of the target variable depends on the actions of all variables that are ancestors of the target variable in graph g * . The action that maximises the target variable is defined using</p><formula xml:id="formula_7">a * = arg max a∈A E[y|a]<label>(4)</label></formula><p>Performance Metric Considering that the goal of our agent is to design a sequence of actions {a :,t } T t=0 or {I t , a It, } T t=0 that maximises the average expected reward for soft and hard interventions respectively, which is equivalent as minimising the expected cumulative regret <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>,</p><formula xml:id="formula_8">R T = T t=1 [E[y|a * ] -E[y|a :,t ]] ; R T = T t=1 [E[y|a * ] -E[y|do(x It = a It )]] .<label>(5)</label></formula><p>Iteratively evaluating the costly function at multiple points enhances the accuracy of surrogate estimates. If we assume that the true data generating mechanism complies with our regularity assumptions, the true data generating mechanism lies within our prior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we propose the Graph Agnostic Causal Bayesian Optimisation (GACBO) algorithm to address the setting where both the causal graph and underlying causal mechanisms are unknown. By assuming the smoothness of functions relating the nodes and the presence of a causal structure in our data-generating process, we can explore the plausible space of models more efficiently. There are three sources of uncertainty that we consider: 1) Causal graph structure uncertainty. We assume no prior knowledge of the DAG structure and consider all DAGS equally likely. To model this uncertainty, we place a uniform prior over all possible DAGs and then update the posteriors over DAGs as will be shown in Section 3.2. The updated beliefs of DAGs enable us to update plausible graph sets and design acquisition functions.</p><p>2) Function uncertainty. Given our regularity assumptions consider a large space of functions in the absence of data points. We use Gaussian Processes to model functions among nodes. This is the main type of uncertainty studied in standard BO techniques Srinivas et al. <ref type="bibr" target="#b39">[40]</ref>, whereas in our setting due to the unknown graph structure, there are exponentially increasing numbers of functions with respect to nodes. We address function uncertainty in Section 3.1.</p><p>3) Noisy observations. We consider noisy observations, however, we assume bounded variance or Subgaussian noise so that the input space Z g i is compact. We model this uncertainty in the GPs. Our method quantifies all the sources of uncertainty by following a model-based approach. We construct a confidence interval and consider graphs and functions within the confidence interval. For all graphs within the confidence interval, the observational uncertainty based on the confidence intervals of individual GPs is propagated through the graph structures. As we encounter data, the uncertainty reduces and our posterior converges to the true graph functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Surrogate models</head><p>Surrogate models help us incorporate our prior beliefs into the modelling process and allow us to enact interventions without performing them in the real environment and also quantify the total uncertainty related to certain outcomes. Define surrogate model m t ∼ M t at time step t as a triple m t = (g t , f g t , ω 2 t ). M t denotes the posterior of plausible models, g t ∼ G t is one possible realisation of posterior G t at time t,</p><formula xml:id="formula_9">f g t = { f g t,i } m i=0</formula><p>where the surrogate function f g t,i ∈ H k g i belongs to the RKHS H k g i which is defined on the input space S g i = Z g i × A g i (and S g i = Z g i for hard interventions) for all nodes i as implied by kernel k g i :</p><formula xml:id="formula_10">S g i × S g i - → R.</formula><p>And we assume subgaussian observational noise of each node</p><formula xml:id="formula_11">ω 2 t = {ω 2 t,i } m i=0 .</formula><p>Surrogate Functions We model surrogate functions using Gaussian processes (GPs). Posterior means µ g i,t and variances σ g i,t for any function parameterising any possible graph g at a given point is calculated according to the GP posterior at time step t. The posterior is calculated using GP update equations <ref type="bibr" target="#b50">[51]</ref>.</p><formula xml:id="formula_12">f g i,t (z g i , a g i ) ∼ N (µ g i,t (z g i , a g i ), σ g i,t (z g i , a g i )),<label>(6)</label></formula><p>where</p><formula xml:id="formula_13">µ g i,t (z g i , a g i ) = k g i,t (z g i , a g i ) ⊤ (K g i + ρ 2 i I) -1 vec(v i,1:t ); σ g i,t (z g i , a g i ) = k g i ((z g i , a g i ), (z g i , a g i )) -k g i,t (z g i , a g i ) ⊤ (K g i + ρ 2 i I) -1 k g i,t (z g i , a g i ),<label>(7)</label></formula><p>where I is used to define the identity matrix, vec</p><formula xml:id="formula_14">(v i,1:t ) = [v i,1 . . . v i,t ] ⊤ , k g i,t (z g i , a g i ) = [k g i ((z g i,1 , a g i,1 ), (z g i , a g i )), . . . , k g i ((z g i,t , a g i,t ), (z g i , a g i ))], [K g i ] t1,t2 = k g i ((z g i,t1 , a g i,t1 ), (z g i,t2 , a g i,t2</formula><p>)). Graph Likelihood The Markov Property of Bayesian networks allows for a compact factorisation of the joint distribution of all observed nodes V = {V 1 , . . . , V m } in the Bayesian Network,</p><formula xml:id="formula_15">p(V |g) = m i=0 p(V i |Z g i ).<label>(8)</label></formula><p>The joint distribution factorises into conditional distributions given its parents in the graph g. In the case of soft interventions any observed node V i is affected by its parents Z g i as well as the actions which appear as extra nodes in the SCM, we use A g i to denote the set of action nodes affecting node i therefore it is calculated as</p><formula xml:id="formula_16">p(V |g) = m i=0 p(V i |Z g i , A g i )<label>(9)</label></formula><p>The distribution factorises into conditional distributions for each variable, given its parents in the DAG and the associated actions for the node. GPs admit a closed-form expression for the marginal likelihood of the t observations v i,1:t of the node V i . p(v i,1:t |g, θ i ) can be calculated as below</p><formula xml:id="formula_17">(2π) -t 2 | Kg i,θ | -1 2 exp - 1 2 v ⊤ i,1:t ( Kg i,θ ) -1 v i,1:t (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where Kg i,θ = K g i,θ + ω 2 i I. The covariance matrix K g i,θ is given by the kernel k g i,θ used and observations collected until time step t (z g i,1 , a g i,1 ) . . . (z g i,t , a g i,t ), (z g i,1 ) . . . (z g i,t ) for soft and hard interventions respectively. The input space of the functions and hence the kernel specified is dependent on the selected graph. The lengthscales θ i = {θ i,j } i∈pag(i) chosen for different input nodes in the selected graph, determine the smoothness of the functions in the RKHS implied by the kernel. The lengthscales chosen for the kernel relate directly to the smoothness of the functions sampled from the GP <ref type="bibr" target="#b6">[7]</ref>. We define priors θ i ∼ π(θ i ) over hyperparameters consistent with our smoothness assumptions.</p><p>The Score is defined as Friedman and Nachman <ref type="bibr" target="#b14">[15]</ref> as S and is calculated as follows. The score shows the probability of the observed values of node V i is v i,1:t given the graph g and dataset D t-1 , where graph g indicates the parents of node V i is Z g i and actions A g i .</p><formula xml:id="formula_19">S(V i , Z g i , A g i |D t ) = p(v i,1:t |g, θ i )π(θ i |g)dθ i ,<label>(11)</label></formula><p>Therefore the probability of observing data D t given g is given as the product of observing the values of each node in i ∈ [m] given the values of its parents according to graph g</p><formula xml:id="formula_20">P (D t |g) = m i=0 S(V i , Z g i , A g i |D t ). (<label>12</label></formula><formula xml:id="formula_21">)</formula><p>The probability of the graph g given D t is directly proportional to the product of the probability of observing the data given graph P (D t |g) and prior probability of graph g p(g) using Bayes Rule,</p><formula xml:id="formula_22">P (g|D t ) ∝ P (D t |g)p(g).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Plausible Models</head><p>Define plausible models at time step t as the set of surrogate models which is likely to contain the true SCM within the confidence intervals with probability at least 1 -δ</p><formula xml:id="formula_23">| f g i,t (z g i , a g i ) -E i,t [z i , a i ]| ≤ β i,t V i,t [z i , a i ],<label>(14)</label></formula><formula xml:id="formula_24">E i,t [z i , a i ] = E g∼p(g|Dt) [µ g i,t-1 (z g i , a g i )]; V i,t [z i , a i ] = V g∼p(g|Dt) [µ g i,t-1 (z g i , a g i )] + E g∼p(g|Dt) [(σ g i,t-1 (z g i , a g i )) 2 ].<label>(15)</label></formula><p>by the law of total expectation and law of total variance <ref type="bibr" target="#b49">[50]</ref>. Note that we use z i instead of z g i as inputs for E i,t (•) and V i,t (•), in this case z i = [m]\i, where all observable nodes other than V i are inputs to the function. The term β i,t is present to ensure confidence bounds. We set β i,t = β T for all i. As shown in Sussex et al. <ref type="bibr" target="#b41">[42]</ref> some kernels exhibit dependence of β T on T, for β T to hold applying a union bound over all time steps t and for all nodes</p><formula xml:id="formula_25">V i for i ∈ [m] is sufficient resulting in β T = O(B + ρ d √ γ t ) where B = max i B i and ρ = max i ρ i .</formula><p>At the time t all f g i within the confidence intervals given by the joint posterior over graphs p(g|D t ) and associated GP posteriors constitute the plausible functions M t . </p><formula xml:id="formula_26">M g t = { fg = { f g i } |V | i=0 s.t. ∀ i : f g i ∈ H ki , ∥ f g i ∥ ki ≤ B i ,</formula><p>Recall B i is the upper bound of the RKHS norm of f g * i (•), ∥f g * i ∥ ≤ B i for some fixed constant B i ≥ 0. The plausible graphs G t are described as</p><formula xml:id="formula_28">G t = {g|∀ i ∃ f g i ∈ M g t , g ∈ G t-1 },<label>(17)</label></formula><p>with G 0 = g ∈ G where G denotes the space of all possible DAGS. The above equation states that for a graph to be one of the plausible graphs at time t there has to be at least one associated function with graph g within the confidence interval for all nodes [m]. </p><p>For a sampled graph g nodes V i are evaluated topologically starting from root nodes {i | Z g i = ∅} in the graph g to the target node m, based on actions a :,t . For a given input to node V i a function f g i within the confidence interval is chosen optimistically such that it results in a desired input ṽg i,t</p><formula xml:id="formula_30">for children nodes {j | i ∈ Z g j } of V i .</formula><p>Resulting in ṽg t = {ṽ g i,t } i∈g a hallucinated set of values for observed nodes included in graph g.</p><p>Optimising Eq.( <ref type="formula" target="#formula_29">18</ref>) using standard techniques is not effective because it involves maximizing a set of functions characterized by bounded (RKHS) norms. To mitigate this problem we use the reparametrization trick used in Sussex et al. <ref type="bibr" target="#b41">[42]</ref> to write any function f g i ∈ fg using a function η i,g :</p><formula xml:id="formula_31">Z g i × A g i - → [-1, 1]</formula><p>, that is,</p><formula xml:id="formula_32">f g i,t ( zg i , ãg i ) = µ g i,t-1 ( zg i , ãg i ) + β t σ g i,t-1 ( zg i , ãg i )η i,g ( zg i , ãg i ),<label>(19)</label></formula><p>where zg i denotes the hallucinated values of Z g i for simulated action ã:,t , The η i,g function chooses plausible yet optimistic models based on the confidence bounds of functions given a graph g. The acquisition function can therefore be expressed in terms of η g :</p><formula xml:id="formula_33">Z g × A g - → [-1, 1] |V (g)|</formula><p>, where |V (g)| is the number of nodes in the graph g,</p><formula xml:id="formula_34">arg max a∈A max g∈Gt max ηg(•) E[y| fg , a].<label>(20)</label></formula><p>The acquisition for hard intervention (Eq. 21 in Appendix) is based on the notion of Minimal Intervention Sets (MIS) <ref type="bibr" target="#b27">[28]</ref>. For each plausible graph, we only compare interventions within the MIS of the given graph to find the intervention which maximises the surrogate model associated with that particular graph and then compare across all possible graphs to find the best plausible intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm</head><p>With the surrogate models, plausible models, and acquisition function defined in the previous section, we now introduce our proposed algorithm to address the challenging setting where the causal graph and associated functions are unknown. The Graph Agnostic Causal BO approach for soft interventions is summarised in Algorithm 1, and the hard intervention version in Algorithm 2. The main difference is we select nodes to intervene on via do-calculus and also choose the values to intervene with for hard intervention. For graph discovery in both algorithms, we introduce a new Causal Subgraph Discovery method in Algorithm 3. We discuss the scalability of our approach in Appendix A. <ref type="bibr" target="#b3">4</ref>.</p><p>In Algorithm 1, we start by constructing confidence bounds for all plausible functions and update the plausible graph set as shown in Section 3.2. Before observing any data, all possible graphs are in the plausible graph set and have equal prior. Then for every feasible graph g t at time step t, we determine the action that maximizes the upper confidence bound of the variable of interest within that particular graph via the reparametrization trick as shown in Section 3.3. We select the action corresponding to the plausible graph with the highest UCB. After selecting the interventions, we can observe the corresponding values from all possible nodes and the dataset is updated which we use to update the GP posteriors for all plausible function. Based on the new posteriors we define new confidence bounds and repeat the steps for T iterations.</p><p>For graph discovery in Algorithm 3, we recursively identify parent variables of observed nodes starting from the target node while ensuring acyclicity. The algorithm initiates by computing the Gaussian Process Score S(V i , Z g i , A g i |D t ) for all potential components, considering the input space composed of combinations of observed nodes and action variables Z i × A i across all nodes i. A list of descendants is maintained to guide the traversal, and while choosing the component for a node, components with inputs containing descendants of the node are eliminated. Normalization is then applied to the Scores of the remaining components, and sampling from a multinomial distribution based on normalized probabilities is performed. The discovery process introduces a bias based on the encountered nodes during traversal, as the descendants of a node are determined by the order of traversal. To avoid such a bias we randomly sample a permutation from a uniform distribution over all possible permutations, to choose the order of iteration. We repeat n times to get n graph samples. Similar to the acquisition function defined in 18, we define an acquisition function for hard interventions, with the only difference being that hard interventions are performed instead of soft interventions. The observational uncertainty is propagated through the resulting mutated graph, the reparameterisation trick is used to find optimistic upper confidence for all plausible graphs arg max</p><formula xml:id="formula_35">I,a I ∈A max g∈Gt max ηg(•) E[y| fg , do(V I = a I )].<label>(21)</label></formula><p>This is slightly different as compared to soft interventions, because a hard intervention mutates the graph, making the node independent of all ancestor nodes and interventions performed on them, thus simplifying the problem. This induces the notion of Minimal Intervention Sets MIS <ref type="bibr" target="#b27">[28]</ref>. A MIS for an SCM ⟨g, Y, V , f g , Ω⟩ is defined as the set of variables X s ∈ P(X) such that there exists no such</p><formula xml:id="formula_36">X ′ s ⊂ X s for which E[Y | do(X s )] = E[Y | do(X ′ s )].</formula><p>We denote the MIS for graph g with target node y as M g,y however since the graph structure is not known to us a priori, we construct our Plausible MIS M y,t , by taking the union over the MIS of plausible graphs at time step t, i.e. M y,t = g∈Gt M g,y . For each plausible graph, we only compare interventions within the MIS of the given graph to find the intervention which maximises the surrogate model associated with that particular graph and then compare across all possible graphs to find the best plausible intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed algorithm GACBO on synthetic environments (Dropwave, Alpine3, Rosenbrock, ToyGraph) and a real-life (Epidemiology Graph) environment introduced in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Our reported metric is average reward which is directly inversely related to cumulative regret. A higher expected reward indicates lower cumulative regret, which suggests better performance. We repeat each experiment 5 times with different seeds and report average rewards ±σ/ √ 5, where σ is the standard deviation across all repeats. For hard intervention, we tested on ToyGraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Causal Subgraph Discovery</head><formula xml:id="formula_37">Input: S i = {S(V i , Z i , A i |D t ), ∀(Z i , A i ) ∈ Z i × A i }, where i ∈ [m], g t = {}, De(m) = {}. function FINDSUBGRAPH(g, i, S i , De(i)) if i ∈ g then return g end if S c i = {S ∈ S i | Z i ∩ De(i) = ∅}, Sc i = { S S∈S c i S ∀S ∈ S c i }, (Z c i , A c i ) ∼ Multinomial( Sc i ) if Z c i = ∅ then g = g ∪ {i : (Z c i , A c i )} else g = {g ∪ {i : (Z c i , A c i )}, P a g (i) ∼ Uniform(Permutations(Z c i )) for j ∈ P a g (i) do De(j) = De(j) ∪ {i}, g = FINDSUBGRAPH(g,</formula><p>Baselines We consider the following baselines to compare with our proposed algorithm 1) Modelbased Causal Bayesian Optimisation MCBO <ref type="bibr" target="#b41">[42]</ref> with the true causal graph, which is the state-of-theart CBO method for cumulative regret objective. 2) We further test MCBO with incorrect graphs to simulate the situation where a causal graph might not be given or wrongly specified. We consider two kinds of incorrect graphs 1) Missing Edges: In such graphs at least one ancestral node of the target variable has a missing parent 2) Extra Edges: All true edges are present and additional edges are added. The correct graph and incorrect graphs used for experiments are provided in the appendix A.2 3) For soft intervention, we also compare with the BO algorithm Gaussian Process Upper Confidence Bound (GP-UCB) <ref type="bibr" target="#b39">[40]</ref>, which indicates the performance without exploring and learning causal structure. GP-UCB cannot be compared with our algorithm in the hard intervention case, since hard interventions require choosing nodes to intervene on, and GP-UCB cannot handle that without modification. Simulations We show the performance of our simulated experiments in Figure <ref type="figure">3</ref>. For soft intervention, we consider three simulated graphs <ref type="bibr" target="#b5">[6]</ref>, namely Dropware, Alpine3 and Rosebrock. For hard intervention, we consider the ToyGraph <ref type="bibr" target="#b0">[1]</ref>, with the intervention sets I = {∅, {0}, {1}, {0, 1}}. The causal graphs we use can be found Figure <ref type="figure">8</ref> in the Appendix. As expected, with the true causal graph, MCBO shows the best performance for almost all experiments and indicates the average rewards we can get when we do not spend samples exploring causal structure. The only exception is Rosenbrock, GP-UCB outperforms MCBO, this is because decomposing the function based on the causal graph of Rosenbrock provides no significant advantage as the output of each function is simply added to the input of the following node. We can observe that MCBO failed to learn good interventions and get good average rewards when there is no true causal graph revealing, even when the causal graph is slightly wrongly specified. When extra edges are specified the rate of learning a good intervention is significantly slower since the dimensionality increases. For the case of missing edges, the true function lies outside the function space and decisions are made on an incorrectly specified model which results in poor performance. Our algorithm GACBO initially suffers a higher regret as it has no causal structure information and performs actions based on incorrect graph structures before it converges to the true graph. We observe it can learn the causal graph quickly and reach to similar performance as MCBO after around 100 rounds for all experiments. Our simulated experiments show that our proposed algorithm can efficiently learn causal structures which benefits the goal of maximising average (cumulative) rewards.</p><p>Real-World Application: Epidemiology We test our proposed algorithm in a real-world application in Epidemiology <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>. We show the associated causal graph and performance in Figure <ref type="figure" target="#fig_4">4</ref>. The experiment aims to minimise HIV viral load by selecting two possible treatments (T, R, refer to Havercroft and Didelez <ref type="bibr" target="#b20">[21]</ref> for details). The agent is allowed to perform two treatments referenced as T, R, and a combination of both treatments simultaneously, i.e., the possible intervention set is I = {∅, {T }, {R}, {T, R}}. This environment is significantly more complex than the ToyGraph Environment due to a multimodal objective function with high observational noise. We observe in this real-world example, that our algorithm significantly outperforms MCBO with a wrongly specified causal graph, even if the causal graph only has two missing edges. GACBO can learn the causal structure efficiently and achieve a similar level of performance compared with MCBO with a known true graph within 100 rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Causal Decision Making The first causal Bayesian optimisation setting was proposed in Aglietti et al. <ref type="bibr" target="#b0">[1]</ref>, which focused on hard interventions and the best intervention identification setting. Sussex et al. <ref type="bibr" target="#b41">[42]</ref> expanded their setting to include soft interventions and noisy environments. They proposed the Model-based Causal Bayesian Optimisation (MCBO) algorithm, which is the state-of-the-art method with a known graph. With unknown graphs for cumulative regret objective, Lu et al. <ref type="bibr" target="#b30">[31]</ref>, De Kroon et al. <ref type="bibr" target="#b9">[10]</ref>, Konobeev et al. <ref type="bibr" target="#b25">[26]</ref> considered causal multi-armed bandits. Lu et al. <ref type="bibr" target="#b30">[31]</ref> studied causal trees, causal forests and proper interval graphs, with regret analysis under a few causal assumptions. De Kroon et al. <ref type="bibr" target="#b9">[10]</ref> utilised an estimator based on separating sets, with no theoretical analysis on regret shown. Konobeev et al. <ref type="bibr" target="#b25">[26]</ref> proposed a RAndomized Parent Search algorithm (RAPS) and showed conditional regret upper bounds. Malek et al. <ref type="bibr" target="#b31">[32]</ref> show that the unknown causal graph be exponentially hard in parents of the outcome and studies the problem under the additive assumption on the outcome. All the above work considered discrete arms (intervention values) and linear bandits, while our work addresses continuous intervention values, non-linear relations between nodes and a more general class of graphs.</p><p>Branchini et al. <ref type="bibr" target="#b7">[8]</ref> studied the setting with an unknown graph for the best intervention identification setting. Their approaches are based on the entropy search criterion. However, directly applying their method to cumulative regret objective would lead to suboptimal performance since one needs to further balance the exploitation-exploration balance between picking actions that lead to the best rewards and learning causal structures. Alabed and Yoneki <ref type="bibr" target="#b2">[3]</ref> studied the CBO problem for unknown causal graph scenario with a specific application to autotuners. To the best of our knowledge, we are the first to study the CBO with unknown graph and cumulative regret objectives.</p><p>Active Causal Discovery von Kügelgen et al. <ref type="bibr" target="#b47">[48]</ref> developed a Bayesian optimal experimental design framework to perform active causal discovery for Gaussian Process networks. Lorch et al. <ref type="bibr" target="#b29">[30]</ref>, Giudice et al. <ref type="bibr" target="#b17">[18]</ref> addressed the problem of causal discovery for graphs with a larger number of nodes. Based on which, Tigas et al. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> performed active causal discovery for larger graphs. Toth et al. <ref type="bibr" target="#b45">[46]</ref> considered the active learning methods for unifying sequential causal discovery and causal reasoning.</p><p>The goals of active causal discovery and Bayesian optimisation are misaligned. While Bayesian optimisation tries to balance exploration and exploitation to minimise cumulative regret, the active causal discovery acquisition function might choose an intervention that has a low reward and does not help future steps of CBO but helps discover the true underlying Causal Graph. Therefore it is suboptimal to first perform active causal discovery and then followed by causal Bayesian optimisation as separate steps. Our algorithm naturally unifies these two steps by making causal discovery a sub-task of causal Bayesian optimisation. See Appendix A.3 for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we study a novel setting of causal Bayesian optimisation with the cumulative regret objective where the causal graph is unknown. To the best of our knowledge, our work is the first work addressing this setting. This is motivated by real-world applications such as Epidemiology where we do not have prior knowledge of causal structure while we deal with optimisation problems. For two settings, namely structural causal models with hard interventions and function networks with soft interventions, we propose a new algorithm Graph Agnostic Causal Bayesian Optimisation (GACBO) to balance picking actions with known high rewards and learning the causal structure and reducing function uncertainty by sampling unknown actions only when it potentially results in better rewards, based on the Upper Confidence Bound (UCB) type of acquisition function. Our algorithm deals with three types of uncertainties causal structure, function space, and noisy observations. We show in simulations and real-world applications that our algorithm outperforms the state-of-art MCBO algorithm when there is no underlying true graph revealed to it. We discuss the theoretical analysis of our algorithm in Appendix A.5 and leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Nomenclature</head><p>Using standard notation, we use Capital letters to denote random variables and lowercase letters to denote the realization of said random variables. We use bold letters to denote sets of certain nodes. The support of a variable is given by curly letters. We use the subscript t to index data observed thus far, and the subscript i is used to index a particular node in a vector, the superscript g is used to refer to the input space Symbol Description Dropwave <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">42]</ref> For our setting we consider A 0 ∈ [-5.12, 5.12] and A 1 ∈ [-5.12, 5.12], we set β = 0.5 and </p><formula xml:id="formula_38">V j j th observed</formula><formula xml:id="formula_39">ϵ i = 0.1∀i ∈ [m] x 0 = f 0 (a 0 , a 1 ) = a 2 0 + a 2 1 + ϵ 0 y = f y (x 0 ) = 1 + cos(12x 0 ) 2 + 0.5x 2 0 + ϵ y<label>(22)</label></formula><formula xml:id="formula_40">f 0 (a 0 , a 1 ) = -100a 1 -a 2 0 2 -(1 -a 0 ) 2 + ϵ 0 f k (a k , a k+1 , x k-1 ) = -100a k+1 -a 2 k 2 -(1 -a k ) 2 + x k-1 + ϵ k i = 1, . . . , m<label>(23)</label></formula><p>Alpine3 <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref> For our experiments we consider a i ∈ [0, 10], for i ∈ m, we consider β = 0.5 and η = 0.1 </p><formula xml:id="formula_41">f 0 (x 0 ) = - √ x 0 sin(x 0 ) + ϵ 0 f i (a i , x i-1 ) = √ a i sin(a i )x i-1 + ϵ i , i = 1, . . . , m<label>(24)</label></formula><formula xml:id="formula_42">X = ϵ x Z = exp(-X) + ϵ z Y = cos(Z) -exp(-Z/20) + ϵ y (25)</formula><p>Epidemiology <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b20">[21]</ref> In our settings, we consider the following input ranges for interventions T ∈ [0, 4] and R ∈ [0, 4], we use β = 1 and noise levels are specified according to the SCM.</p><formula xml:id="formula_43">B = U[-1, 1] T = U[4, 8] L = expit(0.5T + U ) R = 4 + LT Y = 0.5 + cos(4T ) + sin(-L + 2R) + B + ϵ withϵ ∼ N (0, 1)<label>(26)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Causal Discovery and Causal Bayesian Optimization</head><p>Causal discovery from observational data <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> can recover causal graphs up to Markov Equivalence Classes (MEC). <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref> go beyond MEC from purely observational data based on information asymmetry. <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref> study the problem of learning graphs from observational and interventional data.</p><p>Active causal discovery aims to learn the SCM efficiently. For example, <ref type="bibr" target="#b47">[48]</ref> studied causal structure learning actively using Bayesian Optimal Experimental Design (BOED). The acquisition function used in their model seeks to select the intervention that is maximally informative about the underlying causal structure with respect to the current model.</p><p>Bayesian optimisation aims to learn the optimal point of unknown functions. Knowing the causal structure helps us reduce the causal intrinsic dimension of the optimisation problem for hard intervention. In the case of soft interventions, causal knowledge is useful for utilising the information from intermediate nodes and converting a high dimension problem into n smaller dimensional optimisation problems (where n is the number of intermediate nodes).</p><p>However, learning the entire SCM such as in active causal discovery (i.e., all the causal edges and mechanisms of all nodes in their entire domain) is not necessary for causal Bayesian optimisation. This is understood in two separate cases:</p><p>Hard Intervention A hard Intervention mutates the graph, making the intervened node independent of all its parents and ancestors. <ref type="bibr" target="#b27">[28]</ref> demonstrated that the optimal intervention lies within the parents when there are no unobserved confounders. In such a case learning the causal relation between the ancestors of the parents does not help the underlying goal of causal Bayesian optimisation. Consider the example of ToyGraph, in the true data-generating mechanism X 1 is the parent of Y , and on performing do(X 1 = x 1 ) the value of Y is not affected by the value of X 0 , hence for optimization knowing the causal direction or mechanism relating X 0 and X 1 is not required.</p><p>Soft Intervention A soft intervention does not mutate the graph, hence learning the causal relations of the ancestral nodes is still relevant to the downstream optimisation problem, however learning the entire causal structure might still be wasteful. If we have determined (specified by expert knowledge or during a certain step of our active causal discovery process that a certain node is not an ancestor of the target node, then knowing the ancestors or descendants of the node does not contribute to causal Bayesian optimisation. Causal structure in the case of soft intervention utilises values of intermediate nodes to constrain the optimisation problem. Causal structure is only useful when the decomposed problem is simpler than the original problem. For example consider function f (x 1 , x 2 ) = g(h(x 1 ), x 2 ), knowing the intermediate value h(x 1 ) is only useful if the composed function f is more difficult to optimise (because of non-linearity) than the individual functions g and h. We observe in our experiments with the Rosenbrock graph in Section 4 that there is no significant advantage of causal structure when the intermediate functions are purely linear.</p><p>The goals of active causal discovery and Bayesian optimisation are misaligned. While Bayesian optimisation tries to balance exploration and exploitation to minimise cumulative regret, the active causal discovery acquisition function might choose an intervention that has a low reward and does not help future steps of causal Bayesian optimisation but helps discover the true underlying Causal Graph. Therefore it is sub-optimal to firstly perform active causal discovery and then followed by causal Bayesian optimisation as separate steps. Since the active causal discovery step might spend additional samples with low reward learning useless parts of the Causal graph.</p><p>Our algorithm naturally unifies these two steps by making causal discovery a sub-task of causal Bayesian optimisation. If multiple causal graphs exist within our hypothesis space that explains the data collected up to time step t, we only perform an intervention aimed at disambiguation between these graphs if it potentially leads to better rewards than the optimal value observed thus far. Our acquisition function (2) has three maximisation, for a given graph g ∈ G t there are several plausible functions for each node f g i,t and all possible combination of node functions define the function space for the graph g. We use the optimistic reparameterisation trick to find the combination of functions and actions a g which maximises the target node. We do this for all plausible graphs and compare the best possible value for each graph g. We select the plausible graph g with the maximum possible value for target node and the corresponding action a g which maximises it. Consider a hypothetical scenario with two different graphs g 1 , g 2 which disagree on the value of node i for intervention a but the action which maximises the value of target node y g1 , y g2 in g 1 and g 2 is same a * , and the target node values also agree i.e., y * g1 = y * g2 for action a * , even though performing a would help identify the true graph our acquisition function is designed to choose a * . Because y g1 ≤ y * g1 or y g2 ≤ y * g2 for any action a ̸ = a * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Superexponential Scaling of DAGs and Scalability</head><p>The problem setting we addressed in this paper is challenging due to the super-exponential growth of the number of DAGs with the increase in the number of nodes. We only focus on small graphs where all the graphs can be enumerated to study the problem of CBO with unknown graphs in isolation. Our approach can be further improved to be more scalable, by MCMC-based sampling in the space of graphs <ref type="bibr" target="#b17">[18]</ref>, or Differential approaches like DiBS <ref type="bibr" target="#b29">[30]</ref> in latent spaces or topological ordering of nodes. We leave it as a future work.</p><p>Our method in its current state computes the GP score of all possible graph components (all combinations of parents and actions for all observed nodes) and samples graphs based on the GP score and individually optimises and compares all sampled graphs. For larger graphs, the problem becomes intractable as the number of components for which the GP score needs to be calculated increases exponentially. In the initial rounds, the number of graphs that need to be optimised and the number of comparisons that need to be made also increases superexponentially. Causal Bayesian optimisation using the MCBO approach also takes longer for larger graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Discussion on Theoretical Analysis</head><p>Our approach suggests attaining a similar regret bound to MCBO but with an added constant term. However, our method holds potential for a superior regret bound by simultaneously exploring causal structure and exploiting rewards from the outset. Empirical results indicate that our algorithm, GACBO, exhibits a significantly faster increase in average rewards after initial rounds, underscoring its potential for improved regret. The lack of guarantees for the convergence of the posterior to the true graph in finite samples is a major obstacle. A potential theoretical proofing can be achieved by decomposing our regret into two parts:</p><p>• Constant term: For first n samples before learning the true graph, we obtain the constant regret. This is due to the boundness assumption of function (See section 2.3 "Regularity Assumption")</p><formula xml:id="formula_44">||f g * i || ≤ B i .</formula><p>No matter what actions are selected, the upper bound of instant regret can be bounded by 2B i .</p><p>• MCBO regret: the second term is the same as the MCBO regret term since after n samples we've discovered the true graph.</p><p>Effect of Graph Knowledge on Optimisation Theorem 1 of <ref type="bibr" target="#b41">[42]</ref> bounds the regret with high probability when the graph is known but functions are unknown in the case of soft intervention as</p><formula xml:id="formula_45">R T ≤ O(L N f L N σ β N T K N m</formula><p>√ T γ T ) where γ T = max i γ i,T , and N denotes the maximum distance from a root node to V m , K = max i |pa * g (i)| as compared to Standard Bayesian Optimisation that makes no use of graph structure resulting in cumulative regret exponential in m. Assuming the use of the Squared Exponential Kernel for modeling all functions, γ T = O((K + q)(logT ) K+q+1 ) scales exponentially with respect to K and q the length of each action vector. This results in an expression that scales exponentially in K, N . The theorem demonstrates a potentially exponential improvement in the scaling of cumulative regret for possible actions m ≥ K + N .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph Agnostic Causal Bayesian Optimisation (GACBO) workflow. Top: Select plausible graphs based on data collected so far, Right: Perform Causal Bayesian Optimisation on plausible graphs, Bottom: Select the action based on the highest reward among all plausible graphs, Left: Execute selected action, collect Data and repeat steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Problem Settings and Causal Structures: (a) Bayesian optimisation; (b) Structural causal models and hard interventions; (c) Function networks and soft interventions; (c-w1) Incomplete graph for (c), missing X1; (c-w2) Incorrect graph for (c), reversing order of X1 and X2. The blue circles X1 and X2 represent non-manipulative variables, the orange squares A1 and A2 represent actions that can be taken, and Y is the outcome of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and (14) holds true ∀a ∈ A}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Real-world applications: Epidemiology. We show the true causal graph on the left top, where the T,R are potential treatments and Y is the target to be optimised. B,L are non-manipulative. The left bottom shows the designed wrong causal structure for MCBO. The right-hand side shows the performance of our experiments.</figDesc><graphic coords="10,300.13,74.88,158.22,105.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Dropwave: True DAG structure, and Incorrect DAG structures used in Experiment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Alpine3: True DAG structure, and Incorrect DAG structures used in Experiment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Graph Agnostic Causal Bayesian Optimisation(Soft Interventions) (GACBO-S)Input: Parameters {β t } t≥1 , Ω, generic kernel function k i , prior over possible ψ i,0 graph components, prior meansµ i,0 = 0∀i ∈ [m]. for t = 1 . . .T doConstruct confidence bounds for plausible functions M t as in Eq. (16). Construct plausible graphs G t as in Eq. (17) using Algorithm 3. Select a :,t ∈ arg max a∈A max g∈Gt max ηg(•) E[y| fg , a] as in Eq. (20). Observe all nodes v t and update D t = D t-1 ∪ {v t , a :,t }.In addition to MCBO Sussex et al.<ref type="bibr" target="#b41">[42]</ref>, we design our acquisition function to incorporate uncertainty stemming from the absence of a priori knowledge regarding the underlying graph structure, by selecting actions giving the highest possible values according to the plausible graphs and functions,</figDesc><table><row><cell cols="2">Update posterior {{µ g i,t (•), σ g i,t (•)} m i=0 } g∈G .</cell><cell></cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.3 Acquisition Function</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a :,t = arg max a∈A</cell><cell>max g∈Gt</cell><cell>max fg∈M</cell><cell>g t</cell></row></table><note><p>E[y| fg , a].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 2 Graph Agnostic Causal Bayesian Optimisation (Hard intervention) (GACBO-H) Requires: Parameters {β t } t≥1 , Ω, generic kernel function k i , prior over possible graphs G 0 , prior means µ g i,0 = 0∀i ∈ [m], g ∈ G 0 . for t = 1 . . . T do Construct confidence bounds for plausible functions M t as in 16 Construct plausible graphs as in 17. Select I, a I ∈ arg max I,a I ∈A max g∈Gt max ηg(•) E.[y| fg , do(V I = a I )] as in 21. Observe all nodes v t and update D t = D t-1 ∪ {v t , a t } Update posterior</figDesc><table><row><cell>{{µ g i,t (•), σ g i,t (•)} m i=0 } g∈G .</cell></row><row><cell>end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Simulation Results. We compare our method (GACBO) with MCBO with true graph and wrong graphs (missing edges and extra edges), and GP-UCB. As discussed in Section 4, GP-UCB is inappropriate for ToyGraph. For soft intervention, we tested on Dropwave, Alpine3 and Rosenbrock.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">j, S j , De(j))</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">end for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">end if</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">return g</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">end function</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Output: g t = FINDSUBGRAPH(g t , m, S m , De(m))</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Average Reward</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7</cell><cell cols="6">Dropwave: Average Reward GACBO MCBO Missing Edges-MCBO Extra Edges-MCBO GP-UCB</cell><cell></cell><cell cols="2">Average Reward</cell><cell>5 10 15 20 25 30 35 40</cell><cell></cell><cell cols="3">Alpine3: Average Reward GACBO MCBO Missing Edges-MCBO Extra Edges-MCBO GP-UCB</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>Round</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>Round</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Rosenbrock: Average Reward</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Toy Graph: Average Reward</cell></row><row><cell>Average Reward</cell><cell cols="2">2000 1500 1000 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GACBO MCBO Missing Edges-MCBO Extra Edges-MCBO GP-UCB</cell><cell>Average Reward</cell><cell cols="2">2.0 0.8 1.0 1.2 1.4 1.6 1.8</cell><cell></cell><cell></cell><cell></cell><cell>GACBO MCBO Missing Edges-MCBO Extra Edges-MCBO</cell></row><row><cell></cell><cell cols="2">2500</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>Round</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell></cell><cell cols="2">0.6</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>Round</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell cols="3">Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The parents of node i in graphg * f g * i (z g * i , a g * i )functions relating a node i with it's parents and actions f g * (a)The overall function with input action composed of functions {f g * i } m i=0 related by graph g * F g * the set of respective unknown functions associated with g * , i.e. {f g * i } m i=0 Ω a set of independent noises with zero mean and known distribution, i.e. {Ω i } m i=0 pa g * (i) indices of parent nodes of any node, defined for the DAG g * {y t , v t , a t } Observation of reward variable y t and intermediate variables v t for the corresponding action a t D tObservations for actions until time t, is the set {y j , v j , a j } t</figDesc><table><row><cell></cell><cell>variable</cell></row><row><cell>Y</cell><cell>Target variable we seek to optimize corresponds to V m</cell></row><row><cell>V</cell><cell>Set of all observed variables</cell></row><row><cell>X</cell><cell>Set of intervenable variables</cell></row><row><cell>C</cell><cell>Set of non intervenable variables</cell></row><row><cell>A i</cell><cell>Action performed on node i</cell></row><row><cell>g  *</cell><cell>True latent causal graph</cell></row><row><cell>A Z g  *</cell><cell>Action vector composed of {A i } m i=0</cell></row><row><cell></cell><cell>j=0</cell></row><row><cell>G t</cell><cell>Posterior of the distribution over graphs at time t</cell></row><row><cell>g</cell><cell>Random DAG samples from G t</cell></row><row><cell>k g i (•, •) k g i,t (•) K g i µ g i,t (•) σ g i,t (•) GP (µ g i,t , σ g i,t ) H k g i f g i</cell><cell>Kernel defined on input space implied by graph g for node i, gives covariance between two points A vector of covariances of the current input to previous inputs [k g i ((z g i,t , a g i,t ), •)] t i=0 Covariance matrix based on previous D t Mean function based on data D t and kernel k g i (•, •) Variance function based on data D t and kernel k g i (•, •) Gaussian Process f g i (•) ∼ N (µ g i,t (•), σ g i,t (•)) Hilbert Spaces of functions implied by kernel k g i A function sampled from Gaussian Processes GP</cell></row><row><cell>ω i</cell><cell>Observational noise of node i</cell></row><row><cell>M t</cell><cell>Plausible models at time t based on confidence bounds</cell></row><row><cell cols="2">A.2 Simulation Details</cell></row></table><note><p>i</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For environments allowing hard interventions, the optimisation problem can be reduced to the Causal Intrinisic dimension <ref type="bibr" target="#b0">[1]</ref> if interventions on parents is allowed, <ref type="bibr" target="#b27">[28]</ref> shows results that the optimal intervention is always found among parents.For environments which do not allow direct interventions on parents the problem can be studied as a soft intervention for the mutated graph, by treating the intervened nodes as action nodes and propagating uncertainty through the remaining nodes. For certain graphs the depth N of the resulting graph can be reduced significantly, consider by figure <ref type="figure">1</ref> of <ref type="bibr" target="#b0">[1]</ref> with a slight modification where, intervention on the parent nodes {X 100 , Z 100 } is not possible but we are allowed to intervene on {X 99 , Z 99 }, this allows us to reduce N = 2 from N = 100, resulting in an exponential improvement in performance.</p><p>Convergence to the True Graph As the posterior mass on the graph distribution converges to the dirac delta distribution on the true graph p(g) -→ δ g=g * the cumulative regret converges to the cumulative regret accrued when the graph is known. For hard interventions the posterior convergence to the true graph is guaranteed under a few assumptions. For soft intervention models DAGs belonging to Markov Equivalence Classes are further distinguished under the assumptions underpinning the GPN models, considering the functions f i , are not generally invertible <ref type="bibr" target="#b17">[18]</ref>, the GPN usually suggest higher scores to models admitting the true SEM structure as confirmed in our numerical experiments. For cases where functions are invertible <ref type="bibr" target="#b21">[22]</ref> guarantees identifiability by leveraging the asymmetry of residual noise distributions.</p><p>While asymptotic convergence to the true graph structure is guaranteed, there are no known results for finite samples. However, in our numerical experiments, we observe that the graph converges to the essential graph in a small number of samples and potentially observes exponentially less regret as compared to not knowing the graph henceforth. Several studies have considered the problem of learning the causal structure optimally, <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>. Future work could look at more efficient techniques to learn the structure with finite time guarantees to place an upper bound on the cumulative regret for the case when the graph is unknown apriori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Experiment Details</head><p>All our experiments were performed on Google Colab without a GPU or TPU enabled, we used random seeds 47, 42, 73, 66, 13 for 5 repeat for all given algorithms and given environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Limitations and Future Work</head><p>In the current work we focus on the problem of causal Bayesian optimisation with unknown graph, however we make several assumptions which may be violated in practise. We assume no unobserved confounders, this assumption is critical to our causal discovery algorithm and our model based approach for causal Bayesian optimisation is also not resilient to unobserved confounders. We assume additive noise and known noise distribution for each node, this is a strong assumption in practise and needs to be relaxed in future work. Our regularity assumptions might also restrict the application of our method to problems where the relation between a node and it's parents is not highly nonlinear. Our current method does not scale well to larger graphs, however this can be addressed in future work as described in section A.4. We defer providing theoretical guarantees for our method to future work as discussed in A.5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal bayesian optimization</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Paleyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abcd-strategy: Budgeted experimental design for targeted causal structure discovery</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3400" to="3409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Sami</forename><surname>Alabed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiko</forename><surname>Yoneki</surname></persName>
		</author>
		<author>
			<persName><surname>Bograph</surname></persName>
		</author>
		<idno type="DOI">10.1145/3517207.3526977</idno>
		<ptr target="https://doi.org/10.1145/3517207.3526977" />
		<title level="m">Proceedings of the 2nd European Workshop on Machine Learning and Systems</title>
		<meeting>the 2nd European Workshop on Machine Learning and Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bograph: structured bayesian optimization from logs for expensive systems with many parameters</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Alabed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiko</forename><surname>Yoneki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd European Workshop on Machine Learning and Systems</title>
		<meeting>the 2nd European Workshop on Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A characterization of markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Steen A Andersson</surname></persName>
		</author>
		<author>
			<persName><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><surname>Michael D Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian optimization of function networks</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14463" to="14475" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">P</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03357</idno>
		<title level="m">No-regret bayesian optimization with unknown hyperparameters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal entropy optimization</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Branchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Dhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Damoulas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8586" to="8605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal bandits without prior knowledge using separating sets</title>
		<author>
			<persName><forename type="first">Arnoud</forename><surname>De Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Belgrave</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="407" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exact bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interventions and causal inference</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under latent interventions</title>
		<author>
			<persName><forename type="first">Gonçalo</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alves</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Mário</surname></persName>
		</author>
		<author>
			<persName><surname>Figueiredo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="253" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Being bayesian about network structure. a bayesian approach to structure discovery in bayesian networks</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iftach</forename><surname>Nachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3857</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Gaussian process networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian Optimization</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Budgeted experiment design for causal structure learning</title>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saber</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1724" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Giudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giusi</forename><surname>Moffa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11380</idno>
		<title level="m">A bayesian take on gaussian process networks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two optimal strategies for active learning of causal models from interventional data</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="926" to="939" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simulating from marginal structural models with timedependent confounding</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Havercroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="4190" to="4206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A literature survey of benchmark functions for global optimisation problems</title>
		<author>
			<persName><forename type="first">Momin</forename><surname>Jamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-She</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Modelling and Numerical Optimisation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="194" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information-geometric approach to inferring causal directions</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lemeire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Povilas</forename><surname>Daniušis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Steudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost-optimal learning of causal graphs</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1875" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Konobeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jalal</forename><surname>Etesami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11401</idno>
		<title level="m">Causal bandits without graph learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bandit algorithms</title>
		<author>
			<persName><forename type="first">Tor</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structural causal bandits: Where to intervene?</title>
		<author>
			<persName><forename type="first">Sanghack</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural causal bandits with non-manipulable variables</title>
		<author>
			<persName><forename type="first">Sanghack</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4164" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dibs: Differentiable bayesian structure learning</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24111" to="24123" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal bandits with unknown graph structure</title>
		<author>
			<persName><forename type="first">Yangyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Meisami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24817" to="24828" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Additive causal bandits with unknown graph</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23574" to="23589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An interactive approach for bayesian network learning using domain/expert knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Andrés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafín</forename><surname>Masegosa</surname></persName>
		</author>
		<author>
			<persName><surname>Moral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1168" to="1181" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On bayesian methods for seeking the extremum</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Močkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques IFIP Technical Conference</title>
		<meeting><address><addrLine>Novosibirsk</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1974">July 1-7, 1974. 1975</date>
			<biblScope unit="page" from="400" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Active learning of causal bayes net structure</title>
		<author>
			<persName><forename type="first">Kevin P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A bayesian active learning experimental design for inferring signaling networks</title>
		<author>
			<persName><forename type="first">Robert Osazuwa</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vitek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology: 21st Annual International Conference</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-05-03">2017. May 3-7, 2017. 2017</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="134" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511803161</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Constructing bayesian network models of gene expression networks from microarray data</title>
		<author>
			<persName><forename type="first">Pater</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Valerio Aimale</surname></persName>
		</author>
		<author>
			<persName><surname>Wimberly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sham M Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Seeger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.3995</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Drop-wave function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Surjanovic</surname></persName>
		</author>
		<author>
			<persName><surname>Bingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Model-based causal bayesian optimization</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasiia</forename><surname>Makarova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10257</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interventions, where and how? experimental design for causal models at scale</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24130" to="24143" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differentiable multi-target causal bayesian experimental design</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Desi R Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Bauer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="34263" to="34279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Active learning for structure in bayesian networks</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on artificial intelligence</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="863" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Active bayesian causal inference</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16261" to="16275" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic and causal inference: The works of Judea Pearl</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="221" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Optimal experimental design via bayesian optimization: active causal structure learning for gaussian process networks</title>
		<author>
			<persName><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Max-value entropy search for efficient bayesian optimization</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3627" to="3635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Elementary statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gaussian processes for regression</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Characterizing and learning equivalence classes of causal dags under interventions</title>
		<author>
			<persName><forename type="first">Karren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Katcoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5541" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
