<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FACTORED SPACE MODELS: TOWARDS CAUSALITY BETWEEN LEVELS OF ABSTRACTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-20">20 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Scott</forename><surname>Garrabrant</surname></persName>
							<email>scott.garrabrant@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><forename type="middle">G</forename><surname>Mayer</surname></persName>
							<email>matthias.georg.mayer@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Magdalena</forename><surname>Wache</surname></persName>
							<email>magdalena-wache@mailbox.org</email>
						</author>
						<author>
							<persName><forename type="first">Leon</forename><surname>Lang</surname></persName>
							<email>l.lang@uva.nl</email>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Eisenstat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Holger</forename><surname>Dell</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Machine Intelligence Research Institute</orgName>
								<orgName type="institution" key="instit2">Machine Intelligence Research Institute</orgName>
								<orgName type="institution" key="instit3">Machine Intelligence Research Institute</orgName>
								<orgName type="institution" key="instit4">University of Amsterdam</orgName>
								<orgName type="institution" key="instit5">Machine Intelligence Research Institute</orgName>
								<orgName type="institution" key="instit6">Goethe University</orgName>
								<address>
									<settlement>Frankfurt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IT University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FACTORED SPACE MODELS: TOWARDS CAUSALITY BETWEEN LEVELS OF ABSTRACTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-20">20 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2412.02579v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>factored space models</term>
					<term>deterministic causality</term>
					<term>abstraction</term>
					<term>structural independence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causality plays an important role in understanding intelligent behavior, and there is a wealth of literature on mathematical models for causality, most of which is focused on causal graphs. Causal graphs are a powerful tool for a wide range of applications, in particular when the relevant variables are known and at the same level of abstraction. However, the given variables can also be unstructured data, like pixels of an image. Meanwhile, the causal variables, such as the positions of objects in the image, can be arbitrary deterministic functions of the given variables. Moreover, the causal variables may form a hierarchy of abstractions, in which the macro-level variables are deterministic functions of the micro-level variables. Causal graphs are limited when it comes to modeling this kind of situation. In the presence of deterministic relationships there is generally no causal graph that satisfies both the Markov condition and the faithfulness condition. We introduce factored space models as an alternative to causal graphs which naturally represent both probabilistic and deterministic relationships at all levels of abstraction. Moreover, we introduce structural independence and establish that it is equivalent to statistical independence in every distribution that factorizes over the factored space. This theorem generalizes the classical soundness and completeness theorem for d-separation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning causal relationships plays a central role in human intelligence <ref type="bibr" target="#b22">[24]</ref>  <ref type="bibr" target="#b11">[12]</ref>, and learning a causal model is necessary for any system that can generalize out of distribution <ref type="bibr" target="#b19">[21]</ref>. Therefore, a rigorous mathematical theory of causality is key to understanding intelligent behavior.</p><p>There is a large body of literature about the mathematical foundations of causality <ref type="bibr">[17]</ref>, a highly successful area of research, most of which is based on causal graphs. An important feature of causal graphs is that they can encode statistical independence in their structure via d-separation <ref type="bibr" target="#b14">[15]</ref>. It is commonly assumed that a graph G is a useful model for a probability distribution P if it is a perfect map of P , that is, if d-separation in G implies statistical independence in P (Markov condition), and the converse (faithfulness condition) <ref type="bibr" target="#b23">[25]</ref>.</p><p>Figure <ref type="figure">1</ref>: Summary of our results. In a causal graph, the standard criterion for independence is d-separation. However, d-separation is undefined for variables that are deterministic functions of other variables, such as A := X + Y + Z.</p><p>To address this issue, we introduce factored space models (FSMs) as an alternative to causal graphs. FSMs are based on expressing the sample space as a Cartesian product, and can be visualized as a hyper-rectangle. FSMs allow us to define structural independence as an independence criterion which is also defined for deterministic functions of variables. Further, we establish a theorem that generalizes the soundness and completeness of d-separation <ref type="bibr" target="#b14">[15]</ref> to structural independence.</p><p>However, as we show in Section 3, when variables are deterministically related, there is generally no perfect map with these variables. This is an important limitation because deterministic relationships appear in many applications. For example, mathematical relationships, equations in physics, and computer programs all contain deterministic relationships between variables. In particular, deterministic relationships are inherent when the causal variables are unknown, such as in causal representation learning <ref type="bibr" target="#b21">[23]</ref>. For example, if the given variables are the pixels of an image, then the causal variables such as the positions of objects in the image can be arbitrary deterministic functions of the given variables. Moreover, when we model a system at different levels of abstraction, there are deterministic relationships between the detailed, micro-level variables and the abstract, macro-level variables. For example, the temperature of gas in a container is a deterministic function of the kinetic energy of all the individual gas particles in the container.</p><p>We introduce a framework in which works naturally with deterministic relationships, and treats functions of variables on equal footing. Our main contributions are the following:</p><p>1. Factored space models (FSMs), an alternative to causal graphs. FSMs naturally represent all random variables, including deterministic functions of other variables, rather than distinguishing those variables that appear in a particular graph. FSMs are a reframing of finite factored set models <ref type="bibr" target="#b7">[8]</ref>, and they are based on representing the sample space as a Cartesian product, as depicted in Figure <ref type="figure">1</ref>.</p><p>2. Structural independence, a criterion that characterizes statistical independence.</p><p>3. Generalizing the soundness and completeness of d-separation. We establish Theorem 6.2, which generalizes the soundness and completeness theorem for d-separation <ref type="bibr" target="#b14">[15]</ref> to variables that can be deterministically related.</p><p>Overview. The paper proceeds as follows. In section Section 3, we show the limitations of causal graphs when it comes to modeling deterministic relationships. In Section 4, we introduce the FSM framework, prove that structural independence forms a semigraphoid, and show how to construct an FSM from a causal graph. In Section 6, we prove the soundness and completeness of structural independence. Finally, in Section 7, we discuss the implications of our results and provide directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section we review the related work on causal abstractions, and on representing deterministic relationships between variables. We also compare FSMs with causal graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal abstractions</head><p>The causal abstractions framework <ref type="bibr" target="#b2">[3]</ref> [1] <ref type="bibr" target="#b20">[22]</ref> [27] shares our broad aim, that of understanding relationships between variables not well modeled by causal graphs, and in particular when different variables reside at different levels of abstraction. In the causal abstractions framework, different levels of abstractions are conceived as different causal models. Transformations between such models are then used to relate them.</p><p>In our work, we seek to provide a language that can simultaneously do the work of causal models and d-separation, and also express the sort of relations between variables that can occur when we speak of microscale and macroscale variables together. For example, we may describe a gas both on a very detailed level in terms of the kinetic energy of each gas particle, and on a more abstract level in terms of the overall temperature of the gas. In particular, a representation for different levels of abstractions should include deterministic relations-we may want to model a macroscale variable as being determined by the microscale variables. One way to regard the division that exists in causal abstractions framework between the causal models on the one hand and the transformations on the other is that it stems from an inability to place variables that stand in such deterministic relations together in one causal graph. This is because no such graph can be a perfect map once we account for these deterministic relations, as we show in Section 3. We do not develop this here, but we are interested in the application of such ideas to neural network interpretability. The causal abstraction framework has been used in this way <ref type="bibr" target="#b9">[10]</ref> [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representing deterministic relationships</head><p>In a causal graph, the nodes are random variables, and the independences between those variables are represented by d-separation of the nodes. To represent independences between deterministic functions of variables, we need to represent these functions of variables as their own nodes. However, as illustrated in Section 3, when we allow variables that are functions of other variables, d-separation becomes incomplete. There have been various attempts to model causality with variables that are deterministically related. There are approaches like Causal Constraints models <ref type="bibr" target="#b3">[4]</ref> and an axiomatization of causal models with constraints <ref type="bibr" target="#b1">[2]</ref>, which extend causal models to include variables that are deterministically related by a constraint.</p><p>However, these approaches only consider an additional set of functional constraints on a fixed set of given variables rather than modeling any deterministic function of variables as its own variable. Moreover, both of these approaches do not provide a criterion for independence in the same way that d-separation provides an independence criterion nodes in a causal graph. In contrast, structural independence on an FSM, our analog of d-separation, is a criterion for independence that correctly represents independence even when variables are deterministically related.</p><p>There is work which does consider the independence between deterministically related variables <ref type="bibr" target="#b15">[16]</ref>, which provides a causal discovery method based on independence of functions of variables. This type of independence between functions of variables is what we represent in FSMs. However, the existing work only applies to the special case of 2 binary variables, and does not provide a general model for how to represent such independences. In contrast, FSMs can model situations with an arbitrary number of discrete variables. The closest to an independence criterion for variables which can have deterministic relationships is D-separation <ref type="bibr" target="#b10">[11]</ref> (note the capital 'D' to distinguish from d-separation). But just like for d-separation there are simple counterexamples for variables that are independent but not D-separated, as shown in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Causal graphs and FSMs</head><p>Like a causal graph, an FSM models a set of probability distributions on some variables, and implies certain conditional independence relations between them. Other research has also examined ways of representing probability distributions.</p><p>In particular, the factors of an FSM can be regarded as factors of a factor graph <ref type="bibr" target="#b24">[26]</ref>. FSMs generalize causal graphs; we can construct an FSM from a causal graph in order to represent the same independence properties, as we demonstrate in Section 5.2. Here, the factors are independent, which is in line with the spirit of causal modeling, as expressed by the principle of independent mechanisms <ref type="bibr" target="#b18">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Soundness and completeness of d-separation</head><p>The theory of causal graphs provides us with the d-separation criterion <ref type="bibr" target="#b14">[15]</ref>, which characterizes which conditional independence relations are implied by a causal graph. Given three sets of variables X, Y, Z corresponding to sets of nodes of a causal graph, X and Y are conditionally independent given Z in all probability distributions that factorize according to the graph if and only if X and Y are d-separated by Z in the graph.</p><p>We prove a more general version of this theorem, in which d-separation in a graph G is replaced by structural independence in a factored space. Our theorem is more general because in the classic theorem, X, Y , and Z are nodes in a graph, while in our version they can be arbitrary variables, including deterministic functions of variables such as X + Y . In particular, when an FSM is constructed from a causal graph, the classic theorem follows from our theorem.</p><p>3 Causal graphs cannot capture deterministic relationships Methods based on causal graphs are highly successful in modeling independence, intervention and counterfactuals. However, as we illustrate in this section, when there is a deterministic relationship between random variables, there is generally no perfect map of the probability distribution. That is, there is no graph G such that d-separation in G is equivalent to statistical independence.</p><p>Let ⃗ X := (X 1 , . . . , X n ) be a vector-valued random variable, and let Y := 1 n n i=1 X i be the mean of ⃗ X. One can think of Y as an abstraction of X. Further, let Z be a variable that is causally downstream of Y , for example These three independence statements together contradict the intersection axiom</p><formula xml:id="formula_0">Z := Y + N wherein N is independent noise. Then ⃗ X and Z are independent given Y (denoted ⃗ X ⊥ ⊥ Z | Y ), since when Y is known, ⃗ X provides no further information about Z. Moreover, Y ⊥ ⊥ Z | ⃗ X holds, since knowing ⃗ X fully determines Y , so Z provides no further information about Y . Furthermore, we have Z ̸ ⊥ ⊥ (Y, ⃗ X).</formula><formula xml:id="formula_1">[19] of d-separation (d-sep(X, Y | Z) ∧ d-sep(X, Z | Y ) ⇒ d-sep(X, (Y, Z)))</formula><p>. Therefore, there is no graph in which the d-separations are equivalent to the statistical independencies. In particular, the graph in Figure <ref type="figure" target="#fig_0">2</ref> is not a perfect map, because it does not reflect that</p><formula xml:id="formula_2">Y ⊥ ⊥ Z | ⃗ X holds.</formula><p>Even D-separation <ref type="bibr" target="#b10">[11]</ref>, which extends d-separation to include nodes that are deterministic functions of their parents, does not cover this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Factored space models</head><p>In this section, we formally introduce the factored space model (FSM) framework. We define the building blocks of the framework, including derived variables, history, structural independence and structural time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derived variables.</head><p>Random variables are a central object of study in statistics and causality. Formally, a random variable on a sample space Ω is a measurable function X : Ω → Val(X) from Ω to a value space Val(X). When we speak of variables in this paper, we always mean discrete random variables. Any subset of Ω is called event.</p><p>Our contribution is a better way to represent variables that are deterministically related. That is, some variables are a deterministic function of other variables. We define a variable being a deterministic function of another variable as follows: Definition 4.1 (Derived Variable). Let X : Ω → Val(X) and Y : Ω → Val(Y ) be two random variables, and let C ⊆ Ω be an event. Then, we say that Y is derived from</p><formula xml:id="formula_3">X on C, or that it is a deterministic function of X on C if there is a function f : Val(X) → Val(Y ), such that for all ω ∈ C, it holds that Y (ω) = f (X(ω))</formula><p>. We also write this as X ▷ C Y .</p><p>We write X ▷ Y as a shorthand for X ▷ Ω Y , and say that Y is derived from X or a deterministic function of X.</p><p>Given multiple variables X 1 , . . . , X n , we define the variable</p><formula xml:id="formula_4">(X 1 , . . . , X n ) : Ω → Val(X 1 ) × • • • × Val(X n ) by ω → (X 1 (ω), . . . , X n (ω)). Then, if Y ◁ (X 1 , . . . , X n ),</formula><p>we say that Y is a deterministic function of the variables X 1 , . . . , X n .</p><p>Operations on indexed families. As we make heavy use of indexed families, we introduce some operations on indexed families and probability distributions over sets of indexed families. Formally, an indexed family a with the index set I, written as a = (a i ) i∈I is a function f a : I → T from an index set I to a target set T of possible values. However, rather than being viewed as a function, a is treated as a collection of indexed elements a i := f a (i). For example, when we write x ∈ a, that means there is an i ∈ I with x = a i . We define the following operations.</p><p>1. Projection. Let a = (a i ) i∈I be an indexed family. Let j ∈ I, and J ⊆ I. Then the projection of a to j is defined as π j (a) := a j . The projection of a to J is defined as π J (a) := (a j ) j∈J . Further, let A be a set of indexed families which are all defined over the same index set I. Then, we also define π j (A) := {π j (a) | a ∈ A}. Analogously, π J (A) := {π J (a) | a ∈ A}. (Note that when J is empty, π J (A) is a singleton with the empty family as its only element.) We also use the notation A j := π j (A) and A J := π J (A) as a shorthand.</p><p>2. Merge. Let a = (a j ) j∈J , b = (b k ) k∈K be two indexed families over disjoint index sets J and K. Then</p><formula xml:id="formula_5">a • b := (c i ) i∈J•K with c i := a i if i ∈ J and c i := b i if i ∈ K.</formula><p>Note that the merge a • b is similar to a concatenation of vectors. A concatenation of vectors can be seen as the special case of a • b when J • K is ordered and j &lt; k for all j ∈ J and k ∈ K. Unlike concatenation, merging is commutative.</p><p>3. Cartesian product. Let (A i ) i∈I be a family of sets A i over the index set I. Then its Cartesian product is</p><formula xml:id="formula_6">× i∈I A i := {(a i ) i∈I | ∀i ∈ I : a i ∈ A i },</formula><p>which is a singleton if I is empty. For a set B of indexed families over J, and a set C of indexed families over K, wherein J and K are disjoint, we write</p><formula xml:id="formula_7">B × C := {b • c | b ∈ B, c ∈ C}.</formula><p>Note that unlike the Cartesian product over vectors, the Cartesian product over families is commutative because it uses the merge rather than concatenation. Also note that the projection of a Cartesian product is again a Cartesian</p><formula xml:id="formula_8">product. That is, if A = × i∈I A i , then the projection A J equals × i∈J A i .</formula><p>Using these definitions, we now define factored spaces and factored space models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Factored spaces and factored space models</head><p>We now define two central mathematical structures of our framework -factored spaces and factored space models.</p><p>Definition 4.2 (Factored Space). A finite sample space Ω is called a factored space if there is a finite index set I and finite sets Ω i for all i ∈ I such that Ω = × i∈I Ω i holds. The sets Ω i are called the factors of Ω. Moreover, the random variables U i : Ω → Ω i with U i (ω) = π i (ω) are called the background variables of Ω, and we write U = (U i ) i∈I .</p><p>The elements of Ω are indexed families of the form ω = (ω i ) i∈I with ω i ∈ Ω i . When using factored spaces for modeling probability distributions, we consider those distributions that factorize over Ω.</p><p>Definition 4.3 (Factorizing Distribution). Let P be a probability distribution on a factored space Ω. We say that P factorizes over Ω if P (ω) = i∈I P (ω i ) holds for all ω ∈ Ω, wherein P (ω i ) := P (π -1 i (ω i )).</p><p>Note that P factorizes over Ω if and only if all background variables U i are mutually independent under P . As illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, one can think of the factors as axes, and the sample space as a hyper-rectangle along those axes. We remark that U : Ω → Ω is the identity function on Ω, so any variable X : Ω → Val(X) is a deterministic function of U . This is analogous to every variable in a structural causal model [17] being a deterministic function of the independent background variables U . Next, we formally define the notion of a factored space model for a probability distribution P on a finite set Obs of possible observations. Definition 4.4 (Factored Space Model). Let Obs be a finite set and let P be a distribution on Obs. Furthermore, let O : Ω → Obs be a random variable on a factored space Ω. Then we say that the tuple M := (Ω, O) is a factored space model for P if there is a distribution P Ω that factorizes over Ω and satisfies P Ω (O = o) = P (o) for all o ∈ Obs.</p><p>We call a variable X on Ω observed if it is a deterministic function of O, and unobserved otherwise. We remark that every probability distribution P over Obs has a trivial factored space model with a single factor (where Ω = Obs and |I| = 1), but it may have many other factored space models.</p><p>Factored space models arxiv preprint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">History</head><p>In the following, we build up to a notion of structural independence of variables on factored space models which is an analog of d-separation and which applies to arbitrary variables. To this end, we first define the history H(X | C) of a random variable X given an event C ⊆ Ω. The letter C indicates that this is the event we condition on. We would like to define the history such that it is the set of indices i of the background variables U i that X depends on if we condition on C in a distribution that factorizes over Ω. One might think that the set of background variables that X depends on should be defined as the smallest set of variables U J such that U J ▷ C X. However, this condition is not enough, since conditioning on C may make the background variables dependent. To see this, consider the following example.</p><p>Let Ω 1 := Ω 2 := {0, 1} be the outcomes of two independent coin flips, and let P be the joint distribution over the factored space Ω = Ω 1 × Ω 2 . Then, the background variables U 1 and U 2 are the results of first and second coin respectively. Since the coins are independent, P factorizes over Ω. Further, let C := {00, 11} be the event that both coins have the same result. Note that given C, we have that U 1 and U 2 are dependent in P . Thus, the set of background variables that U 1 depends on given C, is {U 1 , U 2 }, rather than just {U 1 }. When C is such that it does not introduce a dependence between U J and U I\J for J ⊆ I, we say that J disintegrates C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.5 (Disintegration).</head><p>Let Ω = × i∈I Ω i be a factored space, let C ⊆ Ω be an event and let J ⊆ I. Then J</p><formula xml:id="formula_9">disintegrates C if C = C J × C I\J .</formula><p>Note that in the previous example, {1, 2} trivially disintegrates C but {1} does not disintegrate C. We can now formally define the history.</p><p>Definition 4.6 (History, Generation). Let Ω = × i∈I Ω i be a factored space with the background variables U . Let</p><formula xml:id="formula_10">J ⊆ I, let X : Ω → Val(X), and let C ⊆ Ω. Then J generates X given C if U J ▷ C X and C = C J × C I\J . The history H(X | C) of X given C is the intersection of all J ⊆ I that generate X given C. That is, H(X | C) := J ⊆ I : J generates X given C .<label>(1)</label></formula><p>Note that, for any event C, we have C = C I × C ∅ and thus the index set I trivially disintegrates C. Moreover, for any variable X, the index set I trivially generates X because U I is the identity function on Ω and X is a function Ω → Val(X), so X is derived from U I on C in the sense of Definition 4.1. Therefore, the set on the right side of (1) contains at least the set I.</p><p>Lemma 4.7 (History is minimal generating set). For a factored space Ω = × i∈I Ω i , let X : Ω → Val(X), and let C ⊆ Ω. Then, H(X | C) is the unique minimal set which generates X given C.</p><p>Proof (sketch). We first show that generation is closed under intersection, see Lemma A.2 in the appendix. It follows that</p><formula xml:id="formula_11">H(X | C) generates X given C. As I is finite, H(X | C) is the unique minimum of {J ⊆ I | J generates X given C}.</formula><p>Therefore, H(X | C) can be equivalently defined as the minimal set that generates X given C.</p><p>Shorthand notation for the history. For an event A ⊆ Ω, we write H(A | C) as a shorthand for H(1 A | C), wherein 1 A is the variable that is 1 for ω ∈ A, and 0 otherwise. When x and y are values of the random variables X and Y , we use the notation</p><formula xml:id="formula_12">H(x | C), H(X | y), H(A | y) or H(x | y).</formula><p>Here, x is a shorthand for the event {ω ∈ Ω | X(ω) = x} which is a common abbreviation when denoting probabilities, such as P (x | y). We write H(X), H(A) and H(x) as a shorthand for the unconditional history</p><formula xml:id="formula_13">H(X | Ω), H(A | Ω) and H(x | Ω) respectively.</formula><p>We remark that the disintegration condition is vacuous for the unconditional history, as</p><formula xml:id="formula_14">Ω always satisfies Ω = Ω J × Ω I\J .</formula><p>History of joint variables. The following lemma formalizes the fact that if a variable X : Ω → Val(X) depends on exactly the positions J 1 ⊆ I in Ω and a variable Y : Ω → Val(Y ) depends on exactly the positions J 2 ⊆ I, then the joint variable (X, Y ) depends on exactly the positions J 1 ∪ J 2 .</p><p>Lemma 4.8 (History of joint variable). Let X and Y be variables defined on a factored space Ω = × i∈I Ω i . For all events C ⊆ Ω, we have</p><formula xml:id="formula_15">H((X, Y ) | C) = H(X | C) ∪ H(Y | C) .</formula><p>The proof is a straightforward application of Lemma 4.7 and can be found in Appendix A.2. Next, we introduce a lemma which establishes the relationship between the history of a variable X and the histories of its values x ∈ Val(X).</p><p>Lemma 4.9 (History of a variable is the union of the histories of events). Let X be a random variable defined on a factored space Ω. For all events C ⊆ Ω, we have</p><formula xml:id="formula_16">H(X | C) = x∈Val(X) H(x | C) .</formula><p>The proof is an inductive application of Lemma 4.8 and can be found in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Structural independence</head><p>In the following, we define two variables X : Ω → Val(X) and Y : Ω → Val(Y ) to be structurally independent if they depend on disjoint factors of the factored space Ω = × i∈I Ω i . We use our notion of history to formalize this definition and extend it to the situation of conditioning on a third variable. This is our analog of d-separation in Bayes nets. Definition 4.10 (Structural Independence). Let X, Y , and Z be random variables in a factored space Ω. Then, X and Y are structurally independent in Ω, denoted as</p><formula xml:id="formula_17">X ⊥ Ω Y , if H(X) ∩ H(Y ) = ∅. Moreover, X and Y are structurally independent given Z, denoted as X ⊥ Ω Y | Z, if we have H(X | z) ∩ H(Y | z) = ∅ for all z ∈ Val(Z) .</formula><p>We choose the name structural independence because it represents those statistical independences which come from a structure in the process that generated the distribution. For example, when throwing two fair coins X 1 and X 2 , then X 1 is statistically independent of the XOR variable</p><formula xml:id="formula_18">X ⊕ = X 1 ⊕ X 2 , because P (x 1 , x ⊕ ) = 0.25 = P (x 1 )P (x ⊕ ) holds for all x 1 , x ⊕ ∈ {0, 1}.</formula><p>However, this independence is not structural because it relies on the exact parameters of the process. If one coin is slightly unfair and P (X 1 = 0) = 51%, then X 1 and X ⊕ are not independent anymore. In contrast, X 1 and X 2 are still independent, no matter the distribution of X 1 and X 2 . In that sense, the independence of X 1 and X 2 is a property of the process which generates a class of distributions rather than a property of a particular distribution. The formal connection between statistical independence and structural independence is established in Section 6, where we prove that X and Y are independent given Z in all product probability distributions over Ω if and only if X and Y are structurally independent given Z in Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Structural time</head><p>Let X and Y be two random variables. In the following, we formalize a notion of X being before Y in the sense that X is always determined before Y in the process that generated X and Y . Definition 4.11 (Structural Time). The structural time ≤ Ω of a factored space Ω compares two variables X and Y on Ω. We say that</p><formula xml:id="formula_19">X is before Y in Ω, denoted X ≤ Ω Y , if H(X) ⊆ H(Y ). We say that X is strictly before Y , denoted X &lt; Ω Y if H(X) ⊊ H(Y ).</formula><p>We can interpret the history H(X) as the set of sources of randomness that X depends on. If H(X) ⊆ H(Y ) holds, then X depends on a subset of the sources of randomness that Y depends on, which fits with the intuition that X is determined before Y . More precisely, in any process in which the values of U i become known one after the other, the value of X will always be determined before the value of Y , or at the same time.</p><p>In Section 5.2 we show that when we construct an FSM from a causal graph, then for variables that are nodes in a causal graph, the structural time is equivalent to the ancestor relation. Structural time is more general than the ancestor relation, since it is defined on all variables on Ω, not only the ones which are nodes in the graph.</p><p>In the following lemma, we show that the structural time can be equivalently expressed in terms of structural independence. We hold that by this lemma the name structural time is philosophically justified, but this topic is outside the scope of this paper. Lemma 4.12 (Structural time and structural independence). Let X and Y be random variables on Ω. Then X is structurally before Y if and only if Y ⊥ Ω Z ⇒ X ⊥ Ω Z holds for all variables Z on Ω.</p><p>The lemma follows from the definition of structural independence, and the proof can be found in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bayesian networks and factored space models</head><p>In this section, we compare factored space models (FSMs) with Bayesian Networks. In Section 5.1 we compare the properties of d-separation and structural independence in terms of the graphoid axioms they fulfill. In Section 5.2 we construct an FSM from a causal graph, and show that node variables in the graph are d-separated if and only if they are structurally independent in the corresponding FSM.</p><p>Factored space models arxiv preprint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graphoid and semigraphoid axioms</head><p>Structural independence is our analog of d-separation. In this section, we compare the properties of d-separation, which forms a compositional graphoid and structural independence, which forms a compositional semigraphoid. This comparison also provides an intuition as to why structural independence represents deterministic relationships correctly while d-separation does not.</p><p>Definition 5.1 (Compositional Semigraphoid). A semigraphoid is a set of triplets (X, Y, Z), usually denoted as X ⊥ Y | Z, that satisfy the symmetry, decomposition, weak union, and contraction axioms, as listed in Table <ref type="table">1</ref>. A graphoid <ref type="bibr" target="#b17">[19]</ref> is a semigraphoid that also satisfies the intersection axiom. A compositional graphoid or semigraphoid additionally satisfies the composition axiom. (FSM)</p><p>1. Symmetry:</p><formula xml:id="formula_20">X ⊥ Y | W ⇒ Y ⊥ X | W ✓ ✓ ✓ 2. Decomposition: X ⊥ Y, Z | W ⇒ X ⊥ Y | W ✓ ✓ ✓ 3. Weak Union: X ⊥ Y, Z | W ⇒ X ⊥ Z | Y, W ✓ ✓ ✓ 4. Contraction: (X ⊥ Y | W ) ∧ (X ⊥ Z | Y, W ) ⇒ X ⊥ Y, Z | W ✓ ✓ ✓ 5. Intersection: (X ⊥ Y | Z, W ) ∧ (X ⊥ Z | Y, W ) ∧ (Y ̸ = Z) ⇒ X ⊥ Y, Z | W ✗ ✓ ✗ 6. Composition: (X ⊥ Y | W ) ∧ (X ⊥ Z | W ) ⇒ X ⊥ Y, Z | W ✗ ✓ ✓</formula><p>green: same as independence red: different from independence Table <ref type="table">1</ref>: Comparison of independence in a probability distribution, d-separation in a directed acyclic graph (DAG), and structural independence on an FSM with regard to the graphoid and composition axioms. Independence satisfies 1.-4., which means it forms a semigraphoid <ref type="bibr" target="#b16">[18]</ref>, d-separation satisfies 1.-6., which means it forms a compositional graphoid. Structural independence satisfies 1.-4. and 6., which means it forms a compositional semigraphoid. Note that in terms of these axioms, structural independence is closer to independence than d-separation, as d-separation differs from independence for both the intersection and the composition axiom, while for structural independence the only difference is the composition axiom. Proposition 5.2. Structural independence is a compositional semigraphoid.</p><p>The proof is straightforward and we defer it to Appendix B.1.</p><formula xml:id="formula_21">Composition. The composition axiom (X ⊥ Y | W ) ∧ (X ⊥ Z | W ) ⇒ (X ⊥ Y, Z | W )</formula><p>illustrates that structural independence is not the same as independence: Structural independence satisfies this axiom intuitively, because if X and Y are influenced by disjoint sources of randomness, and X and Z are influenced by disjoint sources of randomness, then X and Y, Z are influenced by disjoint sources of randomness as well. To see that statistical independence does not satisfy the composition axiom, consider the following example: Let X be a message, Y be a string of uniformly distributed bits, and let Z be their bitwise XOR: Z = X ⊕ Y . Then X and Y are independent, and X and Z are independent, but X and (Y, Z) are dependent, so composition does not hold. This situation could be modeled with an FSM as follows. The variables X and Y are structurally independent, but as Z is computed from X, the histories of X and Z overlap and X and Z are structurally dependent, so composition is not violated. Independence is about the question "Does X provide information about Y ?" while structural independence is about the question "Are X and Y influenced by the same source of randomness?".</p><formula xml:id="formula_22">Intersection. The intersection axiom (X ⊥ Y | Z, W ) ∧ (X ⊥ Z | Y, W ) ∧ (Y ̸ = Z) ⇒ (X ⊥ Y, Z | W ) is particularly interesting. Statistical independence</formula><p>does not in general satisfy the intersection axiom, though intersection does hold in those cases where P (x, y, z, w) is never zero <ref type="bibr" target="#b16">[18]</ref>. However, we want to allow P (x, y, z, w) = 0. In particular, if any variable is a deterministic function of another variable, there must be values x, y, z, w with P (x, y, z, w) = 0. The fact that d-separation satisfies intersection means that d-separation cannot represent the independence relations among a set of variables, some of which are deterministically related. It is therefore an important property of structural independence that the intersection axiom does not hold.</p><p>In the next section, we continue our comparison of d-separation and structural independence by constructing an FSM from a causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">From Bayesian networks to factored space models</head><p>In this section, we construct a factored space from a directed acyclic graph (DAG), and we construct an FSM from a Bayesian network. We show that a distribution P factorizing over a DAG is equivalent to our construction being a factored space model of P . We also show that in our construction, structural independence applied to node variables is equivalent to d-separation, and structural time applied to node variables is equivalent to the ancestor relationship. We also introduce the notion of a perfect map, and use it to show that factored space models are more expressive than Bayesian networks. We start by formally introducing Bayesian networks.</p><p>Bayesian networks. Let G = (V, E, Val) be a directed acyclic graph (DAG), wherein V is the vertex set, and E is the edge set. Moreover, each vertex v ∈ V is associated with a set of possible values Val v with at least 2 elements, and Val = × v∈V Val v is the set of all combinations of values for V .</p><p>For a vertex v, we write pa(v) ⊆ V for the set of parents of v, and an(v) for the set of ancestors of v. If P is a distribution over Val and x v ∈ Val v is a value of v, we write P (x v ) as a shorthand for the probability P ({(y u ) u∈V ∈ Val V : x v = y v }). We say that P factorizes over G, if for all x = (x v ) v∈V ∈ Val, we have</p><formula xml:id="formula_23">P (x) = v∈V P (x v | x pa(v) ) .</formula><p>(</p><formula xml:id="formula_24">)<label>2</label></formula><p>Following the notation in <ref type="bibr" target="#b14">[15]</ref>, a pair B = (G, P ) where P factorizes over G is called Bayesian network or Bayes net.</p><p>In the following, we show how to construct a factored space Ω and a factored space model M G from G. Factors in this construction are closely related to factor graphs <ref type="bibr" target="#b24">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Our construction of a factored space model from a Bayesian network</head><p>Given the DAG G = (V, E, Val), we now construct a factored space model M G = (Ω, O) for the observation space Obs = Val. We first construct a factored space Ω. To this end, we define the index set I via I := v∈V I v , where for all v ∈ V , we let</p><formula xml:id="formula_25">I v := {(v, x pa(v) ) : x pa(v) ∈ Val pa(v) } .<label>(3)</label></formula><p>In other words, the index set I is partitioned into the sets I v , and each element of I v corresponds to a possible value for the vertices in pa(v). If v is a root, we remark that pa(v) = ∅ holds and that Val pa(v) contains the empty tuple () as its only element. Finally, for each i = (v, x pa(v) ) ∈ I, we define the set Ω i via Ω i := Val v . Then Ω := × i∈I Ω i is the factored space constructed from G.</p><p>We now define the random variable O : Ω → Val. To this end, we first define X = (X v ) v∈V , wherein X v : Ω → Val v is a random variable associated with each node v. We define X v recursively as</p><formula xml:id="formula_26">X v (ω) := ω (v,X pa(v) (ω)) .<label>(4)</label></formula><p>In particular, for roots v we have</p><formula xml:id="formula_27">X v (ω) = ω (v,()) ∈ Val v . Since G is acyclic, X v is well-defined also for non-roots v. Note that Val(X) = Val and Val(X v ) = Val v . We call M G = (Ω, O) with O = X the factored space model constructed from G.</formula><p>This concludes the construction.</p><p>To show that M G is in fact a factored space model of a distribution P over Val in the sense of Definition 4.4 when P factorizes over G, we first state the following lemma about the relationship between G and M G .</p><p>Lemma 5.3. Let ∆ * (G) be the set of distributions on Val that factorize over G and let △ ⊗ (Ω) be the set of distributions on Ω that factorize over Ω. Let τ : △ ⊗ (Ω) → ∆ * (G) be the function defined as follows for all P Ω ∈ △ ⊗ (Ω) and x ∈ Val:</p><formula xml:id="formula_28">τ (P Ω )(x) = P Ω (X = x) .</formula><p>Then τ is bijective and its inverse τ -1 satisfies the following for all P ∈ ∆ * (G) and ω ∈ Ω:</p><formula xml:id="formula_29">τ -1 (P )(ω) = (v,x pa(v) )∈I P (ω v,x pa(v) | x pa(v) ) .</formula><p>The proof can be found in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Properties of our construction</head><p>In this section, we fix a DAG G = (V, E, Val), the corresponding factored space model M G = (Ω, O) constructed from G, and the node variables X v for v ∈ V . We show that our construction preserves factorization, d-separation, and the ancestor relation. We start with the factorization property.</p><p>Proposition 5.4 (Factorization property). For all distribution P over Val, we have that P factorizes over G if and only if M G is a factored space model of P .</p><p>Proof. For the forward implication, let P factorize over G, that is P ∈ ∆ * (G). Then, with ι defined as in Lemma 5.3, let P Ω := ι(P ). By Lemma 5.3, τ is the inverse of ι. Therefore, for all x ∈ Val, we have that P (x) = τ (P Ω )(x) = P Ω (X -1 (x)) = P Ω (X = x). Since P Ω also factorizes over Ω by the definition of ι, M G is an FSM of P in the sense of Definition 4.4.</p><p>For the backward implication, let M G be an FSM of P . That is, there is a distribution P Ω that factorizes over Ω such that P Ω (X = x) = P (x). That means P = τ (P Ω ). By Lemma 5.3, P Ω = ι(P ), and therefore P factorizes over G by the definition of ι.</p><p>Next, we show how d-separation in G is preserved as structural independence in M G .</p><formula xml:id="formula_30">Proposition 5.5 (d-separation). Let V 1 , V 2 , V 3 ⊆ V be three sets of nodes in G. Then V 1 and V 2 are d-separated given V 3 in G if and only if X V1 and X V2 are structurally independent given X V3 in M G .</formula><p>We defer the straightforward proof to Appendix B.3.</p><p>Finally, we show how the ancestor relation in G is preserved as the structural time relation between the node variables.</p><formula xml:id="formula_31">Proposition 5.6 (Ancestor relation). Let v 1 , v 2 ∈ V be two distinct nodes in the graph G. Then v 1 is an ancestor of v 2 in G if and only if X v1 &lt; Ω X v2 holds. Proof. The history of X v is H(X v ) = u∈an(v)∪{v} I u . Since I v is nonempty, we have that v 1 is an ancestor of v 2 ⇐⇒ an(v 1 ) ∪ {v 1 } ⊊ an(v 2 ) ∪ {v 2 } ⇐⇒ v∈an(v1)∪{v1} I v ⊊ v∈an(v2)∪{v2} I v ⇐⇒ H(X v1 ) ⊊ H(X v2</formula><p>), which is exactly the definition of X v1 &lt; Ω X v2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Factored space models are more expressive than Bayesian networks</head><p>A DAG whose d-separations capture the independences of a distribution is called a perfect map <ref type="bibr" target="#b14">[15]</ref>. We now define our analog. That is, an FSM whose structural independences capture the independences of a distribution. With this definition, we can show that FSMs are more expressive than DAGs, in that there are distributions P such that there is no perfect map DAG of P while there is a perfect map FSM of P .</p><p>Definition 5.7 (Perfect map). Let G be a DAG with nodes V and value space Val(X). Further, let M = (Ω, O) be a factored space model. Let P be a distribution over some observation space Obs. Then we say that</p><formula xml:id="formula_32">1. G is a perfect map of P if Val(X) = Obs and for all sets of nodes V 1 , V 2 , V 3 ⊆ V , the sets V 1 and V 2 are d-separated given V 3 in G if and only if X V1 and X V2 are independent given X V3 in P .</formula><p>2. M is a perfect map of P with regard to a family of variables X = (X w ) w∈W with X w : Ω → Obs if M is a factored space model of P and for all sets of variables X W1 , X W2 , X W3 ⊆ X, we have that X W1 and X W2 are independent given X W3 in P if and only if X W1 and X W2 are structurally independent given X W3 in Ω.</p><p>Perfect maps are often considered to be the graphs that correspond to a distribution. The following proposition shows that factored spaces are more expressive than DAGs.</p><p>Proposition 5.8 (Perfect maps of graphs and factored spaces). Let P be a distribution over some observation space</p><p>Obs of the form Obs = × v∈V Val(X v ). Then, it holds that 1. If there is a DAG G with nodes V that is a perfect map of P , then there is a factored space model M = (Ω, X), with X = (X v ) v∈V that is also a perfect map of P with regard to X.</p><p>2. If there is a factored space model M that is a perfect map of P with regard to (X v ) v∈V , there may still be no DAG G with nodes V that is a perfect map of P .</p><p>We defer the proof to Appendix B.4. Proposition 5.8 illustrates that factored space models are more expressive than Bayesian networks. We can construct a factored space model M G for a DAG G, which is a perfect map for the same distributions as G. However, it is not possible in general to construct a DAG G from a factored space model M in such a way that G is a perfect map for the same distributions as M.</p><p>In the following section, we show that the soundness and completeness theorem of d-separation <ref type="bibr" target="#b14">[15]</ref> generalizes to this additional expressiveness. That is, we show the soundness and completeness of structural independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Soundness and Completeness of Structural Independence</head><p>This section contains our main result: random variables are conditionally independent in all probability distributions that factorize over a factored space if and only if they are structurally independent. To state the theorem formally, we first review the definition of conditional independence.</p><p>Definition 6.1. Let P be a probability distribution over a sample space Ω, and let A, B, C ⊆ Ω be events. We say that A and B are independent given C, denoted A ⊥ ⊥ P B | C, if the following holds:</p><formula xml:id="formula_33">P (A ∩ C)P (B ∩ C) = P (A ∩ B ∩ C)P (C) .<label>(5)</label></formula><p>If x, y, z are values of the random variables X, Y, Z, then we write x ⊥ ⊥ P y | z if P (x, z)P (y, z) = P (x, y, z)P (z) holds. For random variables X, Y, Z, we write X ⊥ ⊥ P Y | Z if x ⊥ ⊥ P y | z holds for all values x, y, z of X, Y, Z.</p><p>Equation ( <ref type="formula" target="#formula_33">5</ref>) is equivalent to the more common definition</p><formula xml:id="formula_34">P (A | C)P (B | C) = P (A ∩ B | C) if P (C) &gt; 0. If P (C) = 0, our convention is that A ⊥ ⊥ P B | C</formula><p>holds and indeed Equation ( <ref type="formula" target="#formula_33">5</ref>) is trivially true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main theorem</head><p>For random variables X, Y, Z over a factored space Ω, how does probabilistic independence X ⊥ ⊥ P Y | Z (Definition 6.1) relate to structural independence X ⊥ Ω Y | Z (Definition 4.10)? We answer this question as follows: Structural independence implies probabilistic independence for all distributions P that factorize over the factored space (soundness). Conversely, probabilistic independence in all distributions P that factorize over the factored space implies structural independence (completeness).</p><p>Theorem 6.2 (Soundness and Completeness of Structural Independence). Let X, Y, Z be random variables on a factored space Ω = × i∈I Ω i . Then the following statements are equivalent.</p><p>(i) X and Y are structurally independent given Z in Ω.</p><p>(ii) X ⊥ ⊥ P Y | Z holds for all probability distributions P that factorize over Ω.</p><p>In order to prove Theorem 6.2, we first prove the soundness and completeness of structural independence for events in Ω, instead of random variables. It is then easy to lift these results to random variables. For brevity, in what follows we write △ ⊗ (Ω) for the set of all distributions that factorize over Ω. The soundness direction ((i) =⇒ (ii)) of Theorem 6.2 for events is given by the following lemma.</p><p>Lemma 6.3 (Soundness for Events). Let A, B, C ⊆ Ω be events in a factored space Ω = × i∈I Ω i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If the histories satisfy</head><formula xml:id="formula_35">H(A | C) ∩ H(B | C) = ∅, then A ⊥ ⊥ P B | C holds for all P ∈ △ ⊗ (Ω).</formula><p>The proof is a straightforward transformation of definitions and can be found in Appendix C.1. The completeness direction ((ii) =⇒ (i)) of Theorem 6.2 for events is given by the following lemma.</p><p>Lemma 6.4 (Completeness for Events). Let A, B, C ⊆ Ω be events in a factored space Ω = × i∈I Ω i .</p><p>If A ⊥ ⊥ P B | C holds for all P ∈ △ ⊗ (Ω), then the histories satisfy</p><formula xml:id="formula_36">H(A | C) ∩ H(B | C) = ∅.</formula><p>The proof can be found in Appendix C.3 and is the most technical contribution of this paper. In a sense that we make formally precise, we prove that the history contains exactly those indices of I that are probabilistically relevant for A given C. We show that, if A ⊥ ⊥ P B | C holds for all distributions P ∈ △ ⊗ (Ω), then no index i ∈ I can be relevant to both A given C and to B given C. Since the history contains exactly the relevant indices, this implies H(A | C) ∩ H(B | C) = ∅, which establishes Lemma 6.4. Using Lemmas 6.3 and 6.4, the proof of Theorem 6.2 is straightforward.</p><p>Factored space models arxiv preprint Proof of Theorem 6.2. For all random variables X, Y, Z on Ω, we have:</p><formula xml:id="formula_37">X ⊥ Ω Y | Z ⇐⇒ ∀z ∈ Val(Z) : H(X | z) ∩ H(Y | z) = ∅ (Definition 4.10) ⇐⇒ ∀z ∈ Val(Z) : x∈Val(X) H(x | z) ∩ y∈Val(Y ) H(y | z) = ∅ (Lemma 4.9) ⇐⇒ ∀(x, y, z) ∈ Val(X) × Val(Y ) × Val(Z) : H(x | z) ∩ H(y | z) = ∅ ⇐⇒ ∀P ∈ △ ⊗ (Ω) : ∀(x, y, z) ∈ Val(X) × Val(Y ) × Val(Z) :</formula><p>x ⊥ ⊥ P y | z (Lemmas 6.3 and 6.4)</p><formula xml:id="formula_38">⇐⇒ ∀P ∈ △ ⊗ (Ω) : X ⊥ ⊥ P Y | Z . (Definition 6.1)</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Extension: Strong completeness</head><p>Our main theorem, Theorem 6.2, states that the structural independences X ⊥ Ω Y | Z in a factored space Ω are exactly those triples (X, Y, Z) which satisfy the conditional independences X ⊥ ⊥ P Y | Z for all distributions P ∈ △ ⊗ (Ω).</p><p>We now strengthen this result by showing that it is sufficient to exhibit the conditional independences X ⊥ ⊥ P Y | Z "locally" for all distributions P from any non-empty open subset S ⊆ △ ⊗ (Ω). Here, we view the set △ ⊗ (Ω) as a subset of R Ω with the Euclidean topology.</p><p>To strengthen the completeness result, we first show that if a conditional independence holds locally around some distribution Q, then it holds globally on △ ⊗ (Ω). We write d : △ ⊗ (Ω) 2 → R for the Euclidean distance on △ ⊗ (Ω) ⊆ R Ω . Lemma 6.5 (From local to global conditional independence). Let X, Y, Z be random variables in a factored space</p><formula xml:id="formula_39">Ω = × i∈I Ω i . Let Q ∈ △ ⊗ (Ω) be a distribution. If X ⊥ ⊥ Q ′ Y | Z holds for all Q ′ with d(Q, Q ′ ) &lt; ε, then X ⊥ ⊥ P Y | Z holds for all P ∈ △ ⊗ (Ω).</formula><p>The proof is based on the fact that the independence statement has the form of a polynomial, and any polynomial which is zero in an ε-environment must be the zero polynomial. It can be found in Appendix C.2.</p><p>The following proposition strengthens completeness: It is a priori not necessary that the probabilistic independence holds for all distributions that factorize over Ω. Rather, if a probabilistic independence holds for any non-empty open set of distributions that factorize over Ω, then this already implies structural independence. Proposition 6.6 (Strong Completeness). Let X, Y, Z be random variables in a factored space Ω. If there exists a nonempty open set S ⊆ △ ⊗ (Ω) with X ⊥ ⊥ P Y | Z for all P ∈ S, then X ⊥ Ω Y | Z.</p><p>Proof. The proposition follows directly from Theorem 6.2 and Lemma 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and conclusion</head><p>We have introduced the factored space model framework, and defined structural independence, which models statistical independence between variables. Structural independence improves upon d-separation in that it is applicable to variables with deterministic as well as probabilistic relationships. This improvement is useful for modeling systems at different levels of abstraction, which introduces deterministic relationships. We think that having an independence criterion that is applicable between levels of abstraction, rather than only within one level of abstraction, is a very natural representation. More speculatively, we believe that it may help us understand self-referencing systems because there may not even be discrete levels of abstraction. For example, some language models can already be described as thinking about their own thinking process <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b5">[6]</ref>, which in some sense means that high-level summaries of thoughts are causally influencing object-level thoughts.</p><p>Further, we think that a theory of causality that applies across layers of abstraction may be a useful tool to understand agents using the intentional stance, that is as systems that are best described in terms of their goals <ref type="bibr" target="#b4">[5]</ref>, or as systems whose actions are caused by their model of the consequences <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions.</head><p>The contributions of the authors include, but are not limited to, the following:</p><p>• S. Garrabrant originally conceived of and developed the FSM framework, and found the first proof of the soundness and completeness theorem, as presented in <ref type="bibr" target="#b7">[8]</ref>. • M. G. Mayer reframed the ideas from <ref type="bibr" target="#b7">[8]</ref> to the formalism presented in this paper and developed the more elegant proof of the soundness and completeness theorem presented in Appendix C. • M. Wache clarified the relationship between Bayes Nets and FSM, and wrote most of this paper, including developing the overall structure, making connections to the existing literature, developing examples, creating figures, and formalizing proofs. • L. Lang developed the exposition of the proof for the soundness and completeness theorem. • S. Eisenstat developed precursor ideas and examples that grew into the theory of FSMs, and collaborated with Scott Garrabrant to work out the ideas further. • H. Dell provided guidance and regular feedback and edited the paper in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs for Section 4 A.1 History as the minimal generating partition</head><p>In this section, we prove that the history H(X | C) is the unique minimal partition that generates X given C. We start by proving that disintegration is closed under intersection and union. For a subset J ⊆ I, we use J as a shorthand for I \ J.</p><p>Lemma A.1 (Disintegration closed under intersection and union). In a factored space Ω = × i∈I Ω i , let C ⊆ Ω and J, K ⊆ I such that J disintegrates C and K also disintegrates C. Then, J ∩ K disintegrates C, and J ∪ K disintegrates C as well.</p><p>Proof. Let J, K ⊆ I be such that J disintegrates C and K disintegrates C. That is, we have</p><formula xml:id="formula_40">C = C J × C J and C = C K × C K . We claim that C = C J∩K × C J∩K and C = C J∪K × C J∪K hold as well.</formula><p>We first make the following observation:</p><formula xml:id="formula_41">( * ) For all L ⊆ I, we have C ⊆ C L × C L .</formula><p>To see this, let c ∈ C, then we have c = c L • c L , and both c L ∈ C L and c L ∈ C L hold as required. Then ( * ) implies C ⊆ C J∩K × C J∩K and C ⊆ C J∪K × C J∪K , so it remains to prove the reverse inclusions.</p><p>To this end, we let Lemma A.2 (Generation closed under intersection). In a factored space Ω = × i∈I Ω i , let X : Ω → Val(X), let C ⊆ Ω, and let J, K ⊆ I be such that J generates X given C and K generates X given C. Then, J ∩ K generates X given C as well.</p><formula xml:id="formula_42">C × := C J∩K × C J∩K × C J∩K × C J∩K and claim C × ⊆ C. Let c ∈ C × . By the definition of C × , there exist c 1 , c 2 , c 3 , c 4 ∈ C such that the following holds: c = c 1 J∩K • c 2 J∩K • c 3 J∩K • c 4 J∩K = (c 1 J ) J∩K • (c 2 J ) J∩K • (c 3 J ) J∩K • (c 4 J ) J∩K = (c 1 J • c 2 J ) (J∩K)∪(J∩K) • (c 3 J • c 4 J ) (J∩K)∪(J∩K) = (c 1 J • c 2 J ) K • (c 3 J • c 4 J ) K . Since J disintegrates C and c 1 , c 2 ∈ C, we have c 1 J • c 2 J ∈ C. Analogously, c 3 J • c 4 J ∈ C.</formula><p>Proof. Let J, K ⊆ I be such that J generates X given C and K generates X given C. In particular, J and K both disintegrate C. By Lemma A.1, it follows that J ∩ K also disintegrates C. As both J and K generate X given C, we have</p><formula xml:id="formula_43">U J ▷ C X and U K ▷ C X. We claim that U J∩K ▷ X. Let ω, ω ′ ∈ C s.t. ω J∩K = ω ′ J∩K . Since J generates X given C, we have ω 1 = ω J • ω ′ J ∈ C and X(ω) = X(ω 1 ). Since J ∪ K disintegrates C, we have ω 2 = ω J∩K • ω ′ I\(J∩K) ∈ C.</formula><p>Because ω 1 and ω 2 agree on J, we have X(ω 1 ) = X(ω 2 ). Now, since K generates X given C, we have</p><formula xml:id="formula_44">ω 2 = ω J∩K • ω ′ I\(J∩K) ∈ C and X(ω 1 ) = X(ω 2 ). Since ω J∩K = ω ′ J∩K , we have ω 2 = ω and therefore X(ω) = X(ω ′ )</formula><p>Using Lemma A.2, we now prove Lemma 4.7, which we restate here for convenience. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 History of joint variable</head><p>We prove Lemma 4.8, which we restate for convenience.</p><p>Lemma 4.8 (History of joint variable). Let X and Y be variables defined on a factored space Ω = × i∈I Ω i . For all events C ⊆ Ω, we have</p><formula xml:id="formula_45">H((X, Y ) | C) = H(X | C) ∪ H(Y | C) .</formula><p>Proof of Lemma 4.8. We first prove H((X, </p><formula xml:id="formula_46">Y ) | C) ⊆ H(X | C) ∪ H(Y | C).</formula><formula xml:id="formula_47">((X, Y ) | C) ⊆ H(X | C) ∪ H(Y | C).</formula><p>Now we prove the reverse inclusion, that is, </p><formula xml:id="formula_48">H(X | C) ∪ H(Y | C) ⊆ H((X, Y ) | C).</formula><formula xml:id="formula_49">(X | C) ⊆ H((X, Y ) | C) and H(Y | C) ⊆ H((X, Y ) | C), and therefore also H(X | C) ∪ H(Y | C) ⊆ H((X, Y ) | C).</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 History as union of histories of events</head><p>Here we prove Lemma 4.9, which we restate for convenience.</p><p>Lemma 4.9 (History of a variable is the union of the histories of events). Let X be a random variable defined on a factored space Ω. For all events C ⊆ Ω, we have</p><formula xml:id="formula_50">H(X | C) = x∈Val(X) H(x | C) .</formula><p>Proof. We first recall that H(x | C) is a shorthand for H(1 x | C), where 1 x is the indicator random variable defined via 1 x (ω) = 1 if X(ω) = x and 1 x (ω) = 0 otherwise. We observe that each 1 x is a deterministic function of X and that X is a deterministic function of the joint variable (1 x ) x∈Val(X) . In particular, we have</p><formula xml:id="formula_51">X ▷ C (1 x ) x∈Val(X) ▷ C X. By transitivity of ▷ C , this implies H(X | C) = H((1 x ) x∈Val(X) | C). Since Val(X) is finite, we can inductively apply Lemma 4.8 to arrive at H((1 x ) x∈Val(X) | C) = x∈Val(X) H(1 x | C) = x∈Val(X) H(x | C).</formula><p>Thus, the claim follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Structural time is inclusion</head><p>Here we prove Lemma 4.12, which we restate for convenience.</p><p>Lemma 4.12 (Structural time and structural independence). Let X and Y be random variables on Ω. Then X is structurally before Y if and only if Y ⊥ Ω Z ⇒ X ⊥ Ω Z holds for all variables Z on Ω.</p><p>Proof. Let X and Y be random variables in a factored space Ω. We prove the two directions of the equivalence separately.</p><p>"⇒": Suppose that X is structurally before Y . By Definition 4.11, this means H(X) ⊆ H(Y ). We claim that Y ⊥ Ω Z ⇒ X ⊥ Ω Z holds for all variables Z on Ω. To this end, let Z be any variable on Ω and suppose Y ⊥ Ω Z holds. By Definition 4.10, we have</p><formula xml:id="formula_52">H(Y ) ∩ H(Z) = ∅. By H(X) ⊆ H(Y ), this implies H(X) ∩ H(Z) = ∅ and thus X ⊥ Ω Z as claimed.</formula><p>"⇐": Suppose that Y ⊥ Ω Z ⇒ X ⊥ Ω Z holds for all variables Z on Ω. We claim that H(X) ⊆ H(Y ) holds. To this end, let i ∈ H(X) be arbitrary. We choose Z = U i , which implies</p><formula xml:id="formula_53">H(Z) = {i}. Since H(X) ∩ H(Z) = {i} ̸ = ∅, we have X ̸ ⊥ Ω Z. By applying the contrapositive X ̸ ⊥ Ω Z ̸ ⇒ Y ̸ ⊥ Ω Z of the assumption, this implies Y ̸ ⊥ Ω Z,</formula><p>which implies H(Y ) ∩ H(Z) ̸ = ∅ and thus i ∈ H(Y ). Since this holds for all i ∈ H(X), we have H(X) ⊆ H(Y ) as claimed, and so X is structurally before Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arxiv preprint</head><p>Proof. First we must establish that τ is a function from △ ⊗ (Ω) → ∆ * (G). Let P Ω ∈ △ ⊗ (Ω) and P := τ (P Ω ), we claim that P factorizes over G. Indeed, suppose for convenience that 1, 2, . . . , n ∈ V is a topological ordering of the vertices of G. Then for all x ∈ Val, we have the following:</p><formula xml:id="formula_54">P (x) = P Ω (X = x) = P Ω ( n v=1 X v = x v ) = n v=1 P Ω (X v = x v | X 1 = x 1 ∧ • • • ∧ X v-1 = x v-1 ) .</formula><p>Here, the last equality follows from the definition of conditional probability. Recall from (4) that X is defined via</p><formula xml:id="formula_55">X v (ω) = ω (v,X pa(v) (ω))</formula><p>. Since 1, . . . , n is a topological ordering of V , by conditioning on</p><formula xml:id="formula_56">X 1 = x 1 ∧ • • • ∧ X v-1 = x v-1 , we have X v (ω) = ω (v,x pa(v)</formula><p>) . Thus, we can continue the calculation as follows:</p><formula xml:id="formula_57">= n v=1 P Ω (X v = x v | X pa(v) = x pa(v) ) = v∈V P (x v | x pa(v) ) .</formula><p>The first equality follows by inductively applying (4): X v is a deterministic function of X pa(v) and the values ω (v,x pa(v) ) alone. Therefore, by induction on u ∈ V = {1, . . . , n}, for all u ∈ V , we have that X u is a function of the entries ω (u ′ ,xpa(u ′ ) for u ′ ≤ u. Under the conditioning X pa(v) = x pa(v) , this shows that X v is conditionally independent in P Ω from all X u with u &lt; v and u ̸ ∈ pa(v). The ultimate equality follows from the definition of P . The equation shows that P factorizes over G, and thus P ∈ △ * (G).</p><p>It remains to argue that τ is bijective. To do so, it suffices to show that the given function τ -1 is a function from ∆ * (G) → △ ⊗ (Ω) and that τ -1 is indeed the inverse of τ . We first observe that the given function τ -1 is a function from ∆ * (G) → △ ⊗ (Ω), because τ -1 (P ) is by definition a product of distributions over Ω i for all i ∈ I. Finally, we must show that τ -1 is the inverse of τ , so let P Ω ∈ △ ⊗ (Ω) and P ∈ △ * (G). We claim that τ -1 (τ (P Ω )) = P Ω and τ (τ -1 (P ) = P holds. Indeed, for all ω ∈ Ω, we have the following:</p><formula xml:id="formula_58">τ -1 (τ (P Ω ))(ω) = (v,x pa(v) )∈I τ (P Ω )(ω v,x pa(v) | x pa(v) ) (by definition of τ -1 ) = (v,x pa(v) )∈I P Ω (X v = ω v,x pa(v) | X pa(v) = x pa(v) ) (by definition of τ ) = (v,x pa(v) )∈I P Ω (U v,X pa(v) (ω) = ω v,x pa(v) | X pa(v) = x pa(v) ) (by definition of X v ) = (v,x pa(v) )∈I P Ω (U v,x pa(v) (ω) = ω v,x pa(v) ) (by independence) = (v,x pa(v) )∈I P Ω (ω v,x pa(v) ) = P Ω (ω) . (since P Ω factorizes) Further, we have τ (τ -1 (P ))(x) = τ -1 (P )(X = x) (by definition of τ ) = v∈V τ -1 (P )(U (v,x pa(v) ) = x v ) (by Lemma B.2) = v∈V τ -1 (P )(U (v,x pa(v) ) = x v | X pa(v) = x pa(v) ) (by independence) = v∈V τ -1 (P )(X v = x v | X pa(v) = x pa(v) ) (by definition) = v∈V P (x v | x pa(v) ) (by definition of τ -1 ) = P (x) (since P factorizes)</formula><p>Thus, τ is a bijective function with inverse τ -1 . This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Equivalence of d-separation and structural independence for nodes</head><p>Here we prove Proposition 5.5, which states that d-separation for nodes in a graph G holds if and only if the corresponding variables are structurally independent in the FSM M G constructed from G. This proof is an application of Theorem 6.2, but it is also possible to prove Proposition 5.5 directly without Theorem 6.2.</p><p>Factored space models arxiv preprint</p><formula xml:id="formula_59">Proposition 5.5 (d-separation). Let V 1 , V 2 , V 3 ⊆ V be three sets of nodes in G. Then V 1 and V 2 are d-separated given V 3 in G if and only if X V1 and X V2 are structurally independent given X V3 in M G .</formula><p>Proof. By Lemma 5.3, for any distribution P which factorizes over G, there is a P Ω over Ω with P (x) = P Ω (X = x). Lemma 5.3 also implies that for any distribution P Ω over Ω that factorizes over M G there is a distribution P which factorizes over G with P (x) = P Ω (X = x). Therefore, with Definition 6.1, it follows that for all values</p><formula xml:id="formula_60">x V1 , x V2 , x V3 ∈ Val V1 × Val V1 × Val V3</formula><p>x V1 ⊥ ⊥ P x V2 | x V3 for all distributions P that factorize over G ⇐⇒</p><p>x V1 ⊥ ⊥ P Ω x V2 | x V3 for all distributions P Ω that factorize over Ω.</p><p>We have that</p><formula xml:id="formula_62">V 1 and V 2 are d-separated given V 3 in G. (i)</formula><p>⇐⇒ X V1 ⊥ ⊥ P X V2 | X V3 for all distributions P that factorize over G.</p><p>(ii)</p><formula xml:id="formula_63">⇐⇒ X V1 ⊥ ⊥ P Ω X V2 | X V3 for all distributions P Ω that factorize over M G . (iii) ⇐⇒ X V1 ⊥ Ω X V2 | X V3 .</formula><p>Wherein (i) is true by the soundness and completeness of d-separation <ref type="bibr" target="#b14">[15]</ref>, (ii) follows from <ref type="bibr" target="#b5">(6)</ref>, and (iii) is true by Theorem 6.2. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Perfect maps of DAGs and factored spaces</head><p>Here we prove Proposition 5.8, which states that if there is a DAG that is a perfect map of a distribution P , then there is a factored space model that is also a perfect map of P , but not vice versa.</p><p>Proof of Proposition 5.8. To prove 1., let G be a DAG that is a perfect map of P . We want to prove that M G = (Ω, X) is a perfect map of P with regard to X. Let X V1 , X V2 , X V3 ⊆ X be arbitrary. Then, by Proposition 5.5, X V1 and X V2 are structurally independent given X V3 in Ω if and only if they are d-separated in G. Since G is a perfect map, this is equivalent to X V1 and X V2 being independent given X V3 .</p><p>To prove 2., consider the following counterexample. Let Ω = Ω 1 = {0, 1, 2}, let P be an arbitrary distribution over Ω, and let X = (X 1 , X 2 ) with X 1 = U 1 , and</p><formula xml:id="formula_64">X 2 = [U 1 &gt; 0]</formula><p>. Since X 2 is a deterministic function of X 1 , we have the following independence and dependence relations:</p><formula xml:id="formula_65">nontrivial trivial independencies trivial dependencies X 2 ⊥ ⊥ X 2 | X 1 X 1 ⊥ ⊥ X 1 | X 1 X 1 ̸ ⊥ ⊥ X 1 X 1 ̸ ⊥ ⊥ X 1 | X 2 X 2 ⊥ ⊥ X 2 | X 2 X 2 ̸ ⊥ ⊥ X 2 X 1 ̸ ⊥ ⊥ X 2 X 1 ⊥ ⊥ X 1 | {X 1 , X 2 } X 1 ̸ ⊥ ⊥ {X 1 , X 2 } X 2 ⊥ ⊥ X 2 | {X 1 , X 2 } X 2 ̸ ⊥ ⊥ {X 1 , X 2 } ,</formula><p>as well as the independencies which follow directly from the listed independencies. To see that M = (Ω, X) is a perfect map of P , note that the histories are</p><formula xml:id="formula_66">H(X 1 ) = H(X 2 ) = {1}, H(X 1 | X 2 ) = {1}</formula><p>, and H(X 2 | X 1 ) = ∅. One an easily verify that the above independencies correspond to the structural independencies. In particular</p><formula xml:id="formula_67">X 2 ⊥ Ω X 2 | X 1 .</formula><p>However, there is no graph with nodes {v 1 , v 2 } such that v 2 is d-separated from itself given v 1 , since the path from v 2 to itself has zero edges and can only be blocked by itself. For a more complex example when there is no graph that is a perfect map for a distribution, see Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs for Section 6</head><p>Throughout this section, let Ω = × i∈I Ω i be a factored space. Let us state some standard definitions.</p><p>Definition C.1 (Outer product). Let J, K ⊆ I be disjoint. Let P J and P K be distributions over Ω J , and Ω K , respectively. Then the outer product of P J and P K is the distribution P J ⊗ P K over Ω J∪K with (P J ⊗ P K )(α) := P J (α J )P K (α K ) for all α ∈ Ω J∪K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factored space models arxiv preprint</head><p>We are now ready to prove the soundness of structural independence for events. </p><formula xml:id="formula_68">B ∩ C = (B ∩ C) J × C J = C J × (B ∩ C) J ,</formula><p>wherein we use commutativity of the Cartesian product over indexed families in the last step. Moreover, we observe the following identity:</p><formula xml:id="formula_69">A ∩ B ∩ C = (A ∩ C) ∩ (B ∩ C) = ((A ∩ C) J × C J ) ∩ (C J × (B ∩ C) J )) = ((A ∩ C) J ∩ C J ) × (C J ∩ (B ∩ C) J )) = (A ∩ C) J × (B ∩ C) J .</formula><p>Finally, using that P factorizes over the factors and the previous identities, we obtain</p><formula xml:id="formula_70">P (A ∩ C) • P (B ∩ C) = P (A ∩ C) J × C J • P C J × (B ∩ C) J = P J (A ∩ C) J • P J C J • P J (C J ) • P J (B ∩ C) J = P (A ∩ C) J × (B ∩ C) J • P C J × C J = P (A ∩ B ∩ C) • P (C).</formula><p>This is exactly the definition of A ⊥ ⊥ P B | C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Local-to-global principle</head><p>Here we prove the local-to-global principle (Lemma 6.5). We first prove a simple preparatory lemma.</p><p>Lemma C.5. Let Ω = × i∈I Ω i be a factored space and Q, P ∈ △ ⊗ (Ω). For λ ∈ [0, 1], define R λ i = (1 -λ)Q i + λP i and R λ = i∈I R λ i . Let A ⊆ Ω be an event. Then the function λ → R λ (A) is a polynomial of degree at most |I| in λ.</p><p>Proof. We have</p><formula xml:id="formula_71">R λ (A) = a∈A R λ (a) = a∈A i∈I R λ i (a i ) = a∈A i∈I (1 -λ)Q i (a i ) + λP i (a i ) .</formula><p>Thus, R λ (A) is a polynomial of degree at most |I|.</p><p>Now we prove Lemma 6.5, the local-to-global principle, which states that if there is an ε-ball B of distributions</p><formula xml:id="formula_72">Q ′ such that X ⊥ ⊥ Q ′ Y | Z for all Q ′ ∈ B, then X ⊥ ⊥ P Y | Z for all P ∈ △ ⊗ (Ω).</formula><p>Proof of Lemma 6.5. Let B ⊆ △ ⊗ (Ω) be the set of all</p><formula xml:id="formula_73">Q ′ ∈ △ ⊗ (Ω) with d(Q, Q ′ ) &lt; ε. Let ε be such that X ⊥ ⊥ Q ′ Y | Z for all Q ′ ∈ B. Let P ∈ △ ⊗ (Ω), x ∈ Val(X), y ∈ Val(Y )</formula><p>, and z ∈ Val(Z) be arbitrary. We need to show x ⊥ ⊥ P y | z, or equivalently P (x, z)P (y, z) = P (x, y, z)P (z).</p><p>With the parameter λ ∈ [0, 1], we now define a distribution R λ that interpolates between Q and P as follows: For all i ∈ I, we define R λ i = (1 -λ)Q i + λP i , and we let R λ = i∈I R λ i . Then we have R 0 = Q and R 1 = P . Note that the map λ → R λ is continuous. Thus, R λ → Q as λ → 0 + . As B is an open set around Q and R λ → 0 as λ → 0 + , there are then infinitely many λ &gt; 0 with R λ ∈ B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arxiv preprint</head><p>Proof. We define the distribution R as R(ω) := (P (ω) + Q(ω)/2. Since for j ̸ = i, P and Q have identical factors, we have R j = P j = Q j . Further, R has the factor R i with R i (ω i ) = (P i (ω i ) + Q i (ω i ))/2 in i. Consequently, R ∈ △ ⊗ (Ω). By our assumption, A ⊥ ⊥ R B | C holds. We will show that this implies that P</p><formula xml:id="formula_74">(A | C) = Q(A | C) or P (B | C) = Q(B | C) holds. The independence A ⊥ ⊥ R B | C means that R(A ∩ C) • R(B ∩ C) = R(A ∩ B ∩ C) • R(C).</formula><p>By substituting the definition of R and multiplying both sides by 4, we obtain</p><formula xml:id="formula_75">P (A ∩ C) + Q(A ∩ C) • P (B ∩ C) + Q(B ∩ C) = P (A ∩ B ∩ C) + Q(A ∩ B ∩ C) • P (C) + Q(C) .</formula><p>The left side of the equation multiplies out to</p><formula xml:id="formula_76">P (A ∩ C) • P (B ∩ C) + Q(A ∩ C) • Q(B ∩ C) + P (A ∩ C) • Q(B ∩ C) + Q(A ∩ C) • P (B ∩ C)</formula><p>and the right side multiplies out to</p><formula xml:id="formula_77">P (A ∩ B ∩ C) • P (C) + Q(A ∩ B ∩ C) • Q(C) + P (A ∩ B ∩ C) • Q(C) + Q(A ∩ B ∩ C) • P (C).</formula><p>As A ⊥ ⊥ P B | C and A ⊥ ⊥ Q B | C, the first two terms on each side cancel out, and we have</p><formula xml:id="formula_78">P (A ∩ C) • Q(B ∩ C) + Q(A ∩ C) • P (B ∩ C) = P (A ∩ B ∩ C) • Q(C) + Q(A ∩ B ∩ C) • P (C).</formula><p>By definition of △ ⊗ C (Ω), P (C) &gt; 0 and Q(C) &gt; 0, and we can divide both sides of the equation by</p><formula xml:id="formula_79">P (C) • Q(C). P (A | C) • Q(B | C) + Q(A | C) • P (B | C) = P (A ∩ B | C) + Q(A ∩ B | C) = P (A | C) • P (B | C) + Q(A | C) • Q(B | C),</formula><p>where we transformed the right-hand side further by again using the independencies for P and Q. By subtracting the right-hand side from the equation, and then factoring the left-hand side of the result, we obtain</p><formula xml:id="formula_80">P (A | C) -Q(A | C) • Q(B | C) -P (B | C) = 0.</formula><p>Thus, at least one of the factors needs to be zero, which means</p><formula xml:id="formula_81">P (A | C) = Q(A | C) or P (B | C) = Q(B | C).</formula><p>To strengthen the mutual exclusion principle to global irrelevance via an interpolation argument, we first prove the following simple lemma.</p><p>Lemma C.10. Let P, P ′ ∈ △ ⊗ C (Ω). For λ ∈ [0, 1], define P λ j = (1 -λ)P j + λP ′ j and P λ = j∈I P λ j . Then P λ ∈ △ ⊗ C (Ω).</p><p>Proof. P λ ∈ △ ⊗ (ω) by definition. It remains to show that P λ (C) &gt; 0. We have</p><formula xml:id="formula_82">P λ (C) = c∈C j∈I (1 -λ)P j (c j ) + λP ′ j (c j ) ≥ c∈C j∈I (1 -λ)P j (c j ) + j∈I λP ′ j (c j ) = c∈C (1 -λ) |I| P (c) + c∈C λ |I| P ′ (c) = (1 -λ) |I| P (C) + λ |I| P ′ (C) &gt; 0 ,</formula><p>wherein the first step uses the same expansion as the proof of Lemma C.5. For the first inequality, we multiply out the product over j, and remove all the summands that contain both P j (c j ), and P ′ j ′ (c j ′ ), that is summands of the form (1 -λ)P j (c j ) • λP j ′ (c j ′ ) • . . . , which are all non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we prove Lemma C.7, which states that Cohistory</head><formula xml:id="formula_83">(A | C) ∪ Cohistory(B | C) = I.</formula><p>Proof of Lemma C.11. 1. Let supp(P ) ⊆ supp(Q). We now prove the contraposition. If Q(C) = 0, then C ∩ supp(Q) = ∅ by definition of the support. It follows that C ∩ supp(P ) = ∅, and thus, P (C) = 0. 2. Let P = P J ⊗ P J . For all α ∈ Ω J and β ∈ Ω J , we have α • β ∈ supp(P ) ⇐⇒ 0 &lt; P (α • β) = P J (α)P J (β) ⇐⇒ P J (α) &gt; 0 and P J (β) &gt; 0 ⇐⇒ α ∈ supp(P J ) and β ∈ supp(P J ) ⇐⇒ α • β ∈ supp(P J ) × supp(P J ).</p><p>3. Let P = P J ⊗ P J , and Q = Q J ⊗ P J , let supp(P J ) ⊆ supp(Q J ), and let P (C) &gt; 0. By 2., we have</p><formula xml:id="formula_84">supp(P ) = supp(P J ) × supp(P J ) ⊆ supp(Q J ) × supp(P J ) = supp(Q) .<label>(8)</label></formula><p>Since P (C) &gt; 0, it follows from 1. that Q(C) &gt; 0. This concludes the proof.</p><p>We now use Lemma C.11 to show the following lemma. Lemma C.12. Let A, C ⊆ Ω be events and P, Q ∈ △ ⊗ C (Ω). Further, let P j = Q j for all j that are relevant to A given C. Then P</p><formula xml:id="formula_85">(A | C) = Q(A | C).</formula><p>Proof. Essentially, the proof is a progressive application of the definition of irrelevance. We change one factor at a time to transform P into Q. However, as the fact that j is irrelevant to A given C only makes a statement about distributions in △ ⊗ C (Ω), we need to prove that the intermediate distributions are all in △ ⊗ C (Ω). Let J := Cohistory(A | C). Let Q be such that Q J is strictly positive. (Later we strengthen the claim to arbitrary distributions Q ∈ △ ⊗ C (Ω).) Let P j = Q j for all j that are relevant to A given C. Since J is finite, we can express it as J = {j 1 , . . . , j q } with mutually distinct elements j k . We now progressively replace factors in P until it is equal to Q. We do this by defining a sequence P 0 , P 1 , . . . , P q of distributions, wherein P 0 = P and P q = Q. Then we show that P (A | C) k-1 = P (A | C) k for all k ∈ {1, . . . , q}. We define P n recursively as follows.</p><p>1. For all i ∈ I, set P 0 i := P i .</p><p>2. For all k ∈ {1, . . . , q} and j ∈ I, set P k j :=</p><formula xml:id="formula_86">P k-1 j if j ̸ = j k Q j if j = j k . 3. Set P k = i∈I P k i .</formula><p>Note that by construction, we have that P q = Q, and also P k-1 and P k can only differ in j k . Next we show by induction that for all k ∈ {1, . . . , q}, we have P k (C) &gt; 0. First, we note that since P 0 = P , we have P 0 (C) &gt; 0. We now assume that P k-1 (C) &gt; 0 (induction assumption). Since we assumed that Q J is strictly positive, P k j k = Q j k is strictly positive. As P k I\{j k } = P k-1 I\{j k } , we have that supp(P k ) ⊇ supp(P k-1 ) by Lemma C.11. Therefore, P k-1 (C) &gt; 0 implies P k (C) &gt; 0. That means, P k ∈ △ ⊗ C (Ω) for all k ∈ {1, . . . , q}. As all j k are irrelevant to A given C by definition, we have that</p><formula xml:id="formula_87">P (A | C) = P 0 (A | C) = P 1 (A | C) = • • • = P q (A | C) = Q(A | C) .</formula><p>This proves the lemma for those Q for which Q J is strictly positive. We now prove the general case. </p><formula xml:id="formula_88">Let Q ∈ △ ⊗ C (Ω) be arbitrary. Let Q ′ be a probability distribution with Q ′ J = P J = Q J such that Q ′ J is</formula><formula xml:id="formula_89">P (B ∩ C) • P Y -1 (y) ∩ C = z∈Val(Z) P (B ∩ C) • P (Y, Z) -1 (y, z) ∩ C = z∈Val(Z) P B ∩ (Y, Z) -1 (y, z) ∩ C • P (C) (by B ⊥ ⊥ P (Y, Z) | C) = P B ∩ Y -1 (y) ∩ C • P (C).</formula><p>That shows B ⊥ ⊥ P Y | C. The corollary follows from U J = (U J ′ , U J\J ′ ).</p><p>Lemma C.15. Let J ⊆ I, and let P be a distribution over Ω with P = P J ⊗ P J . Further, let A J ⊆ Ω J , and A J ⊆ Ω J .</p><p>Then we have P (A J × A J ) = P J (A J ) • P J (A J ).</p><p>Proof.</p><p>P J (A J ) • P J (A J ) = ω J ∈A J P J (ω J )</p><formula xml:id="formula_90">ω J ∈A J P J (ω J ) = ω J ∈A J ω J ∈A J P J (ω J )P J (ω J ) = ω J •ω J ∈A J ×A J P (ω J • ω J ) (P = PJ ⊗ P J ) = P (A J × A J )</formula><p>For J ⊆ I, α ∈ Ω J and D ⊆ Ω, we define D α as</p><formula xml:id="formula_91">D α := {ω ∈ D : ω J = α} = D ∩ U -1 (α)<label>(9)</label></formula><p>We obtain the following lemma.</p><p>Lemma C.16. Let P be a distribution that factorizes over Ω, let J ⊆ I, α ∈ Ω J , and D ⊆ Ω. Further, let δ α ∈ △(Ω J ) be the delta distribution at α, which is the distribution that assigns probability 1 to α. Then we have the following identities:</p><p>1. P D α ) = P J (α) • P J D α J .</p><p>2. δ α ⊗ P J (D) = P J D α J .</p><p>Proof Let δ α ∈ △ ⊗ (Ω J ) be the delta distribution at α. For this proof, we would like to apply Lemma C.12 to the distributions (δ α ⊗ P J ), and P . The conditions we need in order to apply Lemma C.12 are</p><formula xml:id="formula_92">1. P ∈ △ ⊗ C (Ω)</formula><p>2. (δ α ⊗ P J ) ∈ △ ⊗ C (Ω) 3. (δ α ⊗ P J ) i = P i for all factors i that are relevant to A given C.</p><p>1. is satisfied since P ∈ △ ⊗ (Ω) by our assumption, and P (C) &gt; 0 because P (B ∩ C) &gt; 0. For 2., we need to show that (δ α ⊗ P J ) ∈ △ ⊗ (Ω) and (δ α ⊗ P J )(C) &gt; 0. First, we note that δ α = i∈J δ αi since for any α ′ ∈ Ω J we have i∈J δ αi (α ′ i ) = 1 ⇐⇒ α ′ i = α i for all i ∈ J ⇐⇒ α ′ = α ⇐⇒ δ α (α ′ ) = 1 .</p><p>Since P factorizes over Ω, P J also factorizes, and thus (δ α ⊗ P J ) ∈ △ ⊗ (Ω).</p><p>Next, we show that (δ α ⊗ P J )(C) &gt; 0. Since B = U -1 J (α), B ∩ C = {ω ∈ C : ω J = α} = C α . By Lemma C.16 part 1, it follows that P (B ∩ C) = P (C α ) = P J (α) • P J (C α J ) .</p><p>Since P (B ∩ C) &gt; 0, we also have P J (C α J ) &gt; 0. Further, by Lemma C.16 part 2., we have that δ α ⊗ P J (C) = P J C α J &gt; 0. Therefore, we have (δ α ⊗ P J ) ∈ △ ⊗ C (Ω), and the second condition for Lemma C.12 is satisfied.</p><p>For proving 3., we observe that the set of factors that are relevant to A given C is exactly J, since J is the cohistory of A given C. Further, since (δ α ⊗ P J ) = δ α ⊗ ( i∈J P i ), we have (δ α ⊗ P J ) i = P i for all i ∈ J, and 3. is satisfied.</p><p>We can now apply Lemma C.12, and obtain P (A | C) = (δ α ⊗ P J )(A | C). The right-hand side can be further transformed as follows (using Lemma C.16, part 2 in step ( * )):</p><formula xml:id="formula_95">(δ α ⊗ P J )(A | C) = (δ α ⊗ P J )(A ∩ C) (δ α ⊗ P J )(C) ( * ) = P J (A ∩ C) α J P J C α J = P J A α J ∩ C α J P J C α J = P J A α J | C α J .</formula><p>So overall, we have For the inclusion from right to left, let ω ∈ A ∩ C and α ∈ C K . It suffices to show that ω K • α ∈ A ∩ C, or equivalently δ ω K •α (A ∩ C) = 1. To show this, we would like to apply Lemma C.12 to δ ω and δ ω K •α , so we need to show that both are in △ ⊗ C (Ω). By <ref type="bibr" target="#b9">(10)</ref>, we have δ ω , δ ω K •α ∈ △ ⊗ (Ω). Next we show that both assign positive probability to C. Since ω ∈ A ∩ C, we have ω ∈ C and thus δ ω (C) = 1 &gt; 0. Since ω K ∈ (A ∩ C) K so ω K ∈ C K , we have δ ω K (C K ) = 1. Since α ∈ C K , we have δ α (C K ) = 1. It follows that </p><formula xml:id="formula_96">P (A | C) = P J A α J | C α J .<label>(12</label></formula><formula xml:id="formula_97">δ ω K •α (C) = δ ω K •α C K × C K = δ ω K (C K ) • δ α (C K ) = 1 • 1 = 1 &gt; 0.</formula><formula xml:id="formula_98">δ ω K •α (A ∩ C) = δ ω K •α (A ∩ C) δ ω K •α (C) = δ ω K •α (A | C) = δ ω (A | C) = δ ω (A ∩ C) δ ω (C) = 1.</formula><p>Therefore, ω K • α ∈ A ∩ C. This shows that A ∩ C = (A ∩ C) K × C K . Therefore, H(A | C) ⊆ K. This concludes the proof of Lemma C.8, and thus of the completeness of structural independence for events, Lemma 6.4, and consequently the main theorem, Theorem 6.2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: This graph violates the faithfulness condition: Y and Z are not d-separated by ⃗ X, despite Y ⊥ ⊥ Z | ⃗ X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Factored space with three factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>From this, and the fact that K disintegrates C, we obtain c ∈ C. Therefore, we have C × ⊆ C. By ( * ), we have C ⊆ C J∩K × C J∩K ⊆ C × and C ⊆ C J∩K × C J∩K ⊆ C × . Combined with C × ⊆ C, we have proved C = C J∩K × C J∩K and C = C J∪K × C J∪K as required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 4 . 7 (</head><label>47</label><figDesc>History is minimal generating set). For a factored space Ω = × i∈I Ω i , let X : Ω → Val(X), and letC ⊆ Ω.Then, H(X | C) is the unique minimal set which generates X given C. Proof. By definition, H(X | C) = {J ⊆ I | J generates X given C}. As generation is closed under intersection by Lemma A.2, it follows that H(X | C) generates X given C. As I is finite, H(X | C) is the unique minimum of {J ⊆ I | J generates X given C}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 6 . 3 (</head><label>63</label><figDesc>Soundness for Events). Let A, B, C ⊆ Ω be events in a factored space Ω = × i∈I Ω i .If the histories satisfyH(A | C) ∩ H(B | C) = ∅, then A ⊥ ⊥ P B | C holds for all P ∈ △ ⊗ (Ω). Proof. Suppose that H(A | C) ∩ H(B | C) = ∅ holds. Let P ∈ △ ⊗ (Ω)be an arbitrary distribution that factorizes over Ω. We need to prove A ⊥ ⊥ P B | C. Let J := H(A | C). By H(A | C) ∩ H(B | C) = ∅, we have H(B | C) ⊆ J = I \ J. Since J generates A given C by Lemma 4.7, Lemma C.4 implies A ∩ C = (A ∩ C) J × C J . We now claim that J generates B given C. First, J disintegrates C since J disintegrates C. Since H(B | C) ⊆ J holds and H(B | C) generates B given C, we obtain U J ▷ C U H(B|C) ▷ C 1 B , and so J generates B given C. Thus, Lemma C.4 also implies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>strictly positive, for example Q ′ J could be chosen as the uniform distribution. From Lemma C.11 part 2, it follows that supp(Q′ ) ⊇ supp(Q). Together with Q(C) &gt; 0, this implies Q ′ (C) &gt; 0 by Lemma C.11, part 1. Then P (A | C) = Q ′ (A | C). By the same argument with Q instead of P , we obtain Q(A | C) = Q ′ (A | C), and so P (A | C) = Q(A | C).This concludes the proof. Now we prove a simple preparatory lemma, namely that mixed independence satisfies the following decomposition graphoid property. Lemma C.13 (Decomposition of mixed independence). Let Y, Z be two random variables on Ω and let B, C ⊆ Ω. Then,B ⊥ ⊥ P (Y, Z) | C =⇒ B ⊥ ⊥ P Y | C.Corollary C.14. For J ′ ⊆ J ⊆ I, the independenceB ⊥ ⊥ P U J | C implies B ⊥ ⊥ P U J ′ | C.Factored space models arxiv preprint Proof. Let B ⊥ ⊥ P (Y, Z) | C. Let y ∈ Val(Y ). Then we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>.</head><label></label><figDesc>Part 1 follows from D α = {α} × D α J and from Lemma C.15. For part 2, we noteδ α ⊗ P J (D) = ω∈D (δ α ⊗ P J )(ω) = ω∈D δ α (ω J )P J (ω J ) = ω∈D α P J (ω J ) = ω J ∈D α J P J (ω J ) = P J D α J .Ultimately, the goal is to show thatH(A | C) = Cohistory(A | C) (Lemma C.8). First, we show that U H(A|C) and U Cohistory(A|C) have similar independence properties, namely thatU J ⊥ ⊥ ⊗ U J | C holds both for J = H(A | C) (Lemma C.17) and for J = Cohistory(A | C) (Lemma C.<ref type="bibr" target="#b17">19</ref>). Note that Lemma C.17 is not necessary for our proof of Lemma C.8, but rather serves as a motivation why Lemma C.19 is a useful step in the direction of showing thatH(A | C) = Cohistory(A | C).Lemma C.17.Let J = H(A | C). Then we have U J ⊥ ⊥ ⊗ U J | C. Proof. Let P ∈ △ ⊗ (Ω), α ∈ Ω J and β ∈ Ω J . We first observe that {U J = α} ∩ (C J × C J ) = ({α} ∩ C J ) × C J .Since C = C J × C J by definition of the history, we haveP U -1 J (α) ∩ C P U -1 J (β) ∩ C = P {α} ∩ C J × C J P {β} ∩ C J × C J = P J {α} ∩ C J P J (C J )P J {β} ∩ C J P J (C J ) (Lemma C.15) = P {α • β} ∩ C P (C) (Lemma C.15) = P U -1 J (α) ∩ U -1 J (β) ∩ C P (C). This precisely means that U J ⊥ ⊥ ⊗ U J | C.In Lemma C.<ref type="bibr" target="#b17">19</ref>, we show that this independence also holds with Cohistory(A | C) in place of H(A | C). We first show that the independence holds for A and U Cohistory(A|C) .Lemma C.18. Let A, C ⊆ Ω, and letJ = Cohistory(A | C). Then A ⊥ ⊥ ⊗ U J | C. Proof. Let P ∈ △ ⊗ (Ω) and α ∈ Ω J . Let B := U -1 J (α). We need to show A ⊥ ⊥ P B | C, i.e., P (A ∩ C) • P (B ∩ C) = P (A ∩ B ∩ C) • P (C).Note that if P (B ∩ C) = 0, then both sides of the equation are zero and the independence holds. So we can assume that P (B ∩ C) &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>) Combining these results, we obtainP (A | C) • P (B ∩ C) = P J A α J | C α J • P J (α) • P J C α J(<ref type="bibr" target="#b10">11)</ref> and (12)= P J (α) • P J A α J ∩ C α J = P J (α) • P J (A ∩ C) α J(distributivity of projection and (9))Since (P, Q) ∈ △ ⊗ C,i (Ω), and i / ∈ J by our assumption, we have P J = Q J , and thusP (A | C) = Q(A | C). Thus, i ∈ Cohistory(A | C).Finally, we show the other inclusion: H(A | C) ⊆ Cohistory(A | C) =: K. By Lemma C.4, the history H(A | C) is the smallest set J which satisfies A ∩ C = (A ∩ C) J × C J and C = C J × C J . Thus, it suffices to show that K has these properties. By Lemma C.20, we know that C = C K ×C K . Thus, what remains is to show that A∩C = (A∩C) K ×C K . For the inclusion from left to right, let ω ∈ A ∩ C. It follows that ω K ∈ (A ∩ C) K . Since ω ∈ C, we have ω K ∈ C K , and thus ω = ω K • ω K ∈ (A ∩ C) K × C K .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Thus, δ ω , δ ω K •α are two distributions in △ ⊗ C (Ω) with the same factors in K = Cohistory(A | C). From Lemma C.12, it follows that δ ω (A | C) = δ ω K •α (A | C),and therefore</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We begin by observing that H(Y | C) ∪ H(Y | C) disintegrates C because disintegration is closed under union by Lemma A.1. Moreover, by definition of the history, we have U H(X|C) ▷ C X and U H(Y |C) ▷ C Y . Thus, we have (U H(X|C) , U H(Y |C) ) ▷ C (X, Y ), which is equivalent to U H(X|C)∪H(Y |C) ▷ C (X, Y ). Together with the fact that H(X | C) ∪ H(Y | C) disintegrates C, this means that H(X | C) ∪ H(Y | C) generates (X, Y ) given C. By Lemma 4.7, the history is the smallest generating set. Therefore, we have H</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>By definition of the history, we have U H((X,Y )|C) ▷ C (X, Y ), and therefore also U H((X,Y )|C) ▷ C X and U H((X,Y )|C) ▷ C Y . Together with the fact that H((X, Y ) | C) disintegrates C by definition, this means that H((X, Y ) | C) generates X given C, and H((X, Y ) | C) also generates Y given C. As the history is the smallest generating set by Lemma 4.7, we have H</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs for Section 5 B.1 Compositional semigraphoid</head><p>Here we prove Proposition 5.2, which states that structural independence is a compositional semigraphoid. We first establish the composition axiom. As this holds for all w ∈ Val(W ), we arrive at X ⊥ Ω (Y, Z) | W , which is exactly the conclusion of the composition axiom.</p><p>The remaining four axioms can be shown directly in a straightforward manner, but we instead use the soundness and completeness theorem for a short proof. Proposition 5.2. Structural independence is a compositional semigraphoid.</p><p>Proof. We need to show axioms 1-4 and 6 in Table <ref type="table">1</ref>. Axiom 6 (composition) is true by Lemma B.1. Moreover, Axioms 1-4 follow directly from Theorem 6.2 and the fact that statistical independence is a semigraphoid <ref type="bibr" target="#b16">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Distribution over DAG and corresponding FSM</head><p>Here we provide a proof for Lemma 5.3 which establishes the relationship between a DAG G = (V, E, Val) and the FSM M G = (Ω, X) constructed from G.</p><p>First, we prove a preparatory lemma. Lemma B.2. Let G = (V, E, Val) be a DAG, let M G = (Ω, X) be the factored space model constructed from G. Let P Ω factorize over Ω. Then we have</p><p>Proof. We claim that for any fixed x ∈ Val, we have</p><p>Now we prove Lemma 5.3, which we restate here for convenience. Lemma 5.3. Let ∆ * (G) be the set of distributions on Val that factorize over G and let △ ⊗ (Ω) be the set of distributions on Ω that factorize over Ω. Let τ : △ ⊗ (Ω) → ∆ * (G) be the function defined as follows for all P Ω ∈ △ ⊗ (Ω) and x ∈ Val:</p><p>Then τ is bijective and its inverse τ -1 satisfies the following for all P ∈ ∆ * (G) and ω ∈ Ω:</p><p>arxiv preprint Definition C.2 (Marginal distribution). Let P be an arbitrary distribution over Ω, and let J ⊆ I. Then the marginal distribution of P in J is given by P J = P • U -1 J . For i ∈ I, we write P i := P {i} .</p><p>Note that if P factorizes over Ω, then it is the outer product of its marginal distributions in i ∈ I. That is, P = i∈I P i .</p><p>In that case, the marginal distribution is P J = i∈J P i . For a finite set S, we also write △(S) for the set of all distributions on S. Then the set △ ⊗ (Ω) of all distributions that factorize over Ω satisfies △ ⊗ (Ω) = { i∈I P i | ∀i ∈ I :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of soundness for events</head><p>In this section, we prove Lemma 6.3, the soundness of structural independence for events. We first characterize derived variables (Definition 4.1) in an alternative way as follows. Lemma C.3. Let X : Ω → Val(X) and Y : Ω → Val(Y ) be two random variables and C ⊆ Ω. Then the following statements are equivalent:</p><p>) for all ω ∈ C, and thus (i) holds.</p><p>Recall from Definition 4.6 that J generates X given C if J disintegrates C and U J ▷ C X holds. We naturally extend this definition to events A ⊆ Ω in that J generates A given C if J generates 1 A given C, where 1 A is the indicator random variable that is 1 for all ω ∈ A and 0 otherwise. In the following lemma, we characterize Definition 4.6 differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma C.4 (Alternative characterization of generation).</head><p>Let Ω = × i∈I Ω i be a factored space, let A, C ⊆ Ω, let J ⊆ I be a set with complement J = I \ J, and suppose that J disintegrates C. The following statements are equivalent:</p><p>and by (i) we have</p><p>holds by definition of the Cartesian product. For the other inclusion, we have</p><p>We have b, ω ∈ C and b J = ω J , which by (ii) implies that b ∈ A holds if and only if ω ∈ A holds. Since b ∈ A, we conclude ω ∈ A, which was to be shown. This proves (iii).</p><p>(iii) =⇒ (i): Let ω, ω ′ ∈ C and assume U J (ω) = U J (ω ′ ). In order to establish (i), it suffices to show that</p><p>, wherein the last step uses (iii). This shows ω ′ ∈ A. Thus, (i) holds.</p><p>Finally, recall from Lemma 4.7 that the history H(A | C) is the smallest set J that generates A given C. Together with (i) =⇒ (iii), this proves the final claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arxiv preprint</head><p>Now, consider the function</p><p>which by Lemma C.5 is a polynomial of degree at most 2|I|. For infinitely many λ &gt; 0, we have R λ ∈ B, and thus f (λ) = 0 by construction of B. Since f is a polynomial with infinitely many roots, it must be identically zero. Thus P (x, z)P (y, z) -P (x, y, z)P (z) = f (1) = 0, that is, x ⊥ ⊥ P y | z. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of completeness for events</head><p>Here we prove the completeness of structural independence for events (Lemma 6.4). We introduce further notation for this section:</p><p>That is, the set of distributions P that factorize over Ω and assign a positive probability to C.</p><p>(Ω) and P j = Q j for all j ∈ I \ {i} denote the pairs of distributions in △ ⊗ C (Ω) that can only differ in i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We aim to show that</head><p>A key ingredient to the proof is the concept of irrelevant factors, which form the cohistory.</p><p>Definition C.6 (irrelevant factors, cohistory). Let A, C ⊆ Ω, and let</p><p>We say i is relevant if it is not irrelevant, and (P, Q)-relevant if it is not (P, Q)-irrelevant to A given C. Intuitively, i is relevant to A given C, when changing P only in the factor i can change P (A | C). Note that global irrelevance of i implies the (P, Q)-irrelevance of i for all (P, Q) ∈ △ ⊗ C,i , while global relevance only implies (P, Q)-relevance for at least one pair (P, Q) ∈ △ ⊗ C,i . Recall that for a set J ⊆ I, we write J = I \ J for the complement of J in I. We will show the following two lemmas, which combined result in Lemma 6.4:</p><p>Thus, our task reduces to proving Lemmas C.7 and C.8, to which we devote two separate subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Proof of Lemma C.7</head><p>Let A, B, C ⊆ Ω. and assume A ⊥ ⊥ ⊗ B | C. We want to show that Cohistory(A | C) ∪ Cohistory(B | C) = I, which means that for all i ∈ I, i is globally irrelevant to one of A or B given C. Before we show this, we show a weaker claim, the mutual exclusion principle. It states that for any given pair of distributions (P, Q) ∈ △ ⊗ C,i (Ω), i cannot be (P, Q)-relevant to both A given C and B given C. However, the mutual exclusion principle makes no claim that i is globally irrelevant to A given C or to B given C. We later strengthen the mutual exclusion principle to show the claim about global irrelevance. Assume i is relevant to A given C. That means there is a tuple</p><p>as a topological space with the induced standard topology. Since the function</p><p>Consequently, the mutual exclusion principle (Lemma C.9) guarantees that</p><p>In the rest of the proof, we argue that <ref type="bibr" target="#b6">(7)</ref> extends to all (P ′ , Q ′ ) ∈ △ ⊗ C,i (Ω), which precisely means that i ∈ Cohistory(B | C). We prove this with a similar strategy to Lemma 6.5.</p><p>Ω) be arbitrary. We interpolate between (P, Q) and (P ′ , Q ′ ) as follows. Let P λ j := (1 -λ)P j + λP ′ j and P λ := j∈I P λ j , and let Q λ be defined analogously. Then Lemma C.10 shows that P λ , Q λ ∈ △ ⊗ C (Ω). One can also easily verify that P λ j = Q λ j for all j ̸ = i, and so</p><p>By Lemma C.5, this function is a polynomial in λ. We have (P λ , Q λ ) → (P, Q) for λ → 0 + , which means there are infinitely many λ &gt; 0 for which (P λ , Q λ ) ∈ S. For such λ, we have</p><p>So, for all λ with (P λ , Q λ ) ∈ S we have f (λ) = 0. Thus, the polynomial f has infinitely many roots and must be the zero polynomial. Consequently, we have f (1) = 0, and thus</p><p>Since P ′ and Q ′ are arbitrary, i is globally irrelevant to B given C. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.2 Proof of Lemma C.8</head><p>Let A, C ⊆ Ω. In this section, we show that Cohistory(A | C) = H(A | C). Since Cohistory is the set of relevant factors, this means the history contains exactly the relevant factors. The direction "i is relevant to</p><p>Proof sketch for Lemma C.8. We know that H(A | C) is the smallest set that generates A given C, so it suffices to prove that the set of relevant factors generates A given C. This involves showing that the set of relevant factors disintegrates C, Lemma C.20. To prove Lemma C.20, we establish C as the support of a distribution that factorizes into two components. Establishing that factorization relies on Lemma C.19, which states that the background variables of the cohistory are independent of the background variables of its complement given C. Then, the final proof directly verifies the generation property by using the alternative characterization of generation established in Lemma C. <ref type="bibr" target="#b3">4</ref>. All other lemmas are preparatory.</p><p>Further notation for conditional independence. In Definition 6.1, we introduced the notation X ⊥ ⊥ P Y | Z for random variables X, Y, Z, and A ⊥ ⊥ P B | C for events A, B, C. In what follows, we also mix these notations. For example, X ⊥ ⊥ P Y | C means x ⊥ ⊥ P y | C for all x ∈ Val(X) and y ∈ Val(Y ), where x and y are treated as the events X -1 (x) and Y -1 (y), respectively. Similarly, X ⊥ ⊥ P B | C means x ⊥ ⊥ P B | C for all x ∈ Val(X). Finally, we also write ⊥ ⊥ ⊗ if the independence holds for all P ∈ △ ⊗ (Ω), for example</p><p>We start by considering the support of a distribution P over Ω, namely the event supp(P ) ⊆ Ω such that P (ω) &gt; 0 for all ω ∈ supp(P ). We obtain the following lemma. Lemma C.11. Let J ⊆ I and let P, Q ∈ △(Ω) be two distributions over Ω, and let C ⊆ Ω. Then we have 1. If supp(P ) ⊆ supp(Q), and P (C) &gt; 0, then Q(C) &gt; 0.</p><p>2. If P = P J ⊗ P J , then, supp(P ) = supp(P J ) × supp(P J ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">If P</head><p>Note that P and Q do not have to factorize over Ω here.</p><p>= P (A ∩ B ∩ C).</p><p>(definition of B)</p><p>Multiplying both sides with P (C), it follows that A ⊥ ⊥ P B | C. This concludes the proof. Proof. Let P ∈ △ ⊗ (Ω) be any strictly positive distribution, for example the uniform distribution on each factor. Define Q(D) := P (D | C) for any event D ∈ Ω. Then Q is a distribution on Ω that may not necessarily factorize over all i ∈ I. However, we show that it factors into two components Q J and Q J with supp(Q J ) = C J and supp(Q J ) = C J .</p><p>Let α ∈ Ω J , β ∈ Ω J as before. Further, let Q J and Q J be the marginal distributions of Q as in Definition C.2, which means that we have Q = P J (A ∩ C) J P J (C J ) P J (C J )P J (C J ) = P J (A ∩ C) J P J (C J ) .</p><p>In the step ( * ), we use Lemma C.15 and the fact that P factorizes over Ω. Similarly, we get</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximate Causal Abstractions</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</editor>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2019-07-22">2019. July 22-25, 2019. 2019</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causal models with constraints</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hitchcock</surname></persName>
		</author>
		<idno>PMLR. 2023</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<biblScope unit="page" from="866" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstracting Causal Models</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V33I01.33012678</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2678" to="2685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond structural causal models: Causal constraints models</title>
		<author>
			<persName><forename type="first">Tineke</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The intentional stance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Dennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Didolkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.12205</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Agent Incentives: A Causal Perspective</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
		<idno type="DOI">10.1609/AAAI.V35I13.17368</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11487" to="11495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal Inference with Finite Factored Sets</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Garrabrant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11513[cs.AI</idno>
		<ptr target="https://arxiv.org/abs/2109.11513" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causal abstraction for faithful model interpretation</title>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.04709</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal abstractions of neural networks</title>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9574" to="9586" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying independence in Bayesian networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="507" to="534" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstructing constructivism: causal models, Bayesian learning mechanisms, and the theory theory</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><surname>Henry M Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page">1085</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language models (mostly) know what they know</title>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discovering agents</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.ARTINT.2023.103963</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page">103963</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal Inference via Algebraic Geometry: Feasibility Tests for Functional Causal Structures with Two Binary Observed Variables</title>
		<author>
			<persName><forename type="first">Ciarán</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Spekkens</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2016-0013</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20160013</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graphoids: Graph-Based Logic for Reasoning about Relevance Relations or When would x tell you more about y if you already know z?</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azaria</forename><surname>Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh European Conference on Artificial Intelligence, ECAI 1986</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Benedict du Boulay, David C. Hogg, and Luc Steels. North-Holland</publisher>
			<date type="published" when="1986">July 20-25, 1986. 1986</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="357" to="363" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust agents learn causal world models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Richens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Everitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal Consistency of Structural Equation Models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Rubenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017</title>
		<editor>
			<persName><forename type="first">Kristian</forename><surname>Gal Elidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Kersting</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ihler</surname></persName>
		</editor>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017">August 11-15, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward Causal Representation Learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2021.3058954</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causality in thought</title>
		<author>
			<persName><forename type="first">A</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sloman</surname></persName>
		</author>
		<author>
			<persName><surname>Lagnado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="223" to="247" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data mining: practical machine learning tools and techniques with Java implementations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="76" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Abstraction between Structural Causal Models: A Review of Definitions and Properties</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Massimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zennaro</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2207.08603</idno>
		<idno type="arXiv">arXiv:2207.08603</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
