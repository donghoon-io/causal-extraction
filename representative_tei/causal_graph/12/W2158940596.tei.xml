<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Bayesian Networks as an Inference Engine in KAMET *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Osvaldo</forename><surname>Cairó</surname></persName>
							<email>cairo@itam.mx</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Instituto Tecnológico Autónomo de México (ITAM)</orgName>
								<address>
									<addrLine>Río Hondo 1</addrLine>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Peñaloza</surname></persName>
							<email>rpenalozan@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Instituto Tecnológico Autónomo de México (ITAM)</orgName>
								<address>
									<addrLine>Río Hondo 1</addrLine>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Bayesian Networks as an Inference Engine in KAMET *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During the past decades, many methods have been developed for the creation of Knowledge-Based Systems (KBS). For these methods, probabilistic networks have shown to be an important tool to work with probability-measured uncertainty. However, quality of probabilistic networks depends on a correct knowledge acquisition and modelation.</p><p>KAMET 1 is a model-based methodology designed to manage knowledge acquisition from multiple knowledge sources <ref type="bibr" target="#b0">[1]</ref> that leads to a graphical model that represents causal relations. Up to now, all inference methods developed for these models are rule-based, and therefore eliminate most of the probabilistic information.</p><p>We present a way to combine the benefits of Bayesian networks and KAMET, and reduce their problems. To achieve this, we show a transformation that generates directed acyclic graphs, the basic structure of Bayesian networks <ref type="bibr" target="#b1">[2]</ref>, and conditional probability tables, from KAMET models. Thus, inference methods for probabilistic networks may be used in KAMET models. * This project has been founded by CONACYT, as project number 33038-A, and Asociación Mexicana de Cultura, A.C.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Probabilistic networks have shown to be an important tool for Knowledge-Based Systems (KBS) that include, within their knowledge, probabilistic-measured uncertainty, and relational information about the domain. In particular, if the network model may be represented by a directed acyclic graph (DAG), and includes discrete variables, the use of Bayesian networks is specially adequate. Several methods have been developed to carry out efficiently exact inferences in some particular cases of probabilistic networks (which include Bayesian networks); even more, if some error is tolerated, there are in the literature many methods of approximate inference that reduce considerably the machine time needed to solve the problem <ref type="bibr" target="#b6">[7]</ref>. In the last years, probabilistic networks have shown to have very important aplications, far beyond the theoric ground (see, for example, <ref type="bibr" target="#b4">[5]</ref>).</p><p>The quality of the probabilistic models depends strongly on the correctness of the causality and independence representations. Unfortunately, there has been little progress in the creation of methodologies that create correct probabilistic models. The problem of creating such a representation is equivalent to acquire and represent expert knowledge in an appropriate way. KAMET (Knowledge Acquisition Methodology) is a methodology based on models designed to manage knowledge acquisition, mainly for diagnosis, from multiple knowledge sources. As a result of this methodology, one obtains a probabilistic model that represents causal relations between the possible symptoms and problems, with an inaccuracy associated to each relation.</p><p>An issue that has followed KAMET right from the beginning is to find the way to formalize those models, so that they allow to recognize appropriate causes to the problems given at the time. In this article, two methods for transforming KAMET models into Bayesian networks are proposed. The reason for presenting two transformations instead of just one is that each has benefits, one in the model representation and other in the size of the Bayesian network obtained. These transformations look different at first sight, however, it can be shown that the marginal distributions over the common nodes in both transformations will be identical. Thus, exact inference methods will yield to the same results. To make more clear these transformations and their definitions, we will use a simple KAMET model that represents an electric diagnosis system, shown in figure <ref type="figure" target="#fig_0">1</ref>. This model will be explained in section 2, after a brief description of the KAMET CML. In general, a method developed for KBS will work better if more information about the domain is available. The transformations in this article are no exception for this rule: the calculations of the conditional probabilities associated to each node in the Bayesian network will need, sometimes, additional information that is not given by a KAMET model. We won't be able to get this information, because the methodology followed to construct the model ensures that every piece of available information will be present in it. One may use the prior probability of a node to approximate the unknown conditional probabilities.</p><p>A benefit obtained by a transformation to a Bayesian network is that one can include learning methods (like casebased learning, or regression methods) to improve the quality of the inference process, or to change the uncertainty associated to each node if the conditions change (under the assumption that the causality and independence relations remain unchanged).</p><p>In order to keep the additional benefit obtained by KAMET, making symbolic distinctions between the elements in which the diagnosis is based, nodes representing symptoms will be diamond shaped, and those related with problems will have the shape of a square, as shown in figure <ref type="figure" target="#fig_1">2</ref>, even when the Bayesian net- works will make no difference between them. We will give these nodes the same name they had in the origi-nal KAMET model, except for the symptoms, for which we will add a "S" to prevent confusions. For a better understanding of these transformations, we will first suppose that each of the nodes of a KAMET model represents exclusively a binary variable; this means that a given symptom or problem appears or not. Later, we will generalize the process to use nodes that may take any (finite) number of different values.</p><p>We will suppose that the reader is familiar with the concept of Bayesian networks, and with its propagation methods. The next section provides a brief introduction to KAMET and its Conceptual Modeling Language (CML), which will be necessary for a correct understanding of the transformations. In sections 3 and 4, we present the two transformations followed by a generalization to models with non-binary nodes and some examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">KAMET CML</head><p>As we said before, KAMET is a model-based methodology designed to manage knowledge acquisition (KA) from multiple knowledge sources. The method provides a strong mechanism with which to achieve KA in an incremental fashion, and in a cooperative environment. The models are used to apply the methodology, as means of communication between human experts and knowledge engineers, as tools in reasoning strategies, and for the structuring and description of knowledge, independently of its implementation.</p><p>KAMET consists of four stages: the strategic planning of the project, construction of the initial model, construction of the feedback model, and construction of the final model.</p><p>The models represent knowledge and reasoning of different knowledge sources in a specific knowledge domain. KAMET uses models, because they lead to well structured and maintainable knowledge bases. This methodology seeks to be general, although it is mainly directed toward diagnosis problems.</p><p>Knowledge acquisition requires strong modeling methods, which should provide a rich vocabulary in which the expertise can be expressed in an appropriated way. These methods, for example, should be able to work with logics such as probabilistic <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or paraconsistent <ref type="bibr" target="#b5">[6]</ref> logics. Knowledge and reasoning should be modeled in such a way that models can be exploited in a very flexible fashion.</p><p>The KAMET CML has three levels of abstraction. The first one corresponds to structural constructors and structural components. The structural constructors are used primarily to highlight the problem itself. These are: problem, which expresses an alteration, disorder or abnormality, classification, alterations disordes or abnormalities that can be considered as a classification problem and, thus, may be represented within a table, and subdivision of alterations, disorders or abnormalities (figure <ref type="figure" target="#fig_2">3</ref>). The structural components are used to establish the characteristics and possible solutions of the problem (figure <ref type="figure" target="#fig_3">4</ref>).They have the following meaning: • Symptom is a manifestation or sign related to an alteration, disorder or abnormality.</p><p>• The antecedent expresses the previous circumstances that can be used to judge something that might happen.</p><p>• A solution expresses the possible solution to a disorder, alteration or abnormality. It is always related to structural constructors.</p><p>• Time expresses the duration of structural components and structural constructors.</p><p>• A value expresses the characteristics of syptoms, antecedents or groups.</p><p>• Inaccurate expresses the uncertainty or lack of precision of an intermediate or terminal node.</p><p>• The process expresses the sequence of actions or series of operations required to obtain a result.</p><p>• A formula expresses the calculations that must be compelled in order to determine the alteration, disorder or abnormality.</p><p>• Examination expresses a recommendation or necessity of making studies to determine an alteration, disorder or abnormality.</p><p>The second level of abstraction corresponds to nodes and composition rules. Models are represented by digraphs, with nodes as their vertices and composition rules as arcs. Structural constructors and structural components form the nodes, which are divided in three kinds: initial, intermediate and terminal. Composition rules permit the adequate combination of structural constructors and components (figure <ref type="figure" target="#fig_4">5</ref>): • Division expresses that an alteration, disorder or abnormality is subdivided in</p><p>• The implication expresses connection from an origin toward a complication.</p><p>• Action expresses that something must be completed (a formula or an examination).</p><p>• Union expresses connection between subdivisions.</p><p>The third level of abstraction corresponds to the global model, which should represent the knowledge acquired from multiple knowledge sources. It must have at least one initial node and one terminal node. In order to facilitate the construction and interpretation of the models, structural constructors and components may be named using a numerical or linguistic label.</p><p>Another element related to the models is the indicator, represented by a square (figure <ref type="figure" target="#fig_5">6</ref>), located in the upper righthand corner of the group or the structural component value being referred to. An indicator is named in three different ways: the notations n, n+, and n, m (with m &gt; n) are used to express the exact, minimum, and minimum and maximum number of elements that must be present, respectively.</p><p>A group is a set of linked elements that have times and/or values in common, or are related among them through an indicator; a group may be an element of another group (figure <ref type="figure" target="#fig_6">7</ref>).A chain is the link of two or more groups, symptoms and/or antecedents. In the graphical representation, the order of the link is irrelevant.  turn, act as nodes in their own right.</p><p>For example, the model in figure <ref type="figure" target="#fig_0">1</ref> expresses that the problem P1 can occur due to two different situations. In the first one, the model expresses that if symptoms 1 and 2 are known to be true, then we can deduce that problem P1 is true with probability 0.7. In the second one, the model expresses that if symptoms 1 and 5 are observed then we can conclude that the problem is P1 with probability 0.6. On the other hand, we can deduce that the problem P3 is true with probability 0.4 if symptoms 1 and 4 are known to be true. Finally, we can reach a conclusion that the problem is P2 with probability 0.9 if problems P1 and P3 and the symptom 3 are observed.</p><p>The prior probabilities of the nodes have no symbolic representation in KAMET, but they may still be known; in that case, they are stored in a table. For a more complete description of KAMET, see <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transformation with AND nodes</head><p>An essential part of KAMET models are their molecular nodes, representing the conjunction and the disjunction. The first transformation proposed uses the AND connector<ref type="foot" target="#foot_1">foot_1</ref> , creating a new type of node which does not represent symptoms nor problems; this will be called an AND node, and its representation will be by a triangle, as shown on figure 9. These nodes will have an upper case letter as a name. Note that we won't create a new node for disjunctions because, in general, this would lead to the loss of information on the conditional probabilities of the new OR node given each of its parents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9. an AND node called T.</head><p>The Bayesian network is constructed generating a node for each component in the KAMET model. An AND node is then generated for each conjunction, and a directed edge from each component in it to the AND node is created. The conditional probability table is the trivial one, given by</p><formula xml:id="formula_0">P (AND|N 1 = n 1 , . . . , N k = n k ) = k Y i=1 n i</formula><p>where N j , j = 1, . . . , k are the parents of the AND node; thus, the AND node has a value of 1 if and only if each of its parents has a value of 1<ref type="foot" target="#foot_2">foot_2</ref> . To finish the transformation, a directed edge is created from the AND node to the node pointed by the implication.</p><p>If there exist OR connectors, directed edges are created from those components joined by a connector of this kind to the node pointed by the corresponding implication.</p><p>After applying this transformation to the model in figure <ref type="figure" target="#fig_0">1</ref>, the Bayesian network with AND nodes obtained is that shown in figure <ref type="figure" target="#fig_7">10</ref>.</p><p>We now need to generate the conditional probability tables. The first case is when a node N has only one parent. We then know the conditional probability of the node given that we observed that its parent appeared P (N |pa (N )). However, the model has no information over the conditional probability given that its parent is observed not to appear; in this case, we may fill the table with the prior unconditional probability of N , if we don't have more information about the node.</p><p>The other case is when N has more than one parent. By construction, the parents of N must form a disjunctive molecular node. The probability table will have the value of the uncertainty associated to one of the parents if and only if that parent has a value of true and none of the others. For the other cases (none or more than one of the parents has a value of true), the prior probability of node N may be used. In some cases it's possible to make other kinds of approximations; for example, if two of the parents are known to be mutually exclusive, then putting a value of 0 to the probability of the node given both parents is useful in the propagation stage. On the other side, it is common to suppose that if more symptoms are known to be present, the probability of a problem should be higher; thus, one may insert values higher to those given to each parent individually.</p><p>For the example we are working with, let the problems have prior probabilities given by P (P 1) = 0.25 P (P 2) = 0.1 P (P 3) = 0.2 and suppose that we have additional information that suggests that if we find symptoms S1, S2, and S5, the probability of occurrence of problem P 1 is 0.1 higher than the one given when only symptoms S1 and S2 are present. We will also suppose that all the symptoms have a prior probability of 0.5. The conditional probability tables associated to the nodes in figure 10 are shown below. We omit the tables of the AND nodes. This transformation has the advantage that maintains the representation given by the original KAMET model, with a structure of the same kind; therefore, the relationship between both models is clear. However, it tends to add many nodes, and the resulting graph will be very large and will form many cliques during the moralization process; thus, the propagation and inference methods may become very slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Direct transformation</head><p>The direct transformation eliminates the problem of generating models that are too big, because it doesn't create new nodes, using those that appear in the original model. This transformation can be performed directly, in stead of making many different steps as in the other transformation.</p><p>The first step is to generate the nodes that will be in the graph, one for each symptom or problem, and assign them a name (as in the past section). Each node will have as parents all the nodes that incide over it in the original model, without paying attention to the connectors that unites them.</p><p>In the example we are following, the result of the transformation is given by figure 11.</p><p>After that, the conditional probability tables of each node given its parents are calculated using the information contained in the connectors of the original model. If there are components united by an AND connector, then the corresponding probability will be given only if all of them (and only them) occur. 0.2 0.8 P2 ¬P2 P1,P3,S3 0.9 0.1 otherwise 0.1 0.9</p><p>The prior probability of occurrence of the problems is used in many cells of the table because we don't know what happens when we do not observe all the elements united by an AND connector. Here one may guess that the inference in both models will produce different outcomes; for example, if the conditional probability of the problem P1 given the symptom combination ¬S1,S2,S3 was known(and was different to 0.25), the conditional probability table will be different. However, if that probability was known, it will be represented in the original model, and the transformation using AND nodes will include it in its representation.</p><p>A problem of the direct transformation is that, although it is pretty natural, produces a loss of the graphical information contained in the KAMET models. When AND nodes are used, it is easy to find out the relationship between the parents of a given node, so that one is able to know which symptoms should appear together in order to be able to say something about the problem; in the direct transformation, when we omit the use of this kind of nodes, the graphical model obtained shows a relation of the same kind between all the parents of a node, even when in the original model that is not true. This problem may be partially solved with the observation of the conditional probability tables. How-ever, a clear graphical representation is fundamental in big problems, where an inspection of all the probability tables is impossible.</p><p>So, each transformation has its advantages and disadvantages compared to the other; the choice of one or the other depends on the kind of representation wanted with it, and on the size of the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Equivalence</head><p>We won't show a complete proof of the equivalence of the Bayesian networks obtained by both transformations. Instead, we will give a motivation for this afirmation, in order to make it more clear.</p><p>First, by definition, an AND node will have a value of "true" if and only if all of its parents have the same value. Also, if an AND node performs as a separator in the Bayesian network of the first transformation, then in the directly formed Bayesian network, the set of its parents will perform as a separator too; this is also true backwards. Thus, if any of the transformation-based Bayesian networks satisfies a Markov property, then the other one will also satisfy the same; so, both networks may use the inference methods.</p><p>Also, the prior marginal probability of every node, except of course the AND nodes, is the same in both networks, so if one adds information of the appearance of a symptom or a problem, the marginal probabilities after propagation of the information will still be the same in both models. If the information we receive corresponds to the observation of an AND node, then it is equivalent to the observation of all of its parents, thus we may include this information in the model formed by the direct transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Generalization</head><p>Up to now, we have supposed that all the nodes in the original KAMET model represent binary variables. Nevertheless the same transformations may be generalized to use variables which may take a finite number of different values, shown by the structural constructor value, even if it comes with an indicator. The generalization may not always be used, though, as we'll see next.</p><p>This generalization is made the natural way: the graphical representation of the transformations will be the same, and the difference will appear only in the tables of conditional probability, where all the possible combination of values of each of the parent nodes will appear. Within this table, one must be able to represent the probability of the problem given that an antecedent is not present; for that, we must add another value (typically '0') to represent the absence of all other values. Thus, if a node has k values, then the probability table of a node given this lone parent will have 2 k+1 entries for each possible value of the son; thus, models in which nodes have many values may become intractable for the size of its tables. However, depending on the indicator associated to the values, many of the combinations may not be present, reducing the size of the conditional probability table.</p><p>One important feature of this generalization is that it is independent of the arguments given in the past section for the equivalence. This means that even if the network come from the generalized transformation, these will still be equivalent; so there is no problem in using any of the transformations.</p><p>Notice that a node with multiple values may be seen as a conjunctive molecular node in which each of its atomic nodes represents a subset of the values it may take. With that in mind, the process of filling in the conditional probability tables when multiple-valued nodes are involved has no difference with filling the probability tables in sons of conjunctive molecular nodes. This reasoning may be useful in order to find a way to reduce the size of conditional probability tables in nodes with a large number of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have shown two ways to obtain a Bayesian network from a KAMET model. With these transformations, we have solved one of KAMET´s problems, the lack of inference methods, taking advantage of the well studied inference methods developed for Bayesian networks. By means of these transformations, one is able not only to use inference methods in KAMET models, but also to apply any Bayesian network-based methodology that may be developed. We didn't present a proof for the equivalence, up to inference methods, of these transformations, but it is easy to show it, because the marginal densities are equal in both models.</p><p>Current research in KAMET focuses on developing a new generalization for nodes that have many values, or inference methods that perform efficiently in Bayesian networks that come from this generalization, such that inference problems may be solved in a reasonable time. Research also focuses on implementing these transformations in order to be able to obtain Bayesian networks from graphical representations of the knowledge acquired by KAMET, and to implement some inference methods (exact and approximate) as Problem Solving Methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. a KAMET model representing a simple electric diagnosis system.</figDesc><graphic coords="2,63.42,79.38,226.44,96.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. the symbols of the nodes in the Bayesian networks used in the transformations. (a) shows a symptom and (b) a problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. structural constructors.</figDesc><graphic coords="3,59.10,116.88,235.02,50.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. structural components.</figDesc><graphic coords="3,59.64,275.52,234.00,148.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. composition rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. indicators. (a) indicator n; (b) indicator n+; (C) indicator n, m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. groups. (a) group; (b) group with indicator; (c) recursive group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. transformation of the KAMET model shown before using AND nodes.</figDesc><graphic coords="5,82.92,79.38,187.38,196.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. the direct transformation of the KAMET model of electric diagnosis.</figDesc><graphic coords="6,84.66,79.38,183.90,118.98" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>KAMET is a project that is being carried out in collaboration with the SWI Group at Amsterdam University and Universidad Politécnica de Madrid.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We will use interchangeably the phrases "AND connector" with "conjunction molecular node"; and "OR connector" with "disjunction molecular node".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We use the convention that 1 means that the node has a value of TRUE (it is observed), and 0 means that it's FALSE.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The symbol ¬A means that the node A didn't appear (has a value of false).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Cairó</surname></persName>
		</author>
		<title level="m">Proceedings of the 11th Banff Knowledge Acquisition for Knowledge-Based Systems Workshop (KAW´98)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Gaines</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mussen</surname></persName>
		</editor>
		<meeting>the 11th Banff Knowledge Acquisition for Knowledge-Based Systems Workshop (KAW´98)</meeting>
		<imprint>
			<publisher>SRGD Publications</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Calgary</orgName>
		</respStmt>
	</monogr>
	<note>The KAMET Methodology: Content, Usage and Knowledge Modeling</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Expert Systems and probabilistic network models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reasoning about knowledge and probability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="340" to="367" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A logic for reasoning about probabilities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information and Computation</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="78" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using Bayesian networks to analyze expression data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hunter</surname></persName>
		</author>
		<title level="m">Paraconsistent Logics. Handbook of Defeasible Reasoning and Uncertainty Management Systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Reasoning with Actual and Potential Contradictions</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>MIT Media Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
