<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Discovery with Fewer Conditional Independence Tests</title>
				<funder>
					<orgName type="full">Eric and Wendy Schmidt Center at the Broad Institute</orgName>
				</funder>
				<funder ref="#_MUBNb9p">
					<orgName type="full">NCCIH/NIH</orgName>
				</funder>
				<funder ref="#_vB9JY5N">
					<orgName type="full">DOE-ASCR</orgName>
				</funder>
				<funder ref="#_HaVB7Kp">
					<orgName type="full">Apple</orgName>
				</funder>
				<funder ref="#_YaMCGWX">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-03">3 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kirankumar</forename><surname>Shiragur</surname></persName>
							<email>&lt;kshiragu@broadinstitute.org&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Eric and Wendy Schmidt Center</orgName>
								<orgName type="institution">Broad Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eric and Wendy Schmidt Center</orgName>
								<orgName type="institution">Broad Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Decision Systems</orgName>
								<orgName type="laboratory">Laboratory for Information</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Decision Systems</orgName>
								<orgName type="laboratory">Laboratory for Information</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Discovery with Fewer Conditional Independence Tests</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-03">3 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.01823v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the wellcelebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Causal discovery is a fundamental task in various scientific disciplines including biology, economics, and sociology <ref type="bibr" target="#b30">(King et al., 2004;</ref><ref type="bibr" target="#b8">Cho et al., 2016;</ref><ref type="bibr" target="#b54">Tian, 2016;</ref><ref type="bibr" target="#b53">Sverchkov &amp; Craven, 2017;</ref><ref type="bibr" target="#b41">Rotmensch et al., 2017;</ref><ref type="bibr" target="#b38">Pingault et al., 2018;</ref><ref type="bibr" target="#b14">de Campos et al., 2019;</ref><ref type="bibr" target="#b39">Reichenbach, 1956;</ref><ref type="bibr" target="#b56">Woodward, 2005;</ref><ref type="bibr" target="#b16">Eberhardt &amp; Scheines, 2007;</ref><ref type="bibr" target="#b26">Hoover, 1990;</ref><ref type="bibr" target="#b20">Friedman et al., 2000;</ref><ref type="bibr" target="#b40">Robins et al., 2000;</ref><ref type="bibr" target="#b48">Spirtes et al., 2000;</ref><ref type="bibr" target="#b36">Pearl, 2003)</ref>. Directed acyclic graphs (DAGs) stand out as a popular choice for representing causal relations, with edge directions signifying the flow of information between variables. The core objective of causal discovery is to identify both the edges and their orientations based on available data. While certain structures can be recovered from observational data <ref type="bibr" target="#b55">(Verma &amp; Pearl, 1990)</ref>, orienting the full graph often requires additional experiments or interventions.</p><p>Research on causal structure learning from observational data dates back to the 1990s <ref type="bibr" target="#b55">(Verma &amp; Pearl, 1990;</ref><ref type="bibr" target="#b46">Spirtes et al., 1989)</ref>. As a pioneering work in this direction, the PC algorithm <ref type="bibr" target="#b48">(Spirtes et al., 2000)</ref>, named after the authors Peter Spirtes and Clark Glymour, still remains one of most popular and widely used algorithms. It recovers the structure using observational data through conditional independence (CI) tests, with the number of tests being exponential in the degree of the graph. Following this, many causal discovery algorithms emerged <ref type="bibr" target="#b28">(Kalisch &amp; Bühlman, 2007;</ref><ref type="bibr" target="#b4">Brenner &amp; Sontag, 2013;</ref><ref type="bibr" target="#b0">Alonso-Barba et al., 2013;</ref><ref type="bibr" target="#b42">Schulte et al., 2010)</ref>, accommodating diverse and more general settings, including the presence of latent variables <ref type="bibr" target="#b47">(Spirtes et al., 1999;</ref><ref type="bibr">2013)</ref> and interventional data <ref type="bibr" target="#b17">(Eberhardt et al., 2005;</ref><ref type="bibr" target="#b18">2006)</ref>. However, a common challenge shared by these algorithms is their reliance, to different extents, on an exponential number of CI tests in certain graph parameters. This inherent dependence on an exponential number of tests poses practical challenges, making them unsuitable for many real-world scenarios. Moreover, it suggests that achieving exact causal structure learning can be highly challenging.</p><p>As performing exponential number of tests is limited in many applications, it motivates us to study the following question:</p><p>What useful information about the underlying causal graph can be inferred with fewer conditional independence tests?</p><p>Aligned with this motivation, our work also explores the role of interventions in the structure learning process.</p><p>In our work, we study these questions under standard Markov, faithfulness and causal sufficiency assumptions <ref type="bibr" target="#b32">(Lauritzen, 1996;</ref><ref type="bibr" target="#b48">Spirtes et al., 2000)</ref>. The primary contribution of our work is an efficient algorithm that uses a polynomial number of CI tests and recovers a representation of the underlying causal graph with observational and optionally interventional datasets. This representation consists of a partition of the vertices and a DAG defined over its components which is consistent with the underlying causal graph. In addition, our representation is designed to avoid dummy partitions that group all the vertices into a single component. The definition of our representation ensures that the components in the partition satisfy several additional properties, guaranteeing that each component either contains a single vertex or comprises an edge that could only be oriented after an intervention is performed on one of its endpoints. We refer to this representation as the Causally Consistent Partition Graph (CCPG) representation.</p><p>An important implication of our results is that if the underlying causal graph is fully identifiable using only observational data, our algorithm yields a CCPG with a partition containing components, each of which is of size one. The size-one property of each component means that our algorithm recovers the true causal graph using only a polynomial number of conditional independence tests. We extend this result in the presence of interventions and provide an algorithm that recovers the true causal graph, when the set of interventions provided is sufficient to identify the underlying causal graph. To the best of our knowledge, our algorithms present the first to guarantee recovering the true causal graph using a polynomial number of tests when the graph is either entirely identifiable from observational data or with an additional set of interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>Efficient algorithms for causal structure learning <ref type="bibr" target="#b48">(Spirtes et al., 2000;</ref><ref type="bibr" target="#b11">Claassen et al., 2013)</ref> exist for constant bounded degree graphs, recovering the causal graph with a polynomial number of CI tests. For general causal graphs, current methods often entail an exponential number of CI tests, where <ref type="bibr" target="#b57">(Xie &amp; Geng, 2008;</ref><ref type="bibr" target="#b58">Zhang et al., 2024)</ref> aimed to to reduce such complexity. For Bayesian network learning, finding a minimal Bayesian network is NP-hard, even with a constant-time CI oracle and nodes with at most k ≥ 3 parents. <ref type="bibr" target="#b7">Chickering et al. (2004)</ref> demonstrated this hardness through a polynomial reduction from the NP-complete problem, Degree-Bounded Feedback Arc Set. These findings highlight the contrast between causal structure and minimal Bayesian network learning, suggesting that causal structure learning is notably more straightforward. Our results further reinforce this notion by identifying a special class of causal graphs that can be recovered with a polynomial number of conditional independence tests. For other hardness results on Bayesian network learning, we refer readers to <ref type="bibr" target="#b3">Bouckaert (1994)</ref>; <ref type="bibr" target="#b7">Chickering et al. (2004)</ref> and references therein.</p><p>Learning causal relationships from observational data <ref type="bibr" target="#b55">(Verma &amp; Pearl, 1990;</ref><ref type="bibr" target="#b46">Spirtes et al., 1989;</ref><ref type="bibr">2000;</ref><ref type="bibr" target="#b6">Chickering, 2002;</ref><ref type="bibr" target="#b21">Geiger &amp; Heckerman, 2002;</ref><ref type="bibr" target="#b35">Nandy et al., 2018)</ref> and interventional data <ref type="bibr" target="#b15">(Eberhardt, 2010;</ref><ref type="bibr" target="#b27">Hu et al., 2014;</ref><ref type="bibr" target="#b43">Shanmugam et al., 2015;</ref><ref type="bibr" target="#b24">Greenewald et al., 2019;</ref><ref type="bibr" target="#b52">Squires et al., 2020;</ref><ref type="bibr" target="#b10">Choo et al., 2022;</ref><ref type="bibr" target="#b9">Choo &amp; Shiragur, 2023;</ref><ref type="bibr" target="#b44">Shiragur et al., 2024)</ref> is a well studied problem with a rich literature. We encourage interested readers to explore <ref type="bibr" target="#b23">Glymour et al. (2019)</ref>; <ref type="bibr" target="#b51">Squires &amp; Uhler (2022)</ref> and references therein for a more comprehensive understanding and further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Organization</head><p>The rest of our paper is organized as follows. Section 2 is our preliminary section. In Section 3, we provide all the main results of the paper. In Section 4 and Section 5 combined, we provide our CCPG recovery algorithm when just observational data is available. In Section 6, we extend our results to the case of interventions. We provide numerical results in Section 7. Finally in Section 8, we conclude with a short discussion and few open directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Definitions</head><p>Let G be a directed acyclic graph (DAG) on n vertices in V . For a vertex v ∈ V , let Pa(v), Anc(v), and Des(v) denote the parents, ancestors, and descendants of v respectively. Let Anc A v-structure refers to three distinct vertices u, v, w such that u → v ← w and u, w are not adjacent. An edge u → v is a covered edge <ref type="bibr" target="#b5">(Chickering, 1995)</ref> if Pa[u] = Pa(v). A path in G is a list of distinct vertices, where consecutive vertices are adjacent. We can associate a topological ordering π : V → [n] to any DAG G such that any u → v in G satisfy π(u) &lt; π(v). Note that such topological ordering is not necessarily unique.</p><formula xml:id="formula_0">[v] = Anc(v) ∪ {v} and Des[v] = Des(v) ∪ {v}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">D-Separation and Conditional Independence</head><p>DAGs are commonly used in causality <ref type="bibr" target="#b37">(Pearl, 2009)</ref>, where vertices represent random variables and their joint distribution P factorizes according to the DAG:</p><formula xml:id="formula_1">P (v 1 , . . . , v n ) = n i=1 P (v i | Pa(v i ))</formula><p>. This factorization entails a set of conditional independencies (CIs) in the observational distribution P . These CI relations are fully characterized by d-separation <ref type="bibr" target="#b22">(Geiger &amp; Pearl, 1990)</ref>. Formally, for disjoint vertex sets A, B, C ⊂ V , sets A, B are d-separated by C   <ref type="bibr" target="#b22">(Geiger &amp; Pearl, 1990)</ref>). Under the faithfulness assumption, the reverse also holds, i.e., all CI relations in P are implied by d-separation in G.</p><p>Setup. In this work, we assume that the causal DAG G is unknown. But we assume causal sufficiency (i.e., no latent confounders), faithfulness and access to enough samples from P to determine if A ⊥ B | C for any A, B, C ⊂ V . As all CIs are implied by d-separations, we may infer information about G using these tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Interventions</head><p>An intervention I ⊂ V is an experiment where the conditional distributions P (v | Pa(v)) for v ∈ I are changed into P I (v).<ref type="foot" target="#foot_2">foot_2</ref> Such interventions eliminate the dependency between v and Pa(v). Let G I denote the modified version of G, where all incoming edges to v ∈ I are removed. Let P I denote the interventional distribution, i.e.,</p><formula xml:id="formula_2">P I (v 1 , . . . , v n ) = v∈I P I (v) v̸ ∈I P (v | Pa(v)). Then P I factorizes with respect to G I . We denote A ⊥ I B | C for CI tests in the interventional distribution P I .</formula><p>Setup with Interventions. Similar to the observational setting, we assume faithfulness of P I to G I and access to enough samples from</p><formula xml:id="formula_3">P I to determine if A ⊥ I B | C.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Verifying Intervention Sets and Covered Edges</head><p>When it is possible to perform any number of CI tests: with observational data, a DAG G is in general only identifiable up to its skeleton, v-structures <ref type="bibr" target="#b1">(Andersson et al., 1997)</ref>, and possibly additional edges given by the Meek rules <ref type="bibr" target="#b34">(Meek, 1995)</ref>. Identifiability can be improved with interventional data <ref type="bibr" target="#b25">(Hauser &amp; Bühlmann, 2012)</ref>, where I allows us to infer the edge orientation of any edge cut by I and V \ I.</p><p>A verifying intervention set I for a DAG G <ref type="bibr" target="#b10">(Choo et al., 2022)</ref> is a set of interventions that fully orients G, possibly with repeated applications of the Meek rules. We will make use of the following result in our work.</p><p>Proposition 2.1 (Theorem 9 in <ref type="bibr" target="#b10">(Choo et al., 2022)</ref>). Set I is a verifying intervention set if and only if for every covered edge u → v in G, there is |I ∩ {u, v}| = 1 for some I ∈ I.</p><p>The verification number ν(G) is defined as the minimum size of any verifying intervention set of G. This proposition tells us that ν(G) equals to the minimum size of any vertex cover of the covered edges in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Results</head><p>Here we present our main findings. As highlighted in the introduction, the key contribution of our work lies in recovering a representation of the underlying causal graph that satisfies various desirable properties with very few CI tests. We now provide a formal definition of this representation.</p><formula xml:id="formula_4">Definition 3.1 (CCPG &amp; I-CCPG). A Causally Consistent Partition Graph (CCPG) representation of a DAG G on V consists of a partition of V into components V 1 , . . . , V k</formula><p>and a DAG D between the components such that, (intra-component property):</p><formula xml:id="formula_5">for each i ∈ [k], it holds that |src(V i )| = 1. Furthermore, if |V i | &gt; 1, then G[V i ] has at least one covered edge. (inter-component property): D is topologically ordered, i.e., i → j in D only if V i &lt; V j . It is also consistent with G: (1) if there is no directed edge i → j in D, then there are no edges between V i and V j in G; (2) if there is a directed edge i → j in D, then there is u ∈ V i such that u ∈ Pa(src(V j )).</formula><p>We further define an Interventional Causally Consistent Partition Graph (I-CCPG) representation of G with respect to an intervention set I: an I-CCPG is a CCPG representation of G that additionally satisfies the following strong intracomponent condition: In Definition 3.1, the first property prefers finer partitions, while the second property ensures consistency. Formally, we can show the following properties of these representations, which establish the significance of CCPGs. Proofs for all lemmas in this section can be found in Appendix A. Lemma 3.2 (Properties of CCPG). For any intervention set I (including ∅), the following arguments hold:</p><formula xml:id="formula_6">for each i ∈ [k], if |V i | &gt; 1 then G[V i ] has at least one unintervened 4 covered edge.</formula><formula xml:id="formula_7">• D = G (i.e., partitioning V into individual vertices) is a valid I-CCPG of G. • If the verification number of G is zero, i.e., ν(G) = 0, then D = G is the unique valid I-CCPG of G. • If I is a verifying intervention set of G, then D = G is the unique valid I-CCPG of G.</formula><p>The key algorithmic contribution of our work, proven in Section 5, lies in an efficient algorithm that learns a valid CCPG with only polynomial number of CI tests. Theorem 3.3 (Learning CCPG). Given observational data, there exists an efficient algorithm that performs at most O(n<ref type="foot" target="#foot_4">foot_4</ref> ) CI tests, and outputs a CCPG representation.</p><p>This result extends to the interventional setting as follows; the proof is given in Section 6. We remark here that Eberhardt et al. ( <ref type="formula">2012</ref>) provides a construction of verifying intervention set of size log 2 (n) that is independent of the underlying DAG G. Together with Corollary 3.5, this implies that our algorithm can learn the full causal graph with at most O(n 5 ) CI tests.</p><p>To the best of our knowledge, our results present the first formal characterization of the information recoverable about general causal graphs using polynomial number of CI tests.</p><p>Prior works showed that it is possible to learn sparse causal graphs with n O(k) CI tests <ref type="bibr" target="#b11">(Claassen et al., 2013;</ref><ref type="bibr" target="#b48">Spirtes et al., 2000)</ref>. Here k is an upper bound on the vertex degrees. Note that, the number of CI tests our algorithm requires, O(n 5 ), is a polynomial of n that is independent of any graph parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proxy V-structure and Meek Rule Statements</head><p>In our derivations, we will make use of the following results, which we believe is of separate interest well beyond the scope of this work. The results stated above serve as proxy statements of vstructure and Meek Rule 1. Given the CI tests in the preceding lemmas, if we additionally have confirmed adjacencies between specific pairs of vertices, stronger statements could be made; e.g.,in Lemma 3.6, we could conclude that z is a child (or descendant) of both u and v, and in Lemma 3.8 that w is a child of v. However, since our lemma statements do not assume any knowledge of adjacency, we can only ascertain weaker statements, in the first case that u and v are not descendants of z, and in the second case that v is not a descendant of w.</p><p>While our proxy results reveal weaker relationships among variables, they achieve this using a constant number of CI tests. Uncovering stronger relationships requires adjacency information, which may entail an exponential number of CI tests. Our main contribution lies in leveraging these weaker relationships, along with other favorable properties embedded in our algorithm, to implement the prefix vertex procedure and reconstruct the CCPG representation of the underlying causal graph using few CI tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prefix Vertex Set</head><p>The core component of our CCPG algorithm involves a procedure that produces a series of prefix vertex sets. We now show how such prefix vertex sets can be learned by performing few CI tests.</p><p>This will be helpful to obtain a CCPG representation of G,</p><formula xml:id="formula_8">since the components V 1 , . . . , V k satisfy that V 1 ∪ • • • ∪ V i is a prefix vertex set for all i ∈ [k].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Algorithm for Learning</head><p>We begin by presenting our algorithm for learning a prefix vertex set. This algorithm takes as input a prefix vertex set S ⊊ V (which can be ∅) and produces a larger prefix vertex set S ′ .</p><p>The analysis of Algorithm 1 will be provided in the next section. For an input prefix vertex set S ⊊ V , it makes use of three types of CI tests, which we formalize below.<ref type="foot" target="#foot_5">foot_5</ref>  We also exclude any type-III set F S , defined as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Correctness and Guarantees</head><p>Algorithm 1 satisfies the following guarantees. All omitted proofs can be found in Appendix B. Theorem 4.4. Algorithm 1 outputs a prefix vertex set in O(n 4 ) number of CI tests. In addition, this prefix vertex set contains all the remaining source nodes, i.e., src( S) ⊂ S ′ .</p><p>For its proof, we will make use of the following properties of the type-I, II, and III sets (illustrated in Figure <ref type="figure" target="#fig_9">3</ref>). Next we show that S ′ is a prefix vertex set. For this, we only need to show that ∀w ∈ S′ and y ∈ Des(w), it holds that </p><formula xml:id="formula_9">y ∈ S′ . Since S′ = D S ∪ E S ∪ F S ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relation to Covered Edges</head><p>In Theorem 4.4, we showed that src( S) ⊂ S ′ . In fact, when there are no covered edges coming from src( S), one can show that S ′ = src( S) ∪ S via the following Lemma 4.7.</p><p>Lemma 4.7. Let S be a prefix vertex set. For w ∈ S \ D S , if w ̸ ∈ src( S) and there is no covered edge from Anc[w] ∩ src( S) to Anc[w], then w ∈ E S ∪ F S .</p><p>Corollary 4.8. Let S be a prefix vertex set. If there is no covered edge in S, then S ′ = src( S) ∪ S.</p><p>This result will be useful when deriving CCPG representations using Algorithm 1. To see this, consider the simple case where there is no covered edge in G. Then running Algorithm 1 with S = ∅ we can learn src(V ) by Corollary 4.8. Then running Algorithm 1 with S = src(V ), we can learn the source vertices of V \ src(V ). Applying this iteratively, we can obtain the ground-truth topological order of G. As a consequence, one can easily learn G,<ref type="foot" target="#foot_6">foot_6</ref> which is the sole CCPG representation as there is no covered edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning Causally Consistent Partition Graph Representations</head><p>We now present our algorithm for learning causally consistent partition graph representations. All omitted proofs can be found in Appendix C. Run Algorithm 1 on S to obtain S ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Add S ′ \ S to the end of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update S = S ′ . 8: end while 9: Initialize l 1 = 1. 10: for S i in S = [S 1 , . . . , S m ] do 11:</p><p>Create empty graph T on vertices in S i .</p><p>12: </p><formula xml:id="formula_10">Add v -w to T iff v ̸ ⊥ w | S 1 ∪ • • • ∪ S i-1 . 13: Split S i into components V li , V li+1 , . . . , V</formula><formula xml:id="formula_11">Add i → j to D iff V i ̸ ⊥ V j | V 1 ∪ • • • ∪ V j-1 . 18: end for 19: return V 1 , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, V lm+1 and D</head><p>To show that the components we obtain in the second part Based on this result, we can establish that Algorithm 2 outputs a CCPG representation by proving Theorem 3.3.</p><p>Proof of Theorem 3.3. We show that Algorithm 2 outputs a CCPG representation of G. Since it runs in polynomial time and uses O(n 5 ) CI tests, this will prove Theorem 3.3. Thus by Lemma 5.1, each of V li , V li+1 , . . . , V li+1 corresponds to Des[s] ∩ S i for some s ∈ src( S).</p><formula xml:id="formula_12">Intra-component Property. Consider S i . Denote S 1 ∪ • • • ∪ S i-1 = S. We will show that S i is split into Des[s] ∩ S i for each s ∈ src( S). Since S i ⊂ S, we have S i = ∪ s∈src( S) (Des[s] ∩ S i ). By</formula><p>If |Des[s] ∩ S i | &gt; 1, then it contains a vertex that is not s. Suppose w ∈ Des(s) ∩ S i . By Lemmas 4.7 and 5.1, there is a covered edge from s to some vertex x ∈ Anc[w]. Since S and S ∪ S i are prefix vertex sets, s, w ∈ S i implies x ∈ S i . Thus x ∈ Des[s] ∩ S i and there is a covered edge s → x in Des[s] ∩ S i . Thus, we have proven that each component in V 1 , . . . , V lm+1 satisfies the first property of CCPG.</p><p>Inter-component Property. For each V i , denote the subset that it came from as S hi , i.e., V i ⊂ S hi . By the construction of D in Algorithm 2, no edge i → j in D means either i &gt; j</p><formula xml:id="formula_13">or V i ⊥ V j | V 1 ∪ • • • ∪ V j-1 . If i &gt; j and h i ̸ = h j , then by the fact that S 1 ∪ • • • ∪ S hi is a prefix vertex set, there is no edge from V i to V j in G. If h i = h j = h, then we know that there exists s i ̸ = s j such that V i = Des[s i ] ∩ S h and V j = Des[s j ] ∩ S h . If there is an edge from V i to V j in G, denoted as v → w, then w ∈ Des[v] ⊂ Des[s i ] which means V i ∩ V j ̸ = ∅, a contradiction. Therefore there is no edge from V i to V j in G. If V i ⊥ V j | V 1 ∪ • • • ∪ V j-1 , then clearly there is no edge from V i to V j in G. Thus when there is no i → j in D, there is no edge from V i to V j in G.</formula><p>If there is an edge i → j in D, then we have i &lt; j and</p><formula xml:id="formula_14">V i ̸ ⊥ V j | V 1 ∪ • • • ∪ V j-1 . Note that since S 1 ∪ • • • ∪ S hj is prefixed, it holds that Pa(V j ) ⊂ S 1 ∪ • • • ∪ S hj and Des[V j ] ∩ (S 1 ∪ . . . S hj -1 ) = ∅.</formula><p>As shown above, there is no edge between any other V j ′ ⊂ S hj and V j . Thus we have</p><formula xml:id="formula_15">Pa(V j ) ⊂ V 1 ∪• • •∪V j and Des[V j ]∩(V 1 ∪• • •∪V j-1 ) = ∅. Similarly Pa(V i ) ⊂ V 1 ∪ • • • ∪ V i .</formula><p>Thus by the local Markov property and Bayes rule,</p><formula xml:id="formula_16">8 V i ̸ ⊥ V j | V 1 ∪ • • • ∪ V j-1 only if there is a direct edge u → v from u ∈ V i to v ∈ V j .</formula><p>We now show that there is also an edge u → s where {s} = src(V j ). Assume on the contrary that there is no edge u → s. Since s is the source node of V j and thus a source node of S hj , we have from Pa(</p><formula xml:id="formula_17">V j ) ⊂ S 1 ∪ • • • ∪ S hj that Pa(s) ⊂ S 1 ∪ • • • ∪ S hj -1 . In addition, Des[s] ∩ (S 1 ∪ • • • ∪ S hj -1 ) = ∅. Thus by the local Markov property, u ⊥ s | S 1 ∪ • • • ∪ S hj -1 . However as u → v and v ∈ Des[s], we have u ̸ ⊥ s | S 1 ∪ • • • ∪ S hj -1 ∪ {v}. As a consequence, v ∈ D S1∪•••∪S h j -1 . By Algorithm 1, it is impossible that v ∈ S</formula><p>hj , a contradiction. Therefore we must have u ∈ Pa(s). This proves that D satisfies the second property of CCPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Interventions</head><p>In Sections 4 and 5, we showed how to learn a CCPG representation using observational data. Here we generalize these methods to interventions. This results in a more refined I-CCPG representation of G. All omitted proofs can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Learning Refined Prefix Vertex Sets</head><p>In Section 4.1, we obtained a prefix vertex set by excluding three types of sets D S , E S and F S . With interventions, we can exclude an additional set. To define it, we first characterize what can be learned using interventions.</p><p>8 See Lemma A.1 in Appendix A. Note that when the number of CI tests is not restricted, one can learn all edges cut by I as well as their orientations. Lemma 6.1 shows that it is possible to learn the joint set of descendants using O(n 2 ) CI tests.</p><p>With this set and a few more CI tests, it is possible to learn additional directional information.</p><p>Lemma 6.2. Let S be a prefix subset. Given an intervention I and v ∈ I\S, denote</p><formula xml:id="formula_18">H I S (v) = {u ̸ ∈ S∪Des[I\S] : u ̸ ⊥ v | V \ Des[I \ S]}. 9 Then P a(v) \ (S ∪ Des[I \ S]) ⊆ H I S (v) ⊆ Anc(v) \ (S ∪ Des[I \ S]).</formula><p>Such sets can be useful when deciding if a target of I \ S should be excluded when learning a prefix vertex subset S ′ ⊃ S. Formally, we define the type-IV set as follows.</p><p>Definition 6.3 (Type-IV Set J I S ). Let S be a prefix vertex set. For an intervention I, let</p><formula xml:id="formula_19">J I S = Des(I \ S) ∪ {v ∈ I \ S : H I S (v) ∩ S ̸ = ∅}.</formula><p>Analogous to Lemma 4.5, we can show that J I S satisfies similar properties (illustrated in Figure <ref type="figure" target="#fig_10">5</ref>). Lemma 6.4. Let S be a prefix vertex set. Then ∀w ∈ J I S , we have Des(w) ⊆ J I S . Furthermore, J I S ∩ src( S) = ∅.</p><p>Therefore we can exclude J I S as well, which results in the following modification of Algorithm 1 and for which we can show similar guarantees using Lemma 6.4. Compute type-IV set J I S . 5: end for 6: Compute type-I set D S . 7: Compute type-II and III sets E S , F S . 8: Let U ′ = S \ ∪ I∈I J I S ∪ D S ∪ E S ∪ F S . 9: return S ′ = S ∪ U ′ . Theorem 6.5. Algorithm 3 outputs a prefix vertex set in O(n 4 )+|I|O(n 2 ) CI tests. In addition, this prefix vertex set contains all the remaining source nodes, i.e., src( S) ⊂ S ′ .</p><p>Proof. Similar to the proof of Theorem 3.3, we can use the additional Lemma 6.4 to show that: (1) S ′ is a prefix vertex set, (2) it contains src( S). We now bound the number of CI tests performed by Algorithm 3: note that it bears the additional need to compute J I S compared to Algorithm 1. By Lemmas 6.1 and 6.2, computing J I S for each I ∈ I takes O(n 2 ). Thus the total complexity is O(n 4 )+|I|O(n 2 ).</p><p>In addition, we can show the following property for covered edges coming from src( S). Lemma 6.6. Let S be a prefix vertex set, v ∈ src( S) and v → w be a covered edge. Given any intervention I, if |I ∩ {v, w}| = 1, then w ∈ J I S .</p><p>Proof. If I ∩ {v, w} = {v}, then w ∈ Des(I \ S). If I ∩ {v, w} = {w}, then w ∈ I \ S. We will show that H I S (w)∩ S ̸ = ∅. This in turn proves that w ∈ J I S . Note that since v ∈ src( S), we have v ̸ ∈ S ∪Des[I \S]. Furthermore, since v → w is an edge, we have v ∈ Pa(w). Thus by Lemma 6.2, we have v ∈ Pa(w) \ (S ∪ Des[I \ S]) ⊆ H I S (w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Learning I-CCPG Representations</head><p>We obtain the final algorithm by plugging Algorithm 3 into Algorithm 2, i.e., replacing Algorithm 1 in line 5 by Algorithm 3.</p><p>Proof of Theorem 3.4. By Theorem 6.5, for any i, set S = S 1 ∪ • • • ∪ S i-1 is a prefix vertex set. Therefore using the proof in Theorem 3.3 and the fact that src( S) ⊂ S ∪ S i (Theorem 6.5), we immediately have |src(V i )| = 1 and D satisfies the second property of I-CCPG in Definition 3.1.</p><p>We now show that when</p><formula xml:id="formula_20">|V i | &gt; 1, subgraph G[V i</formula><p>] has at least one unintervened covered edge. Suppose on the contrary that G[V i ] has no unintervened covered edge. Denote {s} = src(V i ). Let w ∈ V i such that w ̸ = s. Similar to Theorem 3.3, we can show that w ∈ Des(s). By Lemma 4.7, there is a covered edge from s to some vertex x ∈ Anc[w] and x ∈ V i . Since G[V i ] has no unintervened covered edge, s → x is intervened by some I ∈ I. Then by Lemma 6.6, we have x ∈ J I S . This contradicts x ∈ V i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>In this section, we test our proposed method and compare it to existing causal discovery methods on synthetic data and a toy real-world example. Source code for these results can be found at <ref type="url" target="https://github.com/uhlerlab/CCPG">https://github.com/uhlerlab/CCPG</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Synthetic Data</head><p>In these experiments, we consider the observational setting and generate samples from linear causal models with additive Gaussian noise, governed by identifiable causal graphs, in particular, in-star-shaped DAGs with varying number of nodes. In such settings, with enough samples, our algorithm is guaranteed to return the ground-truth DAG (Corollary 3.5).</p><p>We compare our Algorithm 2, termed CCPG, to other constraint-based and hybrid methods that rely on conditional independence tests. The constraint-based methods we include are: PC <ref type="bibr" target="#b48">(Spirtes et al., 2000)</ref>, FCI <ref type="bibr" target="#b47">(Spirtes et al., 1999)</ref>, and RFCI <ref type="bibr" target="#b12">(Colombo et al., 2012)</ref>, where we use the order-independent variants from <ref type="bibr">Colombo et al. (2014) (i.e., "stable" versions)</ref>. The hybrid methods we included are: GSP with depth = 4 and depth = ∞ <ref type="bibr" target="#b45">(Solus et al., 2021)</ref>. Implementation details can be found in Appendix E. Runtime Analysis. To test the computational efficiency of our method, we generated 100k samples across graphs of different sizes and reported the runtime averaged across five runs. Figure <ref type="figure">6</ref> shows that CCPG is as fast as hybrid methods (GSP) and significantly more efficient than constraint-based methods (PC, FCI, and RFCI), which can only scale to 20 nodes with reasonable runtime.</p><p>Sample Complexity Analysis. To test the sample efficiency of CCPG, we consider a 10-node in-star-shaped DAG. For each method, we increase the number of samples until it returns the ground-truth DAG and repeat this procedure for five runs. Figure <ref type="figure">7</ref> shows that CCPG requires the least number of samples to recover the true graph.</p><p>In general, constraint-based methods (PC, FCI, and RFCI) have better sample efficiency than hybrid methods, while being significantly more runtime inefficient. In comparison, CCPG, with its polynomial-number of CI test guarantee, enjoys low sample complexity and low runtime complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">A Real-world Example</head><p>To illustrate the utility of the coarser representation learned by CCPG in real-world settings, we include a simple 6variable Airfoil example <ref type="bibr" target="#b2">(Asuncion &amp; Newman, 2007;</ref><ref type="bibr" target="#b31">Lam et al., 2022)</ref>; see <ref type="bibr" target="#b31">Lam et al. (2022)</ref> for a detailed description of this example. Although there is no known ground-truth DAG in this setting, a few causal relations are known: (1) velocity, chord, and attack should be source nodes;</p><p>(2) pressure is downstream of all other nodes. graph in Figure <ref type="figure">9</ref>, CCPG seems to be more consistent with the known causal relations while it contains less information due to a coarser representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>In our work, we studied causal structure learning under the constraint of fewer CI tests. Since exact structure learning may demand an exponential number of CI tests, we defined a representation (CCPG) that captures partial but crucial information about the underlying causal graph. Moreover, we provided an efficient algorithm that recovers a CCPG representation in a polynomial number of CI tests. This result enabled us to design efficient algorithms for the full recovery of causal graphs in two specific settings, utilizing only a polynomial number of CI tests.</p><p>We hope that our work will motivate further exploration of the causal discovery problem under the constraint of fewer CI tests, extending to various settings, including those involving latent variables. Furthermore, our research establishes a foundation for addressing the search problem<ref type="foot" target="#foot_8">foot_8</ref> with reduced tests, suggesting the potential existence of search algorithms capable of recovering the causal graph with a polynomial number of independence tests while performing an approximately optimal number of interventions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For a set</head><label></label><figDesc>of vertices S ⊂ V , denote Pa(S) = ∪ v∈S Pa(s). Similarly define Anc(S), Des(S), Anc[S], and Des[S]. We write src(S) as the set of source nodes within S, that is, src(S) = {v ∈ S | Anc(v) ∩ S = ∅} . Denote S = V \S. Let G[S] be a subgraph of G by removing all vertices in S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>if and only if any path connecting A and B in G is inactive given C. A path is inactive given C when it has a collider 1 d ̸ ∈ Anc[C] or a non-collider c ∈ C; otherwise the path is active given C. Figure 1 illustrates these concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (Left). {1} and {4} are d-separated by {2}, as all paths are inactive given {2}. (Right). {1} and {4} are not d-separated by {2, 3}, as path 1 → 3 ← 4 is active given {2, 3} by collider 3.</figDesc><graphic coords="3,61.85,128.85,218.70,66.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2 illustrates these concepts. Note that when I = ∅, I-CCPG reduces to CCPG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of CCPG &amp; I-CCPG. (Left). Ground-truth G. (Right). A CCPG representation of G, where V1, V2, V3 are indicated by green boxes and D is illustrated in chalk strokes. Vertices 3, 4 can be in one component as 3 → 4 is a covered edge. For I = {4}, the only I-CCPG is G itself (due to strong intra-component condition in Definition 3.1).</figDesc><graphic coords="4,61.85,67.06,218.70,84.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Theorem 3.4 (I-Learning CCPG). Given observational data and interventional data from interventions in I, there exists an efficient algorithm that performs at most O(n 5 ) + |I| • O(n 3 ) CI tests, and outputs an I-CCPG representation. Combining these results with the properties of CCPG in Lemma 3.2, this provides an efficient algorithm for learning the causal graph with polynomial number of conditional independence tests under certain cases, detailed below. Corollary 3.5 (Causal Discovery with Polynomial CI Tests). For a DAG G and its verifying intervention set I (can be ∅), our algorithm recovers the full causal graph with at most O(n 5 ) + |I| • O(n 3 ) CI tests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Lemma 3.6 (Proxy V-Structure). Let S ⊆ V and u, v, z ∈ V \S. If u ⊥ v|S and u ̸ ⊥ v|S ∪ {z}, then u, v ̸ ∈ Des[z]. 5 Definition 3.7 (Prefix Vertex Set). We call S ⊆ V a prefix vertex set if it satisfies: for all w ∈ S, Anc[w] ∩ S = ∅ (vertices in S appear first in the topological order). Lemma 3.8 (Proxy Meek Rule 1). Let S be a prefix subset. If u, v and w are such that u ∈ S, v, w ̸ ∈ S and u ̸ ⊥ v|S and u ⊥ w|S ∪ {v} then v ̸ ∈ Des[w].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Definition 4.1 (Type-I Set D S ). For all w ∈ S, let w ∈ D S if and only if u ⊥ v | S and u ̸ ⊥ v | S ∪ {w} for some u ∈ V and v ∈ S. By the proxy v-structure in Lemma 3.6, these two CI tests indicate that v ̸ ∈ Des[w]. Therefore w can potentially be a descendant of v. Thus D S contains vertices that are potential descendants of some other vertex in S. We will rule out this set when searching for prefix S ′ ⊋ S. Similarly, we can define a type-II set E S . Definition 4.2 (Type-II Set E S ). For all w ∈ S \ D S , let w ∈ E S if and only if u ⊥ v ′ | S ∪ {v} and u ̸ ⊥ v ′ | S ∪ {v, w} for some u ∈ S and v, v ′ ∈ S \ D S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Definition 4.3 (Type-III Set F S ). For all w ∈ S \ D S , let w ∈ F S if and only if u ̸ ⊥ v | S, u ⊥ w | S ∪ {v}, and v ̸ ⊥ w | S for some u ∈ S and v ∈ S \ D S . By the proxy Meek Rule 1 in Lemma 3.8, the first two CI tests u ̸ ⊥ v | S and u ⊥ w | S ∪ {v} guarantee that v ̸ ∈ Des[w]. The remaining CI test v ̸ ⊥ w | S is to ensure that we do not exclude too many vertices in F S , in particular src( S), as we show in the next section. Algorithm 1 Learning a Prefix Vertex Set 1: Input: A prefix vertex set S ⊊ V . CI queries from G. 2: Output: A prefix vertex set S ′ such that S ′ ⊋ S. 3: Compute type-I set D S . 4: Compute type-II and III sets E S , F S . 5: Let U = S \ (D S ∪ E S ∪ F S ). 6: return S ′ = S ∪ U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. DS, ES, FS satisfy that (1) they contain all downstream vertices of any vertex in them; (2) they do not intersect with src( S).</figDesc><graphic coords="5,355.16,67.06,136.08,146.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 4. 5 .</head><label>5</label><figDesc>Let S be a prefix vertex set. If w ∈ D S , then Des[w] ⊂ D S . Furthermore, D S ∩ src( S) = ∅. The same properties hold for E S . Lemma 4.6. Let S be a prefix vertex set. If w ∈ F S , then Des[w] ⊂ E S ∪ F S . Furthermore, F S ∩ src( S) = ∅. Proof of Theorem 4.4. We first show that S ′ returned by Algorithm 1 satisfies src( S) ⊂ S ′ . By Lemmas 4.5 and 4.6, we have (D S ∪ E S ∪ F S ) ∩ src( S) = ∅. As S′ = D S ∪ E S ∪ F S , it must hold that src( S) ⊂ S ′ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Algorithm 2 contains three parts, indicated by colored boxes below. In the first part, we use Algorithm 1 iteratively to learn prefix vertex sets of the form ∅ ⊊ . . . S ⊊ S ′ ⊊ • • • ⊊ V . In the second part, we break each S ′ \ S into smaller components that are pairwise independent given S. As we will see, these components satisfy the property of CCPG in Definition 3.1. In the third part, we build the acyclic graph between the CCPG components. Algorithm 2 Learning a CCPG Representation 1: Input: CI queries from G. 2: Output: A CCPG representation of G. 3: Set S = ∅ and S as empty ordered list. 4: while S ̸ = V do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Illustration of S \ DS (inside the dashed box). It can be split into connected subgraphs based on vertices in src( S) (indicated by the fill color of each vertex in S \ DS).</figDesc><graphic coords="6,321.14,67.06,204.12,189.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Theorem 4.4, we know that S is a prefix vertex set. Since S i ⊂ S \ D S , by Lemma 5.1, Des[s] ∩ S i for different s ∈ src( S) are disjoint. Furthermore, by Theorem 4.4, these disjoint sets are non-empty because s ∈ Des[s] ∩ S i . Therefore src(Des[s]∩S i ) = {s} and we have |src(Des[s]∩S i )| = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of J I S , where I is indicated by the purple circle. J I S satisfies similar properties as DS, ES and FS.</figDesc><graphic coords="7,318.71,67.06,208.98,188.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Runtime comparison across graphs of different sizes.CCPG is as fast as hybrid methods (GSP) and significantly more efficient than constraint-based methods (PC, FCI, and RFCI). The programming language behind each implementation is indicated by dashed or solid lines (see Appendix E for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Learned coarser representation by CCPG in the Airfoil example.PC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>li+1 based on connected components in T . 14: end for 15: Create empty graph D on vertices in [l m+1 ]. 16: for V i , V j with i &lt; j do</figDesc><table><row><cell>17:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Causal Discovery with Fewer Conditional Independence Tests Algorithm 3 Learning a Prefix Vertex Set (w. Interventions) 1: Input: A prefix vertex set S ⊊ V . CI queries from G and G I for each intervention I ∈ I. 2: Output: A prefix vertex set S ′ such that S ′ ⊋ S. 3: for I ∈ I do</figDesc><table><row><cell>4:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Vertex d is a collider on a path iff • → d ← • on the path.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For simplicity, we also write A ⊥ B | C for potential overlapping sets to denote A ⊥ B | C \ (A ∪ B)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We consider hard interventions in this work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>An edge is intervened by I if only one of the vertices is in I.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Similar argument is also proven in Lemma 1 of<ref type="bibr" target="#b33">Magliacane et al. (2016)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We note that u, v, v ′ and w in the definitions below are mutually distinct. We omit writing this for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For i &lt; j in the topological order, the corresponding edge vi → vj ∈ G iff vi ̸ ⊥ vj | v1, . . . , vj-1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Note that Des[I \ S] can be obtained via Lemma 6.1 and (Des(I \ S) \ (I \ S)) ∪ (I \ S).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>The search problem involves finding the minimum set of interventions that orient the entire causal graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>Similar argument is also proven in Lemma 1 of<ref type="bibr" target="#b33">Magliacane et al. (2016)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>Note that Des[I \ S] can be obtained via Lemma 6.1 and (Des(I \ S) \ (I \ S)) ∪ (I \ S).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers for helpful feedback. J.Z. was partially supported by an <rs type="funder">Apple</rs> <rs type="grantName">AI/ML PhD Fellowship</rs>. K.S. was supported by a fellowship from the <rs type="funder">Eric and Wendy Schmidt Center at the Broad Institute</rs>. C.U. was partially supported by <rs type="funder">NCCIH/NIH</rs> (<rs type="grantNumber">1DP2AT012345</rs>), <rs type="funder">ONR</rs> (<rs type="grantNumber">N00014-22-1-2116</rs>), <rs type="funder">DOE-ASCR</rs> (<rs type="grantNumber">DE-SC0023187</rs>), the <rs type="institution">MIT-IBM Watson AI Lab</rs>, and a <rs type="grantName">Simons Investigator Award</rs>.</p></div>
<div><head>Impact Statement</head><p>This paper presents theoretical work whose goal is to advance the field of causal inference. There are many potential societal applications of our work, none of which we feel must be specifically highlighted here.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HaVB7Kp">
					<orgName type="grant-name">AI/ML PhD Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_MUBNb9p">
					<idno type="grant-number">1DP2AT012345</idno>
				</org>
				<org type="funding" xml:id="_YaMCGWX">
					<idno type="grant-number">N00014-22-1-2116</idno>
				</org>
				<org type="funding" xml:id="_vB9JY5N">
					<idno type="grant-number">DE-SC0023187</idno>
					<orgName type="grant-name">Simons Investigator Award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Useful Lemmas</head><p>A.1. Proof of Lemma 3.2 Lemma 3.2 (Properties of CCPG). For any intervention set I (including ∅), the following arguments hold:</p><p>• D = G (i.e., partitioning V into individual vertices) is a valid I-CCPG of G.</p><p>• If the verification number of G is zero, i.e., ν(G) = 0, then D = G is the unique valid I-CCPG of G.</p><p>• If I is a verifying intervention set of G, then D = G is the unique valid I-CCPG of G.</p><p>Proof. Note that if |V i | = 1, then CCPG is actually G. If |V i | &gt; 1, then it means that there is a covered edge in this subset that is not intervened. By Proposition 2.1, this is impossible when I is a verifying intervention set.</p><p>A.2. Proof of Lemma 3.6 Lemma 3.6 (Proxy V-Structure). Let S ⊆ V and u, v, z ∈ V \S. If u ⊥ v|S and u ̸ ⊥ v|S ∪ {z}, then u, v ̸ ∈ Des[z]. 11</p><p>Proof. Let P be an active path (that carries dependency) between u and v when conditioned on S ∪ {z}. Since u ⊥ v|S and u ̸ ⊥ v|S ∪ {z}, we get that there exist a set of vertices w 1 . . . w k that are colliders on P and satisfy: Des[w i ] ∩ S = ∅ and z ∈ Des[w i ].</p><p>Consider the path P and note that it takes the form</p><p>Since all the colliders on paths P 1 , P 2 are in or have their descendants in S and all non-colliders do not belong to S, we have that P 1 , P 2 are active given S. We prove our lemma using the proof by contradiction strategy. For contradiction, let us assume that one of the vertices in {u, v} belong to the set Des[z] and without loss of generality let that vertex be u. Then since z ∈ Des[w i ] for all i, there must be u ∈ Des[w i ] for all i.</p><p>Since u ∈ Des[w k ], let Q be the directed path in the graph that connects w k to u. Note that all the vertices in path Q are all descendants of w k and they do not belong to the set S (because Des[w k ] ∩ S = ∅). Now consider the path (P 2 , Q) that connects vertices v and u and note that the vertex w is a non-collider on the new path (P 2 , Q). Since P 2 and Q are active paths given S and since w ̸ ∈ S, we immediately get that the path (P 2 , Q) is an active path given S, which further implies that u ̸ ⊥ v|S; a contradiction and we conclude the proof.</p><p>A.3. Proof of Lemma 3.8 Lemma 3.8 (Proxy Meek Rule 1). Let S be a prefix subset. If u, v and w are such that u ∈ S, v, w ̸ ∈ S and u ̸ ⊥ v|S and u ⊥ w|S ∪ {v} then v ̸ ∈ Des[w].</p><p>Proof. Suppose on the contrary that there is u, v, w such that u ∈ S, v, w ̸ ∈ S and u ̸ ⊥ v|S and u ⊥ w|S ∪ {v} and v ∈ Des[w].</p><p>Since u ̸ ⊥ v | S, let P be the active path connecting u and v given S. As u ∈ S and v ̸ ∈ S, there is an edge u</p><p>Now denote the vertex on P that is immediate next to v as x. If x ← v, then there must be a collider on P between u ′ and v as S is a prefix subset where u ′ ∈ S and v ̸ ∈ S. Let y be the collider on P between u ′ and v that is closest to u</p><p>∈ S, a contradiction to S being prefix. Thus there must be x → v on P .</p><p>Since we assumed on the contrary that v ∈ Des[w], we can consider the path Q joined by P and the directed path from w to v. Compared to P , the path P has one additional collider v, and has a few additional colliders that lie between v and w which are not in S (as they are all descendants of w and w ̸ ∈ S). Therefore Q is active given S ∪ {v}. This means u ̸ ⊥ w | S ∪ {v}, a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Additional Lemma</head><p>In addition, we will make use of the following lemma.</p><p>Proof. Assume without loss of generality that the vertices in B have the following topological order b 1 , . . . , b m . Then for any i ∈ [m], by the local Markov property <ref type="bibr" target="#b46">(Spirtes et al., 1989)</ref>, we have A ⊥ b i | C ∪ {b 1 , . . . , b i-1 }. Therefore using Bayes rule, we have Since u ⊥ v | S, the path P is inactive given S. From above we know that all non-colliders on P are not in S. Therefore there exists a collider on P that is not in Anc <ref type="bibr">[S]</ref>. Suppose the leftmost and rightmost such colliders are k, k</p><p>Consider the path Q in the graph by cutting out the parts between k, x (and k ′ , x) on P and replacing them with directed edges from k to x (and from k ′ to x). Compared to P , the additional non-colliders on Q are all on the directed path from k to x (or k ′ to x). They are not in S since k, k ′ / ∈ Anc[S], and thus Q has no non-colliders in S.</p><p>Compared to P , there is no collider on P that is not in Anc[S] and is still on Q by the fact that k, k ′ are leftmost and rightmost colliders on P that are not in Anc[S]. Therefore, x must be a collider on Q, or else Q is active given S and u ̸ ⊥ v | S. Therefore all non-colliders on Q are not in S ∪ {x}. Every collider on Q is either x or a collider of P , which is in</p><p>Next we show that D S ∩ src( S) = ∅: for contradiction assume that there exists a vertex a ∈ src( S) such that a ̸ ∈ S\D S , that is a ∈ src( S) and for some vertex u ∈ V, v ∈ S, v ⊥ u|S and v ̸ ⊥ u|S ∪ {a}.</p><p>Since v ⊥ u|S and v ̸ ⊥ u|S ∪ {a}, there exists a path P between v and u which is inactive when conditioned on S but is active upon conditioning on S ∪ {a}. Moreover, this path contains a vertex b that is a collider on P and satisfies: a ∈ Des </p><p>. However, since v k ∈ S, and since we condition on the set S, this should be a collider for the path Q to be active, which is not possible. Thus we get a contradiction, which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2. TYPE-II SET E S</head><p>Proof of Lemma 4.5 for E S . We first show that if w ∈ E S , it must hold that y ∈ E S for any y ∈ Des(w):</p><p>, w}, by Lemma 3.6 (note that the set "S" in the exposition of Lemma 3.6 can be an arbitrary subset), we know that Proof. We first show that if w ∈ F S , then for any y ∈ Des(w), we have</p><p>Since v ̸ ⊥ w | S, there is an active path between v, w given S. Consider extending this path by the directed path from w to y. Note that none of vertices on the directed path from w to y are in S, since S is prefixed and w ̸ ∈ S. Therefore, this extended path is also active given S, which means v ̸ ⊥ y | S.</p><p>Thus, if y ̸ ∈ F S , then it must hold that u ̸ ⊥ y | S ∪ {v}. This means there is an active path, denoted by P , between u and y given S ∪ {v}. Consider extending this path by the directed path from w to y, denoted as Q (which exists in the graph). Compared to P , the additional non-colliders on Q are not in S ∪ {v}: for S, this is because S is prefix, w ̸ ∈ S, and all additional non-colliders are descendants of w; for v, this is because</p><p>Next we show that F S ∩ src( S) = ∅. Assume on the contrary that w ∈ F S ∩ src( S). Since w ∈ F S , we have u ̸ ⊥ v|S, v ̸ ⊥ w|S and u ⊥ w|S ∪ {v} for some u ∈ S and v ∈ S \ D S . By Lemma 3.8, we have v / ∈ Des[w]. However, since v ̸ ⊥ w | S, there must be an active path P between v and w given S. This path cannot have any vertex in S; otherwise consider the first vertex that is in S; since v ̸ ∈ S and S is prefix, such vertex must be a non-collider which would make P inactive given S. Therefore P is fully in S. Since w ∈ src( S), we must have</p><p>there must be an edge → on P . This means that there must be a collider on P . Since this collider is not in S, it makes P inactive given S, a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Remarks</head><p>The above proofs suffice as intermediate results to show Theorem 4.4, which we proved in Section 4.2.</p><p>Regarding Lemma 4.7 in Section 4.3, we will prove it in Appendix C after proving Lemma 5.1, since it depends on Lemma 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Missing Proofs of Causally Consistent Partition Graph Representations</head><p>C.1. Proof of Lemma 5.1</p><p>To prove Lemma 5.1, we will make use of the following lemma.</p><p>Lemma C.1. Let S ⊆ V be a prefix subset, then the following statements hold:</p><p>• S ∪ U is a prefix subset for any U ⊆ src( S).</p><p>Proof. We first prove condition one. For contradiction, we assume that u ̸ ⊥ v|S, which implies that there exists an active path between u and v when conditioned on S. Let P = u -u 1 . . . u k -v be the path and u 1 , . . . , u k be the vertices along the path. Note that k &gt; 0 as u and v are not connected; because an edge between u and v would mean that one of these vertices is not a source node in S.</p><p>Consider u 1 and note that there are two possibilities u → u 1 or u ← u 1 . We start with the first case u → u 1 . Since S is a prefix subset and u ∈ S, u → u 1 implies that u 1 ∈ S. Since no vertex in S is conditioned upon, we have that the vertex u 1 is not a collider on the path P and we have that the edge u 1 -u 2 is directed as u 1 → u 2 . Repeating the same argument for u 2 and all the other vertices in the path, we see that the path P takes the form</p><p>The previous argument implies that v ∈ Des[u] and therefore does not belong to src( S), which is a contradiction to our assumption that u, v ∈ src( S). Now consider the other case where u ← u 1 . Note that since u ∈ src( S), it is immediate that u 1 ∈ S. Furthermore, irrespective of the orientation between u 1 and vertex u 2 , it holds that vertex u 1 is a non-collider on the path P . Moreover, since P is an active path, u 1 should not be conditioned upon. However, since we condition on S and as u 1 ∈ S, we have a contradiction.</p><p>In the above case analysis, we showed that there does not exist an active path between u and v when conditioned on S, which implies that u ⊥ v|S and we conclude the proof for condition one.</p><p>In the remainder of the proof, we focus our attention on condition two. Consider any subset U ⊆ src( S). For contradiction assume that S ∪ U is not a prefix subset, which implies that there exists a vertex v ∈ V \(S ∪ U ) such that v ∈ Anc[u] for some vertex u ∈ S ∪ U . Since S is a prefix subset, it is immediate that u ̸ ∈ S. Therefore, the only case is that u ∈ U . Note that both u and v belong to the set S and v ∈ Anc(u); both these expressions combined contradict the fact that u ∈ src( S). Therefore, it should be the case that S ∪ U is a prefix subset and we conclude the proof. Now we prove Lemma 5.1 restated below.</p><p>Lemma 5.1. Let S be a prefix subset. For any w ∈ S\D S , there is |Anc[w] ∩ src( S)| = 1 . Furthermore, for any other</p><p>Proof. We first show that for any w ∈ S \ D S , we have |Anc</p><p>Now consider the path P by stitching together the two directed paths, one from v 1 to w and another from v 2 to w. All non-colliders on P are not in S since S is a prefix subset. The only collider on P is w. Therefore P is active given S ∪ {w}. We have</p><p>Next we show that for any other w ′ ∈ S \ D S , we have w ̸ ⊥ w ′ | S if and only if Anc[w] ∩ src( S) = Anc[w ′ ] ∩ src( S). For the if direction, denote s = Anc[w] ∩ src( S) = Anc[w ′ ] ∩ src( S) and consider the trek by joining the two directed paths from s to w and from s to w ′ . Since this path has no colliders and it is fully in S (since S is a prefix and s ∈ S), it is active given S. Thus w ̸ ⊥ w ′ | S. For the only if direction, assume on the contrary that w ̸ ⊥ w ′ | S but Anc[w] ∩ src( S) ̸ = Anc[w ′ ] ∩ src( S). Let P be the active path between w, w ′ given S. Then all non-colliders on P are in S and all colliders on P are in Anc[S] = S. This means that P has no colliders; otherwise this collider is a child of the vertex next to it, which is a non-collider that is in S. This means there is an edge from S to S, which is a contradiction with S being prefix. Since P has no colliders, it must satisfy</p><p>This means there is a non-collider on P that is in S, which would make it inactive given S, a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of Lemma 4.7</head><p>To prove Lemma 4.7, we will make use of the following results. Lemma C.2. Let S be a prefix subset and w ∈ S \ (D S ∪ src( S)). By Lemma 5.1, let Anc[w] ∩ src( S) = {v}. Then for any u ∈ S, if there is no directed path from u to w that does not intersect src( S), then</p><p>Proof. Assume on the contrary that u ̸ ⊥ w | S ∪ {v}. Let P be an active path from u to w conditioned on v. If P ∩ S ̸ = {u}, then consider the last node on P that is in S. Since w ̸ ∈ S, this node must be pointing into a node in S on P , which makes this node a non-collider. However, this node belongs to S, which means P is inactive given S ∪ {v}, and thus P ∩ S = {u}.</p><p>There is also no collider on P . Otherwise consider the first collider; it will be in S since P ∩ S = {u}. However, since P is active, it will be in Anc[S ∪ {v}]. This is impossible since v ∈ src( S) and from Lemma C.1, we know that S ∪ {v} is prefixed. Therefore, P must be a directed path from u to w, where the node x adjacent to u is in S. This means that x ∈ Anc[w]. If x ̸ ∈ src( S), then P is a directed path from u to v that does not intersect src( S). If x ∈ src( S), since Anc[w] ∩ src( S) = {v}, then it must hold that x = v. This means P is inactive given S ∪ {v}, a contradiction.</p><p>Lemma C.3. Let S be a prefix subset and v ∈ src( S). Then for any w ∈ S,</p><p>Proof. Suppose w ̸ ∈ Des[v]; we will show that v ⊥ w | S. Assume on the contrary that v ̸ ⊥ w | S. Let P be the active path between v and w given S. If P intersects with S, then consider the last vertex on P that is in S. This vertex must be a non-collider since w ̸ ∈ S. This contradicts P being active given S. Therefore, P is fully in S. Since v ∈ src( S), we have P : v → . . . w. Since w ̸ ∈ Des[v], there must be a collider on P . However, this collider is not in S and S is prefix, which means that P is active given S, a contradiction.</p><p>We now prove Lemma 4.7, restated below. Lemma 4.7. Let S be a prefix vertex set. For w ∈ S \ D S , if w ̸ ∈ src( S) and there is no covered edge from Anc</p><p>Proof of Lemma 4.7. Assume w ̸ ∈ src( S). For contradiction, assume that there is no covered edge from Anc</p><p>there is a directed path from v to w. Consider the longest directed path P from v to w. Let v ′ be the adjacent vertex to v on P , i.e., P : v → v ′ • • • → w. By Lemma 4.5 and w ̸ ∈ D S , we must have v ′ ̸ ∈ D S . Since v → v ′ is not a covered edge, there could only be two cases:</p><p>and the second condition of the lemma is not met. We must have u ̸ ⊥ w | S ∪ {v}. By Lemma C.2, there must exist a directed path Q from u to w that does not intersect src( S). Consider the path between u and w ′ by joining Q and the directed path from v ′ to w. Since this path does not intersect with S ∪ src( S) and w is the only collider on it, we know that this path is active given S ∪ {v} ∪ {w}. Thus u ̸ ⊥ v ′ | S ∪ {v} ∪ {w}. Since the second condition of the lemma is not met, we must have u ̸ ⊥ v ′ | S ∪ {v}. By Lemma C.2, there must exist a directed path from u to v ′ that does not intersect src( S). Since u ̸ ∈ Pa(v ′ ), this path has at least length two. Let k be the vertex on this path such </p><p>This contradicts P being the longest directed path from v to w.</p><p>• There is k </p><p>This contradicts P being the longest directed path from v to w.</p><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Remarks</head><p>The above proofs suffice as intermediate results to show Theorem 3.3, which we proved in Section 5. Proof. For each v ∈ I, denote Des(v, I) as the set of u ∈ I such that u ̸ ⊥ I v. We first show that Des(v, I) is equal to the set of descendants of v such that there exists a directed path from v which is not cut by I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Missing Proofs of Interventions</head><p>Let u / ∈ I such that u ⊥ I v. If u ∈ Des(v, I), then there exists a directed path from v to u in the modified DAG G I , which means it is active given ∅, a contradiction. Therefore the only if direction is proven.</p><p>For the if direction, let u / ∈ I such that u ̸ ⊥ I v. Then there is an active path P : u -• • • -v in the modified DAG G I . Since P is active given ∅, there is no collider on P . Furthermore, since all incoming edges to any vertex in I are removed in the modified DAG, this path must be P : u ← • • • ← v and satisfies P ∩ I = {v}. Thus u ∈ Des(v, I).</p><p>Next we show that ∪ v∈I\S Des(v, I) = Des(I \ S) \ (I \ S). This will prove the lemma.</p><p>Since Des(v, I) is equal to the set of descendants of v such that there exists a directed path from v which is not cut by I, it is clear that ∪ v∈I\S Des(v, I) ⊆ Des(I \ S) \ (I \ S). Now let u ∈ Des(I \ S) \ (I \ S). Then there is v ∈ I \ S such that u ∈ Des(v). Consider the directed path from v to u and let v ′ be the last vertex on this path that is in I. Then u ∈ Des(v ′ , I). By the fact that S is prefix, we have from v ̸ ∈ S that v ′ ̸ ∈ S. Thus u ∈ Des(v ′ , I) ⊂ ∪ v∈I\S Des(v, I) and ∪ v∈I\S Des(v, I) ⊇ Des(I \ S) \ (I \ S). We therefore have ∪ v∈I\S Des(v, I) = Des(I \ S) \ (I \ S). D.2. Proof Lemma 6.2 Lemma 6.2. Let S be a prefix subset. Given an intervention I and v ∈ I \ S, denote H </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Remarks</head><p>The above proofs suffice as intermediate results for proving Theorem 6.5. Then together with Lemma 6.6 (proven in Section 6.1), we can prove Theorem 3.4, which is given in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Details of Numerical Experiments</head><p>Implementation Details. For FCI and RFCI, we used the implementations in <ref type="bibr">Kalisch et al. (2024)</ref>, which is written in R with C++ accelerations. For PC and GSP, we used the implementation in Squires, which us written in python. Our method, CCPG, is written in python. The acceleration of R (with C++) can be viewed by comparing two implementations of PC (stable) in Figure <ref type="figure">6</ref>.</p><p>Remark on causal sufficiency. Among the constraint-based methods, we marked the ones that do not assume causal sufficiency in Figure <ref type="figure">7</ref>. These methods run additional tests to check for unobserved causal variables, and therefore might require more samples compared to, e.g., PC, since the underlying system we test on satisfies causal sufficiency.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scaling up the greedy equivalence search algorithm by constraining the search space of equivalence classes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Alonso-Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gámez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Puerta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of approximate reasoning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="451" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A characterization of Markov equivalence classes for acyclic digraphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Perlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="541" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName><forename type="first">Asuncion</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Properties of bayesian belief network learning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bouckaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><surname>Sparsityboost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6820</idno>
		<title level="m">A new scoring function for learning bayesian network structure</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Transformational Characterization of Equivalent Bayesian Network Structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedof the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95</title>
		<meeting>eedof the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal Structure Identification with Greedy Search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Largesample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reconstructing Causal Biological Networks through Active Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">150611</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Subset verification and search algorithms for causal dags</title>
		<author>
			<persName><forename type="first">D</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiragur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.03180</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Verification and search algorithms for causal DAGs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiragur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning sparse causal models is not np-hard</title>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6824</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining gene expression data and prior knowledge for inferring gene regulatory networks via Bayesian networks using structural restrictions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Applications in Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal Discovery as a Game</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality: Objectives and Assessment</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interventions and Causal Inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the number of experiments sufficient and in the worst case necessary to identify all causal relations among N variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="178" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N-1 Experiments Suffice to Determine the Causal Relations Among N Variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1389</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using bayesian networks to analyze expression data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="601" to="620" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter priors for directed acyclic graphical models and the characterization of several probability distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1412" to="1440" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the logic of causal models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence and Pattern Recognition</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sample Efficient Active Learning of Causal Trees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boix-Adserà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The logic of causal inference: Econometrics and the Conditional Analysis of Causation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics &amp; Philosophy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="234" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Randomized Experimental Design for Causal Graph Discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the pc-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perkovic</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Package &apos;pcalg&apos;. 2024</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Functional genomic hypothesis generation and experimentation by a robot scientist</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G K</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">6971</biblScope>
			<biblScope unit="page" from="247" to="252" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greedy relaxations of the sparsest permutation algorithm</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graphical models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ancestral causal inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal Inference and Causal Explanation with Background Knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95</title>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Highdimensional consistency in score-based and hybrid structure learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3151" to="3183" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition. ISBN 052189560X</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using genetic data to strengthen causal inference in observational research</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Pingault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schoeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Ploubidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rijsdijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dudbridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="566" to="580" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The direction of time</title>
		<author>
			<persName><forename type="first">H</forename><surname>Reichenbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<publisher>Univ of California Press</publisher>
			<biblScope unit="volume">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Marginal structural models and causal inference in epidemiology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brumback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a Health Knowledge Graph from</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rotmensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tlimat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Medical Records. Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The imap hybrid method for learning gaussian bayes nets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Intelligence: 23rd Canadian Conference on Artificial Intelligence, Canadian AI 2010</title>
		<title level="s">Proceedings</title>
		<meeting><address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-06-02">May 31-June 2, 2010. 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Causal Graphs with Small Interventions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meek separators and their applications in targeted causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shiragur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Consistency guarantees for greedy permutation-based causal inference algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="795" to="814" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Causality from probability</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An algorithm for causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4983</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Causaldag: Creation, manipulation, and learning of causal models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<ptr target="https://github.com/uhlerlab/causaldag" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Causal structure learning: a combinatorial perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Active Structure Learning of Causal DAGs via Directed Clique Trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21500" to="21511" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A review of active learning approaches to experimental design for uncovering biological networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sverchkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1005466</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bayesian Computation Methods for Inferring Regulatory Network Models Using Biomedical Data. Translational Biomedical Informatics: A Precision Medicine Perspective</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Equivalence and synthesis of causal models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Sixth Annual Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Making Things Happen: A theory of Causal Explanation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A recursive method for structural learning of directed acyclic graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="459" to="483" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Membership testing in markov equivalence classes via independence queries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiragur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3925" to="3933" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
