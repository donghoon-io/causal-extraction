<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating *</title>
				<funder ref="#_vAPr9Kw">
					<orgName type="full">JSPS</orgName>
				</funder>
				<funder ref="#_bxBrJQU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-04-27">27 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daisuke</forename><surname>Takahashi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
							<email>shohei-shimizu@biwako.shiga-u.ac.jp</email>
						</author>
						<author>
							<persName><forename type="first">Takuma</forename><surname>Tanaka</surname></persName>
							<email>takuma-tanaka@biwako.shiga-u.ac.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Data Science</orgName>
								<orgName type="institution">Shiga University</orgName>
								<address>
									<settlement>Hikone</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Data Science</orgName>
								<orgName type="institution">Shiga University</orgName>
								<address>
									<settlement>Hikone</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Graduate School of Data Science</orgName>
								<orgName type="institution">Shiga University</orgName>
								<address>
									<settlement>Hikone</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-27">27 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.02678v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Counterfactual explanations</term>
					<term>explainable machine learning</term>
					<term>causal discovery</term>
					<term>rating classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on counterfactual probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged counterfactual probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model. Furthermore, explanatory scores were estimated based on counterfactual probabilities. Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph. Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, the use of artificial intelligence (AI) has rapidly increased owing to its revolutionary impact on various aspects of society and industry. This is primarily attributed to advances in deep learning <ref type="bibr" target="#b0">[1]</ref> and ensemble learning algorithms such as LightGBM <ref type="bibr" target="#b1">[2]</ref>. However, it is impossible for humans to understand the basis for the predictions by these models due to the complex calculations and internal structures. Consequently, people are hesitant to utilize machine learning for tasks that require accountability (bank loans, medical diagnosis) <ref type="bibr" target="#b2">[3]</ref>. To address this challenge, numerous methods have been developed in explainable artificial intelligence (XAI) to improve the explainability of black-box models <ref type="bibr" target="#b3">[4]</ref>.</p><p>Among the most promising approaches of XAI is the explanation of predictive models using causal inference <ref type="bibr" target="#b4">[5]</ref>. Causal inference-based XAI considers the correlation between the input and output and the dependence of features; thus, it can efficiently explain the desired predicted value. In particular, the estimation by existing methods, such as decision tree-based feature importance <ref type="bibr" target="#b5">[6]</ref> and SHAP <ref type="bibr" target="#b6">[7]</ref>, is based on the correlation of variables, which may be pseudo-correlation <ref type="bibr" target="#b7">[8]</ref>. Most XAI methods based on causal inference employ counterfactual explanations, which elucidate a prediction by computing the change in the individual's features to modify the prediction to the desired class <ref type="bibr" target="#b8">[9]</ref>. A previous study <ref type="bibr" target="#b9">[10]</ref> presented an XAI system called LEWIS, which leveraged causal models and counterfactual probabilities. This study employed loan approval as an example to estimate a counterfactual probability score for a binary classification. Based on this explanatory score, this method rendered the features requiring change as explicit to modify the prediction if the loan for a customer was predicted to be rejected.</p><p>However, LEWIS requires complete knowledge of the causal graph of features. When building machine learning models, background knowledge such as causal relationships are rarely completely known. This is a major challenge when applying this method to real data. When the causal graph is unknown, it can be estimated by causal discovery <ref type="bibr" target="#b10">[11]</ref>; however, its performance in artificial and real-world data has not been explored.</p><p>This study proposed a new XAI framework that relaxed the constraint of the causal graph being known. As shown in Figure <ref type="figure">1</ref>, a causal discovery can be possibly performed with prior knowledge on the causal structure, and the resulting causal graph can be used to explain the features using counterfactual probabilities. Our contributions are summarized as follows:</p><p>• We conducted numerical experiments to analyze the vari-Fig. <ref type="figure">1</ref>. Framework of counterfactual probability explanations using causal structure information ation of explanatory scores with causal structure and the proposal of useful prior information on causal structure. • Our artificial data experiments implied that the combination of causal discovery with certain prior information proposed above recovered the estimates of the explanatory score better than previous methods without causal discovery or graph assumption.</p><p>• By applying the method to real world financial data from the Shiga bank, Ltd., which is the largest regional bank in Shiga prefecture of Japan, we demonstrated that useful explanations could be made from the graph estimated by causal discovery. To the best of our knowledge, this is the first real-world example of counterfactual probability explanations in case that the causal structure is unknown and is estimated by causal discovery. The remainder of this paper is organized as follows. Section II provides an overview of related research and defines the symbols used in this paper. Section III analyzes the effects of causal structures on explanation scores and proposes the use of useful prior information on the causal structure. In Section IV, using artificial data, we provide counterfactual explanations by combining prior information on the causal structure and causal exploration. Consequently, we examine whether the estimated explanatory score can restore the estimate of the true explanatory score. Section V demonstrates that the proposed method is useful even when the causal graph is unknown using real data. Finally, Section VI summarizes the results and provides future perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>This section introduces the mathematical symbols and formulas used in this paper and outlines the related methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structural Causal Model</head><p>Structural causal model (SCM) <ref type="bibr" target="#b11">[12]</ref> is a mathematical framework for handling causal relationships. SCM comprises a set of endogenous variables V = {V 1 , V 2 , ..., V p }, a set of exogenous variables U = {U 1 , U 2 , ..., U q }, which are variables not determined by the endogenous variables, and a set of functions F = {f 1 , f 2 , ..., f p }, which determines the values of endogenous variables from those of other endogenous and exogenous variables. In addition, we define the parent set of endogenous variables as pa(V i ) and we assume variables in U are independent. Consequently, if SCM is specified by</p><formula xml:id="formula_0">V i = f i (pa(V i ), U i ) (1)</formula><p>and the system is assumed to be autonomous, Equation ( <ref type="formula">1</ref>) is called a structural causal model, wherein the quantitative causal relations of variables are expressed by a directed graph called a causal graph. Here, autonomy implies that changing any function or distribution of variables does not change the distribution of other functions or distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LEWIS</head><p>LEWIS <ref type="bibr" target="#b9">[10]</ref> is an XAI method that provides counterfactual explanations for machine learning predictions based on structural causal models. Consider the following binary classification problem. The explanatory variable is X ∈ V and the value of the explanatory variable is {x, x ′ } ∈ X (x &gt; x ′ ), wherein all the values of the explanatory variables are discretized. Let O ∈ {o, o ′ } be the predicted value corresponding to the explanatory variable, where o is the positive predicted value and o ′ is the negative predicted value. The counterfactual in any causal model M = ⟨V, U, F ⟩ can be defined for a certain individual u ∈ U as <ref type="bibr" target="#b11">[12]</ref> </p><formula xml:id="formula_1">X(u) = x ⇒ Y X=x (u) = Y Mx (u),<label>(2)</label></formula><p>where Y X=x is the counterfactual of the value of Y if the value of X were x, with Y Mx being that of Y if the value of X were x in the causal model M . Further, the counterfactual probability P (Y X=x ) can be expressed as P (Y |do(X = x)) using the intervention symbol do. It represents the conditional probability of Y if the value of X were changed to x and can be computed based on a causal graph. In this setting, LEWIS defines the following three explanatory scores as the probability of a counterfactual, necessity score (Nec)</p><formula xml:id="formula_2">Nec(x, x ′ ) = P (o ′ X=x ′ |x, o),<label>(3)</label></formula><p>sufficiency score (Suf)</p><formula xml:id="formula_3">Suf(x, x ′ ) = P (o X=x |x ′ , o ′ ),<label>(4)</label></formula><p>and necessity and sufficiency score (Nesuf)</p><formula xml:id="formula_4">Nesuf(x, x ′ ) = P (o ′ X=x ′ , o X=x ).<label>(5)</label></formula><p>The necessity score and sufficiency score (called reversal probability) calculate the probability of the degree of change in the predicted value if the explanatory variable had a different value, given the value and predicted value of a given explanatory variable. In particular, the necessity score represents the probability that the predicted value would change if the value of the explanatory variable decreased, whereas the sufficiency score represents the probability that the predicted value would change if the value increased. The necessity and sufficiency score is a score that balances the necessity score and sufficiency score and can be considered as a feature importance in machine learning <ref type="bibr" target="#b9">[10]</ref>.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, LEWIS was used to examine a binary classification model that determines whether a loan was approved or rejected. For example, Nec = 0.1 for a feature implies that if a person whose loan is predicted to be approved decreased the value of a feature, there would be at most a 10% chance of a prediction for the loan to be rejected. In addition, Suf = 0.8 implies that if a person whose loan is predicted to be rejected increased the value of a certain feature, there is a maximum probability of 80% that the loan would be predicted to be approved.</p><p>These three scores are expressed as</p><formula xml:id="formula_5">Nec(x, x ′ ) = P (o ′ |do(X = x ′ )) -P (o ′ |x) P (o|x) ,<label>(6)</label></formula><formula xml:id="formula_6">Suf(x, x ′ ) = P (o|do(X = x)) -P (o|x ′ ) P (o ′ |x ′ ) ,<label>(7)</label></formula><formula xml:id="formula_7">Nesuf(x, x ′ ) = P (o ′ |do(X = x ′ )) -P (o ′ |do(X = x)),<label>(8)</label></formula><p>if the causal graph G corresponding to the causal model M is known and the monotonicity</p><formula xml:id="formula_8">(x &gt; x ′ ⇒ O x &gt; O x ′ )</formula><p>is satisfied. In this case, the global explanation score of LEWIS, maxNesuf(X), is given by the maximum value of Nesuf(x, x ′ ) for all pairs of the value of the explanatory variable, (x, x ′ ). Finally, if no causal graph is provided to LEWIS, it assumes that P (O|do(X)) = P (O|X) under the assumption that there are no confounding factors, which is likely to be violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NUMERICAL EXPERIMENT OF EXPLORING USEFUL PRIOR KNOWLEDGE ON CAUSAL STRUCTURE</head><p>In this section, we analyze the effects of causal structures on explanation scores and propose useful prior information on the causal structure.</p><p>A. Analysis of the influence of causal structure on explanation scores 1) Experimental design: We built a machine learning model leveraging data generated from a three-variable causal structure and analyzed the characteristics of the LEWIS explanation score estimated for the predicted value. Data were generated from the five causal graphs in Figure <ref type="figure" target="#fig_0">2</ref>. Structures A, B, and E are termed the collider, chain, and fork paths, respectively, and each exhibited its own unique independence <ref type="bibr" target="#b11">[12]</ref>. Structure C exhibited confounding behavior, where the variable X was the confounding variable. Structure D contained a variable X that was independent of variable Y . The coefficients of each structural equation were all equal for each variable in structures B-E, and the coefficients of A were set to 1 and 1.5 for X and Z, respectively. After generating data from the five causal structures and building a machine learning model using that data, we estimated the Nesuf score of Equation ( <ref type="formula" target="#formula_4">5</ref>) with LEWIS. This was iterated 100 times and the characteristics of the average estimated value were analyzed. Table <ref type="table">I</ref> presents the detailed experimental settings. In each repetition, error variables were generated from a uniform distribution of [0, 1] using a linear or nonlinear (second-order monomial) function system for the causal structures A-E. The generated data were discretized using the same discretization bin number, and catboost was used as the classifier. Herein, the target variable Y was also discretized to binary values using equally spaced discretization. The sample size of the data was 5 000, which Nesuf was estimated for the predictions of.</p><p>2) Experimental results: The estimates of Nesuf are presented in Tables II and III. The results suggested two possible pieces of prior information on causal structure to determine Nesuf based on a linear causal structure. First, in case of multiple variables that are direct parents of a target variable such as structure A, variables with strong correlations would be more important. In structure A, the correlation coefficients with Y of variables Z and X were 0.8075 and 0.4779 on average, respectively, indicating that variable Z exhibited a stronger correlation than X and was therefore more important. Second, if the target variable is independent and has no directed edge from the variable X, such as in the causal structure D or E, the importance will be zero. Thus, if an explanatory variable is independent of the target variable, the LEWIS explanatory score is zero because intervention on that explanatory variable cannot change the target variable. A similar argument is possible in the case of nonlinear causal structures to determine Nesuf. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Useful prior information on the causal structure</head><p>In Section III-A, we observed that the score differed greatly depending on whether the explanatory variable could yield the predictive variable. In particular, all explanatory scores will be 0 for the explanatory variables that are independent of the target variable. In addition, in case of a directed edge, or reverse causation, from the target variable to the explanatory variable, the target variable and explanatory variable are dependent. However, even if the value of the explanatory variables were changed, the value of the target variable would not be changed; thus, so the value of Nesuf becomes 0, and Suf and Nec provide no information. Therefore, we proposed the following prior information on the causal structure to compute the explanation scores:</p><p>(a) Target variable has the direct parent-child relationship with all explanatory variables, that is, there is a direct causal path from the explanatory variables to the target variable. (b) Target variable is the sink variable, where a sink variable is a variable that does not cause any variable. With prior information (a) (Figure <ref type="figure" target="#fig_1">3a</ref>), the target variable is dependent on all explanatory variables. In this prior information, the estimated score is adjusted by variables that satisfy the backdoor criteria; if they don't satisfy it, the intervention query is directly estimated, as in prior work. Hence if the explanatory variables and target variable are not independent, the estimated score will be at least the same as when no causal graph is used. This facilitates the performing of causal discovery using only explanatory variables. In prior information (b) (Figure <ref type="figure" target="#fig_1">3b</ref>), we considered the influence of variables independent of the target variable and reverse causation, which could not be identified in prior information (a). In this prior information helps eliminate reverse causality from the target variable to the explanatory variables when performing causal discovery including the target variable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL EXPERIMENT AND EVALUATION</head><p>This section describes the simulation and experimental results of explanations based on counterfactual probabilities through causal discovery using artificial data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Numerical experiment setting</head><p>In the numerical experiment, we generated artificial data from the 8-variable causal graph shown in Figure <ref type="figure" target="#fig_2">4</ref>, performed causal discovery based on prior information on the causal structure proposed in the previous section, and estimated the Nesuf for the predicted values. We performed evaluations considering the order of estimated the Nesuf and that of true Nesuf and the error of the estimated Nesuf value. We compared these results with the value of Nesuf estimated assuming that P (Y |do(X)) = P (Y |X), similar to that in case of <ref type="bibr" target="#b9">[10]</ref>. The artificial data to be generated were continuous and mixed data. Continuous variable values were generated from linear or non-linear function systems. For mixed data, only a linear system was used, the probability distribution of the error variables X 1 and X 7 was a Bernoulli distribution, and the probability distribution of the other error variables were a uniform or Gaussian distribution. The model, sample size, discretization method, and the number of discretization bins are presented in Table <ref type="table">I</ref>.</p><p>For the evaluation, we calculated the mean absolute error (MAE) with respect to the true Nesuf estimate. We used MAE, which is the average of the MAE</p><formula xml:id="formula_9">MAE = 1 N N i=1 |maxNesuf Xi,true -maxNesuf Xi,est | (9)</formula><p>for seven variables, X 1 , . . . , X 7 , where N is the number of experimental trials, maxNesuf Xi,true is the true value of maxNesuf(X i ), and maxNesuf xi,est is the estimated value of maxNesuf(X i ). The standard error of maxNesuf(X i ) was also evaluated.</p><p>Here, we assume that a = {a 1 , . . . , a n } and b = {b 1 , . . . , b n } convert the value of the true Nesuf and the estimated Nesuf into a rank for n variables, respectively.  <ref type="bibr" target="#b12">[13]</ref> with</p><formula xml:id="formula_10">SPR = 1 - 6 n i=1 d 2 i n(n 2 -1) ,<label>(10)</label></formula><p>where SPR attains the value -1 ≦ SPR ≦ 1. The closer it is to 1, the more the order of the estimated Nesuf matches the true order of feature importance. Moreover, the closer it is to -1, the more the order relationship is reversed. In this experiment, the evaluation was based on the average MAE, the standard error and the average SPR of 100 trials.</p><p>Next, for causal discovery, we used six methods tailored to specific functional forms. We implemented the causal discovery algorithm in Python. We used the causal-learn <ref type="bibr" target="#b13">[14]</ref> method for PC, linear non-Gaussian acyclic model (LiNGAM) package <ref type="bibr" target="#b14">[15]</ref> for DirectLiNGAM, RESIT, and linear mixed (LiM) and causalnex <ref type="bibr" target="#b15">[16]</ref> method for NOTEARS and NOTEARS-MLP.</p><p>• PC <ref type="bibr" target="#b16">[17]</ref>: The PC algorithm is a causal discovery algorithm based on conditional independence. The estimation algorithm first removes undirected edges by testing conditional independence. The direction of the causal relationship is determined by using v-structures <ref type="bibr" target="#b11">[12]</ref> and orientation rule <ref type="bibr" target="#b17">[18]</ref> for the remaining undirected skeleton. When using a PC to orient undirected edges, the directions may not be determined in the end, and the graph may be estimated as a partially oriented graph.</p><p>In that case, our evaluation was based on the causal graph with all directed edges derived from the partially oriented graph that had the maximum (PC Max) or minimum (PC Min) Spearman's rank correlation coefficient. We used the Fisher-z test to investigate the conditional independence.</p><p>• DirectLiNGAM <ref type="bibr" target="#b18">[19]</ref>: DirectLiNGAM is a causal discovery algorithm that expresses causal relationships using a linear structural equation model called LiNGAM <ref type="bibr" target="#b19">[20]</ref>, which assumes that the graph is acyclic and that the probability distribution of the error terms is non-Gaussian. DirectLiNGAM estimates the causal structure by repeated regression and evaluating the independence of residuals of each variable.</p><p>• RESIT <ref type="bibr" target="#b20">[21]</ref>: RESIT is a causal discovery algorithm used when the causal structure is nonlinear and the error variables are additive (Additive Noise Model: ANM). Similar to DirectLiNGAM, the algorithm estimates the causal direction by evaluating the independence of each variable and the residual of nonlinear regressions.</p><p>• LiM <ref type="bibr" target="#b21">[22]</ref>: LiM causal discovery algorithm extends LiNGAM to handle the mixed data that comprises both continuous and discrete variables. The estimation is performed by first globally optimizing the log-likelihood function on the joint distribution of data with the acyclicity constraint and then applying a local combinatorial search to output a causal graph.</p><p>• NOTEARS <ref type="bibr" target="#b22">[23]</ref>: NOTEARS reformulates DAG structure learning as a continuous optimization problem over real matrices, avoiding combinatorial acyclicity constraints. It introduces a smooth characterization of acyclicity as an equality constraint h(W ) = 0 on the weighted adjacency matrix W. This equality-constrained program is solved using augmented Lagrangian methods and numerical optimizers. This method assumes that the probability distribution of the error term has equal variance. Empirically, it outperforms state-of-the-art methods, especially for linear. • NOTEARS-MLP <ref type="bibr" target="#b23">[24]</ref>: NOTEARS-MLP is that DAG structure learning algorithm extends NOTEARS to handle non-linear functional relationships using MLP which consists of hidden layer units and an activation that sigmoid function. Especially when the causal structure is an additive noise model, this method is identifiable that assuming the nonlinear function are three times differentiable and not linear in any of its arguments.</p><p>The following estimates were based on either prior information (a), prior information (b), or no prior information, denoted by (a), (b), and (0), respectively. PC and DirectLiNGAM can incorporate prior information (a) and (b). However, RESIT, LiM, NOTEARS and NOTEARS-MLP cannot incorporate prior information (b). In addition, we evaluated cases wherein causal discovery was performed without a causal graph (No graph).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of experiment</head><p>The experimental results of MAE ± standard error and SPR are presented in Tables V-X. First, regarding the linear causal structure in Tables V and VI, when the causal discovery matched the true graph, the MAE approached 0 and the SPR approached 1. DirectLiNGAM assumed that the functional relationship was linear and that the probability distribution of the error terms was non-Gaussian. In such cases, MAE was smaller and the SPR was greater than that in the case No graph (Table <ref type="table">V</ref>). As shown in Table <ref type="table">VI</ref>, when a Gaussian distribution was assumed with no prior information or No graph, the MAE was large and the SPR was small for DirectLiNGAM. However, these scores were improved with prior information (a) and (b). Furthermore, the PC and NOTEARS algorithm consistently achieved high accuracy for both distributions. In all cases with a linear structure, we could estimate the true value and order of Nesuf scores better than that case when No graph was assumed.  Tables VII and VIII present the results in the case of a nonlinear causal structure. When the error term was uniform distribution, RESIT with no prior information and PC with prior information yielded smaller MAE values and greater SPR values than that for No graph in Table VII. For the Gaussian distribution causal structure as shown in Table <ref type="table" target="#tab_1">VIII</ref>, NOTEARS-MLP with prior information had the highest performance. However, in this experimental setup, the performance of all methods was lower compared to other setups.</p><p>The reason may be that monotonicity, another assumption of LEWIS, is not satisfied in nonlinear cases. Finally, Tables IX and X present the results of the mixed data. In Table <ref type="table">IX</ref>, the assumptions of LiM were satisfied, thus, the MAE was smaller and the SPR was larger than that when no causal discovery was performed. In addition, in Table <ref type="table">X</ref>, the LiM assumption that the probability distribution of the error terms was non-Gaussian was not satisfied, the score was the same as the case of No graph. Even in the case of linear mixed data, it can be said that the true Nesuf can be partially restored using the proposed prior information of causal structure. This result confirms that prior information (a) is at least the same and more than that as when no causal graph is used. Futhermore prior information (0) and (b) provide relatively high performance compared to the case with No graph. On the other hand, depending on the nature of the data and the causal discovery method, it may be worse than the case with No graph, so it is necessary to consider multiple causal discovery methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATION TO REAL DATA</head><p>In this section, we demonstrate the effectiveness of this framework for the case where the causal graph is unknown by applying our method to real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and preprocessing</head><p>We applied our method to the anonymized credit rating data of 14 018 business customers provided by the Shiga Bank, Ltd. A credit rating was assigned by a bank to a debtor based on the analysis of its financial statements <ref type="bibr" target="#b24">[25]</ref>. Although there were several grades in the credit rating, we simplified the grades to high (excellent) and low (poor), which facilitated its modeling as a binary classification problem. We used the industry type, amount of capital stock, number of employees, most recent annual sales, and total liabilities and equity as the explanatory variables. We performed equal-frequency discretization with 10 bins because the capital stock, number of employees, most recent annual sales, and total liabilities and equity had highly skewed distributions. These discretized variables are ordinal measures with enough many levels and can be seen as continuous variables. As a machine learning model, we used Random Forest, which is an algorithm that performs classification by combining the results of multiple decision trees constructed from randomly selected learning data and explanatory variables; it is an ensemble learning algorithm <ref type="bibr" target="#b25">[26]</ref>. Random Forest was expected to have sufficient predictive accuracy for real data in this study.</p><p>Although the data involved mixed data where only the industry type is discrete variable and the others are regarded as continuous variables, the industry type could not be caused by the other explanatory variables. Thus, we used DirectLiNGAM for continuous variables, assuming that industry type was an exogenous discrete variable that affected all other variables. In this case, DirectLiNGAM can handle discrete variable similarity to the conventional structural equation modeling <ref type="bibr" target="#b26">[27]</ref>.</p><p>Because we aimed at a quantitative analysis of all explanatory variables, we conducted our analysis assuming that the target variable exhibited a direct parent-child relationship with all explanatory variables in the causal structure, that is, prior information (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis results and discussion</head><p>The black lines of Figure <ref type="figure" target="#fig_3">5</ref> show the results with Di-rectLiNGAM for the entire dataset. The interpretation of the causal direction of the results is as follows. The causal direction from capital stock to total liabilities and equity was consistent with domain knowledge because the total liabilities and equity are the sum of debt and equity on the balance sheet. In addition, the causal direction from capital stock to sales was consistent with the domain knowledge that companies conducted business activities based on capital and that this influenced sales. These variables can affect the credit rating on the basis of prior information of the causal structure in Figure <ref type="figure" target="#fig_3">5</ref> (red lines). Next, Nesuf estimated using the causal graph and No graph is shown in Figure <ref type="figure">6</ref>. The importance ranking of each of these variables obtained by LEWIS can be explained as follows using the causal graph shown in Figure <ref type="figure" target="#fig_3">5</ref>. The industry type was an exogenous variable that affected the other four variables on the causal graph. If the industry type were to be changed, the values in the other four variables would also change, which is likely to affect the final predicted value. Thus, its importance in LEWIS is the greatest. Similarly, the value of capital stock changed the value of the sales and total liabilities and equity, and the total liabilities and equity changed the value of sales. The number of employees and sales were less important because changing these values did not change the values of other variables. However, the Nesuf scores estimated with No graph were smaller, and those of the capital stock in particular were significantly smaller, which contradicted domain knowledge.</p><p>The LEWIS reversal probability score is shown in Figure <ref type="figure">7</ref>. Overall, the Suf score (orange) was high, and there was a high probability that changing the value of the variable would change the company from a low-rated one to a high-rated one. Industry type also significantly affected the predicted value if it were changed. Reversal probabilities can assist the decision-making about which variables are likely to change the rating for a low-rated company and can quantify the company's strengths and weaknesses based on each score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This study proposed a new causal XAI framework that combined causal structure information and causal discovery is the probability that the prediction would change by lowering the value of that variable for a company whose rating is predicted to be high. Suf (orange) is the probability that the prediction would change by increasing the value of the variable for a company whose rating is predicted to be low.</p><p>without the knowledge of the causal graph. We analyzed the global explanation scores by using counterfactual explanations based on the causal structure and proposed prior information on the causal structure. Numerical experiments demonstrated the possibility of estimating the global explanatory score and the order of the true feature importance even if the causal graph was not fully known. By applying our method to real data, we demonstrated the usefulness of the proposed framework even if the causal graph is unknown.</p><p>As an extension of LEWIS, a method for multi-class classification was proposed <ref type="bibr" target="#b9">[10]</ref>. However, whether proposed prior information on causal structure is valid even for the multi-class classification remains an open question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Causal graph used in analysis. The values on the directed edges represent the coefficients of the respective structural equations.</figDesc><graphic coords="3,335.42,50.54,204.18,139.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Prior information of causal structure. (a): Target variable Y has the direct parent-child relationship with all explanatory variables. (b): Target variable Y is the sink variable.</figDesc><graphic coords="4,316.51,50.54,241.99,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Causal graph in artificial data experiments</figDesc><graphic coords="5,133.40,50.54,82.19,154.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Causal graph estimated by DirectLiNGAM (black lines). Prior information on the causal structure (red lines).</figDesc><graphic coords="7,324.78,109.66,225.45,216.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Nesuf estimated from causal graph and No graph</figDesc><graphic coords="8,48.96,50.54,251.55,134.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,79.54,50.54,452.93,141.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II MEAN</head><label>II</label><figDesc>OF NESUF IN THE CASE OF LINEAR CAUSAL STRUCTURE</figDesc><table><row><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D E</cell></row><row><cell cols="4">x 0.367 0.498 0.999</cell><cell>0</cell><cell>0</cell></row><row><cell>z</cell><cell cols="3">0.999 0.999 0.325</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell></row><row><cell cols="5">MEAN OF NESUF IN THE CASE OF NONLINEAR CAUSAL STRUCTURE</cell></row><row><cell></cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D E</cell></row><row><cell cols="4">x 0.377 0.556 0.999</cell><cell>0</cell><cell>0</cell></row><row><cell>z</cell><cell cols="3">0.945 0.999 0.949</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table IV presents the experimental settings used in the numerical experiment.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We would like to express our gratitude to <rs type="person">Shiga Bank</rs>, Ltd. for their valuable comments on this study and for providing us with the data used. This work was partially supported by <rs type="funder">JSPS</rs> <rs type="grantNumber">KAKENHI 20K11708</rs> and <rs type="grantNumber">JST CREST JPMJCR22D2</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vAPr9Kw">
					<idno type="grant-number">KAKENHI 20K11708</idno>
				</org>
				<org type="funding" xml:id="_bxBrJQU">
					<idno type="grant-number">JST CREST JPMJCR22D2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Opportunities and challenges in explainable artificial intelligence (XAI): A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11371</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DARPA&apos;s explainable artificial intelligence (XAI) program</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Causal explanations and XAI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beckers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Causal Learning and Reasoning</title>
		<meeting>the First Conference on Causal Learning and Reasoning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From local explanations to global understanding with explainable AI for trees</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Prutkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Himmelfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bansai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17: 31st Conference on Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal Shapley values: exploiting causal knowledge to explain individual predictions of complex models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sijben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Buchr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Classen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020: 34th Conference on Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Causal machine learning: a survey and open problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15475</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining black-box algorithms using probabilistic contrastive counterfactuals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Galhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Management of Data</title>
		<meeting>the 2021 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="577" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, reasoning, and inference</title>
		<meeting><address><addrLine>2nd ed., Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Psychol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="101" />
			<date type="published" when="1904">1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Causal-learn: Causal Discovery in Python</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16405</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Python package for causal discovery based on LiNGAM</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horsburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pilgerstorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Droth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CausalNex</title>
		<imprint/>
	</monogr>
	<note>Computer software],&quot; 2021</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Sci Comput Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An algorithm for deciding if a set of observed independencies has a causal explanation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="323" to="330" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">72</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2009" to="2053" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causal discovery for linear mixed data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Conf. Causal Learn</title>
		<meeting>First Conf. Causal Learn</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="994" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric DAGs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Financial</forename><surname>Services</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agency</forename></persName>
		</author>
		<ptr target="https://www.fsa.go.jp/manual/manualj/manualyokin/14.pdf" />
		<title level="m">Financial Inspection Manual (Inspection Manual for Financial Institutions Accepting Deposits, etc.)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>In Japanese</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond SEM: general latent variable modeling</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="81" to="117" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
