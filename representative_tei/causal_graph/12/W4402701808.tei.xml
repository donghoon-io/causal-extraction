<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2025 NEURAL SPACETIMES FOR DAG REPRESENTATION LEARNING</title>
				<funder ref="#_QK2W73e">
					<orgName type="full">EP-SRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Oxford-Man Institute of Quantitative Finance</orgName>
				</funder>
				<funder ref="#_k4Gmspn">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_2XtCwaK #_r5SGxXx">
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder ref="#_FfBCbbV">
					<orgName type="full">EPSRC AI Hub on Mathematical Foundations of Intelligence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-09">9 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haitz</forename><surname>Sáez De Ocáriz Borde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">McMaster University &amp; Vector Institute</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford &amp; AITHYRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
							<email>kratsioa@mcmaster.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">McMaster University &amp; Vector Institute</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford &amp; AITHYRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
							<email>marcl@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">McMaster University &amp; Vector Institute</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford &amp; AITHYRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nvidia</forename><forename type="middle">Xiaowen</forename><surname>Dong</surname></persName>
							<email>xdong@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">McMaster University &amp; Vector Institute</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford &amp; AITHYRA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<email>michael.bronstein@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">McMaster University &amp; Vector Institute</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">University of Oxford &amp; AITHYRA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2025 NEURAL SPACETIMES FOR DAG REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-09">9 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.13885v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a class of trainable deep learning-based geometries called Neural SpaceTimes (NSTs), which can universally represent nodes in weighted Directed Acyclic Graphs (DAGs) as events in a spacetime manifold. While most works in the literature focus on undirected graph representation learning or causality embedding separately, our differentiable geometry can encode both graph edge weights in its spatial dimensions and causality in the form of edge directionality in its temporal dimensions. We use a product manifold that combines a quasimetric (for space) and a partial order (for time). NSTs are implemented as three neural networks trained in an end-to-end manner: an embedding network, which learns to optimize the location of nodes as events in the spacetime manifold, and two other networks that optimize the space and time geometries in parallel, which we call a neural (quasi-)metric and a neural partial order, respectively. The latter two networks leverage recent ideas at the intersection of fractal geometry and deep learning to shape the geometry of the representation space in a data-driven fashion, unlike other works in the literature that use fixed spacetime manifolds such as Minkowski space or De Sitter space to embed DAGs. Our main theoretical guarantee is a universal embedding theorem, showing that any k-point DAG can be embedded into an NST with 1 + O(log(k)) distortion while exactly preserving its causal structure. The total number of parameters defining the NST is sub-cubic in k and linear in the width of the DAG. If the DAG has a planar Hasse diagram, this is improved to O(log(k) + 2) spatial and 2 temporal dimensions. We validate our framework computationally with synthetic weighted DAGs and real-world network embeddings; in both cases, the NSTs achieve lower embedding distortions than their counterparts using fixed spacetime geometries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are a ubiquitous mathematical abstraction used in practically every branch of science from the analysis of social networks <ref type="bibr" target="#b75">(Robins et al., 2007)</ref> to economic stability <ref type="bibr" target="#b48">(Hurd et al., 2016)</ref> and genomics <ref type="bibr">(Marbach et al., 2012)</ref>. The breadth of social and physical phenomena encoded by large graphs has motivated the machine learning community to seek efficient representations (or embeddings) of graphs in continuous spaces. Since the structure of graphs typically has properties different from Euclidean geometry (e.g., trees exhibit exponential volume growth), a significant effort has been made to identify non-Euclidean geometries which can faithfully capture the structure of general graphs. Examples include hyperbolic embeddings of tree-like graphs <ref type="bibr" target="#b31">(Ganea et al., 2018;</ref><ref type="bibr" target="#b83">Sonthalia &amp; Gilbert, 2020;</ref><ref type="bibr" target="#b81">Shimizu et al., 2021;</ref><ref type="bibr">Kratsios et al., 2023a;</ref><ref type="bibr" target="#b17">b)</ref>, spherical and toroidal embeddings of graphs with cycles <ref type="bibr" target="#b79">(Schoenberg, 1942;</ref><ref type="bibr" target="#b38">Guella et al., 2016;</ref><ref type="bibr" target="#b35">Giovanni et al., 2022b)</ref>, embeddings of graphs with several of these characteristics into product Riemannian geometries <ref type="bibr">(Borde et al., 2023a;</ref><ref type="bibr" target="#b17">b;</ref><ref type="bibr">Giovanni et al., 2022a)</ref>, combinations of constant curvature Riemannian manifolds <ref type="bibr" target="#b64">(Lu et al., 2023)</ref> and manifolds with locally controllable Ricci curvature <ref type="bibr" target="#b35">(Giovanni et al., 2022b)</ref>.</p><p>Figure <ref type="figure">1</ref>: A Neural Spacetime (NST) is a learnable triplet S = (E, D, T ), where E : R N → R D+T is a (feature) encoder network, D : R D+T × R D+T → [0, ∞) is a learnable quasi-metric on R D and T : R D+T → R T is a learnable partial order on R T . Given an input Directed Acyclic Graph (DAG), E optimizes the location of the nodes u, v, w as events in the spacetime manifold û, v, ŵ, while concurrently D and T learn the geometry of space and time themselves. The objective is to find a geometry that can faithfully represent, with minimal distortion, the metric geometry of the input DAG in space as well as its causal connectivity in time.</p><p>The Directed Graph Embedding Problem. The description of many real-world systems requires directed graphs, for example, gene regulatory networks <ref type="bibr">(Marbach et al., 2012)</ref>, flow networks <ref type="bibr" target="#b19">(Deleu et al., 2022)</ref>, stochastic processes <ref type="bibr" target="#b6">(Backhoff-Veraguas et al., 2020)</ref>, or graph metanetworks <ref type="bibr" target="#b63">(Lim et al., 2024)</ref> to name a few. Directed graphs (or digraphs) play a significant role in causal reasoning, where they are used to study the relationship between a cause and its effect in a complex system. These problems make use of representation spaces with causal structures, modeling the directional edges of a discrete graph as causal connections in a continuous representation space <ref type="bibr" target="#b90">(Zheng et al., 2018)</ref>. However, most approaches in causal representation learning primarily focus on capturing directional information while neglecting distance information. Recent works <ref type="bibr" target="#b82">(Sim et al., 2021;</ref><ref type="bibr" target="#b60">Law &amp; Lucas, 2023)</ref> have suggested that Lorentzian spacetimes from general relativity are suitable representation spaces for directed graphs, as these geometries possess an arrow of time that provides them with causal structure (see Appendix A.5) while also being capable of encoding distance.</p><p>Instead of considering a single arrow of time, in this paper we propose to turn to richer causal structures with multiple time dimensions, allowing us to model greater directional complexity.</p><p>Problem Formulation. Our goal is to learn a spacetime manifold representation where nodes in a DAG can be embedded as events. This should be done while capturing edge directionality in the form of causal structure (which includes modelling lack of connectivity as anti-chains), as well as the edge weights in the original discrete structure: these are represented as temporal ordering and spatial distance in the continuous embedding space.</p><p>The Neural Spacetime Model. We propose a trainable geometry with enough temporal and spatial dimensions to encode a broad class of weighted DAGs. In particular, the Neural SpaceTime (NST) model utilizes a MultiLayer Perceptron (MLP) encoder that maps graph node features to an intermediate Euclidean latent space, which is then fragmented into space and time and processed in parallel by a neural metric network (inspired by the neural snowflake model from <ref type="bibr" target="#b52">Borde &amp; Kratsios (2023)</ref>) and a neural partial order. These embed nodes jointly as causally connected events in the continuous representation while respecting the one-hop distance (given by the DAG weights) and directionality of the original discrete graph structure, see Figure <ref type="figure">1</ref>. The spatial component of our model, which considers D spatial dimensions, leverages large-scale and asymptotic embedding approaches relevant in coarse and hyperbolic geometry <ref type="bibr" target="#b70">(Nowak, 2005;</ref><ref type="bibr" target="#b26">Eskenazis et al., 2019)</ref>, as well as the small-scale embedding-based approach of Borde &amp; <ref type="bibr">Kratsios (2023)</ref> inspired by fractal-type metric embeddings. These are often used to encode undirected graphs equipped with their (global) geodesic undirected distance <ref type="bibr" target="#b13">(Bourgain, 1986;</ref><ref type="bibr" target="#b67">Matoušek, 1999;</ref><ref type="bibr" target="#b40">Gupta, 2000;</ref><ref type="bibr" target="#b55">Krauthgamer et al., 2004;</ref><ref type="bibr" target="#b69">Neiman, 2016;</ref><ref type="bibr" target="#b25">Elkin &amp; Neiman, 2021;</ref><ref type="bibr" target="#b5">Andoni et al., 2018;</ref><ref type="bibr" target="#b28">Filtser, 2020;</ref><ref type="bibr">Kratsios et al., 2023a;</ref><ref type="bibr" target="#b1">Abraham et al., 2022)</ref>, which implies that their local distance information can also be embedded <ref type="bibr" target="#b0">(Abraham et al., 2007;</ref><ref type="bibr" target="#b16">Charikar et al., 2010)</ref>. Note that the opposite is not true: optimizing for the local geometry does not guarantee a faithful representation of the global geometry <ref type="bibr" target="#b72">(Ostrovska &amp; Ostrovskii, 2019)</ref>. On the other hand, the temporal component of our trainable geometry parameterizes an order structure ≲ T on multiple T time dimensions. Thus, causality in our representation space simply means that a point x ∈ R D+T precedes another y ∈ R D+T in the sense that x ≲ T y (which only considers order in time T , see Section 3). When T = 1, our geometry corresponds to using a Cauchy time function of the globally hyperbolic Lorentzian spacetimes to define a partial order (see <ref type="bibr">Beem et al. (1996, Page 65)</ref> or <ref type="bibr" target="#b14">Burtscher &amp; Garcia-Heveling (2024)</ref>).</p><p>Our contributions are: (1) We propose a tractable neural network architecture for spacetimes (with multiple time dimensions), ensuring that only valid geometries are learnable; hence, the term neural spacetimes (NSTs). Our approach decouples the representation into a product manifold, which models (quasi-)metrics and (partial) orders independently. (2) Our main theoretical result, Theorem 1, provides a global embeddability guarantee showing that any (finite) poset can be embedded into a neural spacetime with a sufficient number of temporal and spatial dimensions. The embeddings are causal in time and asymptotically isometric in space. Furthermore, the neural spacetime model only requires O(k 2 ) parameters to globally embed a weighted DAG with k nodes. Theorem 2 shows that a broad class of posets can be embedded into a NST with at most two temporal dimensions. (3) We experimentally validate our model on synthetic metric DAG datasets, as well as real-world directed graphs that involve web hyperlink connections and gene expression networks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES: DIRECTED GRAPHS, POSETS, AND QUASI-METRICS</head><p>We now introduce the relevant notions of causal and spatial structure required to formulate our main results and our model. This section concludes by discussing the concept of spacetime embeddings, which represent DAGs in a continuous space encoding both their causal and spatial structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CAUSAL STRUCTURE</head><p>Directed Graphs. Consider a weighted directed graph, G D = (E D , V, W D ), where E D represents a set of directed edges, V a set of vertices, and W D the strength of the connections, respectively. We say that two vertices u, v ∈ V are causally connected, denoted by u ≼ v, if there exists a sequence of directed edges, a path (ν n , ν n+1 )</p><p>N -1 n=1 in V with ν 1 = u and ν N = v. Notice that causal connectivity is a transitive relation; that is, if u ≼ v and v ≼ w, then u ≼ w, for each u, v, w ∈ V . The neighborhood N (u) of a node u ∈ V is the set of nodes sharing an edge with u; i.e. N (u)</p><formula xml:id="formula_0">def. = {v ∈ V : (u, v) ∈ E D or (v, u) ∈ E D }. Every weighted directed graph G D = (E D , V, W D )</formula><p>is naturally a graph upon forgetting edge directions, meaning that it induces a weighted undirected graph G = (E, V, W ) where E def.</p><p>= {{u, v} ∈ V : (u, v) ∈ E D or (v, u) ∈ E D } and where W is the symmetrization of W D , W (u, v) def.</p><p>= max{W D (u, v), W D (v, u)}.</p><p>Remark 1 (Feature Vectors and Nodes). In practice, e.g. in Graph Neural Network (GNN) <ref type="bibr" target="#b78">(Scarselli et al., 2009)</ref> use-cases, one equips the nodes in the graph G = (E D , V, W D ) with a set of feature vectors {x v } v∈V in R N . We thus henceforth identify the set of nodes V in any graph with a set of feature vectors in R N , for some positive integer N , via the identification V ∈ v ↔ x v ∈ R N . Its corresponding poset is depicted using green arrows on the same vertex set {A, B, C, D, E}. Our partial order is not a total order as there is no red or green arrow between C and E. Red arrows encode key DAG structure and green arrows encode all their possible compositions.</p><p>Posets. A broad class of logical relations can be encoded as directed graphs. We consider the class of partially ordered sets (posets): the pair or tuple (V, ≲). In particular, a key characteristic of a poset is that for any two elements u and v in the set the following properties are satisfied: reflexivity,</p><formula xml:id="formula_1">∀u ∈ V, u ≼ u; antisymmetry, u ≼ v ∧ v ≼ u =⇒ u = v; and transitivity, ∀u, v, w ∈ V, u ≼ v ∧ v ≼ w =⇒ u ≼ w.</formula><p>Every poset induces a DAG structure on the vertex set V . Example 1 (From DAGs to Posets). Given the DAG G D = (E D , V, W D ), then we define the relation ≼ on vertices u, v ∈ V such that u ≼ v if either u = v or there exists a directed path from u to v in the graph. This relation establishes (V, ≲) as a poset.</p><p>The antisymmetry condition is not satisfied by all digraphs, but only by certain types of digraphs such as DAGs, since it is incompatible with directed cycles of length greater than 1 (loops other than self-loops). Conversely, posets induce directed graphs. This identification is encoded through the Hasse diagram of a poset, see Example 2, which is a natural way to forget all composite relations in the poset by identifying the key basic skeleton relations encoding all its structure.</p><p>The poset can then be recovered from its Hasse diagram by adding all compositions of edges and self-loops, see Figure <ref type="figure" target="#fig_0">2</ref>. Example 2 (From Posets to DAGS: Hasse Diagrams). The Hasse diagram of a poset (</p><formula xml:id="formula_2">V, ≲) is a directed graph G D = (E D , V ) with V def. = E D and ∀u, v ∈ V , (u, v) ∈ E D iff 1) x u ≤ x v , 2) u ̸ = v and 3) there is no w ∈ V \ {u, v} with x u ≤ x w ≤ x v (so x u ≤ x v is a "minimal relation").</formula><p>Our theoretical results in Section 3.1 focus on embedding guarantees for posets induced by DAGs, which are of interest in causal inference <ref type="bibr" target="#b85">(Textor et al., 2016;</ref><ref type="bibr" target="#b71">Oates et al., 2016)</ref>, causal optimal transport on DAGs <ref type="bibr" target="#b23">(Eckstein &amp; Cheridito, 2023)</ref>, which is particularly important in sequential decisions making <ref type="bibr" target="#b2">(Acciaio et al., 2020;</ref><ref type="bibr" target="#b6">Backhoff-Veraguas et al., 2020;</ref><ref type="bibr" target="#b89">Xu et al., 2020;</ref><ref type="bibr" target="#b24">Eckstein &amp; Pammer, 2024;</ref><ref type="bibr" target="#b57">Kršek &amp; Pammer, 2024;</ref><ref type="bibr" target="#b39">Gunasingam &amp; Wong, 2024)</ref> and in robust finance <ref type="bibr" target="#b8">(Bartl et al., 2021;</ref><ref type="bibr" target="#b3">Acciaio et al., 2024)</ref>), in applied category theory <ref type="bibr" target="#b30">(Fong &amp; Spivak, 2019)</ref> since any thin category is a poset <ref type="bibr" target="#b15">(Chandler, 2019)</ref>, and in biology applications such as gene expressions <ref type="bibr">(Marbach et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPATIAL STRUCTURE</head><p>Quasi-Metric Spaces. Although Riemannian manifolds have been employed to formalize non-Euclidean distances between points, their additional structural characteristics, such as smoothness and infinitesimal angles, present considerable constraints. This complexity often makes it challenging to express the distance function for arbitrary Riemannian manifolds in closed-form. On the other hand, quasi-metric spaces isolate the pertinent properties of Riemannian distance functions without requiring any of their additional structures for graph embedding. A quasi-metric space is defined as a set X equipped with a distance function d : X × X → [0, ∞) satisfying the following conditions for every</p><formula xml:id="formula_3">x u , x v , x w ∈ X: i) d(x u , x v ) = 0 if and only if x u = x v , ii) d(x u , x v ) = d(x v , x u ), iii) d(x u , x v ) ≤ C d(x u , x w ) + d(x w , x v ) , for some constant C ≥ 1. Property (iii) is called the C- relaxed triangle inequality. When C = 1, (X, d</formula><p>) is termed a metric space, and the C-relaxed triangle inequality becomes the standard triangle inequality. Quasi-metrics arise naturally when considering local embeddings since the triangle inequality is only required to hold locally, allowing for small neighborhoods of distinct points in a point metric space to be embedded independently from one another. Furthermore, these geometries share many of the important properties of metric spaces, e.g. the Arzela-Ascoli theorem holds <ref type="bibr" target="#b88">(Xia, 2009)</ref>. Moreover, if property (i) is removed, such that several points may be indistinguishable by their distance information, then we say that d is a pseudoquasi-metric <ref type="bibr" target="#b49">(Kim, 1968;</ref><ref type="bibr" target="#b58">Kũnzi, 1992)</ref>. This naturally occurs when additional time dimensions are used to encode causality, but distance is ignored. Example 3. Every weighted graph G = (E, V, W ) induces a metric space (V, d G ); e.g. using the shortest path (graph geodesic) distance d G , defined for each u, v ∈ V by</p><formula xml:id="formula_4">d G (u, v) def. = inf N -1 n=1 W (ν n , ν n+1 ) : ∃ (ν 1 = u, ν 2 ), . . . , (ν N -1 , ν N = v) ∈ E .</formula><p>(1)</p><p>One could define the inf of the empty set to be max v,u∈V W (u, v) + 1 (instead of ∞, which is unsuitable for learning and embedding). However, following the spacetime representation literature, we are only interested in learning the distance between causally connected nodes (Section 3.2). We emphasize that only simple weighted digraphs are considered here, meaning that we do not allow for self-loops, and each ordered pair of nodes has at most one edge between them. As later discussed in Section 3.1 and Section 3.2, NSTs can model causal connectivity in one direction (but no undirected edges), as well as lack of causal connectivity between events (anti-chains).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SPACETIME EMBEDDINGS</head><p>We will work in a class of objects with the causal structure of DAGs/posets and the distance structure of weighted undirected graphs. We formalize this class of causal metric spaces as follows.</p><p>Definition 1 (Causal (Quasi-)Metric Space). A triple (X , d, ≲) such that (X , d) is a (quasi-)metric space and (X , ≲) is a poset, is called a causal (quasi-)metric space.</p><p>Having defined a meaningful class of causal metric spaces, we now formalize what it means to approximately create a copy of a causal metric space into another. The goal is to begin with a complicated discrete structure, such as a DAG, and embed it into a well-behaved continuous structure. Definition 2 (Spacetime Embedding). Let (X , d, ≲ x ) and (Y, ρ, ≲ y ) be causal (quasi-)metric spaces. A map f : X → Y is a spacetime embedding if there are constants D ≥ 1 and c &gt; 0</p><formula xml:id="formula_5">such that for each x 1 , x 2 ∈ X (i) Causal Embedding: x 1 ≲ x x 2 ⇔ f (x 1 ) ≲ y f (x 2 ) (ii)Metric Embedding: c ρ(f (x 1 ), f (x 2 )) ≤ d(x 1 , x 2 ) ≤ D cρ(f (x 1 ), f (x 2 )).</formula><p>The constant D is called the distortion, and c is the scale of the spacetime embedding. A spacetime embedding f : X → Y for which D = c = 1 is called isocausal.</p><p>Remark 2 (Optimal Distortion). Note that D = 1 is the minimal possible distortion<ref type="foot" target="#foot_0">foot_0</ref> . This is because D &lt; 1 is impossible and D = 1 yields an equality between the rescaled target metric c ρ and the original metric d; i.e. cρ(f</p><formula xml:id="formula_6">(x 1 ), f (x 2 )) = d(x 1 , x 2 ) for all x 1 , x 2 ∈ X .</formula><p>Our spacetime embeddings are illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. We encode causality using the notion of an order embedding from order theory <ref type="bibr" target="#b17">(Davey &amp; Priestley, 2002)</ref>, also used in general relativity <ref type="bibr" target="#b56">(Kronheimer &amp; Penrose, 1967;</ref><ref type="bibr" target="#b84">Sorkin, 1991;</ref><ref type="bibr" target="#b74">Reid, 2003;</ref><ref type="bibr" target="#b45">Henson, 2009;</ref><ref type="bibr" target="#b10">Benincasa &amp; Dowker, 2010)</ref>, jointly with the notion of a metric embedding core to theoretical computation science <ref type="bibr" target="#b41">(Gupta &amp; Hambrusch, 1992;</ref><ref type="bibr" target="#b42">Gupta, 1999;</ref><ref type="bibr" target="#b43">Gupta et al., 2004)</ref> and representation learning <ref type="bibr" target="#b83">(Sonthalia &amp; Gilbert, 2020)</ref>.</p><formula xml:id="formula_7">B D C A E</formula><p>(a) Spatial Embedding: the objective is to replicate the distance between nodes, counted in a minimal number of hops, with the Euclidean distance on R 2 . In the neural spacetime, however, the distance will be non-Euclidean.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEURAL SPACETIMES</head><p>Let N ∈ N + be the dimensionality of the feature vectors x u , x v ∈ R N associated with nodes u, v ∈ V , where V represents the set of nodes of the DAG G D = (E D , V, W D ). Fix a space dimension D ∈ N + and a time dimension T ∈ N + . A neural spacetime (NST) is a learnable triplet S = (E, D, T ), where E : R N → R D+T is an encoder network, e.g. an MLP, D : R D+T ×R D+T → [0, ∞) is a learnable quasi-metric on R D and T : R D+T → R T is a learnable partial order on R T . We implement D and T as two neural networks that process the spatial and temporal dimensions of encoded feature vectors xu , xv</p><formula xml:id="formula_8">def. = E(x u ), E(x v ) ∈ R D+T in parallel.</formula><p>The encoder network, E : R N → R D+T , maps the original node feature vectors of the input graph to the dimensions of space and time used by the NST representation. We employ this mapping as an intermediate Euclidean space upon which to learn the quasi-metric and partial order. The model learns to allocate relevant information to each dimension through gradient descent, rather than attempting to manually specify which of the original node feature dimensions should correspond to space and time.</p><p>To define D, which will be implemented using a variation of the original neural snowflake (see equation 10 from Appendix A.3). The original neural snowflake, and our upgrade thereof, employs two neural networks: the first represents the metric space as vectorial data in R D , and the second perturbs the Euclidean distance thereon. Together, any finite metric space may be perfectly (isometrically) embedded in this manner, whereas only one of the networks alone is not enough; e.g. expander graphs cannot be isometrically embedded into any low-dimensional Euclidean space <ref type="bibr">(Kratsios et al., 2023a, Proposition 13)</ref>.</p><p>We consider the following activation function, σ s,l . The activation σ s,l depends on two trainable parameters s, l &gt; 0. In the spirit of the fractalesque structure of neural snowflakes, s controls whether small-scale distances should be expanded or contracted relative to the distance on R D . Similarly, l controls the large-scale distances of points and dictates whether those should be expanded or contracted. See Appendix C.1 for an extended discussion. Definition 3 (Neural (Quasi-)Metric Activation). For any s, l &gt; 0, we define the neural (quasi-) metric activation to be the map σ s,l : R → R given for each x ∈ R by</p><formula xml:id="formula_9">σ s,l (x) def. = sgn(x) |x| s if |x| &lt; 1 sgn(x) |x| l if |x| ≥ 1 (2)</formula><p>where the sign function sgn returns 1 for x ≥ 0 and -1 for x &lt; 0.</p><p>If s = l then σ s,l (x) = sgn(x) |x| s and one recovers the key component of the snowflake activation function of Borde &amp; <ref type="bibr">Kratsios (2023)</ref> used in the majority of the proofs of its metric embedding guarantees. Note that s and l are not required to be coupled in any way.</p><p>A neural (quasi-)metric is a map D : R D+T × R D+T → [0, ∞) with iterative representation:</p><formula xml:id="formula_10">D(x u , xv ) def. = W J σ s J ,l J • (u J-1 ); u j def.</formula><p>= W j σ sj ,lj • (u j-1 ) for j = 1, . . . , J -1,</p><formula xml:id="formula_11">u 0 def. = |σ s0,l0 • (x u ) 1:D -σ s0,l0 • (x v ) 1:D |,<label>(3)</label></formula><p>for each xu , xv ∈ R D+T , where the depth parameter J ∈ N + , where the s 0 , l 0 , . . . , s J , l J &gt; 0, and • denotes component-wise composition (i.e. the function is applied element-wise, which is standard in Deep Learning), weight matrices W j ∈ I + D (an invertible positive matrix, see Appendix A.2 ) for j &lt; J and W J ∈ (0, ∞) 1×d all of which have positive entries, and where the absolute value | • | is also applied component-wise. Note that (•) 1:D extracts the first D dimensions of the input vector.</p><p>Moreover, we seek an ordering ≲ on R D+T so that u ≼ v in the poset if and only if their respective embeddings xu and xv are ordered in the feature space via ≲. Our class of trainable orders are parameterized with the following type of neural networks, which we call neural partial orders.</p><p>Consider a map T : R D+T → R T admitting the iterative representation</p><formula xml:id="formula_12">T (x u ) def. = z J , ∀j ∈ {1, . . . , J}, z j def. = V j σ sj ,sj •LR•(z j-1 )+b j , z 0 def. = (x u ) D+1:D+T ,<label>(4)</label></formula><p>where LR stands for LeakyReLU and • for function composition, with the depth parameter J ∈ N + , weight matrices V j ∈ I + T , and bias terms b 1 , . . . , b J ∈ R T . Also, note that T (x u ) ∈ R T , whereas when applying the subscript T (x u ) t ∈ R, the operation returns the entry of the coordinate embedding at dimension t. At each layer of T we use the (single trainable parameter, s = l in equation 2) activation s, . . . , s J &gt; 0. Moreover, we would like to highlight that D takes both xu , xv ∈ R D+T as input to compute distances, whereas T processes inputs independently. Hence, given any such T we define the partial order ≲ T on R D+T given, for each xu , xv ∈ R D+T , by</p><formula xml:id="formula_13">xu ≲ T xv ⇐⇒ T (x u ) t ≤ T (x v ) t , ∀t = D + 1, . . . , D + T.</formula><p>(5)</p><p>The next proposition shows that ≲ T always defines a partial order on the time dimensions of R D+T , for any T . This is formalized by noting that, for any D ∈ N + , the quotient space R D+T / ∼ under the equivalence relation x ∼ x ⇔ x 1:D = x1:D is isomorphic (as a vector space) to R T . Thus, there is no loss in generality in assuming that D = 0. Proposition 1 (Neural Spacetimes Always Implement Partial Orders). If T ∈ N + , D = 0, and T : R D+T → R T admits a representation as equation 4, then ≲ T is a partial order on R D+T . See page 21 for proof.</p><p>Just as the neural snowflakes of Borde &amp; <ref type="bibr">Kratsios (2023)</ref>, neural (quasi-)metrics implements a quasimetric on the spatial dimension R D in R D+T (ignoring the time dimensions used only to encode causality). A key difference between the two is that our formulation allows for the weighting or discovery of the importance of each spatial dimension, as well as the rescaling of each spatial dimension individually. </p><formula xml:id="formula_14">&lt; s j , l j ≤ 1 then D is a metric on R D .</formula><p>Furthermore, if T ∈ N + , then D is a pseudo-quasi-metric on R D+T . See page 21 for proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EMBEDDING GUARANTEES</head><p>We now present our main theoretical result which is the (global) embedding guarantees for NSTs. Definition 4 (Width). Let (P, ≲) be a poset. A S ⊆ P is called an anti-chain if for each u, v ∈ S neither u ≼ v nor v ≼ u. The width of (P, ≲) is the maximal cardinality of any anti-chain in P .</p><p>Note that to model anti-chains with our framework, more than one time dimension is needed. This allows our model to represent lack of causal connectivity between nodes that do not share a directed path between them in the graph. The result below uses the notion of doubling constant and metric space dimension from fractal geometry, see Appendix A.1.</p><p>Theorem 1 (Universal Spacetime Embeddings). Fix W, N ∈ N + , K &gt; 0, and let (P, d, ≲) be a finite causal metric space with doubling constant K, width W , and P def.</p><p>= {x v } v∈V ⊆ R N . There exists a D ∈ O(log(K)), an MLP E : R N → R D+W , T : R D+W → R W with representation equation 4, and D : R D+W × R D+W → [0, ∞) with representation equation 3 such that for each x u , x v ∈ P (i) Order Embedding:</p><formula xml:id="formula_15">x u ≼ x v if and only if E(x u ) ≲ T E(x v ), (ii) Metric Embedding: d(x u , x v ) ≤ D E(x u ), E(x v ) ≤ O log(K) 5 d(x u , x v ).</formula><p>Moreover, setting D = k, (ii) can be improved to an isocausal embedding, i.e. d(x u , x v ) = D E(x u ), E(x v ) . In either case, the number of non-zero parameters determining the neural spacetime triplet S = (E, D, T ) is Õ D + W + k 5/<ref type="foot" target="#foot_1">foot_1</ref> D 4 N . See page 25 for proof.</p><p>Theorem 1 guarantees that one never needs more than W time dimensions<ref type="foot" target="#foot_2">foot_2</ref> and #P space dimensions to have a (perfect) isocausal spacetime embedding. However, note that W + O(log(#P )) spacetime dimensions are guaranteed to provide an embedding with a very small distortion, which is likely good enough for most practical problems. Theorem 1 is extremal, in the sense that it holds for all causal metric spaces. One may, therefore, ask: How much can the result be improved for causal metric spaces with favourable properties?</p><p>In analogy with the Minkowski spacetime representations in <ref type="bibr" target="#b60">Law &amp; Lucas (2023)</ref>, which can accommodate directed line graphs, we now investigate when few (at most two) time dimensions are enough to guarantee a causal embedding for a poset. Using <ref type="bibr" target="#b7">Baker et al. (1970)</ref>, we first characterized posets which admit a spacetime embedding with two time dimensions via their Hasse diagrams. Proposition 3 (When two time dimensions are not enough). If the Hasse diagram of a poset (P, ≲) is not a planar graph, then there is no spacetime embedding E : P → R D+W , D : R D+W × R D+W → [0, ∞), T : R D+W → R W with W = 1, 2. See page 26 for proof.</p><p>We now improve both the temporal and spatial guarantees of the spacetime embedding in Theorem 1 for the posets characterized by the preceding proposition. To leverage the planar structure of the Hasse diagrams of these posets and thus enhance the spatial component of our spacetime embeddings, we instead equip any such poset (P, ≲) with the metric d H , defined as the graph geodesic distance on the undirected Hasse diagram of (P, ≲).</p><p>Theorem 2 (Low-Distortion Spacetime Embeddings in Time Dimensions). Fix k, N ∈ N + , and let (P, ≲, d H ) is k-point poset whose Hasse diagram for (P, ≲) is planar, P ⊂ R N . There is a ReLU MLP E : R N → R O(log(k))+2 , D : R O(log(k))+2 × R O(log(k))+2 → [0, ∞) with representation equation 3, and T : R O(log(k))+2 → R 2 as in equation 4 satisfying: for each x u , x v ∈ P (i) Causality:</p><formula xml:id="formula_16">x u ≼ x v if and only if E(x u ) ≲ T E(x v ), (ii) Low-Distortion: d H (x u , x v ) ≤ D E(x u ), E(x v ) ≤ O(log(k) 2 ) d H (x u , x v ).</formula><p>The number of parameters in S is O k 5/2 D 4 N log(N ) log k 2 diam(P, d H ) . Proof on page 26.</p><p>Our training procedure (Section 3.2) focuses on embedding the local structure of directed graphs and is supported by our theoretical guarantees that NSTs are expressive enough to encode the global structure of weighted DAGs. By focusing on local embeddings, our neural spacetime model can be trained to embed very large directed graphs, since local distance information is readily available, but global distance information is generally expensive to compute. Interestingly, Remark 4 (Local vs. Global NST.). The global and local embeddability of directional information is equivalent in our case due to the transitivity properties of DAGs and our representation spaces.</p><p>For completeness, we compare neural spacetimes to hyperbolic representation in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COMPUTATIONAL IMPLEMENTATION</head><p>In this section, we provide a discussion on how to bridge the theoretical embedding guarantees presented earlier into a computationally tractable model that can be optimized via gradient descent. Further details are provided in Appendix C.</p><p>Representing graph edge directionality as a partial order in the embedding manifold. Given a weighted directed graph</p><formula xml:id="formula_17">G D = (E D , V, W D ), let x u , x v ∈ R N be</formula><p>the feature vectors associated with the nodes u, v ∈ V , which we want to embed as events in our spacetime: S takes as input these features and maps them to an embedding in the manifold. The edge set E D induces a binary non-symmetric adjacency matrix A. In our case, G D is not any digraph but a DAG, hence if A uv = 1 ⇒ A vu = 0. Both entries can only be equal when they are 0, A uv = A vu = 0. In the latter case, the nodes may either be causally disconnected, or causally connected but not neighbors. If the input graph does not satisfy these properties our spacetime embedding parametrized by the NST will not be able to faithfully embed the input graph in terms of its edge directionality.</p><p>Representing graph edge weights as distances in the embedding manifold. Likewise, W D induces a distance matrix D with entries D uv being the causal distance between events u, v. Although theoretically this could be computed as the shortest path graph geodesic distance for two causality connected and distant u and v (similar to equation 1 but for weighted directed graphs, see Appendix D.1), in practice we only optimize for the one-hop neighborhood. In particular, if</p><formula xml:id="formula_18">u = v ⇒ D uv = 0, if u ≼ v ∧ v ∈ N (u) ∧ u ̸ = v ⇒ D uv &gt; 0.</formula><p>The distance D uv is ignored otherwise, that is, we do not model the distance between nodes in the original graph that are causally connected but outside the one-hop neighborhood of each other nor do we model the distance between events that are not causality connected (anti-chains). This is in line with the literature on graph construction via Lorentzian pre-length spaces <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023)</ref>. In the case of NSTs, it is achieved using the connectivity A uv as a mask in the loss function.</p><p>Local geometry optimization and global geometry implications. Note that although we optimize for the one-hop neighborhood of each node only, transivity of the causal connectivity of nodes across hops will be satisfied by definition of the partial order (Remark 4). Additionally, although the partial order between anti-chains is not directly optimized, as the number of time dimensions increases, it is increasingly probable that ≲ T will not be satisfied for nodes not causally connected, as desired. Conversely, there is no guarantee that when directly evaluating the distance between causally connected nodes using D we will obtain the graph geodesic distance between nodes. In summary, if x u , x v , x w ∈ R N are feature vectors for nodes u, v, w ∈ V , and xu , xv , xw ∈ R D+T are their E encodings in the intermediate Euclidean space used by the NST, if</p><formula xml:id="formula_19">(x u ≲ T xv ) ∧ (x v ≲ T xw ) ⇒ xu ≲ T xw . On the other hand, in general D(x u , xv ) + D(x v , xw ) = D uv + D vw ,</formula><p>and provided that this particular path corresponds to the graph geodesic  </p><formula xml:id="formula_20">D uv + D vw = d G (u, w), but D(x u , xv ) + D(x v , xw ) ̸ = D(x u , xw ).</formula><formula xml:id="formula_21">L G D (X, A, D) def. = u M u=u1 v M v=v1 L uv (S(x u , x v ), (A uv , D uv )) ,<label>(6)</label></formula><p>where x u extracts the row feature vector for u from matrix X. The goal is to learn appropriate embeddings so that nodes connected by directed edges are represented as causally connected events in the time dimensions of the manifold, and respect the distance D uv induced by the graph weights in the space dimensions. To do so, we can further divide the loss function into a (causal) distance loss, L D uv , and a causality loss, L C uv . We remind the reader of the definition xu , xv def.</p><p>= E(x u ), E(x v ) ∈ R D+T . Hence the loss can be partitioned into:</p><formula xml:id="formula_22">L uv def. = L D uv (D(x u , xv ), (A uv , D uv )) + L C uv (T (x u ), T (x v ), A uv ) .<label>(7)</label></formula><p>We dissect the two loss terms. The distance loss for a pair of nodes is the mean squared error (MSE) loss between the predicted and ground truth distance, with masking given by the adjacency matrix:</p><formula xml:id="formula_23">L D uv def. = A uv MSE (D(x u , xv ), D uv ) .<label>(8)</label></formula><p>On the other hand, the causality loss (with respect to the one-hop neighborhood) is</p><formula xml:id="formula_24">L C uv def. = A uv L * C T t=1 SteepSigmoid(T (x u ) t -T (x v ) t ) ,<label>(9)</label></formula><p>where L * C is a function that takes as input the expression above and</p><formula xml:id="formula_25">SteepSigmoid(x) = 1 1+e -10x</formula><p>(we omit some details here for simplicity, see Appendix C.3 for details). The loss for two causally connected events u ≼ v in the first neighborhood of each other (A uv = 1) is minimized when xu ≲ T xv is satisfied (equation 5). If the time coordinates of xv associated with the event v are all greater than those of xu for u, then all SteepSigmoid activation functions will return ≈ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>Synthetic Weighted DAG Embedding. We generate DAGs (see Appendix D.2) embedded in the 2D plane with associated local distances between nodes given by several metrics. We measure the embedding capabilities of NSTs compared to closed-form spacetimes such as the Minkowski and De Sitter spaces in <ref type="bibr" target="#b60">Law &amp; Lucas (2023)</ref>. Although we do not optimize the geometries in the case of the baselines, we do use a neural network encoder to map points to events in the manifolds. We quantify both the average and maximum metric distortion (the ratio between true and predicted distances) following <ref type="bibr">Kratsios et al. (2023b)</ref>, as well as the accuracy of the time embedding. As can be seen  <ref type="table" target="#tab_1">1</ref>, we are always able to embed edge directionality (0 for no edges embedded correctly, 1 for all edges embedded correctly). In terms of metric distortion, as the embedding dimension increases, both average and maximum distortion decrease. NSTs are particularly good at retraining low distortions in low-dimensional embedding spaces. See Table <ref type="table">9</ref> for more results.</p><p>Real-World Network Embedding. We test our approach on real-world networks. In Table <ref type="table" target="#tab_2">2</ref>, we present results for the Cornell, Texas, and Wisconsin (WebKB) datasets <ref type="bibr">(Rozemberczki et al., 2021)</ref>, which are based on webpages represented as nodes and directed hyperlinks between them. All the nodes have bag-of-word features that we use as input of the neural spacetime encoder. The neural partial order encodes the hyperlink directionality between websites, and the neural (quasi-)metric learns the connectivity strength as the cosine similarity between connected webpage features. We achieve very low distortions, showcasing the embedding capabilities of our network. Note that, from a metric learning perspective, real-world datasets are generally less challenging than the metrics presented in Table <ref type="table" target="#tab_1">1</ref>, which we chose to be particularly unconventional on purpose. In terms of the time embedding, we manage to mostly encode directionality; however, these datasets are not pure DAGs since they contain some directed cycles, so it is not possible to embed them perfectly. We also work with real-world gene regulatory network datasets <ref type="bibr">(Marbach et al., 2012)</ref> in line with spacetime representation learning literature <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023;</ref><ref type="bibr" target="#b82">Sim et al., 2021)</ref>. We achieve good spatial and causal embeddings for the In silico, Escherichia coli, and Saccharomyces cerevisiae datasets, see Table <ref type="table" target="#tab_2">2</ref>. Experimental details and hyperparameters for all setups can be found in Appendix D. Additionally, we present tree (spatial only) embedding experiments comparing neural snowflakes to neural (quasi-)metrics in NSTs, as well as hyperbolic neural networks (HNNs), in Appendix D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced the concept of neural spacetimes, a computational framework utilizing neural networks to construct spacetime geometries with multiple time dimensions for DAG representation learning. We decouple our representation into a product manifold of space, equipped with a quasi-metric, and time, which captures causality via a partial order. We propose techniques to build, optimize, and stabilize artificial neural networks with fractalesque activation functions, ensuring the learnability of valid geometries. Our main theoretical contributions include a global embeddability guarantee for posets into neural spacetimes, with embeddings being causal in time and asymptotically isometric in space. We demonstrate the efficacy of our approach through experiments on synthetic metric DAG embedding datasets and real-world directed graphs, showcasing its superiority over existing spacetime representation learning methods with fixed closed-form geometries.</p><p>Limitations. Our guarantees are restricted to embeddings for DAGs rather than arbitrary digraphs. We also find that, from an optimization perspective, it is easier to optimize the geometry locally rather than globally. This limitation is computational; as the number of nodes in the DAG grows, it becomes increasingly challenging to compute the ground truth shortest-path geodesic distance and the global causal structure between nodes, which are used as ground truth for training. However, all our theoretical guarantees apply both globally and locally. Additionally, the local transitivity of causal connectivity will implicitly hold globally, even when performing local optimization only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL BACKGROUND</head><p>In this section, we discuss relevant background on the dimension of a metric space and invertible matrices, which are used as part of our proofs in Appendix B. We also review the original neural snowflake model, pseudo-Riemannian manifolds, Lorentzian manifolds, and spacetimes in physics. Additionally, we provide an extended discussion on past literature in spacetime representation learning in the context of machine learning, and compare neural spacetimes to hyperbolic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DIMENSION AND SIZE OF A METRIC SPACES</head><p>The doubling constant in fractal geometry quantifies the complexity of a metric space by indicating how many smaller balls are needed to cover a larger ball. It is related to the Assouad dimension and other fractal dimensions. A small doubling constant suggests a low fractal dimension, while a large constant indicates a higher fractal dimension. This constant helps analyze self-similarity in fractals by providing a scale-invariant description of the space.</p><p>Our quantitative results use the following fractal notion of dimension/size of a metric space. Definition 5 (Doubling Constant). The doubling constant K ≥ 1 of a metric space (X, d) is the smallest integer k ≥ 1 for which the following holds: for every center x ∈ X and each radius r &gt; 0, every (open) ball B(x, r)</p><p>def.</p><p>= {u ∈ X : d(x, u) &lt; r} can be covered by at most k (open) balls of radius r/2; i.e., there exist x 1 , . . . ,</p><formula xml:id="formula_26">x k ∈ X such that B(x, r) ⊆ k i=1 B(x i , r/2).</formula><p>By this definition, K is the smallest possible value of k that satisfies the covering condition for all x ∈ X and all r &gt; 0. Note that every finite metric space, such as every weighted graph with the shortest path/geodesic distance, has a finite doubling constant. For example, if X is finite and has at least two points, then K ≥ 2.</p><p>The size of a metric space can also be quantified by its diameter and its separation. These respectively quantify the maximal and minimal distances between points therein. Definition 6 (Diameter). Let (X, d) be a metric space. The diameter of (X, d) is defined to be diam(X, d) def.</p><p>= sup x,x∈X d(x, x).</p><p>Definition 7 (Separation). Let (X, d) be a metric space. If X has at-least two points, then its separation is defined to be sep(X, d)</p><formula xml:id="formula_27">def. = inf x,x∈X, x̸ =x d(x, x), otherwise sep(X, d) def. = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 INVERTIBLE POSITIVE MATRICES</head><p>We employ square matrices that are both invertible and have positive entries for constructing our neural quasi-metric. These matrices guarantee that our snowflake produces trainable quasi-metrics, allowing the implemented distance function to distinguish between points (i.e., identify when two vectors are equal). In Appendix B, we demonstrate that all matrices in I + D are invertible and preserve [0, ∞) D under matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 NEURAL SNOWFLAKES</head><p>Neural spacetimes build on the neural snowflake model for weighted undirected graph embedding. Here, we describe the original construction in Borde &amp; <ref type="bibr">Kratsios (2023)</ref>.</p><p>Neural Snowflakes. Neural snowflakes leverage quasi-metric spaces and implement a learnable adaptive geometry</p><formula xml:id="formula_28">∥x u -xv ∥ f def. = f (∥x u -xv ∥), where xu , xv ∈ R D . In particular, it is a map f : [0, ∞) → [0, ∞), with iterative representation f (t) = u 1+|p| J u j = B j ψ aj ,bj (A j u j-1 ) C j for j = 1, . . . , J u 0 = u (10)</formula><p>where for j = 1, . . . , J, A j is a dj × d j-1 matrix, B j is a d j × dj -matrix, and C j is a 3 × 1 matrix all of which have non-negative weights and at least one non-zero weight. Furthermore, for j = 1, . . . , J, 0 &lt; a j ≤ 1, 0 ≤ b j ≤ 1, d 1 , . . . , d J ∈ N + , and d 0 = 1 = d J . Also, p ∈ R. Note that p controls the relaxation of the triangle inequality (C = 2 p ) and that for p ̸ = 0, the neural snowflake is a quasi-metric. The input to the neural snowflake, denoted as u, represents the Euclidean distance between xu and xv . This value serves as an intermediate representation within the neural snowflake, which is subsequently warped. In principle, u need not be the Euclidean distance. In the original neural snowflake implementation, ψ a,b : R → [0, ∞) is a tensorized trainable activation function which sends any vector u ∈ R K , for some</p><formula xml:id="formula_29">K ∈ N + , to the K × 3 matrix ψ a,b (u) whose k th row is ψ a,b (u) k def. = 1 -e -|u k | , |u k | a , log(1 + |u k |) b ,<label>(11)</label></formula><p>with 0 &lt; a and 0 ≤ b ≤ 1 being trainable. Such a construction provides universal graph embedding guarantees for weighted undirected graphs (Borde &amp; <ref type="bibr">Kratsios, 2023)</ref>. In this work, we will generalize this learnable adaptive geometry formulation to directed graphs borrowing ideas from spacetimes and pseudo-Riemannian manifolds.</p><p>A.4 PSEUDO-RIEMANNIAN MANIFOLDS AND LORENTZIAN SPACETIMES Next, we discuss pseudo-Riemannian manifolds and spacetimes, from which we draw inspiration for our neural spacetime construction (Section 3). Our framework generalizes neural snowflakes and their weighted undirected graph embedding properties to also incorporate causal connectivity and graph directionality. In other words, they implement an isocausal trainable embedding.</p><p>Pseudo-Riemannian manifolds are a generalization of Riemannian manifolds where the metric tensor g is not constrained to be positive definite. A d-dimensional pseudo-Riemannian manifold is a smooth manifold M equipped with a pseudo-Riemannian metric g. Formally, a pseudo-Riemannian metric tensor is a smooth, symmetric, non-degenerate bilinear form (called a scalar product) defined on the tangent bundle T M (disjoint union of all tangent spaces). The metric tensor at each point p ∈ M is denoted by g p . The metric assigns to each tangent space T p M a signature (±, ±, . . . , ±).</p><p>In particular, the tangent space T p M admits an orthonormal basis {e 1 , e 2 , . . . , e d } that satisfies ∀i ∈ {1, . . . , d}, g p (e i , e i ) = ±1. If g p (e i , e i ) = 1 for all i, then M is a Riemannian manifold, its metric tensor is positive definite, and its metric signature is (+, +, . . . , +).</p><p>Lorentzian manifolds are a subset of pseudo-Riemannian manifolds with a specific metric signature, typically denoted as (-, +, +, . . . , +) or (+, -, -, . . . , -). In our case, we define distances between causally connected events to be positive, and hence adopt the second sign convention. With this convention, a nonzero tangent vector v of a Lorentzian manifold is called causal if it satisfies</p><formula xml:id="formula_30">g p (v, v) ≥ 0, and it is called chronological if g p (v, v) &gt; 0.</formula><p>Spacetimes. The definition of a spacetime in general relativity varies depending on the author. Spacetimes are a subset of Lorentzian manifolds often defined so that they are equipped with a causal structure <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023)</ref>. Some spacetimes are called globally hyperbolic <ref type="bibr" target="#b32">(Geroch, 1970)</ref>. Global hyperbolicity ensures that the spacetime possesses a well-defined causal structure, meaning there are no closed timelike curves (CTCs) and every inextendible causal curve intersects every Cauchy surface (a spacelike hypersurface that slices through the entire manifold exactly once). This property allows for the deterministic evolution of physical fields and particles forward in time.</p><p>In mathematical terms, if there exists a smooth vector field V on M such that g p (V(p), V(p)) &gt; 0 for all p ∈ M , then the Lorentzian manifold is called a spacetime and it contains a causal structure. 4-dimensional spacetimes with signature (-, +, +, +) or (+, -, -, -) are used to model the physical world in general relativity. Spacetimes can exhibit various geometries, from flat Minkowski spacetime (used in special relativity) to curved spacetimes that account for gravitational effects predicted by Einstein's field equations. In summary, a spacetime often refers to a Lorentzian manifold that additionally satisfies certain physical and causal conditions. We refer the reader to <ref type="bibr" target="#b9">Beem et al. (1996)</ref> for details.</p><p>Relation to Neural Spacetimes. Neural Spacetimes are an adaptive geometry aiming to learn, in a differentiable manner, a suitable embedding in which nodes in a DAG can be represented as causally connected events. Although Neural Spacetimes (Section 3) draw inspiration from the concepts above-separating space and time into different dimensions and capturing causal connectivity between events-strictly speaking, Neural Spacetimes utilize higher time dimensions (T &gt; 1) than classical spacetimes used to model the physical world. Moreover, they also incorporate a quasimetric to model distance in the space dimensions, which relaxes the smoothness and other requirements inherent to pseudo-Riemannian metrics.</p><p>A.5 RELATED WORK ON SPACETIME REPRESENTATION LEARNING Related work in the machine learning literature that considers pseudo-Riemannian manifolds with multiple time dimensions includes ultrahyperbolic representations <ref type="bibr" target="#b61">(Law &amp; Stam, 2020;</ref><ref type="bibr" target="#b59">Law, 2021)</ref>. Ultrahyperbolic geometry is a generalization of the hyperbolic and elliptic geometries to pseudo-Riemannian manifolds. However, the works in <ref type="bibr" target="#b61">Law &amp; Stam (2020)</ref>; Law (2021) mostly focus on the optimization of such representations, they only consider undirected graphs and do not consider partial ordering.</p><p>Spacetime representation learning <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023)</ref> considers Lorentzian spacetimes (i.e. with one time dimension) to represent directed graphs. Following the formalism in Appendix A.4 with a metric signature (+, -, . . . , -), and noting ---→ x u x v , the logarithmic map of x v at x u , they consider that there exists an edge from u to v iff g xu ( ---→</p><p>x u x v , t) &gt; 0 and 0 &lt; g xu ( ---→ x u x v , ---→ x u x v ) &lt; ε where t is an arbitrary tangent vector that defines the future direction and ε &gt; 0 is a hyperparameter that defines the maximal length of the geodesic from x u to x v to draw a directed edge. The hyperparameter ε allows them to avoid connecting u to all the descendants of v. Their work is limited to the optimization of embeddings (i.e. not neural networks). We draw inspiration from <ref type="bibr" target="#b60">Law &amp; Lucas (2023)</ref> but consider multiple time dimensions in a framework that is easy to optimize for neural networks. Our local geometry optimization framework also allows us to ignore the hyperparameter ε, which is necessary when there is only one time dimension. In our case causality is controlled by the multi-dimensional neural partial order ≲ T instead. Also, note that all the previous works discussed above require selecting an appropriate manifold to model the embedding space a priori. Our work, on the other hand, not only optimizes the location of points in space but also the manifold itself. In other words, our framework models a trainable geometry.</p><p>A.6 HYPERBOLIC SPACES AS COMPARED TO NEURAL SPACETIME EMBEDDINGS Hyperbolic geometry has been shown to be relevant for describing undirected graphs without cycles, which are called trees. Indeed, in <ref type="bibr" target="#b77">Sarkar (2011)</ref>, it was shown that these trees can be algorithmically embedded into hyperbolic space with low distortion. Additionally, <ref type="bibr" target="#b31">Ganea et al. (2018)</ref> constructed a class of hyperbolic neural networks (HNNs) which can exploit trees represented in these spaces, and <ref type="bibr">Kratsios et al. (2023b)</ref> demonstrated that finite trees can indeed be embedded with arbitrarily low distortion. However, in Borde et al. (2023a), it was shown that an isometric embedding is not possible, except at "infinity" in the hyperbolic boundary of such a space <ref type="bibr" target="#b22">Dyubina &amp; Polterovich (2001)</ref> (which lies outside the manifold itself). The issue arises because hyperbolic space is a Riemannian manifold, and thus, it is incompatible with the branches in the tree itself. This is not the case for the fractal geometry implemented by the neural spacetime and neural snowflake of <ref type="bibr" target="#b52">Borde &amp; Kratsios (2023)</ref>. Indeed, in <ref type="bibr">(Maehara, 2013, Theorem 3.6)</ref>, it was shown that any finite tree can be isometrically embedded in Euclidean space R d , for d large enough, but with the "snowflaked" Euclidean distance between any two points x, y ∈ R d given by ∥x -y∥ α where α = 1/2. Since neural spacetime and neural snowflake can implement such fractal geometries, they are better suited to the geometry of trees.</p><p>In this paper, we propose a framework to represent DAGs that do not contain directed cycles. However, the underlying undirected graph of a DAG can contain cycles. This is for example the case for the DAG containing four nodes {ν i } 4 i=1 that satisfy only the following partial orders: ν 1 ≼ ν 2 ≼ ν 4 and ν 1 ≼ ν 3 ≼ ν 4 , without causality relation between ν 2 and ν 3 . The underlying undirected graph is an undirected cycle ν 1 , ν 2 , ν 4 , ν 3 , ν 1 , which is not appropriate for hyperbolic geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOFS B.1 PROPERTIES OF NEURAL SPACETIMES</head><p>Proof of Proposition 1. Fix T ∈ N + and set D = 0. Let T : R D+T → R T be a mapping with representation equation 4. We define the relation x ≲ T y if and only if T (x) t ≤ T (y) t for each t = 1, . . . , T.</p><p>We now show that ≲ T is a partial order on R D+T by verifying the three required properties.</p><p>Reflexivity: For any x ∈ R D+T , we have T (x) t ≤ T (x) t for every t = 1, . . . , T . Thus, x ≲ T x.</p><p>Antisymmetry: Suppose x, y ∈ R D+T satisfy x ≲ T y and y ≲ T x.</p><p>Then, for every t = 1, . . . , T , T (x) t ≤ T (y) t and T (y) t ≤ T (x) t , which implies that T (x) t = T (y) t for all t. Since each matrix V j ∈ I + T is injective (as noted in equation 12), and both the leaky ReLU function and the affine shifts are injective, the composition T is injective. Therefore, T (x) = T (y) implies that x = y.</p><p>Transitivity: Let x, y, z ∈ R D+T be such that x ≲ T y and y ≲ T z.</p><p>Then, for every t = 1, . . . , T , we have</p><formula xml:id="formula_31">T (x) t ≤ T (y) t and T (y) t ≤ T (z) t .</formula><p>By the transitivity of the standard order on R, it follows that T (x) t ≤ T (z) t for all t. Hence, x ≲ T z.</p><p>Since ≲ T is reflexive, antisymmetric, and transitive, it is a partial order on R D+T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 2.</head><p>Comment: The main challenge in this proof is showing that D separates points in R D ; that is, for any x, y ∈ R D we have D(x, y) = 0 if and only if x = y. Note that in the main text D takes as input the spatial coordinates of the Euclidean embeddings xu , xv ∈ R D+T instead, but we avoid this notation here for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetry of D:</head><p>The map D is symmetric since</p><formula xml:id="formula_32">|σ s0,l0 (x) -σ s0,l0 (y)| d i=1 = |σ s0,l0 (y) -σ s0,l0 (x)| d i=1 ;</formula><p>meaning that the map u → u 0 is symmetric. Consequentially, D is symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Negativity of D:</head><p>Since each of the matrices W 1 , . . . , W J have non-negative entries then they map [0, ∞) D to itself. To see this note that for any = W J σ s J+1 ,l J+1 (u J ),</p><formula xml:id="formula_33">d, D ∈ N + if u ∈ [0, ∞) D , W ∈ R d×D</formula><formula xml:id="formula_34">u j def. = W j σ sj ,lj • (u j-1 )</formula><p>for j = 1, . . . , J -1,</p><formula xml:id="formula_35">u 0 def. = |σ s0,l0 (x) -σ s0,l0 (y)|,</formula><p>where, we recall equation 2, which states that for each s, l &gt; 0 and x ∈ R (the activation function is applied pointwise)</p><formula xml:id="formula_36">σ s,l (x) def. = sgn(x) |x| s if |x| &lt; 1 sgn(x) |x| l if |x| ≥ 1 .</formula><p>Since W 1 = λI D + | W| and some matrix W ∈ R D×D ; then, the entries of W are non-negative and for i, j = 1, . . . , D we have that</p><formula xml:id="formula_37">(W 1 ) i,i = λ + D j=1 | Wi,j | &gt; D j=1 | Wi,j | ≥ D j=1; j̸ =i | Wi,j | def. = R i ,<label>(12)</label></formula><p>where, we emphasize that the strictness of the first inequality is due to the positivity of λ. Thus, the Gershgorin circle theorem, see <ref type="bibr" target="#b33">Gershgorin (1931)</ref>, implies that the eigenvalues of W 1 belong to the set Λ 1 ⊆ C defined by</p><formula xml:id="formula_38">Λ 1 def. = D i=1 B 2 (W 1 ) i,i , R i</formula><p>where, for u ∈ C and r ≥ 0 we define B 2 u, r def.</p><p>= {z ∈ C : ∥u -z∥ ≤ r}. Since the computation in equation 12 showed that R i &lt; (W 1 ) i,i for each i = 1, . . . , D then 0 ̸ ∈ Λ 1 . Thus, W 1 is invertible.</p><formula xml:id="formula_39">Consequentially, R D ∋ u → W 1 u ∈ R D is injective.</formula><p>For any specification of s, l &gt; 0 the (componentwise) map σ s,l • : R D ∋ u → (σ s,l (u i )) D i=1 ∈ R D is injective as it is componentwise monotone increasing. In the notation of equation 3: Since u 0 = 0 if and only if x = y then the map</p><formula xml:id="formula_40">R D × R D ∋ (x, y) → W 1 → u 1 ∈ R D</formula><p>is equal to the zero vector if and only if x = y. Since W 2 ∈ [0, ∞) d then D(x, y) ≥ 0; thus, D(x, y) = 0 if and only if x = y. We have just shown that D is point-separating (i.e. D(x, y) = 0 if and only if x = y) and in the process have seen that D is positive (i.e. D(x, y) ≥ 0). That is, D separates points in R D .</p><p>Case II: Pseudo-Metric for Positive Time Dimensions if T &gt; 0: If instead T ∈ N + , then let 0 T denote the zero vector in R T , and x, y ∈ R D with x ̸ = y. Then, D (0 T , x), (0 T , y) = 0. Thus, D does not separate points whenever T &gt; 0. Note that from a computational perspective, we account for this using masking during training, see Section 3.2 and Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relaxed Triangle Inequality:</head><p>It remains to show that a relaxed triangle inequality holds. Again, we consider the case when T = 0, with the general case following identically up to a more cumbersome notation. Let x, y ∈ R D , using the notation of equation 3, for j = 0, . . . , J define the constant</p><formula xml:id="formula_41">β j def.</formula><p>= max{max{s j , l j } -1, 0}.</p><p>Note that, β j = 0 whenever both s j and l j ≤ 1 and it equals to (max{s j , l j }-1) otherwise. Further, note that if s j = l j &gt; 1, then β j = s j -1.</p><p>By definition of the operator norm of each matrix W j we have that: for j = 1, . . . , J ∥u j ∥ ≤ ∥W j ∥ op ∥σ sj ,lj (u j-1 )∥.</p><p>(13)</p><p>By <ref type="bibr" target="#b88">(Xia, 2009</ref>, Example 2.2): for each j = 1, . . . , J we have that</p><formula xml:id="formula_42">∥σ sj ,lj (u j-1 )∥ ≤ 2 βj ∥u j-1 ∥. (<label>14</label></formula><formula xml:id="formula_43">)</formula><p>Upon combining the bounds in equation 13 and equation 14 for each j = 1, . . . , J we arrive at</p><formula xml:id="formula_44">D(x, y) ≤ J j=1 ∥W j ∥ op 2 βj ∥u 0 ∥ = 2 J j=1 βj J j=1 ∥W j ∥ op ∥u 0 ∥.<label>(15)</label></formula><p>Again using <ref type="bibr">(Xia, 2009, Example 2</ref>.2) we have that</p><formula xml:id="formula_45">∥u 0 ∥ = d i=1 |σ s0,l0 (x i ) -σ s0,l0 (y i )| 2 1/2 ≤ d i=1 2 β0 |x i -y i | 2 1/2 =2 β0 d i=1 |x i -y i | 2 1/2 =2 β0 ∥x -y∥.<label>(16)</label></formula><p>Upon combining the estimates in equation 15 with those in equation 16 we find that</p><formula xml:id="formula_46">D(x, y) ≤2 J j=1 βj J j=1 ∥W j ∥ op ∥u 0 ∥ ≤2 J j=1 βj J j=1 ∥W j ∥ op 2 β0 ∥x -y∥ (17) =2 J j=0 βj J j=1 ∥W j ∥ op ∥x -y∥.<label>(18)</label></formula><p>Finally, notice that if for each j = 1, . . . , J the matrix W j is orthogonal and 0 &lt; s j , l j ≤ 1 then equation 18 becomes 1 ∥x -y∥; in which case D is a metric. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 EMBEDDING RESULTS</head><p>We will routinely use the following ordering. Definition 9 (Product Order). Let T ∈ N + . The product, or coordinate, order ≲ × on R T is defined for each x, y ∈ R T by x ≲ × y ⇔ x t ≤ y t for all t = 1, . . . , T.</p><p>Equivalently, x ≲ × y if 1 = T t=1 I xt≤yt . (We use ≲ T to refer to the ordering given by the neural partial order specifically).</p><p>A key step in our main results is the ability of neural spacetimes to encode the product order in time and snowflake metrics in space. This is quantified by the following helper lemma. Lemma 1 (An Implementation Lemma for Neural Spacetimes). Let α &gt; 0, 1 ≤ p &lt; ∞, and T, D ∈ N + . Then, there exist maps T : R D+T → R T and D : R D+T × R D+T → [0, ∞), with respective representations equation 3 and equation 4, such that for all x, y ∈ R D+T (i) Implementation of Product Ordering: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 1.</head><p>In what follows we use, for any i ∈ N + , let I i denotes the i × i identity matrix, let 0 i denote the zero vector in R i , and we use 1 R i to denote the identity map on R i .</p><p>Step 1 -Implementation of the Product Order Observe that the product ordering (Definition 9) and the ordering equation 5 coincide if T (x) = (x) D+1:D+T for all x ∈ R D+T . Consider the map T : R D+T → R T defined for any x ∈ R D+T by</p><formula xml:id="formula_47">T (x) def. = I T σ 1,1 • (x) D+1:D+T + 0 T (19)</formula><p>is of the form of equation 4 and by construction T (x) = x D+1:D+T for all x ∈ R D+T . By construction, T has: depth 1, width T , and T + 2 non-zero parameters (the identity matrix diagonal entries plus s and l).</p><p>Step 2 -Implementation of the α-Snowflake of the ℓ p -Quasi-Metric</p><formula xml:id="formula_48">Fix J = 2, set s 0 = l 0 def. = 1, s 1 = l 1 def.</formula><p>= p, W 1 = I D , and</p><formula xml:id="formula_49">s 2 = l 2 def. = α/p, 1 D def. = W 2 ∈ R 1×D with (W 2 ) i = 1 for each i = 1, . . . , D. Therefore, u 0 def. = |σ 1,1 (x i ) -σ 1,1 (y i )| d i=1 = |x i -y i | d i=1 ∴ u 1 def. = I D σ p,p • (u 0 ) = |x i -y i | p d i=1 ∴ u 2 def. = σ α/p,α/p 1 D u 1 = D i=1 |x i -y i | p α p = ∥(x) 1:D -(y) 1:D ∥ α p .<label>(20)</label></formula><p>Consequentially, the map D(x, y)</p><p>def.</p><p>= u 2 , with u 2 defined by equation 20, is of the form of equation 3 and satisfies: for each</p><formula xml:id="formula_50">x, y ∈ R D+T D(x, y) = ∥(x) 1:D -(y) 1:D ∥ α p .</formula><p>Observe that D has depth 2, width d, and 2</p><formula xml:id="formula_51">• 3 + D + D = 2(3 + D) non-zero parameters.</formula><p>The following is a more technical version of Theorem 1, which we prove here as it implies the version found in the main text of our manuscript. Note that so far we have used x, y ∈ R D+T for our proofs and derivations. Next, we use x u , x v ∈ R N to denote the original node features of two given nodes u and v. Recall the identification V ∈ v ↔ x v ∈ R N from Section 2.1. x, y used thus far would correspond to the encoded node features E(x u ), E(x v ). Theorem 3 (Universal Spacetime Embeddings). Fix W, N, k ∈ N + , K &gt; 0, and let (P, ≲, d) be a k-point causal metric space with doubling constant K, width W , and feature encoding P def.</p><p>= {x v } v∈V ⊆ R N . There exists a D ∈ O(log(K)), T def.</p><p>= W , an MLP E : R N → R D+T , T : R D+T → R T with representation equation 4, and D : R D+T × R D+T → [0, ∞) with representation equation 3 such that: for each x u , x v ∈ P (i) Order Embedding:</p><formula xml:id="formula_52">x u ≼ x v if and only if E(x u ) ≲ T E(x v ), (ii) Metric Embedding: d(x u , x v ) ≤ D E(x u ), E(x v ) ≤ O log(K) 5 d(x u , x v ),</formula><p>Furthermore, there is some D ≤ k such that (ii) can be improved to</p><formula xml:id="formula_53">d(x u , x v ) = D E(x u ), E(x v )</formula><p>In either case, we have the following parametric complexity estimates: Proof of Theorem 1. Let α ∈ (1/2, 1) and δ ∈ (0, 1], both of which will be fixed retroactively.</p><p>Step 1 -Causal Embedding: By <ref type="bibr" target="#b20">(Dilworth, 1950</ref>, Theorem 1.1), (P, ≲) has width W only if there exists an order embedding Ẽ⋆ : P → ({0, 1} W , ≲ × ). Since the inclusion ι of ({0, 1} W , ≲ × ) into (R W , ≲ × ) trivially defines an order embedding, then</p><formula xml:id="formula_54">E (1) def. = ι • Ẽ : (P, ≲) → (R W , ≲ ×</formula><p>) is an order embedding.</p><p>Step 2 (Case I) -Metric Embedding -Low-Distortion Case: By Naor and Neiman's Assouad embedding theorem, as formulated in <ref type="bibr" target="#b68">(Naor &amp; Neiman, 2012</ref>, Theorem 1.2), there exists an absolute constant c &gt; 0 such that for each 1/2 &lt; α &lt; 1 and 0 &lt; δ ≤ 1, there exists a bi-Lipschitz embedding</p><formula xml:id="formula_55">E (2) (P, d 1-α ) → (R D , ∥ • ∥ 2 ) satisfying: for each x u , x v ∈ P d 1-α (x u , x v ) ≤ ∥E (2) (x u ) -E (2) (x v )∥ 2 ≤ c log(K) 1 -α 1+δ d 1-α (x u , x v )<label>(21)</label></formula><p>where<ref type="foot" target="#foot_4">foot_4</ref> D ∈ O(log(K)/δ). Equivalently, for each</p><formula xml:id="formula_56">x u , x v ∈ P d(x u , x v ) ≤ ∥E (2) (x u ) -E (2) (x v )∥ 1/(1-α) 2 ≤ c 1/(1-α) log(K) 1 -α 1+δ 1-α d(x u , x v ).<label>(22)</label></formula><p>Step 2 (Case II) -Metric Embedding -Isometric Case:</p><p>Since (P, d) is such that P is finite, i.e. k = #P &lt; ∞ then,<ref type="foot" target="#foot_5">foot_5</ref> implies that setting α def.</p><p>= γ(k -1)/2 def.</p><p>= log 2 (1 + 1/(k -1))/2 ∈ (0, 1] (with the case where α = 1 only being achieved when P is a singleton) there exists some D ∈ N + and Ẽ(2) : P → R D such that: for each</p><formula xml:id="formula_57">x u , x v ∈ P d(x u , x v ) α = ∥ Ẽ(2) (x u ) -Ẽ(2) (x v )∥ 2 . (<label>23</label></formula><formula xml:id="formula_58">)</formula><p>Since D</p><p>def.</p><p>= dim span{ Ẽ(x u )} xu∈P ≤ #P = k and since all D dimensional linear of R D subspaces are isometrically isomorphic to the D-dimensional Euclidean space; then there exists some linear surjection T : R D → R D which restricts to a bijective isomorphism from span{ Ẽ(x u )} xu∈P to R D (both equipped with Euclidean metrics). Then, the composite map</p><formula xml:id="formula_59">E (2) def. = T • Ẽ(2) : (P, d α ) → (R D , ∥ • ∥) is an isometric embedding. Consequentially: for each x u , x v ∈ P we have that d(x u , x v ) = ∥E (2) (x u ) -E (2) (x v )∥ 1/α 2</formula><p>(24) and we emphasize that D ≤ k.</p><p>Step 3 -Interpolation: Define the "spacetime embedding" E : P → R D+W by: for each</p><formula xml:id="formula_60">x u ∈ P E(x u ) def. = E (1) (x u ), E (2) (x u ) . (<label>25</label></formula><formula xml:id="formula_61">)</formula><p>We memorize/interpolate E using <ref type="bibr">(Kratsios et al., 2023a, Lemma 20)</ref>; thus, there exists a ReLU MLP Ê : R N → R D+W satisfying: for each x u ∈ P we have that</p><formula xml:id="formula_62">Ê(x u ) = E(x u ). (<label>26</label></formula><formula xml:id="formula_63">)</formula><p>Furthermore, <ref type="bibr">(Kratsios et al., 2023a, Lemma 20)</ref> guarantees that number of trainable parameters defining</p><formula xml:id="formula_64">E is O k 5/2 D 4 N log(N ) log k 2 aspect(P, d) ,</formula><p>where similarly to <ref type="bibr" target="#b54">Krauthgamer et al. (2005)</ref> the aspect ratio of the finite metric space (P, d) is defined by</p><formula xml:id="formula_65">aspect(P, d) def. = max xu,xv∈P d(x u , x v ) min xu,xv∈P ; xu̸ =xv d(x u , x v ) def. = diam(P, d) sep(P, d) .</formula><p>Thus, the number of non-zero parameters determining E is at-most</p><formula xml:id="formula_66">O k 5/2 D 4 N log(N ) log k 2 diam(P, d) sep(P, d) .</formula><p>Furthermore, by equation 22 (in case I) and by definition of E, we have that: for each</p><formula xml:id="formula_67">x u , x v ∈ P d(x u , x v ) ≤ ∥π • E(x u ) -π • E(x v )∥ 1/(1-α) 2 ≤ c 1/(1-α) log(K) 1 -α 1+δ 1-α d(x u , x v ) (27) x u ≲ x v ⇔ E(x u ) ≲ T E(x v ). (<label>28</label></formula><formula xml:id="formula_68">)</formula><p>Retroactively set δ def.</p><p>= 1/4 ∈ (0, 1] and α def.</p><p>= 3/4 ∈ (1/2, 1). Then, equation 27 and equation 28 become</p><formula xml:id="formula_69">d(x u , x v ) ≤ ∥π • E(x u ) -π • E(x v )∥ 1/(1-α) 2 ≤ 1024 c 4 log(K) 5 d(x u , x v ) (29) x u ≲ x v ⇔ E(x u ) ≲ T E(x v ). (<label>30</label></formula><formula xml:id="formula_70">)</formula><p>In case II, instead, equation 24 implies that:</p><formula xml:id="formula_71">for each u, v ∈ V d(x u , x v ) = ∥π • E(x u ) -π • E(x v )∥ 1/α 2 (31) x u ≲ x v ⇔ E(x u ) ≲ T E(x v ). (<label>32</label></formula><formula xml:id="formula_72">)</formula><p>Step 4 -Encoding Product Order in time and snowflake in space as (T , D)</p><p>In either case, since R D+W is equipped with the product order on its last W (temporal) dimensions and the 1/(1 -α) (resp. 1/α)-snowflake of the Euclidean (ℓ 2 ) metric on its first D (spatial) dimension, then the conditions for Lemma 1 are met. Applying Lemma 1 concludes the proof.</p><p>Proof of Proposition 3. Set W ∈ {1, 2} and D ∈ N + . By definition, of the order on R D+W , given in Lemma 1 (i), a spacetime embedding E : P → R D+W exists if and only if p • E : P → R W is an order embedding into (R W , ≲ × ) where ≲ × is the product order and p : R D+W ∋ (</p><formula xml:id="formula_73">x i ) D+W i=1 → (x i ) W</formula><p>i=1 ∈ R W is the canonical projection. By <ref type="bibr">(Baker et al., 1970, Theorem 6</ref>.1) an order embedding of (P, ≲) into (R 2 , ≲ × ) exists if and only if (P, ≲) has a planar Hasse diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2. Step 1 -Time Embedding</head><p>Since the Hasse diagram of (P, ≲) has been assumed to be planar, then <ref type="bibr">(Baker et al., 1970, Theorem 6.1)</ref> implies that there exists an other embedding</p><formula xml:id="formula_74">E (1) : (P, ≲) → (R 2 , ≲ × ).</formula><p>Step 2 -Spatial Embedding Since we have assumed that the Hasse diagram of (P, ≲) is planar, then we may instead use <ref type="bibr">(Rao, 1999, Theorem 9)</ref> to deduce that there exists a D ∈ N + and an injective map</p><formula xml:id="formula_75">E (2) : P → R D such that: for each x u , x v ∈ P 1 c log(k) d H (x u , x v ) ≤ ∥E (2) (x v ) -E (2) (x u )∥ 2 ≤ d H (x u , x v ),<label>(33)</label></formula><p>where c &gt; 0 is an absolute constant. Resealing E (2) by a factor of c log(k), and multiplying across equation 33 by c log(k), we find that: for each</p><formula xml:id="formula_76">x u , x v ∈ P d H (x u , x v ) ≤ ∥E (2) (x v ) -E (2) (x u )∥ 2 ≤ c log(k) d H (x u , x v ).<label>(34)</label></formula><p>Furthermore, by remark at the beginning of <ref type="bibr">(Rao, 1999, Section 4</ref>) by the Johnson-Lindenstrauss lemma, as formulated in <ref type="bibr">(Dubhashi &amp; Panconesi, 2009, Theorem 2</ref>.1), one may take k def.</p><p>= #P an incur an additional factor of c log(k), for an absolute constant c &gt; 0, in the distortion in equation 34; that is find that: for each u, v ∈ P</p><formula xml:id="formula_77">d H (x u , x v ) ≤ ∥E (2) (x v ) -E (2) (x u )∥ 2 ≤ C log(k) d H (x u , x v ),<label>(35)</label></formula><p>where C</p><p>def.</p><p>= cc &gt; 0. Set p def.</p><p>= 1 and recall that ∥ k) , for any C &gt; 0. Thus, equation 35 implies that: for each x u , x v ∈ P we have</p><formula xml:id="formula_78">• ∥ 2 ≤ ∥ • ∥ 1 ≤ C log(k)∥ • ∥ 2 on R C log(</formula><formula xml:id="formula_79">d H (x u , x v ) ≤ ∥E (2) (x v ) -E (2) (x u )∥ p 1 ≤ C ′ log(k) 2 d H (x u , x v ),<label>(36)</label></formula><p>where</p><formula xml:id="formula_80">C ′ def. = C C &gt; 0 and p def.</formula><p>= 1.</p><p>Step 3 -Interpolation Embedding Pick some</p><formula xml:id="formula_81">x ⋆ ∈ R D+2 \ [∪ 2 i=1 E (i) (P )]. Since P ⊂ R N then consider the map E : R N → R D+2 defined for each x ∈ R N by E(x) def. = E (1) (x), E (2) (x) if x ∈ P x ⋆ if x ̸ ∈ P.</formula><p>We memorize/interpolate E using <ref type="bibr">(Kratsios et al., 2023a, Lemma 20)</ref> over the finite set P . Whence, there exists a ReLU MLP Ê : R N → R D+W satisfying: for each</p><formula xml:id="formula_82">x u ∈ P Ê(x u ) = E(x u ).<label>(37)</label></formula><p>Again, as in the proof of Theorem 1, <ref type="bibr">(Kratsios et al., 2023a, Lemma 20)</ref> guarantees that number of trainable parameters defining E is</p><formula xml:id="formula_83">O k 5/2 D 4 N log(N ) log k 2 aspect(P, d H ) ,</formula><p>where, in this case, aspect ratio of the finite metric space (P, d H ) is defined by</p><formula xml:id="formula_84">aspect(P, d H ) def. = max xu,xv∈P d H (x u , x v ) min xu,xv∈P ; xu̸ =xv d H (x u , x v ) def. = diam(P, d H ) sep(P, 1) = diam(P, d H ),</formula><p>where we used the fact that the minimal edge weights between adjacent distance nodes are equal to 1 in an unweighted graph. Consequentially, the number of non-zero parameters determining E is</p><formula xml:id="formula_85">O k 5/2 D 4 N log(N ) log k 2 diam(P, d H ) .</formula><p>Step 4 -Encoding Product Order in time and snowflake in space as (T , D)</p><p>Since R D+2 is equipped with the product order on its last two (time) dimensions and the p def.</p><p>= 1/αsnowflake of the Euclidean (ℓ 2 ) metric on its first D (space) dimension, then the conditions for Lemma 1 are met. Applying Lemma 1 concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPUTATIONAL IMPLEMENTATION OF NEURAL SPACETIMES</head><p>In this appendix, we provide an extended discussion and additional details regarding the computational implementation of neural spacetimes. First, we explain what makes an activation function fractalesque, followed by an analysis of the behavior of our proposed activation function for neural (quasi-)metrics and neural partial orders. Moreover, we provide an algorithmic description of the entire pipeline, extend the discussion on causality loss enforcement and time embeddings, and propose optimization strategies at both the local and global geometry levels. Additionally, we explore potential classical algorithms for computing the optimal embedding dimensions, weight initialization strategies for the networks, and the algorithmic differences between neural snowflakes and neural (quasi-)metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 WHAT MAKES AN ACTIVATION FUNCTION FRACTALESQUE?</head><p>The terminology snowflake arises as follows. Consider the activation function σ : R → R given by σ(x) = |x| α and set α = log(4)/ log(3). Then, d α (x, y) def.</p><p>= σ(|x -y|) defines a metric on R and the metric space (R, d α ) is isometric to the (von) Koch snowflake fractal (Figure <ref type="figure" target="#fig_8">4</ref>) X ⊂ R 2 endowed with the distance obtained by restriction of the Euclidean distance on R 2 to X . Here, α is chosen such that (X , d α ) has positive and finite α-Hausdorff measure. Indeed, for any general α ∈ (0, 1) one can show that d α defines a metric on R with the property that any line segment has infinite length, see <ref type="bibr" target="#b80">Semmes (1996)</ref>. Thus, intuitively, the snowflake space (R, d α ) contains infinitely more space to place points in while also maintaining a similar geometry to the Euclidean line; see e.g. <ref type="bibr">(Acciaio et al., 2024, Lemma 7.1)</ref>. See <ref type="bibr" target="#b86">Tyson &amp; Wu (2005)</ref> for an intrinsic characterization of metric spaces which arise as snowflakes of a subset of a (possibly multidimensional) Euclidean space.</p><p>More generally, consider a continuous activation function σ : R → R . Since σ is continuous and</p><formula xml:id="formula_86">[0, 1] is compact then σ admits a modulus of continuity ω : [0, ∞) → R on [0, 1]; i.e. for each x, y ∈ R |σ(x) -σ(y)| ≤ ω(|x -y|).</formula><p>(38) We think of σ as being fractalesque if it is sub-Hölder and non-Lipschitz near 0. That is, there is an α ∈ (0, 1) (note that α &lt; 1) and some L &gt; 0, such that the right-hand side of equation 38 can be bounded above as</p><formula xml:id="formula_87">|σ(x) -σ(y)| ≤ ω(|x -y|) ≤ L |x -y| α .</formula><p>(39) Thus, if the upper-bound in equation 39 holds for an activation function σ, e.g. our snowflake activation function defined in equation 11, then the Euclidean distance between points in the image of the componentwise application of σ are comparable to those of the snowflaked space (R d , ∥ • ∥ α ); making it fractalesque.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 THE ACTIVATION FUNCTION IN EQUATION 2</head><p>In this work, we leverage fractalesque activation functions, which exhibit different training dynamics from typical activation functions used in artificial neural networks. We aim to visualize this type of activations and to gain an intuitive understanding of their nature.</p><p>Neural spacetimes leverage the following equation:</p><formula xml:id="formula_88">σ s,l (x) def. = sgn(x) |x| s if |x| &lt; 1 sgn(x) |x| l if |x| ≥ 1,</formula><p>where the sign function sgn returns 1 for x ≥ 0 and -1 for x &lt; 0. Both neural (quasi-)metrics and neural partial orders in neural spacetimes implement variations of this expression.</p><p>We visualize the activation using different values for s and l in Figure <ref type="figure" target="#fig_11">5a</ref>. As we can see from the plot, the function is antisymmetric about the y-axis, monotonically increasing, and behaves differently depending on the magnitude of the input. The network can learn to optimize s and l alongside linear projection weights, which can route inputs to different regions of the function, to model different scales independently.   In Figure <ref type="figure" target="#fig_11">5b</ref> we plot a finite difference approximation of its derivative. The derivative rapidly increases for absolute values of the input near 0 for exponents s &lt; 1, meaning it has a very high second derivative around these values. This can indirectly destabilize training. Moreover, the absolute value of the derivative itself is also large. This can easily make training unstable as well, especially if we compose this function with itself over multiple layers; when computing backpropagation using the chain rule, the gradients can easily explode.</p><p>In the case of the neural partial order, we implement a variation of the activation which is not able to distinguish between small and large scales since s = l. Although this would be theoretically enough, and corresponds to the activation in Figure <ref type="figure" target="#fig_11">5c</ref>, we empirically found it to be slow at training. Instead, we compose the fractalesque activation with a LeakyReLU (refer back to equation 4), which we find aids optimization and accelerates learning. As we can see in Figure <ref type="figure" target="#fig_11">5d</ref>, this results in a clear difference in the behavior of the function on the negative and positive axes of the input. By changing the exponential coefficient, which is learned by gradient descent, the network can substantially alter the behavior of the activation. As before, for exponential coefficients smaller than 1, this activation can lead to instabilities for small input values. For s = 1 we recover LeakyReLU.</p><p>Based on these observation we restrict all activations to learn s and l coefficients greater than 1 only. These allows us to use fractalesque activations while restristing the functions to behave more similarly to activations used in the literature such as ReLU, LeakyReLU or SiLU functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 ALGORITHMIC DESCRIPTION</head><p>We complement the mathematical descriptions of neural spacetimes in Section 3 with the following algorithm summaries. As discussed in the main text, let N ∈ N + be the dimensionality of the feature vectors x u , x v ∈ R N associated with nodes u, v ∈ V , where V represents the set of nodes of the digraph G D = (E D , V, W D ). Fix a space dimension D ∈ N + and a time dimension T ∈ N + .</p><p>A neural spacetime is a learnable triplet S = (E, D, T ), where:</p><formula xml:id="formula_89">• E : R N → R D+T is an encoder network (MLP), • D : R D+T × R D+T → [0, ∞) is a learnable quasi-metric on R D , and</formula><p>• T : R D+T → R T is a learnable partial order on R T .</p><p>In particular, we implement D and T as two distinct artificial neural networks inspired by neural snowflakes, which process the space and time dimensions of encoded feature vectors xu , xv</p><formula xml:id="formula_90">def. = E(x u ), E(x v ) ∈ R D+T in parallel. Algorithm 1: Neural (quasi-)metric, D Require: xu , xv ▷ Two Encoded Node Features Vectors return s uv ▷ Distance u 0 ← |σ s0,l0 • (x u ) 1:D -σ s0,l0 • (x v ) 1:D | For j = 1 to J u j ← W j σ sj ,lj • (u j-1 ) end s uv ← u J Algorithm 2: Neural Partial Order, T Require: xu ▷ Encoded Node Feature Vector return t u ▷ Temporal Encoding z 0 ← (x u ) D+1:D+T For j = 1 to J z j ← V j σ sj • LeakyReLU •(z j-1 ) + b j end t u ← z J Algorithm 3: Neural Spacetime, S = (E, D, T ) (Forward Pass) Require: x u , x v ▷ Two Node Features Vectors return s uv , t u , t v ▷ Distance and Temporal Encodings xu , xv ← E(x u ), E(x v ) ▷ Enconde Feature Vectors s uv ← D(x u , xv ) ▷ Compute Distance t u , t v ← T (x u ), T (x v )</formula><p>▷ Apply Temporal Encoding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 CAUSALITY LOSS AND TIME EMBEDDING</head><p>In Section 3.2 we introduced the procedure used to optimize our neural spacetime model. The metric embedding in space is relatively simple an analogous to previous works (Borde <ref type="bibr">&amp; Kratsios, 2023;</ref><ref type="bibr">Kratsios et al., 2023b)</ref>. In this appendix we expand on the computational approach used for embedding causality and optimizing the time embedding.</p><p>The causality loss is given in the main text by:</p><formula xml:id="formula_91">L C uv def. = A uv L * C T t=1 SteepSigmoid(T (x u ) t -T (x v ) t ) ,</formula><p>with SteepSigmoid(x) = 1 1+e -10x (the value 10 was found experimentally, making the function too steep can lead to training instabilities), and where L * C slightly modifies the expression (equation 40):</p><formula xml:id="formula_92">T t=1 SteepSigmoid(T (x u ) t -T (x v ) t ).</formula><p>For the sake of understanding, let us focus on the equation above first.</p><formula xml:id="formula_93">SteepSigmoid(x) → 0 as x → -∞.</formula><p>In particular, SteepSigmoid(x) tends to 0 faster than Sigmoid(x) as x → -∞ (see Figure <ref type="figure" target="#fig_12">6</ref>). In asymptotic notation:</p><formula xml:id="formula_94">lim x→-∞ SteepSigmoid(x) Sigmoid(x) = 0. Importantly, SteepSigmoid (T (x u ) t -T (x v ) t ) ≈ 0∀ (T (x u ) t -T (x v ) t ) &lt; 0 even if |T (x u ) t -T (x v ) t | is small.</formula><p>As discussed in Section 3.2, the loss for two causally connected events u ≼ v in the first neighborhood of each other (A uv = 1) is minimized when xu ≲ T xv is satisfied (equation 5).</p><p>In our mathematical formulation, the exact magnitude of the negative difference is not important. At first glance, a more straightforward way of imposing this condition is to use ReLU activation functions instead. However, these are discontinuous and lead to unstable training, which we verified experimentally. Hence, we want a continuous function that is easy to optimize and reaches zero quickly as the partial order is satisfied. Remember that we are optimizing the distance loss for the spatial component of the spacetime embedding and the time embedding using the causality loss simultaneously. If the causality loss does not reach zero quickly enough, we will be wasting computation trying to make the difference between partial embeddings more negative for no reason and, as a consequence, failing to further optimize the metric distortion of the embedding when we have already satisfied causality.</p><p>Finally, to ensure that the causality loss provides good gradients while being zero as soon as the partial order is satisfied we use the following expression, which we converged to empirically and that satisfies all our requirements:</p><formula xml:id="formula_95">L C uv def. = A uv T t=1 SteepSigmoid(T (x u ) t -T (x v ) t ) × (1 -TotalCorrect),<label>(40)</label></formula><p>where the second term is not differentiable but makes the loss zero when all the directed edges have been correctly embedded. To compute this we can indeed use the ReLU function:</p><formula xml:id="formula_96">TotalCorrect def. = u M u=u1 v M v=v1 A uv • I T t=1 ReLU(T (x u ) t -T (x v ) t ) = 0 |E D | ,</formula><p>where the expression in the enumerator counts how many times the function evaluates to zero for connected nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 HOW TO THEORETICALLY ENFORCE THE GLOBAL CAUSAL GEOMETRY</head><p>In Section 3.2, we discuss that for practical purposes, we only focus on encoding local geometry, which in turn optimizes the causal geometry of the neural spacetime implicitly for causally connected events due to transitivity. For anti-chains, the no causality condition will be satisfied with high probability, especially as the number of time dimensions increases. For completeness, here we provide a loss function to enforce the global causal geometry of DAGs if the causal connectivity in the input geometry was easily computable. Note that for large graphs, this becomes very computationally expensive since we need to verify that for anti-chains, there is no path between those two given nodes.</p><formula xml:id="formula_97">L C uv def. = A ′ uv T t=1 ReLU(T (x u ) t -T (x v ) t ) Check: Causality + B uv min t ReLU(ε + T (x v ) t -T (x u ) t ) Check: No Causality ,<label>(41) where B def.</label></formula><p>= (I (u ≼v∧v ≼u) ) uv with entries B uv for two events u and v, which can be deduced from A ′ , and where ε &gt; 0 is a margin. A ′ uv in this case is not the adjacency matrix, but a mask which is 1 for causally connected nodes or events. Note that here unlike in the main text we are not restricting causality to the first hop. Also, B is symmetric:</p><formula xml:id="formula_98">B uv = B vu .</formula><p>The loss above enforces causal connectivity and lack of causal connectivity. Both xu ≲ T xv and xv ≲ T xu must be satisfied by our representation for anti-chains. T &gt; 1 is required, otherwise it is not possible to avoid causality in at least one direction. In the simplest case, when T = 2 and t ∈ {t 1 , t 2 } the following must be satisfied if we associate xu with u and xv with v:</p><formula xml:id="formula_99">T (x v ) t1 &gt; T (x u ) t1 and T (x u ) t2 &gt; T (x v ) t2 , or T (x v ) t2 &gt; T (x u ) t2 and T (x u ) t1 &gt; T (x v ) t1 .</formula><p>To satisfy the no causality condition when T &gt; 2, as long as one of the time dimensions breaks ≲ T in equation 5 it suffices. ε would be included in this hypothetical optimization objective to avoid the collapse of the encoder network to a single embedding. Lastly, we would like to highlight that this causality loss should be optimized alongside a distance loss based on the graph geodesic distance instead of the local distance. Although this is a theoretical exercise, in practice it may be best to substitute ReLU with SteepSigmoid for optimization purposes as in Section C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 POTENTIAL CLASSICAL ALGORITHMS TO COMPUTE SPACE AND TIME DIMENSIONS</head><p>Theorem 1 shows that the number of time dimensions should be at least equal to the width of the poset being embedded by the NST. In theory, the width of a poset could be computed in polynomial time <ref type="bibr" target="#b27">(Felsner et al., 2003)</ref>. Furthermore, for posets with a width of less than 4, there are algorithms with a runtime of O(n log(n)) (sub-quadratic time) that can detect if the poset has a width of at most 4. Nevertheless this theoretical results become impractical for large graphs. Similarly, Theorem 1 shows that the required number of spatial dimensions needed to obtain a faithful lowdistortion metric embedding of the graph underlying a poset is determined by its doubling constant. It is known that an exact computation of this doubling dimension is generally NP-hard <ref type="bibr" target="#b36">Gottlieb &amp; Krauthgamer (2013)</ref>. Alternatives algorithms to obtain accurate upper bounds (up to an absolute constant factor) are also discussed in the literature <ref type="bibr" target="#b44">(Har-Peled &amp; Mendel, 2005)</ref>, but these are in general impractical for large DAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 WEIGHT INITIALIZATION AND HANDLING THE ABSENCE OF NORMALIZATION</head><p>Weight initialization is important to avoid initial network predictions from shrinking or exploding. Networks with fractalesque activations are particularly susceptible to this problem, as they use activations with exponential coefficients. This is not such a prevalent issue if we use the well-established ReLU activation or its variants <ref type="bibr">(ELU, LeakyReLU, etc.)</ref>, since this activations only zero out part of the axis and leave the rest of the transformation linear.</p><p>In the original neural snowflake model (equation 10), the matrices A, B, and C are initialized with weights sampled from a uniform distribution spanning from 0 to 1, which are then normalized according to the dimensions of each matrix. Specifically, for matrix A, the weights are drawn from a distribution ranging between 0 and 1/(d A1 d A2 ), where d A1 represents the number of rows and d A2 represents the number of columns of matrix A. We follow the same approach to initialize W in equation 3. While better initialization alternatives may be possible, it was noted previously in (Borde &amp; <ref type="bibr">Kratsios, 2023</ref>) that drawing the weights from a normal Gaussian or using Xavier initialization can lead to instabilities and exploding numbers in the forward pass. We also observe that this initialization technique is important to avoid the metric prediction becoming either too small or too large when making the network deeper since we cannot naively apply normalization layers inside our parametric representation of the metric. Additionally, we find that neural (quasi-)metrics are better than the original neural snowflakes in maintaining the same order of magnitude in its distance prediction as the number of layers is increased. C.7 NEURAL SNOWFLAKES VS NEURAL (QUASI-)METRICS Next, we discuss similarities and differences between the original neural snowflake and neural (quasi-)metrics. We first provide an algorithmic description of neural snowflakes, which can be used to compare against that presented in Appendix C.2 for the neural (quasi-)metric model.</p><formula xml:id="formula_100">Algorithm 4: Neural Snowflake Require: xu , xv ▷ Two Node Features Vectors return s ij ▷ Distance similarity measure u 0 ← ||x u -xv || 2 ▷ Euclidean Distance For j = 1 to J ûj-1 ← A j u j-1 ▷ Linear Projection Σ j ← ψû j-1 ▷ Snowflake Activation u j ← B j Σ j C j ▷ Linear Projections end s ij ← u 1+|p| J ▷ Quasi-metric</formula><p>Embedding Experiments. In terms of experiments, generally, we find that neural (quasi-)metrics have a smoother optimization landscape. They tend to converge faster for a low number of epochs, and in the case of snowflakes, we experimentally observed sudden drops in MSE loss, similar to those reported in Appendix I of <ref type="bibr" target="#b52">(Borde &amp; Kratsios, 2023)</ref>. As shown in Section D.1, the neural (quasi-)metric construction performs better at tree embedding. What we observed is that if trained for a sufficient duration, snowflakes will eventually find a good local minimum. On the other hand, our new model seems more reliable in terms of optimization and achieves better performance for lower epochs.</p><p>Optimizing exponents. In Equation <ref type="formula" target="#formula_29">11</ref>, a and b are in principle presented as learnable components via gradient descent. However, in practice, exponents can be unstable to optimize via backprop <ref type="bibr" target="#b52">(Borde &amp; Kratsios, 2023)</ref>. The original neural snowflake fixes a = b = 1, to avoid optimization issues. Nevertheless note that p, which controls the quasi-metric in Equation 10 is indeed optimized. Optimizing exponents closer to the final computations of the network is in general more stable. We hypothesize that this could be due to the fact that there are less chain-rule multiplications, reducing the likelihood of gradient explosion. We observe similar behaviour for neural (quasi-)metrics, note that our activation function relies on learning the exponents (equation 2). In general we find that we can learn the exponents for all layers in this configuration. But we also experimented with trying to approximate fractal metrics and observed that under some extreme cases it may be better to only optimize the exponents of the last layer for stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAILS</head><p>In this appendix we expand on the experimental procedure used to obtain the results presented in the main text in Section 4. Additionally, we present more validation experiments, including undirected tree embeddings, synthetic DAG embeddings with different metrics and varying levels of connectivity, and large DAG citation network embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 UNDIRECTED TREE EMBEDDING</head><p>Overview of Results for Preliminary Undirected Embeddings. Before proceeding to DAGs, following <ref type="bibr">Kratsios et al. (2023b)</ref>, we first test the spatial component of the NST model, on embedding trees and compare it to Euclidean embeddings implemented by an MLP and to hyperbolic embeddings via HNNs <ref type="bibr" target="#b31">Ganea et al. (2018)</ref>. We use the same training procedure as in <ref type="bibr">Kratsios et al. (2023b)</ref> and perform experiments for both binary and ternary trees. In the case of the snowflake geometry, inputs are mapped using an MLP, and the metric induced by the tree structure is learned by a neural (quasi-)metric with 4 layers operating on the embedding space dimensionality. We measure distortion as the ratio between true and predicted distances. Consistent with the theoretical results in <ref type="bibr" target="#b40">(Gupta, 2000)</ref>, we find that in general, the difference in average distortion between metric spaces is not as pronounced as the maximum distortion since most metric spaces are "treelike" on average. Indeed, we find that our proposed model better restricts the maximum distortion and that it is able to achieve an average distortion of 1.00 even with an embedding space of dimension 2 only. For completeness we also test the neural snowflake model in the same task, which although it is able to minimize the MSE loss, the (max) distortion is very high. Additional discussion comparing neural snowflakes and neural (quasi-)metrics can be found in Appendix C.7. Experimental Procedure for Undirected Tree Embeddings. Neural spacetimes provide guarantees in terms of global and local embeddings. In general, from a computational perspective, local embedding is more tractable. However, in these particular preliminary experiments for metric embedding, similar to <ref type="bibr">(Kratsios et al., 2023b)</ref>, we embed the full undirected tree geometry. The distance between any two nodes u, v ∈ V simplifies to the usual shortest path distance on an unweighted graph</p><formula xml:id="formula_101">d T (u, v) = inf i : ∃ {v, v 1 }, . . . {v i-1 , u} ∈ E . (<label>42</label></formula><formula xml:id="formula_102">)</formula><p>Using the expression above, we compute the tree induced graph geodesic between nodes. We work with both binary and ternary trees with all edge weights being equal to 1. To find all-pairs shortest path lengths we use Floyd's algorithm <ref type="bibr" target="#b29">(Floyd, 1962)</ref>. Note that Floyd's algorithm has a running time complexity of O(|V | 3 ) and running space of O(|V | 2 ). This makes it not scalable for large graphs.</p><p>The networks receive the x u and y u coordinates of the graph nodes in R 2 and are tasked with mapping them to a new embedding space which must preserve the distance. An algorithm is employed to generate input coordinates, mimicking a force-directed layout of the tree. In this simulation, edges act as springs, drawing nodes closer, while nodes behave as objects with repelling forces, similar to anti-gravity effects. This process continues iteratively until the positions reach equilibrium. The algorithm can be replicated using the NetworkX library and the spring layout for the graph.</p><p>The networks need to find an appropriate way to gauge the distance. We adjust the network parameters by computing the Mean Squared Error (MSE) loss, which compares the actual distance between nodes given by the tree topology, d true , to the distance predicted by the network mappings, d pred :</p><p>Loss = MSE(d true , d pred ).</p><p>For the Multi-Layer Perceptron (MLP) baseline, the predicted distance is calculated as:</p><formula xml:id="formula_103">d pred = ∥MLP(x u , y u ) -MLP(x v , y v )∥ 2 ,</formula><p>where (x u , y u ) and (x v , y v ) represent the coordinates in R 2 of a synthetically generated tree. On the other hand, for the HNN, we utilize the hyperbolic distance between embeddings with fixed curvature of -1:</p><formula xml:id="formula_104">d pred = d -1 (HNN(x u , y u ), HNN(x v , y v )).</formula><p>In the case of HNNs, we employ the hyperboloid model, incorporating an exponential map at the pole to map the representations to hyperbolic space, as detailed in <ref type="bibr">Borde et al. (2023b)</ref>. Notably, we do not even need any hyperbolic biases to discern the performance disparity between MLP and HNN models <ref type="bibr">(Kratsios et al., 2023b)</ref>.</p><p>Note that in the previous two baselines, the geometry of the space is effectively fixed, and we only optimize the location of the node embeddings in the manifold. In the case of neural snowflakes, the metric is also parametric and optimized via gradient descent. This means that we change both the location of events in the manifold and the geometry of the manifold itself during optimization. Following (Borde &amp; Kratsios, 2023), the neural (quasi-)metric model also leverages the encoder E which is implemented as an MLP (same as in the Euclidean baseline). The predicted distance is</p><formula xml:id="formula_105">d pred = d N QM (MLP(x u , y u ), MLP(x v , y v )),</formula><p>where d N QM is parametrized by a neural (quasi-)metric network. Importantly, feature vectors are passed independently to the metric learning network (unlike in (Borde &amp; <ref type="bibr">Kratsios, 2023)</ref> where an intermediate Euclidean distance representation is fed instead).</p><p>In terms of hyperparameters, we generate trees with 1000 nodes and embed them into features of dimensionality 2 and 4 in different experiments. We employ a batch size of 10,000 to learn the distances, train for 10 epochs with a learning rate of 3 × 10 -3 and AdamW optimizer, and apply a max gradient norm of 1. All encoders have a total of 10 hidden layers with 100 neurons and a final projection layer to the embedding dimension. The neural (quasi-)metric has a total of 4 layers, with a hidden layer dimension equal to the event embedding dimensions, that is, either 2 or 4, and the last layer projects the representation to a scalar, i.e., the predicted distance. We use the same configuration for the snowflakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 METRIC DAG EMBEDDING</head><p>In this appendix, we provide additional details on the synthetic weighted DAG embedding experiments described in Section 4, focusing on local embedding. Similar to the previous setup, the networks receive the x u and y u coordinates of the graph nodes in R 2 and are tasked with mapping these coordinates to a new embedding space that preserves both the distance and directionality of the generated DAG. We train the embedding following Section 3.2. As discussed in the main text, we optimize the spacetime embedding by running gradient descent based on both a distance and a causality loss. The procedure used to train the neural (quasi-)metric in terms of the spatial embedding is identical to that described in Appendix D.1.</p><p>We use the Graphviz dot layout algorithm to generate the input coordinates, positioning the nodes from top to bottom according to the directionality of the DAG. This process can be replicated using the NetworkX library. Additionally, the coordinates are normalized to lie between 0 and 1. The DAG is constructed from a random graph where each node has a 0.9 probability of being connected to other nodes, defining the directionality and connectivity of the DAG (see Figure <ref type="figure">7</ref>). The metric distance is then calculated based on the spatial coordinates of the nodes after they have been embedded in the plane. The specific distance functions used in the experiments are listed in Table <ref type="table" target="#tab_1">1</ref> and are based on <ref type="bibr" target="#b52">(Borde &amp; Kratsios, 2023)</ref>.</p><p>In terms of hyperparameters, we generate DAGs with 50 nodes and embed them into spacetimes of D = T = 2, 4, 10. The 50 nodes induce 50 2 possible distances. We do not apply mini-batching; we optimize all of them at the same time. We train for 5,000 epochs with a learning rate of 10 -4 using the AdamW optimizer, and apply a max gradient norm of 1. The initial encoders have a total of 10 hidden layers with 100 neurons each and a final projection layer to the embedding dimension D + T . The neural (quasi-)metric and the neural partial order have a total of 4 layers, each with a hidden layer dimension of 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 VARYING GRAPH CONNECTIVITY</head><p>We conduct additional experiments to analyze the effect of graph topology on the embedding results. Specifically, we fix the synthetic metric to ∥x-y∥ 0.5 log(1+∥x-y∥) 0.5 and experiment with graphs having varying connection probabilities, which control the sparsity of the random DAGs, i.e., their connectivity. Across all geometries, we observe that as the DAG becomes more sparsified, its directionality is captured less accurately. However, it also becomes easier to achieve lower distortion, as expected, due to the reduction in the number of edges to encode, see Table <ref type="table" target="#tab_5">4</ref>. WebKB datasets. In these datasets <ref type="bibr">(Rozemberczki et al., 2021)</ref>, nodes represent web pages, while edges denote hyperlinks between them. Each node's features are given by the bag-of-words representation of its web page. We summarize the datasets in Table <ref type="table" target="#tab_6">5</ref>.</p><p>As discussed in the main text in Section 4, these datasets are not pure DAGs. As a preprocessing step, we remove self-loops. However, we do not remove cycles, nor do we further process edge directionality. This explains why in Table <ref type="table" target="#tab_1">1</ref>, it is not possible to get a perfect embedding in time.</p><p>The local metric between nodes is computed as the cosine similarity between the node feature embeddings (since the datasets do not have edge weights). Note that here we do not embed graphs in the plane; we already have bag-of-words features that can be passed to the encoder, unlike in the Dream5 datasets. The Dream5 datasets <ref type="bibr">(Marbach et al., 2012)</ref> were originally introduced as part of the Network Inference Challenge, where participants were provided with four microarray compendia and tasked with deducing the structure of the underlying transcriptional (gene) regulatory networks. Each compendium encompasses numerous microarray experiments, spanning various genetic, drug, and environmental alterations (or simulations thereof, in the case of the in-silico network). In this work, instead of focusing on network structure prediction, we test our neural spacetime algorithm in the context of graph embedding, following the spacetime representation literature <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023;</ref><ref type="bibr" target="#b82">Sim et al., 2021)</ref>. We embed the graph topology in R 2 using Graphviz dot layout and compute the strength between the connections based on the cosine similarity, since the original networks do not have weights. In Table <ref type="table" target="#tab_7">6</ref> we record the number of nodes and edges for each dataset. Note that these are not pure DAGs either (but it is possible to embed over 99% of the directed edges correctly, see Table <ref type="table" target="#tab_10">10</ref>). For training, we follow Appendix D.2 with learning rate 10 -4 .</p><p>Ogbn-arxiv Large Citation Graph Embedding. Given that the previous graphs are relatively small, we conduct an additional experiment on the ogbn-arxiv dataset <ref type="bibr" target="#b47">(Hu et al., 2021)</ref>, which is orders of magnitude larger (see Table <ref type="table" target="#tab_8">7</ref>). We use a similar procedure as before: we remove selfloops to avoid returning similarity scores of 1.0 and also remove edges between nodes whose feature vectors have a cosine similarity score greater than 0.99. In this particular dataset, we found that some nodes had almost identical features, which, due to numerical truncation when computing the distance between nodes, resulted in a distance of zero and led to numerical instabilities. This resulted in 269 rejected edges out of 1,166,243, or 0.02%. We train all models for 40 epochs, using the AdamW optimizer with a learning rate of 10 -4 and a batch size of 10,000. We report the results in Table <ref type="table" target="#tab_9">8</ref> below. Once again, we observe that, under the same training conditions, NSTs are superior at embedding compared to their fixed geometry counterparts. However, we observe a degradation in performance compared to previous experiments with smaller graphs. Additionally, note that we do not modify the parameter count of the neural networks.  </p><formula xml:id="formula_106">z v = x v ∥ (x v ) 1:D ∥ t v , where xv = E(x v ), t v = T (x v ),<label>(43)</label></formula><p>where we have augmented the original features with NST-based features optimized according to the graph topology. In the equation above x v corresponds to the original features for node v, E and T are the frozen feature encoder and partial order, which were originally optimized alongside D. As in the main text, the subscript in (x v ) 1:D means that we utilize the first D spatial dimensions from the feature vector, and we use || for concatenation. We use pre-trained checkpoints with D = T = 10.</p><p>We display the downstream results in Table <ref type="table" target="#tab_11">11</ref> below. We experiment with different downstream classifiers, including MLPs, GCNs <ref type="bibr" target="#b50">(Kipf &amp; Welling, 2017)</ref>, GATs <ref type="bibr" target="#b87">(Veličković et al., 2018)</ref>, CPGNN-MLPs <ref type="bibr" target="#b91">(Zhu et al., 2021)</ref>, and CPGNN-Cheby <ref type="bibr" target="#b91">(Zhu et al., 2021;</ref><ref type="bibr" target="#b18">Defferrard et al., 2016)</ref>. Note that CPGNN is not just a network but also a framework for training on heterophilic graphs.</p><p>It employs an estimator network, such as an MLP or a Chebyshev polynomial-based model, for prediction. The CPGNN method incorporates estimator pretraining and utilizes a compatibility matrix, which is initialized using the training node labels and the training mask. This matrix is further refined using the Sinkhorn-Knopp algorithm for initialization. Furthermore, the compatibility matrix is regularized during training through an additional loss term on top of the cross-entropy loss.</p><p>We reimplement all baselines and test them on the Cornell, Texas, and Wisconsin datasets using 10-fold splits, based on the masks provided in PyTorch Geometric. Following <ref type="bibr" target="#b91">(Zhu et al., 2021)</ref> we use weight decay of 5 × 10 -4 for all networks and pretrain the estimators in CPGNN-MLP and CPGNN-Cheby for 100 steps. All our downstream classifiers use two layers with a ReLU activation function and are trained with a learning rate of 0.01 for 400 steps using AdamW. We experiment with varying hidden dimensions-10, 20, 30, and 64-the last of which is the hidden dimension used by the baselines in <ref type="bibr" target="#b91">(Zhu et al., 2021)</ref>. In our experiments, we study the effect of adding geometric node features based on the NST encoding to downstream classifiers. We find that the MLP classifier achieves the best downstream performance when augmented with NST features, particularly for the Texas and Wisconsin datasets. For Cornell, the best downstream performance is achieved by the MLP without NST features, which attains 71% accuracy with a hidden dimension of 64. The counterpart augmented with NST features achieves 70% accuracy, making the difference not particularly significant. Overall, in Cornell, we observe that adding NST features is especially beneficial for smaller classifiers with fewer hidden dimensions.</p><p>These experiments confirm that the NST has learned meaningful features that can be used by downstream classifiers. However, we invite future research in this direction, as this paper primarily focuses on embeddings, and this appendix is only an initial exploration of the applicability of NSTs to downstream tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The red arrows illustrate the Hasse diagram (DAG) with directed edge set {(A, B), (B, C), (B, D), (D, E)}.Its corresponding poset is depicted using green arrows on the same vertex set {A, B, C, D, E}. Our partial order is not a total order as there is no red or green arrow between C and E. Red arrows encode key DAG structure and green arrows encode all their possible compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Temporal Embedding: the goal of the causal embedding is to match the directions in the embedded graph (direction of red arrows) to the order in R 2 with the product order (direction of gray arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Spacetime Embeddings (Definition 2): We illustrate a spacetime embedding of the directed graph in Figure 2 into R 4 = R 2 × R 2 with 2-space dimensions and 2 time dimensions. Notice that the spatial component of the spacetime embedding is not a causal embedding and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The latter inequality is acceptable and not required to faithfully encode the graph edge weights.Training and Loss Function. Let us use the matrix X ∈ R M ×N to denote the collection of feature vectors for M = |V| nodes associated with the DAG, G D . The NST minimizes the following loss:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Definition 8 (Invertible Positive Matrices I + D ). Let D ∈ N + . A D × D matrix W is invertible and positive if it can be represented as W = λ I D + | W|, where λ &gt; 0, W is an arbitrary D × D matrix, and | • | denotes entrywise absolute value applied to the matrix W. The set of all such D × D invertible positive matrices W is denoted by I + D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, and (W) i,k ≥ 0 for each i = 1 . . . , D and k = 1, . . . , d then: for i = 1, . . . , D we have that(Wu) i = D j=1 W i,j u j ≥ 0. Thus, D(x, y) ≥ 0 for each x, y ∈ R D .Case I : D Separates Points if T = 0: Suppose that T = 0; otherwise we will show that D does not separate points.It is sufficient to show the result for J = 1, with the general case following directly by recursion. Consider the map D : R D+T × R D+T → [0, ∞) with iterative representation given in equation 3; i.e. for each x, y ∈ R D+T ∼ = R D D(x, y) def.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(x) D+1:D+T ≲ T (y) D+1:D+T ⇔ T (x) t ≤ T (y) t for t = D + 1, . . . , D + T (ii) Implementation of Snowflake of the ℓ p metric: D(x, y) = ∥(x) 1:D -(y) 1:D ∥ α p Furthermore, the parametric complexity of T and D are given by: (a) Depth: Depth(T ) = 1 and Depth(D) = 2 (b) Width: Width(T ) = T and Width(D) = D (c) No. Non-zero Parameters: No. Param(T ) = T + 2 and No. Par(D) = 2(3 + D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( i )</head><label>i</label><figDesc>Geometric Complexity: Together, D and T are defined by a total of D + T + 8 non-zero parameters, (ii) Encoding Complexity: E depends on O k 5/2 D 4 N log(N ) log k 2 diam(P, d) sep(P, d) non-zero parameters. Consequentially, together, D, T , and E depend at-most on a total of O D + T + k 5/2 D 4 N log(N ) log k 2 diam(P, d) sep(P, d) non-zero parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Koch snowflake with increasing number of refinement iterations from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Activation function used by neural partial order (equation 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: NST activation visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between (standard) Sigmoid and SteepSigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>DAG embedding results. Embedding dimension D = T = 2, 4, 10.</figDesc><table><row><cell>Metric</cell><cell>Embedding Dim</cell><cell>Distortion (average ± std)</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>1.09 ± 0.24</cell><cell>3.18</cell><cell>1.0</cell><cell></cell></row><row><cell>∥x -y∥ 0.5 log(1 + ∥x -y∥) 0.5 ∥x -y∥ 0.1 log(1 + ∥x -y∥) 0.9 1 -exp -(∥x-y∥-1) log(∥x-y∥)</cell><cell>4 10 2 4 10 2 4 10</cell><cell>1.02 ± 0.06 1.00 ± 0.03 1.16 ± 0.45 1.02 ± 0.07 1.00 ± 0.04 1.51 ± 1.18 1.11 ± 0.41 1.01 ± 0.05</cell><cell>1.51 1.24 6.21 1.75 1.47 13.55 8.92 1.31</cell><cell>1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0</cell><cell>Neural Spacetime</cell></row><row><cell></cell><cell>2</cell><cell>2.86 ± 5.22</cell><cell>72.66</cell><cell>0.99</cell><cell></cell></row><row><cell>∥x -y∥ 0.5 log(1 + ∥x -y∥) 0.5</cell><cell>4</cell><cell>1.70 ± 2.77</cell><cell>71.09</cell><cell>0.99</cell><cell></cell></row><row><cell>∥x -y∥ 0.1 log(1 + ∥x -y∥) 0.9 1 -exp -(∥x-y∥-1) log(∥x-y∥)</cell><cell>10 2 4 10 2 4 10</cell><cell>1.21 ± 1.33 6.77 ± 133.68 1.70 ± 5.21 1.19 ± 1.09 11.37 ± 114.98 2.49 ± 8.72 1.18 ± 2.49</cell><cell>35.58 1669.83 77.03 25.18 1876.54 198.04 82.67</cell><cell>0.99 0.99 0.99 0.99 0.98 0.98 0.99</cell><cell>Minkowski</cell></row><row><cell></cell><cell>2</cell><cell>∞ ±</cell><cell>∞</cell><cell>0.99</cell><cell></cell></row><row><cell>∥x -y∥ 0.5 log(1 + ∥x -y∥) 0.5</cell><cell>4</cell><cell>-4.33 ± 816.47</cell><cell>10235.71</cell><cell>0.99</cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>288.17 ± 9794.97</cell><cell>324027.5</cell><cell>0.99</cell><cell></cell></row><row><cell>∥x -y∥ 0.1 log(1 + ∥x -y∥) 0.9 1 -exp -(∥x-y∥-1) log(∥x-y∥)</cell><cell>2 4 10 2 4 10</cell><cell>9.40 ± 2226.84 174.69 ± 3637.32 -10.66 ± 739.71 -183.71 ± 9600.71 83.04 ± 4313.82 418.26 ± 6599.80</cell><cell>63968.21 115851.88 8997.62 39648.66 97524.21 150543.73</cell><cell>0.99 0.99 0.99 0.94 0.94 0.94</cell><cell>De Sitter</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Embedding results for real-world web page hyperlink and gene regulatory networks.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Neural Spacetime</cell><cell></cell><cell cols="2">Minkowski</cell><cell></cell><cell cols="2">De Sitter</cell><cell></cell></row><row><cell>Dataset</cell><cell>Embed. Dim</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell></row><row><cell></cell><cell>2</cell><cell>1.00 ± 0.07</cell><cell>1.31</cell><cell>0.93</cell><cell>1.07 ± 0.70</cell><cell>9.43</cell><cell>0.94</cell><cell>-55.83 ± 890.45</cell><cell>3950.88</cell><cell>0.92</cell></row><row><cell>Cornell</cell><cell>4</cell><cell>1.00 ± 0.04</cell><cell>1.08</cell><cell>0.94</cell><cell>1.00 ± 0.00</cell><cell>1.01</cell><cell>0.94</cell><cell>-20.60 ± 249.49</cell><cell>403.46</cell><cell>0.94</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.04</cell><cell>1.08</cell><cell>0.94</cell><cell>1.00 ± 0.00</cell><cell>1.00</cell><cell>0.94</cell><cell>0.80 ± 126.26</cell><cell>1543.07</cell><cell>0.93</cell></row><row><cell></cell><cell>2</cell><cell>1.01 ± 0.10</cell><cell>2.27</cell><cell>0.89</cell><cell>1.12 ± 1.73</cell><cell>31.27</cell><cell>0.90</cell><cell>-0.29 ± 84.42</cell><cell>818.10</cell><cell>0.90</cell></row><row><cell>Texas</cell><cell>4</cell><cell>1.00 ± 0.01</cell><cell>1.05</cell><cell>0.90</cell><cell>1.00 ± 0.00</cell><cell>1.00</cell><cell>0.90</cell><cell>42.03 ± 795.51</cell><cell>13939.25</cell><cell>0.90</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.00</cell><cell>1.00</cell><cell>0.90</cell><cell>1.01 ± 0.01</cell><cell>1.05</cell><cell>0.90</cell><cell>2.60 ± 70.33</cell><cell>1107.60</cell><cell>0.90</cell></row><row><cell></cell><cell>2</cell><cell>1.00 ± 0.10</cell><cell>1.67</cell><cell>0.89</cell><cell>5.07 ± 65.99</cell><cell>1410.03</cell><cell>0.90</cell><cell>2.06 ± 63.46</cell><cell>1291.31</cell><cell>0.89</cell></row><row><cell>Wisconsin</cell><cell>4</cell><cell>1.00 ± 0.04</cell><cell>1.16</cell><cell>0.89</cell><cell>1.00 ± 0.04</cell><cell>1.19</cell><cell>0.90</cell><cell>-0.78 ± 27.91</cell><cell>114.24</cell><cell>0.90</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.04</cell><cell>1.20</cell><cell>0.89</cell><cell>1.13 ± 0.69</cell><cell>16.28</cell><cell>0.90</cell><cell>0.04 ± 215.94</cell><cell>2862.19</cell><cell>0.89</cell></row><row><cell></cell><cell>2</cell><cell>1.06 ± 0.47</cell><cell>18.54</cell><cell>1.00</cell><cell>105.42 ± 4671.85</cell><cell>209248.72</cell><cell>0.94</cell><cell>-63.59 ± 1866.69</cell><cell>56626.97</cell><cell>0.92</cell></row><row><cell>In silico</cell><cell>4</cell><cell>1.00 ± 0.09</cell><cell>1.73</cell><cell>1.00</cell><cell>0.25 ± 54.57</cell><cell>1315.76</cell><cell>0.95</cell><cell>-468.81 ± 33021.14</cell><cell>65289.22</cell><cell>0.92</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.05</cell><cell>1.32</cell><cell>1.00</cell><cell>1.00 ± 0.05</cell><cell>3.69</cell><cell>0.99</cell><cell>-129.13 ± 9623.30</cell><cell>261531.59</cell><cell>0.93</cell></row><row><cell></cell><cell>2</cell><cell>1.02 ± 0.45</cell><cell>15.37</cell><cell>1.00</cell><cell>-4.25 ± 149.61</cell><cell>438.34</cell><cell>0.97</cell><cell>34.65 ± 2637.50</cell><cell>119047.23</cell><cell>0.91</cell></row><row><cell>E. coli</cell><cell>4</cell><cell>1.00 ± 0.06</cell><cell>2.62</cell><cell>1.00</cell><cell>1.00 ± 0.01</cell><cell>1.08</cell><cell>0.98</cell><cell>-2.00 ± 3294.81</cell><cell>130509.59</cell><cell>0.91</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.05</cell><cell>1.17</cell><cell>1.00</cell><cell>1.00 ± 0.01</cell><cell>1.01</cell><cell>0.99</cell><cell>-8.26 ± 94.57</cell><cell>652.96</cell><cell>0.91</cell></row><row><cell></cell><cell>2</cell><cell>1.05 ± 0.34</cell><cell>10.18</cell><cell>1.00</cell><cell>-2.38 ± 173.57</cell><cell>151.43</cell><cell>0.91</cell><cell>55.36 ± 3960.09</cell><cell>160278.39</cell><cell>0.90</cell></row><row><cell>S. cerevisiae</cell><cell>4</cell><cell>1.00 ± 0.07</cell><cell>1.63</cell><cell>1.00</cell><cell>1.04 ± 2.25</cell><cell>63.17</cell><cell>0.98</cell><cell>-28.60 ± 1175.67</cell><cell>63086.54</cell><cell>0.90</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.05</cell><cell>1.57</cell><cell>1.00</cell><cell>1.01 ± 0.02</cell><cell>1.39</cell><cell>0.99</cell><cell>-121.17 ± 7550.16</cell><cell>84724.25</cell><cell>0.91</cell></row><row><cell>in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Tree Embedding distortion leveraging Euclidean, Hyperbolic, Neural Snowflake and Neural (Quasi-)metric spaces.</figDesc><table><row><cell>Tree Type Embedding dim</cell><cell>Geometry</cell><cell cols="4">Avg Distortion St dev Distortion Max Distortion MSE</cell></row><row><cell></cell><cell>Euclidean</cell><cell>1.66</cell><cell>3.53</cell><cell>1224.17</cell><cell>26.27</cell></row><row><cell>2</cell><cell>Hyperbolic Neural (Quasi-)Metric</cell><cell>1.04 1.00</cell><cell>1.61 0.23</cell><cell>402.52 3.09</cell><cell>12.76 10.09</cell></row><row><cell>Binary</cell><cell>Snowflake Euclidean</cell><cell>1.01 1.15</cell><cell>3.01 0.68</cell><cell>2261.35 159.74</cell><cell>9.47 10.19</cell></row><row><cell>4</cell><cell>Hyperbolic Neural (Quasi-)Metric</cell><cell>1.00 1.00</cell><cell>0.17 0.19</cell><cell>11.03 3.87</cell><cell>4.14 4.88</cell></row><row><cell></cell><cell>Snowflake</cell><cell>1.00</cell><cell>0.65</cell><cell>539.71</cell><cell>5.92</cell></row><row><cell></cell><cell>Euclidean</cell><cell>1.69</cell><cell>3.17</cell><cell>602.96</cell><cell>11.55</cell></row><row><cell>2</cell><cell>Hyperbolic Neural (Quasi-)Metric</cell><cell>1.09 1.00</cell><cell>1.23 0.20</cell><cell>135.81 6.33</cell><cell>5.55 3.34</cell></row><row><cell>Ternary</cell><cell>Snowflake Euclidean</cell><cell>1.01 1.17</cell><cell>4.78 0.82</cell><cell>4017.99 185.73</cell><cell>3.64 4.82</cell></row><row><cell>4</cell><cell>Hyperbolic Neural (Quasi-)Metric</cell><cell>1.00 1.00</cell><cell>0.15 0.16</cell><cell>16.56 4.19</cell><cell>1.37 1.72</cell></row><row><cell></cell><cell>Snowflake</cell><cell>1.00</cell><cell>0.47</cell><cell>237.88</cell><cell>2.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure7: Example DAG with 10 nodes and using the first metric in Table1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.49</cell><cell></cell></row><row><cell>0. 29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.35</cell><cell>0.63</cell><cell>0. 60</cell><cell>0.5 3</cell><cell></cell></row><row><cell>0.2 1</cell><cell>0.65</cell><cell>0.61</cell><cell cols="2">0. 67</cell><cell>0. 91</cell><cell>0.35 0.45</cell></row><row><cell>0.84 0.4 7</cell><cell>0.74</cell><cell>0.6 5</cell><cell>0.44 0.4 8</cell><cell>0. 71 0. 75</cell><cell></cell><cell>0.6 0</cell></row><row><cell cols="2">0.5 9 0.35</cell><cell cols="2">0. 89</cell><cell></cell><cell cols="2">0.12</cell></row><row><cell>0.65</cell><cell>0.46</cell><cell></cell><cell>0.4 8</cell><cell></cell><cell></cell><cell>0.3 9</cell></row><row><cell>0.56</cell><cell></cell><cell></cell><cell>0.45</cell><cell>0. 49</cell><cell></cell><cell>0.34</cell></row><row><cell></cell><cell></cell><cell>0. 65</cell><cell></cell><cell>0.4 2</cell><cell></cell></row><row><cell></cell><cell>0. 15</cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell></row><row><cell>0.2 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Embedding results for arxiv citation network.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Neural Spacetime</cell><cell></cell><cell cols="2">Minkowski</cell><cell></cell><cell cols="2">De Sitter</cell><cell></cell></row><row><cell>Connectivity</cell><cell>Embed. Dim</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell></row><row><cell></cell><cell>2</cell><cell>1.09 ± 0.24</cell><cell>3.18</cell><cell>1.00</cell><cell>2.86 ± 5.22</cell><cell>72.66</cell><cell>0.99</cell><cell>∞ ±</cell><cell>∞</cell><cell>0.99</cell></row><row><cell>90%</cell><cell>4</cell><cell>1.02 ± 0.06</cell><cell>1.51</cell><cell>1.00</cell><cell>1.70 ± 2.77</cell><cell>71.09</cell><cell>0.99</cell><cell>-4.33 ± 816.47</cell><cell>10,235.71</cell><cell>0.99</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.03</cell><cell>1.24</cell><cell>1.00</cell><cell>1.21 ± 1.33</cell><cell>35.58</cell><cell>0.99</cell><cell>288.17 ± 9,794.97</cell><cell>324,027.5</cell><cell>0.99</cell></row><row><cell></cell><cell>2</cell><cell>1.14 ± 0.42</cell><cell>3.94</cell><cell>1.00</cell><cell>3.35 ± 10.44</cell><cell>215.65</cell><cell>1.00</cell><cell>-238.02 ± 5,432.33</cell><cell>13,879.30</cell><cell>0.99</cell></row><row><cell>50%</cell><cell>4</cell><cell>1.01 ± 0.07</cell><cell>1.38</cell><cell>1.00</cell><cell>1.27 ± 0.82</cell><cell>12.45</cell><cell>1.00</cell><cell>-12.61 ± 2,253.31</cell><cell>43,124.32</cell><cell>0.99</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.18</cell><cell>1.06</cell><cell>1.00</cell><cell>1.02 ± 0.19</cell><cell>4.03</cell><cell>1.00</cell><cell>-53.30 ± 1,600.69</cell><cell>18,756.20</cell><cell>0.99</cell></row><row><cell></cell><cell>2</cell><cell>1.00 ± 0.30</cell><cell>2.65</cell><cell>0.98</cell><cell>1.01 ± 2.41</cell><cell>15.96</cell><cell>0.93</cell><cell>-1.18 ± 19.43</cell><cell>69.29</cell><cell>0.92</cell></row><row><cell>10%</cell><cell>4</cell><cell>1.00 ± 0.01</cell><cell>1.01</cell><cell>1.00</cell><cell>1.00 ± 0.02</cell><cell>1.03</cell><cell>0.96</cell><cell>1.09 ± 14.32</cell><cell>130.62</cell><cell>0.91</cell></row><row><cell></cell><cell>10</cell><cell>1.00 ± 0.00</cell><cell>1.00</cell><cell>0.98</cell><cell>1.00 ± 0.02</cell><cell>1.08</cell><cell>0.96</cell><cell>2.39 ± 20.30</cell><cell>176.60</cell><cell>0.93</cell></row><row><cell cols="6">D.3 REAL-WORLD NETWORK EMBEDDING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of WebKB datasets</figDesc><table><row><cell>Name</cell><cell cols="2">#nodes #edges</cell></row><row><cell>Cornell</cell><cell>183</cell><cell>298</cell></row><row><cell>Texas</cell><cell>183</cell><cell>325</cell></row><row><cell>Wisconsin</cell><cell>251</cell><cell>515</cell></row><row><cell cols="3">synthetic DAG experiments. In terms of training, the procedure is the same as in Appendix D.2 and</cell></row><row><cell>using a learning rate of 10 -4 .</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Statistics of Dream5 datasets</figDesc><table><row><cell>Name</cell><cell cols="2">#nodes #edges</cell></row><row><cell>In silico</cell><cell>1,565</cell><cell>4,012</cell></row><row><cell>Escherichia coli</cell><cell>1,081</cell><cell>2,066</cell></row><row><cell>Saccharomyces cerevisiae</cell><cell>1,994</cell><cell>3,940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistics of Ogbn-arxiv dataset</figDesc><table><row><cell>Name #nodes</cell><cell>#edges</cell></row><row><cell cols="2">Arxiv 169,343 1,166,243</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Embedding results for arxiv citation network.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Neural Spacetime</cell><cell></cell><cell cols="2">Minkowski</cell><cell></cell><cell cols="2">De Sitter</cell><cell></cell></row><row><cell>Dataset</cell><cell>Embed. Dim</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell><cell>Distortion</cell><cell>Max Distortion</cell><cell>Directionality</cell></row><row><cell>Arxiv</cell><cell>2</cell><cell>1.88 ± 4.37</cell><cell>1,031.11</cell><cell>0.80</cell><cell>4.64 ± 7,322.96</cell><cell>1,811,858.75</cell><cell>0.83</cell><cell>-133.12 ± 16,142.16</cell><cell>1,845,115.5</cell><cell>0.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Embedding results for real-world web page hyperlink graph datasets and gene regulatory networks.</figDesc><table><row><cell>Neural Spacetime</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Node classification results for heterophilic graphs, including the test accuracy mean and standard deviation. ✓ denotes that the input features to the downstream classifier consist of the original graph input features plus the NST featuresz v = x v ∥ (x v ) 1:D ∥ t v .In contrast, ✗ indicates that only x v is passed into the model.</figDesc><table><row><cell>Dataset</cell><cell>Hidden Dim</cell><cell>NST Features</cell><cell>MLP</cell><cell>GCN</cell><cell>GAT</cell><cell>CPGNN-MLP</cell><cell>CPGNN-ChebNet</cell></row><row><cell></cell><cell>10</cell><cell>✓</cell><cell>0.55 ± 0.09</cell><cell>0.43 ± 0.09</cell><cell>0.48 ± 0.08</cell><cell>0.54 ± 0.05</cell><cell>0.51 ± 0.05</cell></row><row><cell></cell><cell>10</cell><cell>✗</cell><cell>0.40 ± 0.12</cell><cell>0.43 ± 0.06</cell><cell>0.48 ± 0.07</cell><cell>0.47 ± 0.10</cell><cell>0.46 ± 0.11</cell></row><row><cell>Cornell</cell><cell>20 20</cell><cell>✓ ✗</cell><cell>0.61 ± 0.09 0.56 ± 0.11</cell><cell>0.44 ± 0.09 0.43 ± 0.06</cell><cell>0.49 ± 0.08 0.49 ± 0.09</cell><cell>0.51 ± 0.05 0.43 ± 0.11</cell><cell>0.50 ± 0.10 0.41 ± 0.07</cell></row><row><cell></cell><cell>30</cell><cell>✓</cell><cell>0.70 ± 0.06</cell><cell>0.44 ± 0.09</cell><cell>0.47 ± 0.09</cell><cell>0.47 ± 0.12</cell><cell>0.52 ± 0.07</cell></row><row><cell></cell><cell>30</cell><cell>✗</cell><cell>0.70 ± 0.05</cell><cell>0.43 ± 0.07</cell><cell>0.50 ± 0.10</cell><cell>0.42 ± 0.11</cell><cell>0.41 ± 0.11</cell></row><row><cell></cell><cell>64</cell><cell>✓</cell><cell>0.70 ± 0.07</cell><cell>0.43 ± 0.09</cell><cell>0.50 ± 0.06</cell><cell>0.50 ± 0.08</cell><cell>0.52 ± 0.07</cell></row><row><cell></cell><cell>64</cell><cell>✗</cell><cell>0.71 ± 0.07</cell><cell>0.43 ± 0.07</cell><cell>0.50 ± 0.08</cell><cell>0.40 ± 0.10</cell><cell>0.40 ± 0.11</cell></row><row><cell></cell><cell>10</cell><cell>✓</cell><cell>0.58 ± 0.14</cell><cell>0.58 ± 0.08</cell><cell>0.56 ± 0.09</cell><cell>0.61 ± 0.05</cell><cell>0.62 ± 0.08</cell></row><row><cell></cell><cell>10</cell><cell>✗</cell><cell>0.41 ± 0.22</cell><cell>0.49 ± 0.08</cell><cell>0.50 ± 0.07</cell><cell>0.63 ± 0.10</cell><cell>0.62 ± 0.10</cell></row><row><cell>Texas</cell><cell>20 20</cell><cell>✓ ✗</cell><cell>0.64 ± 0.07 0.57 ± 0.10</cell><cell>0.56 ± 0.07 0.49 ± 0.07</cell><cell>0.54 ± 0.08 0.50 ± 0.10</cell><cell>0.61 ± 0.06 0.61 ± 0.10</cell><cell>0.63 ± 0.08 0.61 ± 0.10</cell></row><row><cell></cell><cell>30</cell><cell>✓</cell><cell>0.72 ± 0.08</cell><cell>0.58 ± 0.08</cell><cell>0.52 ± 0.09</cell><cell>0.61 ± 0.06</cell><cell>0.64 ± 0.06</cell></row><row><cell></cell><cell>30</cell><cell>✗</cell><cell>0.67 ± 0.09</cell><cell>0.49 ± 0.07</cell><cell>0.52 ± 0.07</cell><cell>0.61 ± 0.10</cell><cell>0.62 ± 0.10</cell></row><row><cell></cell><cell>64</cell><cell>✓</cell><cell>0.74 ± 0.05</cell><cell>0.55 ± 0.07</cell><cell>0.54 ± 0.06</cell><cell>0.61 ± 0.06</cell><cell>0.68 ± 0.07</cell></row><row><cell></cell><cell>64</cell><cell>✗</cell><cell>0.68 ± 0.05</cell><cell>0.48 ± 0.07</cell><cell>0.47 ± 0.07</cell><cell>0.57 ± 0.10</cell><cell>0.55 ± 0.11</cell></row><row><cell></cell><cell>10</cell><cell>✓</cell><cell>0.62 ± 0.13</cell><cell>0.49 ± 0.05</cell><cell>0.48 ± 0.07</cell><cell>0.48 ± 0.05</cell><cell>0.51 ± 0.08</cell></row><row><cell></cell><cell>10</cell><cell>✗</cell><cell>0.54 ± 0.16</cell><cell>0.45 ± 0.05</cell><cell>0.46 ± 0.05</cell><cell>0.61 ± 0.03</cell><cell>0.58 ± 0.05</cell></row><row><cell>Wisconsin</cell><cell>20 20</cell><cell>✓ ✗</cell><cell>0.76 ± 0.10 0.70 ± 0.09</cell><cell>0.50 ± 0.06 0.44 ± 0.06</cell><cell>0.49 ± 0.08 0.48 ± 0.05</cell><cell>0.50 ± 0.10 0.60 ± 0.06</cell><cell>0.52 ± 0.11 0.58 ± 0.05</cell></row><row><cell></cell><cell>30</cell><cell>✓</cell><cell>0.78 ± 0.06</cell><cell>0.49 ± 0.06</cell><cell>0.47 ± 0.09</cell><cell>0.50 ± 0.08</cell><cell>0.53 ± 0.11</cell></row><row><cell></cell><cell>30</cell><cell>✗</cell><cell>0.72 ± 0.05</cell><cell>0.44 ± 0.07</cell><cell>0.49 ± 0.08</cell><cell>0.61 ± 0.03</cell><cell>0.57 ± 0.04</cell></row><row><cell></cell><cell>64</cell><cell>✓</cell><cell>0.81 ± 0.05</cell><cell>0.49 ± 0.06</cell><cell>0.51 ± 0.10</cell><cell>0.51 ± 0.12</cell><cell>0.53 ± 0.13</cell></row><row><cell></cell><cell>64</cell><cell>✗</cell><cell>0.76 ± 0.08</cell><cell>0.43 ± 0.07</cell><cell>0.49 ± 0.06</cell><cell>0.57 ± 0.04</cell><cell>0.57 ± 0.07</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The symbol D is also used in other parts of the text to denote the spatial dimensionality of the NST. We stick to D here since it is standard in the metric embedding literature, but the two should not be confused.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Since any Wj must have only non-negative entries, then if Wj is additionally orthogonal, it must be a permutation matrix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>W here is used for time dimensions and not edge weigths.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>J Benitez, Takashi Furuya, Florian Faucher, Anastasis Kratsios, Xavier Tricoche, and Maarten V de Hoop. Out-of-distributional risk bounds for neural operators with applications to the helmholtz equation. arXiv preprint arXiv:2301.11509, 2023. Haitz Sáez de Ocáriz Borde and Anastasis Kratsios. Neural snowflakes: Universal latent graph inference via trainable latent geometries. In The Twelfth International Conference on Learning Representations, 2023.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Note that ε def.= 1 -α in the notation of<ref type="bibr" target="#b68">Naor &amp; Neiman (2012)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>The isometric embeddability into a Euclidean snowflake characterizes finite metric spaces; see(Le Donne  et al., 2018, Corollary 2.2).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors thank <rs type="person">James Lucas</rs> and the anonymous reviewers for helpful feedback on early versions of this manuscript. <rs type="person">M. Bronstein</rs> acknowledges this research is partially supported by the <rs type="funder">EPSRC</rs> <rs type="grantName">Turing AI World-Leading Research Fellowship</rs> No. <rs type="grantNumber">EP/X040062/1</rs> and the <rs type="funder">EPSRC AI Hub on Mathematical Foundations of Intelligence</rs>: <rs type="programName">An "Erlangen Programme</rs>" for AI No. <rs type="grantNumber">EP/Y028872/1</rs>. X. Dong acknowledges support from the <rs type="funder">Oxford-Man Institute of Quantitative Finance</rs> and the <rs type="funder">EP-SRC</rs> (<rs type="grantNumber">EP/T023333/1</rs>). A. Kratsios acknowledges financial support from an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery Grant</rs> No. <rs type="grantNumber">RGPIN-2023-04482</rs> and No. <rs type="grantNumber">DGECR-2023-00230</rs>.   </p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k4Gmspn">
					<idno type="grant-number">EP/X040062/1</idno>
					<orgName type="grant-name">Turing AI World-Leading Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_FfBCbbV">
					<idno type="grant-number">EP/Y028872/1</idno>
					<orgName type="program" subtype="full">An &quot;Erlangen Programme</orgName>
				</org>
				<org type="funding" xml:id="_QK2W73e">
					<idno type="grant-number">EP/T023333/1</idno>
				</org>
				<org type="funding" xml:id="_2XtCwaK">
					<idno type="grant-number">RGPIN-2023-04482</idno>
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
				<org type="funding" xml:id="_r5SGxXx">
					<idno type="grant-number">DGECR-2023-00230</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>We believe that the potential societal consequences are minimal and do not require specific highlighting at this time. We commit to ongoing awareness of the broader implications of our work and will remain vigilant in assessing any future societal impacts that may emerge as our theoretical framework is applied in practical settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 ADDITIONAL RESULTS</head><p>In this appendix, we provide extended tables of the results presented in the main text.</p><p>Table <ref type="table">9</ref>: DAG embedding results. Embedding dimension D = T = 2, 4, 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Spacetime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Embedding Dim Distortion (average ± stand dev) Max Distortion Directionality ∥x -y∥ 0.5 log(1 + ∥x -y∥) In this section, we aim to provide additional intuition to the reader on how NSTs compare to the fixed-geometry embedding baselines and why we expect them to perform better.</p><p>For all experiments, we use a feature encoder that maps the graph node features to the relevant manifold. We assume that the output of the feature encoder is in Euclidean space in all cases. We then proceed according to the specific geometry. This approach aligns with previous works such as neural snowflakes (Borde &amp; <ref type="bibr">Kratsios, 2023)</ref> and other studies on product manifold embeddings <ref type="bibr">(Borde et al., 2023b)</ref>.</p><p>Intuitively, one can understand the problem as a two-step process, which in practice is learned jointly via end-to-end gradient descent. In the first step, the encoder learns to position the graph nodes in space-that is, it learns to map the original node features to coordinates in the manifold. In the second step, the distance between points on the manifold is evaluated, either based on a given metric when the geometry is fixed or by learning the geometry (the metric itself) in a data-driven fashion.</p><p>In the cases of Minkowski and De Sitter space, the geometry is not learned but given. More specifically, in Minkowski space, we use the feature outputs of the Euclidean feature encoder, apply a change in the metric tensor, and recognize that one coordinate has a different signature. In this particular case, we do not require an exponential map since the space is flat. For De Sitter space, the situation is similar: only the positioning of points in the manifold is optimized via gradient descent, while the geometry remains fixed. In terms of implementation and metric calculation, De Sitter space is a curved manifold, making computations more complicated than for Minkowski space. We use an embedding approach that avoids exponential maps: we map points into a De Sitter hyperboloid in a higher-dimensional Minkowski space and compute geodesic distances there. The geometric operations we utilize include normalizing spatial components, computing the De Sitter inner product, and handling both timelike and spacelike separations. Additionally, we would like to highlight that, although the authors in <ref type="bibr" target="#b60">(Law &amp; Lucas, 2023</ref>) also use Minkowski and De Sitter spaces, our baselines are more powerful. This is because we employ a neural network feature encoder to optimize the placement of node embeddings within the manifold NSTs are intrinsically different in that their geometry is not fixed but rather random at initialization and parametrized by a neural network that always outputs a pseudo-quasi-metric by construction. The specific pseudo-quasi-metric it generates is learned during optimization based on the input graph. Hence, in this case, both the position of the points on the manifold and the geometry of the manifold itself are learned jointly via gradient descent. For NSTs, we do not need exponential maps either since the construction is based on warping a norm. During optimization, the inputs are the graph node features, and the targets are both the distances between nodes (induced by the graph geometry) and the causality structure (given by the edge directions). Since neural spacetimes are able to reshape the geometry of the embedding space as a function of the data, they are inherently more flexible and they are able to embed DAGs with less distortion (the best possible value for distortion is 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 DOWNSTREAM APPLICATION TESTING: NODE CLASSIFICAITION ON EMBEDDED HETEROPHILIC GRAPHS</head><p>Lastly, we evaluate the downstream performance of our encoding approach in transductive node classification for heterophilic graphs, specifically using the Cornell, Texas, and Wisconsin datasets.</p><p>It is important to note that this is not a standard application of NSTs, as their original design aims to embed DAGs with minimal distortion. NSTs typically use the feature encoder network as an initial intermediate mapping to a latent space, which is then utilized by the learnable quasi-metric and learnable partial order to encode distance and directionality respectively. While the neural quasimetric takes two node feature vectors as input and outputs a scalar distance value (making it unsuitable as direct input for a downstream node classification task), the neural partial order operates pointwise and returns a feature vector. Therefore, we adopt the following approach. First, we train the complete NST model to encode the original graph into the continuous geometry, simultaneously optimizing all three networks composing the NST, E, D, and T . Once trained, we use the frozen feature encoder and neural partial order as pretrained featurizers to encode the original node features from the dataset at hand. These transformed features are then fed into a downstream network that is trained as a node-wise classifier. That is, the new node features for the graphs become:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local embeddings of metric spaces</title>
		<author>
			<persName><forename type="first">Ittai</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Bartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-ninth annual ACM Symposium on Theory of Computing</title>
		<meeting>the thirty-ninth annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="631" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metric embedding via shortest path decompositions</title>
		<author>
			<persName><forename type="first">Ittai</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Filtser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="314" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal optimal transport and its links to enlargement of filtrations and continuous-time stochastic optimization</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Acciaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Backhoff-Veraguas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasiia</forename><surname>Zalashko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="2918" to="2953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Designing universal causal deep learning models: The geometric (hyper) transformer</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Acciaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudmund</forename><surname>Pammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Finance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="671" to="735" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integral probability metrics pac-bayes bounds</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baruch</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3123" to="3136" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Snowflake universality of wasserstein spaces</title>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Sci. Éc. Norm. Supér</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="700" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">All adapted topologies are equal. Probability Theory and Related Fields</title>
		<author>
			<persName><forename type="first">Julio</forename><surname>Backhoff-Veraguas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bartl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Beiglböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Eder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="1125" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new characterization of partial orders of dimension two</title>
		<author>
			<persName><forename type="first">Kirby</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Fishburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. New York Acad. Sci</title>
		<idno type="ISSN">0077-8923</idno>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="1749" to="6632" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The wasserstein space of stochastic processes</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bartl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Beiglböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudmund</forename><surname>Pammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14245</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>John K Beem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">L</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName><surname>Easley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global lorentzian geometry. Routledge</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalar curvature of a causal set</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Dionigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fay</forename><surname>Benincasa</surname></persName>
		</author>
		<author>
			<persName><surname>Dowker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">181301</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural latent geometry search: Product manifold inference via gromov-hausdorffinformed bayesian optimization</title>
		<author>
			<persName><forename type="first">Haitz</forename><surname>Sáez De Ocáriz Borde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismael</forename><forename type="middle">Morales</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Gij638d76O" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems, 2023a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent graph inference using product manifolds</title>
		<author>
			<persName><forename type="first">Haitz</forename><surname>Sáez De Ocáriz Borde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Barbero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=JLR_B7n_Wqr" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, 2023b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The metrical interpretation of superreflexivity in Banach spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bourgain</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02766125</idno>
		<ptr target="https://doi.org/10.1007/BF02766125" />
	</analytic>
	<monogr>
		<title level="j">Israel J. Math</title>
		<idno type="ISSN">0021-2172</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Time functions on lorentzian length spaces</title>
		<author>
			<persName><forename type="first">Annegret</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Garcia-Heveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales Henri Poincaré</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On Thin Posets and Categorification. ProQuest LLC</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chandler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ph.D.)-North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">ISBN 978-1392-60821-0. Thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local global tradeoffs in metric embeddings</title>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Makarychev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2487" to="2512" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to lattices and order</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Priestley</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511809088</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511809088" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:3016223" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian structure learning with generative flow networks</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">António</forename><surname>Góis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chinenye Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Rankawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HElfed" />
	</analytic>
	<monogr>
		<title level="m">The 38th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022. 8j9g9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A decomposition theorem for partially ordered sets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dilworth</surname></persName>
		</author>
		<idno type="DOI">10.2307/1969503</idno>
		<ptr target="https://doi.org/10.2307/1969503" />
	</analytic>
	<monogr>
		<title level="j">Ann. of Math</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="166" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Concentration of measure for the analysis of randomized algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Devdatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Dubhashi</surname></persName>
		</author>
		<author>
			<persName><surname>Panconesi</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511581274</idno>
		<ptr target="https://doi-org.libaccess.lib.mcmaster.ca/10.1017/CBO9780511581274" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explicit constructions of universal R-trees and asymptotic geometry of hyperbolic spaces</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Dyubina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iosif</forename><surname>Polterovich</surname></persName>
		</author>
		<idno type="DOI">10.1112/S002460930100844X</idno>
		<ptr target="https://doi.org/10.1112/S002460930100844X" />
	</analytic>
	<monogr>
		<title level="j">Bull. London Math. Soc</title>
		<idno type="ISSN">0024- 6093</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="727" to="734" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optimal transport and wasserstein distances for causal models</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Cheridito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14085</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational methods for adapted optimal transport</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudmund</forename><surname>Pammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1A</biblScope>
			<biblScope unit="page" from="675" to="713" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Near isometric terminal embeddings for doubling metrics</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3319" to="3337" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nonpositive curvature is not coarsely universal. Inventiones mathematicae</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Eskenazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manor</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Naor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">217</biblScope>
			<biblScope unit="page" from="833" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognition algorithms for orders of small width and graphs of small Dilworth number</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felsner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Spinrad</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:ORDE.0000034609.99940.fb</idno>
		<ptr target="https://doi.org/10.1023/B:ORDE.0000034609.99940.fb" />
	</analytic>
	<monogr>
		<title level="j">Order</title>
		<idno type="ISSN">0167-8094</idno>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1572" to="9273" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A face cover perspective to ℓ 1 embeddings of planar graphs</title>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Filtser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithm 97: Shortest path</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
		<idno type="DOI">10.1145/367766.368168</idno>
		<ptr target="https://doi.org/10.1145/367766" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1962-06">jun 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An invitation to applied category theory</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Spivak</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108668804</idno>
		<ptr target="https://doi.org/10.1017/9781108668804" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Seven sketches in compositionality</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<title level="m">Hyperbolic neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain of dependence</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geroch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="437" to="449" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Uber die abgrenzung der eigenwerte einer matrix</title>
		<author>
			<persName><surname>Gershgorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">lzv. Akad. Nauk. USSR. Otd. Fiz-Mat. Nauk</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="749" to="754" />
			<date type="published" when="1931">1931</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Heterogeneous manifolds for curvature-aware graph embedding</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Luise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Heterogeneous manifolds for curvature-aware graph embedding</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Luise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Proximity algorithms for nearly doubling spaces</title>
		<author>
			<persName><forename type="first">Lee-Ad</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Krauthgamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1759" to="1769" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Poincaré inequalities and dimension free concentration of measure</title>
		<author>
			<persName><surname>Nathael Gozlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Inst. Henri Poincaré Probab. Stat</title>
		<idno type="ISSN">0246-0203</idno>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1778" to="7017" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An extension of a theorem of Schoenberg to products of spheres</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Guella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Menegatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Peron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Banach J. Math. Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="685" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Gunasingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Kam Leonard</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06625</idno>
		<title level="m">Adapted optimal transport between gaussian processes in discrete time</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding tree metrics into low-dimensional Euclidean spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="116" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Load balanced tree embeddings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><forename type="middle">E</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Hambrusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="595" to="614" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Embedding tree metrics into low dimensional euclidean spaces</title>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-first annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-first annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cuts, trees and l1-embeddings of graphs</title>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="269" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast construction of nets in low dimensional metrics, and their applications</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Manor</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first annual symposium on Computational geometry</title>
		<meeting>the twenty-first annual symposium on Computational geometry</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="150" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The causal set approach to quantum gravity. Approaches to quantum gravity: Toward a new understanding of space, time and matter</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Henson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="393" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Instancedependent generalization bounds via optimal transport</title>
		<author>
			<persName><forename type="first">Songyan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parnian</forename><surname>Kassraie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.00687" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Contagion!: Systemic Risk in Financial Networks</title>
		<author>
			<persName><surname>Thomas R Hurd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pseudo quasi metric spaces</title>
		<author>
			<persName><forename type="first">Yong-Woon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Japan Academy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1009" to="1012" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Small transformers compute universal metric embeddings</title>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Debarnot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Dokmanić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1533" to="7928" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Paper No. [170</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitz Sáez</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ocáriz</forename><surname>Borde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09250</idno>
		<title level="m">Capacity bounds for hyperbolic neural network representations of latent tree structures</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Anastasis</forename><surname>Kratsios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martina</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudmund</forename><surname>Pammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05576</idno>
		<title level="m">Tighter generalization bounds on digital computers via discrete optimal transport</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Measured descent: a new embedding method for finite metrics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geom. Funct. Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="839" to="858" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Measured descent: A new embedding method for finite metrics</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manor</forename><surname>James R Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the structure of causal spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Kronheimer</surname></persName>
		</author>
		<author>
			<persName><surname>Penrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="481" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kršek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gudmund</forename><surname>Pammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.11958</idno>
		<title level="m">General duality and dual attainment for adapted transport</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Complete quasi-pseudo-metric spaces</title>
		<author>
			<persName><surname>Kũnzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica Hungarica</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="146" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ultrahyperbolic neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spacetime representation learning</title>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qV_M_rhYajc" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ultrahyperbolic representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><surname>Stam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Isometric embeddings of snowflakes into finite-dimensional Banach spaces</title>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Le Donne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Rajala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Walsberg</surname></persName>
		</author>
		<idno type="DOI">10.1090/proc/13778</idno>
		<ptr target="https://doi.org/10.1090/proc/13778" />
	</analytic>
	<monogr>
		<title level="j">Proc. Amer. Math. Soc</title>
		<idno type="ISSN">0002- 9939</idno>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1088" to="6826" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph metanetworks for processing diverse neural architectures</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lorraine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ijK5hyxs0n" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">AMES: A differentiable embedding space selection framework for latent graph inference</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitz</forename><surname>Sáez De Ocáriz Borde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tIrGgIn8jr" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Euclidean embeddings of finite metric spaces</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="2848" to="2856" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Wisdom of crowds for robust gene network inference</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Prill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Aderhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Cordero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Dondelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Esposito</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.2016</idno>
	</analytic>
	<monogr>
		<title level="m">Rina Foygel, Alberto de la Fuente, Jan Gertheiss, and Ralf Zimmer</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On embedding trees into uniformly convex Banach spaces</title>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Matoušek</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02785579</idno>
		<ptr target="https://doi.org/10.1007/BF02785579" />
	</analytic>
	<monogr>
		<title level="j">Israel J. Math</title>
		<idno type="ISSN">0021-2172</idno>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1565" to="8511" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Assouad&apos;s theorem with dimension independent of the snowflaking</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Neiman</surname></persName>
		</author>
		<idno type="DOI">10.4171/RMI/706</idno>
		<ptr target="https://doi.org/10.4171/RMI/706" />
	</analytic>
	<monogr>
		<title level="j">Rev. Mat. Iberoam</title>
		<idno type="ISSN">0213-2230</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2235" to="0616" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Low dimensional embeddings of doubling metrics. Theory of Computing Systems</title>
		<author>
			<persName><surname>Ofer Neiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="133" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Coarse embeddings of metric spaces into Banach spaces</title>
		<author>
			<persName><forename type="first">Piotr</forename><forename type="middle">W</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Amer. Math. Soc</title>
		<idno type="ISSN">0002-9939</idno>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1088" to="6826" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Estimating causal structure using conditional dag models</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><forename type="middle">Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sach</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">54</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">On embeddings of locally finite metric spaces into lp</title>
		<author>
			<persName><forename type="first">Sofiya</forename><surname>Ostrovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><forename type="middle">I</forename><surname>Ostrovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">474</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="666" to="673" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Small distortion and volume preserving embeddings for planar and Euclidean metrics</title>
		<author>
			<persName><forename type="first">Satish</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1145/304893.304983</idno>
		<ptr target="https://doi.org/10.1145/304893.304983" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Annual Symposium on Computational Geometry</title>
		<meeting>the Fifteenth Annual Symposium on Computational Geometry<address><addrLine>Miami Beach, FL; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Manifold dimension of a causal set: Tests in conformally flat spacetimes</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review D</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24034</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Recent developments in exponential random graph (p*) models for social networks</title>
		<author>
			<persName><forename type="first">Garry</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippa</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="215" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Low distortion delaunay embedding of trees in hyperbolic plane</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on graph drawing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Positive definite functions on spheres</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Duke Math. J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="1942">1942</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On the nonexistence of bilipschitz parameterizations and geometric problems about a -∞-weights</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Semmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revista Matematica Iberoamericana</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="410" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks++</title>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName><surname>Harada</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>id=Ec85b0tUwbA</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Directed graph embeddings in pseudo-riemannian manifolds</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Wiatrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Brayne</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">235446829</biblScope>
		</imprint>
	</monogr>
	<note>Páidí Creed, and Saee Paliwal</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Tree! i am no tree! i am a low dimensional hyperbolic embedding</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Sonthalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="845" to="856" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Relativity and gravitation: Classical and quantum</title>
		<author>
			<persName><surname>Rafael D Sorkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="150" to="173" />
		</imprint>
	</monogr>
	<note>Spacetime and causal sets</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Robust causal inference using directed acyclic graphs: the r package &apos;dagitty</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Textor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benito</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Zander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Gilthorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George Th</forename><surname>Liśkiewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Ellison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1887" to="1894" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Characterizations of snowflake metric spaces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jeremy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jang-Mei</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales Fennici Mathematici</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="336" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The geodesic problem in quasimetric spaces</title>
		<author>
			<persName><forename type="first">Qinglan</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12220-008-9065-4</idno>
		<ptr target="https://doi-org.libaccess.lib.mcmaster.ca/10.1007/s12220-008-9065-4" />
	</analytic>
	<monogr>
		<title level="j">J. Geom. Anal</title>
		<idno type="ISSN">1050-6926</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="479" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Cot-gan: Generating sequential data via causal optimal transport</title>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Munn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Acciaio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8798" to="8809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: Continuous Optimization for Structure Learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11168" to="11176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
