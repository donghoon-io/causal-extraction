<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DFT: A Dual-branch Framework of Fluctuation and Trend for Stock Price Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-09">9 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengqi</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">DFT: A Dual-branch Framework of Fluctuation and Trend for Stock Price Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-09">9 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2411.06065v1[cs.CE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stock price prediction is of significant importance in quantitative investment. Existing approaches encounter two primary issues: First, they often overlook the crucial role of capturing short-term stock fluctuations for predicting high-volatility returns. Second, mainstream methods, relying on graphs or attention mechanisms, inadequately explore the temporal relationships among stocks, often blurring distinctions in their characteristics over time and the causal relationships before and after. However, the high volatility of stocks and the intricate market correlations are crucial to accurately predicting stock prices. To address these challenges, we propose a Dual-branch Framework of Fluctuation and Trend (DFT), which decomposes stocks into trend and fluctuation components. By employing a carefully design decomposition module, DFT effectively extracts short-term fluctuations and trend information from stocks while explicitly modeling temporal variations and causal correlations. Our extensive experiments demonstrate that DFT outperforms existing methods across multiple metrics, including a 300% improvement in ranking metrics and a 400% improvement in portfolio-based indicators. Through detailed experiments, we provide valuable insights into different roles of trends and fluctuations in stock price prediction. Code is available at <ref type="url" target="https://github.com/cq-">https://github.com/cq-</ref>dong/DFT 25.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Stock price prediction is a fundamental task in the field of quantitative investment <ref type="bibr" target="#b4">(Fan and Shen 2024)</ref>. However, predicting stock price trends is extremely challenging due to the high volatility and chaos of the stock market. Active trading behaviors such as buying and selling by investors drive stock price fluctuations. In addition, the stock market is also affected by many factors, including economic indicators, financial reports, political events, investor sentiment, etc. <ref type="bibr" target="#b17">(Qian et al. 2024)</ref>. Achieving a high prediction accuracy remains an ongoing challenge in this domain.</p><p>Many works have achieved remarkable results in improving prediction performance. Traditional machine learning methods, such as decision trees and support vector machines, are used to model stock return changes <ref type="bibr" target="#b15">(Nugroho, Adji, and Fauziati 2014;</ref><ref type="bibr" target="#b1">Chen and Guestrin 2016;</ref><ref type="bibr" target="#b9">Kamble 2017;</ref><ref type="bibr" target="#b22">Xie et al. 2013</ref>). However, these methods require the manual construction of large financial indicator features, Figure <ref type="figure">1</ref>: Overall performance comparsion on the CSI800 and S&amp;P500 stock datasets. and it is difficult to model complex dynamic correlations between stocks.</p><p>With the advancement of deep learning, existing methods use the powerful representation ability of neural networks to mine stock trends and correlations. At present, there are two main research ideas for stock return prediction.</p><p>(1) Time correlation. Stock price movements are caused by continuous changes in supply and demand, and time trend changes have obvious dependencies. Examples include recurrent neural networks <ref type="bibr" target="#b14">(Nelson, Pereira, and De Oliveira 2017;</ref><ref type="bibr" target="#b2">Cho et al. 2014)</ref>, convolutional neural networks <ref type="bibr" target="#b0">(Bai, Kolter, and Koltun 2018)</ref>, which model stock trends through individual stock time series features. (2) Stock correlation. Different stocks in the market form complex and dynamic dependencies due to various factors such as division of labor and industry status, and stock price fluctuations will affect each other. Fusion of stock spatiotemporal features through graphs or self-attention mechanisms <ref type="bibr" target="#b17">(Qian et al. 2024;</ref><ref type="bibr" target="#b20">Xia et al. 2024;</ref><ref type="bibr" target="#b10">Li et al. 2024</ref>) can effectively improve prediction capabilities. However, these methods face two limitations.</p><p>First, existing works ignore the importance of mining individual stock fluctuation information to predict stock returns. Stock trend information is a long-term, relatively stable change, representing the overall direction; stock fluctuation information refers to the short-term change component after removing the long-term trend, representing the profitability of the stock itself and reflecting the change in the power comparison of the long-short game. Fully exploiting volatility information can capture the implied price-volume patterns from the individual stock perspective and enhance forecasting ability when combined with the overall trend. Previous works often treat stock information as a whole without distinguishing between trend and fluctuation components, or they perform a simple decomposition followed by fusion and embedding, which leads to the mutual interference of trends and fluctuations between different stocks in the market.</p><p>Second, they often oversimplify the time correlation of stocks. Specifically, the correlation of stocks is dynamically updated rather than fixed, and the features of different time steps have time series feature differences. In addition, the interaction of stock representations depends on the time order. The information on earlier steps will affect the information on later steps and vice versa. However, when conducting correlation mining, existing methods either perform time alignment operations <ref type="bibr">(Yoo et al. 2021a)</ref>, which oversimplifies the unique properties of individual stocks at different time steps, or fail to distinguish the temporal representation of stocks, which ignores the causal relationship before and after the stock information <ref type="bibr" target="#b10">(Li et al. 2024)</ref>, weakening the model's time modeling ability.</p><p>To solve the above problems, we propose a Dual-branch Framework of Fluctuation and Trend (DFT) for Stock Price Prediction. We believe that fluctuation information is a unique attribute of individual stocks, while trends are the concentrated expression of many stocks. Therefore, we design a fluctuation-trend dual-branch model to extract features separately. By separately modeling the order of correlation, we avoid the interference of fluctuation between stocks, and can give a full play to the different roles of the two in capturing stock characteristics, which is more conducive to prediction returns. In addition, we skillfully combine the advantages of RWKV time series representation and the relationship modeling ability of the self-attention mechanism while preserving the attributes of different time steps, maintaining the causal correlation of time series, and fully mining the complex cross-time causal relationship and stock correlation. Our contributions are summarized as follows:</p><p>• We propose a new dual-branch stock prediction framework to effectively capture the temporal and stock correlations from two aspects: fluctuation and trend. To the best of our knowledge, this is the first stock price prediction model that uses a learning-based method to capture long-term trends and short-term fluctuation information simultaneously.</p><p>• We consider the role of differentiated representations of different time steps in time series correlation and causal correlation and fully mine the complex time dependency in stock data.</p><p>• We conducted experiments to validate the designs of our proposed method and demonstrated its superiority compared to baselines. Through a comprehensive analysis, we provide valuable insights into the different roles of trends and fluctuations in stock price forecasting and the impact of temporal causality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Stock Trend Prediction. In quantitative trading, the ability to predict stock trends is crucial. To achieve this, a multifactor model is commonly employed <ref type="bibr" target="#b17">(Qian et al. 2024)</ref>. This model considers multiple influencing factors from an econometric perspective, including trading volume, price dynamics, and fundamental company data such as earnings and debt ratios. With the popularity of deep learning, recurrent neural networks <ref type="bibr" target="#b14">(Nelson, Pereira, and De Oliveira 2017;</ref><ref type="bibr" target="#b5">Feng et al. 2019</ref>) and convolutional neural networks <ref type="bibr" target="#b0">(Bai, Kolter, and Koltun 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Overview</head><p>In this section, we first give the definition of the stock price prediction task in this paper. Then, the proposed DFT model is presented, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The framework mainly includes three steps: (1) Stock decomposition. We design a decomposition module to obtain the overall trend information and daily fluctuation information of each stock in the viewing window T . (2) Correlation modeling. Taking into account the different characteristics of fluctuation and trend information mentioned above, we design an independent branch containing time and stock correlation for modeling.</p><p>(3) Output prediction. We aggregate the time step information output by the two branches and input it into the prediction layer to obtain the prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem definition</head><p>Following existing works on stock market analysis <ref type="bibr" target="#b18">(Sawhney et al. 2020;</ref><ref type="bibr" target="#b8">Huynh et al. 2023;</ref><ref type="bibr" target="#b10">Li et al. 2024)</ref>, for the stock price prediction problem, we focus on predicting the stock return ratio because it normalizes the market price variety between different stocks compared to the absolute price change. Specifically, for each time step of t ∈ [1, T ], we collect the features xu,t ∈ R F of each stock u ∈ S, and the return ratio is expressed as ru = (c u,t+d -c u,t+1 )/c u,t+1 , where c u,t represents the closing price of stock u on day t, so the return ratio here represents the relative change of the closing price within d days. Consistent with (Yang et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stock decomposition</head><p>Input embedding layer. For each stock xu,t ∈ R F , u ∈ S, t ∈ [1, T ], we employ a fully connected layer to obtain the embedding representation, which is X :</p><formula xml:id="formula_0">{x u,t } u∈S,t∈[1,T ] = F F N (x u,t ) ∈ R D .</formula><p>Information decomposition module. Previous studies <ref type="bibr" target="#b6">(Gao, Wang, and Yang 2023;</ref><ref type="bibr" target="#b20">Xia et al. 2024</ref>) often ignore the fact that stock prices are affected by recent trends and daily fluctuations. We emphasize the importance of decomposing information into trends and fluctuations, especially the importance of fluctuations information for the final return prediction r u . We first extract the trend component X t from embedding X . Considering that trend information is a long-term and stable feature, We decompose the input using AvgPool with kernel size k a , which blurs local irregularities and makes the output features smoother. After that, we use the time correlation module (TC, the TC module will be introduced in detail later) for dynamic learning. The final trend can be expressed as</p><formula xml:id="formula_1">X t : {x t u,t } u∈S,t∈[1,T ] = α * Avg(x u,t ) + β * T C(x u,t )</formula><p>, where α, β denote learnable parameters. The fluctuation X f : x f u,t can be obtained by subtracting x t u,t from x u,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation modeling</head><p>Fluctuation branch. In the fluctuation branch, in order to fully exploit fluctuation information for earnings forecasting, we perform correlation modeling on the time dimension of the same stock u and on the stock dimension at the same time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Correlation modeling (TC)</head><p>We not only retain the information of each time step, but also allow the first t ∈ [1, T ] time steps of each stock u to interact with each other in order to meet the causal relationship of time modeling. We employ the RWKV model <ref type="bibr" target="#b16">(Peng et al. 2023</ref>) that combines the power of RNN and Transformer. The model absorbs the idea of RNN, so it can naturally maintain the time sequence of input features, and its internal linear attention mechanism also satisfies the causality of the input. The RWKV model structure is shown in Figure <ref type="figure" target="#fig_1">3</ref>, which mainly includes Time-Mixing and Channel-Mixing blocks. The Time-Mixing block uses three learnable parameter matrices µ r , µ k , µ v to balance the current input and the previous input. That is, R = µ r x, K = µ k x, V = µ v x, where R, K and V represent the acceptance vector, key vector, and value vector, respectively. WKV in Figure <ref type="figure" target="#fig_1">3</ref> is the linear attention operation in RWKV. The process can be expressed as:</p><formula xml:id="formula_2">wkv t = t-1 i=1 e -(t-1-i)w+ki ⊙ v i + e u+kt ⊙ v t t-1 i=1 e -(t-1-i)w+ki + e u+kt .</formula><p>where w, u are two trainable parameters, t is the current timestamp index, ⊙ represents element-wise multiplication. In the numerator and denominator, information of historical time steps. Stock Correlation modeling (SC) After modeling the correlation of stocks in the time dimension, we consider modeling the correlation between stocks at the same time step to provide additional information for the stocks. There is no temporal order and causality between different stocks, so we use the self-attention mechanism to model the correlation between stocks more flexibly. The stock fluctuation sequence h f t ∈ R S×D at each time step t is used as the input sequence of the self-attention mechanism, and then the attention matrix M ∈ R S×S is obtained, so as to observe the correlation effect between any two stocks at different time steps. The process can be expressed as:</p><formula xml:id="formula_3">Q t = W Q H f t , K t = W K H f t , V t = W V H f t , Z f t = || u∈S z f u,t = FFN(MHA(Q t , K t , V t ) + H f t ), where W Q , W K , W V are trainable parameters, H f t = || u∈S h f</formula><p>u represents local embedding of all stocks, FFN is a MLP with ReLU activation and residual connection, and MHA is the multi-head attention mechanisms.</p><p>The whole process of the fluctuation branch is as follows: first, the embedding of each stock's changes is obtained through correlation modeling in the time dimension, and then the information of the mutual influence between stocks is obtained through correlation modeling in the stock dimension:</p><formula xml:id="formula_4">{z f u,t } u∈S,t∈[1,T ] = SC(T C(x f u,t )</formula><p>). Trend branch. The trend branch is similar to the fluctuation branch, both of which include modeling the time correlation and stock correlation. The trend branch first models the correlation of stocks at the same time step because stock trends are more stable than their fluctuations. Recognizing the correlation between the trends of different stocks can provide mutual information, leading to a more accurate trend representation at the current time step. In addition, integrating the trend information of all stocks captures the overall market trend. Similar to fluctuations, trends also exhibit temporal order and causality, which is why we use RWKV to model time correlations. After two stages of correlation modeling, the trend embedding not only leverages the information between different stocks but also integrates trend information across the time dimension. The operations are defined as follows:</p><formula xml:id="formula_5">{z t u,t } u∈S,t∈[1,T ] = T C(SC(x t u,t )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output prediction</head><p>Besides the trend and fluctuation embeddings, we also introduce market guidance information. Since the dimension of the market feature is small, we will first process it to obtain the information embedding Z m of the same dimension. We use a convolution kernel with kernel = stride = k c , padding = 0 to perform a convolution operation, which is then linearly mapped to obtain Z m . This process can be formulated as:</p><formula xml:id="formula_6">Z = F F N (Z t + Z f + Z m ),</formula><p>where Z t , Z f and Z m represent the trend branch output, the fluctuation branch output, and the market guidance information embedding, respectively. In order to generate the final stock representations, we use a method similar to MAS-TER <ref type="bibr" target="#b10">(Li et al. 2024)</ref> to aggregate the embeddings of stocks in T time steps, and use the embedding z u,T of the last time step T as the query vector to aggregate the attention score results:</p><formula xml:id="formula_7">λ u,t = exp(z T u,t W λ z u,T ) T i=1 exp(z T u,i W λ z u,T )</formula><p>.</p><p>We use a linear layer as the predictor and evaluate the results by Mean Squared Error (MSE) loss function.</p><formula xml:id="formula_8">ru = F F N ( t∈[1,T ] λ u,t z u,t ), Loss = u∈S MSE (r u , ru ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets. We evaluate our framework on the Chinese and US stock market with CSI300, CSI800 and S&amp;P500 stock datasets. CSI300 and CSI800 contain the 300 and 800 stocks with the highest market capitalization in the Shanghai and Shenzhen Stock Exchanges. S&amp;P500 <ref type="bibr" target="#b4">(Fan and Shen 2024)</ref> includes the information about industries in the S&amp;P500 index from the Yahoo Finance database. The time range is from 2008.01.01 to 2023.12.31. The statistics of the datasets are summarised in Table <ref type="table" target="#tab_1">1</ref>. We follow the experimental settings of MASTER <ref type="bibr" target="#b10">(Li et al. 2024</ref>) and construct sample features from the collected data using the publicly available Al-pha158 indicators <ref type="bibr" target="#b23">(Yang et al. 2020</ref>). The lookback window length T and prediction interval d are set as 8 and 5, respectively. For market representation, we constructed 63 features for CSI300 and CSI800 datasets with CSI300, CSI500 and CSI800 market indices . We include GSPC, DJI and NDX market indices for US market. The refereable interval d ′ is set to 5, 10, 20, 30, 60.</p><p>Baselines. We compare the performance of DFT with several stock price forecasting baselines from different categories.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Comparison</head><p>Table <ref type="table" target="#tab_3">2</ref> reports the overall performance. On three representative stock datasets, DFT has achieved an absolute lead compared with both the classic time series prediction model and the latest stock price prediction SOTA. DFT has a 250% ∼ 500% improvement over SOTA <ref type="bibr" target="#b10">(Li et al. 2024)</ref>  We also find that although DLinear and TimeMixer employ the decomposition block in their framework, they still fail to perform well. We attribute this improvement to the trend and fluctuation dual-branch framework, which independently models the strong correlation between two different types of information after effectively decomposing them.</p><p>In order to verify the profitability of the model, we conduct a backtest experiment during the test dataset period as well. Following previous works <ref type="bibr" target="#b10">(Li et al. 2024;</ref><ref type="bibr" target="#b6">Gao, Wang, and Yang 2023)</ref>, we adopt "Top30-Drop30" strategy to simulate daily trading. "Top30" means keeping the stocks with the top 30 return ratios; and "Drop30" means dropping any stock whose score falls out of the top 30, regardless of its previous performance. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, within the red box area, when the benchmark indicates an overall decline, most models also exhibit negative returns. However, our model consistently maintains a high positive return during the whole test periods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To validate the design choices in our proposed framework, we perform an ablation experiment by removing components individually: Decomposition (w/o Decomposition),    <ref type="table" target="#tab_4">3</ref>. We observe the following points:</p><p>• When the Decomposition module is removed, the performance of the variant drops significantly, which demonstrates the necessity of using dual branches to decompose information because it can avoid interference between the two types of information. • The absence of correlation modeling in either the trend branch or the fluctuation branch results in decreased model performance, with a more significant drop observed in the latter. This indicates that fluctuation information requires more extensive correlation analysis than trend information. • The model performance drops when either the time correlation module (TC) or the stock correlation module (SC) is removed, indicating that both time dependence and stock dependence are significant factors in the stock market.</p><p>• Owing to masking future information, improved results are achieved in both TC-maskSA model and ours, which demonstrates the effectiveness of the temporal causality discussed in this paper for predicting stock prices. • Notably, we modify the MASTER structure by incorporating a plug-and-play decomposition block and adopting a dual-branch structure. As shown in MASTERmodified, the performance is improved by more than two times compared with the original MASTER, which reinforces the generalized ability and effectiveness of the proposed blocks.</p><p>We also conducted experiments to verify the effect of the order of correlation modeling on the trend and fluctuation branches. T CSC stands for modeling time correlation first, then stock correlation, and SCT C the other way around. The results are shown in Table <ref type="table" target="#tab_6">4</ref>.</p><p>After adjusting the order, the metrics of both model variants decrease, including a serious decline after replacing the order of the fluctuation branch, which proves that if the volatility information is first modeled on the stock dimension, it will lead to confusion about the volatility information of different stocks and increase the difficulty of model learning. Trend information, on the other hand, is more stable and better suited to prioritize stock information interactions for comprehensive information. This experiment also confirms our original intention that the trend is a centralized reflection of the whole stock movement and fluctuation is a unique feature of individual stock changes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Sensitivity</head><p>We have performed hyperparameter sensitivity experiments on the CSI800 dataset to explore the effect of different settings on our framework performance in Figure <ref type="figure" target="#fig_5">5</ref>. We focus on the Avgpool kernel k a , the market convolution kernel k c , the RWKV heads, the lookback window T , and the prediction interval d. IC and RIC are selected as the metrics.</p><p>Avgpool kernel. As shown in Figure <ref type="figure" target="#fig_5">5</ref>(a), we find that larger pool kernels k a perform better because the model is able to aggregate trend information over a longer period of time. Therefore, the extracted trend information is more representative of the long-term movement of the stocks.</p><p>Market Convolution kernel. We observe that smaller convolution kernels k c can only focus on a short period of time, and the market often changes dramatically; when the convolution kernel is large, it often leads to blurring and loss of previous market information.</p><p>RWKV head. We find that an appropriate number of heads can fully utilize representation embedding, while too many or too small heads reduce information utilization.</p><p>Lookback window. We also explore the effect of different lookback window lengths T on the predictive effectiveness of the model. As shown in Figure <ref type="figure" target="#fig_5">5</ref>(d), for most of the datasets, the model tends to perform better when the lookback window is longer because it has access to more historical information to capture trends and changes in volatility.</p><p>Prediction interval. A shorter prediction length makes it difficult to learn random labels, resulting in poor prediction results. Properly increasing the prediction length can make the model perform better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Due to diverse stock trading mechanisms and market rules in different countries, how to fully capture the market information requires more granular modeling of the unique trading features of each market. Considering the limited space and resources, we focus our experiments on Chinese and US stock market. We also find that the indicators are greatly affected by extreme samples. If the stock data with extreme returns are retained during data preprocessing, the evaluation results of most models are greatly affected. Therefore, we use a method similar to previous work <ref type="bibr" target="#b7">(Gao et al. 2024)</ref> to handle the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduce a new framework for stock price prediction, DFT, which re-examines the composition and importance of stock information, decomposes it into trend and fluctuation components, and uses independent correlation modeling branches to process it. At the same time, considering that the fluctuation component is a separate attribute of a stock, while the trend can be jointly reflected by the stock market, the opposite correlation modeling process is set up, and the causal relationship of time series data such as stocks is innovatively considered. On the three mainstream data sets, compared with all benchmarks, DFT has a huge improvement in all four indicators compared to the previous SOTA model. In short, DFT proposes a new perspective for stock price prediction. In the future, our goal is to study stock price prediction with finer granularity in time and explore greater possibilities for high-frequency trading.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our model DFT structure, a framework for independently modeling the correlation of time and stock dimensions in fluctuation and trend information.</figDesc><graphic coords="3,54.00,54.00,504.01,196.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The RWKV module structure.</figDesc><graphic coords="4,65.93,77.72,214.65,138.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>TimeMixer<ref type="bibr" target="#b19">(Wang et al. 2024</ref>): state-of-the-art (SOTA) temporal prediction models using Transformer or MLP structure.• DTML(Yoo et al. 2021a): an effective stock correlation mining method, which adopts the attention mechanism to mine the dynamic correlation among stocks and incorporates the market information into modeling. • StockMixer (Fan and Shen 2024): a simple but powerful MLP-based architecture that performs indicator mixing, followed by time mixing, and finally stock mixing to imitate information exchange and market influences explicitly. • MASTER<ref type="bibr" target="#b10">(Li et al. 2024</ref>): a state-of-the-art stock price forecasting model, which uses the transformer to model the momentary and cross-time stock correlation and leverages market information for automatic feature selection.Implementation. We implement the model using PyTorch and follow the data settings of previous work<ref type="bibr" target="#b10">(Li et al. 2024</ref>). The model is trained using CosineAnnealingLR (Loshchilov and Hutter 2016) with a warm-up phase and optimized by the Adam optimizer. The learning rate is kept in the range 2 × 10 -4 , 3 × 10 -3 . The warm-up epochs are set to 10 and the number of epochs between two restarts is 15. A total of 75 epochs are trained and the results of the last epoch are selected for testing. All experiments are conduct on a server equipped with an Intel(R) Xeon(R) CPU E5-2678 v3 (48 CPUs, 125GB) and a NVIDIA GeForce RTX 3090 GPU (24 GB). We run the training and testing procedures 5 times, and the average and standard deviation are reported for all methods.Metrics. To comprehensively measure the performance of the framework, we employ ranking metrics and return on investment metrics based on previous work<ref type="bibr" target="#b10">(Li et al. 2024)</ref>. IC and RankIC (RIC) are the Pearson coefficient and Spearman coefficient averaged at a daily frequency. ICIR and RankI-CIR (RICIR) are normalized metrics of IC and RankIC by dividing the standard deviation. The above four metrics evaluate stock selection ability and overall ranking performance of model. We also use investment-based indicators AR and IR. AR measures the annual expected excess return generated by the investment, while IR measures the risk-adjusted performance of an investment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>in rankingbased indicators and a 400% ∼ 1000% improvement in portfolio-based indicators. By visualizing the metrics in Figure 1, it is clear that our model outperforms other baselines by a wide margin. It is worth noting that while MASTER, DTML and StockMixer also design their own correlation modeling modules, our model still achieves the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cumulative portfolio returns on the CSI800 and S&amp;P500 test sets. Benchmark represents the CSI300 and S&amp;P500 market indices, respectively.</figDesc><graphic coords="5,319.50,355.01,238.50,230.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The results of hyperparameter sensitivity. The xaxis in the plots represents Avgpool kernel size, convolutional kernel size, RWKV heads, lookback window length, and predict interval, respectively. The results of (a), (b) and (c) are obtained in the CSI800 dataset.</figDesc><graphic coords="7,319.50,115.07,238.46,146.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Detailed statistics of the datasets.</figDesc><table><row><cell>Market</cell><cell cols="3">CSI300 CSI800 S&amp;P500</cell></row><row><cell>Stocks</cell><cell>300</cell><cell>800</cell><cell>500</cell></row><row><cell>Training period</cell><cell cols="3">2008/01/01-2020/03/31</cell></row><row><cell>Validation period</cell><cell cols="3">2020/04/01-2020/06/30</cell></row><row><cell>Test period</cell><cell cols="3">2020/07/01-2023/12/31</cell></row><row><cell># Features</cell><cell></cell><cell>158 + 63</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p>• LSTM</p><ref type="bibr" target="#b14">(Nelson, Pereira, and De Oliveira 2017)</ref></p>: a Long Short-Term Memory network based stock price forecasting method. • Informer</p><ref type="bibr" target="#b27">(Zhou et al. 2021)</ref></p>, DLinear</p><ref type="bibr" target="#b26">(Zeng et al. 2023)</ref></p>, iTransformer</p><ref type="bibr" target="#b13">(Liu et al. 2023),</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 0.007 1.54 * ± 0.62 0.092 * ± 0.003 0.88 * ± 0.08 1.75 * ± 0.22 14.2 * ± 1.8 ± 0.009 1.11 * ± 0.19 0.053 * ± 0.006 0.60 * ± 0.15 0.84 * ± 0.10 8.2</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>IC ↑</cell><cell>ICIR ↑</cell><cell>RankIC ↑</cell><cell>RankICIR ↑</cell><cell>AR ↑</cell><cell>IR ↑</cell></row><row><cell></cell><cell>LSTM</cell><cell>0.041 ± 0.003</cell><cell>0.29 ± 0.04</cell><cell>0.043 ± 0.005</cell><cell>0.30 ± 0.04</cell><cell>0.09 ± 0.05</cell><cell>0.9 ± 0.5</cell></row><row><cell></cell><cell>Informer</cell><cell>0.040 ± 0.008</cell><cell>0.31 ± 0.06</cell><cell>0.037 ± 0.010</cell><cell>0.31 ± 0.06</cell><cell>0.14 ± 0.03</cell><cell>1.5 ± 0.4</cell></row><row><cell></cell><cell>DLinear</cell><cell>0.040 ± 0.001</cell><cell>0.25 ± 0.01</cell><cell>0.037 ± 0.001</cell><cell>0.26 ± 0.01</cell><cell>0.04 ± 0.01</cell><cell>0.4 ± 0.1</cell></row><row><cell></cell><cell>TimeMixer</cell><cell>0.039 ± 0.001</cell><cell>0.26 ± 0.01</cell><cell>0.037 ± 0.002</cell><cell>0.26 ± 0.01</cell><cell>0.06 ± 0.01</cell><cell>0.6 ± 0.1</cell></row><row><cell>CSI300</cell><cell>iTransformer</cell><cell>0.043 ± 0.001</cell><cell>0.28 ± 0.01</cell><cell>0.042 ± 0.003</cell><cell>0.28 ± 0.01</cell><cell>0.07 ± 0.01</cell><cell>0.6 ± 0.1</cell></row><row><cell></cell><cell>DTML</cell><cell>0.049 ± 0.004</cell><cell>0.34 ± 0.02</cell><cell>0.049 ± 0.004</cell><cell>0.34 ± 0.02</cell><cell>0.16 ± 0.01</cell><cell>1.6 ± 0.2</cell></row><row><cell></cell><cell>StockMixer</cell><cell>0.041 ± 0.006</cell><cell>0.29 ± 0.06</cell><cell>0.041 ± 0.008</cell><cell>0.29 ± 0.07</cell><cell>0.09 ± 0.08</cell><cell>0.9 ± 0.8</cell></row><row><cell></cell><cell>MASTER</cell><cell>0.055 ± 0.002</cell><cell>0.41 ± 0.02</cell><cell>0.054 ± 0.002</cell><cell>0.41 ± 0.02</cell><cell>0.20 ± 0.02</cell><cell>1.9 ± 0.2</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">0.143  *  ± 0.001 1.65  *  ± 0.06 0.097  *  ± 0.001 0.94  *  ± 0.04 0.80  *  ± 0.06</cell><cell>8.7  *  ± 0.9</cell></row><row><cell></cell><cell>LSTM</cell><cell>0.043 ± 0.004</cell><cell>0.36 ± 0.05</cell><cell>0.043 ± 0.006</cell><cell>0.35 ± 0.04</cell><cell>0.15 ± 0.04</cell><cell>1.2 ± 0.3</cell></row><row><cell></cell><cell>Informer</cell><cell>0.043 ± 0.004</cell><cell>0.39 ± 0.04</cell><cell>0.047 ± 0.006</cell><cell>0.39 ± 0.04</cell><cell>0.18 ± 0.04</cell><cell>1.6 ± 0.3</cell></row><row><cell></cell><cell>DLinear</cell><cell>0.033 ± 0.001</cell><cell>0.27 ± 0.01</cell><cell>0.033 ± 0.001</cell><cell>0.27 ± 0.01</cell><cell>0.06 ± 0.01</cell><cell>0.5 ± 0.1</cell></row><row><cell></cell><cell>TimeMixer</cell><cell>0.033 ± 0.001</cell><cell>0.29 ± 0.01</cell><cell>0.033 ± 0.003</cell><cell>0.28 ± 0.01</cell><cell>0.05 ± 0.01</cell><cell>0.4 ± 0.1</cell></row><row><cell>CSI800</cell><cell>iTransformer</cell><cell>0.034 ± 0.001</cell><cell>0.28 ± 0.01</cell><cell>0.034 ± 0.001</cell><cell>0.28 ± 0.01</cell><cell>0.07 ± 0.01</cell><cell>0.6 ± 0.1</cell></row><row><cell></cell><cell>DTML</cell><cell>0.044 ± 0.001</cell><cell>0.35 ± 0.02</cell><cell>0.049 ± 0.003</cell><cell>0.35 ± 0.02</cell><cell>0.14 ± 0.01</cell><cell>1.1 ± 0.1</cell></row><row><cell></cell><cell>StockMixer</cell><cell>0.032 ± 0.002</cell><cell>0.27 ± 0.03</cell><cell>0.034 ± 0.002</cell><cell>0.27 ± 0.02</cell><cell>0.06 ± 0.02</cell><cell>0.5 ± 0.1</cell></row><row><cell></cell><cell>MASTER</cell><cell>0.048 ± 0.001</cell><cell>0.44 ± 0.03</cell><cell>0.051 ± 0.003</cell><cell>0.44 ± 0.02</cell><cell>0.19 ± 0.02</cell><cell>1.6 ± 0.1</cell></row><row><cell cols="3">Ours LSTM Informer DLinear TimeMixer 0.138  S&amp;P500 0.006 ± 0.001 0.015 ± 0.001 0.013 ± 0.001 0.014 ± 0.001 iTransformer 0.014 ± 0.001</cell><cell>0.04 ± 0.01 0.08 ± 0.01 0.09 ± 0.01 0.10 ± 0.02 0.09 ± 0.01</cell><cell>0.005 ± 0.001 0.013 ± 0.002 0.012 ± 0.001 0.013 ± 0.001 0.013 ± 0.001</cell><cell>0.04 ± 0.01 0.09 ± 0.01 0.09 ± 0.01 0.10 ± 0.02 0.09 ± 0.01</cell><cell>0.09 ± 0.03 0.07 ± 0.02 0.08 ± 0.01 0.07 ± 0.01 0.09 ± 0.02</cell><cell>0.8 ± 0.2 0.6 ± 0.2 0.7 ± 0.1 0.7 ± 0.1 0.7 ± 0.2</cell></row><row><cell></cell><cell>DTML</cell><cell>0.012 ± 0.001</cell><cell>0.10 ± 0.01</cell><cell>0.011 ± 0.001</cell><cell>0.11 ± 0.01</cell><cell>0.05 ± 0.02</cell><cell>0.5 ± 0.1</cell></row><row><cell></cell><cell>StockMixer</cell><cell>0.013 ± 0.001</cell><cell>0.10 ± 0.01</cell><cell>0.012 ± 0.001</cell><cell>0.09 ± 0.04</cell><cell>0.06 ± 0.01</cell><cell>0.6 ± 0.1</cell></row><row><cell></cell><cell>MASTER</cell><cell>0.021 ± 0.001</cell><cell>0.14 ± 0.01</cell><cell>0.019 ± 0.001</cell><cell>0.15 ± 0.01</cell><cell>0.08 ± 0.02</cell><cell>0.7 ± 0.1</cell></row><row><cell></cell><cell>Ours</cell><cell>0.114</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* * * ± 1.2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall performance comparison. The best results are in bold and the second-best results are underlined. And * denotes statistically significant improvement (measured by t-test with p-value &lt; 0.01) over all baselines.</figDesc><table><row><cell></cell><cell>IC</cell><cell>ICIR</cell><cell>RIC</cell><cell>RICIR</cell></row><row><cell>w/o Decomposition</cell><cell>0.045</cell><cell>0.43</cell><cell>0.047</cell><cell>0.44</cell></row><row><cell>w/o Fluctuation</cell><cell>0.050</cell><cell>0.44</cell><cell>0.052</cell><cell>0.45</cell></row><row><cell>w/o Trend</cell><cell>0.134</cell><cell>1.53</cell><cell>0.086</cell><cell>0.87</cell></row><row><cell>w/o SC</cell><cell>0.071</cell><cell>0.75</cell><cell>0.057</cell><cell>0.46</cell></row><row><cell>w/o TC</cell><cell>0.097</cell><cell>0.93</cell><cell>0.077</cell><cell>0.69</cell></row><row><cell>TC-SA</cell><cell>0.121</cell><cell>1.36</cell><cell>0.088</cell><cell>0.81</cell></row><row><cell>TC-maskSA</cell><cell>0.131</cell><cell>1.40</cell><cell>0.091</cell><cell>0.84</cell></row><row><cell>Ours</cell><cell cols="3">0.138 1.54 0.092</cell><cell>0.88</cell></row><row><cell>MASTER-modified</cell><cell>0.128</cell><cell>1.38</cell><cell>0.088</cell><cell>0.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on CSI800 to validate the effectiveness of the proposed dual-branch decomposition framework and modeling temporal causality mechanism. The best results are in bold.</figDesc><table><row><cell>correlations in Fluctuation branch (w/o Fluctuation), cor-</cell></row><row><cell>relations in Trend branch (w/o Trend), Stock Correlation</cell></row><row><cell>(w/o SC) and Time Correlation (w/o TC). In order to verify</cell></row><row><cell>the effectiveness of our temporal causality mechanism, we</cell></row><row><cell>use self attention (TC-SA) and masked self attention (TC-</cell></row><row><cell>maskSA) to model time correlation instead of RWKV. The</cell></row><row><cell>experiments are performed on the CSI800 dataset, and the</cell></row><row><cell>results are presented in Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note><p>DFT performance with varying correlation orders. The best results are in bold.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>t-1 i=1 e -(t-1-i)w+ki ⊙ v i represents the information accumulation of [1, t -1]. Since i &lt; t, the time step t can only obtain the stock information before t, thus ensuring the temporal causality of the input information. Channel-Mixing block obtains the input of Time-Mixing block for information fusion again, and mixes different channel features through nonlinear mapping. The calculation methods of R ′ , K ′ and V ′ are similar. Finally, the original time embedding of the stock X f is well modeled by RWKV, H f : h f u = RWKV x f u ∈ R T ×D , and each time step t retains local details and encodes the</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their insightful reviews.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility Checklist</head><p>This paper:</p><p>• Includes a conceptual outline and/or pseudocode description of AI methods introduced. (yes) • Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results. (yes) • Provides well marked pedagogical references for lessfamiliare readers to gain background necessary to replicate the paper. (yes) Does this paper make theoretical contributions? (no) Does this paper rely on one or more datasets? (yes)  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical Multi-Scale Gaussian Transformer for Stock Movement Prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4640" to="4646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">StockMixer: A Simple Yet Strong MLP-Based Architecture for Stock Price Forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8389" to="8397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal relational ranking for stock prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">StockFormer: Learning Hybrid Trading Machines with Predictive Coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4766" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06656</idno>
		<title level="m">DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient integration of multi-order dynamics and internal dynamics in stock movement prediction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Sixteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="850" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Short and long term stock trend prediction using decision tree</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kamble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Intelligent Computing and Control Systems (ICICCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1371" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MASTER: Market-Guided Stock Transformer for Stock Price Forecasting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling the Stock Relation with Graph Network for Overnight Stock Movement Prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4541" to="4547" />
		</imprint>
		<respStmt>
			<orgName>International Joint Conferences on Artificial Intelligence Organization</orgName>
		</respStmt>
	</monogr>
	<note>Special Track on AI in FinTech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-based capsule network for stock movement prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on financial technology and natural language processing</title>
		<meeting>the first workshop on financial technology and natural language processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06625</idno>
		<idno>arXiv:1608.03983</idno>
		<title level="m">iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</title>
		<imprint>
			<date type="published" when="2016">2023. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sgdr: Stochastic gradient descent with warm restarts</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stock market&apos;s price movement prediction with LSTM neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decision support system for stock trading using multiple indicators decision tree</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S D</forename><surname>Nugroho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fauziati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 The 1st International Conference on Information Technology, Computer, and Electrical Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Gv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13048</idno>
		<title level="m">Rwkv: Reinventing rnns for the transformer era</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14642" to="14650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal hypergraph convolution network for stock movement forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CI-STHPAN: Pre-trained Attention Network for Stock Selection with Channel-Independent Spatio-Temporal Hypergraph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="9187" to="9195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3584" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic frames to predict stock price movement</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Creamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the association for computational linguistics</title>
		<meeting>the 51st annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11189</idno>
		<title level="m">Qlib: An ai-oriented quantitative investment platform</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2021a. Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Soun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="2037" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Soun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2037" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are transformers effective for time series forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="11121" to="11128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
