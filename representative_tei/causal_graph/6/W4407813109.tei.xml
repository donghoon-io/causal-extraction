<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Objective Causal Bayesian Optimization</title>
				<funder ref="#_GGK8GK7">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Bavarian State Ministry of Education, Science and the Arts</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-20">20 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shriya</forename><surname>Bhatija</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul-David</forename><surname>Zuercher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Thumm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Bohné</surname></persName>
						</author>
						<title level="a" type="main">Multi-Objective Causal Bayesian Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-20">20 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.14755v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In decision-making problems, the outcome of an intervention often depends on the causal relationships between system components and is highly costly to evaluate. In such settings, causal Bayesian optimization (CBO) can exploit the causal relationships between the system variables and sequentially perform interventions to approach the optimum with minimal data. Extending CBO to the multi-outcome setting, we propose multi-objective causal Bayesian optimization (MO-CBO), a paradigm for identifying Paretooptimal interventions within a known multi-target causal graph. We first derive a graphical characterization for potentially optimal sets of variables to intervene upon. Showing that any MO-CBO problem can be decomposed into several traditional multi-objective optimization tasks, we then introduce an algorithm that sequentially balances exploration across these tasks using relative hypervolume improvement. The proposed method will be validated on both synthetic and real-world causal graphs, demonstrating its superiority over traditional (non-causal) multi-objective Bayesian optimization in settings where causal information is available.</p><p>1. We formally define MO-CBO as a new class of optimization problems.</p><p>2. We provide a formal method to reduce the search space of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Decision-making problems arise in a variety of domains, such as healthcare, manufacturing or public policy, and involve manipulating variables to obtain an outcome of interest. In many such domains, interventions are inherently costly, and practical applications are subject to budgetary constraints. Moreover, these systems are often governed by causal mechanisms, which can be exploited to efficiently approach optimal outcomes in a targeted and cost-efficient manner. A well-established strategy for optimizing such 1 Department of Computer Engineering, Technical University of Munich, Munich, Germany 2 Department of Engineering, University of Cambridge, Cambridge, United Kingdom 3 The Alan Turing Institute, London, United Kingdom. Correspondence to: Shriya Bhatija &lt;shriya.bhatija@tum.de&gt;. expensive-to-evaluate black-box functions is Bayesian optimization <ref type="bibr" target="#b18">(Shahriari et al., 2016)</ref>, but it cannot leverage the causal structure between its input variables. To this end, causal Bayesian optimization (CBO) <ref type="bibr" target="#b0">(Aglietti et al., 2020)</ref> was introduced to generalize Bayesian optimization to settings where causal information is available. While existing CBO variants focus on optimizing a single objective, realworld systems often require simultaneous optimization of multiple outcomes. Here, the aim is to establish optimal trade-offs between objectives, a so called Pareto front, instead of identifying a single objective's global optimum. As an example, consider the graph in Figure <ref type="figure" target="#fig_0">1</ref> (a), depicting the causal relationships between prostate specific antigen (PSA) and its risk factors <ref type="bibr">(Ferro et al., 2015)</ref>. For patients sensitive to Statin medications, the aim is to determine how to reduce the relevant risk factors to minimize both Statin and PSA simultaneously. Figure <ref type="figure" target="#fig_0">1 (b)</ref> shows how the optimal trade-offs between the targets could behave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint: Under Review</head><p>We propose multi-objective causal Bayesian optimization (MO-CBO) as a method that generalizes CBO to such optimization problems involving multiple outcome variables. A visualization of our methodology is given in Figure <ref type="figure" target="#fig_1">2</ref>, highlighting some of the key contributions of our work: MO-CBO problems, thereby disregarding sets of variables that are not worth intervening upon based on the graph topology.</p><p>3. We propose CAUSAL PARETOSELECT, an efficient algorithm for the exploration of multiple intervention strategies in parallel, guided by a custom acquisition function.</p><p>4. We experimentally show that our approach can surpass traditional (non-causal) multi-objective Bayesian optimization in settings where causal relationships are known, achieving more cost-effective, diverse, and accurate solutions.</p><p>To the best of our knowledge, there exists no other multiobjective optimization method in the literature that can consider the causal structure. We prove that our MO-CBO methodology establishes optimal trade-offs between target variables, which state-of-the-art methods cannot reach for all experimental setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>We combine multi-objective Bayesian optimization (MOBO) and techniques from causal inference to achieve multiobjective causal Bayesian optimization. Developments in MOBO range from single-point methods <ref type="bibr" target="#b10">(Knowles, 2006;</ref><ref type="bibr" target="#b25">Zuluaga et al., 2013;</ref><ref type="bibr" target="#b3">Belakaria et al., 2020)</ref> and batch methods <ref type="bibr" target="#b23">(Zhang et al., 2010;</ref><ref type="bibr" target="#b11">Konakovic Lukovic et al., 2020;</ref><ref type="bibr" target="#b6">Daulton et al., 2021)</ref> to the derivations of efficient acquisition functions <ref type="bibr" target="#b5">(Couckuyt et al., 2014;</ref><ref type="bibr" target="#b9">Henrández-Lobato et al., 2014)</ref>. Most relevant for our work is DGEMO <ref type="bibr" target="#b11">(Konakovic Lukovic et al., 2020)</ref>, a well-established MOBO algorithm that employs a novel batch selection strategy emphasizing on sample diversity in the input space.</p><p>Leveraging tools from causal inference to make causallyinformed decisions is a field called causal decision-making. Within this field, there is a growing body of research specifically focused on advancing CBO <ref type="bibr" target="#b0">(Aglietti et al., 2020)</ref>.</p><p>These advancements include extensions such as constrained CBO <ref type="bibr" target="#b2">(Aglietti et al., 2023)</ref>, dynamic CBO <ref type="bibr" target="#b1">(Aglietti et al., 2021)</ref>, and various other variants <ref type="bibr" target="#b4">(Branchini et al., 2023;</ref><ref type="bibr" target="#b8">Gultchin et al., 2023;</ref><ref type="bibr" target="#b19">Sussex et al., 2023;</ref><ref type="bibr">2024;</ref><ref type="bibr" target="#b16">Ren &amp; Qian, 2024;</ref><ref type="bibr" target="#b22">Zeitler &amp; Astudillo, 2024)</ref>. They are all designed to optimize a single target variable, rendering them infeasible for applications with multiple outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this paper, random variables and their realizations are denoted in the upper and lower case, respectively. Sets and vectors are written in bold. For a set X, its power set is denoted as P(X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-objective Bayesian optimization</head><p>Multi-objective Bayesian optimization simultaneously minimizes (or, maximizes) a set of black-box objectives f 1 , . . . , f m : X → R with minimal function evaluations. Due to potential conflicts between objectives, it aims to find trade-off solutions, known as Pareto optima:</p><formula xml:id="formula_0">Definition 2.1 (Pareto optimality). A point x ∈ X is called Pareto-optimal if there is no other x ′ ∈ X such that f i (x) ≥ f i (x ′ ) for all 1 ≤ i ≤ m, and f i (x) &gt; f i (x ′ ) for at least one 1 ≤ i ≤ m.</formula><p>The set of Pareto-optimal points in X is called Pareto set, denoted P s . The Pareto front is the image of the Pareto set under the objective functions, given by</p><formula xml:id="formula_1">P f = {f (x) = (f 1 (x), . . . , f m (x)) | x ∈ P s }.</formula><p>At each iteration of a MOBO algorithm, prior data is used to fit a surrogate model of the objectives, for which Gaussian processes <ref type="bibr" target="#b15">(Rasmussen, 2004)</ref> are predominantly used.</p><p>Based on the surrogates, an approximation Pf of the Pareto front is computed. The next point/batch to be evaluated is determined by maximizing the acquisition function, which estimates the utility of evaluating the objectives at a given point/batch. The most commonly used acquisition function is the hypervolume indicator H <ref type="bibr" target="#b24">(Zitzler &amp; Thiele, 1999)</ref>.</p><p>The larger the hypervolume, the better Pf approximates the true Pareto front. To determine how much the hypervolume would increase if a batch of samples B ⊆ X was added to the current approximation, hypervolume improvement is used:</p><formula xml:id="formula_2">HVI(f (B), Pf ) = H( Pf ∪ f (B)) -H( Pf ).<label>(1)</label></formula><p>Since DGEMO <ref type="bibr" target="#b11">(Konakovic Lukovic et al., 2020)</ref> is the relevant MOBO algorithm for our work, we briefly describe its batch selection strategy. It considers hypervolume improvement as well as sample diversity in the input space.</p><p>To this end, the so called diversity regions R 1 , . . . , R K are constructed by using the current Pareto front approximation to group the optimal points based on their performance properties in the input space. Formally, a batch is chosen as follows:</p><formula xml:id="formula_3">B = arg max B⊆X,|B|=B HVI(f (B), Pf ) s.t. max 1≤k≤K δ k (B) -min 1≤k≤K δ k (B) ≤ 1,<label>(2)</label></formula><p>where B denotes the batch size and the functions δ k (•) are defined as the number of elements from B that belong to R k . For the complete selection algorithm, we refer to <ref type="bibr" target="#b11">Konakovic Lukovic et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Causality</head><p>Graph notation A graph G = (V, E) is defined by a finite vertex set V and an edge set</p><formula xml:id="formula_4">E ⊆ V × V, contain- ing ordered pairs of distinct vertices. The subgraph of G restricted to V ′ ⊆ V is given by G[V ′ ] = (V ′ , E[V ′ ]), where E[V ′ ] = {(i, j) ∈ E | i, j ∈ V ′ }. For V ∈ V,</formula><p>the set of its parents, ancestors and descendants in G is denoted as pa(V ) G , an(V ) G , and de(V ) G , respectively. Here, no vertex is a parent, an ancestor, or a descendant of itself. Conversely, with a capital letter, this notation is extended to include the argument in the result, i.e., Pa(V ) G = pa(V ) G ∪ {V }. Moreover, we define these relations for sets of variables</p><formula xml:id="formula_5">V ′ ⊆ V, i.e., pa(V ′ ) G = V ∈V ′ pa(V ) G and Pa(V ′ ) G = V ∈V ′ Pa(V ) G .</formula><p>Equivalent conventions apply to the ancestor and descendant relationships.</p><p>Structural causal models Let ⟨V, U, F, P (U)⟩ be a structural causal model (SCM) <ref type="bibr" target="#b14">(Pearl, 2000)</ref> and G its associated acyclic graph that encodes the underlying causal mechanisms. Specifically, U is a set of independent exogenous random variables distributed according to the probability distribution P (U), V is a set of endogenous random variables, and</p><formula xml:id="formula_6">F = {f V } V ∈V is a set of deterministic functions such that V = f V (pa(V ) G , U V ), where U V ⊆ U is the set of exogenous variables affecting V ∈ V. The set U V ∩U W consists of unobserved confounders between V, W ∈ V,</formula><p>which are the exogenous variables influencing both V and W . Within V there are three different types of variables to be distinguished: non-manipulative variables C that cannot be modified, treatment variables X which can be set to specific values, and output variables Y = {Y 1 , . . . , Y m } which represent the outcome of interest. We consider only real-valued SCMs, where all endogenous variables have continuous domains. For X s ⊆ X, CC(X s ) G refers to the c-component of G <ref type="bibr" target="#b21">(Tian &amp; Pearl, 2002)</ref>, which, in this context, is the maximal set of variables that includes X s and is connected via unobserved confounders. The joint distribution of V, which is determined by P (U), is referred as observational distribution and denoted P (V).</p><p>Interventions A set X s ∈ P(X) is also called an intervention set. The interventional domain of an intervention set is given as D(X s ) = × X∈Xs D(X) and determines the feasible values of X s . An intervention on X s involves replacing the structural equations f X with a constant x, for all X ∈ X s . This action is denoted with the do-operator do(X s = x s ), where the intervention value is x s ∈ D(X s ).</p><p>The graph G Xs represents this intervention and is obtained by removing the incoming edges into X s . The observational distribution of G Xs is denoted as P (V|do(X s = x s )) and called interventional distribution. For X s = ∅, no intervention is performed and the observational and interventional distributions coincide. The tuple (X s , x s ) is referred to as an intervention set-value pair. Given two sets X s , X ′ s ⊆ X and x s ∈ D(X s ), we write by x s [X ′ s ] the values of x s corresponding to X s ∩ X ′ s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The MO-CBO Problem</head><p>In our setting, we require that the causal relationships encoded in G are known while the underlying parametrizations, i.e., F and P (U), can be unknown. This restricted information is denoted as ⟨G, Y, X, C⟩ and the assumption allows generalization across systems that share the same causal structure.</p><p>A MO-CBO problem aims to identify intervention set-value pairs (X s , x s ) that can minimize all target variables in Y simultaneously. The outcomes of an intervention do(X s = x s ) are captured as the expected values:</p><formula xml:id="formula_7">E P (Yi|do(Xs=xs)) [Y i ] := µ i (X s , x s ),<label>(3)</label></formula><p>where P (Y i |do(X s = x s )) denotes the interventional distribution of Y i , for all i = 1, . . . , m. We write µ(X s , x s ) = (µ 1 (X s , x s ), . . . , µ m (X s , x s )) for the vector notation. Since opposing causal relationships among the variables can cause conflicting single-objective optima, we consider multi-objective optimization settings, where the aim is to find Pareto-optimal solutions to establish tradeoffs between the objective functions. This motivates the application of Pareto optimality to intervention set-value pairs:</p><p>Definition 3.1 (Pareto-optimal intervention set-value pair). Given S ⊆ P(X), an intervention set-value pair (X s , x s ) with X s ∈ S, x s ∈ D(X s ) is called Pareto-optimal for S, if there is no other intervention set-value pair</p><formula xml:id="formula_8">(X ′ s , x ′ s ) with X ′ s ∈ S, x ′ s ∈ D(X ′ s ) such that µ i (X ′ s , x ′ s ) ≤ µ i (X s , x s ) for all 1 ≤ i ≤ m, and µ i (X ′ s , x ′ s ) &lt; µ i (X s , x s ) for at least one 1 ≤ i ≤ m.</formula><p>Definition 3.2 (Causal Pareto front). The space of all Paretooptimal intervention set-value pairs for a given S ⊆ P(X) is called the causal Pareto set for S, denoted P c s (S). The corresponding causal Pareto front for S, denoted P c f (S), is the m-dimensional image of the causal Pareto set under the objectives µ i , 1 ≤ i ≤ m.</p><p>We define MO-CBO problems as identifying the causal Pareto front P c f (P(X)) over the set of all intervention sets, and we refer to P c f (P(X)) simply as the causal Pareto front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Decomposition of MO-CBO Problems</head><p>To navigate the discovery of Pareto-optimal intervention set-value pairs, we aim to simplify the search space.</p><p>Definition 3.3 (Local MO-CBO problem). Let X s ∈ P(X) be an intervention set. Then, the multi-objective optimization problem defined by the objective functions µ i (X s , • ) :</p><formula xml:id="formula_9">D(X s ) → R, x s → µ i (X s , x s ), 1 ≤ i ≤ m, is called the local MO-CBO problem w.r.t. X s .</formula><p>For the local MO-CBO problem w.r.t. X s ∈ P(X), its Pareto set shall be denoted as P l s (X s ) and the associated Pareto front as P l f (X s ). Each local MO-CBO problem corresponds to a standard multi-objective optimization task, solvable with existing methods. The following proposition decomposes MO-CBO problems into such local problems. The proof is given in Appendix A.</p><p>Proposition 3.4. Given ⟨G, Y, X, C⟩, let S ⊆ P(X) be a non-empty collection of intervention sets. Then, it holds</p><formula xml:id="formula_10">P c f (S) ⊆ |S| s=1 P l f (X s ).<label>(4)</label></formula><p>Therefore, discovering the causal Pareto front requires to identify Pareto-optimal solutions of the local MO-CBO problems with respect to all intervention sets X s ∈ P(X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Solving MO-CBO Problems</head><p>In this section, we propose our methodology for solving MO-CBO problems, which is depicted in Figure <ref type="figure" target="#fig_1">2</ref>. In summary, we reduce the search space to a subset S ⊆ P(X), solve the corresponding local MO-CBO problems to find their Pareto fronts, and extract only Pareto-optimal interventionset value pairs to construct the causal Pareto front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reducing the Search Space</head><p>The complexity of solving the local MO-CBO problems with S = P(X) rises exponentially with the number of treatment variables, making this strategy impracticable for most tasks. This section proposes a superior method of exploiting the graph topology to identify a minimal subset S ⊆ P(X) with P c f (P(X)) = P c f (S). To this end, we generalize the results from <ref type="bibr" target="#b12">Lee &amp; Bareinboim (2018)</ref> to the multi-objective setting. All proofs and derivations are given Appendix B. For now, we assume that there are no non-manipulative variables, i.e., C = ∅.</p><p>First, we reduce the search space by disregarding intervention sets where some variables do not affect the targets:</p><formula xml:id="formula_11">Definition 4.1 (Minimal intervention set). A set X s ∈ P(X) is called a minimal intervention set if, for every SCM con- forming to G, there exists no subset X ′ s ⊂ X s such that for all x s ∈ D(X s ) it holds µ(X s , x s ) = µ(X ′ s , x s [X ′ s ]), for all 1 ≤ i ≤ m.</formula><p>We denote the set of minimal intervention sets with M G,Y . The following proposition characterizes such sets in a given causal graph</p><formula xml:id="formula_12">G. It is proven in Appendix B.1. Proposition 4.2. X s ∈ P(X) is a minimal intervention set if and only if it holds X s ⊆ an(Y) G Xs .</formula><p>Next, we adapt the notion of so called possibly-optimal minimal intervention sets <ref type="bibr" target="#b12">(Lee &amp; Bareinboim, 2018)</ref> for Paretooptimality. Intuitively said, a minimal intervention set is called possibly Pareto-optimal if it includes a Pareto-optimal intervention set-value pair whose outcome is unattainable with any other intervention set, for at least one SCM conforming to G. Definition 4.3 (Possibly Pareto-optimal minimal intervention set). A set X s ∈ M G,Y is called possibly Paretooptimal if, for at least one SCM conforming to G, there exists x s ∈ D(X s ) such that (X s , x s ) is Pareto-optimal for P(X), and for no</p><formula xml:id="formula_13">X ′ s ∈ M G,Y \X s , x ′ s ∈ D(X ′ s ) it holds µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m.</formula><p>We denote the set of possibly Pareto-optimal minimal intervention sets with O G,Y . Next, we establish graphtheoretical criteria to identify such sets in a given causal graph. First, the proposition below, which we proof in Appendix B.2, considers a special case:</p><formula xml:id="formula_14">Proposition 4.4. If no Y i is confounded with an(Y i ) G via unobserved confounders, then pa(Y)</formula><p>G is the only possibly Pareto-optimal minimal intervention set.</p><p>To characterize possibly Pareto-optimal minimal intervention sets in arbitrary graphs, we extend the following two definitions from <ref type="bibr" target="#b12">Lee &amp; Bareinboim (2018)</ref> to the multiobjective setting. They aim to identify a region, starting from Y, that is governed by unobserved confounders, along with its outside border that determines the realization of variables within the region.</p><formula xml:id="formula_15">(a) Y 2 Y 1 X 1 X 3 X 2 X 4 (b) X 4 X 1 X 2 Y 1 Y 2 X 3</formula><p>Definition 4.5 (Minimal unobserved confounders' territory).</p><p>Let</p><formula xml:id="formula_16">H = G[An(Y) G ]. A set of variables T in H, with Y ⊆ T, is called a UC-territory for G w.r.t. Y if De(T) H = T and CC(T) H = T. The UC-territory T is said to be minimal, denoted T = MUCT(G, Y), if no T ′ ⊂ T is a UC-territory. Definition 4.6 (Interventional border). Let T = MUCT(G, Y). Then, B = pa(T) G \T is called the inter- ventional border for G w.r.t. Y, denoted as IB(G, Y).</formula><p>Example We illustrate these two concepts with the causal graphs from Since Y 1 has an unobserved confounder with X 4 , we update</p><formula xml:id="formula_17">T = CC(Y) G = {Y 1 , Y 2 , X 4 },</formula><p>and thereafter add all the descendants of X 4 , obtaining T = {Y 1 , Y 2 , X 4 , X 1 }. Since there are no more unobserved confounders between T and An(Y) G \T, the minimal UC-territory has been found and is given by MUCT</p><formula xml:id="formula_18">(G, Y) = {Y 1 , Y 2 , X 1 , X 4 } along with IB(G, Y) = {X 2 , X 3 }.</formula><p>Interventional borders can fully characterize possibly Paretooptimal minimal intervention sets, as seen in the following two results, both of which we prove in Appendix B.2.</p><p>Proposition 4.7. IB(G Xs , Y) is a possibly Pareto-optimal minimal intervention set for any X s ∈ P(X).</p><p>Theorem 4.8. A set X s ∈ P(X) is a possibly Paretooptimal minimal intervention set if and only if it holds</p><formula xml:id="formula_19">IB(G Xs , Y) = X s .</formula><p>The following corollary states that intervening on the interventional border of a set X s ∈ P(X) is at least as optimal as intervening on the set X s itself.</p><p>Corollary 4.9. Let X s ∈ P(X) and</p><formula xml:id="formula_20">X ′ s = IB(G Xs , Y). For any x s ∈ D(X s ) there exist x ′ s ∈ D(X ′ s ) such that it holds µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m.</formula><p>In the setting of Corollary 4.9, it is possible to construct an SCM conforming to G such that strict inequality holds in at least one component. For such constructions, we refer to Appendix B.2.</p><p>Finally, we show that it suffices to only consider possibly Pareto-optimal minimal intervention sets to solve MO-CBO problems. Due to the significance of this result, we also present its proof here.</p><p>Proposition 4.10. It holds</p><formula xml:id="formula_21">P c f (P(X)) = P c f (O G,Y ).</formula><p>Proof. ⊆:</p><formula xml:id="formula_22">Assume P c f (P(X)) ̸ ⊆ P c f (O G,Y ). Then, there exists z ∈ R m , with z = µ(X s , x s ) for some intervention set-value pair (X s , x s ), such that z ∈ P c f (P(X)) and z ̸ ∈ P c f (O G,Y ). If X s ∈ O G,Y , it follows that (X s , x s ) is not Pareto-optimal for O G,Y , which is a contradiction since it is Pareto-optimal for P(X). Conversely, if X s ∈ P(X)\O G,Y , we set X ′ s = IB(G Xs , Y)</formula><p>and from Corollary 4.9, we infer that, for some SCM conforming to G, there exists</p><formula xml:id="formula_23">x ′ s ∈ D(X ′ s ) with µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m, and µ(X ′ s , x ′ s ) &lt; µ(X s , x s ), for at least one 1 ≤ i ≤ m. This results in z ̸ ∈ P c f (P(X)), which is a contradiction. ⊇: Assume P c f (O G,Y ) ̸ ⊆ P c f (P(X)). Then, there exists z ∈ R m , with z = µ(X s , x s ), such that z ∈ P c f (O G,Y ) and z ̸ ∈ P c f (P(X)). There exists some X ′ s ∈ P(X)\O G,Y , x s ∈ D(X ′ s ) such that (X ′ s , x ′ s ) is Pareto optimal and for which it holds µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m, and µ(X ′ s , x ′ s ) &lt; µ(X s , x s ), for at least one 1 ≤ i ≤ m. Since X ′</formula><p>s is not possibly Pareto-optimal, we infer from Corollary 4.9 that for X ′′ s = IB(G, Y) there exists</p><formula xml:id="formula_24">x ′′ s ∈ D(X ′′ s ) such that µ(X ′′ s , x ′′ s ) ≤ µ(X ′ s , x ′ s ), for all 1 ≤ i ≤ m. Hence, it holds µ(X ′′ s , x ′′ s ) ∈ P c f (P(X)), which is a contradiction to z ∈ P c f (O G,Y ) since X ′′ s ∈ O G,Y .</formula><p>Using Proposition 4.10, we reduce the search space of MO-CBO problems to S = O G,Y .</p><p>Example We illustrate the search space reduction with the causal graphs from Figure <ref type="figure" target="#fig_2">3</ref>. Note that in both cases it holds P(X) = 2 |X| = 16. In Figure <ref type="figure" target="#fig_2">3</ref> (a), there are no unobserved confounders and it follows</p><formula xml:id="formula_25">O G,Y = pa(Y) G = {X 1 , X 2 }.</formula><p>In Figure <ref type="figure" target="#fig_2">3</ref> (b), the intervention sets {X 1 , X 2 , X 3 } and {X 2 , X 3 } satisfy the condition from Theorem 4.8, and thus,</p><formula xml:id="formula_26">O G,Y = {{X 2 , X 3 }, {X 1 , X 2 , X 3 }}.</formula><p>We now consider the more general case with C ̸ = ∅, where non-manipulative variables can be present. The definitions for minimal intervention set and possibly Paretooptimal minimal intervention set are a straightforward extension. <ref type="bibr" target="#b13">Lee &amp; Bareinboim (2019)</ref>  </p><formula xml:id="formula_27">I 0 = D I for s = 1 to |S| do Fit surrogates μi (X s , • ) with D I 0 , i = 1, . . . , m Approximate P l s (X s ) and P l f (X s ) using μ1 , . . . , μm end for for n = 1 to N do for s = 1 to |S| do Select batch B s = {x b s } B b=1 via Equation (2) end for Select batch B ŝ from {B 1 , . . . , B |S| } via Equation (6) Intervene on X ŝ with B ŝ Augment D I n = D I n-1 ∪ {(X ŝ, x b ŝ), µ(X ŝ, x b ŝ))} B b=1</formula><p>Update surrogates μi (X ŝ, • ) with D I n , i = 1, . . . , m Approximate P l s (X ŝ) and P l f (X ŝ) using μ1 , . . . , μm end for Compute P l s (X s ),</p><formula xml:id="formula_28">P l f (X s ) from D I N , s = 1, . . . , |S| Compute P c s (S) and P c f (S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Algorithm</head><p>We introduce the algorithm CAUSAL PARETOSELECT<ref type="foot" target="#foot_0">foot_0</ref> , summarized in Algorithm 1. It assumes a known causal graph ⟨G, Y, X, C⟩, prior interventional data D I , and a set S ⊆ P(X) that specifies which local problem to consider. The idea is to alternately solve the local MO-CBO problems using the batch multi-objective Bayesian optimization algorithm DGEMO <ref type="bibr" target="#b11">(Konakovic Lukovic et al., 2020)</ref>.</p><p>More specifically, CAUSAL PARETOSELECT operates as follows: For each local MO-CBO problem w.r.t. X s ∈ S, we first fit the surrogate model to the objectives µ i (X s , • ), 1 ≤ i ≤ m, via independent Gaussian processes as proposed by DGEMO. Based on the means of the Gaussian process posteriors, we compute approximations of P l s (X s ) and P l f (X s ) utilizing the Pareto discovery approach from DGEMO. After this initial step, at each iteration, the most promising intervention set is selected for batch evaluation. The dataset is augmented with the newly evaluated batch of samples. For the corresponding intervention set, we again update the surrogate model, as well as Pareto set and front approximations of the associated local problem. After completing all iterations, CAUSAL PARETOSELECT identifies the Pareto sets and fronts for each local MO-CBO problem using the collected objective function evaluations D I N . Among those, P c s (S) and P c f (S) are determined by computing the Pareto-optimal points, which is justified by Proposition 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acquisition Function</head><p>We specify the batch selection strategy of our method in more detail. For a local MO-CBO problem w.r.t. X s ∈ S, let R 1 (X s ), . . . , R K (X s ) ⊆ D(X s ) denote the identified diversity regions from DGEMO, discussed in Section 2.1. CAUSAL PARETOSELECT seeks to balance the exploration of the Pareto fronts associated to several different local MO-CBO problems. Evaluating all B s , s = 1, . . . , |S|, during a single iteration, is an inefficient strategy. Rather, at each iteration, we select the batch with the most promising hypervolume improvement. To this end, we introduce the term relative hypervolume improvement, which we define as</p><formula xml:id="formula_29">RHVI(µ(X s , B s ), P l f (X s )) = HVI(µ(X s , B s ), P l f (X s )) H(P l f (X s ))</formula><p>.</p><p>(5) As the name suggests, relative hypervolume improvement is a normalized measure of improvement and therefore enables the assessment of batch evaluation across different intervention sets. Given B 1 , . . . , B |S| , we propose the following batch selection strategy for CAUSAL PARETOSELECT:</p><formula xml:id="formula_30">B ŝ = arg max Bs∈{B1,...,B |S| } RHVI(µ(X s , B s ), P l f (X s )). (6)</formula><p>Overall, the proposed batch selection is designed to alternately advance the Pareto fronts P l f (X 1 ), . . . , P l f (X |S| ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate CAUSAL PARETOSELECT with S = O G,Y on the causal graphs of Figure <ref type="figure" target="#fig_2">3</ref> (a) (SYNTHETIC-1), Figure <ref type="figure" target="#fig_2">3</ref> (b) (SYNTHETIC-2), and Figure <ref type="figure" target="#fig_0">1</ref> (HEALTH), featuring both synthetic and real-world settings. The full description of the SCMs can be found in Appendix C. We assume an initial dataset</p><formula xml:id="formula_31">D I = {((X s , x k s ), µ(X s , x k s ))} K,|S|</formula><p>k=1,s=1 with K = 5 samples per intervention set. In each experiment, we aim to minimize the target variables. The batch size is set to 5. For reproducibility, all experiments are run across 10 random seeds with varying initializations of D I .</p><p>To the best of our knowledge, there exists no other multiobjective optimization method in the literature that can consider the causal structure ⟨G, Y, X, C⟩. Thus, our baseline method corresponds to the MOBO algorithm DGEMO <ref type="bibr" target="#b11">(Konakovic Lukovic et al., 2020)</ref>, applying it to all treatment variables simultaneously and neglecting available causal information. Here, the objectives are µ i (X, • ) : D(X) → R, x → µ(X, x), i = 1, . . . , m. We compare CAUSAL PARE-TOSELECT to the the baseline in regards to their approximations of P c f (P(X)). To this end, we measure the accuracy of these approximations to the causal Pareto front using the generational distance (GD) as well as calculate the diversity of the solutions using the inverted generational distance (IGD) <ref type="bibr" target="#b17">(Schutze et al., 2012)</ref>. GD is defined as the average distance from any point in the approximated front  to its closest point on the ground-truth front. Conversely, IGD represents the average distance from any point in the ground-truth front to its closest point on the approximated front. The progressions of these performance metrics across all experiments are depicted in Figure <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Problems</head><p>SYNTHETIC-1 Here, it holds O G,Y = {{X 1 , X 2 }} as discussed in Section 4.1. The experimental results for SYNTHETIC-1 are shown in Figure <ref type="figure" target="#fig_7">5</ref> and demonstrate that the identified solutions from our method and DGEMO closely align with the grount-truth front P c f (P(X)). This alignment is further confirmed by the generational distance progressions in Figure <ref type="figure" target="#fig_5">4</ref> (a), which approach zero for both algorithms. Theoretically this result is expected, as</p><formula xml:id="formula_32">µ(X, x) = µ(O G,Y , x[O G,Y ]</formula><p>) guarantees that the baseline contains Pareto-optimal solutions. Moreover, we observe that CAUSAL PARETOSELECT offers a better coverage of the front, a finding supported by its lower inverted generational distance in Figure <ref type="figure" target="#fig_5">4</ref>  SYNTHETIC-2 In this setting, an unobserved confounder exists between Y 1 and X 4 , placing SYNTHETIC-2 in the general case where hidden confounders may influence target variables through their ancestors. Consequently, it holds results, illustrated in Figure <ref type="figure" target="#fig_8">6</ref>, demonstrate that while the baseline method fails to identify solutions on the causal Pareto front, CAUSAL PARETOSELECT does discover them. This finding is further reflected through the performance metrics in Figure <ref type="figure" target="#fig_5">4</ref>, which progress to significantly lower values for our method. Further experiments reveal that only interventions on {X 2 , X 3 } can yield solutions on the causal Pareto front. This can be explained as follows: The baseline strategy disrupts the causal path X 4 → X 1 → Y 1 , letting the unobserved confounder to influence Y 1 without propagating through the aforementioned path. In contrast, our approach allows interventions on {X 2 , X 3 }, preserving this causal structure. This distinction is crucial as the structural assignment of U denoting the unobserved confounder (all structural equations are specified in Appendix C.2). Not intervening on X 1 , causes this term to always be negative, yielding lower function values for Y 1 . However, if we do intervene on X 1 , it is positive with probability 0.5, causing higher values for Y 1 in the averaged outcomes.</p><formula xml:id="formula_33">O G,Y = {{X 2 , X 3 }, {X 1 , X 2 , X 3 }}. The experimental</formula><formula xml:id="formula_34">Y 1 includes the term -X 1 • X 2 • U/2, with</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real-world Problem</head><p>HEALTH We revisit the causal graph from Figure <ref type="figure" target="#fig_0">1</ref>, which is based on real-world causal relationships in the healthcare setting <ref type="bibr">(Ferro et al., 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduced MO-CBO as a new problem class in order to optimize multiple target variables within a known causal graph by sequentially performing interventions on the system. We proved that a MO-CBO problem can be decomposed into a collection of |P(X)| = 2 |X| local problems, and solving it essentially corresponds to solving these local problems. To reduce the search space, we derived theoretical results that identify possibly Pareto-optimal minimal intervention sets in a given causal graph. We proved that these sets comprise a minimal collection of local problems that are guaranteed to contain the optimal solutions of any MO-CBO problem. Moreover, we introduced CAUSAL PARETOSELECT as an algorithm that iteratively selects and solves local MO-CBO problems in the reduced search space based on relative hypervolume improvement.</p><p>Our theoretical and empirical findings highlight two distinct cases: When no unobserved confounders exist between target variables and their ancestors, both MO-CBO and MOBO can recover the ground-truth causal Pareto front. However, our approach demonstrates greater cost efficiency while constructing a more diverse set of solutions. In contrast, when unobserved confounders are present between targets and their ancestors, traditional MOBO approaches can fail to approximate the ground truth, whereas MO-CBO demonstrates efficient discovery of Pareto-optimal solutions. This occurs because unobserved confounders can propagate effects through the causal graph, and naively disrupting these paths can lead to suboptimal solutions.</p><p>In our algorithm, the surrogate model assumes independent outcomes which may limit efficiency since it overlooks shared endogenous confounders. Future work could enhance cost effectiveness by integrating multi-task Gaussian processes to better capture shared information across treatment variables. Other directions for future research include the adaptation of existing CBO variants to the multi-objective case. For instance, combining dynamic CBO <ref type="bibr" target="#b1">(Aglietti et al., 2021)</ref> with MO-CBO would lead to a MO-CBO variant that can handle time-dynamic causal models. As the field of causal decision-making continues to grow, we anticipate more progress in the development of multi-objective frameworks to address complex, real-world challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. We further emphasize that the health-related causal graph serves as a simplified illustrative example and, as such, carries no ethical concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decomposition of MO-CBO Problems</head><p>Recall the definition of the local MO-CBO problems.</p><p>Definition 3.3 (Local MO-CBO problem). Let X s ∈ P(X) be an intervention set. Then, the multi-objective optimization problem defined by the objective functions µ</p><formula xml:id="formula_35">i (X s , • ) : D(X s ) → R, x s → µ(X s , x s ), 1 ≤ i ≤ m, is called local MO-CBO problem w.r.t. X s .</formula><p>For the local MO-CBO problem w.r.t. X s ∈ P(X), we denote its Pareto set as P l s (X s ) and the associated Pareto front as P l f (X s ). Proposition 3.4. Given ⟨G, Y, X, C⟩, let S ⊆ P(X) be a non-empty collection of intervention sets. Then, it holds</p><formula xml:id="formula_36">P c f (S) ⊆ |S| s=1 P l f (X s ). (<label>7</label></formula><formula xml:id="formula_37">)</formula><p>Proof. Assume for contradiction that</p><formula xml:id="formula_38">P c f (S) ̸ ⊆ |S| s=1 P l f (X s ).</formula><p>Then, there exists some z ∈ R m such that z ∈ P c f (S) and z ̸ ∈ P l f (X s ) for all s = 1, . . . , |S|. For some intervention set X ′ s ∈ S and intervention value</p><formula xml:id="formula_39">x ′ s ∈ D(X ′ s ), it holds z = (µ 1 (X ′ s , x ′ s ), . . . , µ m (X ′ s , x ′ s ))</formula><p>. Let P l s (X 1 ), . . . , P l s (X |S| ) be the Pareto sets of the associated local MO-CBO problems w.r.t. X 1 , . . . , X |S| , respectively. Since z ̸ ∈ P l f (X ′ s ), it follows x ′ s ̸ ∈ P l s (X ′ s ), i.e. x ′ s is not Pareto-optimal in the local MO-CBO problem w.r.t. X ′ s . Thus, there exists another intervention value</p><formula xml:id="formula_40">x ′′ s ∈ D(X ′ s ) such that µ i (X ′ s , x ′ s ) ≥ µ i (X ′ s , x ′′ s ) for all i and µ i (X ′ s , x ′ s ) &gt; µ i (X ′ s , x ′′ s ) for at least one 1 ≤ i ≤ m.</formula><p>In other words, the intervention set-value pair (X ′ s , x ′ s ) is not Pareto-optimal for S since it is dominated by (X ′ s , x ′′ s ). Therefore, z ̸ ∈ P c f (S) which is a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reducing the Search Space</head><p>Lee &amp; Bareinboim (2018) leverage the graph topology of an SCM to identify intervention sets that are redundant to consider in any optimisation scheme. Their formalism exploits the rules of do-calculus to identify invariances and partial-orders among intervention sets, in order to obtain those sets that could potentially yield optimal outcomes for a given graph. To take advantage of their ideas for this paper, the relevant concepts and their theoretical properties must be extended to accommodate multi-target settings, which will be the focus of this section.</p><p>Let ⟨V, U, F, P (U)⟩ denote an SCM and G its associated acyclic graph that encodes the underlying causal mechanisms.</p><p>Recall that we assume C = ∅, i.e., there are no non-manipulative variables. In this section, we require the notation</p><formula xml:id="formula_41">E P (W|do(Xs=xs)) [W] := E[W|do(X s = x s )] for sets X s ⊆ X, W ⊆ V.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Equivalence of Intervention Sets</head><p>As a first step, we establish invariances within P(X) in regards to the effects of intervention sets on the target variables.</p><p>Recall the following definition from the main part of the paper.</p><p>Definition 4.1 (Minimal intervention set). A set X s ∈ P(X) is called a minimal intervention set if, for every SCM conforming to G, there exists no subset</p><formula xml:id="formula_42">X ′ s ⊂ X s such that for all x s ∈ D(X s ) it holds µ(X s , x s ) = µ(X ′ s , x s [X ′ s ]), for all 1 ≤ i ≤ m.</formula><p>We denote the set of minimal intervention sets with M G,Y . In other words, no subset of a minimal intervention set can achieve the same expected outcome on Y. Intervention sets, that are not minimal in the sense of Definition 4.1, are redundant to consider in any optimization task. Proposition 4.2. X s ∈ P(X) is a minimal intervention set if and only if it holds X s ⊆ an(Y) G Xs .</p><p>Proof. (If) Let x s ∈ D(X s ) be any intervention value. Assume that there is a subset</p><formula xml:id="formula_43">X ′ s ⊂ X s such that E[Y i |do(X s = x s )] = E[Y i |do(X ′ s = x s [X ′ s ])] for all 1 ≤ i ≤ m.</formula><p>Consider an SCM with real-valued variables where each V ∈ V is associated with its own binary exogenous variable U V with P (U V = 1) = 0.5. Let the function of an endogenous variable be the sum of values of its parents. For the sake of contradiction, assume X s ⊆ an(Y) G Xs . Then, there exists a directed path from X s \X ′ s to some Y i without passing</p><formula xml:id="formula_44">X ′ s . Hence, setting W = X s \X ′ s to the values w = E[W|do(X ′ s = x s [X ′ s ])] + 1 yields E[Y i |do(W = w, X ′ s = x s [X ′ s ])] &gt; E[Y i |do(X ′ s = x s [X ′ s ])], contradicting the assumption. (Only if) Assume that X s ̸ ⊆ an(Y) G Xs . Then, for X ′ s = X s ∩ an(Y) G Xs it holds X ′ s ⊂ X s and by the third rule of do-calculus, for every x s ∈ D(X s ) it holds E[Y i |do(X s = x s )] = E[Y i |do(X ′ s = x s [X ′ s ])], 1 ≤ i ≤ m</formula><p>. This is a contradiction because X s was assumed to be a minimal intervention set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Partial-Orders among Intervention Sets</head><p>Recall the definition of possibly Pareto-optimal minimal intervention sets. Definition 4.3 (Possibly Pareto-optimal minimal intervention set). A set X s ∈ M G,Y is called possibly Pareto-optimal if, for at least one SCM conforming to G, there exists x s ∈ D(X s ) such that (X s , x s ) is Pareto-optimal for P(X), and for no</p><formula xml:id="formula_45">X ′ s ∈ M G,Y \X s , x ′ s ∈ D(X ′ s ) it holds µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m.</formula><p>Characterizing such sets is the aim of this section. For simplicity, we first consider the special case in which G exhibits no unobserved confounders between Y i and any of its ancestors.</p><p>Proposition 4.4. If no Y i is confounded with an(Y i ) G via unobserved confounders, then pa(Y) G is the only possibly Pareto-optimal minimal intervention set.</p><p>Proof. Let X s = pa(Y) G , and let pa(Y) G ̸ = X ′ s be another minimal intervention set with</p><formula xml:id="formula_46">x ′ s ∈ D(X ′ s ). Define Z = X s \(X ′ s ∩ X s ) and W = X ′ s \(X ′ s ∩ X s ). Moreover, we choose an intervention value x * s ∈ D(X s ) such that it dominates x s ∈ D(X s ) which is given by x s [X ′ s ] = x ′ s [X s ] and x s [Z] = E[Z|do(X ′ s = x ′ s )]. If x s is non-dominated, define x * s = x s .</formula><p>Then, for all i = 1, . . . , m it holds</p><formula xml:id="formula_47">E[Y i |do(X s = x * s )] = E[Y i |do(X s ∩ X ′ s = x * s [X ′ s ], Z = x * s [Z])] (8) ≤ E[Y i |do(X s ∩ X ′ s = x s [X ′ s ], Z = x s [Z])] (9) = E[Y i |do(X s ∩ X ′ s = x s [X ′ s ], Z = x s [Z], W = x ′ s [W])] (10) = E[Y i |do(X s ∩ X ′ s = x s [X ′ s ], W = x ′ s [W]), Z = x s [Z]] (11) = E[Y i |do(X s ∩ X ′ s = x s [X ′ s ], W = x ′ s [W])] (12) = E[Y i |do(X ′ s = x ′ s )],<label>(13)</label></formula><p>where the inequality holds because x s (weakly) dominates x * s . Note that the second and third equalities are derived through the third and second rules of do-calculus, respectively. The second rule of do-calculus assumes that Y i is not confounded with an(Y i ) G via unobserved confounders. For X s = pa(Y) G , it is possible to construct an SCM, conforming to G, such that strict inequality holds for some Y i , see the proof of Theorem B.2. This shows that pa(Y) G is the only possibly Pareto-optimal minimal intervention set.</p><p>We continue and study the more general case where unobserved confounders can be present between Y i and any of its ancestors. For this intent, we extend two existing concepts, called minimal unobserved-confounders' territory and interventional border <ref type="bibr" target="#b12">(Lee &amp; Bareinboim, 2018)</ref>, to the multi-objective setting. Using these notions, we derive results which can fully characterize possibly Pareto-optimal minimal intervention sets in the aforementioned scenario.</p><formula xml:id="formula_48">Definition 4.5 (Minimal unobserved confounders' territory). Let H = G[An(Y) G ]. A set of variables T in H, with Y ⊆ T, is called a UC-territory for G w.r.t. Y if De(T) H = T and CC(T) H = T. The UC-territory T is said to be minimal, denoted T = MUCT(G, Y), if no T ′ ⊂ T is a UC-territory.</formula><p>A minimal UC-territory for G w.r.t. Y can be constructed by extending a set of variables, starting from Y, and iteratively updating the set with the c-component and descendants of the set. More intuitively, it is the minimal subset of An(Y) G that is governed by unobserved confounders, where at least one target Y i is adjacent to an unobserved confounder. We have already described these concepts in the main part. Before connecting the notion of minimal UC-territory and interventional border to possibly Pareto-optimal minimal intervention sets, we require the following proposition: Proof. (Case B ⊆ X s ) Let x s ∈ D(X s ) be an intervention value. Then, by the third rule of do-calculus, it holds</p><formula xml:id="formula_49">E[Y i |do(X s = x s )] = E[Y i |do(X s ∩ (T ∪ B) = x s [T ∪ B])], 1 ≤ i ≤ m. Since X s ∩ (T ∪ B) = S, by setting s = x s [T ∪ B], it follows E[Y i |do(X s = x s )] = E[Y i |do(S = s)].</formula><p>(Case B ̸ ⊆ X s ) Let x s ∈ D(X s ) be an intervention value. We define B ′ = S\(X s ∩S) = S\(X s ∩(T∪B)) = B\(X s ∩B) and W = X s \(X s ∩ S) = X s \(X s ∩ (T ∪ B)). Moreover, let s * ∈ D(S) such that it dominates s ∈ D(S), which is given by s</p><formula xml:id="formula_50">[B ′ ] = E[B ′ |do(X s = x s )] and s[X s ] = x s [T ∪ B].</formula><p>If s is non-dominated, we set s * = s. Then, for all i = 1, . . . , m it holds</p><formula xml:id="formula_51">E[Y i |do(S = s * )] = E[Y i |do(X s ∩ (T ∪ B) = s * [X s ], B ′ = s * [B ′ ])] (14) ≤ E[Y i |do(X s ∩ (T ∪ B) = s[X s ], B ′ = s[B ′ ])] (15) = E[Y i |do(X s ∩ (T ∪ B) = s[X s ], B ′ = s[B ′ ], W = x s [W])] (16) = E[Y i |do(X s ∩ (T ∪ B) = s[X s ], W = x s [W]), B ′ = s[B ′ ]] (17) = E[Y i |do(X s ∩ (T ∪ B) = s[X s ], W = x s [W])] (18) = E[Y i |do(X s = x s )],<label>(19)</label></formula><p>where the inequality holds because s is (weakly) dominated by s * . Furthermore, the second and third equalities are derived through the third and second rules of do-calculus, respectively.</p><p>The following proposition is a building block for characterizing possibly Pareto-optimal minimal intervention sets via interventional borders. The proof is similar to the one given by <ref type="bibr" target="#b12">Lee &amp; Bareinboim (2018)</ref> Proposition B.2. IB(G, Y) is a possibly Pareto-optimal minimal intervention set.</p><p>Proof. The intuition of this proof is to construct an SCM, conforming to G, for which the single best strategy involves intervening on IB(G, Y). Let T and B denote MUCT(G, Y) and IB(G, Y), respectively. Every exogenous variable in U shall be a binary variable with its domain being {0, 1}. Let ⊕ denote the exclusive-or function and the logical OR operator.</p><p>(Case T = Y) In this case, B corresponds to the parents of Y. Therefore, no target variable Y i is confounded with an(Y i ) G via unobserved confounders. Define an SCM such that</p><p>• Each endogenous variable V ∈ V is influenced by an exogenous variable U V ∈ V;</p><p>• f Yi = u Yi ⊕ pa Yi with P (U Yi = 0) ≈ 1, for all i = 1, . . . , m;</p><p>• f X = ( u X ) ⊕ ( pa X ) for X ∈ X and P (U = 0) = 0.5 for every U ∈ U\(</p><formula xml:id="formula_52">m i=1 U Yi ).</formula><p>By the third rule of do-calculus and by taking conditional expectations, it holds</p><formula xml:id="formula_53">E[Y i |do(B = 0)] = E[Y i |do(pa(Y i ) G = 0)] (20) = E[Y i |do(pa(Y i ) G = 0), U Yi ̸ = 0]P (U Yi ̸ = 0) + E[Y i |do(pa(Y i ) G = 0), U Yi = 0]P (U Yi = 0) (21) ≈ 0 (22)</formula><p>for every 1 ≤ i ≤ m. Meanwhile, all other interventions yield expectations greater than or equal to 0.5 in at least one component. Therefore, B is a possibly Pareto-optimal minimal intervention set.</p><p>(Case T ⊂ Y) In this case, at least one target variable Y i has an unobserved confounder with its ancestors. As a first step, it will be shown that there exists an SCM, conforming to H = G[T ∪ B], where the intervention do(B = 0) is the single best strategy. To achieve this, we first define individual SCMs for each unobserved confounder in H[T], and merge them into a single SCM where do(B = 0) is indeed the best strategy. Let U ′ = {U j } k j=1 be the set of unobserved confounders in H[T].</p><p>Given U j ∈ U ′ , let B (j) and R (j) denote its two children. We define an SCM M j , where the graph structure is given by  and all bidirected edges, except for U j , are removed. In order to set the structural equations for variables in H j , the vertices will be labelled via colour coding: Let vertices in De B (j)  H \De R (j) H be labelled as blue, De R (j) H \De B (j) H as red, and De B (j) H ∩ De R (j) H as purple. All target variables are coloured as purple as well. Moreover, B (j) and R (j) shall perceive U j as a parent coloured as blue with value U j and red with value 1 -U j , respectively. The blue-, redand purple-coloured variables are set to 3 if any of their parents in B is not 0. Otherwise, their values are determined as follows. For every blue and red vertex, the associated structural equation returns the common value of its parents of the same colour and returns 3 if coloured parents' values are not homogeneous. For every purple vertex, its corresponding equation returns 2 if every blue, red and purple parent is 0,1, and 2, respectively, and returns 1 if 1,0,1, respectively.</p><formula xml:id="formula_54">H j = H De B (j) , R (j) H ∪ B ∩ pa De B (j) , R (j) H ,<label>(23)</label></formula><p>Next, the SCMs M 1 , . . . , M k will be merged into one single SCM, that conforms to H, and for which do(B = 0) is the single best intervention. Note that in M j all variables can be represented with just two bits. To construct a unified SCM, variables in T are represented with 2k bits, where M j takes the 2j -1 th and 2j th bits. Every target variable Y i is represented as a sequence of bits and binarised as follows. Y i is set to 0 if its 2j -1 th and 2j th bits are 00, 01 or 10 for every 1 ≤ j ≤ k, and 1 otherwise. Let P (U j = 1) = 0.5 for U j ∈ U ′ . Therefore, it holds Y i = 0 if do(B = 0) and Y i = 1 if do(B ̸ = 0). If any variable in T is intervened, then at least one SCM M j will be disrupted, resulting in an expectation larger than or equal to 0.5 for at least one target variable. In the multi-target setting, it may happen that some target variables do not occur in any of the M j 's. This happens if a target Y i has no parents in T, but only in B. For all such Y i 's, we set f Yi = u Yi ⊕ pa Yi with P (U Yi = 0) ≈ 1. As such, the newly constructed SCM enforces E[Y i |do(B = 0)] ≈ 0. Meanwhile, all other interventions yield expectations greater than or equal to 0.5 As a last step, the previously defined SCM for H = G[T ∪ B], will be extended to an SCM for G. However, we can ignore joint probability distributions for any exogenous variables only affecting endogenous variables outside of H. Setting structural equations for endogenous variables outside of H is redundant as well. For V ∈ An(Y) G \T, we define the structural equations as f V = ( u V ) ⊕ ( pa V ). For U ∈ U\U ′ , we set P (U = 0) = 0.5 if U 's child(ren) is disjoint to T, and P (U = 0) ≈ 1 otherwise. Note that do(B = 0) is still the single optimal intervention. Therefore, B is a possibly Pareto-optimal minimal intervention set.</p><p>In order to illustrate the construction of an SCM where do(IB(G, Y) = 0) is the single best strategy, consider Figure <ref type="figure" target="#fig_12">8</ref>, showing an exemplary graph and its colour-coded subgraphs, H 1 and H 2 , for each unobserved confounder. Figure <ref type="figure" target="#fig_13">9</ref> presents the associated values for M 1 and M 2 , as well as values for the target variables in the final SCM M. The next proposition generalizes the previous one.</p><p>Proposition 4.7. IB(G Xs , Y) is a possibly Pareto-optimal minimal intervention set for any X s ∈ P(X).</p><p>Proof. Let X s be an intervention set. Let us denote T = MUCT(G Xs , Y), B = IB(G Xs , Y) and T 0 = MUCT(G, Y). Using the strategy from Theorem 4.7, we construct an SCM for G[T ∪ B] while ignoring unobserved confounders between T and T 0 \T. Let U ′ be the set of such unobserved confounders. Now, the SCM needs to be modified to ensure that do(B = 0) is the single best intervention. Every U ∈ U ′ shall flip (i.e., 0 ← → 1) the value of its endogenous child in T whenever U = 1. Let P (U = 0) ≈ 1, so that it holds E[Y i |do(B = 0)] ≈ 0. Intervening on B ̸ = 0 or on any variable in T results in expectations around 0.5 or above.</p><p>Notably, Proposition 4.7 extends Proposition B.2 when X s ̸ = ∅. Note that, by iterating over all intervention sets X s ∈ P(X), we can discover possibly Pareto-optimal minimal intervention sets in a given graph. The following theorem is an extension of the main result by <ref type="bibr" target="#b12">Lee &amp; Bareinboim (2018)</ref> to the scenario where multiple target variables are present. It shows that the aforementioned strategy suffices to find not some, but all, such sets. (26</p><formula xml:id="formula_55">) = E[Y i |do(B ∩ X s = b[X s ], W = x s [W]), B ′ = b[B ′ ]] (27) = E[Y i |do(B ∩ X s = b[X s ], W = x s [W])] (28) = E[Y i |do(X s = x s )],<label>(29)</label></formula><p>where the inequality holds because b is (weakly) dominated by b * . Furthermore, the second and third equalities are derived through the third and second rules of do-calculus, repectively.</p><p>Theorem 4.8 provides a necessary and sufficient condition for a set of variables to be a possibly Pareto-optimal minimal intervention set. The proof of the theorem gives the following corollary:</p><p>Corollary 4.9. Let X s ∈ P(X) and X ′ s = IB(G Xs , Y). For any x s ∈ D(X s ) there exist x ′ s ∈ D(X ′ s ) such that it holds µ(X ′ s , x ′ s ) ≤ µ(X s , x s ), for all 1 ≤ i ≤ m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. SYNTHETIC-1</head><p>We introduce the first synthetic MO-CBO problem in our experimental study, referred to as SYNTHETIC-1, which is defined by the causal graph G and associated structural assignments presented in Figure <ref type="figure" target="#fig_0">10</ref>. The interventional domains are specified as D(X 1 ), D(X 2 ) = [-1, 2] and D(X 3 ), D(X 4 ) = [-1, 1]. Moreover, all exogenous variables follow the standard normal distribution, and there are no unobserved confounders.</p><formula xml:id="formula_56">Y 2 Y 1 X 1 X 3 X 2 X 4 X 1 = exp((X 3 -X 4 )/2) + U X 1 X 2 = ((X 3 -X 4 )/2) 3 + U X 2 X 3 = U X 3 X 4 = U X 4 Y 1 = (X 1 + X 2 ) 2 + U Y 1 Y 2 = (X 1 + X 2 -10) 2 + U Y 2 U X i , U Y i ∼ N (0, 1)</formula><p>Figure <ref type="figure" target="#fig_0">10</ref>. SYNTHETIC-1. An SCM consisting of four treatment and two output variables, depicted with grey and red nodes, respectively. There are no unobserved confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. SYNTHETIC-2</head><p>SYNTHETIC-2 is the next MO-CBO problem of our experimental study, defined by the causal graph G and associated structural equations in Figure <ref type="figure" target="#fig_0">11</ref>. The interventional domains are D(X 1 ) = [-2, 5], D(X 4 ) = [-4, 5] and D(X i ) = [0, 5] for i = 1, 2. Moreover, the exogenous variables U Xi , U Yi follow a Gaussian distribution, and there is an unobserved confounder U influencing the target variable Y 1 and its ancestor X 4 .</p><formula xml:id="formula_57">X 4 X 1 X 2 Y 1 Y 2 X 3 X 1 = X 4 /2 + U 2 X 1 X 2 = U 2 X 2 X 3 = U 2 X 3 X 4 = U + U 2 X 4 Y 1 = ln(1 + X 2 1 ) + 2 • X 2 2 -X 1 • X 2 • U/2 + U 2 Y 1 Y 2 = sin(X 2 2 ) -X 2 3 -X 2 • X 3 + 50 + U 2 Y 2</formula><p>U ∈ {-4, 4}, P (U = -4) = P (U = 4) = 0.5</p><formula xml:id="formula_58">U X i , U Y i ∼ N (0, 1)</formula><p>Figure <ref type="figure" target="#fig_0">11</ref>. SYNTHETIC-2. An SCM consisting of four treatment and two output variables, depicted with grey and red nodes, respectively. It includes an unobserved confounder, denoted via the dashed bi-directed edge, affecting one output and its ancestor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. HEALTH</head><p>The MO-CBO problem HEALTH is defined by the causal graph and structural equations in Figure <ref type="figure" target="#fig_1">12</ref>. This model originates from previous works of <ref type="bibr">Ferro et al. (2015)</ref>, and is based on real-world causal relationships.  <ref type="bibr" target="#b0">(Aglietti et al., 2020)</ref>, as well as for several of its variants (e.g. <ref type="bibr" target="#b8">Gultchin et al. (2023)</ref> and <ref type="bibr" target="#b2">Aglietti et al. (2023)</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. MO-CBO problem in healthcare. (a) Causal graph where red, grey, and orange nodes depict target, manipulative, and nonmanipulative variables, respectively. (b) The solution consists of interventions that yield optimal trade-offs between the targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the MO-CBO methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Two causal graphs with X = {X1, X2, X3, X4}, Y = {Y1, Y2}. (a) No unobserved confounders. (b) An unobserved confounder between X4 and Y1 depicted with the dashed bi-directed edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>In Figure 3 (a), there are no unobserved confounders and thus, it holds CC(Y) G = Y and De(Y) G = Y. It follows MUCT(G, Y) = {Y 1 , Y 2 } and IB(G, Y) = {X 1 , X 2 }. In Figure 3 (b), we construct the minimal UC-territory, starting from T = Y, as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Convergence to the causal Pareto front across all experiments, measured by the generational distance (upper row) and inverted generational distance (lower row). Lower values indicate better approximations of the ground-truth. The number of interventions refers to the count of intervention variables that were intervened upon. Shaded areas represent ± standard deviation across 10 random seeds.</figDesc><graphic coords="7,55.87,188.91,155.90,85.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a). This improvement stems from avoiding unnecessary interventions on X 3 and X 4 , allowing for more exploratory interventions on O G,Y within the same number of interventions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. SYNTHETIC-1. Causal Pareto front approximations. Our method offers a higher coverage of the ground-truth causal Pareto front.</figDesc><graphic coords="7,325.41,365.75,198.42,110.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. SYNTHETIC-2. Causal Pareto front approximations. By utilizing the causal relation, our approach tightly fits the groundtruth causal Pareto front.</figDesc><graphic coords="8,73.41,78.10,198.42,110.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure7. HEALTH. Causal Pareto front approximations on a realworld healthcare application. Our approach yields a better coverage of the ground-truth.</figDesc><graphic coords="8,73.41,559.73,198.42,110.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Definition 4.6 (Interventional border). Let T = MUCT(G, Y). Then, B = pa(T) G \T is called the interventional border for G w.r.t. Y, denoted as IB(G, Y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Proposition B.1. Let T be a minimal UC-territory and B an interventional border for G w.r.t. Y. Let X s ⊆ X be an intervention set and S = (T ∩ X s ) ∪ B. Then, for any x s ∈ D(X s ) there exists s ∈ D(S)such that E[Y i |do(S = s)] ≤ E[Y i |do(X s = x s )], for all i = 1, . . . , m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Original causal graph G and its color-coded subgraphs for each unobserved confounder.</figDesc><graphic coords="14,152.64,67.06,291.61,103.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Values for M1, M2 and M given X4 = X5 = 0. The target variables are shown as bit sequences, Y ′ 1 and Y ′ 2 , as well as binary values, Y1 and Y2.</figDesc><graphic coords="14,60.30,206.16,476.28,113.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Theorem 4.8. A set X s is a possibly Pareto-optimal minimal intervention set if and only if it holds IB(G Xs , Y) = X s .Proof. (If) This is a special case of Proposition 4.7.(Only if) Let X s be a minimal intervention set and x s ∈ D(X s ) an intervention value. DenoteT = MUCT(G Xs , Y), B = IB(G Xs , Y), T 0 = MUCT(G, Y) and B 0 = IB(G, Y). From Theorem B.1, we know that no POMIS intersects with An(B 0 ) G \B 0 and thus, it is possible to conclude X s ⊆ T 0 ∪ B 0 \Y. Note that it holds X s ⊆ An(B) G since otherwise it would follow X s ∩ T ̸ = ∅, which contradicts that X s is neither a descendant of some variable nor confounded in G Xs . Let B ′ = B\(X s ∩ B) and W = X s \(X s ∩ B). Moreover, we define an intervention value b * ∈ D(B) such that it dominates b ∈ D(B), which is given by b[B ′ ] = E[B ′ |do(X s = x s )] and b[X s ] = x s [B]. If b is non-dominated, we set b * = b.Then, for all i = 1, . . . , m, it holdsE[Y i |do(B = b * )] = E[Y i |do(B ∩ X s = b * [X s ], B ′ = b * [B ′ ])] (24) ≥ E[Y i |do(B ∩ X s = b[X s ], B ′ = b[B ′ ])] (25) = E[Y i |do(B ∩ X s = b[X s ], B ′ = b[B ′ ], W = x s [W])]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>It captures prostate-specificantigen (PSA) levels in causal relation to its risk factors, such as BMI, calorie intake (CI) and aspirin usage. The variable Aspirin indicates the daily aspirin regimen while Statin denotes a subject' statin medication. Additionally, PSA represents the total antigen level circulating in a subject's blood, measured in ng/mL. For patients sensitive to Statin medications, the aim is to determine how to manipulate relevant risk factors to minimize both Statin and PSA. To this end, we treat both Statin and PSA as target variables. The treatment variables include BMI, Weight, CI, and Aspirin usage with interventional domains D(BMI) = [20, 30], D(Weight) = [50, 100], D(CI) = [-100, 100] and D(Aspirin) = [0, 1]. We choose to consider a specific age groups of interest, and define U age as a Gaussian random variable with mean 65 and standard deviation 1, focusing on individuals close to the age of 65. The single-objective version of HEALTH, aiming to minimize only PSA, has previously been used to demonstrate the applicability of CBO</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The full implementation of our algorithm is available at https://github.com/ShriyaBhatija/MO-CBO.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This work was financially supported by the <rs type="funder">Bavarian State Ministry of Education, Science and the Arts</rs>, the <rs type="affiliation">Alan Turing Institute's studentship scheme</rs>, and the <rs type="funder">Engineering and Physical Sciences Research Council</rs> [<rs type="grantNumber">EP/S023917/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GGK8GK7">
					<idno type="grant-number">EP/S023917/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HEALTH. An SCM with between variables such as age, BMI, aspirin and statin usage, and their effects on PSA levels <ref type="bibr" target="#b8">(Gultchin et al., 2023)</ref>. U(•, •) denotes a uniform distribution and tN (a, b) a standard Gaussian distribution truncated between a and b. Red, grey, and orange nodes depict target, manipulative, and non-manipulative variables, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal bayesian optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paleyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic causal bayesian optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Damoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10549" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constrained causal Bayesian optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="304" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-aware search framework for multiobjective bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jayakodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Doppa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10044" to="10052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Causal entropy optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Branchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Damoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>The 26th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8586" to="8605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast calculation of multiobjective probability of improvement and expected improvement criteria for pareto optimization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Couckuyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deschrijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dhaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="page" from="575" to="594" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2187" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Use of statins and serum levels of prostate specific antigen</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lunet</surname></persName>
		</author>
		<imprint>
			<publisher>Acta Urológica Portuguesa</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Functional causal Bayesian optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gultchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictive entropy search for efficient global optimization of black-box functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henrández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="918" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="50" to="66" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diversity-guided multi-objective bayesian optimization with batch evaluations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Konakovic Lukovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17708" to="17720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural causal bandits: Where to intervene?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural causal bandits with non-manipulable variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4164" to="4172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian Processes in Machine Learning</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causal bayesian optimization via exogenous distribution learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using the averaged hausdorff distance as a performance measure in evolutionary multiobjective optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Esquivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="504" to="522" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of bayesian optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model-based causal bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makarova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial causal bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makarova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the testable implications of causal models with hidden variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Eighteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal elicitation for bayesian optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Causal Inference Workshop at UAI 2024</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expensive multiobjective optimization by moea/d with gaussian process model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Virginas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="456" to="474" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="page" from="257" to="271" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active learning for multi-objective optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sergent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Püschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="462" to="470" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
