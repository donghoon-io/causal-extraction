<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Datasheet for Datasets: An Evaluation Guide for Real-World Data Analysis and Data Collection Design Using Bayesian Networks</title>
				<funder ref="#_Vutmd6V">
					<orgName type="full">UK EPSRC</orgName>
				</funder>
				<funder ref="#_GHxSqvV">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Surgo Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-04-14">14 April 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bradley</forename><surname>Butcher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Predictive Analytics Lab (PAL)</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Surgo Ventures</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Predictive Analytics Lab (PAL)</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Predictive Analytics Lab (PAL)</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sema</forename><forename type="middle">K</forename><surname>Sgaier</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Surgo Ventures</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Harvard T. H</orgName>
								<orgName type="institution" key="instit2">Chan School of Public Health</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Global Health</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grace</forename><surname>Charles</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Surgo Ventures</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Novi</forename><surname>Quadrianto</surname></persName>
							<email>n.quadrianto@sussex.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Predictive Analytics Lab (PAL)</orgName>
								<orgName type="institution">University of Sussex</orgName>
								<address>
									<settlement>Brighton</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nandini</forename><surname>Ramanan</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Heinrich Hertz Institute (FHG)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Datasheet for Datasets: An Evaluation Guide for Real-World Data Analysis and Data Collection Design Using Bayesian Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-14">14 April 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3389/frai.2021.612551</idno>
					<note type="submission">This article was submitted to Machine Learning and Artificial Intelligence, a section of the journal Frontiers in Artificial Intelligence Received: 30 September 2020 Accepted: 11 February 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>bayesian network</term>
					<term>causality</term>
					<term>causal modeling</term>
					<term>lower middle income country</term>
					<term>machine learning</term>
					<term>big data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Developing data-driven solutions that address real-world problems requires understanding of these problems' causes and how their interaction affects the outcome-often with only observational data. Causal Bayesian Networks (BN) have been proposed as a powerful method for discovering and representing the causal relationships from observational data as a Directed Acyclic Graph (DAG). BNs could be especially useful for research in global health in Lower and Middle Income Countries, where there is an increasing abundance of observational data that could be harnessed for policy making, program evaluation, and intervention design. However, BNs have not been widely adopted by global health professionals, and in real-world applications, confidence in the results of BNs generally remains inadequate. This is partially due to the inability to validate against some ground truth, as the true DAG is not available. This is especially problematic if a learned DAG conflicts with pre-existing domain doctrine. Here we conceptualize and demonstrate an idea of a "Causal Datasheet" that could approximate and document BN performance expectations for a given dataset, aiming to provide confidence and sample size requirements to practitioners. To generate results for such a Causal Datasheet, a tool was developed which can generate synthetic Bayesian networks and their associated synthetic datasets to mimic real-world datasets. The results given by well-known structure learning algorithms and a novel implementation of the OrderMCMC method using the Quotient Normalized Maximum Likelihood score were recorded. These results were used to populate the Causal Datasheet, and recommendations could be made dependent on whether expected performance met user-defined thresholds. We present our experience in the creation of Causal Datasheets to aid analysis decisions at different stages of the research process. First, one was deployed to help determine the appropriate sample size of a planned study of sexual and reproductive health in Madhya Pradesh, India. Second, a datasheet was created to estimate the performance of an existing maternal health survey we conducted in Uttar Pradesh, India. Third, we validated generated performance estimates and investigated current limitations on the well-known ALARM dataset. Our experience demonstrates the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>utility of the Causal Datasheet, which can help global health practitioners gain more confidence when applying BNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>To meet ambitious global health and development goals in lower and middle income countries (LMICs), policy decisions have been increasingly reliant on data-driven approaches to provide necessary insights. This has spawned numerous programs ranging from specific subjects at the sub-national and national level (e.g., Community Behavior Tracking Survey in Uttar Pradesh, India, and the Social And Living Standards Measurement in Pakistan) to broad health topics with multinational participation (e.g., the Multiple Indicator Cluster Surveys developed by the United Nations Children's Fund, and the USAID-backed Demographic and Health Survey) <ref type="bibr" target="#b11">(Croft et al., 2018;</ref><ref type="bibr" target="#b30">Khan and Hancioglu, 2019;</ref><ref type="bibr" target="#b27">Huang et al., 2020;</ref><ref type="bibr">Pakistan Bureau of Statistics, 2020)</ref>. These programs have a mandate to collect and disseminate accurate and populationrepresentative health, nutrition, and population data in the developing world. These surveys allow governments and international agencies to monitor trends across health program areas and set priorities for health policy, interventions, and program funding <ref type="bibr" target="#b15">(Fabic et al., 2012)</ref>. As a result, there has been an explosion of data being generated that has the potential to be used to not only monitor/evaluate the status quo but to inform health intervention design.</p><p>Global health and development problems are often complex. An understanding of these complexities is often needed to get the right intervention to the right person, at the right time and place-also known as a Precision Public Health approach (Desmond-Hellmann, 2016; <ref type="bibr" target="#b31">Khoury et al., 2016;</ref><ref type="bibr" target="#b10">Chowkwanyun et al., 2018)</ref>. Traditionally, for informing intervention design, randomized controlled trials (RCT) remain the gold standard. However, due to cost, lack of infrastructure, and other practical reasons, RCTs are not always possible in LMICs. As a result, many of the available data collected are observational only and limited in scope. Without an RCT, quantifying which variables are the proximate causes of an outcome or determining causes and effects for specific set of variables remains a challenge for global health practitioners. Moreover, RCTs are by design conducted with the intent to test a narrow set of hypotheses, not to explore unknown causal structures -a potential missed opportunity to target public health solutions more precisely.</p><p>Causal inference and discovery approaches such as causal Bayesian Network (BN) can fill this void. BNs readily deal with observational data, can utilize numerous algorithms to facilitate automatic causal discovery, allow for expert-specified constraints, and can infer the causal effects of hypothetical interventions <ref type="bibr" target="#b44">(Pearl, 1995;</ref><ref type="bibr" target="#b3">Arora et al., 2019;</ref><ref type="bibr" target="#b20">Glymour et al., 2019)</ref>. Despite causal Bayesian Network's many offerings, we have not seen a wide adoption in real-world problems <ref type="bibr" target="#b3">(Arora et al., 2019;</ref><ref type="bibr" target="#b36">Kyrimi et al., 2020;</ref><ref type="bibr" target="#b58">Sgaier et al., 2020)</ref>. We have found that validating the structure, parameterization, predictive accuracy, and generalizability of BN presents a significant hurdle and is subject to considerable debate and interpretation when applied to data with real-world complexity. Our inability to communicate uncertainty in structure learning algorithm performance for specific datasets can call entire models into question <ref type="bibr" target="#b65">(van der Bles et al., 2019)</ref>. Generally, practitioners using BNs must resort to domain expertize to validate model structure, if they do not forgo validation entirely <ref type="bibr" target="#b0">(Aguilera et al., 2011;</ref><ref type="bibr" target="#b39">Lewis and McCormick, 2012;</ref><ref type="bibr" target="#b42">Moglia et al., 2018)</ref>. This makes BN model results especially difficult to defend when they, even if just in part, contradict previous domain beliefs or doctrines. Thus, BN results are often presented as a proof-ofconcept of techniques to show that the method can recover insights already known rather than as an actionable model for discovery, change, or intervention <ref type="bibr" target="#b39">(Lewis and McCormick, 2012;</ref><ref type="bibr" target="#b42">Moglia et al., 2018;</ref><ref type="bibr" target="#b50">Raqeujo-Castro et al., 2018)</ref>.</p><p>The problem of not knowing how well machine learning algorithms will perform in real-world conditions is not restricted to causal discovery and inference and has been subject to a broader debate. One proposed solution is adopting the standard "datasheet" practice of constructing and accompanying any given dataset with a full description of the data, its collection context, operating characteristics (i.e., the characteristics of the data to which a machine learning algorithms is applied), and test results (i.e., the expected performance of the machine learning algorithm) <ref type="bibr" target="#b18">(Gebru et al., 2018)</ref>. Measuring the expected causal discovery and inference performance and their uncertainties for any given dataset is, however, not straightforward. First, it is not clear what performance metrics should be used to measure BN algorithms' ability to recover the ground truth causal structure when the ground truth is unknown. In addition, such data may not include the appropriate variables to establish causal or interventional sufficiency, can have incomplete observations, and may be imbalanced <ref type="bibr" target="#b63">(Spirtes et al., 2000;</ref><ref type="bibr" target="#b45">Pearl, 2009;</ref><ref type="bibr" target="#b34">Kleinberg and Hripcsak, 2011;</ref><ref type="bibr" target="#b46">Peters et al., 2017)</ref>. Lastly, the sample size of a dataset may be insufficient to support BN analyses <ref type="bibr" target="#b67">(Wang and Gelfand, 2002)</ref>. Perhaps due to the data challenges mentioned above, the evaluation of novel BN algorithms has been largely based on standard synthetic datasets such as ALARM, Insurance, Child and others <ref type="bibr" target="#b5">(Beinlich et al., 1989;</ref><ref type="bibr" target="#b12">Dawid, 1992;</ref><ref type="bibr" target="#b6">Binder et al., 1997;</ref><ref type="bibr" target="#b55">Scutari, 2009)</ref>, which can have vastly different characteristics compared to real-world data at hand. One suggested method for ranking algorithms' performance is to assume the intersection of the structures found by a collection of algorithms as the partial ground truth as in the Intersection-Validation method by <ref type="bibr" target="#b66">Viinikka et al. (2018)</ref>. However, the Intersection-Validation method will often neglect to consider the most complex relationships, and while it provides relative sample size requirements for each algorithm, it cannot directly inform the data collection process. We face the following quandary: with real-world data we lack the ground truth against which to evaluate the modeling algorithms, and with synthetic data we lack the complexity and limitations that are typically imposed in real-world circumstances <ref type="bibr" target="#b19">(Gentzel et al., 2019)</ref>.</p><p>To solve this quandary and to empower practitioners to estimate uncertainty levels around the causal structures learned under the typical contexts and constraints applicable to their analytical problem of interest, we propose an approach to attach two types of causal extension to such datasheet proposed by <ref type="bibr" target="#b18">Gebru et al. (2018)</ref> to 1) inform study design at the data collection stage to enable subsequent causal discovery analysis similar to, in spirit, conducting power analysis before sample size is determined, and 2) describe expected causal discovery and inference algorithm performance and corresponding uncertainty when presented an existing dataset. The key idea is to generate synthetic data with a spectrum of properties that mimic the existing or projected realworld data. We call our instantiation of this capability the 'Causal Datasheet Generation Tool', or CDG-T.</p><p>In this work, our goal is to provide further confidence in BN results from the perspective of practitioners' needs. BNs are introduced in 2.1 of the Materials and Methods Section. In Section 2.2 we briefly look at pertinent related work. In Section 2.3, we introduce the approach taken in generating causal datasheets, including a brief discussion the assumptions that are made. Following this in Sections 2.4-2.7 we define the data characteristics used to generate synthetic data, what structure learning algorithms were explored, definitions of the performance metrics used in the datasheets, and the two datasheet usage scenarios. In Section 3, Results, we illustrate the usage of three example datasheets. First, to inform data collection design in an LMIC setting, we provide an example on how a Causal Datasheet was used in planning of a Sexual Reproductive Health survey in Madhya Pradesh, India, where the performance value is computed over a range of potential variables and sample sizes. Next, for evaluating data suitability for BN we provide two example Causal Datasheets for existing data evaluation: one example for an existing dataset in the global development domain (a survey about Reproductive Maternal Neonatal Child Health (RMNCH) that we administered in Uttar Pradesh, India), and another generated for the well-known ALARM dataset <ref type="bibr" target="#b5">(Beinlich et al., 1989)</ref>. Lastly, we note the implications and future research directions in the Discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Bayesian Network</head><p>A Bayesian network (G, Θ) for a set of variables X consists of two components: a directed acyclic graph (DAG), and a set of parameters Θ. The DAG (V, E) of a BN encodes the statistical dependence among the set of variables X by means of the set of edges E which connect nodes V (Figure <ref type="figure" target="#fig_0">1</ref>). Each node V i ∈ V corresponds to one variable X i ∈ X.</p><p>Conversely, the absence of an edge between variables suggests a statistical (conditional) independence. Thus, a BN induces the factorization:</p><formula xml:id="formula_0">P(X|G, Θ) D i 1 P X i Π Xi , Θ Xi</formula><p>where the global distribution P(X|G, Θ) factorizes into a set of local distributions; one for each X i with parameters Θ X i , conditional on its parents Π X i .</p><p>Discrete BNs assume that a variable X i is distributed multinomially conditioned on a configuration of its parents (X i k Π Xi ) ∼ Mul(π ik|j ), where π ik|j P(X i k Π Xi j) is the probability when X i k conditioned on the jth value of the possible parent combinations. These discrete conditional distributions can be represented as conditional probability tables (CPTs) <ref type="bibr" target="#b26">(Heckerman et al., 1995)</ref>.</p><p>A factorization can represent multiple DAGs, this set of DAGs is known as the equivalence class and are said to be Markov equivalent. BNs of the same equivalence class share the same skeleton: the underlying undirected graph, and V-structures. The skeleton of a DAG is the undirected graph resulted by ignoring every edge's directionality. A V-structure is an unshielded common effect; that is, for the pattern of edges A → C ← B, A and B are independent (Figure <ref type="figure" target="#fig_1">2</ref>). In this example, by having two edges pointing at it, C is said to have an in-degree of 2; A and B are the parent nodes, and C is the child node. The combination of both skeleton and V-structures is known as a complete partially directed acyclic graph, or CPDAG, and represents the equivalence class of DAGs for a factorization. Thus, we believe that how well structural learning algorithms recover the ground truth from observational data should include both skeleton and V-structure recovery.</p><p>In order for a Bayesian network to be considered causal, the parents of each of the nodes must be its direct causes. A node A is considered a direct cause of C if varying the value of A, while all other nodes remain unchanged, effects the distribution of C i.e.,:</p><formula xml:id="formula_1">P(C|do(A a 1 ), do(B b 1 )) ≠ P(C|do(A a 2 ), do(B b 1 ))</formula><p>Where P(C|do(A a 1 )) is the interventional distribution; the distribution of C given an intervention on A that sets it to a 1 . Additionally, the causal Markov assumption must be made to treat a BN as causal; it is assumed all common causes are present within G, with each node being independent conditioned on its direct causes <ref type="bibr" target="#b25">(Hausman and Woodward, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>When learning a Bayesian network we are attempting to model the underlying generative model behind a given dataset. Performance of causal discovery algorithms is a function of both the distance to the underlying causal structure, as well as the distance to the true parameters. However, measuring the distance is generally not possible as the ground truth is unavailable. One strategy to obtain some expected level of Bayesian network performance, in the absence of any ground truth to compare against, is to construct a proxy of the groundtruth. Conceptually, this is similar to the previously mentioned intersection-validation method. In this method a proxy agreement graph is constructed by taking the intersection of the output from many structure learning algorithms <ref type="bibr" target="#b66">(Viinikka et al., 2018)</ref>. These algorithms are then ranked by how many samples it takes to reach this agreement graph. This forms a dependence between the selection of algorithms and the proxy, and by extension the ranking. Forming a proxy independent of algorithm choice is desirable. Synthetic data is system-generated data which is not obtained by any direct measurement. Generally, the goal of this generated data is to mimic some real-world data, given some user-defined parameters. One can create synthetic data by two means; by modification or generation. Data can be modified, normally through anonymization, to create a synthetic dataset. Alternatively, generative models such as Generative Adversarial Networks, Variational Auto-encoders or Normalizing Flows can be sampled from to create the data <ref type="bibr" target="#b33">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b22">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b51">Rezende and Mohamed, 2015)</ref>. In this study, we required a generative model which could be explicitly represented as a BN, in order to ascertain how well BN learning procedures performed. As BNs are generative models themselves, our goal is to directly create Bayesian networks with similar properties to the underlying generative model behind the real-world processes.</p><p>Previous studies have used synthetic Bayesian networks to evaluate performance of structure learning algorithms <ref type="bibr" target="#b64">(Tasaki et al., 2015;</ref><ref type="bibr" target="#b2">Andrews et al., 2018;</ref><ref type="bibr" target="#b17">Gadetsky et al., 2020;</ref><ref type="bibr" target="#b71">Zhang et al., 2020;</ref><ref type="bibr" target="#b21">Gogoshin et al., 2020)</ref> (Table <ref type="table" target="#tab_0">1</ref>). These are often limited in terms of user-controllable parameters, with structures being sampled uniformly from the space of DAGs, or limited in terms of variation in topology. Other studies use standard benchmark datasets <ref type="bibr" target="#b57">(Scutari et al., 2019;</ref><ref type="bibr" target="#b49">Ramanan and Natarajan, 2020)</ref>. A flexible synthetic generation system would allow the user to specify many parameters which influence the BN generation, in order to match a given real dataset as closely as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Causal Datasheet Generation Tool</head><p>There are two primary goals of the Causal Datasheet. The first goal is to provide some expectation of performance given the basic, observable, characteristics of a dataset. The second goal is to provide guidance as to how many samples will be required in order to meet desired performance levels. The proof-of-concept approach we employ is described in the subsequent Section, followed by an outline of the assumptions made using this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Approach</head><p>Our general approach is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. In order to provide a performance estimate of structure and parameter learning for a given real dataset, we generate a set of synthetic Bayesian networks to act as a proxy for real data. Because we will have access to the ground-truths of these synthetic networks, we can calculate the performance of the structure learning, parameter learning, and any downstream estimates. Performance estimates will only be accurate so long as the generated synthetic datasets are similar enough to the given real dataset. We therefore generate synthetic BNs, and corresponding datasets, with matching observable characteristics of the real dataset. These characteristics include number of samples, variables, and levels. This corresponds to box I1 of Figure <ref type="figure" target="#fig_2">3</ref>. In addition to the observable characteristics, there  A small Python library which can generate and sample synthetic BNs was developed, and can be found at: <ref type="url" target="https://pypi.org/project/BayNet/">https://pypi.org/project/ BayNet/</ref>. An example Jupyter notebook of how to generate a datasheet can be found at: <ref type="url" target="https://github.com/predictiveanalytics-lab/datasheet_generation.The">https://github.com/predictiveanalytics-lab/datasheet_generation.The</ref> BN and data generation, as well as the learning and evaluation process is described in Algorithm 1, corresponding to boxes A1-A4 in Figure <ref type="figure" target="#fig_2">3</ref>. Box A1 concerns the generation of the synthetic data, details of which can be found in Section 2.4. Box A2 and A3 concern learning a BN using the synthetic data, details of the structure learning algorithms used can be found in Section 2.5. Box A4 concerns the evaluation of the learned models, the metrics used can be found in Section 2.6.</p><p>The synthetic BN generation is performed T times per set of data characteristics, where T is a user-defined number of trials, in order to capture performance variation. While we attempt to capture as much of the space of possible BNs as we can, the number of experiments that can be performed are limited by finite computation resources. In our experiments, we set T to 10 to balance the total computation time for generating the data sets and learning the model spanning across configurations and to capture result uncertainty simply due to random seeding.</p><p>We envision that this extensive evaluation is synthesized into a digestible Causal Datasheet for Existing Datasets or Causal Datasheet for Data Collection format (Figure <ref type="figure" target="#fig_2">3</ref>, Box O1). Domain experts can then assess whether this level of performance is sufficient for a particular application. Due to the flexibility of this system, we can not only construct proxies of existing datasets, but of datasets we plan to collect. In this manner, data collection can be designed around desired performance of our models. This concept is extensible to other systems with the capability to produce and evaluate synthetic data sets and structural learning algorithms. As the capability and flexibility of these systems increase, so too will the accuracy of the estimates within the Causal Datasheet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Assumptions</head><p>A number of assumptions are made in the generation of this synthetic data: A) we introduce no latent confounders. In order for a BN to be considered causal, one must assume there are no confounders absent. There are potentially complex repercussions of having confounders latent in a BN, but this is currently not examined. B) parameters are generated from a Dirichlet distribution assuming the α vector is uniform. The implications of this simplification are, given sufficient samples, the mean of the distributions drawn will be uniform. Therefore, generally, the marginal distributions of all nodes in the synthetic BNs will be uniform-this can make the BN learning process easier, potentially inflating performance estimates for cases where variables are highly imbalanced. This is further discussed in Section 4. Initial work has been performed to go beyond this simplification, and can be found in the supplementary material. C) it is assumed that the unobervable characteristics of a real dataset have been appropriately selected. We have assumed the used structure types can sufficiently represent the underlying DAG of a given real dataset. In the case these are incorrectly set, this could lead to incorrect performance estimates. D) we assume that synthetic data can sufficiently mimic a real dataset. Initial work has been performed to guide whether Assumptions C and D hold, and can be found in the supplementary material. These assumptions do not invalidate the concept of the Causal Datasheet, but must be kept in mind when interpreting results of a datasheet generated using CDG-T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dataset Characteristics</head><p>To study the variability of structural learning performance with different synthetic data properties, we defined two classes of dataset characteristics that can be varied to produce a distribution of synthetic data: observable and non-observable (Table <ref type="table" target="#tab_2">2</ref>).</p><p>Observable characteristics are those which the designer of the dataset has control and can be easily calculated (e.g., sample size and number of variables). Non-observable characteristics are properties of the underlying truth (e.g., degree distribution, type of structure, or imbalance). Non-observable characteristics can be estimated, but doing so introduces modeling assumptions. When evaluating a real-world dataset in practice, one could look up a Causal Datasheet with corresponding observable characteristics, to estimate performance uncertainty from the unobservable characteristics. Number of samples, number of variables, and average variable levels are straightforward; we describe the other characteristics below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Structure Type</head><p>We make use of five existing graph generation algorithms when creating synthetic Bayesian networks (Figure <ref type="figure">4</ref>):</p><p>• Forest Fire: A growing network model which resembles how forest fires spread to nearby nodes <ref type="bibr" target="#b38">(Leskovec et al., 2005)</ref>. • IC-DAG: A graph generation algorithm which samples uniformly from the set of DAGs <ref type="bibr" target="#b28">(Ide and Cozman, 2002)</ref>. • Barabasi-Albert An evolving graph generation algorithm which adds edges to a new node dependent on current indegree <ref type="bibr" target="#b4">(Barabási and Albert, 1999</ref>). • Waxman: Nodes are placed uniformly in a rectangular domain <ref type="bibr" target="#b69">(Waxman, 1988</ref>). • Small-World: A type of graph where most nodes are not direct neighbors, but the shortest path between any two nodes is generally low <ref type="bibr" target="#b68">(Watts and Strogatz, 1998)</ref>.</p><p>BNs decompose into a set of local distributions P(X i Π X i ). This property is utilized in structure learning algorithms; local scores or conditional independence tests are used to test a parent → child relationship. The difficulty in correctly identifying an edge is a function of the data-to-parameter ratio. The DAG has a direct effect on the number of parameters, as the higher the in-degree for a node, the more parameters it will have: (r i -1)q i , where q i is the product of number of parent levels. It follows then, that the DAG influences the difficulty of learning a BN by its distribution of node in-degrees. Having some control over this distribution is essential in order to complete a comprehensive evaluation. <ref type="bibr" target="#b21">Gogoshin et al. (2020)</ref>, <ref type="bibr" target="#b2">Andrews et al. (2018), and</ref><ref type="bibr" target="#b71">Zhang et al. (2020)</ref> generate random networks with caps on maximum in-degree (Table <ref type="table" target="#tab_0">1</ref>). <ref type="bibr" target="#b64">Tasaki et al. (2015)</ref> use graph generation algorithms in order to create synthetic BNs, but limit their use to a single type of topology. Here, we make use of multiple graph generation algorithms, allowing us to model many different realistic distributions of in-degrees, without having to specify them explicitly. Knowledge of what type of graphs are present in a particular domain can be incorporated by stratifying the structure type. New structure types can also be added in the case where current structure types do not sufficiency represent the topology of a specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Maximum In-degree</head><p>Maximum in-degree is the parameter which controls the cap on the number of parents each node can have within a network. We have found structures with high in-degrees have a major effect on the performance of structure learning algorithms. Having a parameter which can control this is crucial given prior knowledge about maximum in-degree is available. Unlike other studies, in the absence of domain knowledge we did not specifically cap the maximum in-degree in addition to what structural type would implicitly generate as previously mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Conditional Probability Table Imbalance: α</head><p>The CPT for each node X i in the synthetic Bayesian network are populated from parameters drawn from a Dirichlet distribution, with α i α qiri . Where r i is the cardinality of variable X i , and q i is the product of cardinalities of parent set of X i . A hyper-parameter, α, controls the over-all conditional imbalance, and thus connection strength, in the network. Consider an example of populating a CPT for a node with three levels and one parent. In the case the parent has two levels, two multinomial distributions must be drawn. One for each parent configuration. For example, using an α value of 12, applying the normalization: α i 12 3•2 2. Given this low value, the Dirichlet will likely draw a two low entropy multinomial distributions such as [.8, .1, .1] and [.2, .7, .1]. As these distributions are substantially different from one another, the relationship between the parent and child should be relatively easy to observe once data has been drawn conditioned on the parent value. Note that α i is a vector α i ∈ R + , and the simplifying assumption has been made that it is uniform. Thus, the α controls the imbalance of the distributions drawn, but does not enforce any consistency as to which values are imbalanced across the conditional distribution. While we currently Algorithm 1: CDG-T Overview. Given:</p><p>• The observable properties of a real dataset (sample size n, number of variables p, average levels l)</p><p>• The unobservable estimates of a real dataset (structure type t, CPT imbalance α)</p><p>• The number of trials T • Hyper-parameters of structure learning algorithms For i 1 → T 1. Generate a DAG G with variables V where |V| p according to user-specified structure-types 2. Sample number of levels for each variable from U(2, M) where l (2 + M)/2 3. Populate the parameters of the BN B (G, Θ) with multinomial distributions θ Vi drawn from a Dirichlet distribution with α Vi α qV i •rV i for each variable V i ∈ V 4. Draw s synthetic samples from synthetically created BN B to create synthetic dataset X S 5. Learn a DAG using a set of structure learning algorithms using the synthetic samples 6. Learn the parameters with maximum likelihood estimation (MLE) using the learned DAG and synthetic samples 7. Record the structural performance (skeleton/v-structure precision, recall) and interventional performance (PCOR) present results from a uniform α estimate within the datasheets, we have completed preliminary work to estimate a non-uniform α from any given dataset. Descriptions of this method, as well as results, can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Structure Learning Algorithms</head><p>The causal discovery step of training a causal Bayesian network is performed by structure learning algorithms <ref type="bibr" target="#b20">(Glymour et al., 2019)</ref>. In the current iteration of the Causal Datasheet three state-of-the-art structure learning algorithms are used. Each of the algorithms represents an example of constraint-based, scorebased, and hybrid class of structural learning algorithms:</p><p>• Peter-Clark (PC): A Constraint-based algorithm. This algorithm starts with the graph fully connected, then uses (conditional) independence tests to iteratively prune edges.</p><p>The chi-square test with mutual information is used <ref type="bibr" target="#b63">(Spirtes et al., 2000)</ref>. • Greedy Equivalence Search (GES): A greedy Score-based algorithm, which goes through phases of adding then removing edges where doing so increases the score, alternating until the score no longer improves <ref type="bibr" target="#b9">(Chickering, 1995)</ref>. A commonly used information theoretic score, the Bayesian Information Criterion (BIC), is used <ref type="bibr" target="#b54">(Schwarz, 1978</ref>). • OrderMCMC: A Hybrid algorithm. This optimizes a score in the space of topological node orderings, rather than DAGs. This is an implementation of Markov Chain Monte Carlo method in the topological node ordering space <ref type="bibr" target="#b16">(Friedman and Koller, 2003)</ref>, based on modifications proposed by <ref type="bibr" target="#b35">Kuipers et al. (2018)</ref>. Each order is scored by summing the maximum score for each node, out of parent sets falling in the intersection of those permitted by the order and those in a candidate set. This set is typically initialized using a constraint-based method, such as PC, however we instead use pairwise mutual information in an effort to decouple the algorithm's performance from that of PC. This candidate parent set is greedily expanded to improve recall. This combination of constrained global search with greedy means performance should be lower bounded by GES, but at much higher computation cost. For consistency with the GES algorithm the BIC score is used. We were also interested in a novel application of OrderMCMC using a recently developed score-the Quotient Normalized Maximum Likelihood (qNML) score; we included this to demonstrate differences due to choice of score <ref type="bibr" target="#b61">(Silander et al., 2018)</ref>.</p><p>These algorithms were selected in order to include one of each constraint-based, score-based, and hybrid structure learning algorithms. PC and GES were used as the constraint-based and score-based representatives as they are easily available algorithms, in terms of implementation <ref type="bibr" target="#b55">(Scutari, 2009;</ref><ref type="bibr" target="#b29">Kalainathan and Goudet, 2019)</ref>. OrderMCMC was used as the hybrid representative as it is the algorithm which is currently employed for the real-world examples in Section 3. The OrderMCMC implementation is currently proprietary; access can be granted from the authors on a per-request basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Score Functions</head><p>Score-based and hybrid structure learning algorithms' performance is highly dependent on choice of score function. While equivalent in the infinite data limit, the qNML and BIC scores differ significantly for small sample sizes <ref type="bibr" target="#b61">(Silander et al., 2018)</ref>. This is due to the difference in penalization; while both are based on the minimum description length (MDL) principle, they take differing approaches. The BIC takes a Bayesian approach to the penalization, using the number of parameters scaled by the log of the sample size <ref type="bibr" target="#b54">(Schwarz, 1978)</ref>; while the qNML is based on the NML (Normalized Maximum Likelihood), an exact formulation of the minimax code length regret <ref type="bibr" target="#b23">(Grünwald and Grunwald, 2007)</ref>. Both are score equivalent and free of tuning hyper-parameters.</p><p>Another score function prominent in literature is Bayesian Dirichlet equivalent uniform (BDeu) <ref type="bibr" target="#b7">(Buntine, 1991)</ref>, however it has been shown to be highly sensitive to its hyper-parameter, the effective sample size <ref type="bibr" target="#b60">(Silander et al., 2012)</ref>-it is therefore impossible to give a reasonable estimate of performance, thus making it unsuitable for use in a datasheet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Structural Performance</head><p>Discovery of the entire causal topology through structure learning algorithms is an appealing feature of BNs in global health settings. This sets it apart from simply testing causality between a candidate cause and the outcome interest (bivariate causal discovery), where a practitioner would be ignorant of the interaction of the system as a whole. To empirically evaluate structure learning methods with different synthetic characteristics, we measure the precision and recall of the learned structure with respect to the ground truth structure. This allowed us to separate errors into learning false edges v not identifying true edges, as opposed to quantifying aggregated structural distance measures (e.g., Structural Hamming Distance (de Jongh and Druzdzel, 2009)). For the same reason, we did not include a summarization of precision and recall, such as the F1 score. Having a clear separation of precision and recall is important in decision making; situations may arise where practitioners must favor one over the other, and the two are often a trade-off.</p><p>Structure learning algorithms estimate a DAG up to the equivalence class (CPDAG). Therefore, we do not calculate the precision and recall with respect to the true DAG, but the learned skeleton and V-structures to their ground truth counterparts. Evaluating with respect to the DAG, while helpful from an easeof-interpretability standpoint, introduces randomly directed edges correlated to the infrequency of V-structures. This correlation can lead to misleading hypothesis when performing experiments across many different types of structure with varying prevalence of V-structures.</p><p>Precision and recall for the skeleton and V-structures of a structure are calculated in the standard manner:</p><formula xml:id="formula_2">Precision TP TP + FP , Recall TP TP + FN</formula><p>In the case of the skeleton true positives (TPs) are the number of undirected edges which are in both the true and learnt structure. False positives (FPs) are the number of undirected arcs which are in the learnt, yet not present in the true structure. False negatives (FNs) are the number of undirected arcs which are in the true, but not in the learnt structure. For V-structures, true positives are the number of V-structures which are present in both the learnt CPDAG and the true DAG. False positives are the number of V-structures in the learnt CPDAG while not in the true DAG. False negatives are present in the true DAG, but not the learnt CPDAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Interventional Performance</head><p>One of the key uses of a causal Bayesian Network model is that, for a given outcome variable of interest, one can test hypothetical interventions on each variable. One can then compute the interventional odds ratio (OR) of how the outcome may change based on the intervention.</p><formula xml:id="formula_3">OR P(A a 1 |do(B b 1 )) P(A a 1 |do(B b 2 )) P(A a 2 |do(B b 1 )) P(A a 2 |do(B b 2 )) a c b d</formula><p>The results of this intervention encompasses both the causal structure learned, and the parameters (estimated by Maximum Likelihood) of the conditional probability tables at each variable. We calculate the standard error for the odds ratios by:</p><formula xml:id="formula_4">SE 1 aN + 1 bN + 1 cN + 1 dN</formula><p>where N is the number of samples in the training data. 95% Confidence intervals are then obtained by log(OR) ± 1.96 √ • SE. In our Causal Datasheet we also wish to estimate how well we can approximate the true impact of interventions. A metric has been developed to measure the proportion of correct interventional odds ratios (PCOR) to quantify the impact of different learned structures on the interventional odds ratios. The measure was designed to provide an answer the question to practitioners: how trustworthy should these interventional odds ratios be with my dataset?</p><p>We calculate this metric by first splitting odds ratios into three types of effect: Protective (less than 1), detrimental (greater than 1), and neutral where the confidence interval crosses 1 (Figure <ref type="figure" target="#fig_3">5</ref>). This is represented by the piecewise function in Eq. 2. The piecewise function is then used within PCOR (Eq. 1) to calculate the proportion of correctly categorized ORs.</p><formula xml:id="formula_5">PCOR O, O |O| i 1 max f (O i ) • f O i , 0 |O| i 1 f (O i ) 2 (1) with, f (O) ⎧ ⎪ ⎨ ⎪ ⎩ -1 if protective, 0 if neutral, 1 if detrimental, (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where O is the set of odds ratios obtained by performing all possible interventions on target T on the true BN B and O are the corresponding odds ratio estimates from the learnt BN B. For a synthetic Bayesian network, the target is heuristically selected as the variable with the maximum number of ancestors. Due to variation in importance of interventions vs. outcomes, we allow the user to specify a threshold of PCOR. Recommendations from the Causal Datasheet should then be based off whether this threshold was met. Ultimately, because PCOR relies on both the structure and parameters of the network being learnt sufficiently well, these do not need to be individually assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Causal Datasheet for Datasets</head><p>There are two types of Causal Datasheets one may follow dependent on usage: a Causal Datasheet for Data Collection and a Causal Datasheet for Existing Datasets. CDG-T is designed to allow the adjustment of numerous characteristics of synthetic data to mimic that a real-world dataset. For existing datasets, as well as the user-defined characteristics, the structure types are varied in order to capture variation in performance due to differing causal structures. For data collection, the sample size and number of variables are also varied so a user could decide what combinations of sample size and number of variables (and if applicable, structural learning algorithm) best meet the user's analytic needs in a lookup table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.1">Causal Datasheet for Data Collection</head><p>If researchers are designing their own survey or evaluation instrument, determining the sample size is a critical step. Researchers want to include a sufficient number of samples so that they can have confidence in their model results, but do not want to waste time, money, or effort by collecting unnecessarily large numbers of samples. Researchers are often constrained by budgetary concerns in low-resource settings, and adding an extra thousand samples may end up costing thousands of extra dollars in effort.</p><p>In traditional public health and medical studies, a priori power analysis is the preferred tool for quantifying the samples size needed to sufficiently detect changes, treatment effects, or associations between variables <ref type="bibr" target="#b47">(Pourhoseingholi et al., 2013)</ref>. The Datasheet for Data Collection can fill a similar role for BN analysis. In the Datasheet for Data Collection, users can specify a range of desired, potential sample size and variable size and then estimate performance for models of interest.</p><p>The resulting datasheet is organized in four main sections: Recommendations, Proportion of Correct Odds Ratios, Skeleton Precision and Recall, and V-structure Precision and Recall. The recommendations Section outlines the main takeaways and suggestions from the data creator/curator who examined the expected performance across a given range of sample sizes and variable sizes, and should be treated as a guide. Proportion of Correct Odds Ratios, Skeleton and V-structure performance sections allow a user to look up combinations of sample sizes and variable sizes that would fit the analysis requirement for correctness of intervention effect, correct edges ignoring directions, and correct edges considering directions accordingly. The resulting datasheet produces surface plots so a user can explore in either or both sample size or variable size dimensions that may maximize expected (median) model learning performance given the user's desired study design. One should also consider the variation of measure performances -lower variation is better, as it suggests an algorithm is less sensitive to the unobserved characteristics. This is provided as Inter-Quartile Range (IQR) in tables corresponding to the surface plots. If two algorithms, and/or two combination of sample size and variable sizes result in similar distribution of performance measures (median and IQR), then one may choose either one. This datasheet provides a general guidance for determining the best combination of sample size and number of variables to maximize BN model performance. The Datasheet for Existing Datasets assists researchers in determining the suitability of using BNs to meet research objectives, given they already know their sample size and number of variables. The goal of this datasheet is to provide insight into how much confidence they should have in BN models learnt from this dataset. Additionally, researchers may use this datasheet to determine which algorithms to use, and how much of a ground truth DAG they can expect to recover. For example, researchers in global health often rely on previously deployed datasets to generate insights. A public health researcher might want to use data from an existing survey to generate causal insights around health decisions in a particular area. They could use the Datasheet for Existing Datasets to evaluate its suitability for generating insights and to inform feature engineering decisions. This type of Causal Datasheet starts with the data characteristics used to generate the synthetic data sets and to compute the various metrics followed with recommendations to a potential audience who may be considering using the data set to infer causal relationships. This leading section outlines the main takeaways and suggestions from the data creator/curator who examined the data set. The main body of the datasheet is then broken down to 1. Correctness of causal effects, 2. Learning the Skeleton, 3. Learning the Direction, 4. Improving with More Samples, and 5. Improving with Less Variables. The goal is to offer the potential data user the best guesses of expected performance from different perspectives, recognizing that different applications may call for choosing an algorithm or feature engineering approach optimized for different measures. These measures are described above and on a scale from 0 to 1, with 1 being perfect. They are depicted with violin plots to aid assessment of expected uncertainties given a measure. If there are multiple "bumps" in the violin plots, in our experience, this is because of different structural learning performance as results of learning different data sets generated with different structure types. If desired, performance stratification by structure type and further investigation may be warranted; however this is beyond the scope of the current work. In Correctness of Causal Effects, the PCOR metrics is used as an attempt to estimate how well we can approximate intervention impact using different structural learning algorithms where 1 represents all protective, neutral and detrimental intervention effects are likely to be correctly captured. In Learning the Skeleton Section, the precision and recall of edges, ignoring the directions, are presented. In Learning the Direction Section, edge directions (as a V-structures) are also considered in the precision and recall. Skeleton learning performance measures are usually better than V-structure learning performance; if one is not too concerned with learning the causal directions, one may be satisfied with good skeleton learning performance alone. Moreover, one may care more about recall over precision (e.g., in an exploratory study aiming to identify all potential relationships between variables) or vice versa. Lastly, the Improving with More Samples and Improving with Less Variables sections show how much more data or how many variables to reduce to improve structure learning performance for different algorithms, assuming relevant causal statistics is not degraded by the reduction of variables in the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Meta-Feature Similarity Between Synthetic Datasets and Existing Dataset of Interest</head><p>Our approach makes the assumption that we could tune the characteristics of the synthetic datasets such that the synthetic datasets are similar to the dataset of interest, and thus it is reasonable to suggest that expected performance on the dataset of interest could be approximated by metrics computed on the synthetic datasets. How similar are the synthetic datasets to the existing dataset of interest? We should not compare them using only data characteristics that are used to generate the synthetic datasets. Instead, we borrow the concept of defining meta-features and computing dataset similarities from the Bayesian optimization hyperparameter initialization literature <ref type="bibr" target="#b70">(Wistuba et al., 2015)</ref>. For categorical datasets, information theoretic meta-features (i.e., Shannon's entropy and Concentration Coefficient) are computed for both the real and synthetic datasets <ref type="bibr" target="#b41">(Michie et al., 1994;</ref><ref type="bibr" target="#b1">Alexandros and Melanie, 2001)</ref>. The similarity between them then is computed as the mean cosine similarity, with 0 being completely dissimilar and 1 being identical. For additional details, please refer to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Datasheet for Data Collection Example: Survey Design of a Study of Sexual and Reproductive Health</head><p>We first used a Datasheet for Data Collection, generated using CDG-T, to determine the appropriate sample size of a survey we deployed in Madhya Pradesh, India (Supplementary Material A). In 2019, we had the opportunity to use the CDG-T to determine the sample size of a large-scale survey of sexual and reproductive health (SRH) we conducted in Madhya Pradesh, India. Determining sample size of this study was important because it had implications for the overall budget and timeline of our project. Typically we wish to have a survey to capture as many variable as possible (provided the survey is not too long) with as few samples as possible. Our survey sought to quantify a wide range of causal drivers around family planning decisions. These variables included demographics, knowledge and beliefs, risk perceptions, past experiences, and structural determinants such as accessibility. We estimated that we would have between 30-60 variables that would be critical causal drivers of sexual and reproductive health decisions. From previous work, we estimated that causal variables would have, on average, three levels. We decided to use the Datasheet for Data Collection to determine model performance for between 5,000 and 15,000 survey respondents before commissioning the field study. While we had determined that 5,000 respondents was likely a large enough sample to have sufficient power for predictive regression models, we did not know whether this sample size would have sufficient performance for a causal Bayesian network model. The range of 5,000 to 15,000 samples represented the budget constraints of our survey.</p><p>For simplicity, we varied the potential number of variables to be included in the model and the potential sample sizes while keep other synthetic data property constant (Table <ref type="table" target="#tab_3">3</ref>).</p><p>The datasheet revealed insights around the optimal sample size for our study. We found that, in general, the OrderMCMC algorithm was the best for PCOR, skeleton precision and recall, and V-structure precision (Table <ref type="table" target="#tab_4">4</ref>) and recall (Table <ref type="table" target="#tab_5">5</ref>). When comparing model performance metrics, we found that 5,000 samples would likely not be enough to build robust BN models for designing interventions because, across all numbers of variables, the V-structure recall was low (&lt;0.42, GES and PC) or was high but had high IQR with both OrderMCMC instantiations &gt; 0.40. Our datasheet showed that as we increased sample size, the IQR of V-structure recall for the OrderMCMC decreased. In order to have better confidence in our Bayesian network models, we determined that we would need a sample of around 15,000 respondents to balance our desire of having at least 50 variables while minimizing the IQR of V-structure recall (Table <ref type="table" target="#tab_5">5</ref>).</p><p>Fortunately, our budget constraints allowed us to expand our sample size to meet this constraint. However, in many cases, organizations operating in LMICs would not be able to treble their sample size. Here, the CDG-T also provides useful advice. For example, if our sample size remained at 5,000, reducing the number of causal variables from 60 to 30 would cause V-structure recall to increase for all algorithms and V-structure IQR to decrease. This would significantly improve confidence in the produced BN models, but with the implication that a potentially different analytical question may be necessary.</p><p>The CDG-T Datasheet for Data Collection provides useful information even if researchers decide that they do not want to reduce variables or increase sample size by estimating the performance of a DAG before a survey is carried out. This allows researchers to know what kind of insights and results they will be able to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Datasheet for Existing Datasets Example: Analysis of an Existing Global Health Survey (Surgo Household Dataset)</head><p>As the second example, we generated a Causal Datasheet for a global development dataset we administered in Uttar Pradesh, India in 2016 <ref type="bibr" target="#b62">(Smittenaar et al., 2020)</ref> (Supplementary Material B). For simplicity, we refer to this dataset as Surgo Household survey or SHH. It sought to quantify household reproductive, maternal, neonatal, and child health (RMNCH) journeys and to understand the drivers of various RMNCH behaviors. In all, we surveyed over 5,000 women on various RMNCH behaviors and outcomes. From this survey, we initially identified 41 variables we thought represented critical causal drivers of RMNCH outcomes and behaviors such as birth delivery locations and early breastfeeding initiation. We were interested in understanding which interventions might be most important for different health outcomes. While it was possible to use our datasets to generate DAGs, we could not validate their structures, nor could we assign confidence to graphs generated using different structural learning algorithms.</p><p>Using survey dataset characteristics, we generated synthetic dataset experiments with similar properties (Table <ref type="table" target="#tab_6">6</ref>). Using a method described in Section 2.8, we computed information theoretic similarity between the synthetic datasets and the SHH data; the result is that they are indeed similar with a similarity score of 0.89. This is supportive of the assumption that the expected performance on the SHH data can be reasonably approximated by computing the metrics on the corresponding synthetic datasets.</p><p>The expected BN algorithm skeleton (Figures <ref type="figure" target="#fig_4">6A,</ref><ref type="figure">B</ref>), V-structure (Figures <ref type="figure" target="#fig_4">6C,</ref><ref type="figure">D</ref>), and PCOR score (Figure <ref type="figure" target="#fig_5">7A</ref>) were then attached to the datasheet for each of the structure learning algorithms. As our primary goal was to successfully simulate interventions, we set a threshold of 0.8 on the PCOR score. Meeting this threshold would imply we could have reasonable confidence in our model and the estimates it produced.</p><p>The outputs from the Causal Datasheet provided a number of key insights for this dataset:</p><p>1. While all structure learning algorithms could achieve high skeleton precision and recall (Figures <ref type="figure" target="#fig_4">6A,</ref><ref type="figure">B</ref>), the OrderMCMC algorithms (with either BIC or qNML) had superior median predicted performance for V-structure precision and recall (Figures <ref type="figure" target="#fig_4">6C,</ref><ref type="figure">D</ref>). 2. The median PCOR for OrderMCMC (qNML) (0.89) and OrderMCMC (BIC) met our threshold test of 0.8 (Figure <ref type="figure" target="#fig_5">7A</ref>). There are cases when the PCOR is 0 due to target variables being determined as independent during structure learning. This is not a major concern as it will be clear to a practitioner when this has occurred once the DAG has been visualized. 3. Decreasing the number of variables from 40 to 20 could improve the mean V-structure recall by ≈ 0.08 (Supplementary Material B) and PCOR by (Figure <ref type="figure" target="#fig_5">7B</ref>). 4. Structure type, specifically the distribution of in-degree, has a large effect on expected performance levels, particularly on V-structure precision (Figure <ref type="figure" target="#fig_4">6C</ref>). This leads to a multi-modal distribution of performance where similar structures are grouped. If a practitioner can ascertain the ground truth structure type or the distribution of in-degree, even if he/she cannot ascertain the ground truth structure itself, the uncertainty of the performance estimation can be reduced.</p><p>Specifically on ground truth skeleton recovery, we found that PC may provide marginally higher performance on skeleton precision (1 vs. 0.98), but performs poorly in recall by comparison with OrderMCMC (qNML) (0.75 vs. 1). OrderMCMC (BIC) had the highest median skeleton precision at 1. OrderMCMC (qNML), PC and GES were at 0.97, 0.91 and 0.36 respectively. However, the precision with PC is less sensitive with a IQR of 0.02, whereas OrderMCMC (qNML) was 0.21 and OrderMCMC (BIC) was 0.05.</p><p>On ground truth V-structure recovery, OrderMCMC (BIC) performs best in terms of V-structure precision, but suffers with V-structure recall compared to OrderMCMC (qNML). Particularly on structure types with higher in-degrees, forest fire and Barabasi-Albert. PC recall for V-structures is much worse than OrderMCMC (qNML) (0.37 vs. 1). Overall GES with BIC is a poor performer for our needs, especially when V-structure is concerned despite having the same score function as OrderMCMC (BIC).</p><p>These insights were invaluable for decision making in the relevant context and showed us that we would need to further reduce our number of variables or seek expert input before we could have confidence in our understanding of the effects of interventions on maternal health outcomes. The results also suggested that the OrderMCMC algorithm qNML would generally provide the best overall model performance among those tested. Ultimately, given we had a clear outcome variable of interest, we used multivariate regression to select 18 out of 40  variables based on significance of regression coefficients from the original dataset; this reduction in the number of variables allowed us to have more confidence in our resulting DAG structures. It should be pointed out that there are many general-purpose feature selection schemes, but feature selection with the intent for subsequent causal structural discovery is not well understood and beyond the scope of this study <ref type="bibr" target="#b24">(Guyon et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal Datasheet for Existing Datasets Example: ALARM</head><p>As a third example, a Causal Datasheet for the well-known ALARM dataset was generated for the purpose of validating the estimates being produced by CDG-T (Supplement C). The characteristics of this dataset were approximated, mimicking how a researcher might use the Causal Dataset Generation Tool. The ALARM dataset has 37 variables, and an average of 2.8 levels per variable <ref type="bibr" target="#b5">(Beinlich et al., 1989)</ref>. Aside from a few binary variables, most variables have ordinal values. In this test case a sample size of 5,000 was used. Synthetic BNs with similar characteristics were then generated, the exact values used can be found in Table <ref type="table" target="#tab_7">7</ref>.</p><p>Using a method described in Section 2.8, we computed information theoretic similarity between the synthetic datasets and the ALARM data; the result is that they are indeed similar with a similarity score of 0.91. This is supportive of the assumption that the expected performance on the ALARM dataset can be reasonably approximated by computing the metrics on the corresponding synthetic datasets. Experiments using these Synthetic BNs were then performed, with the results summarized in the datasheet.  From this figure we can see good skeleton performance can be obtained using OrderMCMC, but V-Structures are more likely to be correct (precision) when using the BIC and less likely to be missed (recall) when using the qNML score.</p><p>Frontiers in Artificial Intelligence | www.frontiersin.org</p><p>While the synethetic datasets are similar to the ALARM dataset, they are not identical. As the ALARM dataset has a known corresponding ground-truth, it can be used to test the limitations of our current approach due the assumptions we make when generating synethetic datasets.</p><p>One such assumption is that when sampling parameters from a Dirichlet distribution we have assumed α is uniform, this means (on average) the marginal distributions of the variables will be balanced. Imbalanced marginal distributions can degrade structure learning performance, as information supporting conditional dependence becomes more scarce with the same amount of data.</p><p>Comparing the CDG-T estimate generated with the uniform α assumption to the actual performance obtained on the ALARM dataset shows general alignment with the PC and GES algorithms, but an overestimation of performance with OrderMCMC (Figure <ref type="figure" target="#fig_6">8</ref>). This difference can be observed when looking at V-structure recall (Figure <ref type="figure" target="#fig_6">8D</ref>). Our preliminary analysis suggests that when α is not assumed to be uniform, such misalignment decreases. Moreover, information theoretic similarity also increases (Supplementary Material D) from 0.91 to 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>Having a Causal Datasheet that describes the expected performance in recovering ground truth structures for any given dataset can be tremendously valuable to both machine learning scientists and practitioners. We were particularly interested in scenarios where data characteristics are suboptimal for data-driven causal BN learning, which is often the case for LMIC scenarios. This perspective differs from other evaluative reviews of algorithm in the sense that we are not only concerned with different structural learning algorithms' (and score function choices') maximum capacity to recover the ground truth, but also how they differ in more constrained cases <ref type="bibr" target="#b48">(Raghu et al., 2018)</ref>. We have shown how Causal Datasheets can aid in the planning of studies that have the analytical goal of causal discovery and inference, and in analysis of studies after existing data have been collected. Our general approach of creating synthetic datasets that approximate the real-world data should accommodate other causal inference methods such as Neyman-Rubin causal models (the potential outcomes framework) in theory <ref type="bibr" target="#b53">(Rubin, 2005)</ref>.</p><p>In addition to the number of variables and sample size demonstrated in the case studies, we have also observed that extreme imbalance of levels, in-degree and structure type all affect structural learning performance. In practice, even observable characteristics may be beyond the modeler's control. Sometimes the sample size is restricted by resources such as survey budget or similar data have already been collected. Sometimes a variable may be very imbalanced (e.g., very few unhealthy samples vs. many healthy samples). Often, data are collected with specific questions in mind and may not contain all the right variables for another specific outcome of interest. However, upon referring to such Causal Datasheets, there may be scenarios where seemingly imperfect dataset could still yield useful insights, given a tolerable level of error. Moreover, one's tolerance may be different for precision and recall errors. In constrained scenarios, our results suggest that practitioners may be able to increase algorithm performances by additional feature engineering or transformation of the data by reducing the number of variables for example. However, one should be cautioned against too much data processing as it runs the risk of transforming the ground truth represented by the data as well.</p><p>We briefly discuss the potential impact of our Causal Datasheet work on algorithmic fairness research. <ref type="bibr" target="#b18">Gebru et al. (2018)</ref> advocated that every dataset is accompanied with a datasheet to promote transparency and accountability, including to highlight if the dataset has unwanted bias toward a particular demographic. It is important for us to understand how and why demographic information, especially protected characteristics (e.g., race, gender, age), influences other variables in a dataset. Causal reasoning has recently been shown to be a powerful tool for understanding sources of algorithmic bias, and for mitigating bias in an algorithmic decision system <ref type="bibr" target="#b8">(Chiappa and Isaac, 2018;</ref><ref type="bibr" target="#b40">Loftus et al., 2018;</ref><ref type="bibr" target="#b59">Sharmanska et al., 2020)</ref>. Most existing causalitybased algorithmic fairness methods require knowledge of the causal graph. One option is to learn causal structure from observational data. It is important to acknowledge that potentially very misleading conclusions might be drawn if incorrect causal structure is used <ref type="bibr" target="#b32">(Kilbertus et al., 2020)</ref>. Our Causal Datasheet can be used to help researchers and practitioners assess whether they can have confidence in the inferred structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Assumptions and Limitations</head><p>While we believe the datasheet has utility in its current form, there are still a number of improvements to be made. Assumptions are made when building the Causal Datasheets. In order to present the results from synthetic experiments as performance which can be expected on empirical data, we must assume that synthetic data can act as a proxy for empirical data. Furthermore, the synthetic data in use can be improved upon; there may be other pertinent data characteristics that we had not considered or considered but incorrectly assumed. These include the assumed ground truth structure types, where the five graph generation algorithms used offer no guarantee of orthogonality. While using known structure types can be an advantage if a practitioner suspects their DAG may be of a particular distribution, using multiple can clearly bias results if  they generate in-degree distributions which are too similar. A future direction of research would be to use generative graph models which when seeded with an initial DAG can preserve degree distribution, among other properties <ref type="bibr" target="#b37">(Leskovec and Faloutsos, 2007)</ref>. How to form a DAG as a seed in an unbiased and useful way is non-trivial. A high-recall DAG could be used in an attempt to provide an upper-bound on difficulty, under the assumption the in-degree distribution would be at least on par with reality. Alternatively an agreement graph, as in the Intersection-Validation paper, could be used as a seed to provide a DAG with less bias to any one structure learning algorithm. For simplicity we have also assumed that the conditional imbalance parameter applies to the entire dataset, but it is entirely possible that a real dataset has a large variance around the imbalance of parameters.</p><p>Validating our tool with ALARM demonstrates there are special cases which are not yet entirely modeled in our synthetic data generation. The current simplifying assumption of uniform α values when sampling parameters from a Dirichlet distribution can clearly lead to overestimation of performance in some scenarios. Development of α-estimation techniques, or other methods of incorporating non-uniform α values is a clear next step. Some initial work can be found in the supplementary material. Introducing further modeling assumptions, whether by generative graph or α estimation techniques, can increase the specificity of the provided estimates. However, introducing bias in this way must be done with caution. As it could yield certain, yet incorrect, performance estimates. A method of determining whether introduced assumptions are correct, and to address the gap between real and synthetic data must be developed. Some initial work on this can again be found in the supplementary material.</p><p>Others have shown that algorithm outputs are sensitive to hyper-parameters specific to that algorithm. For example, BDeu is a popular score but it is highly sensitive to its only hyper-parameter, the equivalent sample size <ref type="bibr" target="#b56">(Scutari, 2018)</ref>. This is part of the reason we included qNML and BIC in the current study as they do not have hyper-parameters. Estimation of hyper-parameters is often not trivial and may challenging to generalize across a spectrum of real-world data. Additionally, we have only considered BN here, which cannot accommodate cyclical causal relationships. Finally, we have assumed that the input datasets had no latent confounders and the datasets are at least meet the interventional sufficiency criteria <ref type="bibr" target="#b45">(Pearl, 2009;</ref><ref type="bibr" target="#b46">Peters et al., 2017)</ref>, which is known to be a problem.</p><p>There are also practical limitations as well. We had considered data with discrete variables only; however this approach can be extended to algorithms that deal with continuous variables as well. We did not consider computational power needed for different algorithms. While we bear a faithful optimism that computation power of current hardware will increase to eventually overcome this barrier, this is a useful addition to the Causal Datasheet. Similarly the computation time to generate the synthetic data sets is also highly dependent on the hardware. However, since it took about 1 min to generate 50 synthetic data sets (40 variables, 5,000 samples, five structure types and 10 repetitions) on a workstation equipped with an AMD EPYC 7742 CPU and 256 GB of RAM, we think this will not be a big problem for most in the long run as cloud computing solutions become democratized and cheaper. With ten repeats (i.e., T 10), our largest set of experiments had 1,750 datasets to generate, learn, and evaluate; taking around 40 h to complete. Running times are highly dependent on configuration, as well as the machine being used, and should be selected appropriately for individual circumstances. Whether 10 repeats of each experiment are required, or is sufficient, remains unknown. While the synthetic BN and datasets generation is inexpensive (timewise), structure learning on the data is not, and by-far takes the most time of any component in our datasheet generation pipeline. For example, while data generation for the ALARM dataset takes 1 min, the structure learning takes close to an hour. Another clear direction of future work is to perform analysis to determine when enough experiments have taken place to reach some performance convergence. We have made assessments based on purely precision and recall of the ground truth, ignoring the fact that our OrderMCMC implementation takes longer than PC and GES; however there may be circumstances where computational speed outweighs the benefit of accuracy gains. Real-world data often come with missingness that are either random (MAR), completely at random (MCAR), and missing not at random (MNAR) <ref type="bibr" target="#b52">(Rubin, 1976)</ref>. We have developed the capacity to produce synthetic datasets with missingness. Determining missingness characteristics along with the appropriate imputation method for use in the datasheet is a future research direction. Lastly, we were inspired by the problem of inferring causality from global development datasets and have estimated the range of data characteristics subjectively in that domain. By all means, the range of data characteristics considered in this study may be very different for a different sub-domain. For example, the number of variables for agricultural data may be many more than that of a disease treatment survey. We leave these theoretical and practical limitations as potential areas for improvement to further the usage of Bayesian networks in practice.</p><p>In summary, a standard practice of reporting projected range of causal discovery and inference performance can help practitioners 1) during the experimental design phase, when they are interested in designing experiments with characteristics suitable for BN analysis, 2) during the analysis phase, when they are interested in choosing optimal structural learning algorithms and assigning confidence to DAGs, and 3) at the policy level, when they must justify their insights generated from BN analysis. We believe that this type of evaluation should be a vital component to a general causal discovery and inference work flow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc>FIGURE 1 | A DAG with three variables (A-C) and two edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 |</head><label>2</label><figDesc>FIGURE 2 | A CPDAG with four variables (A-D), with two possible DAG instantiations. The edges forming the V-structure (A-C) are purple, and the two alternative (B-D) connections are in red.</figDesc><graphic coords="4,141.62,67.75,312.23,71.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 |</head><label>3</label><figDesc>FIGURE 3 | Illustration of the approach used to create the Causal Datasheets presented in Section 3. Algorithm 1 refers to the algorithm labelled Algorithm 1: CDG-T Overview below.</figDesc><graphic coords="5,117.69,67.75,359.97,152.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 |</head><label>5</label><figDesc>FIGURE 5 | Illustration demonstrating the three categories of odds ratio types. Protective effects are within the bounds of the yellow Section, Detrimental within red, and neutral where the confidence intervals intersect 1 (examples within the purple boxes).</figDesc><graphic coords="9,309.09,67.75,238.08,166.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 6 |</head><label>6</label><figDesc>FIGURE 6 | Performance obtained running the structure learning algorithms on the datasets produced for the Surgo Household datasheet (Datasheet for Existing Datasets). The width of the violin plots are the kernel density estimates of the distribution, the white dots represent the median, and the vertical thick and thin lines represent the IQRs and ranges respectively. (A): Skeleton Precision; (B): Skeleton Recall; (C): V-structure Precision; (D): V-structure Recall.From this figure we can see good skeleton performance can be obtained using OrderMCMC, but V-Structures are more likely to be correct (precision) when using the BIC and less likely to be missed (recall) when using the qNML score.</figDesc><graphic coords="13,117.69,337.32,359.97,309.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 7 |</head><label>7</label><figDesc>FIGURE 7 | (A):The PCOR score obtained running the structure learning algorithms on the datasets produced for the Surgo Household dataset datasheet. Performance for OrderMCMC is generally above the set threshold. There are cases where the there are no paths to the target variable, causing the PCOR to be 0. (B):The PCOR scores obtained by reducing the number of variables. Median performance passes the threshold on the OrderMCMC results when variables are ≤ 20.</figDesc><graphic coords="14,87.82,67.75,419.73,199.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 8 |</head><label>8</label><figDesc>FIGURE 8 | The performance obtained on CDG-T datasets vs. the ALARM dataset over 100 runs. (A): Skeleton Precision; (B): Skeleton Recall; (C): V-structure Precision; (D): V-structure Recall. There is a general alignment on the skeleton estimates, but V-Structure Recall is overestimated for the OrderMCMC methods.</figDesc><graphic coords="15,123.70,379.11,347.96,296.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,59.81,188.33,475.64,275.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 |</head><label>1</label><figDesc>Existing synthetic Bayesian network generation methods, with network attributes each method implements control over. Properties compared are those known both to vary, and to influence structure learning performance (✓) signifies a feature is implemented (7) it is not, and (-) a partial implementation. Existing methods do not support latent confounding or offer control over parameter generation, and offer only random DAGs and at most one pre-defined structure type. Our work presents a step toward fully flexible structure generation, with these features the main remaining known limitations.</figDesc><table><row><cell>Method/Property</cell><cell>Flexible DAG Generation</cell><cell>Flexible Parameter Generation</cell><cell>Controllable levels</cell><cell>Latent confounding</cell></row><row><cell>CDG-T (ours)</cell><cell>-</cell><cell>-</cell><cell>✓</cell><cell>7</cell></row><row><cell>Gogoshin et al. (2020)</cell><cell>-</cell><cell>7</cell><cell>✓</cell><cell>7</cell></row><row><cell>Zhang et al. (2020)</cell><cell>-</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>Andrews et al. (2018)</cell><cell>7</cell><cell>7</cell><cell>✓</cell><cell>7</cell></row><row><cell>Tasaki et al. (2015)</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 |</head><label>2</label><figDesc>A table on the observability of the properties of BNs, as well as the values the synthetic generation tool can use.</figDesc><table><row><cell>Characteristic</cell><cell>Observable</cell><cell>Possible values</cell></row><row><cell>Number of Samples</cell><cell>✓</cell><cell>1-1,000,000</cell></row><row><cell>Number of variables</cell><cell>✓</cell><cell>1-500</cell></row><row><cell>Average variable levels</cell><cell>✓</cell><cell>1-10</cell></row><row><cell>Structure type</cell><cell>7</cell><cell>Forest fire, IC-DAG, barabasi-albert, waxman, Small world</cell></row><row><cell>Maximum in-degree</cell><cell>7</cell><cell>1-∞</cell></row><row><cell>α (imbalance)</cell><cell>7</cell><cell>0-∞</cell></row></table><note><p>FIGURE 4 | 20-node examples of the five structure types used to generate synthetic BNs. As the waxman structure type is a random geometric graph, the connectivity is proportional to the number of nodes-at 20 nodes the DAG remains sparse.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 |</head><label>3</label><figDesc>The values used for each property when creating synthetic BNs (and their associated datasets) for the SRH datasheet. Combinatorial total of 175 property values, Each configuration of properties was repeated 10 times. In total 1750 Bayesian networks and datasets were created.</figDesc><table><row><cell>Property</cell><cell>Values</cell></row><row><cell>Variables</cell><cell>30, 35, 40, 45, 50, 55, 60</cell></row><row><cell>Samples</cell><cell>5,000, 7,500, 10,000, 12,500, 15,000</cell></row><row><cell>Average levels</cell><cell>3</cell></row><row><cell>Structure types</cell><cell>Forest fire, barabasi-albert, IC-DAG, waxman, Small-world</cell></row><row><cell>Maximum in-degree</cell><cell>Uncapped</cell></row><row><cell>α</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 |</head><label>4</label><figDesc>Pivot Table of V-structure precision. Rows stratify by number of variables. Columns are over samples size. V-structure Precision performance is provided as: Median (IQR). Highest precision in each sample/variable combination is in bold.</figDesc><table><row><cell>Number of</cell><cell>Algorithm</cell><cell>5,000</cell><cell>7,500</cell><cell>10,000</cell><cell>12,500</cell><cell>15,000</cell></row><row><cell>variables</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell>GES</cell><cell>0.37 (0.31)</cell><cell>0.39 (0.31)</cell><cell>0.47 (0.37)</cell><cell>0.39 (0.31)</cell><cell>0.41 (0.31)</cell></row><row><cell>30</cell><cell>OrderMCMC (BIC)</cell><cell>1.00 (0.04)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.04)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.00)</cell></row><row><cell>30</cell><cell>OrderMCMC (qNML)</cell><cell>1.00 (0.65)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell></row><row><cell>30</cell><cell>PC</cell><cell>0.86 (0.24)</cell><cell>0.89 (0.13)</cell><cell>0.89 (0.13)</cell><cell>0.89 (0.12)</cell><cell>0.89 (0.13)</cell></row><row><cell>40</cell><cell>GES</cell><cell>0.37 (0.24)</cell><cell>0.36 (0.29)</cell><cell>0.36 (0.21)</cell><cell>0.38 (0.27)</cell><cell>0.32 (0.37)</cell></row><row><cell>40</cell><cell>OrderMCMC (BIC)</cell><cell>1.00 (0.03)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.03)</cell><cell>1.00 (0.02)</cell></row><row><cell>40</cell><cell>OrderMCMC (qNML)</cell><cell>0.97 (0.50)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell></row><row><cell>40</cell><cell>PC</cell><cell>0.89 (0.16)</cell><cell>0.87 (0.13)</cell><cell>0.88 (0.15)</cell><cell>0.89 (0.14)</cell><cell>0.88 (0.16)</cell></row><row><cell>50</cell><cell>GES</cell><cell>0.32 (0.20)</cell><cell>0.33 (0.32)</cell><cell>0.35 (0.25)</cell><cell>0.32 (0.28)</cell><cell>0.34 (0.30)</cell></row><row><cell>50</cell><cell>OrderMCMC (BIC)</cell><cell>0.98 (0.04)</cell><cell>1.00 (0.03)</cell><cell>1.00 (0.03)</cell><cell>0.99 (0.04)</cell><cell>1.00 (0.03)</cell></row><row><cell>50</cell><cell>OrderMCMC (qNML)</cell><cell>0.97 (0.61)</cell><cell>0.99 (0.06)</cell><cell>1.00 (0.01)</cell><cell>1.00 (0.01)</cell><cell>1.00 (0.00)</cell></row><row><cell>50</cell><cell>PC</cell><cell>0.86 (0.16)</cell><cell>0.87 (0.12)</cell><cell>0.88 (0.12)</cell><cell>0.88 (0.16)</cell><cell>0.88 (0.15)</cell></row><row><cell>60</cell><cell>GES</cell><cell>0.34 (0.20)</cell><cell>0.34 (0.22)</cell><cell>0.32 (0.24)</cell><cell>0.30 (0.26)</cell><cell>0.34 (0.25)</cell></row><row><cell>60</cell><cell>OrderMCMC (BIC)</cell><cell>0.98 (0.03)</cell><cell>0.99 (0.03)</cell><cell>1.00 (0.02)</cell><cell>0.98 (0.03)</cell><cell>0.99 (0.03)</cell></row><row><cell>60</cell><cell>OrderMCMC (qNML)</cell><cell>0.93 (0.50)</cell><cell>0.98 (0.23)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.02)</cell></row><row><cell>60</cell><cell>PC</cell><cell>0.86 (0.20)</cell><cell>0.87 (0.20)</cell><cell>0.86 (0.17)</cell><cell>0.90 (0.17)</cell><cell>0.88 (0.16)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 |</head><label>5</label><figDesc>Pivot Table of V-structure recall. Rows stratify by number of variables. Columns are over samples size. V-structure Recall performance is provided as: Median (IQR). Highest recall in each sample/variable combination is in bold.</figDesc><table><row><cell>Number of</cell><cell>Algorithm</cell><cell>5,000</cell><cell>7,500</cell><cell>10,000</cell><cell>12,500</cell><cell>15,000</cell></row><row><cell>variables</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell>GES</cell><cell>0.21 (0.25)</cell><cell>0.28 (0.33)</cell><cell>0.30 (0.34)</cell><cell>0.31 (0.25)</cell><cell>0.33 (0.36)</cell></row><row><cell>30</cell><cell>OrderMCMC (BIC)</cell><cell>0.94 (0.33)</cell><cell>0.95 (0.28)</cell><cell>0.95 (0.24)</cell><cell>0.99 (0.21)</cell><cell>0.99 (0.17)</cell></row><row><cell>30</cell><cell>OrderMCMC (qNML)</cell><cell>1.00 (0.02)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell></row><row><cell>30</cell><cell>PC</cell><cell>0.43 (0.49)</cell><cell>0.49 (0.49)</cell><cell>0.54 (0.42)</cell><cell>0.56 (0.44)</cell><cell>0.56 (0.45)</cell></row><row><cell>40</cell><cell>GES</cell><cell>0.26 (0.19)</cell><cell>0.31 (0.25)</cell><cell>0.30 (0.26)</cell><cell>0.33 (0.25)</cell><cell>0.36 (0.23)</cell></row><row><cell>40</cell><cell>OrderMCMC (BIC)</cell><cell>0.84 (0.47)</cell><cell>0.91 (0.43)</cell><cell>0.92 (0.43)</cell><cell>0.94 (0.39)</cell><cell>0.97 (0.28)</cell></row><row><cell>40</cell><cell>OrderMCMC (qNML)</cell><cell>1.00 (0.20)</cell><cell>1.00 (0.03)</cell><cell>1.00 (0.03)</cell><cell>1.00 (0.00)</cell><cell>1.00 (0.00)</cell></row><row><cell>40</cell><cell>PC</cell><cell>0.37 (0.26)</cell><cell>0.41 (0.33)</cell><cell>0.47 (0.23)</cell><cell>0.46 (0.28)</cell><cell>0.48 (0.27)</cell></row><row><cell>50</cell><cell>GES</cell><cell>0.25 (0.17)</cell><cell>0.28 (0.13)</cell><cell>0.32 (0.20)</cell><cell>0.32 (0.17)</cell><cell>0.36 (0.22)</cell></row><row><cell>50</cell><cell>OrderMCMC (BIC)</cell><cell>0.86 (0.42)</cell><cell>0.90 (0.43)</cell><cell>0.91 (0.40)</cell><cell>0.93 (0.39)</cell><cell>0.94 (0.38)</cell></row><row><cell>50</cell><cell>OrderMCMC (qNML)</cell><cell>0.97 (0.40)</cell><cell>1.00 (0.33)</cell><cell>1.00 (0.19)</cell><cell>1.00 (0.21)</cell><cell>1.00 (0.05)</cell></row><row><cell>50</cell><cell>PC</cell><cell>0.36 (0.17)</cell><cell>0.38 (0.22)</cell><cell>0.39 (0.31)</cell><cell>0.40 (0.32)</cell><cell>0.39 (0.30)</cell></row><row><cell>60</cell><cell>GES</cell><cell>0.24 (0.17)</cell><cell>0.27 (0.18)</cell><cell>0.29 (0.17)</cell><cell>0.30 (0.21)</cell><cell>0.33 (0.26)</cell></row><row><cell>60</cell><cell>OrderMCMC (BIC)</cell><cell>0.72 (0.39)</cell><cell>0.80 (0.38)</cell><cell>0.84 (0.35)</cell><cell>0.84 (0.34)</cell><cell>0.86 (0.35)</cell></row><row><cell>60</cell><cell>OrderMCMC (qNML)</cell><cell>0.93 (0.34)</cell><cell>0.98 (0.33)</cell><cell>0.99 (0.31)</cell><cell>0.98 (0.29)</cell><cell>1.00 (0.30)</cell></row><row><cell>60</cell><cell>PC</cell><cell>0.34 (0.29)</cell><cell>0.35 (0.33)</cell><cell>0.35 (0.34)</cell><cell>0.38 (0.35)</cell><cell>0.39 (0.34)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 |</head><label>6</label><figDesc>The values used for each property when creating synthetic BNs (and their associated datasets) for the SHH datasheet. Combinatorial total of 80 property values, Each configuration of properties was repeated 10 times. In total 800 Bayesian networks and datasets were created. Italicized values were only used when presenting results pertaining to increasing samples or decreasing variables.</figDesc><table><row><cell>Property</cell><cell>Values</cell></row><row><cell>Variables</cell><cell>40, 30, 20, 10</cell></row><row><cell>Samples</cell><cell>5,000, 7,500, 10,000, 12,500</cell></row><row><cell>Average levels</cell><cell>3</cell></row><row><cell>Structure types</cell><cell>Forest fire, barabasi-albert, IC-DAG, waxman, Small-world</cell></row><row><cell>Maximum in-degree</cell><cell>Uncapped</cell></row><row><cell>α</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 |</head><label>7</label><figDesc>The values used for each property when creating synthetic BNs (and their associated datasets) for the ALARM datasheet. Combinatorial total of five property values, Each configuration of properties was repeated 10 times. In total 50 Bayesian networks and datasets were created.</figDesc><table><row><cell>Property</cell><cell>Values</cell></row><row><cell>Variables</cell><cell>40</cell></row><row><cell>Samples</cell><cell>5,000</cell></row><row><cell>Average levels</cell><cell>3</cell></row><row><cell>Structure types</cell><cell>Forest fire, barabasi-albert, IC-DAG, waxman, Small-world</cell></row><row><cell>Maximum in-degree</cell><cell>Uncapped</cell></row><row><cell>α</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Frontiers in Artificial Intelligence | www.frontiersin.org April 2021 | Volume 4 | Article 612551</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>FUNDING</head><p>The authors declare that this study received funding from <rs type="funder">Surgo Foundation</rs>. NQ is supported by the <rs type="funder">European Research Council (ERC)</rs> funding, grant agreement No <rs type="grantNumber">851538</rs>, and <rs type="funder">UK EPSRC</rs> project <rs type="grantNumber">EP/P03442X/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GHxSqvV">
					<idno type="grant-number">851538</idno>
				</org>
				<org type="funding" xml:id="_Vutmd6V">
					<idno type="grant-number">EP/P03442X/1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY STATEMENT</head><p>The original contributions presented in the study are included in the article/Supplementary Material, further inquiries can be directed to the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>All authors listed have made a substantial, direct and intellectual contribution to the work, and approved it for publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The author would like to thank Wray Buntine for early discussion of the structural learning algorithms and performance metrics. We would also like to thank the reviewers for their insightful comments, which helped us to significantly improve the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>The Supplementary Material for this article can be found online at: <ref type="url" target="https://www.frontiersin.org/articles/10.3389/frai.2021.612551/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/frai.2021.612551/ full#supplementary-material</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian networks in environmental modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salmeron</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.envsoft.2011.06.004</idno>
	</analytic>
	<monogr>
		<title level="j">Environ. Model. Softw</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1376" to="1388" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model selection via meta-learning: a comparative study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1142/S0218213001000647</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Artif. Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="525" to="554" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scoring Bayesian networks of mixed variables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41060-017-0085-7</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Data Sci. Analytics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian networks for risk prediction using real-world data: a tool for precision medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jval.2019.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Value in Health</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="439" to="445" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.286.5439.509</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The alarm monitoring system: a case study with two probabilistic inference techniques for belief networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Beinlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Suermondt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="247" to="256" />
			<pubPlace>Aime</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive probabilistic networks with hidden variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="213" to="244" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Theory refinement on Bayesian networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the seventh conference on Uncertainty in artificial intelligence<address><addrLine>Los Angeles, CA; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1991-04-13">1991. April 13. 1991</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A causal Bayesian networks viewpoint on fairness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A transformational characterization of equivalent Bayesian network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the eleventh conference on Uncertainty in artificial intelligence<address><addrLine>San Francisco, CA; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995-04-13">1995. April 13. 1995</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">precision&quot; public healthbetween novelty and hype. New Engl</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowkwanyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galea</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMp1806634</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="1398" to="1401" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Guide to DHS statistics Tech. Rep., the demographic and health surveys program</title>
		<author>
			<persName><forename type="first">T</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Agency for International Development</publisher>
			<pubPlace>United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prequential analysis, stochastic complexity and Bayesian inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics</title>
		<meeting><address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="109" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A comparison of structural distance measures for causal Bayesian network models recent advances</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Jongh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
	<note>in Intelligent Information systems, challenging problems of science computer science series</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progress lies in precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Desmond-Hellmann</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aai7598</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page">731</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A systematic review of demographic and health surveys: data availability and utilization for research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Fabic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno type="DOI">10.2471/BLT.11.095513</idno>
	</analytic>
	<monogr>
		<title level="j">Bull. World Health Organ</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="604" to="612" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1020249912095</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learn</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="95" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Low-variance black-box gradient estimates for the plackett-luce distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gadetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Struminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth {AAAI} Conference on Artificial Intelligence AAAI Press</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10126" to="10135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Datasheets for datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<idno>arxiv:1803.09010</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Res. Repository</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The case for evaluating causal models using interventional measures and empirical data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gentzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural Information Processing Systems 32</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11722" to="11732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.3389/fgene.2019.00524</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Genet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Synthetic data generation with probabilistic Bayesian networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gogoshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branciamore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rodin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Grünwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grunwald</surname></persName>
		</author>
		<title level="m">The minimum description length principle</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliferis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<idno type="DOI">10.4018/978-1-7998-5781-5.ch007</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Feature Selection</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="63" to="82" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Independence, invariance and the causal Markov condition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<idno type="DOI">10.1093/bjps/50.4.521</idno>
	</analytic>
	<monogr>
		<title level="j">Br. J. Philos. Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="521" to="583" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning Bayesian networks: the combination of knowledge and statistical data. Machine Learn</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022623210503</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Closing the gap on institutional delivery in northern India: a case study of how integrated machine learning approaches can enable precision public health</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blanchard</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjgh-2020-002340</idno>
	</analytic>
	<monogr>
		<title level="j">BMJ Global Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2340</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random generation of Bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian symposium on artificial intelligence</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="366" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goudet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02278</idno>
		<title level="m">Causal discovery toolbox: uncover causal relationships in python</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple indicator cluster surveys: delivering robust data on children and women across the globe</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hancioglu</surname></persName>
		</author>
		<idno type="DOI">10.1111/sifp.12103</idno>
	</analytic>
	<monogr>
		<title level="j">Stud. Fam. Plann</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="279" to="286" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Precision public health for the era of precision medicine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iademarco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Riley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.amepre.2015.08.031</idno>
	</analytic>
	<monogr>
		<title level="j">Am. J. Prev. Med</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="398" to="401" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The sensitivity of counterfactual fairness to unmeasured confounding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 35th Uncertainty in Artificial Intelligence Conference</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Editors</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><surname>Gogate</surname></persName>
		</editor>
		<meeting>The 35th Uncertainty in Artificial Intelligence Conference<address><addrLine>Tel Aviv; Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2020. July 2019</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
	<note>PMLR: Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A review of causal inference for biomedical informatics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hripcsak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2011.07.001</idno>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1102" to="1112" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient sampling and structure learning of Bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moffa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bayesian Networks in Healthcare: the chasm between research enthusiasm and clinical adoption</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kyrimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>medRxiv</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable modeling of real graphs using kronecker multiplication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International conference on machine learning</title>
		<meeting>the 24th International conference on machine learning<address><addrLine>Corvallis, OR; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007-06">2007. June 2007</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining<address><addrLine>Chicago, IL; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005-08">2005. August 2005</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revealing the complexity of health determinants in resource-poor settings</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Mccormick</surname></persName>
		</author>
		<idno type="DOI">10.1093/aje/kws183</idno>
	</analytic>
	<monogr>
		<title level="j">Am. J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="1051" to="1059" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05859</idno>
		<title level="m">Causal reasoning for algorithmic fairness</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1080/00401706.1995.10484383</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Stat. Classification</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="298" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Bayesian network model to explore practice change by smallholder rice farmers in Lao pdr</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thephavanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thammavong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sodahak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khounsy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.agsy.2018.04.004</idno>
	</analytic>
	<monogr>
		<title level="j">Agric. Syst</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="84" to="94" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pakistan social and living standards measurement survey (PSLM) 2018-19 national/provincial (social report)</title>
		<ptr target="https://www.pbs.gov.pk/content/pakistan-social-and-living-standards-measurement" />
	</analytic>
	<monogr>
		<title level="m">Government of Pakistan</title>
		<imprint>
			<publisher>Pakistan Bureau of Statistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">From Bayesian networks to causal networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="157" to="182" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference. 2nd Edn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sample size calculation in medical studies</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pourhoseingholi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahimzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterol. Hepatol. Bed Bench</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluation of causal structure learning methods on mixed data types</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Benos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 ACM SIGKDD workshop on causal disocvery</title>
		<meeting>2018 ACM SIGKDD workshop on causal disocvery<address><addrLine>London, United Kingdom; London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">2018. August 2018</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="48" to="65" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Causal learning from predictive modeling for observational data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdata.2020.535976</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">535976</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring the interlinkages of water and sanitation across the 2030 agenda: a bayesian network approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Raqeujo-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gine-Garriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez-Foguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International sustainable development research society conference</title>
		<meeting><address><addrLine>Messina, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-13">2018. June 13, 2018-July 15, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/63.3.581</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214504000001880</idno>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Estimating the Dimension of a Model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
	</analytic>
	<monogr>
		<title level="j">Annals Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0908.3817</idno>
		<title level="m">Learning bayesian networks with the bnlearn r package</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dirichlet bayesian network scores and the maximum relative entropy principle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="337" to="362" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Who learns better bayesian network structures: Accuracy and speed of structure learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Graafland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gutiérrez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijar.2019.10.003</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="235" to="253" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The case for causal AI</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sgaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stanford social innovation review (summer issue)</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Stanford Social Information Review</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Contrastive examples for addressing the tyranny of the majority</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06524</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Myllymaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5293</idno>
		<title level="m">On sensitivity of the map Bayesian network structure to the equivalent sample size parameter</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Quotient normalized maximum likelihood criterion for learning Bayesian network structures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leppä-Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jääsaari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Playa Blanca, Lanzarote</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
			<biblScope unit="page" from="948" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bringing greater precision to interactions between community health workers and households to improve maternal and newborn health outcomes in India</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smittenaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Engl</surname></persName>
		</author>
		<idno type="DOI">10.9745/GHSP-D-20-00027</idno>
	</analytic>
	<monogr>
		<title level="j">Global Health Sci. Practice</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="358" to="371" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search. Adaptive computation and machine learning. 2nd Edn</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bayesian network reconstruction using systems genetics data: comparison of mcmc methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sauerwine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toyoshiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gaiteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Neto</surname></persName>
		</author>
		<idno type="DOI">10.1534/genetics.114.172619</idno>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="973" to="989" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Communicating uncertainty about facts, numbers and science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Bles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Galvao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zaval</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.181870</idno>
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">181870</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Intersection-validation: a method for evaluating structure learning without ground truth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Viinikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eggeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on artificial intelligence and statistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the twenty-first international conference on artificial intelligence and statistics<address><addrLine>Playa Blanca, Lanzarote; Playa Blanca, Lanzarote, Canary Islands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">2018. April 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1570" to="1578" />
		</imprint>
		<respStmt>
			<orgName>PMLR Proceedings of Machine Learning Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A simulation-based approach to Bayesian sample size determination for performance under a given model and for separating models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelfand</surname></persName>
		</author>
		<idno type="DOI">10.1214/ss/1030550861</idno>
	</analytic>
	<monogr>
		<title level="j">Qual. Eng</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="505" to="508" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;smallworld&apos;networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
		<idno type="DOI">10.1038/30918</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Routing of multipoint connections</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Waxman</surname></persName>
		</author>
		<idno type="DOI">10.1109/49.12889</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Selected Areas Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1617" to="1622" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Learning data set similarities for hyperparameter optimization initializations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="DOI">10.5555/3053836.3053842</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Baicis: a novel Bayesian network structural learning algorithm and its comprehensive performance evaluation against open-source software</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Narain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Akmaev</surname></persName>
		</author>
		<idno type="DOI">10.1089/cmb.2019.0210</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="698" to="708" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
