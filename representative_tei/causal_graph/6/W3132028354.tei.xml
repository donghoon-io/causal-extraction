<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiased Graph Neural Networks with Agnostic Label Selection Bias</title>
				<funder ref="#_VcRkpsr">
					<orgName type="full">Zhejiang Province Natural Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">CAST</orgName>
				</funder>
				<funder>
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
				<funder ref="#_Pf5Z4Xn #_cZfYKfG #_Np6TdX5 #_7DKaCKA #_SbZhPtc #_nPpNGGp">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_bnt8rdq">
					<orgName type="full">BUPT Excellent Ph.D. Students Foundation</orgName>
				</funder>
				<funder ref="#_6N4jpNk">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-01-25">25 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaohua</forename><surname>Fan</surname></persName>
							<email>fanshaohua@bupt.edu</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<email>xiaowang@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
							<email>kunkuang@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Nian</forename><surname>Liu</surname></persName>
							<email>nianliu@bupt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
							<email>wangbai@bupt.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Trustworthy Distributed Computing and Service (MoE)</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Debiased Graph Neural Networks with Agnostic Label Selection Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-25">25 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.07708v2[cs.LG]</idno>
					<note type="submission">received 05 Feb. 2021; revised 02 Sep. 2021; accepted 29 Dec 2021.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Casual Inference</term>
					<term>Selection Bias</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing Graph Neural Networks (GNNs) are proposed without considering the selection bias in data, i.e., the inconsistent distribution between the training set with test set. In reality, the test data is not even available during the training process, making selection bias agnostic. Training GNNs with biased selected nodes leads to significant parameter estimation bias and greatly impacts the generalization ability on test nodes. In this paper, we first present an experimental investigation, which clearly shows that the selection bias drastically hinders the generalization ability of GNNs, and theoretically prove that the selection bias will cause the biased estimation on GNN parameters. Then to remove the bias in GNN estimation, we propose a novel Debiased Graph Neural Networks (DGNN) with a differentiated decorrelation regularizer. The differentiated decorrelation regularizer estimates a sample weight for each labeled node such that the spurious correlation of learned embeddings could be eliminated. We analyze the regularizer in causal view and it motivates us to differentiate the weights of the variables based on their contribution on the confounding bias. Then, these sample weights are used for reweighting GNNs to eliminate the estimation bias, thus help to improve the stability of prediction on unknown test nodes. Comprehensive experiments are conducted on several challenging graph datasets with two kinds of label selection biases. The results well verify that our proposed model outperforms the state-of-the-art methods and DGNN is a flexible framework to enhance existing GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>G Raph Neural Networks (GNNs) are powerful deep learn- ing algorithms on graphs with various applications <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Existing GNNs mainly learn a node embedding through aggregating the features from its neighbors, and such message-passing framework is supervised by the node label in an end-to-end manner. During this training procedure, GNNs will effectively learn the correlation between the structure patterns and node features with the node labels, so that GNNs are capable of learning the embeddings of new nodes and inferring their labels.</p><p>One basic requirement of GNNs making precise predictions on unseen test nodes is that the distribution of labeled training nodes and test nodes is the same, i.e., the structure and feature of labeled training and test nodes follow the similar pattern, so that the learned correlation between the current graph and labels can be well generalized to the new nodes. However, in reality, there are two inevitable issues. <ref type="bibr" target="#b0">(1)</ref> Because it is difficult to control the graph collection in an unbiased manner, the relationship between the collected real-world graph and the labeled nodes is inevitably biased. Training on such a graph will cause biased correlation with node labels. Taking a scientist collaboration network as an example, if most scientists with "machine learning" (ML) label collaborate with those with "computer vision" (CV) label, existing GNNs may learn spurious correlation, i.e., scientists who cooperate with CV scientists are ML scientists. If a new ML scientist only connects with ML scientists or the scientists in other areas, it will be probably misclassified. <ref type="bibr" target="#b1">(2)</ref> The test nodes in the real scenario are usually not available in the training phase, implying that the distribution of new nodes is agnostic. Once the distribution is inconsistent with that in the training nodes, the performance of all the current GNNs will be hindered. Even transfer learning is able to solve the distribution shift problem, however, it still needs the prior of test distribution, which actually cannot be obtained beforehand. Therefore, the agnostic label selection bias greatly affects the generalization ability of GNNs on unknown test data.</p><p>In order to observe selection bias in real graph data, we conduct an experimental investigation to validate the effect of selection bias on GNNs (see Section II-A). We select training nodes with different biased degrees for each dataset, making the distribution of training nodes and test nodes inconsistent. The results clearly show that selection bias drastically hinders the performance of GNNs on unseen test nodes. Moreover, with heavier bias, the performance drops more. Further, we theoretically analyze how the data selection bias results in the estimation bias in GNN parameters (see Section II-B). Based on the stable learning technique <ref type="bibr" target="#b4">[5]</ref>, we can assume that the learned embeddings consist of two parts: stable variables and unstable variables. The data selection bias will cause spurious correlation between these two kinds of variables. Thereby we prove that with the inevitable model misspecification, the spurious correlation will further cause the parameter estimation bias. Once the weakness of the current GNNs with selection bias is identified, one natural question is "how to remove the estimation bias in GNNs?"</p><p>In this paper, we propose a novel Debiased Graph Neural Network (DGNN) framework for stable graph learning by jointly optimizing a differentiated decorrelation regularizer and a weighted GNN model. Specifically, the differentiated decorrelation regularizer is able to learn a set of sample weights under differentiated variable weights, so that the spurious correlation between stable and unstable variables would be greatly eliminated. Based on the causal view analysis of the decorrelation regularizer, we theoretically prove that the weights of variables can be differentiated by the regression coefficients. Compared with existing decorrelation methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the proposed regularizer is able to remove the spurious correlation while maintaining a higher effective sample size and requiring less prior knowledge. Moreover, to better combine the decorrelation regularizer with existing GNN architecture, the theoretical result shows that adding the regularizer to the embeddings learned by the penultimate layer could be both theoretically sound and flexible. Then the sample weights learned by the decorrelation regularizer are used to reweight the GNN loss so that the parameter estimation could be unbiased.</p><p>In summary, the contributions of this paper are three-fold: i) We investigate a new problem of learning GNNs with agnostic label selection bias. The problem setting is general and practical for real applications. ii) We bring the idea of variable decorrelation into GNNs to relieve bias influence on model learning and propose a general framework DGNN that could be adopted to various GNNs. iii) We conduct experiments on real-world graph benchmarks with two kinds of agnostic label selection biases, and the experimental results demonstrate the effectiveness and flexibility of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EFFECT OF LABEL SELECTION BIAS ON GNNS</head><p>In this section, we first summarize the main notations used in this paper in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Investigation</head><p>We conduct an experimental investigation to examine whether the existing GNNs are sensitive to the distribution shifts caused by the label selection bias. One motivating example is that due to the research interests of the researcher that they are more likely to label the interdisciplinary documents that cite more papers from different subjects in a citation network, while testing may be conducted on nodes with any neighborhood distribution. Based on this example, the main idea of the experimental investigation is that we will perform two representative GNNs: GCN <ref type="bibr" target="#b1">[2]</ref> and GAT <ref type="bibr" target="#b2">[3]</ref> on three widely used graph datasets: Cora, Citeseer, Pubmed <ref type="bibr" target="#b6">[7]</ref> with different bias degrees. If the performance drops sharply comparing with the scenarios without selection bias, this will demonstrate that GNNs cannot generalize well in selection bias settings. To simulate the agnostic selection bias scenario, we first follow the inductive setting in <ref type="bibr" target="#b7">[8]</ref> that masks the validation and test nodes as the training graph G train in the training phase, and then infer the labels of validation and test nodes with whole graph G test . In this way, the distribution of test node can be considered agnostic. Following <ref type="bibr" target="#b8">[9]</ref>, we design a biased label selection method on training graph G train . The selection variable e is introduced to control whether the node will be selected as labeled nodes, where e = 1 means selected and 0 otherwise. For node i, we calculate its neighbor distribution ratio: r i = |{j|j ∈ N i , y j ≠ y i }|/|N i |, where N i is neighborhood of node i in G train and y j ≠ y i means the label of central node i is not the same as the label of its neighborhood node j. And r i measures the difference between the label of central node i with the labels of its neighborhood. Then we average all the nodes' r to get a threshold t. For each node, the probability to be selected is:</p><formula xml:id="formula_0">P (e i = 1|r i ) = r i ≥ t 1 - r i &lt; t ,</formula><p>where ∈ (0.5, 1) is used to control the degree of selection bias and a larger means a heavier bias. We set as {0.7, 0.8, 0.9} to get three bias degrees for each dataset, termed as Light, Medium, Heavy, respectively. We select 20 nodes for each class for training and the validation and test nodes are the same as <ref type="bibr" target="#b9">[10]</ref>. Furthermore, we take the unbiased datasets as baselines, where the labeled nodes are selected randomly. Figure <ref type="figure" target="#fig_0">1</ref> is the results of GCN, GAT and our proposed method, GCN/GAT-DVD, on these datasets with four bias degrees. We can find that: i) Compared with the unbiased scenario, when performing GCN/GAT on biased datasets, they suffer from serious performance decrease, indicating that selection bias greatly affects the GNNs' performance. ii) All lines decrease monotonically with the increase of bias degree, demonstrating that heavier biases will cause larger performance reduction. iii) GCN/GAT-DVD outperforms the corresponding base models (i.e., GCN/GAT) consistently and achieves larger improvements in heavier bias degree scenarios, indicating that our proposed method could relieve the effect of selection bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theoretical Analysis</head><p>The above experiment empirically verifies the effect of selection bias on GNNs. Here we theoretically analyze the effect of selection bias on estimating the parameters in GNNs. First, because biased labeled nodes have biased neighborhood structure and features, GNNs will encode this biased information into the node embeddings, which is validated by the experimental investigation. Based on stable learning technique <ref type="bibr" target="#b4">[5]</ref>, we make the following assumption: Assumption 1. The node embeddings learned by GNNs for each node can be decomposed as H = {S, V}, where S represents the stable variables and V represents the unstable variables. Specifically, for both training and test environments,</p><formula xml:id="formula_1">E(Y|S = s, V = v) = E(Y|S = s).</formula><p>Under Assumption 1, the distribution shift between training set and test set is mainly induced by the variation in the joint distribution over (S, V), i.e., P(S train , V train ) ≠ P(S test , V test ). However, there is an invariant relationship between stable variables S and outcome Y in both training and test environments, which can be expressed as P(Y train |S train ) = P(Y test |S test ). Assumption 1 can be guaranteed by Y⊥V|S. Thus, one can solve the stable prediction problem by developing a function g(⋅) based on S. However, one can hardly identify such variables in GNNs.</p><p>Without loss of generality, we take Y as a continuous variable for analysis and have the following assumption: Assumption 2. The true generation process of target variable Y contains not only the linear combination of stable variables S, but also the nonlinear transformation of stable variables.</p><p>Based on the above assumptions, we formalize the label generation process as follows:</p><formula xml:id="formula_2">Y = f (X, A) + ε = Sβ S + Vβ V + g( S) + ε,<label>(1)</label></formula><p>where S and V are latent stable variables and unstable variables to generate label Y, which can be learned by GNNs from raw graph data, β S and β V are the corresponding linear coefficients and they represent the effect of each latent variable on outcome Y, ε is the independent random noise, and g(⋅) is the nonlinear transformation function of stable variables.</p><p>According to Assumption 1, we know that coefficients of unstable variables V are actually 0 (i.e., β V = 0).</p><p>For a classical GNN model with a linear regression predictor, its prediction function can be formulated as:</p><formula xml:id="formula_3">Ŷ = Ĝ (X, A; θ g ) S βS + Ĝ (X, A; θ g ) V βV + ε,<label>(2)</label></formula><p>where Ĝ (X, A; θ g ) ∈ R N ×p denotes the node embeddings learned by a GNN, such as GCN and GAT, the output variables of Ĝ (X, A; θ g ) can be decomposed as stable variables Ĝ (X, A; θ g ) S ∈ R N ×m and unstable variables Ĝ (X, A; θ g ) V ∈ R N ×q (m + q = p) corresponding to S and V in Eq. ( <ref type="formula" target="#formula_2">1</ref>). Compared with Eq. ( <ref type="formula" target="#formula_2">1</ref>), we can find that the parameters of GNN model could be unbiasedly estimated if the nonlinear term g( S) = 0 (i.e., there does not exist any nonlinear relationship in the label generation process that cannot be learned by GNNs), because the GNN model in Eq. ( <ref type="formula" target="#formula_3">2</ref>) will have the same label generation mechanism as Eq. ( <ref type="formula" target="#formula_2">1</ref>). However, as common used GNNs only have several layers which may limit their nonlinear power and the real-world graph data is far more complicated, it is reasonable to assume that there is a nonlinear term g( S) ≠ 0 that cannot be fitted by the GNNs. Under this assumption, next, we taking a vanilla GCN <ref type="bibr" target="#b1">[2]</ref> as an example to illustrate how the distribution shift will induce parameter estimation bias. A two-layer GCN can be formulated as Âσ( ÂXW (0) )W (1) , where Â is the normalized adjacency matrix, W is the transformation matrix at each layer and σ(⋅) is the Relu activation function. We decompose GCN as two parts: one is embedding learning part Âσ( ÂXW (0) ), which can be decomposed as</p><formula xml:id="formula_4">[S T , V T ], corresponding to Ĝ (X, A; θ g ) S</formula><p>and Ĝ (X, A; θ g ) V in Eq. ( <ref type="formula" target="#formula_3">2</ref>), and the other part is W (1) , where the learned parameters can be decomposed as [ βS , βV ], corresponding to [ βS , βV ] in Eq. ( <ref type="formula" target="#formula_3">2</ref>). We aim at minimizing the least-square loss:</p><formula xml:id="formula_5">L GCN = n i=1 (S T i βS + V T i βV -Y i ) 2 . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>According to the derivation rule of partitioned regression model <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, with S = S and V = V, we have:</p><formula xml:id="formula_7">βV -β V = ( 1 n n i=1 V T i V i ) -1 ( 1 n n i=1 V T i g(S i )) + ( 1 n n i=1 V T i V i ) -1 ( 1 n n i=1 V T i S i )(β S -βS ),<label>(4)</label></formula><formula xml:id="formula_8">βS -β S = ( 1 n n i=1 S T i S i ) -1 ( 1 n n i=1 S T i g(S i )) + ( 1 n n i=1 S T i S i ) -1 ( 1 n n i=1 S T i V i )(β V -βV ),<label>(5)</label></formula><p>where n is labeled node size, S i is i-th sample of S,</p><formula xml:id="formula_9">1 n ∑ n i=1 V T i g(S i ) = E(V T g(S)) + o p (1), 1 n ∑ n i=1 V T i S i = E(V T S) + o p (1) and o p (1)</formula><p>is the error which is negligible.</p><p>Ideally, βVβ V = 0 indicates that there is no bias between the estimated and the real parameter. However, if E(V T S) ≠ 0 or E(V T g(S)) ≠ 0 in Eq. ( <ref type="formula" target="#formula_7">4</ref>), βV will be biased, leading to the biased estimation on βS in Eq. ( <ref type="formula" target="#formula_8">5</ref>) as well, i.e, the true effect of learned embeddings on label Y can not be estimated precisely. Since the correlation between V and S (or g(S))</p><p><ref type="foot" target="#foot_0">foot_0</ref> might shift in test phase, the biased parameters learned in training set is not the optimal parameters for predicting testing nodes. Therefore, to increase the stability of prediction, we need to unbiasedly estimate the parameters of βV by removing the correlation between V and S (or g(S)) on training graph, making</p><formula xml:id="formula_10">E(V T S) = 0 and E(V T g(S)) = 0. Note that 1 n ∑ n i=1 S T i g(S i</formula><p>) in Eq. ( <ref type="formula" target="#formula_8">5</ref>) can also cause estimation bias, but the relation between S and g(S) is stable across environments, which do not affect the stability to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL A. Revisiting on Variable Decorrelation in Causal View</head><p>To decorrelate V and S (or g(S)), we should decorrelate the output variables of Ĝ (X, A; θ g ). <ref type="bibr" target="#b4">[5]</ref> proposes a Variable Decorrelation (VD) term with sample reweighting technique to eliminate the correlation between each variable pair, in which the sample weights are learned by jointly minimizing the moment discrepancy between each variable pair:</p><formula xml:id="formula_11">L V D (H) = p j=1 ||H T .j Λ w H .-j /n -H T .j w/n ⋅ H T .-j w/n|| 2 2 ,<label>(6)</label></formula><p>where H ∈ R n×p means the variables needed to be decorrelated, i.e., Ĝ (X, A; θ g ) of GNNs, H .j is j-th variable of H, H .-j = H\H .j means all the remaining variables by setting the value of j-th variable in H as zero, w ∈ R n×1 is the sample weights,</p><formula xml:id="formula_12">∑ n i=1 w i = n and Λ w = diag(w 1 , ⋯, w n ) is the corresponding diagonal matrix. As we can see, L V D (H) can be reformulated as ∑ j≠k ||H T .j Λ w H .k /n-H T .j w/n⋅H T .k w/n|| 2 2 , and it aims to let E(H T .j H .k ) = E(H T .j )E(H .k</formula><p>) for each variable pair j and k. L V D (H) decorrelates all the variable pairs equally. However, decorrelating all the variables requires sufficient samples <ref type="bibr" target="#b4">[5]</ref>, i.e., n → ∞, which is hard to be satisfied, especially in the semi-supervised setting. In this scenario, we cannot guarantee L V D (H) = 0. Therefore, the key challenge is the difficulties of removing the spurious correlation that has the largest impact on the unbiased estimation when L V D (H) ≠ 0.</p><p>Inspired by confounding balancing technique in observational studies <ref type="bibr" target="#b11">[12]</ref>, we revisit the VD regularizer in causal view and show how to differentiate each variable pair. Confounding balancing techniques are often used for causal effect estimation of treatment T , where the distributions of confounders X are different between treated (T = 1) and control (T = 0) groups because of non-random treatment assignment. One could balance the distribution of confounders between treatment and control groups to unbiasedly estimate causal treatment effects <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Most balancing approaches exploit moments to characterize distributions, and balance them by adjusting sample weights w as follows: w = arg min</p><formula xml:id="formula_13">w || ∑ i∶T i =1 X i -∑ i∶T i =0 w i ⋅ X i || 2 2 .</formula><p>After balancing, the treatment T and the confounders X tend to be independent.</p><p>Given one targeted variable j, its decorrelation term,</p><formula xml:id="formula_14">L V D j = ||H T .j Λ w H .-j /n -H T .j w/n ⋅ H T .-j w/n|| 2 2 , is to make H .j independent from H .-j</formula><p>2 , which is same as the confounding balancing term making treatment and confounders independent. Thereby, L V D j can also be viewed as a confounding balancing term, where H .j is treatment and H .-j is confounders, illustrated in Fig. <ref type="figure" target="#fig_3">2(a)</ref>. Hence, our target can be explained as unbiased estimation of causal effect of each variable which is invariant across training and test set. As different variable may contribute unequally to the confounding bias, it is necessary to differentiate the confounders. The target of differentiating confounders exactly matches our target that removing the correlation of variables that has the largest impact on the unbiased estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Differetiated Variable Decorrelation</head><p>Considering a continuous treatment, the causal effect of treatment can be measured by Marginal Treatment Effect Function (MTEF) <ref type="bibr" target="#b15">[16]</ref>, and defined as:</p><formula xml:id="formula_15">M T EF = E[Y i (t)]-E[Y i (t-∆t)] ∆t</formula><p>, where Y i (t) represents the potential outcome of sample i with treatment status T = t, and ∆t denotes the increasing level of treatment. With the sample weights w decorrelating the treatment and the confounders, we can estimate the MTEF by:</p><formula xml:id="formula_16">M T EF = 1 n i ∑ i∶T i =t w i ⋅ Y i (T i ) -1 n j ∑ j∶T j =t-∆t w j ⋅ Y j (T j ) ∆t ,<label>(7)</label></formula><p>where n i and n j are the number of samples for two groups, respectively. Next, we theoretically analyze how to differentiate confounders' weights with the following theorem. Theorem 1. In observational studies, different confounders make unequal confounding bias on Marginal Treatment Effect Function (MTEF) with their own weights, and the weights can be learned via regressing outcome Y on confounders X and treatment variable T .</p><p>Proof. Recalling the Assumption 1, we rewrite the label generation process Eq. ( <ref type="formula" target="#formula_2">1</ref>) under MTEF setting as:</p><formula xml:id="formula_17">Y (t) = k ξ k X .k + γt + g t (S) + ε,<label>(8)</label></formula><p>2 Nonlinear relationships between variables can be incorporated by considering high-order moments in Eq. ( <ref type="formula" target="#formula_11">6</ref>), for example, a polynomial augmented function where α = [ξ, γ] are the linear coefficients, and g T =t (S) is the output of nonlinear transformation of the stable variables S when treatment T is t. Note that if T ∉ S, changing the value of T will not change the value of g t (S), T ∈ S otherwise.</p><formula xml:id="formula_18">f (H) = (H, H 2 , H .i H .j , H 3 , H .i H .j H .k , ⋯).</formula><p>Under above formulation, we write estimator of M T EF as:</p><formula xml:id="formula_19">M T EF = 1 n i ∑ i∶T i =t w i Y i (T i ) -1 n j ∑ j∶T j =t-∆t w j Y j (T j ) ∆t = 1 n i ∑ i∶T i =t w i (∑ k ξ k X ik + γt + g T =t (S i ) + ) ∆t - 1 n j ∑ j∶T j =t-∆t w j (∑ k ξ k X jk + γ(t -∆t) + g t-∆t (S i ) + ) ∆t = 1 n i ∑ i∶T i =t w i γt -1 n j ∑ j∶T j =t-∆t w j γ(t -∆t) ∆t + 1 n i ∑ i∶T i =t w i ∑ k ξ k X ik -1 n j ∑ j∶T j =t-∆t w j ∑ k ξ k X ik ∆t + 1 n i ∑ i∶T i =t w i g t (S i ) -1 n j ∑ j∶T j =t-∆t w j g t-∆t (S i ) ∆t + φ( ) = M T EF + k≠t ξ k ( ∑ i∶T i =t 1 n i w i X ik -∑ j∶T j =t-∆t 1 n j w j X jk ∆t ) + 1 n i ∑ i∶T i =t w i g t (S i ) -1 n j ∑ j∶T j =t-∆t w j g t-∆t (S i ) ∆t + φ( ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_20">1 n i ∑ i∶T i =t w i γt-1 n j ∑ j∶T j =t-∆t w j γ(t-∆t) ∆t</formula><p>is the ground truth of M T EF , φ(ε) means the noise term, and φ(ε) ≃ 0 with Gaussian noise. According to the last equation, to reduce the bias of M T EF , we need regu-late the term ∑ k ξ k (</p><formula xml:id="formula_21">1 n i ∑ i∶T i =t w i X ik -1 n j ∑ j∶T j =t-∆t w j X jk ∆t</formula><p>) and</p><formula xml:id="formula_22">1 n i ∑ i∶T i =t w i g t (S i )-1 n j ∑ j∶T j =t-∆t w j g t-∆t (S i ) ∆t</formula><p>, where the second term has the unknown term g T (S i ) so that we can only try to reduce the first term.</p><formula xml:id="formula_23">1 n i ∑ i∶T i =t w i X ik -1 n j</formula><p>∑ j∶T j =t-∆t w j X jk ∆t means the difference of the k-th confounder between treated and control samples. The parameter ξ k represents the confounding bias weight of the k-th confounder, and it is the coefficient of X .k . Moreover, because our target is to learn the weight of each variable pair, i.e., between treatment and each confounder, we also need to learn the weight γ of treatment T . Hence, according to Eq. ( <ref type="formula" target="#formula_17">8</ref>), the confounder weights and treatment weight can be learned by regressing observed outcome Y on confounders X and treatment T .</p><p>Due to the connection between treatment effect estimation with variable decorrelation as analyzed in Section III-A, we utilize Theorem 1 to reweight the variable weight in variable decorrelation term. When apply the Theorem 1 to GNNs, the confounders X should be H .-j and treatment T is H .j , where the embedding H is learned by Ĝ (X, A; θ g ) in Eq. <ref type="bibr" target="#b1">(2)</ref>. And the variable weights α is equal to the regression coefficients for H, i.e, β in Eq. <ref type="bibr" target="#b1">(2)</ref>. Then the Differentiated Variable Decorrelation (DVD) term can be formulated as follows:  all the sample weights to be 0. The term w ⪰ 0 constrains each sample weight to be non-negative. After variable reweighting, the weighted decorrelation term in Eq. ( <ref type="formula">10</ref>) can be rewritten as</p><formula xml:id="formula_24">min w L DV D (H) = p j=1 (α T ⋅ abs(H T .j Λ w H .-j /n -H T .j w/n ⋅ H T .-j w/n)) 2 + λ 1 n n i=1 w 2 i + λ 2 ( 1 n n i=1 w i -1) 2 , s.t.w ⪰ 0<label>(</label></formula><formula xml:id="formula_25">∑ j≠k α 2 j α 2 k ||H T .j Λ w H .k /n -H T .j w/n ⋅ H T .k w/n|| 2 2</formula><p>, and the weight for variable pair j and k would be α 2 j α 2 k , hence it considers both the weights of treatment and confounder. Then we derive the uniqueness property of w as follows: <ref type="formula">10</ref>) is unique.</p><formula xml:id="formula_26">Theorem 2 (Uniqueness). If λ 1 n ≫ p 2 + λ 2 , p 2 ≫ max(λ 1 , λ 2 ), |H i,j | ≤ c and |α i | ≤ c for some constant c, the solution ŵ ∈ {w ∶ |w i | ≤ c} to minimize Eq. (</formula><p>Proof. See Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Debiased GNN Framework</head><p>In this section, we describe the framework of Debiased GNN (DGNN) that incorporates VD/DVD term with GNNs in a seamless way. As analyzed in Section II-B, decorrelating Âσ( ÂXW (0) ) could make parameter estimation of GCN unbiased. However, most GNNs follow a layer-by-layer stacking architecture, and the output embedding of each layer is more easy to obtain in implementing. Since Âσ( ÂXW (0) ) is the aggregation of the first layer embedding σ( ÂXW (0) ),</p><p>decorrelating Âσ( ÂXW (0) ) may lack the flexibility that incorporates VD/DVD term with other GNN architectures. Fortunately, we have the following theorem to identify a more flexible way to combine variable decorrelation with GNNs.</p><p>Theorem 3. Given p pairwise uncorrelated variables Z = (Z 1 , Z 2 , ⋯, Z p ), with a linear aggregation operator Â, the variables of Y = ÂZ are still pairwise uncorrelated.</p><p>Proof.</p><formula xml:id="formula_27">Let Z = {Z 1 , Z 2 , ⋯, Z p } be p pairwise uncorre- lated variables. ∀Z i , Z j ∈ Z, (Z<label>(1)</label></formula><p>i , Z</p><p>i , ⋯, Z</p><formula xml:id="formula_29">i ) and (Z (1) j , Z<label>(n)</label></formula><p>j , ⋯, Z (n) j ) are n simple random samples drawn from Z i and Z j respectively, and have same distribution with Z i and Z j . Given a linear aggregation matrix Â = (a ij ),</p><formula xml:id="formula_31">∀s, v ∈ (1, 2, ⋯, n), let Y (s) i = ∑ n k=1 a sk Z (k) i and Y (v) j = ∑ n l=1 a vl Z (l)</formula><p>j , and we have following derivation:</p><formula xml:id="formula_32">Cov(Y (s) i , Y (v) j ) = Cov( n k=1 a sk Z (k) i , n l=1 a vl Z (l) j ) = n k=1 n l=1 a sk a vl Cov(Z (k) i , Z (l) j ) = n k=1 n l=1 a sk a vl δ ij ,</formula><p>where δ ij = 0 when i ≠ j, otherwise δ ij = 1. Therefore, when i ≠ j, we have Cov(Y</p><formula xml:id="formula_33">(s) i , Y (v) j ) = 0 and Cov(Y i , Y j ) = 0. Extended the conclusion to multiple variable, Y = (Y 1 , Y 2 , ⋯, Y n ) are pairwise uncorrelated.</formula><p>The theorem indicates that if the variables of embeddings Z are uncorrelated, after any form of linear neighborhood aggre-gation Â, e.g., average, attention or sum, the variables of transformed embeddings Y would be also uncorrelated. Therefore, decorrelating σ( ÂXW (0) ) can also reduce the estimation bias.</p><p>For a K layers of GNN, we can directly decorrelate the output of (K -1)-th layer, i.e., σ( Â⋯σ( ÂXW (0) )⋯W (K-2) ) for a K layers of GCN.</p><p>The previous analysis finds a flexible way to incorporate VD/DVD term with GNNs, however, recall that we analyze GNNs based on the least-squares loss in Eq. ( <ref type="formula" target="#formula_5">3</ref>), and most existing GNNs are designed for the classification task. Therefore, in the following, we analyze that the previous conclusions are still applicable in classification. We consider the cases that the softmax layer is used as the output layer of GNNs and loss is the cross-entropy error function. We use the Newton-Raphson update rule <ref type="bibr" target="#b16">[17]</ref> to bridge the gap between linear regression and multi-classification. According to the Newton-Raphson update rule, the update formula for transformation matrix W (K-1) of the last layer of GCN can be derived:</p><formula xml:id="formula_34">W (new) .j = W (old) .j -(H T RH) -1 H T (HW (old) .j -Y .j ) = (H T RH) -1 {H T RHW (old) .j -H T (HW (old) .j -Y .j )} = (H T RH) -1 H T Rz, (<label>11</label></formula><formula xml:id="formula_35">) where R kj = -∑ N n=1 H n W (old) .k (I kj -H n W (old)</formula><p>.j ) is a weighing matrix and I kj is the element of the identity matrix, and z = HW</p><formula xml:id="formula_36">(old) .j -R -1 (Y .j -W .j H</formula><p>) is an effective target value. Eq. ( <ref type="formula" target="#formula_34">11</ref>) takes the form of a set of normal equations for a weighted least-squares problem. As the weighing matrix R is not constant but depends on the parameter vector W</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(old)</head><p>.j , we must apply the normal equations iteratively. Each iteration uses the last iteration weight vector W (old) .j to calculate a revised weighing matrix R and regresses the target value z with HW (new) .j</p><p>. Therefore, the variable decorrelation can also be applied to the GNNs with softmax classifier to reduce the estimation bias in each iteration. Note that according to update formula Eq. ( <ref type="formula" target="#formula_34">11</ref>), we should calculate the inverse matrix (H T RH) -1 in each iteration, which requires high computation. In practice, we use gradient descent methods to approximate Newton-Raphson update rule and it works well in experiments.</p><p>Fig. <ref type="figure" target="#fig_3">2(b</ref>) is the framework of GNN-DVD, where we input the labeled nodes' embeddings H(K-1) into the regularizer L DV D ( H(K-1) ). As GCN has the formula softmax( ÂH (K-1) W (K-1) ), the variable weights of H(K-1)</p><formula xml:id="formula_37">used for differentiating L DV D ( H(K-1) ) can be computed from α = Var(W (K-1) , axis = 1)<label>(12)</label></formula><p>where Var(⋅, axis = 1) refers to calculating the variance of each row of some matrix and it reflects each variable's weight for classification which is similar to the regression coefficients. Note that when incorporating VD term with GNNs, we do not need compute the variable weights. Then the sample weights w learned by DVD term have the ability to remove the correlation in H(K-1) . Inspired by sample weighting methods <ref type="bibr" target="#b17">[18]</ref>, we propose to use this sample weights to reweight softmax loss:</p><formula xml:id="formula_38">min θ L G = l∈Y L w l ⋅ ln(q( H(K) l ) ⋅ Y l ),<label>(13)</label></formula><p>where q(⋅) is the softmax function, Y L is the set of labeled node indices and θ is the set of parameters of GCN. After reweighting nodes by these weights, we can create a pseudo-population where the biases in node neighborhood are effectively reduced, with which the off-the-shelf GNN models can achieve more accurate prediction under agnostic environments. The whole algorithm is summarized in Algorithm 1.</p><p>Algorithm To optimize our GNN-DVD algorithm, we propose an iterative method. Firstly, we let w = ω ⊙ ω to ensure nonnegativity of w and initialize sample weight ω i = 1 for each sample i and GNN's parameters θ with random uniform distribution. Once the initial values are given, in each iteration, we fix the sample weights ω and update the GNN's parameters θ by L G with gradient descent, then compute the confounder weights α from the linear transform matrix W (K-1) . With α and fixing the GNN's parameters θ, we update the sample weights ω with gradient descent to minimize L DV D (H (K-1) ).</p><p>We iteratively update the sample weights w and GNN's parameters θ until L G converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extension to GAT</head><p>We can easily incorporate the VD/DVD term with other GNNs. We combine them with GAT and more extensions leave as future work. GAT utilizes an attention mechanism to aggregate neighbor information. It also follows the linear aggregation and transformation steps. Similar to GCN, the hidden embedding H(K-1) is the input of VD/DVD term, and the variable weights α are calculated from the transformation matrix W (K-1) and the sample weights w are used to reweight the softmax loss. Note that the original paper utilizes the same transformation matrix W (K-1) for transforming embedding and learning attention values. Because α means the importance of each variable for classification, and it should be calculated from transformation matrix W (K-1) for transforming embedding, hence we use separate matrices for transforming embedding and learning attention values, respectively. This modification does not change the performance of GAT in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complexity Analysis</head><p>Compared with base models (e.g., GCN and GAT), the main incremental time cost is the complexity from VD/DVD term. For a training graph with n labeled nodes, we analyze the time complexity of the VD/DVD term in each iteration. For calculating the VD loss, its complexity is O(np , where E is the number of edges and c is the dimension of input node features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>In our paper, we propose to integrate two decorrelation terms (i.e., VD/DVD term) with GNN models to eliminate the estimation bias. Here we discuss the advantages and disadvantages of these two terms. VD term aggressively decorrelates all the variables learned by GNNs, however, it theoretically requires a large number of samples to achieve this goal. To overcome this dilemma, the DVD term is proposed to differentiate the variable weights in the VD term, aiming to remove the most unexpected correlation. However, due to the existence of unknown term g(S) in Eq. ( <ref type="formula" target="#formula_17">8</ref>), introducing more parameters to optimize may increase the instability of the model. Hence, when the number of labeled samples is large, performing GNN-VD may induce more stable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets</head><p>Here, we validate the effectiveness of our methods on node classification with two kinds of selection bias, i.e., label selection bias and small sample selection bias. For label selection bias, we employ three widely used graph datasets: Cora, Citeseer and Pubmed <ref type="bibr" target="#b6">[7]</ref>. As in Section II-A, we get three biased degrees as well as the original unbiased labeled nodes for each dataset. For small sample selection bias, we conduct the experiments on NELL dataset <ref type="bibr" target="#b18">[19]</ref>, where each class only has at most 1/5/10 labeled nodes for training. Due to the large scale of this dataset, the test nodes are easily to have distribution shifts from training nodes. The details of the datasets are summarized in Table <ref type="table" target="#tab_4">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We compare our proposed framework with several related baselines:</p><p>• Base models: GCN <ref type="bibr" target="#b1">[2]</ref> and GAT <ref type="bibr" target="#b2">[3]</ref> are classical GNN methods. We utilize them as the base models in our framework, so they are the most related baselines to validate the effectiveness of the proposed framework. • GNM-GCN/GAT <ref type="bibr" target="#b19">[20]</ref>: A GNN method which considers unbalanced label selection bias problem in transductive setting. They also utilize GCN/GAT as their base models.</p><p>• Chebyshev <ref type="bibr" target="#b1">[2]</ref>: It is a GCN-based method utilizing thirdorder Chebyshev filters.</p><p>• SGC <ref type="bibr" target="#b7">[8]</ref>: It is a simplified GCN-based method, which reduces the excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers.</p><p>• APPNP <ref type="bibr" target="#b20">[21]</ref>: It is one of the state-of-the-art GNN methods that combines PageRank with GCN.</p><p>• Planetoid <ref type="bibr" target="#b9">[10]</ref>: It is a classical semi-supervised graph embedding method. We use its inductive variant. • MLP: It is a two-layer multilayer perceptron trained on the labeled nodes with only node features as input.</p><p>• DGNN: It is the debiased GNN framework proposed in this paper. We incorporate the VD/DVD term with GCN/GAT under our proposed framework called GCN/GAT-VD/DVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Setup</head><p>As the Section II-A has described, for all datasets, to simulate the agnostic selection bias scenario, we first follow the inductive setting in <ref type="bibr" target="#b7">[8]</ref> that masks the validation and test nodes in the training phase and validation and test with the whole graph so that the test nodes will be agnostic. For GCN and GAT, we utilize the same two-layer architecture as their original paper <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. We use the following sets of hyperparameters for GCN on Cora, Citeseer, Pubmed: 0.5 (dropout rate), 5⋅10 -4 (L2 regularization) and 32 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 ⋅ 10 -5 (L2 regularization) and 64 (number of hidden units). For GAT on Cora, Citeseer, we use: 8 (first layer attention heads), 8 (features each head), 1 (second layer attention head), 0.6 (dropout), 5⋅10</p><p>-4 (L2 regularization); and for Pubmed: 8 (second layer attention head), 1 ⋅ 10 -3 (L2 regularization), other parameters are the same as Cora and Citeseer. To fair comparison, the GNN part of our model uses the same architecture and hyper-parameters with the base model and we grid search λ 1 and λ 2 from {0.01, 0.1, 1, 10, 100}. For other baselines, we use the optimal hyper-parameters in the original literatures on each dataset. For all the experiments, we run each model 10 times with different random seeds and report its average Accuracy results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Label Selection Bias Datasets</head><p>The results are given in Table <ref type="table" target="#tab_5">III</ref>, and we have the following observations. First, the proposed models (i.e., GCN/GAT with VD/DVD terms) always achieve the best performances in most cases, which well demonstrates that the effectiveness of our proposed debiased GNN framework. Second, comparing with base models, our proposed models all achieve up to 17.0% performance improvements, and gain larger improvements under heavier bias scenarios. Since the major difference between our model with base models is the VD/DVD regularizer, we can safely attribute the significant improvements to the effective decorrelation term and its seamless joint with GNN models. Third, GCN/GAT-DVD achieves better results than GCN/GAT-VD in most cases. It validates the importance and effectiveness of differentiating variables' weights in the semi-supervised setting. Moreover, our model still outperforms baselines in the unbiased setting. In real applications, it is hard to control the collection process without any distribution shift from the training set to the test set <ref type="bibr" target="#b22">[23]</ref>. Therefore, the problem we study is ubiquitous in reality and our method is effective in most scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on Small Sample Selection Bias Datasets</head><p>As NELL is a large-scale graph, we cannot run GAT on a single GPU with 16GB memory. We only perform GCN-VD/DVD and compare with representative methods which can perform on this dataset. The results are shown in Table <ref type="table" target="#tab_6">IV</ref>. First, GCN-VD/DVD achieves significant improvements over GCN. It indicates that selection bias could be induced by a small number of labeled nodes and our proposed method relieve the estimation bias. Moreover, with fewer labeled nodes, i.e., larger selection bias, our methods achieve larger improvements over base models. It further validates our method is an effective method against heavy bias. Moreover, GCN-DVD further improves GCN-VD with a large margin on NELL-1 dataset. It means that decorrelating all the variable pairs equally is suboptimal, and our differentiated strategy is effective when labeled nodes are scarce. With the number of labeled nodes increases, it may not necessary to differentiate the variable weights, but GCN-DVD achieves competitive results with GCN-VD and still outperforms the base model with a clear margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Sample Weight Analysis</head><p>Here we analyze the effect of sample weights w. We compute the amount of correlation in the labeled nodes' embeddings H(K-1) learned by standard GCN and the weighted embeddings of the same layer learned by GCN-DVD. Note that, the weights are the last iteration of sample weights of GCN-DVD. Following <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, the amount of correlation of GCN and GCN-DVD is measured by Frobenius norm of crosscorvairance matrix</p><formula xml:id="formula_39">||C|| 2 F computed from variables of H(K-1)</formula><p>and weighted H(K-1) respectively, where C ij represents the   covariance between pairwise variable i and j and the main diagonal of C is set as zero vector. Figure <ref type="figure" target="#fig_4">3</ref> shows the amount of correlation in unweighted and weighted embeddings, and we observe that the embeddings' correlations in all datasets are reduced, indicating that the weights learned by GCN-DVD can reduce the correlation between embedded variables. Moreover, as it is hard to reduce the correlation to zero, the necessity of differentiating variables' weights is further validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Parameter sensitivity</head><p>We study the sensitiveness of parameters and report the results of GCN-DVD on three citation networks in Fig. <ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref><ref type="figure" target="#fig_7">6</ref>. The experimental results show that GCN-DVD is relatively stable to λ 1 and λ 2 with wide ranges in most cases, indicating the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Training time per epoch</head><p>We report the results for the mean training time of GCN and GCN-DVD per epoch (forward pass, cross-entropy calculation, backward pass) for 200 epochs on Cora, Citeseer and Pubmed datasets, measured in seconds wall-clock time, in Table <ref type="table" target="#tab_7">V</ref>. These methods are performed on a RTX 3090 GPU Card. As we can see, the training time of GCN-DVD term has the same order of magnitude with GCN. More importantly, the training time of the DVD term will not be influenced by the base model we choose, i.e., when the base model is GAT, the running time of DVD term will not change. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORKS</head><p>In the past few years, Graph Neural Networks (GNNs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> have become the major technology to capture patterns encoded in the graph due to its powerful representation capacity. Recently, KPGNN <ref type="bibr" target="#b30">[31]</ref> applies GNNs to the social event detection task by preserving the incremental knowledge emerging in social data. MRFas-GCN <ref type="bibr" target="#b31">[32]</ref> integrates GCN with a Markov Random Fields (MRF) model to deal with the semi-supervised community detection problem. Not only pursuing the performance of GNNs   on clean data, <ref type="bibr" target="#b32">[33]</ref> proposes an exploratory adversarial attack method, called EpoAtk, to test whether existing GNNs are robust with adversarial perturbations on graphs. Although the current GNNs have achieved great success, when applied to the inductive setting, they all assume that training nodes and test nodes follow the same distribution. However, this assumption does not always hold in real applications. GNM <ref type="bibr" target="#b19">[20]</ref> first pays attention to the label selection problem on graph learning, and it learns an IPW estimator to estimate the probability of each node to be selected and uses this probability to reweight the labeled nodes. However, it heavily relies on the accuracy of the IPW estimator, which depends on the label assignment distribution of the whole graph, hence it is more suitable for the transductive setting.</p><p>To enhance the stability in unseen varied distributions, some literatures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b33">[34]</ref> have revealed the connection between correlation and prediction stability under model misspecification. Moreover, a kind of literatures <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> have studied the problem of removing the features correlation effect in neural networks, which brings great benefits for deep neural networks. However, these methods are built on simple regressions or regular neural networks such as CNNs, but GNNs have more complex architectures and properties needed to be considered. We also notice that <ref type="bibr" target="#b5">[6]</ref> propose a differentiated variable decorrelation term for linear regression. However, this decorrelation term requires multiple environments with different correlations between stable variables and unstable variables available in the training stage while our method does not require this prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we investigate a general and practical problem: learning GNNs with agnostic label selection bias. The selection bias will inevitably cause the GNNs to learn the biased correlation between aggregation mode and class label and make the prediction unstable. We propose a novel debiased GNN framework, which combines the decorrelation technique with GNNs in a unified framework. Extensive experiments well demonstrate the effectiveness and flexibility of DGNN. </p><formula xml:id="formula_40">F(w) = L 1 + λ 1 L 1 + λ 2 L 2 .</formula><p>We first calculate the Hessian matrix of F(w), denoted as H e , to prove the uniqueness of the optimal solution ŵ, as follows:</p><formula xml:id="formula_41">H e = ∂ 2 L 1 ∂w 2 + λ 1 ∂ 2 L 2 ∂w 2 + λ 2 ∂ 2 L 3</formula><p>∂w 2 For the term L 1 , we can rewrite it as:</p><formula xml:id="formula_42">L 1 = j≠k α 2 i α 2 k ( 1 n n i=1 H i,j H i,k w i -( 1 n n i=1 H i,j w i )( 1 n n i=1 H i,k w i )) 2 = j≠k α 2 i α 2 k ((<label>1</label></formula><formula xml:id="formula_43">n n i=1 H i,j H i,k w i ) 2 -(<label>2</label></formula><formula xml:id="formula_44">n n i=1 H i,j H i,k w i )( 1 n n i=1 H i,j w i )( 1 n n i=1 H i,k w i ) + ((<label>1</label></formula><formula xml:id="formula_45">n n i=1 H i,j w i )( 1 n n i=1 H i,k w i )) 2 )</formula><p>And when |H i,j | ≤ c, for any variable j and k, and </p><formula xml:id="formula_46">|w i | ≤ c,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Effect of selection bias on GCN and GAT.</figDesc><graphic coords="3,96.66,61.05,129.60,79.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: (a) Diagram of decorrelating node embeddings with confounding balance. H(K-1) is the node embedding matrix to be decorrelated. T is the treatment variable, corresponding to one target variable in H(K-1) . X means the confounders, corresponding to the remaining variables of the target variable in H(K-1) . Y is the outcome, corresponding to labels. (b) The framework of GNN-DVD. The same color in the two figures represents the same kind of variable.</figDesc><graphic coords="5,308.42,159.76,113.89,82.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b9">10)</ref> where abs(⋅) means the element-wise absolute value operation, preventing positive and negative values from eliminating. Term</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 i</head><label>2</label><figDesc>is added to reduce the variance of sample weights to achieve stability, and the formula λ2 ( 1 n ∑ n i=1 w i -1) 2 avoids</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Embedding correlation analysis on unweighted and weighted GCN.</figDesc><graphic coords="9,125.46,335.31,113.39,74.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Accuracy of GCN-DVD with different λ 1 and λ 2 on different biased Cora datasets.</figDesc><graphic coords="10,101.16,209.97,129.59,93.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Accuracy of GCN-DVD with different λ 1 and λ 2 on different biased Citeseer datasets.</figDesc><graphic coords="10,101.15,358.88,129.59,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Accuracy of GCN-DVD with different λ 1 and λ 2 on different biased Pubmed datasets.</figDesc><graphic coords="10,381.26,358.90,129.60,93.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table I and then formulate our target problem: Problem 1 (Semi-supervised Learning on Graph with Agnostic Label Selection Bias). Given a training graph G train = {A train , X train , Y train }, where A train ∈ R</figDesc><table /><note><p>N ×N (N nodes) represents the adjacency matrix, X train ∈ R N ×D (D features) refers to the node feature vectors and Y train ∈ R n×C (n labeled nodes, C classes) refers to the available labels for training (n ≪ N ), the task is to learn a GNN g θ (⋅) with parameter θ to precisely predict the label of nodes on test graph G test = {A test , X test , Y test }, where distribution Ψ(G train ) ≠ Ψ(G test ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Glossary of Notations. The adjacency matrix of G train or G test X train/test The node feature vectors of G train or G test Y train/test The node label vectors of G train or G test H/ Ĝ (X, A; θ g ) Node embeddings matrix learned by GNNs</figDesc><table><row><cell cols="2">Notation Description</cell><cell></cell></row><row><cell>G train</cell><cell>Training graph</cell><cell></cell></row><row><cell>G test</cell><cell>Test graph</cell><cell></cell></row><row><cell>A train/test</cell><cell></cell><cell></cell></row><row><cell cols="2">S The stable variables in H</cell><cell></cell></row><row><cell>V</cell><cell cols="2">The unstable variables in H</cell></row><row><cell cols="3">S The latent stable variables to generate Y</cell></row><row><cell>V</cell><cell cols="2">The unstable variables to generate label Y</cell></row><row><cell>βS</cell><cell cols="2">The linear coefficients for S</cell></row><row><cell>βV</cell><cell cols="2">The linear coefficients for V</cell></row><row><cell cols="3">g(⋅) The non-linear transformation for stable variables S</cell></row><row><cell>β S β V</cell><cell>The linear coefficients for The linear coefficients for</cell><cell>S V</cell></row><row><cell>T</cell><cell>Treatment variable</cell><cell></cell></row><row><cell cols="2">X Confounders</cell><cell></cell></row><row><cell>w</cell><cell>Sample weights</cell><cell></cell></row><row><cell>α</cell><cell cols="2">Variable weights in DVD term</cell></row><row><cell>ξ</cell><cell cols="2">The linear coefficients for confounders X</cell></row><row><cell>γ</cell><cell cols="2">The linear coefficient for treatment T</cell></row><row><cell>Y</cell><cell></cell><cell></cell></row></table><note><p>i (t) The potential outcome of sample i with treatment t</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For updating w, the complexity is dominated by the step of calculating the partial gradients of the function L DV D (H) with respect to variable w. The complexity of</figDesc><table><row><cell>2 ), where p is</cell></row><row><cell>the dimension of embeddings. And for DVD loss, its complexity</cell></row><row><cell>is the same as VD, as the complexity of calculating variable</cell></row><row><cell>weights α is O(np), which is relatively small comparing with O(np 2 ). ∂L DV D (H) is O(np 2 ). In total, the complexity of each iteration ∂w for VD/DVD term in Algorithm 1 is O(np 2 ). And it is quite</cell></row><row><cell>smaller than the base models (e.g., the complexity of GCN is O(Ecp 2 )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Dataset statistics    </figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="3">Classes Features Bias degree ( )</cell><cell>Bias type</cell></row><row><cell>Cora</cell><cell>Citation network</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.7/0.8/0.9</cell><cell>Label selection bias</cell></row><row><cell>Citeseer</cell><cell>Citation network</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.7/0.8/0.9</cell><cell>Label selection bias</cell></row><row><cell cols="2">Pubmed Citation network</cell><cell cols="2">19,717 44,338</cell><cell>3</cell><cell>500</cell><cell>0.7/0.8/0.9</cell><cell>Label selection bias</cell></row><row><cell>NELL</cell><cell cols="4">Knowledge graph 65,755 266,144 210</cell><cell>5,414</cell><cell cols="2">1/5/10 labeled nodes per class Small sample selection bias</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Performance of three citation networks. The '*' indicates the best results of the baselines. Best results of all methods are indicated in bold. '% gain over GCN/GAT' means the improvement percent of GCN/GAT-DVD against GCN/GAT.</figDesc><table><row><cell>Method</cell><cell>Unbiased</cell><cell>Cora Light</cell><cell>Medium</cell><cell>Heavy</cell><cell>Unbiased</cell><cell cols="2">Citeseer Light Medium</cell><cell>Heavy</cell><cell>Unbiased</cell><cell cols="3">Pubmed Light Medium</cell><cell>Heavy</cell></row><row><cell>MLP</cell><cell>0.5296</cell><cell>0.5624</cell><cell>0.5197</cell><cell>0.5087</cell><cell>0.5438</cell><cell>0.4532</cell><cell>0.3757</cell><cell>0.3893</cell><cell>0.6914</cell><cell>0.6852</cell><cell></cell><cell>0.6620</cell><cell>0.6378</cell></row><row><cell>Planetoid [10]</cell><cell>0.6650</cell><cell>0.5890</cell><cell>0.5240</cell><cell>0.5180</cell><cell>0.6720</cell><cell>0.5160</cell><cell>0.5140</cell><cell>0.4880</cell><cell>0.744</cell><cell>0.7160</cell><cell></cell><cell>0.6770</cell><cell>0.6680</cell></row><row><cell>Chebyshev [22]</cell><cell>0.7407</cell><cell>0.7116</cell><cell>0.7006</cell><cell>0.6809</cell><cell>0.7232  *</cell><cell>0.6542</cell><cell>0.6276</cell><cell>0.5920</cell><cell>0.7450</cell><cell>0.7358</cell><cell></cell><cell>0.6862</cell><cell>0.6732</cell></row><row><cell>SGC [8]</cell><cell>0.779</cell><cell>0.7800</cell><cell>0.7800</cell><cell>0.7530</cell><cell>0.724</cell><cell>0.6780</cell><cell>0.6730  *</cell><cell>0.6200</cell><cell>0.781</cell><cell>0.7880</cell><cell>*</cell><cell>0.7560</cell><cell>0.6800</cell></row><row><cell>APPNP [21]</cell><cell>0.8132  *</cell><cell>0.7913</cell><cell>0.7689</cell><cell>0.7629</cell><cell>0.6862</cell><cell>0.6478</cell><cell>0.6052</cell><cell>0.5903</cell><cell>0.7731</cell><cell>0.7639</cell><cell></cell><cell>0.7369</cell><cell>0.6862</cell></row><row><cell>GNM-GCN [20]</cell><cell>0.7594</cell><cell>0.7423</cell><cell>0.7531</cell><cell>0.7196</cell><cell>0.6054</cell><cell>0.5793</cell><cell>0.5717</cell><cell>0.5125</cell><cell>0.7654</cell><cell>0.7552</cell><cell></cell><cell>0.7381</cell><cell>0.7072</cell></row><row><cell>GNM-GAT [20]</cell><cell>0.7976</cell><cell>0.7875</cell><cell>0.7638</cell><cell>0.7404</cell><cell>0.6832</cell><cell>0.6524</cell><cell>0.6487</cell><cell>0.5865</cell><cell>0.7666</cell><cell>0.7438</cell><cell></cell><cell>0.7568</cell><cell>0.6891</cell></row><row><cell>GCN [2]</cell><cell>0.7909</cell><cell>0.7851</cell><cell>0.7775</cell><cell>0.7422</cell><cell>0.7075</cell><cell>0.6786</cell><cell>0.5952</cell><cell>0.5551</cell><cell>0.7845  *</cell><cell>0.7673</cell><cell></cell><cell>0.7545</cell><cell>0.7247  *</cell></row><row><cell>GCN-VD</cell><cell>0.7980</cell><cell>0.7951</cell><cell>0.7855</cell><cell>0.7522</cell><cell>0.7122</cell><cell>0.6844</cell><cell>0.6676</cell><cell>0.6408</cell><cell>0.7888</cell><cell>0.7727</cell><cell></cell><cell>0.7729</cell><cell>0.7399</cell></row><row><cell>GCN-DVD</cell><cell>0.7951</cell><cell>0.7959</cell><cell>0.7885</cell><cell>0.7555</cell><cell>0.7128</cell><cell>0.6908</cell><cell>0.6769</cell><cell>0.6496</cell><cell>0.7874</cell><cell>0.7741</cell><cell></cell><cell>0.7746</cell><cell>0.7542</cell></row><row><cell>% gain over GCN</cell><cell>0.53%</cell><cell>1.38%</cell><cell>1.41%</cell><cell>1.79%</cell><cell>0.75%</cell><cell>1.8%</cell><cell>14.2%</cell><cell>17.0%</cell><cell>0.37%</cell><cell>0.89%</cell><cell></cell><cell>2.67%</cell><cell>4.07%</cell></row><row><cell>GAT [3]</cell><cell>0.8100</cell><cell>0.8067  *</cell><cell>0.8019  *</cell><cell>0.7578</cell><cell>0.7224</cell><cell>0.7033  *</cell><cell>0.6683</cell><cell>0.6475  *</cell><cell>0.7714</cell><cell>0.7665</cell><cell></cell><cell>0.7579  *</cell><cell>0.7068</cell></row><row><cell>GAT-VD</cell><cell>0.8133</cell><cell>0.8146</cell><cell>0.8079</cell><cell>0.7708</cell><cell>0.7288</cell><cell>0.7149</cell><cell>0.6833</cell><cell>0.6611</cell><cell>0.7732</cell><cell>0.7783</cell><cell></cell><cell>0.7689</cell><cell>0.7149</cell></row><row><cell>GAT-DVD</cell><cell>0.8139</cell><cell>0.8179</cell><cell>0.8119</cell><cell>0.7694</cell><cell>0.7294</cell><cell>0.7172</cell><cell>0.6825</cell><cell>0.6627</cell><cell>0.7735</cell><cell>0.7788</cell><cell></cell><cell>0.7723</cell><cell>0.7210</cell></row><row><cell>% gain over GAT</cell><cell>0.48%</cell><cell>1.39%</cell><cell>1.26%</cell><cell>1.53%</cell><cell>0.97%</cell><cell>1.97%</cell><cell>2.12%</cell><cell>2.34%</cell><cell>0.27%</cell><cell>1.6%</cell><cell></cell><cell>1.9%</cell><cell>2.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of NELL. The '*' indicates the best results of the baselines. Best results of all methods are indicated in bold. 'Improvement' means the improvement percent of GCN-VD/DVD (selected better results) against GCN.</figDesc><table><row><cell>Dataset</cell><cell>MLP</cell><cell cols="2">Planetoid SGC</cell><cell>GCN</cell><cell cols="2">GCN-VD GCN-DVD Improvement</cell></row><row><cell>NELL-1</cell><cell cols="2">0.2385 0.3901</cell><cell cols="2">0.4128 0.4416  *</cell><cell>0.4652</cell><cell>0.4734</cell><cell>7.2%</cell></row><row><cell>NELL-5</cell><cell cols="2">0.4938 0.3519</cell><cell cols="2">0.6295 0.7030  *</cell><cell>0.7424</cell><cell>0.7361</cell><cell>5.6%</cell></row><row><cell cols="3">NELL-10 0.5838 0.5149</cell><cell cols="2">0.6275 0.7615  *</cell><cell>0.7734</cell><cell>0.7727</cell><cell>1.6%</cell></row><row><cell></cell><cell>(a) Cora</cell><cell></cell><cell></cell><cell>(b) Citeseer</cell><cell></cell><cell>(c) Pubmed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>The training time per epoch.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>GCN</cell><cell>1.29 × 10 -2</cell><cell>2.00 × 10 -2</cell><cell>1.11 × 10 -1</cell></row><row><cell>GCN-DVD</cell><cell>6.19 × 10 -2</cell><cell>8.46 × 10 -2</cell><cell>2.42 × 10 -1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>we have ∂ 2 ∂w 2 ( 1 n ∑ Therefore, if λ 1 n ≫ p 2 +λ 2 n 2 , equivalent to λ 1 n ≫ p 2 +λ 2 , H e isan almost diagonal matrix. Hence, H e is positive definite<ref type="bibr" target="#b37">[38]</ref>.Then the functionF(w) is convex on C = {w ∶ |w i | ≤ c},and has unique optimal solution ŵ. Moreover, because L 1 is our major decorrelation term, we hope L 1 to dominate the terms λ 1 L 2 and λ 2 L 3 . On C, we haveL 1 = O(1), L 2 = O(1), and α When p 2 ≫ max(λ 1 , λ 2 ), L 1 will dominate the regularization terms L 2 and L 3 .</figDesc><table><row><cell cols="7">With some algebras, we can also have</cell></row><row><cell></cell><cell cols="3">∂ ∂w 2 = 2 L 2</cell><cell cols="2">1 n I,</cell><cell>∂ ∂w 2 = 2 L 3</cell><cell>1 n 2 11 T ,</cell></row><row><cell>thus,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H e = O(</cell><cell>p n 2 ) + 2</cell><cell cols="3">λ 1 n I +</cell><cell cols="2">λ 2 n 2 11 T =</cell><cell>λ 1 n I + O(</cell><cell>p 2 + λ 2 n 2 ).</cell></row><row><cell cols="3">( 1 n ∑ n i=1 H i,j w i )( 1 n ∑ O(p 2 ).</cell><cell cols="4">2 i α k ( 1 2 n ∑ 2 = O(1). Thus L 1 = n i=1 H i,j H i,k w i -i=1 H i,k w i )) n</cell></row><row><cell>n i=1 H i,j H i,k w i ) i=1 H i,k w i ) n = O( 1 2 = O( 1 n 2 ), n 2 ) and i=1 H i,j H i,k w i )( 1 n n ∑ ∂w 2 ( 1 ∂ 2 i=1 H i,j w i )( 1 n ∑ ∂ 2 ∂w 2 (( 2 n ∑ n n ∑ n i=1 H i,j w i )( 1 n ∑ n i=1 H i,k w i )) = O( 1 Then with |α i | ≤ c, we n 2 ). have α 2 i α 2 k ∂ 2 ∂w 2 ( 1 n ∑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>n i=1 H i,j H i,k w i -( 1 n ∑ n i=1 H i,j w i )( 1 n ∑ n i=1 H i,k w i )) 2 = O( 1 n 2 ). L 1 is sum of p(p -1) such terms. Then we have ∂ 2 L 1 ∂w 2 = O( p 2 n 2 ).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We assume all variables are centered with zero mean. This assumption could be satisfied by adding a normalization layer after the learned embeddings.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work is partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U20B2045</rs>, <rs type="grantNumber">62192784</rs>, <rs type="grantNumber">62172052</rs>, <rs type="grantNumber">61772082</rs>, <rs type="grantNumber">62002029</rs>) and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">2021RC28</rs>). <rs type="person">Kun Kuang</rs> is supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62006207</rs>), <rs type="programName">Young Elite Scientists Sponsorship Program</rs> by <rs type="funder">CAST</rs> and <rs type="funder">Zhejiang Province Natural Science Foundation</rs> (No. <rs type="grantNumber">LQ21F020020</rs>). <rs type="person">Shaohua Fan</rs> is supported by <rs type="funder">BUPT Excellent Ph.D. Students Foundation</rs> (No. <rs type="grantNumber">CX2021311</rs>) and <rs type="funder">China Scholarship Council</rs>. We thank <rs type="person">Dr. Tianchi Yang</rs> for discussion on the proof of Theorem 3.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pf5Z4Xn">
					<idno type="grant-number">U20B2045</idno>
				</org>
				<org type="funding" xml:id="_cZfYKfG">
					<idno type="grant-number">62192784</idno>
				</org>
				<org type="funding" xml:id="_Np6TdX5">
					<idno type="grant-number">62172052</idno>
				</org>
				<org type="funding" xml:id="_7DKaCKA">
					<idno type="grant-number">61772082</idno>
				</org>
				<org type="funding" xml:id="_SbZhPtc">
					<idno type="grant-number">62002029</idno>
				</org>
				<org type="funding" xml:id="_6N4jpNk">
					<idno type="grant-number">2021RC28</idno>
				</org>
				<org type="funding" xml:id="_nPpNGGp">
					<idno type="grant-number">62006207</idno>
					<orgName type="program" subtype="full">Young Elite Scientists Sponsorship Program</orgName>
				</org>
				<org type="funding" xml:id="_VcRkpsr">
					<idno type="grant-number">LQ21F020020</idno>
				</org>
				<org type="funding" xml:id="_bnt8rdq">
					<idno type="grant-number">CX2021311</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stable learning via differentiated variable decorrelation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="2185" to="2193" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A property of partitioned generalized regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nurhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puntanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in statistics-theory and methods</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1579" to="1583" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hainmueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="46" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causal inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stable prediction across unknown environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven variable decomposition for treatment effect estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of the effect of a continuous treatment: a machine learning approach with an application to treatment for traumatic brain injury</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kreif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grieve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health economics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1213" to="1228" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counterfactual prediction for bundle treatment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph-based semi-supervised learning with non-ignorable non-response</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decorrelated clustering with data selection bias</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bitegcn: A new gcn architecture via bidirectional convolution of topology and features on text-rich networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WSDM</publisher>
			<biblScope unit="page" from="157" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metapathguided heterogeneous graph neural network for intent recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2478" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One2multi graph autoencoder for multi-view graph clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3070" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning backtrackless aligned-spatial graph convolutional networks for graph classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Knowledgepreserving incremental social event detection via heterogeneous gnns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="3383" to="3395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph convolutional networks meet markov random fields: Semi-supervised community detection in attribute networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="152" to="159" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploratory adversarial attacks on graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1136" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stable learning via sample reweighting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5692" to="5699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decorrelation of neutral vector variables: Theory and applications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularizing cnns with locally constrained decorrelations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Roca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Removing the feature correlation effect of multiplicative noise</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">He is a fourth-year Ph.D. student in the Department of Computer Science of Beijing University of Posts and Telecommunications and currently works as a visiting student at MILA. His main research interests including graph mining and causal machine learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakatsukasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shaohua Fan received the B.E. degree in 2015 from Northeastern University and M.S. degree in 2018 from Beijing University of Posts and Telecommunications</title>
		<title level="s">Linear Algebra and its Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page" from="242" to="248" />
		</imprint>
	</monogr>
	<note>He has published several papers in major international conferences, including KDD, WWW, IJCAI, and CIKM etc</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">He is a professor and deputy director of Beijing Key Lab of Intelligent Telecommunications Software and Multimedia at present. His research interests are in data mining, machine learning, and evolutionary computing</title>
		<imprint>
			<date type="published" when="2007">2016. 2007</date>
			<pubPlace>Tianjin, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiao Wang is an Associate Professor in the School of Computer Science, Beijing University of Posts and Telecommunications ; Computer Science and Technology, Tianjin University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include data mining, social network analysis, and machine learning. Until now, he has published more than 70 papers in refereed journals and conferences. He has published more than 100 papers in refereed journals and conferences</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He was a visiting scholar with Prof. Susan Athey&apos;s Group at Stanford University. His main research interests include Causal Inference, Artificial Intelligence, and Causally Regularized Machine Learning</title>
	</analytic>
	<monogr>
		<title level="m">He has published over 40 papers in major international journals and conferences, including SIGKDD, ICML, ACM MM, AAAI, IJCAI, TKDE, TKDD, Engineering, and ICDM</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Kun Kuang received his Ph.D. degree from Tsinghua University</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Zhejiang University ; Computer Science of Beijing University of Posts and Telecommunications ; Xian Jiaotong University, Xian, China and Ph.D. degree from the Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note>She was the director of Beijing Key Lab of Intelligent Telecommunications Software and Multimedia</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
