<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiased Multimodal Understanding for Human Language Sequences</title>
				<funder ref="#_2a87J5j">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-13">13 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
							<email>dkyang20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinjie</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Metaverse &amp; Intelligent Medicine</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cognition and Intelligent Technology Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Debiased Multimodal Understanding for Human Language Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-13">13 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.05025v3[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human multimodal language understanding (MLU) is an indispensable component of expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviours. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MLU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, limiting performance and generalizability across new subjects. Motivated by this observation, we introduce a recapitulative causal graph to formulate the MLU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective causal intervention module to disentangle the impact of subjects acting as unobserved confounders and achieve model training via true causal effects. As a plug-and-play component, SuCI can be widely applied to most methods that seek unbiased predictions. Comprehensive experiments on several MLU benchmarks clearly show the effectiveness of the proposed module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As a research hotspot that combines linguistic and nonverbal behaviours (e.g., visual and acoustic modalities), human multimodal language understanding (MLU) has attracted significant attention from computer vision <ref type="bibr">(Yang et al. 2022a</ref>), natural language processing <ref type="bibr" target="#b32">(Tian et al. 2022)</ref>, and speech recognition communities <ref type="bibr">(Yang et al. 2022b,d)</ref> in recent years. Thanks to the progressive development of multimodal language benchmarks <ref type="bibr" target="#b8">(Hasan et al. 2019;</ref><ref type="bibr" target="#b51">Zadeh and Pu 2018;</ref><ref type="bibr" target="#b52">Zadeh et al. 2016)</ref>, extensive studies <ref type="bibr" target="#b29">(Rahman et al. 2020;</ref><ref type="bibr" target="#b7">Han, Chen, and Poria 2021;</ref><ref type="bibr" target="#b21">Liu et al. 2018;</ref><ref type="bibr" target="#b49">Yu et al. 2021;</ref><ref type="bibr" target="#b33">Tsai et al. 2019;</ref><ref type="bibr">Liang et al. 2021b;</ref><ref type="bibr" target="#b22">Lv et al. 2021;</ref><ref type="bibr" target="#b27">Pham et al. 2019;</ref><ref type="bibr" target="#b30">Sun et al. 2022;</ref><ref type="bibr" target="#b13">Lei et al. 2023</ref>) have presented impressive multimodal models on training data containing distinct subjects, diverse topics, and different modalities. Despite the achievements of previous approaches by exploiting representation learning architectures <ref type="bibr">(Yang et al. 2022a;</ref><ref type="bibr">Liang et al. 2021b</ref>) and fusion strategies <ref type="bibr" target="#b33">(Tsai et al. 2019;</ref><ref type="bibr" target="#b22">Lv et al. 2021)</ref>, they invariably suffer from a prediction bias when applied to testing samples of new subjects.</p><p>The harmful prediction bias is mainly caused by the subject variation problem. Specifically, different subjects' expression styles and behaviours (e.g., facial expressions or acoustic information) from the training data are highly idiosyncratic in social communication, affected by the subjects' customs or culture <ref type="bibr" target="#b15">(Li and Deng 2020)</ref>. Once welldesigned models are trained on such data, subject-specific semantic correlations (e.g., particular facial action unit cooccurrence <ref type="bibr" target="#b2">(Chen et al. 2022</ref>)) would inexorably affect performance and generalizability. Worse still, the spurious connections between the trained models and specific subjects will be transmitted via multiple modalities when the data paradigm is extended from isolated to multimodal situations. Recall the prominent MLU benchmarks (e.g., MOSI <ref type="bibr" target="#b52">(Zadeh et al. 2016)</ref> for sentiment analysis or UR FUNNY <ref type="bibr" target="#b8">(Hasan et al. 2019</ref>) for humor detection), whose collectors advocated video-level data splitting so that segments from the same video will not appear across train, valid, and test splits. Although the trained models may avoid memorizing the average affective state of a subject <ref type="bibr">(Liang et al. 2021a)</ref>, they cannot generalize well across new subjects. The examples in Figure <ref type="figure" target="#fig_0">1</ref> provide strong evidence of this. Concretely, subjects 1, 2, and 3 tend to use sentimentally unimportant words "but" and "just" to express negative emotions. In this case, the MLU model <ref type="bibr" target="#b15">(Li, Wang, and Cui 2023)</ref> is misled to focus on spurious clues from the textual utterances of the subjects and make an entirely incorrect prediction when applied to subject 4. Similar observations are found in the visual and audio modalities. For instance, the trained model erroneously takes subject-specific facial appearances (e.g., "grimace and pursed mouth" from subjects 1 and 2) and acoustic behaviours (e.g., "agitated tone" from subjects 2 and 3) as semantic shortcuts to infer frustrating negative sentiment.</p><p>Motivated by the above observations, this paper aims to improve MLU methods by causal demystification rather than beating them. We propose a Subject Causal Intervention module (SuCI) to disentangle the impact of semantic differences among subjects from multimodal expressions. Specifically, we first formulate a universal causal graph to analyze the MLU procedure and interpret the causalities To sum up, this paper has the following contributions. (i) We investigate the subject variation problem in MLU tasks via a tailored causal graph and identify the subjects as confounders which misleads the models to capture subject-specific spurious correlations and cause prediction bias. (ii) Based on the causal theory of backdoor adjustment, we present SuCI, a subject causal intervention module to remove prediction bias and confounding effects of subjects. (iii) We evaluate the effectiveness of SuCI on several MLU benchmarks. Experimental results show that SuCI can significantly and consistently improve existing baselines, achieving new SOTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Human Multimodal Language Understanding. Benefiting from available human communication resources and data, MLU benchmarks <ref type="bibr" target="#b51">(Zadeh and Pu 2018;</ref><ref type="bibr" target="#b52">Zadeh et al. 2016;</ref><ref type="bibr" target="#b8">Hasan et al. 2019)</ref> with different scales and typologies have been increasingly developed and applied in recent years. Recent MLU tasks focus on subject-centered intention understanding and behavior analysis from text, visual, and audio modalities, including but not limited to emotion recognition <ref type="bibr" target="#b22">(Lv et al. 2021)</ref>, sentiment analysis <ref type="bibr" target="#b7">(Han, Chen, and Poria 2021)</ref>, and humor detection <ref type="bibr" target="#b8">(Hasan et al. 2019)</ref>. Considering the heterogeneous nature of multimodal languages, numerous works have presented seminal network structures <ref type="bibr" target="#b27">(Pham et al. 2019;</ref><ref type="bibr">Liang et al. 2021b;</ref><ref type="bibr" target="#b49">Yu et al. 2021;</ref><ref type="bibr" target="#b30">Sun et al. 2022)</ref>, fusion strategies <ref type="bibr" target="#b33">(Tsai et al. 2019</ref><ref type="bibr" target="#b34">(Tsai et al. , 2018;;</ref><ref type="bibr" target="#b50">Zadeh et al. 2017;</ref><ref type="bibr" target="#b29">Rahman et al. 2020)</ref>, and representation learning paradigms <ref type="bibr">(Yang et al. 2022a,d,c;</ref><ref type="bibr" target="#b14">Li, Yang, and Zhang 2023)</ref>. For instance, MFSA <ref type="bibr">(Yang et al. 2022d</ref>) presented a factorized representation strategy to learn similarities and differences among multimodal languages. Despite achievements, existing methods invariably suffer from performance bottlenecks due to subject-related prediction bias. In comparison, we identify subjects in MLU benchmarks as harmful confounders from a causal perspective and improve different models with our SuCI.</p><p>Causal Demystification. Causal demystification as a potential statistical theory aims to pursue causal effects among observed variables rather than their shallow correlations. Benefiting from the advances in learning-based technologies <ref type="bibr">(Yang et al. 2023e,b,d,c;</ref><ref type="bibr">Liu et al. 2023a,b)</ref>, several studies exploring causality are mainly divided into two channels: causal intervention <ref type="bibr" target="#b18">(Lin et al. 2022;</ref><ref type="bibr">Yang et al. 2021</ref>) and counterfactual reasoning <ref type="bibr" target="#b28">(Qian et al. 2021;</ref><ref type="bibr" target="#b31">Tang et al. 2020)</ref>. Intervention <ref type="bibr">(Pearl 2009a)</ref> focuses on altering the natural tendency of the independent variable to vary with other variables to eliminate the impact of adverse effects. Counterfactuals depict imagined outcomes produced by factual variables under different treatment conditions <ref type="bibr">(Pearl 2009b)</ref>. Given the unbiased estimations provided by causal inference, it is increasingly applied to various downstream tasks such as computer vision <ref type="bibr">(Yang, Zhang, and Cai 2021;</ref><ref type="bibr">Yang et al. 2023a)</ref>, natural language processing <ref type="bibr" target="#b53">(Zhang et al. 2021;</ref><ref type="bibr" target="#b10">Huang, Liu, and Bowman 2020)</ref>, and reinforcement learning <ref type="bibr" target="#b3">(Dasgupta et al. 2019)</ref>. In this paper, we address the confounding effect from multiple modalities by embracing causal intervention, which is more generalizable and adaptable for multimodal approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology Structural Causal Graph in MLU Tasks</head><p>To systematically diagnose the confounding effect present in MLU tasks, we first design a recapitulative causal graph to summarize the causal relationships among variables.</p><p>The mainstream graphical notation following the structured causal model <ref type="bibr" target="#b25">(Pearl et al. 2000</ref>) is adopted due to its intuitiveness and interpretability. Concretely, a causal graph G = {N , E} is considered a directed acyclic graph, which represents how a set of variables N convey causal effects through the causal links E. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(a), there are four variables in the MLU causal graph, including multimodal inputs X, multimodal representations M , unintended confounders Z, and predictions Y . Note that our causal graph applies to various MLU approaches since it is highly general, imposing no constraints on the detailed implementations. The causalities are shown below.</p><p>▶ Link Z → X. Multimodal expressions from distinct subjects are recorded to produce multimodal inputs X, where X is a generalized definition of text X t , visual X v , and audio X a modalities for simplicity, i.e., X = {X t , X v , X a }. Subjects are identified as harmful confounders Z due to the subject-related prediction bias caused by different human expression customs and differences <ref type="bibr" target="#b8">(Hasan et al. 2019;</ref><ref type="bibr" target="#b51">Zadeh and Pu 2018)</ref>. In this case, Z is a collective denomination of multimodal confounding sources. For training inputs X, Z determines the subject-related biased content that is recorded, i.e., Z → X. According to the causal theory <ref type="bibr">(Pearl 2009a</ref>), the confounders Z are the common cause of X and corresponding predictions Y . The positive effect of the subject-agnostic multimodal semantics provided by M follows the desired causal path X → M → Y , which we aim to achieve and pursue. Unfortunately, Z causes subject-related prediction bias and misleads trained models to learn subject-specific misleading semantics rather than pure causal effects, leading to biased predictions on uninitiated subjects. The detrimental effects follow the backdoor paths X ← Z → Y and</p><formula xml:id="formula_0">▶ Link Z → M ← X. M</formula><formula xml:id="formula_1">X ← Z → M → Y .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Intervention via Backdoor Adjustment</head><p>Following the causal graph in Figure <ref type="figure" target="#fig_1">2</ref>(a), the MLU model relies on the likelihood P (Y |X) for predictions that suffer from backdoor effects, which can be decomposed by the Bayes rule as follows:</p><formula xml:id="formula_2">P (Y |X) = z P (Y |X, M = Fm(X, z))P (z|X),<label>(1)</label></formula><p>where F m (•) denotes any vanilla MLU model to learn the multimodal representations M . z is a stratum of confounders (i.e., a subject), which introduces the observational bias via P (z|X). Theoretically, an ideal solution would be to collect massive data samples to ensure that subjects with different expression characteristics are included in the training and testing sets. However, this way is unrealistic due to social ethics issues <ref type="bibr" target="#b12">(Jones 1999</ref>). To address this, we embrace causal intervention P (Y |do(X)) to interrupt the adverse effects propagating between X and Y along the backdoor paths via the backdoor adjustment theory <ref type="bibr">(Pearl 2009a</ref>). The do(•) operator is an efficient approximation to implement the empirical intervention <ref type="bibr" target="#b6">(Glymour, Pearl, and Jewell 2016)</ref>. In our case, backdoor adjustment means measuring the causal effect of each stratum in the subject confounders and then performing a weighted integration based on the prior proportions of samples from different subjects in the training data to estimate the average causal effect. From Figure <ref type="figure" target="#fig_1">2</ref>(b), the impact from Z to X is cut off since the model would enable the subject prototype as the confounder in each stratum to contribute equally to the predictions Y by P (Y |do(X)). Eq. ( <ref type="formula" target="#formula_2">1</ref>) with the intervention is formulated as:</p><formula xml:id="formula_3">P (Y |do(X)) = z P (Y |X, M = Fm(X, z))P (z). (2)</formula><p>The model is no longer disrupted by subject-specific spurious correlations in backdoor paths since z no longer affects X. P (z) is the prior probability that depicts the proportion of each z in the whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject De-confounded Training with SuCI</head><p>We present a plug-in Subject Causal Intervention module (SuCI) to convert the theoretical intervention in Eq. ( <ref type="formula">2</ref>) into a practical implementation. As Figure <ref type="figure">3</ref> shows, SuCI can be readily integrated into the vanilla MLU model to estimate P (Y |do(X)) through the subject de-confounded training.</p><p>The implementation details are as follows.</p><p>Subject-specific Feature Disentanglement. How to effectively disentangle subject-specific features from heterogeneous modalities is key to determining confounders. Considering that subject-related spurious semantics are usually distributed among different frames of multimodal sequences <ref type="bibr">(Yang et al. 2022a)</ref>, we first devise a dynamic fusion  </p><formula xml:id="formula_4">ξ m = ϕ(x m w xm + b xm ) ∈ R Tm×1 ,<label>(3)</label></formula><formula xml:id="formula_5">p m = ξ T m x m ∈ R dm ,<label>(4)</label></formula><formula xml:id="formula_6">s = G([pt, pv, pa]; θG) ∈ R ds×1 ,<label>(5)</label></formula><p>where [ , ] stands for the concatenation operator and</p><formula xml:id="formula_7">d s = d t + d v + d a ,</formula><p>The generator is implemented as a two-layer perceptron with GeLU (Hendrycks and Gimpel 2016) activation. s is distilled by the following objective:</p><formula xml:id="formula_8">L sub = CE(Ds(s; θD s ), ys) + MSE(Dt(s; θD t ), 1 Ct ),<label>(6)</label></formula><p>where CE(•) and MSE(•) represent the cross-entropy loss and mean squared error loss, respectively. D s (•; θ Ds ) and D t (•; θ Dt ) are the subject discriminator and task discriminator parameterized by θ Ds and θ Dt , respectively, which consist of feed-forward neural layers. y s is the subject label determined by the dataset's subject index. C t is the number of categories in downstream tasks. In Eq. ( <ref type="formula" target="#formula_8">6</ref>), s is supervised to encourage the output logits of the task discriminator to be equally distributed among all prediction categories to exclude task-related semantic information. Also, the subject discriminator is optimized to ensure that s belongs to a given subject, i.e., contains subject-specific semantics.</p><p>Confounder Construction. Confounder construction aims to make the model measure the causal effect of subject confounders among different strata during training to avoid subject-related prediction bias. Considering that subject features are similar within the same stratum and different across strata <ref type="bibr">(Pearl 2009a)</ref>, we utilize all the features from a specific subject as the subject prototype, which represents the universal attributes of confounders in a specific stratum.</p><p>Concretely, we maintain a memory cache for each subject during training to store and update the subject prototype as a confounder, which is computed as</p><formula xml:id="formula_9">z i = 1 Ni Ni k=1 s i k .</formula><p>N i is the number of training samples for the i-th subject, and s i k denotes the k-th feature of the i-th subject. z i is updated at the end of each epoch. In practice, each z i is initialized by the uniform distribution to ensure stable training at the first epoch. Based on the number of subjects N c , a stratified confounder dictionary is constructed, which is formulated as Z = [z 1 , z 2 , . . . , z Nc ]. Intervention Instantiation. Estimating P (Y |do(X)) in practice is high overhead since it requires the forward computation of X and z for each pair. To reduce the overhead, we introduce the Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b37">(Xu et al. 2015)</ref> to achieve feature-level approximation for intervention instantiation:</p><formula xml:id="formula_10">P (Y |do(X)) NWGM ≈ P (Y |X, M = z Fm(X, z)p(z)). (7)</formula><p>In this case, causal intervention makes X contribute fairly to the predictions of Y by incorporating every z. We parameterize a network model to approximate Eq. ( <ref type="formula">7</ref>) as follows:</p><formula xml:id="formula_11">P (Y |do(X)) = Wmm + W h E[h(z)],<label>(8)</label></formula><p>where W m ∈ R d h ×d and W h ∈ R d h ×ds are the learnable parameters. m ∈ R d×1 is the multimodal representation produced by the vanilla MLU model. The above approximation is reasonable since the effect on Y is attributed to M and Z in the MLU causal graph. Then, E[h(z)] is approximated as an adaptive aggregation for all confounders according to backdoor adjustment, which is formulated as: </p><formula xml:id="formula_12">E[h(z)] = Nc i=1 ψizip(zi),<label>(9)</label></formula><formula xml:id="formula_13">ψi = ϕ( (Wqm) T (W k zi) √ ds ),<label>(10)</label></formula><p>where W q ∈ R dn×d and W k ∈ R dn×ds are mapping matrices. p(z i ) = Ni N , where N is the number of training samples. In practice, m from one sample queries each z i in the confounder dictionary Z ∈ R Nc×ds to obtain the samplespecific attention set {ψ i } Nc i=1 . The intuition insight is that samples from one subject are impacted by varying degrees of confounder z i of other subjects. Objective Optimization. The standard cross-entropy loss is employed as the task-related training objective, which is expressed as L task = -1 Ni Ni i=1 y t • log ŷt , where y t is the task-related ground truth. The overall objective function is computed as L all = L sub + L task .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Benchmarks and Model Zoo</head><p>Benchmarks. Extensive experiments are conducted on three mainstream MLU benchmarks. Concretely, MOSI <ref type="bibr" target="#b52">(Zadeh et al. 2016</ref>) is a multimodal human sentiment analysis dataset consisting of 2,199 video segments. The standard data partitioning is 1,284 samples for training, 284 samples for validation, and 686 samples for testing. These samples contain a total of 89 distinct subjects from video blogs. Each sample is manually annotated with a sentiment score ranging from -3 to 3. MOSEI (Zadeh and Pu 2018) is a large-scale human sentiment and emotion recognition benchmark containing 22,856 video clips from 1,000 different subjects and 250 diverse topics. Among these samples, 16,326, 1,871, and 4,659 samples are used as training, validation and testing sets. The sample annotation protocols used for sentiment analysis are consistent with MOSI. UR FUNNY <ref type="bibr" target="#b8">(Hasan et al. 2019</ref>) is a multimodal human humor detection dataset that contains 16,514 video clips from 1,741 subjects collected by the TED portal. There are 10,598, 2,626, and 3,290 samples in the training, validation, and testing sets. Each sample provides the target punchlines and associated con-  UR FUNNY, respectively. We implement the selected methods and SuCI on NVIDIA Tesla A800 GPUs utilizing the PyTorch toolbox, where other training settings are aligned to their original protocols.</p><formula xml:id="formula_14">Methods Acc7 ↑ (%) Acc2 ↑ (%) F 1 ↑ (%) EF-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art Methods</head><p>We compare the SuCI-based models with extensive SOTA methods, including EF-LSTM, LF-LSTM, C-MFN <ref type="bibr" target="#b8">(Hasan et al. 2019)</ref>, Graph-MFN <ref type="bibr" target="#b51">(Zadeh and Pu 2018)</ref>, TFN <ref type="bibr" target="#b50">(Zadeh et al. 2017)</ref>, MFM <ref type="bibr" target="#b34">(Tsai et al. 2018)</ref>, RAVEN <ref type="bibr" target="#b35">(Wang et al. 2019)</ref>, MCTN <ref type="bibr" target="#b27">(Pham et al. 2019)</ref>, TCSP <ref type="bibr" target="#b36">(Wu et al. 2021)</ref>, PMR <ref type="bibr" target="#b22">(Lv et al. 2021)</ref>, and FDMER <ref type="bibr">(Yang et al. 2022a</ref>).</p><p>Results on MOSI Benchmark. (i) From Table <ref type="table" target="#tab_0">1</ref>, SuCI consistently improves the performance of the selected methods on all metrics. Concretely, the overall gains of Acc 7 , Acc 2 , and F 1 scores among the five models increased by 4.8%, 4.8%, and 4.6%, respectively. These improvements confirm that our module can break through the performance bottlenecks of most baselines in a model-agnostic manner. (ii) SuCI can bring more favorable gains for decoupled learning-based efforts. For instance, MISA and DMD achieve salient relative improvements among different metrics of 1.8%∼3.5% and 1.6%∼2.9%, respectively. A reasonable explanation is that the decoupling pattern diffuses the spurious semantics caused by subject confounders in multiple feature subspaces. In this case, SuCI's de-confounding ability is more effective. (iii) Compared to recent PMR and FDMER with complex network stacking and massive parameters <ref type="bibr" target="#b22">(Lv et al. 2021;</ref><ref type="bibr">Yang et al. 2022a)</ref>, MMIM achieves the best results by equipping our SuCI.</p><p>Results on MOSEI Benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset Evaluation</head><p>Since the used MOSI and MOSEI benchmarks have the same annotation protocols, we establish cross-dataset evaluations for MOSI-training→MOSEI-testing and MOSEI-training→MOSI-testing in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>, respectively. The design intuition is that exploring prediction performance on testing data with different distributions than the training data (i.e., out-of-distribution, OOD) helps verify confounding effects and model generalizability. The five vanilla methods show severe performance deterioration compared to the results in the Independent Identically Distributed (IID) setting from Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>. For instance, the testing results on MO-SEI and MOSI decreased by the average Acc 7 of 4.64% and 2.42% across all methods. This is inevitable since spurious correlations between trained models and specific subjects are exacerbated and amplified in uninitiated subjects under the OOD setting. Fortunately, our SuCI significantly improves the results of all models in cross-dataset evaluations. These substantial gains confirm that our module favorably mitigates the subject variation problem and enhances the generalizability and robustness of the vanilla models. Table <ref type="table">6</ref>: Ablation studies on the three benchmarks. "w/" and "w/o" mean the with and without.</p><p>1 0 02 0 03 0 04 0 05 0 06 0 07 0 08 0 09 0 01 0 0 0 6 7 6 8 6 9 7 0 7 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In Table <ref type="table">6</ref>, we perform ablation studies to evaluate the impact of all components in SuCI. We report results for the Acc 2 metrics due to similar trends for the other metrics. Necessity of Subject Feature Learning. (i) We first replace our dynamic fusion mechanism with the average pooling operation along the frame length to obtain refined representations p m . The insufficient gains on all benchmarks suggest that assigning adaptive weights based on the distinct frame element contributions in multimodal sequences facilitates better capturing subject-related semantics. (ii) Subsequently, we separately remove the subject and task discriminators to investigate the role of feature disentanglement. Intuitively, the subject discriminator provides substantial gains for both methods since it supervises the generator to yield discriminative subject features s that are better used for confounder construction. (iii) The task discriminator is equally critical as it ensures that task-related information is excluded from s to distill the pure subject bias. Importance of Different Modalities. (i) When the text, visual, and audio modalities from the subjects are removed separately in de-confounded training, the improvements in SuCI for the baselines show significant deterioration. These decreased results confirm that subject-specific spurious characteristics are transmitted in multimodal utterances and that considering the complete modalities is necessary.</p><p>(ii) The reason for the pronounced effect of the text modality may be the adverse statistical shortcuts caused by the highly uneven distribution of textual words across the samples of distinct subjects, which is a confounder inducer. Rationality of Confounder Dictionary. We provide two candidates of the same size as the default confounder dictionary Z to evaluate the rationality of our confounder construction. These two alternative dictionaries are obtained by random initialization and unsupervised K-Means++ clustering <ref type="bibr" target="#b0">(Bahmani et al. 2012</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation</head><p>To show the difference between the model approximate P (Y |X) and P (Y |do(X)), we randomly select several samples in each sentiment category on MOSEI for visualization. As Figure <ref type="figure" target="#fig_6">5</ref> shows, the sample distributions from the different categories of the vanilla baseline prediction usually appear confounded and fragmented. In comparison, the causality-based SuCI improves the predicted results so that the samples in the same category are more compact and different categories are well separated. The observation suggests that our causal intervention promotes discriminative predictions by mitigating the subject prediction bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper is the first to reveal the long-neglected subject variation problem in MLU tasks and identify subjects as essentially harmful confounders from a causal perspective. Thus, we present a subject causal intervention module (SuCI) to remove the prediction bias caused by the subjectspecific spurious correlations. Extensive experiments show the broad applicability of SuCI in diverse methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>IFigure 1 :</head><label>1</label><figDesc>Figure 1: Examples on the MOSI benchmark illustrate the subject variation problem. Multimodal expressions from four subjects potentially convey distinct semantic correlations due to their different customs and styles in expressing sentiments.</figDesc><graphic coords="2,357.68,280.22,98.91,76.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The causal graph explains causal effects of MLU procedure. Nodes denote variables and arrows denote the direct causal effects. (a) The conventional likelihood estimation P (Y |X). (b) The causal intervention P (Y |do(X)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>denotes the refined multimodal representations extracted by any MLU model, which acts as a mediator before the final classifier. The link Z → M indicates detrimental Z confounding models to capture subjectspecific characteristics embedded in M to produce spurious semantic correlations. Several intuitive examples are the semantically unimportant words from the text modality and the agitated tones from the audio modality in Figure 1. Furthermore, M contains universal multimodal feature semantics from X that can be reflected via the causal link X → M . ▶ Link M → Y ← Z. The causal path M → Y reveals that the impure M confounded by Z further impacts the final predictions Y of downstream MLU tasks. Meanwhile, adverse confounders' prior information in the training data implicitly interferes with Y along the link Z → Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: A general MLU pipeline for the subject de-confounded training. The red dotted box shows the core component that achieves the approximation to causal intervention: our SuCI. SuCI can be readily integrated into the vanilla MLU model via backdoor adjustment to mitigate subject-specific spurious correlations and achieve debiased predictions in downstream tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where m ∈ {t, v, a}, ϕ(•) is the softmax function, w xm ∈ R dm×1 , and b xm ∈ R Tm×1 are the learnable parameters. The attention vectors ξ m adaptively capture salient semantics and produce informative representations p m based on the contributions of different frames. Subsequently, we introduce an adversarial strategy to disentangle the subjectspecific semantics and avoid the task-related semantics. The design philosophy is to distill pure subject features for better confounder construction. Specifically, a subject generator G(•; θ G ) is presented to project p t , p v , and p a of multimodal utterances from the subject into a common space to yield a subject-specific feature s, expressed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation study results for the number of subject confounders on the UR FUNNY benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Quantitative results (i.e., binary or seven classifications) of vanilla and SuCI-based DMD on the MOSEI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(i)  The random Z would significantly impair the performance and even underperform the vanilla methods, justifying the proposed subject prototypes as confounders. (ii) The performance gains of the clustered Z are sub-optimal due to the lack of subject-specific information supervision, leading to the indistinguishability and coupling of confounding effects across different subjects. Effectiveness of Adaptive Aggregation. Here, the adaptive aggregation strategy is explored by eliminating the attention weights ψ i and the prior probabilities p(z i ) in E[h(z)], respectively. The consistent performance drops on all benchmarks imply that characterizing the importance and proportion of each subject confounder is indispensable for achieving effective causal intervention based on subject debiasing. Impact of Subject Confounder Number. Finally, we test the impact of different numbers of subject confounders on SuCI performance in Figure4. The SuCI-based models all exhibit overall rising gain trends as the training subjects increase. These findings suggest that sufficient stratified confounders facilitate better backdoor adjustment implementation and accurate average causal effect estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of different methods and SuCIbased models on the MOSI benchmark. Improved results and corresponding gains compared to vanilla methods are marked in bold and red, respectively.</figDesc><table><row><cell>Methods</cell><cell>Acc7 ↑ (%)</cell><cell>Acc2 ↑ (%)</cell><cell>F 1 ↑ (%)</cell></row><row><cell>EF-LSTM</cell><cell>33.7</cell><cell>75.3</cell><cell>75.2</cell></row><row><cell>LF-LSTM</cell><cell>35.3</cell><cell>76.8</cell><cell>76.7</cell></row><row><cell>Graph-MFN</cell><cell>29.6</cell><cell>75.4</cell><cell>76.6</cell></row><row><cell>TFN</cell><cell>32.1</cell><cell>73.9</cell><cell>73.4</cell></row><row><cell>LMF</cell><cell>32.8</cell><cell>76.4</cell><cell>75.7</cell></row><row><cell>MFM</cell><cell>36.2</cell><cell>78.1</cell><cell>78.1</cell></row><row><cell>RAVEN</cell><cell>33.2</cell><cell>78.0</cell><cell>76.6</cell></row><row><cell>MCTN</cell><cell>35.6</cell><cell>79.3</cell><cell>79.1</cell></row><row><cell>TCSP</cell><cell>-</cell><cell>80.9</cell><cell>81.0</cell></row><row><cell>PMR</cell><cell>40.6</cell><cell>83.6</cell><cell>83.4</cell></row><row><cell>FDMER</cell><cell>42.1</cell><cell>84.2</cell><cell>83.9</cell></row><row><cell>MulT</cell><cell>39.5</cell><cell>82.6</cell><cell>82.5</cell></row><row><cell>MulT + SuCI</cell><cell>40.7 +1.2</cell><cell>83.4 +0.8</cell><cell>83.4 +0.9</cell></row><row><cell>MISA</cell><cell>40.2</cell><cell>81.8</cell><cell>81.9</cell></row><row><cell>MISA + SuCI</cell><cell>41.6 +1.4</cell><cell>83.3 +1.5</cell><cell>83.1 +1.2</cell></row><row><cell>Self-MM</cell><cell>41.6</cell><cell>83.9</cell><cell>84.1</cell></row><row><cell>Self-MM + SuCI</cell><cell>42.0 +0.4</cell><cell>84.3 +0.4</cell><cell>84.6 +0.5</cell></row><row><cell>MMIM</cell><cell>41.8</cell><cell>84.1</cell><cell>84.1</cell></row><row><cell>MMIM + SuCI</cell><cell>42.4 +0.6</cell><cell>84.9 +0.8</cell><cell>84.8 +0.7</cell></row><row><cell>DMD</cell><cell>41.0</cell><cell>83.3</cell><cell>83.2</cell></row><row><cell>DMD + SuCI</cell><cell>42.2 +1.2</cell><cell>84.6 +1.3</cell><cell>84.5 +1.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of different models on the MO-SEI benchmark.texts from multimodal utterances to support the detection of subject humor/non-humor in the binary labels. Following widely adopted implementations(Yang et al. 2022a;<ref type="bibr" target="#b15">Li, Wang, and Cui 2023)</ref>, we use the 7-class accuracy (Acc 7 ), binary accuracy (Acc 2 ), and F 1 score to evaluate the results on MOSI and MOSEI. The standard binary accuracy (Acc 2 ) is utilized for evaluation on UR FUNNY.</figDesc><table><row><cell>LSTM</cell><cell>47.4</cell><cell>78.2</cell><cell>77.9</cell></row><row><cell>LF-LSTM</cell><cell>48.8</cell><cell>80.6</cell><cell>80.6</cell></row><row><cell>Graph-MFN</cell><cell>45.0</cell><cell>76.9</cell><cell>77.0</cell></row><row><cell>RAVEN</cell><cell>50.0</cell><cell>79.1</cell><cell>79.5</cell></row><row><cell>MCTN</cell><cell>49.6</cell><cell>79.8</cell><cell>80.6</cell></row><row><cell>TCSP</cell><cell>-</cell><cell>82.8</cell><cell>82.7</cell></row><row><cell>PMR</cell><cell>52.5</cell><cell>83.3</cell><cell>82.6</cell></row><row><cell>FDMER</cell><cell>53.8</cell><cell>83.9</cell><cell>83.8</cell></row><row><cell>MulT</cell><cell>51.2</cell><cell>82.1</cell><cell>81.9</cell></row><row><cell>MulT + SuCI</cell><cell>52.4 +1.2</cell><cell>83.2 +1.1</cell><cell>82.9 +1.0</cell></row><row><cell>MISA</cell><cell>51.3</cell><cell>82.3</cell><cell>82.3</cell></row><row><cell>MISA + SuCI</cell><cell>52.6 +1.3</cell><cell>83.5 +1.2</cell><cell>83.2 +0.9</cell></row><row><cell>Self-MM</cell><cell>52.9</cell><cell>83.9</cell><cell>83.8</cell></row><row><cell>Self-MM + SuCI</cell><cell>53.6 +0.7</cell><cell>84.2 +0.3</cell><cell>84.2 +0.4</cell></row><row><cell>MMIM</cell><cell>53.7</cell><cell>84.4</cell><cell>84.3</cell></row><row><cell>MMIM + SuCI</cell><cell>54.4 +0.7</cell><cell>85.5 +1.1</cell><cell>85.2 +0.9</cell></row><row><cell>DMD</cell><cell>53.5</cell><cell>84.1</cell><cell>84.0</cell></row><row><cell>DMD + SuCI</cell><cell>54.6 +1.1</cell><cell>85.8 +1.7</cell><cell>85.7 +1.7</cell></row></table><note><p><p><p><p><p>Model Zoo. Considering the applicability, we choose five representative models with different network structures to verify the proposed plug-in SuCI. A brief overview is as follows. MulT</p><ref type="bibr" target="#b33">(Tsai et al. 2019</ref></p>) constructs multimodal transformers to learn element dependencies between pairwise modalities. MISA (Devamanyu, Roger, and Soujanya 2020) captures modality-invariant and modality-specific diverse representations based on feature disentanglement. Self-MM</p><ref type="bibr" target="#b49">(Yu et al. 2021</ref></p>) utilizes a self-supervised paradigm to learn additional emotion semantics from unimodal label generation. MMIM (Han, Chen, and Poria 2021) proposes a hierarchical mutual information maximization to alleviate the loss of valuable task-related clues. DMD (Li, Wang, and Cui 2023) designs cross-modal knowledge distillations to bridge the semantic gap among modalities. aligned data points are employed across all benchmarks. In the SuCI implementation, the hidden dimensions d h and d n are set to 64 and 128, respectively. The size d s of each subject confounder is 325, 409, and 456 on MOSI, MOSEI, and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different models on the UR FUNNY benchmark.</figDesc><table><row><cell>Methods</cell><cell>Acc7 ↑ (%)</cell><cell>Acc2 ↑ (%)</cell><cell>F 1 ↑ (%)</cell></row><row><cell>MulT</cell><cell>46.9</cell><cell>77.6</cell><cell>77.4</cell></row><row><cell>MulT + SuCI</cell><cell>48.5 +1.6</cell><cell>80.2 +2.6</cell><cell>80.3 +2.9</cell></row><row><cell>MISA</cell><cell>46.6</cell><cell>77.2</cell><cell>77.1</cell></row><row><cell>MISA + SuCI</cell><cell>48.3 +1.7</cell><cell>78.9 +1.7</cell><cell>79.4 +2.3</cell></row><row><cell>Self-MM</cell><cell>47.7</cell><cell>78.5</cell><cell>78.3</cell></row><row><cell>Self-MM + SuCI</cell><cell>48.2 +0.5</cell><cell>79.5 +1.0</cell><cell>79.6 +1.3</cell></row><row><cell>MMIM</cell><cell>49.3</cell><cell>79.6</cell><cell>79.3</cell></row><row><cell>MMIM + SuCI</cell><cell>51.8 +2.5</cell><cell>82.5 +2.9</cell><cell>82.4 +3.1</cell></row><row><cell>DMD</cell><cell>48.9</cell><cell>80.8</cell><cell>80.7</cell></row><row><cell>DMD + SuCI</cell><cell>51.6 +2.7</cell><cell>82.4 +1.6</cell><cell>82.4 +1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Cross-dataset evaluation of models trained on the MOSI training set and tested on the MOSEI testing set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Table2provides perfor-Cross-dataset evaluation of models trained on the MOSEI training set and tested on the MOSI testing set. mance comparison results on MOSEI. (i) The SuCI-based models outperform the vanilla methods by large margins on all metrics. For example, SuCI helps the latest DMD to achieve new SOTA performance with considerable absolute gains of 1.1%, 1.7%, and 1.7% in Acc 7 , Acc 2 , and F 1 scores, respectively. These observations show the broad applicability and usefulness of our module in removing the subject-related prediction bias. (ii) The improvements provided by SuCI on MOSEI are more significant than those on MOSI. The plausible deduction is that MOSEI contains richer subjects and their highly idiosyncratic multimodal utterances in various scenarios. Thus, SuCI can more accurately remove spurious correlations caused by appropriately extracted subject confounders and offer sufficient gains. Results on UR FUNNY Benchmark. From Table3, we show the detection results using the target punchline for fair comparisons with other works. (i) The SuCI-based models achieve consistent performance increases and accomplish competitive and better results than previous methods.</figDesc><table><row><cell>Methods</cell><cell>Acc7 ↑ (%)</cell><cell>Acc2 ↑ (%)</cell><cell>F 1 ↑ (%)</cell></row><row><cell>MulT</cell><cell>37.4</cell><cell>80.2</cell><cell>79.9</cell></row><row><cell>MulT + SuCI</cell><cell>38.7 +1.3</cell><cell>81.7 +1.5</cell><cell>81.5 +1.6</cell></row><row><cell>MISA</cell><cell>37.8</cell><cell>80.5</cell><cell>80.7</cell></row><row><cell>MISA + SuCI</cell><cell>39.0 +1.2</cell><cell>82.2 +1.7</cell><cell>82.3 +1.6</cell></row><row><cell>Self-MM</cell><cell>38.9</cell><cell>82.0</cell><cell>82.1</cell></row><row><cell>Self-MM + SuCI</cell><cell>39.5 +0.6</cell><cell>82.7 +0.7</cell><cell>82.4 +0.3</cell></row><row><cell>MMIM</cell><cell>39.3</cell><cell>82.5</cell><cell>82.5</cell></row><row><cell>MMIM + SuCI</cell><cell>41.1 +1.8</cell><cell>83.4 +0.9</cell><cell>83.3 +0.8</cell></row><row><cell>DMD</cell><cell>38.6</cell><cell>81.6</cell><cell>81.4</cell></row><row><cell>DMD + SuCI</cell><cell>40.6 +2.0</cell><cell>83.0 +1.4</cell><cell>82.9 +1.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0113503</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2a87J5j">
					<idno type="grant-number">2021ZD0113503</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vattani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.6402</idno>
		<title level="m">Scalable k-means++</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Openface: an open source facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal intervention for subject-deconfounded facial action unit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08162</idno>
		<title level="m">Causal reasoning from metareinforcement learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">COVAREP-A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MISA: Modality-invariant and-specific representations for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Devamanyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soujanya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00412</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ur-funny: A multimodal language dataset for understanding humor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06618</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Counterfactually-augmented SNLI training data does not yield better generalization than unaugmented data</title>
		<idno type="arXiv">arXiv:2010.04762.iMotions.2017</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Facial expression analysis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bounded rationality</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of political science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="297" to="321" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13205</idno>
		<title level="m">Text-oriented Modality Reinforcement Network for Multimodal Sentiment Analysis from Unaligned Multimodal Sequences</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards Robust Multimodal Sentiment Analysis under Uncertain Signal Missing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1497" to="1501" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled Multimodal Distilling for Emotion Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2023</date>
			<biblScope unit="page" from="6631" to="6640" />
		</imprint>
	</monogr>
	<note>Deep facial expression recognition: A survey</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07502</idno>
		<title level="m">Multiscale benchmarks for multimodal representation learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2021a. Multibench</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention Is Not Enough: Mitigating the Distribution Discrepancy in Asynchronous Multimodal Sequence Fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8148" to="8156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A causal debiasing framework for unsupervised salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1610" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">2023a. Stochastic video normality network for abnormal event detection in surveillance videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date>110986</date>
			<publisher>Knowledge-Based Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05087</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2554" to="2562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Models, reasoning and inference</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CambridgeUniversityPress</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Counterfactual Inference for Text Classification Debiasing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5434" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference Association for Computational Linguistics Meeting (ACL)</title>
		<meeting>the Conference Association for Computational Linguistics Meeting (ACL)</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2020">2020. 2359</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CubeMLP: An MLP-based model for multimodal sentiment analysis and depression estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3722" to="3729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Debiasing NLU models via causal intervention and counterfactual reasoning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11376" to="11384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference. Association for Computational Linguistics Meeting (ACL)</title>
		<meeting>the Conference. Association for Computational Linguistics Meeting (ACL)</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06176</idno>
		<title level="m">Learning factorized multimodal representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics Meeting (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4730" to="4738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2023a. Context De-Confounded Emotion Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="19005" to="19015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Disentangled Representation Learning for Multimodal Emotion Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1642" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">2022b. Contextual and Cross-modal Interaction for Multi-modal Speech Emotion Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotion Recognition for Multiple Context Awareness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13697</biblScope>
			<biblScope unit="page" from="144" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20459" to="20470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1708" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How2comm: Communicationefficient and collaboration-pragmatic multi-agent perception</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023d.</date>
			<biblScope unit="page" from="23383" to="23392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What2comm: Towards Communication-Efficient Collaborative Perception via Feature Decoupling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th ACM International Conference on Multimedia</title>
		<meeting>the 31th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7686" to="7695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal attention for vision-language tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9847" to="9857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10790" to="10797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<title level="m">Tensor fusion network for multimodal sentiment analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">De-biasing distantly supervised named entity recognition via causal intervention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
