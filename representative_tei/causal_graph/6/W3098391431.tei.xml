<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Cooking Workflow Construction for Food Recipes</title>
				<funder>
					<orgName type="full">International Research Centres in Singapore Funding Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-08-20">20 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
							<email>chenjingjing@fudan.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yugang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><forename type="middle">2020</forename><surname>Chua</surname></persName>
						</author>
						<author>
							<persName><surname>Multi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Xi&apos;an Jiaotong University Xi&apos;an</orgName>
								<address>
									<region>Shanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">City University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">National University of Singapore Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Fudan University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">National University of Singapore Singapore</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country>Singapore USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<postCode>2020</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Cooking Workflow Construction for Food Recipes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-20">20 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413765</idno>
					<idno type="arXiv">arXiv:2008.09151v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Food Recipes</term>
					<term>Cooking Workflow</term>
					<term>Multi-modal Fusion</term>
					<term>MM-Res Dataset</term>
					<term>Cause-and-Effect Reasoning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding food recipe requires anticipating the implicit causal effects of cooking actions, such that the recipe can be converted into a graph describing the temporal workflow of the recipe. This is a non-trivial task that involves common-sense reasoning. However, existing efforts rely on hand-crafted features to extract the workflow graph from recipes due to the lack of large-scale labeled datasets. Moreover, they fail to utilize the cooking images, which constitute an important part of food recipes. In this paper, we build MM-ReS, the first large-scale dataset for cooking workflow construction, consisting of 9,850 recipes with human-labeled workflow graphs. Cooking steps are multi-modal, featuring both text instructions and cooking images. We then propose a neural encoder-decoder model that utilizes both visual and textual information to construct the cooking workflow, which achieved over 20% performance gain over existing hand-crafted baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Multimedia and multimodal retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, millions of cooking recipes are available online on cooking sharing platforms, such as AllRecipes, Cookpad, and Yummly, etc. A recipe is usually presented in multimedia setting, with textual description of cooking steps aligned with cooking images to illustrate the visual outcome of each step. See Figure <ref type="figure" target="#fig_3">2</ref>(a) for multimedia presentation of the recipe for "Blueberry Crumb Cake". These information potentially provide opportunity for multi-modal analysis of recipes, including cuisine classification <ref type="bibr" target="#b26">[27]</ref>, food recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46]</ref>, recipe recommendation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> and cross-modal image-to-recipe search <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. A common fundamental problem among these tasks is in the modeling of the cause-and-effect relations of this cooking workflow construction. In this paper, we investigate this problem leveraging multiple modalities.</p><p>In food recipes, two cooking steps can be either sequential or parallel, as exemplified in Figure <ref type="figure" target="#fig_2">1</ref>. Sequential means we cannot perform one step without finishing the other, while parallel indicates that the two steps are independent and can be performed at the same time. Based on these relations between pairs of cooking steps, we can draw a cooking workflow that describes the temporal evolution of the food's preparation; see Figure <ref type="figure" target="#fig_3">2(b)</ref>. Formally, a cooking workflow is represented as a graph, where each node represents a cooking step. The nodes are chained in temporal order, where a link represents a sequential relation.</p><p>The problem of cooking workflow construction has not been fully explored and mostly addressed with text-only analysis. For example, text-based dependency parsing is employed for workflow construction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>, and the hierarchical LSTM has been applied to model the causality effect for feature embedding <ref type="bibr" target="#b6">[7]</ref>. However, we believe this problem should be addressed with multimodal analysis for two reasons.</p><p>First, textual descriptions and cooking images usually play complementary roles in detecting cause-and-effect relations. In a large bowl mix together the bread crumbs, the parmesan cheese, and the salt.</p><p>In another bowl, mix together the eggs, milk and garlic.  shows an example on why relying on text description alone is insufficient. In this example, we cannot infer the casual relation purely from the text description, as it does not mention where the strawberries are to be placed. But the pragmatic problem can be solved by looking at the cooking images of the two instructions. Similarly, the use images alone does not give sufficient clue to determine the casual relation between two steps. As shown in Figure <ref type="figure" target="#fig_2">1</ref>(b), although the cooking images look similar, the two steps are actually parallel, which can only be inferred from the clue "in another bowl" in the text description.</p><p>Second, a multi-modal cooking workflow has wider applications in both real-world cooking and recipe-related research. In real life, a workflow with both images and texts provides a more intuitive guidance for cooking learners. Novel recipe-based applications can also be proposed: in image-to-recipe generation, it is easier for machines to write a recipe following the guidance of a cooking workflow; in cross-modal retrieval, the model can benefit from the additional knowledge of cause-and-effect relations; in food recommendation, two recipes can be associated based on the structural similarity of their cooking workflows.</p><p>Despite its importance, understanding the cause-and-effect relations in a cooking recipe is a non-trivial problem, usually requiring an in-depth understanding of both the visual and textual information. On the visual side, visually similar steps are not necessarily sequential, exemplified by Figure <ref type="figure" target="#fig_2">1</ref>(b). Therefore, fine-grained ingredient recognition is often required; e.g., the two steps in Figure <ref type="figure" target="#fig_2">1</ref>(a) are sequential because they both operate on the strawberries. However, accurate ingredient recognition is quite challenging because of the variety in appearance of ingredients, resulting from various cooking and cutting methods <ref type="bibr" target="#b5">[6]</ref>. On the textual side, understanding causal relations also requires an in-depth understanding of the cooking instruction as well as the contexts from previous steps.</p><p>Neural networks, especially deep visual and language models such as ResNET <ref type="bibr" target="#b11">[12]</ref> and BERT <ref type="bibr" target="#b7">[8]</ref>, offer promising solutions for the above challenges by learning deep visual and textual features. However, training them often requires a large amount of labeled data. Existing datasets, i.e., the Recipe Flow-graph Corpus (r-FG) <ref type="bibr" target="#b47">[48]</ref> and the Carnegie Mellon University Recipe Database (CURD) <ref type="bibr" target="#b37">[38]</ref> only have 208 and 260 labeled cooking workflows, respectively. Moreover, none of these datasets include cooking images. Due to the limited dataset scale, existing methods are largely restricted to using hand-crafted textual features, such as matching words <ref type="bibr" target="#b15">[16]</ref>, and syntactic parsing <ref type="bibr" target="#b47">[48]</ref>. These features are only able to capture shallow semantics, in addition to ignoring visual information.</p><p>To address the above problems, we construct a large-scale dataset, namely the Multi-modal Recipe Structure dataset (MM-ReS), consisting of 9,850 recipes with labeled cooking workflows. Each recipe contains an average of 11.26 cooking steps, where each step comprises of both textual instructions and multiple cooking images. We then propose a neural model which employs the Transformer architecture <ref type="bibr" target="#b38">[39]</ref> and the idea of Pointer Network <ref type="bibr" target="#b39">[40]</ref> to construct the cooking workflow. We compare our method with existing handcrafted baseline <ref type="bibr" target="#b15">[16]</ref> as well as strong neural baselines such as BERT <ref type="bibr" target="#b7">[8]</ref> and Multimodal Bitransformers (MMBT) <ref type="bibr" target="#b18">[19]</ref>. Experiment results show that neural-based models outperform hand-crafted baseline by a large margin. Neural models which utilize both recipe texts and cooking images generally perform better than models using a single modality. Among them, our proposed model achieves the best average F 1 score. To the best of our knowledge, this is the first work that explores multi-modal information for detecting cause-and-effect relationship in cooking recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Cooking Workflow Construction</head><p>Existing works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref> for cooking workflow construction can be categorized into ingredient-level and instructionlevel methods, based on the granularity of the workflow.</p><p>Ingredient-level methods aim to parse a recipe into a work-flow graph, where each vertex represents either a cooking action or a raw ingredient, and directed edges represent the "action flow" (describing the temporal execution sequence) or "ingredient flow" (tracking the ingredient sources). Early work manually built the workflow graph for accurate recipe retrieval <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, requiring laborious human labeling. An unsupervised hard-EM approach was proposed to automatically build workflow graphs by alternately optimizing a segmentation and a graph model <ref type="bibr" target="#b17">[18]</ref>. The segmentation extracts actions from the text recipe while the graph model defined a distribution over the connections between actions. Yamakata et al. <ref type="bibr" target="#b46">[47]</ref> further proposed to enrich the workflow graph with cooking tools and duration with a semi-supervised method with four steps: word segmentation, recipe term identification, edge weight estimation, and manual action graph refinement. Nevertheless, ingredient-level methods do not attain high quality workflows for real-world applications due to two reasons: (1) the results are highly dependant on NLP tasks -such as named entity recognition, co-reference resolution and dependency parsing -which are noisy due to varied writing style in recipes, and (2) the lack of large-scale labeled fine-grained recipe structure data, also infeasible due to the required manual effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blueberry Crumb Cake</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⋯ ⋯ ⋯</head><p>Step 1: Go Get Stuff 1-1/2 cups of sugar 2 sticks softened butter … mixing spoon, etc</p><p>Step 2: Wash the Blueberries Rinse off the blueberries in a colander and give them a few shakes to help get the water off. Let them sit to dry.</p><p>Step 3: Grease the Pan Lightly grease the pan with the end of a stick of butter ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 4: Sweet Butter</head><p>Add two sticks of softened butter to a large bowl. Pour in a cup and a half of sugar. Beat it until its thoroughly mixed.</p><p>Step 15: Eat Cut yourself a piece of cake and enjoy. If you are feeling generous, you can cut a piece of cake for others as well.</p><p>(a) The cooking recipe of "Blueberry Crumb Cake".</p><p>Step 2: Wash the Blueberries</p><p>Step 3: Grease the Pan</p><p>Step 4: Sweet Butter</p><p>Step 5: Eggs</p><p>Step 6: Vanilla</p><p>Step 7: Preheat the Oven</p><p>Step 8: Dry Stuff</p><p>Step 9: Wet Stuff</p><p>Step 10: Mixing It All Together</p><p>Step 11: Pan It</p><p>Step 12: Crumby</p><p>Step 13: Spread Crumbs</p><p>Step 14: Bake (b) The cooking workflow for "Blueberry Crumb Cake". Compared with ingredient-level methods, instruction-level workflow is more practical in terms of scalability. In <ref type="bibr" target="#b15">[16]</ref>, an ingredientinstruction dependency tree representation named SIMMER was proposed to represent the recipe structure. SIMMER represents a recipe as a dependency tree with ingredients as leaf nodes and recipe instructions as internal nodes. In SIMMER, several handcrafted text features were designed to train the Linear SVM-rank model for predicting links between instructions. Similar to <ref type="bibr" target="#b15">[16]</ref>, we also focus on instruction-level workflow construction; however, we study from the perspective of multi-modal learning by considering both text procedures and cooking images in the recipe. Moreover, instead of defining hand-crafted features, we improve the feature extraction using neural models to obtain deep semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prerequisite Relation Detection</head><p>The key to building a cooking workflow lies in detecting the parallel/sequential relationship, which is essentially a kind of prerequisite relation. Despite being a relatively new research area, datadriven methods for learning concept prerequisite relations have been explored in multiple domains. In educational data mining, prerequisite relations have been studied among courses or course concepts for curriculum planning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51]</ref>. Pan et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> proposed hand-crafted features such as video references and sentence references for learning prerequisite relations among concepts in MOOCs. Besides education domain, prerequisite relation has also been mined between Wikipedia articles <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>, concepts in textbooks <ref type="bibr" target="#b43">[44]</ref>, as well as concepts in scientific corpus <ref type="bibr" target="#b8">[9]</ref>.</p><p>Existing methods are limited to hand-crafted textual features, such as the maximum matching words <ref type="bibr" target="#b15">[16]</ref>, reference distance <ref type="bibr" target="#b19">[20]</ref>, and complexity level distance <ref type="bibr" target="#b30">[31]</ref>. Although these features capture shallow semantics, they are mostly domain-dependent and not transferable across applications. Furthermore, as existing work has focused only on pure text, such as Wikipedia page and textbooks.</p><p>How to best make use of the multimedia nature of documents in describing causality has been insufficiently investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal Food Analysis</head><p>Cross-modal learning in food domain has started to attract research interest in recent years. Novel tasks such as ingredient/food recognition <ref type="bibr" target="#b3">[4]</ref>, cross-modal retrieval <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> and recipe generation <ref type="bibr" target="#b33">[34]</ref> have been proposed, and several large food and recipe datasets have been developed; for example, Cookpad <ref type="bibr" target="#b10">[11]</ref> and Recipe1M+ <ref type="bibr" target="#b23">[24]</ref>. Existing neural-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> typically learn a joint embedding space between food images and recipe texts. For example, in <ref type="bibr" target="#b27">[28]</ref>, a deep belief network is used to learn the joint space between food images and ingredients extracted from recipes. However, previous works consider a recipe as a whole, but ignore its inherent structure. Different from these works, our work investigate the cause-and-effect relations inherent in cooking recipes, based on which we can learn better recipe representations to benefit downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET: MM-RES</head><p>Cooking workflow construction is a novel task that lacks a largescale dataset. To facilitate future research, we construct the Multimodal Recipe Structure (MM-ReS) dataset, containing 9,850 real food recipes. MM-ReS is the first large scale dataset to simultaneously contain: (1) labeled cooking workflow for each food recipe, and (2) cooking images and text descriptions for each cooking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We collect food recipes from two cooking sharing platforms: Instructables <ref type="foot" target="#foot_0">1</ref> and AllRecipes<ref type="foot" target="#foot_1">foot_1</ref> (statistics summarized in Table <ref type="table" target="#tab_0">1</ref>): Coarse-grained Fine-grained</p><p>• Instructables is one of the largest do-it-yourself (DIY) sharing platforms, which contains millions of user-uploaded DIY projects, including over 30,000 food recipes. Users post step-by-step cooking instructions to the recipe, with each step accompanied by one or multiple cooking images (see Figure <ref type="figure" target="#fig_3">2</ref>(a) as an example). We crawled all recipes under the category "food" and excluded none-English recipes, resulting in a total of 32,733 recipes. On average, each crawled recipe contains 5.65 cooking steps while each step contains 2.32 cooking images. As the recipes are written by contributing users, the texts are relatively noisy and include information irrelevant to the cooking procedure, such as "Look, we are done, excited?". The cooking steps divided by users are often in coarse-grained, with each step containing multiple cooking actions.</p><p>• AllRecipes is an advertising-based revenue generator, presented as a food focused online social networking service. The recipes on the website are posted by members of the Allrecipes.com community. They are categorized by season, type (such as appetizer or dessert), and ingredients. We crawled all English recipes from the website and obtain 65,599 valid recipes. Compared with the recipes from Instructables, the recipes in AllRecipes are written by experts, therefore the texts are of high quality and the cooking steps are more fine-grained, with each step only corresponds to one or two cooking actions. Despite with high quality, recipes do not have cooking images associated with each step. However, a portion of recipes have high-quality cooking videos made by the website, serving as a good source to extract cooking images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Processing</head><p>We first process the collected data in two steps: 1) Data Filtering. We first select high-quality recipes from the collected data to construct our final dataset. For recipes from Instructables, we discard recipes that contain less than 7 steps <ref type="foot" target="#foot_2">3</ref> as their cooking workflows are likely to form trivial chains, rather than a graph structure. We also ensure that each step has both a text description and at least one cooking image. User-contributed cooking steps are often lengthy, describing multiple cooking actions. We split steps consisting of more than 3 sentences into individual sentences, treating each as one cooking step. We obtain 5, 071 highquality recipes from Instructables after data filtering. For recipes from AllRecipes, the cooking steps are already fine-grained. We then rank the recipes by the number of cooking steps, and selecting the first 5, 000 recipes that have well-made cooking videos.</p><p>2) Key Frame Extraction. To obtain cooking images for recipes in AllRecipes, we extract key frames from cooking videos. We first extract frames from each recipe video with fixed time intervals using the ffmpeg<ref type="foot" target="#foot_3">foot_3</ref> video processing toolkit. We then select key frames by filtering out images that are similar or with low resolution. Specifically, we extract visual features for each candidate frame using pre-trained ResNet-50 <ref type="bibr" target="#b11">[12]</ref>. If the cosine similarity between two consecutive frames are above a certain threshold, we only keep one frame and delete the other. In the end, we obtain 131,135 cooking images (an average of 26.23 images for a recipe).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Alignment between Text and Image</head><p>We then align each cooking step with its cooking images. For recipes from Instructables, because each long step has been split into multiple mini-steps, we need to assign cooking images for each mini-step. For AllRecipes, the cooking images extracted from cooking video are not assigned to each cooking step in the recipe. Therefore, we ask human annotators to align cooking images to their corresponding cooking steps. Specifically, we hire 16 undergraduates who are native English speakers with cooking experience as annotators. We build an annotation platform in which the alignment task is formulated in the form of multiple-choice. For each step, we show its text description at the top, and its candidate cooking images below. The annotator is required to choose the image(s) that matches the text description, and choose "No Picture Present That is Related" if there is no image can be matched. Moreover, we also filter out irrelevant cooking steps in this process: if the text description is not related to cooking, the annotator is required to choose the "Sentence Not Related To Cooking Procedure". In total, 227,082 cooking images are aligned to 110,878 cooking steps, with 10,000 steps are doubly annotated to determine the inter-annotator agreement. The whole annotation process takes 2 months, costing 240 man-hours. The inter-annotator agreement reached a CohenâĂŹs Kappa of 0.82, suggesting a substantial agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cooking Workflow Construction</head><p>After each cooking step is aligned with its cooking images, we then construct the cooking workflow for each of the 10,071 recipes (5071 from Instructables; 5000 from AllRecipes) obtained after data processing. We hire 22 English-speaking undergraduate students with cooking experience to manually annotate the cooking flow. To facilitate the annotation, we build an annotation platform as shown in Figure <ref type="figure" target="#fig_4">3</ref>. The recipe is shown on the left table, with each row being a cooking step. Note that we filter out the steps labeled as irrelevant in the text-image alignment process. When the annotator We hire two students expertise in cooking to run a quality control over the annotated recipes, filtering out 221 lowquality annotations. Among the 9,850 valid recipes, 1,500 recipes are randomly sampled for double annotation to determine the interannotator agreement. We covert each annotation as an one-hot vector of all possible node pairs, based which the CohenâĂŹs Kappa is calculated as 0.71, suggesting a substantial agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data Statistics</head><p>The MM-ReS dataset contains 9,850 recipes, 110,878 steps, and 179,975 aligned cooking images. Detailed data statistics are in Table 2. The MM-ReS dataset has two distinct features compared with other existing food datasets. First, it is the first food dataset that has multi-modal information on step-level, with each cooking step associated with both texts and images. Existing food datasets either only have text (e.g., YOUCOOK2 <ref type="bibr" target="#b51">[52]</ref>) or images (e.g., Food-101 <ref type="bibr" target="#b0">[1]</ref>), or the cooking image is on the recipe-level rather than step-level (e.g., Recipe1M+ <ref type="bibr" target="#b24">[25]</ref>). Second, our dataset contains 9, 850 humanannotated cooking workflows; this scale exceeds other datasets with recipe workflows by almost two magnitudes, such as r-FG <ref type="bibr" target="#b47">[48]</ref> and the CURD <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>We first formally define the problem of cooking workflow construction, then introduce our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><formula xml:id="formula_0">A recipe R is composed of n cooking steps, denoted as R = {S 1 , • • • , S n },</formula><p>where S i is the i-th step. Each cooking step S is further represented as its text description and cooking images, i.e., S = {W, I}, where the text description</p><formula xml:id="formula_1">W = (w 1 , • • • , w | W |</formula><p>) is a word sequence, and</p><formula xml:id="formula_2">I = {x 1 , • • • , x | I |</formula><p>} is a set of cooking images. The cooking workflow of a recipe R is defined as a directed graph G = (V, E), where each cooking step S i is represented as a vertex in V. A directed edge e = ⟨S i , S j ⟩ from S i to S j exists if:</p><p>(1) i &lt; j, i.e., step S i appears before S j in the recipe.</p><p>(2) S i and S j have a causal dependency, i.e., we cannot perform step S j without finishing step S i .</p><p>Figure <ref type="figure" target="#fig_3">2</ref>(b) shows an example of cooking workflow.</p><p>Step 3 is a prerequisite step of 11 since the pan has to be prepared before adding blueberries into it. However, step 8 and step 9 can be processed in parallel since the dry ingredients and wet ingredients can be prepared independently. By following the edges, we can clearly tell how the food can be prepared in an efficient and collaborative way. Given a recipe R as input, the objective is to build the cooking workflow G. The major challenge lays in how to judge whether two steps have a causal dependency. We address this by extracting deep semantic features from both images and texts and proposing a neural model based on the Transformer <ref type="bibr" target="#b38">[39]</ref> and the Pointer Network <ref type="bibr" target="#b39">[40]</ref> to detect causal relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Framework</head><p>The i-th cooking step is</p><formula xml:id="formula_3">S i = {w i,1 , w i,2 , • • • , w i, L i }, {x i,1 , • • • , x i, M i } ,</formula><p>where L i is the number of words in the text description (w i, j denotes the j-th word), and x i,k is the k-th cooking image associated with the step. For each step S i , the goal of our model is to predict p(S j |S i ), i.e., the probability that S j is a prerequisite step of S i . Given a training set of N recipe-workflow pairs {(R (i) , G (i) )} N i=1 , our model is trained to maximize the following likelihood function:</p><formula xml:id="formula_4">N i=1 1 |E (i) | ⟨S j ,S k ⟩ ∈E (i ) log p(S k |S j )<label>(1)</label></formula><p>where E (i) is the set of edges in the workflow graph G (i) . Our model is designed as an encoder-decoder architecture, composed of a recipe encoder and a relation decoder. The recipe encoder is a hierarchical structure. First, a cooking step encoder is trained to learn the vector representation of a cooking step by integrating the information from text descriptions and cooking images (Section 4.3). The step embeddings are then fed into a transformerbased recipe encoder for capturing global dependencies between steps (Section 4.4). Finally, the relation decoder utilizes the information captured by the recipe encoder to predict the prerequisite steps for each step one by one using a pointer network (Section 4.5). Figure <ref type="figure" target="#fig_5">4</ref> shows the overall architecture of our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cooking Step Encoder</head><p>The cooking step encoder consists of an image encoder and an instruction encoder to learn embeddings for cooking images and instruction texts, respectively. The visual and textual embeddings are then fused to obtain the embedding for the cooking step.</p><p>Image Encoder. We use pre-trained ResNET-50 <ref type="bibr" target="#b11">[12]</ref> to extract features for cooking images. To make the model more adaptable to the food domain, we fine-tune the pre-trained ResNet-50 with Recipe1M <ref type="bibr" target="#b34">[35]</ref> dataset, which contains 251, 980 training images of 1, 047 food categories (e.g., chocolate cake, cookie). During finetuning, the image features of ResNET-50 are projected to a softmax output layer to predict the food category during training. After finetuning, we drop the softmax layer and use the outputs from last layer as image features. Given the cooking images {x i,1 , • • • , x i, M i } for step S i , the extracted visual feature for x i, j is denoted as f i, j . The image encoder takes the average of f i,1 , • • • , f i, M i as the visual embedding, denoted as F i .</p><p>Instruction Encoder. Given the text description of step S i , denoted as a word sequence {w i,1 , w i,2 , • • • , w i, L i }, we use the pretrained GLoVE <ref type="bibr" target="#b32">[33]</ref> as the word embeddings and employ a bidirectional LSTM <ref type="bibr" target="#b13">[14]</ref> to encode contextual information for words from both directions. We then aggregate the LSTM hidden states h i,1 , • • • , h i,T i into a single vector C i to represent the instruction text. Observing that some keywords like "another" and "set aside" may provide clues for casual relations, we obtain C i by applying a self-attention layer <ref type="bibr" target="#b38">[39]</ref> and aggregating the hidden states based on the learned attention weights. This endows the encoder with the ability to pay more attention to useful word clues. The vector C i is regarded as the semantic embedding of the instruction text.</p><p>Multi-Modal Fusion. We then propose the following two methods to fuse the image embedding F i and the instruction embedding C i . 1) Concatenation. We apply a linear transformation separately for F i and C i , and then we concatenate the two transformed vectors and apply a two-layer feed-forward to obtain the final step embedding, denoted as T i .</p><p>2) MMBT. The concatenation-based method does not consider the interactions between F i and C i . To address this, we employ the Multimodal Bitransformer model (MMBT) <ref type="bibr" target="#b18">[19]</ref> to capture highorder visual-textual interactions. MMBT feeds the text description and the cooking images together into a Transformer <ref type="bibr" target="#b38">[39]</ref> encoder to obtain the fused step embedding T i . We use the pretrained version of MMBT <ref type="foot" target="#foot_4">5</ref> and fine-tune it during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recipe Encoder</head><p>Intuitively, judging the causal relation between two steps S i and S j also requires understanding the context information around S i and S j . To this end, we input the n step embeddings {T 1 , • • • ,T n } into a recipe encoder, which is composed of a stack of M transformer layers <ref type="bibr" target="#b38">[39]</ref>. This outputs a set of contextualized step embeddings {E 1 , • • • , E n }, where each step embedding E t not only encode the information of the step S t , but also contains information of its context steps</p><formula xml:id="formula_5">S 1 , • • • , S t -1 , S t +1 , • • • S T .</formula><p>Each transformer layer has three sub-layers. Multi-head selfattention mechanism is used for capturing the global dependencies between the n steps in the recipe. A fusion gate is used to combine the input and output of the attention layer, which yields a selfaware and global-aware vector representation for each step. Finally, layer normalization <ref type="bibr" target="#b29">[30]</ref> is implemented as the last part of the layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Causal Relation Decoder</head><p>The causal relation decoder sequentially predicts the causal relations for each step from S 1 to S n . At time step t, the decoder takes the embedding of the current step S t as input, and it outputs a probability distribution over its previous steps {S 1 , • • • , S t -1 }, denoted as P t , where P t,k represents the probability for the step S k being a prerequisite step of S i .</p><p>The causal relation decoder is composed of a stack of M transformer layers, plus a pointer layer over the final output of the decoder stack. Specifically, each transformer decoder layer consists of three parts: the multi-head cross-attention mechanism, a fusion gate, and a normalization layer. The query for the multi-head attention is the input embedding for the current step t, and the keys and values are the encoder outputs {E 1 , • • • , E n }. This allows the decoder to attend over all the steps S 1 , • • • , S n in the recipe to gather relevant information to predict the causal relation of the current step t; in other words, having a global understanding of the recipe contexts in decision making. These attention outputs are then fed to a fusion gate, followed by a normalization layer, similar to the recipe encoder. We denote the final output vector from the last decoder layer at time step t as O t . Finally, we stack a pointer layer for predicting the probability distribution P t based on the decoder output O t , which is formulated as follows:</p><formula xml:id="formula_6">Q t = ReLU(O t W Q ); K = ReLU(E 1:t -1 W K )<label>(2)</label></formula><formula xml:id="formula_7">P t = Sigmoid( Q t K T √ d )<label>(3)</label></formula><p>where P t is the output of the pointer layer, in which P t,k represents the probability that a causal relationship exists from S k to S t . W Q and W K are parameter matrices. Note that we use the sigmoid function rather than softmax in the pointer layer as each step can be linked to multiple previous steps, therefore each P t,k is an independent probability between 0 and 1.</p><p>After decoding, we obtain a set of model predictions P = {P i, j |1 ≤ j &lt; i ≤ n}. We then use the following steps to construct the workflow graph. First, we select all edges ⟨S j , S i ⟩ satisfying P i, j &gt; θ as candidate edges, where θ is a pre-defined threshold and is set to 0.5 in the experiment. Second, we prune the graph by removing all redundant edges. Specifically, we remove the direct edge ⟨S j , S i ⟩ if there exists a longer path S j → • • • S k • • • → S i from S j to S i , because S j → S i is implied in this longer path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Data and Metrics</head><p>We evaluate the performance of our method for cooking workflow construction on the MM-ReS dataset. The 9, 850 recipes in the dataset are randomly split into training (80%), validation (10%), and testing set (10%). We report the performance on the testing set and tune the model parameters on the validation set. To evaluate the quality of the output workflow graph, we use the precision, recall, and F 1 score of predicting edges with respect to the ground-truth edges. The overall accuracy of the task is computed at the edge level (counting all edges in the data set), and at the recipe level (average accuracy over all recipes). Denote the ground-truth / predicted edge set for the i-th recipe in the test set as E (i) and Ê(i) , the edge-level precision (P e ) / recall (R e ) and the recipe-level precision (P r ) /recall (R r ) are calculated as follows:</p><formula xml:id="formula_8">P e = N i=1 e ∈ | Ê(i) | I(e ∈ E (i) ) N i=1 | Ê(i) | ; R e = N i=1 e ∈ |E (i ) | I(e ∈ Ê(i) ) N i=1 |E (i) | P r = N i=1 e ∈ | Ê(i) | I(e ∈ E (i) ) | Ê(i) | ; R r = N i=1 e ∈ |E (i ) | I(e ∈ Ê(i) ) |E (i) |<label>(4)</label></formula><p>I(s) is an indicator function, returning 1 if the condition s is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We conduct a comprehensive performance evaluation for the following 10 methods, which can be categorized into three groups based on the use of information.</p><p>5.2.1 Textual-Only. We first choose three baselines that only utilize instruction texts in the recipe to build cooking workflow.</p><p>• Hand-crafted Features. The work of <ref type="bibr" target="#b15">[16]</ref> proposed several hand-crafted textual features, such as TF-IDF, to detect causal relations between two recipe instructions and train an SVM for relation classification. The predicted pairwise relations are used as the edges of the workflow graph.</p><p>• BERT Pairwise Detector. To evaluate the effectiveness of deep textual features, we propose a baseline that applies BERT <ref type="bibr" target="#b7">[8]</ref> for pairwise causal relation detection. Specifically, we use BERT in double-sentence mode, in which the texts from step S i and S j are concatenated as a single sequence separated by a special token <ref type="bibr">[SEP]</ref>. After taking this concatenated sequence as input, the output vector of the BERT is then linked to a feed forward network with 2 hidden layers to predict casual relations.</p><p>• Ours (Instruction Encoder). For our ablation study, we also employ a variant of our model that only has the instruction encoder when encoding a step. In this way, we ignore the cooking images by removing the image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Image-Only.</head><p>We then include 3 baselines that detect causal relations purely based on the cooking images.</p><p>• Image Similarity Detector. This is a weak baseline that judges causal relations based on image similarity, defined as the normalized cosine distance between ResNet-50 visual features. As a step is associated with multiple cooking images, we calculate the average / maximum / minimum image similarity between the cooking images from two steps. An SVM classifier is trained to learn the weights of these three similarities for relation detection.</p><p>• Feed-forward Neural Detector. We adopt a two-layer feed forward neural network as a baseline for image-based relation detector.</p><p>It takes as input the concatenation of the ResNet-50 visual features from two steps, and outputs a binary classification on whether the two steps are in sequential.</p><p>• Ours (Image Encoder). This is a variant of our model that only has the image encoder when encoding a step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Multi-Modal</head><p>. Finally, we compare the following four methods that utilize multi-modal information of both images and texts.</p><p>• Multi-modal Hand-crafted Features. We create a baseline that enriches the feature set of <ref type="bibr" target="#b15">[16]</ref> by adding the three image similarity features described in the baseline Image Similarity Detector.</p><p>• MMBT Pairwise Detector. This baseline applies MMBT <ref type="bibr" target="#b18">[19]</ref> for pairwise causal relation detection. Specifically, we concatenate the texts and images from step S i and S j as a sequence of embeddings, which is taken as inputs of the MMBT model for predicting the casual relation between S i and S j . The MMBT model is trained by sampling the same number of sequential/parallel step pairs from the training set as positive/negative examples.</p><p>• Ours (Concatenation). This is our full model, using vector concatenation to fuse the visual and textual embeddings.</p><p>• Ours (MMBT). This is our full model, using MMBT to fuse the visual and textual embeddings.  Although utilizing multimodal information is in general beneficial, the means for multimodal fusion is key in taking full advantage of the two modalities. When applying simple feature concatenation (M3) to fuse the two modalities, it only leads to an average F 1 gain of 0.58, compared against the model utilizing only textual information (T3). However, we observe an average F 1 gain of 4.35 when using MMBT (M4) as the fusion method, compared with the method of feature concatenation (M3). We believe this is because the self-attention mechanism in MMBT allows the model to learn the semantic correspondence between cooking images and text descriptions, enabling a more efficient complementing between visual and textual information.</p><p>RQ2. Which modality is more effective in predicting casual relations -cooking images or instruction texts? Although the visual and textual information are complementary, textual information is a necessity in detecting casual relations. Our ablation study shows that the with instruction-only encoder (T3) outperforms the model with image-only encoder (I3) alone by 13.32 average F 1 . By comparing T1 with I1, textual features are also turned out to be more effective than visual features for hand-crafted features. This is inline with our intuition that judging casual relations require more deductive reasoning over cooking instructions, rather than inferring intuitively from cooking images. Another possibility is that the image embedding obtained by ResNET do not capture fine-grained visual features (e.g., color, position, or state of certain ingredients) that are crucial for casual relations.</p><p>RQ3. Do neural-based models perform better than models with hand-crafted features? Within all three groups of methods (text-only, image-only, and multi-modal), our transformer-based neural models (T3, I3, M4) significantly outperform the methods using hand-crafted features (T1, I1, M1). The average improvements are 19.81 in precision, 15.41 in recall, and 17.88 in F 1 . For text-only models, the BERT model (T2) outperforms the hand-crafted features (T1) by a large margin. This can be explained by BERT's ability to capture high-level linguistic features such as sentence composition and semantic dependency, which are very important for this task. For image-only models, the feed forward network (I2) achieves comparable results with the model using image similarity (I1), but our model with image encoder (I3) improves I1 by a large margin. This shows that image similarity already serves as a strong clue in determining pairwise casual relations, but the model is more effective when it gets access to all the cooking images in the recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We investigate the problem of automatically building cooking workflows for food recipes, leveraging both text and images. We build the Multi-Modal Recipe Structure dataset (MM-ReS), the first largescale dataset for this task, containing 9,850 food recipes with labeled cooking workflows. We also present a novel encoder-decoder framework, which applies Multimodal Bitransformers (MMBT) to fuse visual and textual features. Our solution couples the use of the transformer model and the pointer network to utilize the entire recipe context. Experimental results on MM-ReS show that considering multimodal features enables better performance for detecting causal relations, and the cooking images are highly complementary to procedure text.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1(a) Now roll out your dough and cut it into 6-8 slices. It's time to put the strawberries on. Don't forget to put a teaspoon of filling first. Sequential (a) Sequential relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Parallel (b) Parallel relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of sequential relationship (a) and parallel relationship (b) between two cooking steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The cooking recipe "Blueberry Crumb Cake" (a) and its corresponding cooking workflow (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our cooking workflow annotation platform.</figDesc><graphic coords="4,324.69,83.68,226.77,102.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The general framework of the proposed model for cooking workflow construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The cooking workflow of "Eggplant Parmesan" (a), and the predicted workflows for different methods (b-d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Ours (Image Encoder)A RESULT VISUALIZATION AND ERROR ANALYSISTo intuitively understand the effectiveness of multi-modal fusion, we select the recipe of "Eggplant Parmesan" for case study in Figure 5. The linking errors made by each method are highlighted in red. Three errors are made by the model when we only utilize the cooking image for casual relation detection (I3). The model tend to mistakenly treat two steps as sequential if their cooking images are visually similar. For example, Step 4 and Step 5 are in parallel but are predicted as sequential since they look similar visually. The same mistake happens between Step 6 and Step 7. The text-only model (T3) correct the above two errors, but mistakenly treat Step 2 and Step 3 as parallel, as shown in Figure 6(c). The sequential relation between these two steps are easy to judge visually, but hard to tell from text description because Step 3 does not mention "Eggplant" in contexts. When fusing both visual and textual information, our multi-modal model (M4) gets the best result, making only one error in this example (the relation between Step 4 and Step 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of collected recipes</figDesc><table><row><cell>Data Source</cell><cell>Instructables</cell><cell>AllRecipes</cell></row><row><cell># Recipes</cell><cell>32,733</cell><cell>64,500</cell></row><row><cell># Cooking Images</cell><cell>184,941</cell><cell>-</cell></row><row><cell># Sentences</cell><cell>161,046</cell><cell>120,615</cell></row><row><cell>Text Quality</cell><cell>Noisy</cell><cell>Clean</cell></row><row><cell>Cooking Steps</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The recipe-, image-, step-, and sentence-level data statistics of the MM-ReS dataset certain step, its cooking images are shown in the middle. The cooking workflow for the recipe is shown on the right, in which each node represents a step in the left recipe. Initially, the cooking workflow is empty with no link between nodes. The annotator is required to construct the workflow by chaining the nodes based on relations, where sequential relation results in a link between two nodes. The annotation takes 1 months, costing 310 man-hours.</figDesc><table><row><cell cols="2">Type Features</cell><cell>Number</cell><cell>Type</cell><cell>Features</cell><cell>Number</cell></row><row><cell></cell><cell>Number</cell><cell>9,850</cell><cell></cell><cell>Number</cell><cell>110,878</cell></row><row><cell></cell><cell># Recipes from Instructables</cell><cell>5,013 (50.9%)</cell><cell></cell><cell># Cooking Steps</cell><cell>81,615 (73.6%)</cell></row><row><cell></cell><cell># Recipes from AllRecipes</cell><cell>4,837 (49.1%)</cell><cell></cell><cell># Non-Cooking Steps</cell><cell>29,263 (26.4%)</cell></row><row><cell>Recipe</cell><cell># Avg. Steps / Recipe</cell><cell>11.26</cell><cell>Step</cell><cell cols="2"># Cooking Steps with Images 65,969 (80.83%)</cell></row><row><cell></cell><cell># Avg. Cooking Steps / Recipe</cell><cell>8.29</cell><cell></cell><cell># Avg. Sentences / Step</cell><cell>1.29</cell></row><row><cell></cell><cell># Avg. Tokens / Recipe</cell><cell>228.8</cell><cell></cell><cell># Avg. Tokens / Step</cell><cell>20.33</cell></row><row><cell></cell><cell># Avg. Images / Recipe</cell><cell>23.05</cell><cell></cell><cell># Avg. Images / Step</cell><cell>2.05</cell></row><row><cell>Image</cell><cell>Number # Images linked to recipe</cell><cell>227,082 179,975 (79.25%)</cell><cell>Sentence</cell><cell>Number # Avg. Tokens / Sentence</cell><cell>143,580 15.70</cell></row><row><cell>moves over a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with baselines and the ablation study. The best performance is in bold.</figDesc><table><row><cell></cell><cell>Models</cell><cell>P</cell><cell>Edge-level R</cell><cell>F 1</cell><cell>P</cell><cell>Recipe-level R</cell><cell>F 1</cell><cell>P</cell><cell>Average R</cell><cell>F 1</cell></row><row><cell></cell><cell>T1. Hand-crafted Features</cell><cell cols="6">57.64 54.39 55.97 59.91 57.66 58.76</cell><cell cols="3">58.78 56.03 57.37</cell></row><row><cell>Text-Only</cell><cell>T2. BERT Pairwise Detector T3. Ours (Instruction Encoder)</cell><cell cols="6">67.69 83.12 74.61 76.09 85.14 78.99 76.13 71.36 73.67 78.86 76.32 77.34</cell><cell cols="3">71.89 84.13 76.80 77.50 73.84 75.51</cell></row><row><cell></cell><cell>I1. Image Similarity Detector</cell><cell cols="6">42.10 47.33 44.56 36.81 67.64 43.96</cell><cell cols="3">39.46 57.49 46.80</cell></row><row><cell>Image-Only</cell><cell>I2. Feed-forward Neural Detector</cell><cell cols="6">35.97 47.15 40.81 50.27 56.51 50.77</cell><cell cols="3">43.12 51.83 45.79</cell></row><row><cell></cell><cell>I3. Ours (Image Encoder)</cell><cell cols="6">57.60 59.33 58.44 65.34 68.64 68.93</cell><cell cols="3">61.47 63.99 62.19</cell></row><row><cell></cell><cell cols="7">M1. Multi-modal Hand-crafted Features 60.38 58.76 59.56 62.23 60.05 61.12</cell><cell cols="3">61.31 59.41 60.34</cell></row><row><cell>Multi-Modal</cell><cell>M2. MMBT Pairwise Detector</cell><cell cols="6">61.86 87.40 72.44 73.76 88.80 78.90</cell><cell cols="3">67.81 88.10 75.67</cell></row><row><cell></cell><cell>M3. Ours (Concatenation)</cell><cell cols="6">71.20 76.32 73.67 76.26 80.79 78.50</cell><cell cols="3">73.73 78.56 76.09</cell></row><row><cell></cell><cell>M4. Ours (MMBT)</cell><cell cols="9">77.59 79.32 78.45 82.10 83.36 82.44 79.84 81.34 80.44</cell></row><row><cell cols="2">5.3 Performance Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>summarize the experimental results comparing against all baseline methods. We analyze the results by answering the following four research questions (RQ).RQ1. Do multi-modal models perform better than ones using a single modality? Utilizing multi-modal information does achieve better performance in general. M1 outperforms T1 by 2.97 in average F , as M1 has three additional visual-based features. This shows that cooking images provide complementary information to the original text-based feature set of T1. Similar results are also observed for neural models. After fusing the textual embeddings of T3 and the visual embeddings of I3 with MMBT, the resultant hybrid M4 model improves over T3 and I3 by 4.93 and 18.25, respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.instructables.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.allrecipes.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We exclude the initial steps that introduce ingredients.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.ffmpeg.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/facebookresearch/mmbt/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research is supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="funder">International Research Centres in Singapore Funding Initiative</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of <rs type="funder">National Research Foundation, Singapore</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Food-101 -Mining Discriminative Components with Random Forests</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings</title>
		<author>
			<persName><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10542" to="10550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal recipe retrieval with rich food attributes</title>
		<author>
			<persName><forename type="first">Jing-Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-MM</title>
		<meeting>ACM-MM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zero-shot Ingredient Recognition by Multi-Relational Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Jing-Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical Multiscale Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling concept dependencies in a scientific corpus</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gully</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cooking navi: assistant for daily cooking in kitchen</title>
		<author>
			<persName><forename type="first">Reiko</forename><surname>Hamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin'ichi</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuichi</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidehiko</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-MM</title>
		<meeting>ACM-MM</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="371" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cookpad Image Dataset: An Image Collection as Infrastructure for Food Research</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichiro</forename><surname>Someya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Kikuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1229" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling Restaurant Context for Food Recognition</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="430" to="440" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Personalized Classifier for Food Image Recognition</title>
		<author>
			<persName><forename type="first">Shota</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Amano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2836" to="2848" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting the structure of cooking recipes</title>
		<author>
			<persName><forename type="first">Jermsak</forename><surname>Jermsurawong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving structural analysis of cooking recipe text</title>
		<author>
			<persName><forename type="first">Shihono</forename><surname>Karikome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE technical report. Data engineering</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mise en place: Unsupervised interpretation of instructional recipes</title>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thandavam</forename><surname>Ganesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Ponnuraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised Multimodal Bitransformers for Classifying Images and Text</title>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrat</forename><surname>Bhooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Firooz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Testuggine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019 Workshop on Visually Grounded Interaction and Language (ViGIL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring prerequisite relations among concepts</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1668" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recovering concept prerequisite relations from university course dependencies</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Pursel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining learning-dependency between knowledge units from text</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="335" to="345" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperbolic Visual Embedding Learning for Zero-Shot Recognition</title>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9273" to="9281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Javier</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>CoRR abs/1810.06553</idno>
		<title level="m">Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time mobile recipe recommendation system using food ingredient recognition</title>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiyuki</forename><surname>Kawano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiji</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IMMPD</title>
		<meeting>IMMPD</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You Are What You Eat: Exploring Rich Recipe Information for Cross-Region Food Analysis</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="950" to="964" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Being a supercook: Joint food attributes and multimodal content modeling for recipe retrieval and exploration</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitao</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinda</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1100" to="1113" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flow Graph Corpus from Recipe Texts</title>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuro</forename><surname>Sasada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2370" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch Normalization in the final layer of generative networks</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Mullery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Whelan</surname></persName>
		</author>
		<idno>CoRR abs/1805.07389</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prerequisite relation learning for concepts in moocs</title>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1447" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation</title>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="875" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inverse Cooking: Recipe Generation From Food Images</title>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Giró-I-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10453" to="10462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3020" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Cross-Modal Embeddings for Cooking Recipes and Food Images</title>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3068" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crowdsourced comprehension: predicting prerequisite structure in wikipedia</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talukdar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Building Educational Applications Using NLP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SOUR CREAM: Toward semantic processing of recipes</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CMU-LTI-08-005</idno>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointer Networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Workflow extraction from cooking recipes</title>
		<author>
			<persName><forename type="first">Kirstin</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirjam</forename><surname>Minor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Bergmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCBR 2011 Workshops</title>
		<meeting>the ICCBR 2011 Workshops</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Cross-Modal Embeddings With Adversarial Networks for Cooking Recipes and Food Images</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11572" to="11581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Substructure similarity measurement in chinese recipes</title>
		<author>
			<persName><forename type="first">Liping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using prerequisites to extract concept maps fromtextbooks</title>
		<author>
			<persName><forename type="first">Shuting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Pursel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hybrid semantic item model for recipe search by example</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ISM</title>
		<meeting>IEEE ISM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="254" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geolocalized Modeling for Dish Recognition</title>
		<author>
			<persName><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1187" to="1199" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A method for extracting major workflow composed of ingredients, tools, and actions from cooking procedural text</title>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Imahori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMEW</title>
		<meeting>ICMEW</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A method for extracting major workflow composed of ingredients, tools, and actions from cooking procedural text</title>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Imahori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICME Workshops</title>
		<meeting>ICME Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature extraction and summarization of recipes using flow graph</title>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Imahori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsumi</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SocInfo</title>
		<meeting>SocInfo</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="241" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cooking Recipe Search by Pairs of Ingredient and ActionâĂŤWord Sequence vs Flow-graph RepresentationâĂŤ</title>
		<author>
			<persName><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuro</forename><surname>Sasada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Imahori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Japanese Society for Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Concept graph learning from educational data</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards Automatic Learning of Procedures From Web Instructional Videos</title>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7590" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
