<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration *</title>
				<funder ref="#_aEvdwk3">
					<orgName type="full">NCRR</orgName>
				</funder>
				<funder ref="#_cDUe9GQ #_DvcfuCA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-10-07">October 7, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erich</forename><surname>Kummerfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Health Informatics</orgName>
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Rix</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of biostatistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simulations evaluating resampling methods for causal discovery: ensemble performance and calibration *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-07">October 7, 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.02047v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery can be a powerful tool for investigating causality when a system can be observed but is inaccessible to experiments in practice. Despite this, it is rarely used in any scientific or medical fields. One of the major hurdles preventing the field of causal discovery from having a larger impact is that it is difficult to determine when the output of a causal discovery method can be trusted in a real-world setting. Trust is especially critical when human health is on the line.</p><p>In this paper, we report the results of a series of simulation studies investigating the performance of different resampling methods as indicators of confidence in discovered graph features. We found that subsampling and sampling with replacement both performed surprisingly well, suggesting that they can serve as grounds for confidence in graph features. We also found that the calibration of subsampling and sampling with replacement had different convergence properties, suggesting that one's choice of which to use should depend on the sample size.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many scientific disciplines seek to use controlled experiments to build, contradict, or confirm causal models in their respective fields. It is often not possible to conduct controlled experiments, however, for a variety of practical and ethical reasons. Fields that study human health are especially vulnerable to these limitations. In such circumstances, researchers must rely on observational data. Causal discovery methods provide a way to learn causal information from observational data, and so one might expect causal discovery methods to be heavily utilized in the medical sciences, but at the time of writing this is not the case. One major reason for this is that, unlike in some other machine learning domains, it is difficult to determine whether or not the output of a causal discovery method is trustworthy in a real-world setting. Prediction algorithms can be evaluated relatively simply by, for example, measuring the area under the receiver operator curve (AUC) on a holdout sample. However causal graphs do not lend themselves to such an evaluation. An alternative approach is needed.</p><p>In this paper we investigate one approach to evaluating the performance of causal discovery methods: resampling. Resampling methods such as the bootstrap, and less commonly the jackknife, have been used heavily in other areas of statistics and machine learning, and it seems natural to extend this approach to causal discovery. Many of the known statistical properties of the bootstrap and jackknife do not apply in an obvious way to the causal discovery setting, however, leaving open the problem of evaluating resampling as a method for evaluating causal discovery applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>We are only aware of one previous publication investigating resampling calibration in the context of causal discovery algorithms. Naeini, Jabbari, and Cooper <ref type="bibr" target="#b14">[15]</ref> examined the the calibration of directed edges in a bootstrapped version of the Really Fast Causal Inference <ref type="bibr" target="#b4">[5]</ref> algorithm and found that they were generally well calibrated. In this paper, we look at both bootstrapping and jackknifing, use a different simulation setup, use a different base search algorithm, and evaluate all edge types.</p><p>Most of the previous work in bootstrapping graphical models has been done in the broader field of learning bayesian networks. Friedman, Goldszmidt, and Wyner successfully used bootstrapped hill-climbing, showed that bootstrapped hill-climbing's edge probabilities were well calibrated, and found that bootstrapped hill-climbing substantially outperformed the unbootstrapped version <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Steck and Jaakola showed that when using hill-climbing on bootstrapped data, the Bayesian Information Criterion (BIC) score is biased towards overly dense graphs, sometimes severely, and derived a correction <ref type="bibr" target="#b22">[23]</ref>. They also showed that the jackknife is not affected by this bias. It is unclear at this time whether these results extend to causal discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Methods</head><p>This paper reports the results of a series of simulations that we ran as a first step towards a better understanding of resampling in the context of causal discovery. Three different types of simulations were run: one simulation randomly generated causal models with random structures and random linear Gaussian parameters, and two simulations used existing expert models meant to be representative of medical phenomena. In these simulations, data were generated from a causal graphical model, and then repeatedly resampled, resulting in large collections of data sets. Each of these resampled data sets was then analyzed with a causal discovery algorithm, producing a corresponding large collection of graphs. We then calculated the proportion of times each pair of variables had a particular kind of relationship, as represented by the edge (or lack thereof) connecting them, within this collection of graphs. Finally, we evaluated these proportions as forecasts of whether the edge was present or not in the data generating model, and calculated both the full Brier score of the forecasts as well as their reliability (calibration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Results</head><p>In our simulations, subsampling (jackknifing) and resampling with replacement (bootstrapping) performed well both in terms of overall Brier score and in terms of reliability, even at smaller sample sizes, for linear Gaussian networks. Both procedures had poor Brier scores with data simulated from expert models over categorical variables. The jackknife also had poor calibration on the expert models, while the bootstrap had excellent calibration on one expert model and poor calibration on the other. The two procedures appeared to have different convergence properties when the sample size was varied. The bootstrap had better performance than the jackknife in most, but not all, circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Outline</head><p>This paper proceeds as follows. In section 2, a brief background is provided on the material covered in this paper: causal discovery, resampling, and the Brier score, a standard method for evaluating calibration. Section 3 provides the specifications for the simulations we ran. Section 4 provides the outcomes of those simulations, including evaluations of both the ensemble accuracy and calibration of the resampling techniques. The paper concludes with section 5, which discusses the results, their implications, and their limitations, and considers future directions for this line of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal discovery</head><p>Causal discovery algorithms estimate the structure of a causal model <ref type="bibr" target="#b16">[17]</ref>, represented as a graph, using data generated from that model <ref type="bibr" target="#b21">[22]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of a causal graph. Because of the general structure of the problem, these algorithms have a common high-level form: they take data as input, and produce a graph as output. Beyond that, however, they vary considerably. The field is too large to review here, so we will only discuss the algorithm used in this investigation; for recent introductory papers on causal discovery, see <ref type="bibr" target="#b5">[6]</ref> or <ref type="bibr" target="#b11">[12]</ref>.</p><p>Greedy Equivalence Search (GES) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19</ref>] is a fast and scalable causal discovery algorithm that is correct when there are no causal cycles or unmeasured common causes. It accomplishes this by performing an intelligent search within the space of all causal models to find the model that optimizes a model fit statistic chosen by the user. The most common fit statistic used for this purpose is the Bayesian Information Criterion (BIC) score:</p><formula xml:id="formula_0">BIC = -2 log L + k log n</formula><p>Where L is the likelihood of the data given the model, n is the sample size, and k is the number of free parameters in the model. L is computed after fitting the model's free parameters to the data with a maximum likelihood estimate (MLE) procedure, so it is the highest likelihood that the model can assign to the data. The implementation of GES used in this paper includes a penalty discount parameter as well, which acts as a tuning parameter for preferring more complex or more simple models by modifying the strength of the second term in the BIC score. This modified BIC score's second term is thus dk log n, where d is the user-assigned value for the penalty discount parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Resampling</head><p>The bootstrap was introduced by Efron <ref type="bibr" target="#b6">[7]</ref> and comes in two primary forms: the parametric bootstrap fits a (parametric) model to the data and samples from the model, and the non parametric bootstrap instead samples from the empirical distribution function by sampling with replacement from the data. We use only non parametric bootstrapping in this paper. In statistics, the bootstrap can be used for many things, including estimating the standard error of an estimator, and constructing confidence intervals. The delete d jackknife is a similar procedure, where one takes the original sample and randomly deletes d observations from the data. d is usually chosen on the order of √ n, as small numbers can lead to inconsistency when the statistic of interest is not smooth. We chose to include the jackknife due to <ref type="bibr" target="#b22">[23]</ref>, where the jackknife performed well and generally chose sparser models than the bootstrap in the the context of Bayesian Networks.</p><p>In machine learning the bootstrap has primarily been used as a way to improve the performance of prediction algorithms via bootstrap aggregating, or bagging. Bagging was introduced by Breiman <ref type="bibr" target="#b1">[2]</ref> as way to improve the predictive performance of learning algorithms. Bagging involves generating m bootstrap replicates of the original data and employing a rule to condense the m models into a single value. In the case of bagging classifiers, <ref type="bibr" target="#b1">[2]</ref> suggests generating a frequency vector and classifying based off the most frequent observation, which is the procedure we used in this paper. In the context of learning Bayesian networks, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b8">[9]</ref> define the probability of a graph feature (e.g. individual edges and adjacencies) as</p><formula xml:id="formula_1">p n (f ) = 1 |D n | Dn I(f ∈ Ĝ(D n ))</formula><p>where D n is any possible data set of size n sampled from the Bayesian network B, and Ĝ(D n ) is the graph learned from D n . They use the bootstrap to estimate p</p><formula xml:id="formula_2">p n (f ) ≈ 1 m m I(f ∈ Ĝ(D m n ))</formula><p>where D m n is the m-th bootstrapped data set. As <ref type="bibr" target="#b10">[11]</ref> notes, the resulting feature frequencies from the bagging procedure are not probabilities. We treat these quantities only as frequencies with evaluable calibration.</p><p>Resampling adds a few levels of complexity to typical causal discovery. First, there are different resampling procedures to choose from. Second, running the learning algorithm on resampled data can be biased <ref type="bibr" target="#b22">[23]</ref>, which may require adjustment in other parts of the procedure in order to correct for this bias. Third, there are multiple ways to use the resampling graphs, such as aggregating them into a single object or looking at the distribution of their features. We explore both of these uses in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Brier score and reliability</head><p>In order to evaluate the calibration of these resampling techniques for GES, we treat the resampling frequencies as forecasts, and evaluate their forecasting ability using the Brier score <ref type="bibr" target="#b2">[3]</ref>, a standard tool for evaluating the performance of probabilistic forecasts for binary events. The Brier score is simply the mean squared error between the predicted probability and the observed outcomes:</p><formula xml:id="formula_3">Brier Score = 1 n n s=1 (f s -o s ) 2</formula><p>where n is the sample size, f s is the forecast probability of the event in sample s, and o s is the observed outcome in sample s, with 0 meaning the event did not occur and 1 meaning the event did occur. Since this is a measure of error, larger values are worse: the best Brier score possible is 0, while the worst score possible is 1.</p><p>We also used one of the components that the Brier score can be decomposed into <ref type="bibr" target="#b13">[14]</ref>, typically called reliability or calibration. Here we will call it reliability, in order to distinguish it from the more general concept of calibration. This component evaluates how close each forecast is to the actual frequency with which the event occurs when that forecast was made:</p><formula xml:id="formula_4">Reliability = 1 n K k=1 n k (f k -ōk ) 2</formula><p>where K is the number of distinct forecasts, n k is the number of samples where forecast k was made, f k is the forecast probability of the event when forecast k is made, and ōk is the observed frequency of the event occurring among samples where forecast k was made. Like with the overall Brier score, smaller reliability scores are preferred. The optimal reliability is 0, while the worst possible reliability is 1. Ferro and Fricker <ref type="bibr" target="#b7">[8]</ref> showed that the standard decomposition of the Brier score is biased, and provided a modified version of reliability with less bias. We used both versions, and found the difference to be minor. We report only the bias-corrected reliability in this paper.</p><p>3 Simulation Specifications</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulation design</head><p>We implemented a series of simulations to evaluate the performance of resampling as a method for determining confidence in individual applications of causal discovery. The overall structure of these simulations is as follows.</p><p>First, a graphical model is selected and its parameters are assigned values. For the linear Gaussian models, the graphical structures were randomly generated, and the parameter values are randomly drawn from defined distributions. For the expert model simulations, both the graphical structures and parameter values were determined by the expert model. Second, data are randomly generated from the model, up to a specified sample size. For all simulations, sample sizes were varied from 100 to 1000 in increments of 100. Both steps are then repeated, resulting in a collection of matched graph, data pairs. Finally, each data set is analyzed, and the output of that analysis is compared against the matched data-generating graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm and resampling specifications</head><p>While we varied our overall analysis techniques, we only made use of one pre-existing causal discovery method, the Greedy Equivalence Search (GES) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. GES was always run with the Bayesian Information Criterion (BIC) and a penalty discount of 2, and all other parameters were left at their default values. We used R <ref type="bibr" target="#b17">[18]</ref> to automate our application of GES to all simulated data sets. For all simulations we used the implementation of GES found in Rix's open source R package "Causality"<ref type="foot" target="#foot_0">foot_0</ref> , as we found it to be faster than alternative methods for running GES in R.</p><p>We used two different resampling methods. First, based on the bootstrap method, we used sampling with replacement to the full original sample size. Second, based on the jackknife method, we used sampling without replacement to 90% of the original sample size. In both cases, 200 resampled data sets would be made from the original data set. For the rest of the paper we will refer to these procedures simply as "bootstrap" and "jackknife" procedures.</p><p>The resampling methods were used in two different ways. For a direct comparison with standard GES, we used the resampling methods to produce an ensemble estimate of the data generating graph via a bagging procedure. The ensemble estimate is calculated by taking the set of output graphs from the set of resampled data sets, and letting the graphs vote on the relationship between every pair of variables, with the highest vote winning. This produces a partially directed graph as output, but is not guaranteed to be a pattern, or even to be acyclic.</p><p>We also used the resampling methods to produce forecasts of the edges of the data generating graph. The forecasts were calculated similar to that of the ensemble estimates, except rather than producing a single graph as output, the output is a table of the proportion of votes each relationship got for each variable pair. These proportions can then be treated as forecasts of the probability that that variable pair has that particular relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model specifications</head><p>Linear Gaussian models We produced 500 independent graph, data pairs of linear Gaussian models and data sets at each sample size. The data was generated using the graphical user interface of the open source Tetrad software package<ref type="foot" target="#foot_1">foot_1</ref> , version 6.6.0. All graphs were generated with 100 variables and 100 edges. Their structures were randomly selected with the random forward process. Each graph was assigned parameters independently in the following way. Each variable is a weighted sum of the value of its parents in the graph and its own independent noise term. Each variable's independent noise term has a Normal distribution with mean 0 and a variance drawn uniformly between 1 and 3. The weight of the independent noise term is always 1, and each incoming edge has a weight drawn from SplitUniform(-1.5, -0.5; 0.5, 1.5).</p><p>Expert models We used the Child <ref type="bibr" target="#b20">[21]</ref> and Hepar2 <ref type="bibr" target="#b15">[16]</ref> expert models. Data from both was generated using the bnlearn R package <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. 100 data sets were generated from each model at each sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear Gaussian simulation results</head><p>Graph estimation performance To evaluate the impact of using resampling ensembles on GES, we calculated the Structural Hamming Distance (SHD) <ref type="bibr" target="#b23">[24]</ref> of the standard GES algorithm as well as the jackknife and bootstrap ensemble methods. SHD is a numeric summary of the edgewise distance of the graph estimate from the data generating graph: each missing edge, extra edge, or wrongly oriented edge adds 1 point to the SHD. Figure <ref type="figure" target="#fig_1">2</ref> shows a plot of the SHD by sample size in our simulations. Since in these simulations the data generating graphs all had 100 nodes and 100 edges, these errors are fairly small, suggesting that our linear Gaussian data generation procedure was relatively easy for GES to learn, even at lower sample sizes.</p><p>Perhaps because the learning problem was relatively easy, the difference between the three methods were small. The jackknife ensemble approach had similar SHD to standard GES, while the bootstrap ensemble approach performed worse at the lowest sample size of 100, but by sample size 300 was slightly outperforming the other two methods. We also calculated and plotted the adjacency recall and precision, and arrowhead recall and precision, for these methods, but omit them here as the results were captured by the SHD plot. Those plots are available on request.</p><p>Forecasting performance To evaluate the bootstrap and jackknife procedures as methods for forecasting the probability that a given edge was present in the data generating model, we calculated the Brier score and reliability of each method. Figure <ref type="figure" target="#fig_2">3</ref> shows a plot of the Brier scores of the bootstrap and jackknife procedures across all sample sizes. This plot mimics the ensemble performance plot, as the bootstrap procedure had worse Brier scores at lower sample sizes, but better scores at higher sample sizes. Compared to the ensemble plot, the crossover point also occurs at a higher sample size, as the bootstrap procedure does not catch up to the jackknife procedure until the sample size is at least 400. Overall, though, both methods performed reasonably well, with their Brier scores at or below .05 for most sample sizes. Figure <ref type="figure" target="#fig_3">4</ref> shows a plot of the bias-corrected reliability of the bootstrap and jackknife procedures across all sample sizes. Overall both methods performed well, as the bootstrap procedure never rose above 0.01, corresponding to forecasts that are typically within 10% of the observed event frequency, and the jackknife procedure never rose above 0.003, corresponding to forecasts that are typically within 5% of the observed event frequency. This plot again mimics the ensemble performance plot and Brier score plot, however the difference between the methods appears more pronounced, with the bootstrap procedure performing proportionally much worse than the jackknife at sample sizes below 400, but much better at sample sizes above 800. Also of note, the bootstrap's reliability appears to converge quickly towards 0 as the sample sizes increases, while the jackknife's reliability appears independent of sample size, staying near 0.002 at all sample sizes. Figure <ref type="figure" target="#fig_4">5</ref> shows a plot of the actual frequency of correct edges against the forecast frequency of those edges being correct, for both methods at sample sizes 100, 500, and 1000. Visual inspection of this plot also suggests that both methods are well calibrated, but reveals some additional differences in their performance. Forecasts from the bootstrap procedure appear to underestimate the actual frequency, while forecasts from the jackknife procedure appear to overestimate the actual frequency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expert model simulation results</head><p>Hepar2 The Hepar2 model was developed by Onisko <ref type="bibr" target="#b15">[16]</ref> as a model of the causes and effects of liver disorders. It was originally developed to aid in the diagnosis of liver disorders. It has 70 nodes and 123 edges. The model was downloaded from the bnlearn Bayesian Network Repository, and the data were simulated from the model using the bnlearn package <ref type="bibr" target="#b19">[20]</ref>.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the accuracy performance of GES, GES with a bootstrap ensemble, and GES with a jackknife ensemble on the Hepar2 simulated data. All methods performed poorly on all sample sizes tested: SHD ranged between 90 and 130, which is a large number of errors given the size of the hepar2 model. All methods appear to still be in the process of converging by sample size 1000. The bootstrap ensemble outperformed both raw GES and the jackknife ensemble. An inspection of the more detailed recall and precision statistics of these methods revealed that most errors were recall errors. All methods had fairly high precision, meaning that identified edges were typically correct, however all methods also had very low recall, meaning that the methods typically did not find most of the edges in the hepar2 model. Figures <ref type="figure" target="#fig_6">7</ref> and<ref type="figure" target="#fig_7">8</ref> show plots of the resampling procedures' Brier scores and reliability, respectably. The bootstrap procedure performed notably better in both cases. While neither method did as well as we might want on total Brier score at any sample size, the bootstrap procedure's forecasts were still highly reliable at all sample sizes. Somewhat interestingly, neither procedure appears to be converging monotonically towards the ideal Brier score or reliability in the sample sizes tested. For both Brier score and reliability, the jackknife procedure is actually performing worse as the sample size grows, until 300 samples, where it turns and improves continuously up through sample size 1000. For the bootstrap procedure, performance slowly gets worse until around sample size 500, where it seems to stabilize.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> shows a calibration plot for both procedures at sample sizes 100, 500, and 1000. Both methods were typically overestimating the likelihood of edges being correct, with the jackknife procedure making more extreme errors of this kind than the bootstrap procedure. The jackknife's calibration performance is also much noisier than the bootstrap's calibration   child The Child model <ref type="bibr" target="#b20">[21]</ref> models the effects of birth asphyxia and the outcomes of some related medical exams. It has 20 nodes and 25 edges. The model was downloaded from the bnlearn Bayesian Network Repository, and the data were simulated from the model using the bnlearn package <ref type="bibr" target="#b19">[20]</ref>.</p><p>Figure <ref type="figure" target="#fig_9">10</ref> shows the accuracy performance of GES, GES with a bootstrap ensemble, and GES with a jackknife ensemble on the Child simulated data. As with the Hepar2 simulation, all methods performed poorly for the sample sizes tested, and appear to still be converging at sample size 1000. The bootstrap ensemble again appears to have the best performance, although the difference is small. Figures <ref type="figure" target="#fig_10">11</ref> and<ref type="figure" target="#fig_11">12</ref> show plots of the Brier scores and reliability, respectably, for both resampling procedures. As with the Hepar2 simulation, Brier scores were poor overall, the jackknife procedure had poor reliability, and the bootstrap procedure outperformed the jackknife procedure on both measures at all sample sizes. Unlike the Hepar2 simulation, the bootstrap did not have strong reliability in the Child simulation. Also unlike the Hepar2 simulation, the Brier scores and reliability for the bootstrap and jackknife procedures changed in similar ways as the sample size increased from 100 to 1000. Both traced a concave shape, with performance degrading from sample size 100 to approximately sample size 500, where it began to improve in an accelerated manner up to sample size 1000, the maximum tested.  Figure <ref type="figure" target="#fig_12">13</ref> shows a calibration plot for both procedures at sample sizes 100, 500, and 1000. As with the Hepar2 simulation, overestimating actual performance is more common than understimating performance. 5 Discussion and conclusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implications</head><p>In carrying out these simulations, we hoped to begin answering some important questions about resampling in the context of causal discovery. For example, when it comes to estimating the causal graphical model of an unknown causal data generating process, should we prefer off-the-shelf causal discovery methods, or a bootstrap ensemble, or a jackknife ensemble? These simulations suggest that bootstrapping might be preferred, and that the difference between all three options may not be large, at least for the GES algorithm. It also appears to be the case that this question does not have a simple answer: the preferred method might depend on sample size and the difficulty of the task.</p><p>Another question is: Should we prefer bootstrapping or jackknifing as indicators of confidence in discovered graph features? As with accuracy, it again appears that the preferred method might depend on sample size and the difficulty of the task. Overall, though, in these simulation bootstrapping had better calibration than jackknifing in most cases.</p><p>Finally, can resampling be considered a reliable way to evaluate confidence in discovered graph features? These simulations indicate that in some circumstances it can be. Bootstrapping had very good reliability for both the linear Gaussian and Hepar2 simulations, which is impressive given that the data generating models used in these two simulations are quite different. While its performance on the Child simulation was far from what one would want, the plot suggests that it may improve at higher sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Limitations and future directions</head><p>Our findings have a number of limitations. First, they may not generalize to other search algorithms, such as PC or FCI <ref type="bibr" target="#b21">[22]</ref>, as we only tested the GES algorithm. In our limited experience PC performs substantially better when bootstrapped, but we have not carried out a complete simulation study to confirm this. It is also unclear how well resampling on causal discovery algorithms that consider latent variables performs, although some results on that problem has been produced by <ref type="bibr" target="#b14">[15]</ref>. These areas provide a natural extension to the work presented in this paper, and would be useful for some of our current work in medical science domains <ref type="bibr" target="#b0">[1]</ref> Our findings also may not apply to real world problems that are very different from the simulated models and expert models evaluated here, or to data sets with sample sizes outside the range of those we tested. We only looked at sample sizes in the range of 100 to 1000. Sample sizes below 100 should probably be analyzed with caution under any circumstances, but there are many real world data sets with sample sizes much larger than 1000.</p><p>We also only considered a small subset of possible kinds of graphical structures. The linear Gaussian simulations were fairly sparse, and we only considered two expert models, which have invariant causal structure. Our findings may not apply to data generating models with graphs that are very dense or very sparse, with small-world graphs, or with graphs structurally different than those we used in our simulations in other ways.</p><p>None of our simulations included models with more than 100 variables. As such, our results may not apply to data with thousands of variables or more. We also did not consider continuous variable models with non-Gaussian noise or nonlinear relationships, so our results may not apply to data-generating models that have non-Gaussian noise or nonlinear relationships.</p><p>Overall, it is still unclear at this time why calibration may have good performance in some circumstances and bad performance in other circumstances. If resampling is not always well calibrated, we should identify features, ideally identifiable from the data themselves or from easily accessible meta-data, that determine the calibration of these resampling methods. Extending these simulations to other algorithms and other data-generating models will help us to understand what those features might be.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a causal graph. In this example, Alcohol Use and Gallstones cause Liver Disorder, which in turn causes Fatigue.</figDesc><graphic coords="4,212.40,72.00,187.20,105.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Structural Hamming Distance of algorithm output on raw data compared to data generating model. Although overall performance is good relative to the size of the graphs, errors are still present.</figDesc><graphic coords="8,189.00,72.00,234.00,197.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plot of the Brier scores of forecasts from the bootstrap and jackknife procedures.</figDesc><graphic coords="8,189.00,431.29,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of the bias-corrected reliability (calibration) of the bootstrap and jackknife procedures.</figDesc><graphic coords="9,189.00,158.01,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on simulated linear Gaussian data sets with sample sizes 100, 500, and 1000. The diagonal black line represents perfect forecasts.</figDesc><graphic coords="9,189.00,470.30,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of Structural Hamming Distance for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="10,189.00,310.19,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of Brier score for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="11,189.00,126.59,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of reliability for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="11,189.00,455.57,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model with sample size 500. The diagonal black line represents perfect forecasts.</figDesc><graphic coords="12,189.00,99.54,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparison of Structural Hamming Distance for bootstrap and jackknife procedures on data sets simulated from the child expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="12,189.00,484.78,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison of Brier score for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="13,189.00,201.35,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison of reliability for bootstrap and jackknife procedures on data sets simulated from the Hepar2 expert model, for sample sizes 100 to 1000. Lower values are better.</figDesc><graphic coords="13,189.00,439.08,234.00,156.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison of forecast accuracy to actual accuracy for bootstrap and jackknife procedures on data sets simulated from the Child expert model with sample size 500. The diagonal black line represents perfect forecasts.</figDesc><graphic coords="14,189.00,72.00,234.00,156.54" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/tzimiskes/causality</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.phil.cmu.edu/tetrad/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>The authors would like to thank the <rs type="institution">Center for Causal Discovery</rs> for developing and supporting the open source Tetrad software package.</p></div>
			</div>
			<div type="funding">
<div> *   <p>This work was supported by funding from <rs type="funder">NCRR</rs> <rs type="grantNumber">1UL1TR002494-01</rs>, <rs type="grantNumber">1R01MH116156-01A1</rs>, and <rs type="grantNumber">1R03MH117254-01A1</rs> to EK.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aEvdwk3">
					<idno type="grant-number">1UL1TR002494-01</idno>
				</org>
				<org type="funding" xml:id="_cDUe9GQ">
					<idno type="grant-number">1R01MH116156-01A1</idno>
				</org>
				<org type="funding" xml:id="_DvcfuCA">
					<idno type="grant-number">1R03MH117254-01A1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal network modeling of the determinants of drinking behavior in comorbid alcohol use and anxiety disorder</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Anker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Burwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alcoholism: clinical and experimental research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="91" to="97" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of bayesian-network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002-02">Feb (2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the foundations of causal discovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap methods: Another look at the jackknife</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bias-corrected decomposition of the brier score</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Fricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1954" to="1960" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data analysis with bayesian networks: A bootstrap approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the application of the bootstrap for computing confidence measures on features of induced bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer New York Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal discovery algorithms: A practical guide</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Danks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12470</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
	<note>UAI&apos;95</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new vector partition of the probability score</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An assessment of the calibration of causal relationships learned using rfci and bootstrapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Workshop on Data Mining for Medical Informatics</title>
		<imprint>
			<publisher>Causal Inference for Health Data Analytics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic causal models in medicine: Application to diagnosis of liver disorders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Onisko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Ph. D. dissertation, Inst. Biocybern. Biomed. Eng., Polish Academy Sci</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Warsaw, Poland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<title level="m">R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of data science and analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning bayesian networks with the bnlearn R package</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning in probabilistic expert systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="447" to="465" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bias-corrected bootstrap and model uncertainty</title>
		<author>
			<persName><forename type="first">H</forename><surname>Steck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
