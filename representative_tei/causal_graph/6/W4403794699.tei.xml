<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View</title>
				<funder ref="#_Yhu3HJm">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-16">16 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
							<email>jing.ma5@case.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<postCode>44106</postCode>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-16">16 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2409.09858v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph learning</term>
					<term>Causality</term>
					<term>Trustworthy AI</term>
					<term>Generalization</term>
					<term>Causal machine learning</term>
					<term>Graph neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph machine learning (GML) has been successfully applied across a wide range of tasks. Nonetheless, GML faces significant challenges in generalizing over out-of-distribution (OOD) data, which raises concerns about its wider applicability. Recent advancements have underscored the crucial role of causality-driven approaches in overcoming these generalization challenges. Distinct from traditional GML methods that primarily rely on statistical dependencies, causality-focused strategies delve into the underlying causal mechanisms of data generation and model prediction, thus significantly improving the generalization of GML across different environments. This paper offers a thorough review of recent progress in causality-involved GML generalization. We elucidate the fundamental concepts of employing causality to enhance graph model generalization and categorize the various approaches, providing detailed descriptions of their methodologies and the connections among them. Furthermore, we explore the incorporation of causality in other related important areas of trustworthy GML, such as explanation, fairness, and robustness. Concluding with a discussion on potential future research directions, this review seeks to articulate the continuing development and future potential of causality in enhancing the trustworthiness of graph machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, graph machine learning (GML), such as graph neural network (GNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, has garnered tremendous attention across various research communities, springing up in many high-stakes scenarios such as economic analysis <ref type="bibr" target="#b2">[3]</ref>, scientific discovery <ref type="bibr" target="#b3">[4]</ref>, crime prediction <ref type="bibr" target="#b4">[5]</ref>, and pandemic screening <ref type="bibr" target="#b5">[6]</ref>. Despite its burgeoning success in various tasks, GML still faces critical challenges, particularly in generalization on out-of-distribution (OOD) data, which casts doubts on its trustworthiness for broader applications. Compared with other data types, generalization on graphs faces unique challenges due to the complex nature of graph structure and the intrinsic dependencies within it, along with the multiple types of distribution shifts in node attributes and graph structure. Therefore, directly employing OOD generalization approaches in other data types often fails on graphs. The unique challenges of GML generalization have spurred a wave of research aimed at enhancing the generalization capabilities of GML through diverse approaches <ref type="bibr" target="#b6">[7]</ref>.</p><p>Among existing studies in GML generalization, causality-involved GML methods <ref type="bibr" target="#b7">[8]</ref> have made eye-catching progress. Unlike conventional GML approaches that primarily leverage statistical dependencies for downstream tasks, recent studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> suggest that causality often plays a more pivotal role in understanding the underlying data generation mechanisms. For instance, traditional GML techniques tend to overfit and rely excessively on spurious correlations <ref type="bibr" target="#b10">[11]</ref>, which, while effective in some scenarios, often fail to capture the causal mechanisms that fundamentally govern the relationships within the data, thereby undermining the models' ability to generalize across new data domains. Causality <ref type="bibr" target="#b11">[12]</ref>, with its focus on causal relationships rather than merely statistical dependencies, thereby can naturally discern true causal mechanisms and eliminate spurious correlations. This motivation has led to a growing enthusiasm for integrating causality into GML frameworks.</p><p>In this paper, we systematically review recent advancements in causality-involved GML generalization, covering their objectives, technologies, and effectiveness from different angles. More specifically, we first introduce the principal concepts and intuition that employ causality to enhance the generalization of graph models. We then categorize existing methods into distinct branches, detailing their techniques and exploring their interconnections. Additionally, we broaden our discussion to include the application of causality in other critical aspects of trustworthy graph machine learning. Finally, we highlight unresolved issues, articulate ongoing challenges, and outline prospective directions for future research in this rapidly evolving field.</p><p>Differences from existing surveys. There have been a couple of surveys in related areas, including graph OOD generalization <ref type="bibr" target="#b6">[7]</ref>, causality-inspired graph neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>, causality learning in graphs <ref type="bibr" target="#b8">[9]</ref>, causal machine learning <ref type="bibr" target="#b9">[10]</ref>, and trustworthy GML <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. The surveys closest to us are <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, but our work distinguishes itself from existing ones in the following aspects: (1) Main Focus. Our survey mainly centers on OOD generalization within GML from a causal perspective, a topic that other surveys have not primarily addressed. (2) Organization. We have structured the review of existing works differently, offering a unique layout that enhances understanding and integration of the mentioned materials. (3) Timeline. We provide coverage of the most recent advancements and discussions in this field. To be best of our knowledge, there has not been a comprehensive survey focusing on this unique topic.</p><p>The general organization of this survey is presented below:</p><p>‚Ä¢ Overview and preliminaries. In Section 2, we introduce the background knowledge and give an overview of this area. First, we will present the key concepts in causality learning, the common scenarios and tasks of GML, and the generalization issues. We also introduce the motivations and challenges of incorporating causality into graph machine learning.</p><p>‚Ä¢ Method review. In Section 3, we categorize existing approaches of causality-involved GML generalization into different groups and introduce their techniques and connections. We will start with the basic intuition of these studies, and then review the related frameworks and technologies. These studies mainly cover invariant learning, causal model-based methods, and stable learning.</p><p>‚Ä¢ Connection to other trustworthy domains. In Section 4, we extend our discussion to the use of causality in other related areas of trustworthy GML, such as explanation, fairness, and robustness. We explore the intrinsic connections between these domains to offer readers a more comprehensive and expandable sight for the whole picture of the related research areas.</p><p>‚Ä¢ Future work. In Section 5, we summarize current efforts in causality-involved GML generalization. Furthermore, we look forward and outline promising future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causality and Causal Inference</head><p>In this section, we provide important background knowledge on causal inference. We start with the key tasks and concepts in causal inference, introducing the notations, definitions, and frameworks. Generally, causal inference aims to investigate the causality between different data variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Structural causal model (SCM)</head><p>). Structural causal model <ref type="bibr" target="#b11">[12]</ref> is a widely adopted framework to model causal relationships. An SCM can be defined with a triplet of sets (U, V, F), here, U is a set of exogenous variables, V is a set of endogenous variables, and</p><formula xml:id="formula_0">F = {f 1 (‚Ä¢), f 2 (‚Ä¢), ..., f |V| (‚Ä¢)</formula><p>} is a set of functions (known as structural equations) that describe the causal relationships between variables. For each V ‚àà V, there is a structural equation</p><formula xml:id="formula_1">V = f V (pa V , U V ), where pa V ‚äÜ V \ V , U V ‚äÜ U are variables that directly cause V .</formula><p>Each SCM is associated with a causal graph, usually, it is a directed acyclic graph (DAG) with variables represented as nodes and causal relationships represented as directed edges. The conditional independence relationships between variables are straightforwardly reflected by the causal graph. Different from dependencies, causal relations only spread through directed paths whose edges are in the same direction. On causal graphs, there are three types of basic junctions: chain (e.g., X ‚Üí Z ‚Üí Y , here X has causal effects on Y through a mediator Z), fork (e.g., X ‚Üê Z ‚Üí Y , here Z serves as a confounder which brings non-causal dependency between X and Y ), and collider (e.g., X ‚Üí Z ‚Üê Y , here Z is a collider bringing non-causal dependency between X and Y when conditioning on Z). One of the key challenges in causal inference is to identify the causal relationships or effects out of all the statistical dependencies. The gold standard approach of causal inference -randomized controlled trials (RCTs) are often infeasible or unethical to practice in the real world. Here, we introduce a couple of other commonly used approaches and their related concepts. A foundational concept in SCM is the do-operator do(‚Ä¢), which stands for an intervention. Based on this, we have the following definition of causal effect in an average case: Definition 2 (Average treatment effect). The average causal effect of a certain treatment (a.k.a. cause) T (for simplicity, we usually assume it is a binary variable) on an outcome Y can be formalized as follows:</p><formula xml:id="formula_2">AT E = E[Y |do(T = 1)] -E[Y |do(T = 0)]</formula><p>For a pair of treatment T and outcome Y , there often exist backdoor paths that possibly bring non-causal dependencies for them. A backdoor path neither is a directed path nor contains any collider. Typical ways for unbiased causal effect estimation include backdoor adjustment, frontdoor adjustment, and instrumental variable (IV)-based approaches.</p><p>Definition 3 (Backdoor adjustment). Under certain basic causal assumptions<ref type="foot" target="#foot_0">foot_0</ref> , for a pair of treatment T and outcome Y , if there are variables Z that (1) block all the backdoor paths between T and Y , (2) do not contain any descendants of T , we have</p><formula xml:id="formula_3">P (Y |do(t)) = Z P (Z)P (Y |t, Z)</formula><p>In certain cases (e.g., when hidden confounders exist), it is difficult to find an observed variable set Z for backdoor adjustment. Alternatively, we can search for other variable sets which either meet the frontdoor criterion, or serve as instrumental variables. For variables satisfying frontdoor criterion, we can use frontdoor adjustment <ref type="bibr" target="#b11">[12]</ref> to estimate causal effect. Similarly, for instrumental variables (IVs), causal effects can be determined using appropriate IV-related methodologies <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causality and Graph Machine Learning</head><p>A graph can be denoted by G = (X, A), where X ‚àà R n√ód denotes the node features and A ‚àà {0, 1} n√ón is an adjacency matrix representing the graph edges. Here, n is the number of nodes, and d is the feature dimension. GML involves a variety of tasks related to graphs with the prediction target Y in different granularities, such as local-level prediction tasks like node classification and link prediction, as well as graph-level tasks like graph classification. In GML, models are specifically designed to handle the unique challenges posed by graph data, e.g., capturing the dependencies and relationships between nodes. Prominent models in this domain include many branches extending traditional neural network architectures to operate on graph structures, such as the representative graph convolutional network (GCN) <ref type="bibr" target="#b45">[46]</ref>, graph attention network (GAT) <ref type="bibr" target="#b46">[47]</ref>, graph Causality-involved GML OOD generalization Invariant learning (Section 3.1) Node-level invariance EERM <ref type="bibr" target="#b17">[18]</ref>, INL <ref type="bibr" target="#b18">[19]</ref>, FLOOD <ref type="bibr" target="#b19">[20]</ref>, SILD <ref type="bibr" target="#b20">[21]</ref>, DIDA <ref type="bibr" target="#b21">[22]</ref> Graph-level invariance GIL <ref type="bibr" target="#b22">[23]</ref>, DIR <ref type="bibr" target="#b23">[24]</ref>, GSAT <ref type="bibr" target="#b24">[25]</ref>, RGCL <ref type="bibr" target="#b25">[26]</ref>, CIGA <ref type="bibr" target="#b26">[27]</ref>, GALA <ref type="bibr" target="#b27">[28]</ref>, DisC <ref type="bibr" target="#b28">[29]</ref>, IGM <ref type="bibr" target="#b29">[30]</ref>, LECI <ref type="bibr" target="#b30">[31]</ref>, MoleOOD <ref type="bibr" target="#b31">[32]</ref>, iMoLD <ref type="bibr" target="#b32">[33]</ref> Causal modeling (Section 3.2)</p><p>Backdoor adjustment CAL <ref type="bibr" target="#b33">[34]</ref>, CAL+ <ref type="bibr" target="#b34">[35]</ref>, CaNet <ref type="bibr" target="#b35">[36]</ref> Frontdoor adjustment DSE <ref type="bibr" target="#b36">[37]</ref> Instrumental variable RCGRL <ref type="bibr" target="#b37">[38]</ref> Graph models inspired causal models E-invariant GR <ref type="bibr" target="#b38">[39]</ref>, gMPNN <ref type="bibr" target="#b39">[40]</ref> Counterfactual reasoning CFLP <ref type="bibr" target="#b40">[41]</ref> Stable learning (Section 3.3) OOD-GNN <ref type="bibr" target="#b41">[42]</ref>, StableGNN <ref type="bibr" target="#b42">[43]</ref>, DGNN <ref type="bibr" target="#b43">[44]</ref>, L2R-GNN <ref type="bibr" target="#b44">[45]</ref> Figure <ref type="figure">1</ref>: The representative methods for causality-involved GML OOD generalization.</p><p>variational autoencoder <ref type="bibr" target="#b47">[48]</ref>, and graph transformer <ref type="bibr" target="#b48">[49]</ref>. These models have achieved remarkable performance in graph-related tasks, but there are still concerns regarding their trustworthiness in different aspects. One of the key concerns is the generalization ability of these GML methods, as many of them easily fall short under distribution shift (i.e., P train (G, Y ) Ã∏ = P test (G, Y )) in OOD data. Noticeably, such distribution shift may come from both node features and the complicated graph structure.</p><p>In recent years, many researchers have realized the necessity of incorporating causality into machine learning to enhance model trustworthiness. As aforementioned, causality distinguishes itself from other generalization methods by its inherent ability to uncover causal relationships within data, which is particularly valuable in GML. However, implementing causality in GML presents multifaceted challenges. Firstly, causality within graphs is intrinsic; generalizing on graphs is inherently complex and often involves obscure causal relationships across domains. Secondly, causal assumptions that hold in traditional settings may not directly apply to graphs, complicating the adaptation of standard causal methods to this context. Addressing these challenges, there has been a growing interest in exploring the interconnection between causality and GML, yielding insightful contributions and promising prospects for the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodologies</head><p>In this section, we will introduce the principles and methodologies of leveraging causality for GML generalization. We categorize the mainstream of works into invariant learning, causal modeling, and stable learning. Fig. <ref type="figure">1</ref> shows an overview of the categorization for these methods. A more detailed comparison of these methods is in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Invariant Learning</head><p>Invariant learning targets on capturing the relations that are invariant across different domains. It is motivated by the fact that spurious correlations often vary under distribution shifts. The general principle of invariant learning is improving generalization by extracting the invariant factors to make predictions, while the spurious correlations are filtered out. , prediction task, whether environment labels are known, whether the method assumes a single training environment, whether the method is supported by an SCM, whether there is a theoretical guarantee for the method, and application scenario.</p><p>e. Although invariant learning is not explicitly driven by causal inference, much literature has discussed their intrinsic connections <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. In general, in a causal view, the direct causes for the label Y should have invariant relationships in different domains. The idea of invariant learning is first proposed for tabular data, including representative methods such as invariant risk minimization (IRM) <ref type="bibr" target="#b10">[11]</ref> and EIIL <ref type="bibr" target="#b51">[52]</ref>. However, on graphs, directly applying these methods often results in unsatisfying performance. Invariant learning on graphs often extracts a subgraph as a rationale that generalizes across domains. Accordingly, many graph-specific invariant learning methods have been proposed in recent years, and take one of the mainstreams in OOD generalization for GML. Node-level invariance learning. Explore-to-Extrapolate Risk Minimization (EERM) <ref type="bibr" target="#b17">[18]</ref> uses multiple adversarial context generators to simulate (virtual) environments even under a single (real) environment, and a GNN model is trained by minimizing the mean and variance of risks from these simulated environments. Different from the single environment setting in EERM, Li et al. <ref type="bibr" target="#b18">[19]</ref> argue that nodes are often from multiple latent environments in the real world, and propose an approach INL that can infer node environments with a contrastive modularity-based graph clustering method and learn invariant node representations. A recent unique work Flexible invariant Learning framework for Out-Of-Distribution generalization on graphs (FLOOD) <ref type="bibr" target="#b19">[20]</ref> combines invariant learning and bootstrapped learning. It first constructs multiple training environments based on data augmentation, then adopts a bootstrapped learning module. In this way, their encoder is more flexible than traditional invariant encoders as it can be refined on the test set for better generalization.</p><p>Graph-level invariance learning. Graph Invariant Learning (GIL) <ref type="bibr" target="#b22">[23]</ref> is a GNN-based model that identifies the invariant subgraph for graph classification tasks. It is the first work of invariant graph representation learning under mixed latent environments without the supervision of environment labels. Discovering Invariant Rationales (DIR) <ref type="bibr" target="#b23">[24]</ref> is an algorithm that infers invariant causal parts by conducting causal interventions, but it needs a complicated iterative process to break and assemble subgraphs during training. Another more straightforward method Graph Stochastic Attention (GSAT) <ref type="bibr" target="#b24">[25]</ref> is based on the information bottleneck principle. It learns invariant subgraphs by learning stochasticity-reduced attention. Li et al. <ref type="bibr" target="#b25">[26]</ref> propose Rationale-aware Graph Contrastive Learning (RGCL), which combines invariant rationale discovery with graph contrastive learning to improve generalization and interoperability. CIGA <ref type="bibr" target="#b26">[27]</ref>, with a supported causal graph shown in Fig. <ref type="figure">2(e)</ref>, proposes an information-theoretic objective to extract invariant subgraphs with a theoretical guarantee to handle distribution shift under different SCMs. In a follow-up work, Chen et al. <ref type="bibr" target="#b27">[28]</ref> analyze the failure cases of existing methods such as CIGA, and introduce minimal assumptions for feasible invariant graph learning. They propose a framework GALA with provable invariant subgraph identifiability for OOD generalization. DisC <ref type="bibr" target="#b28">[29]</ref>, inspired by a causal graph shown in Fig. <ref type="figure">2(d)</ref>, disentangles the given graph into a causal substructure and a bias substructure. The disentanglement is conducted with a parameterized edge mask generator. Then two GNNs are trained to encode the causal and bias substructures respectively into their representations trained with causal/bias-aware loss functions. To further decorrelate causal and bias variables, DisC also generates unbiased counterfactual training samples. For all of the invariant learning biased methods, a common conclusion is that the effectiveness of invariant learning is greatly dependent on the variety of environments. Realizing this, a co-mixup strategy IGM <ref type="bibr" target="#b29">[30]</ref> is proposed, which jointly adopts environment mixup and invariant mixup to generate sufficiently diverse environments. Many existing graph OOD generalization methods either do not assume the existence of environment labels, or do not fully exploit them. Differently, LECI <ref type="bibr" target="#b30">[31]</ref> is proposed to utilize pre-collected environment information for graph-specific OOD generalization. It discovers causal invariant subgraphs by leveraging the two causal independence properties regarding label and environment: E ‚ä• ‚ä• G C and Y ‚ä• ‚ä• G S and designs an adversarial learning strategy to jointly optimize these two causal independence properties. Here, E is the environment, G C and G S are the causal subgraph and spurious subgraph, respectively. Domain-specific invariant learning. SILD <ref type="bibr" target="#b20">[21]</ref> is the first work to study distribution shifts on dynamic graphs in the spectral domain, it captures invariant and variant spectral patterns with disentangled spectrum masks for both node classification and link prediction tasks. Dynamic graph Attention network (DIDA) <ref type="bibr" target="#b21">[22]</ref> handles complex spatio-temporal distribution shifts in dynamic graphs by using a spatio-temporal attention network to identify variant and invariant spatio-temporal patterns. In molecular representation learning (MRL),  <ref type="bibr" target="#b33">[34]</ref>, CAL+ <ref type="bibr" target="#b34">[35]</ref>, CaNet <ref type="bibr" target="#b35">[36]</ref>, DSE <ref type="bibr" target="#b36">[37]</ref>, DisC <ref type="bibr" target="#b28">[29]</ref>, CIGA <ref type="bibr" target="#b26">[27]</ref>, E-invariant GR <ref type="bibr" target="#b38">[39]</ref>, and gMPNN <ref type="bibr" target="#b39">[40]</ref>. G, C, S, Y , Z, and E denote graphs, causal variables, spurious variables, prediction labels, graph representations, and environments, respectively. G C and G S in (e) represent the causal subgraph and spurious subgraph; while G S and G * S in (c) represent an explanatory subgraph and a surrogate subgraph, respectively. For those causal graphs with nodes in different colors, observed/unobserved variables are in grey/white. Dashed lines show unknown correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùëÜ</head><p>MoleOOD <ref type="bibr" target="#b31">[32]</ref> is the first work that formulates the OOD problem in MRL that leverages the invariance principle. It includes an environment inference model and guides their encoder in learning environment-invariant molecular substructures. In a similar context, invariant Molecular representation in Latent Discrete space (iMoLD) <ref type="bibr" target="#b32">[33]</ref> separates the molecular graph representation into invariant and spurious parts with a GNN scorer, and uses a task-agnostic self-supervised learning objective to further improve invariance identification. This approach does not require environment inference in other works like MoleOOD and GIL, and thus avoids the additional assumptions and knowledge on environment distrbutions required in environment inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Modeling</head><p>Inspired by studies exploring causal relationships within graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, a notable line of research has emerged that explicitly constructs SCMs on graphs to enhance GML OOD generalization. This research typically relies on predefined assumptions about the underlying SCM, which are illustrated through clear causal graphs that illustrate the causal relationships among the graph, labels, causal features, and spurious features. An overview of these commonly used causal graphs is presented in Fig. <ref type="figure">2</ref>. These methods often employ traditional causal inference principles to reduce spurious correlations and improve generalization. Based on the key techniques, we categorize these methods into several branches: backdoor adjustment, frontdoor adjustment, instrumental variable-based methods, and those related specifically to graph modeling.</p><p>Backdoor adjustment. Causal Attention Learning (CAL) <ref type="bibr" target="#b33">[34]</ref> is based on the causal assumption that there exist shortcut features that serve as confounders between the causal features and graph prediction. Under this assumption, CAL employs attention modules to estimate the causal features and shortcut features of the input graph, and conduct backdoor adjustment to mitigate the spurious correlations led by the backdoor path <ref type="figure">2(a)</ref>. A follow-up work CAL+ <ref type="bibr" target="#b34">[35]</ref> generally inherits the idea of CAL, but it further enhances the method with a memory bank to improve the diversity of shortcut feature samplings and a prototype module to enhance the consistency of intra-class causal features. For node-level distribution shift, CaNet <ref type="bibr" target="#b35">[36]</ref> uses a straightforward causal graph shown in Fig. <ref type="figure">2</ref>(b), relying on backdoor adjustment and variational inference. It counteracts the confounding bias by collaboratively training an environment estimator and a GNN predictor.</p><formula xml:id="formula_4">C ‚Üê G ‚Üí S ‚Üí Z ‚Üí Y , shown in Fig.</formula><p>Frontdoor adjustment and instrumental variable. Wu et al. <ref type="bibr" target="#b36">[37]</ref> also attribute the OOD distribution shift to the confounder effect. As shown in Fig. <ref type="figure">2(c</ref>), they propose a method Deconfounded Subgraph Evaluation (DSE) which introduces a surrogate G * S between the explanatory subgraph G S and model prediction Y to mitigate confounding bias via frontdoor adjustment. The surrogates are generated by a generative model based on a conditional variational graph auto-encoder. Another approach RCGRL <ref type="bibr" target="#b37">[38]</ref> eliminates confounding bias by generating instrumental variables (IV) under unconditional moment restrictions. On graphs, the conditions of instrumental variables are often hard to satisfy, instead, RCGRL proposes an active IV generation approach that transfers the conditional moment restrictions into unconditional ones with theoretical support.</p><p>Graph models inspired causal models. E-invariant GR <ref type="bibr" target="#b38">[39]</ref> constructs a fine-granularity causal graph (shown in Fig. <ref type="figure">2(f)</ref>) including graphon, label, training/text environment, node attributes and edges of training/test graph, and number of nodes. With this causal graph, it aims to learn environment-invariant graph representations that are generalizable to shifts in graph size and node attributes. The invariant representations are learned based on the stability of subgraph densities in graphon random graph models <ref type="bibr" target="#b52">[53]</ref>. This unique work incorporates Stochastic Block Models (SBMs) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> and graphon random graph models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b55">56]</ref> inside its causal model. A study in a similar setting <ref type="bibr" target="#b39">[40]</ref> extends to link prediction problem by proposing a new family of structural pairwise embeddings gMPNN, with its causal graph shown in Fig. <ref type="figure">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(g).</head><p>Counterfactual reasoning. In the context of causal inference, the term "counterfactual" stands for a hypothetical scenario that deviates from actual events. For instance, one might ask, "Would the label have changed if a specific subgraph had been altered?". In this line of research, CFLP <ref type="bibr" target="#b40">[41]</ref> employs counterfactual reasoning for OOD link prediction. It focuses on the causal relationships between the graph structure and the existence of links, i.e., "would the link still exist if the graph structure became different?". This method is achieved by training GNN-based link predictors to predict both actual (factual) links and counterfactual links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stable Learning</head><p>Stable learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b56">57]</ref>, with its primary goal of learning a model that can perform uniformly well in any environment, originates from the sampling reweighting or covariate balancing strategies in causal effect estimation <ref type="bibr" target="#b57">[58]</ref>. More specifically, many traditional causal inference methods estimate causal effects by using sampling reweighting to assign sample weights that balance the distribution of covariates across different treatment groups. In stable learning, each input feature is treated as a treatment, while other variables are considered covariates. It focuses on learning weights to decorelate the features, thereby balancing the covariate distribution with respect to each feature treated as a treatment. In this way, the correlation between each feature (treatment) and the prediction label (outcome) results from a direct causal effect. Then a predictive model based on correlation can achieve better generalization across varied data environments.</p><p>In this area of studies, OOD-GNN <ref type="bibr" target="#b41">[42]</ref> adopts a nonlinear graph representation decorrelation method that leverages random Fourier features. It reduces the statistical dependencies between relevant and irrelevant graph representations by iteratively optimizing the sample weights and the graph encoder. This approach promotes model generalization against multiple shift types, including graph size, node attribute, and graph structure. Different from OOD-GNN which learns a single embedding for each graph, StableGNN <ref type="bibr" target="#b42">[43]</ref> extracts high-level representations for subgraphs with graph pooling layers from the input graph. Based on these highlevel representations, StableGNN designs a causal variable distinguishing regularizer to de-bias the training distribution through sample weight learning. Similarly, Debiased GNN (DGNN) <ref type="bibr" target="#b43">[44]</ref> devises a differentiated decorrelation regularizer for debiasing at the node level. Later on, researchers argue that methods like StableGNN and OOD-GNN may suffer from the overly aggressive objective that eliminates the dependencies between all the variables across the graph representations, leading to an excessively small sample size. With this motivation, they propose L2R-GNN <ref type="bibr" target="#b44">[45]</ref> which first clusters the variables in graph representations based on the correlation stability, and then only learns weights to eliminate correlations between variables across different clusters, instead of removing correlations between any pair of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Despite the categorizations outlined above, many methods demonstrate deep interconnections and underlying equivalencies. For example, several invariant learning techniques, such as CIGA <ref type="bibr" target="#b26">[27]</ref> and Disc <ref type="bibr" target="#b28">[29]</ref>, are explicitly underpinned by structural causal models, as shown in Fig. <ref type="figure">2</ref>, positioning them at the intersection of multiple categories. Additionally, some other techniques are also closely related to the approaches discussed, including disentangled representation learning <ref type="bibr" target="#b28">[29]</ref>, representation decorrelation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, causal interventions <ref type="bibr" target="#b35">[36]</ref>, counterfactual reasoning <ref type="bibr" target="#b40">[41]</ref>, and data augmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref>. These connections highlight the complex, often overlapping landscape of methodologies within this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Connection to Other Domains in Trustworthy Graph Machine Learning</head><p>Even though this survey mainly focuses on causality-based generalization for GML, it is worth mentioning that many principles and technologies in this area closely connect to other domains of trustworthy GML. Here, we give a conceptual overview of these intrinsic connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Explanation</head><p>Although explanation has many different definitions, in graphs, there are generally two main categories for GML explanation, including those that aim to identify the rationale that contributes most to prediction (factual explanation) with the question "what contributes most to the prediction?", and counterfactuals that can achieve a certain desired outcome (counterfactual explanation) with the question "what is the slightest change I can make on the input graph to achieve a desired prediction?".</p><p>Factual explanation. With its ultimate goal, factual explanation naturally relates to generalization w.r.t. the common focus on the rationale that causes the prediction target. This concept is intuitively equivalent to the invariant variables in invariant learning, or causal variables in causal models that are stable across environments. Therefore, many explanation methods also naturally have a dual objective in generalization, including invariant learning methods such as DIR <ref type="bibr" target="#b23">[24]</ref>, GSAT <ref type="bibr" target="#b24">[25]</ref>, GREA <ref type="bibr" target="#b58">[59]</ref>; causal modeling methods like CAL/CAL+ <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> and DSE <ref type="bibr" target="#b36">[37]</ref>, and Granger causality <ref type="bibr" target="#b59">[60]</ref> based methods such as GEM <ref type="bibr" target="#b60">[61]</ref> and CI-GNN <ref type="bibr" target="#b61">[62]</ref>.</p><p>Counterfactual explanation. Counterfactual explanation <ref type="bibr" target="#b62">[63]</ref> studies the problem of making perturbations on the input graph to change the model prediction. Even though the original concept of counterfactual explanation does not involve causal inference, recent studies <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref> have started to discuss the benefit of explicitly incorporating causality into counterfactual explanation. In this context, it is worth noting that making perturbations itself would result in certain outcomes, which relates to counterfactual reasoning in a causal context. Therefore, many recent works utilize causal methods for graph counterfactual explanation such as CLEAR <ref type="bibr" target="#b64">[65]</ref>, which involves a deep graph generative model to promote causality in counterfactual explanation. Another work CF 2 [66] combines both factual reasoning and counterfactual reasoning to obtain the necessary explanation for graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness</head><p>Adversarial robustness refers to the ability of a model to perform correctly and maintain its integrity under adversarial attacks. These attacks are typically slight perturbations on the original input data, also known as adversarial examples. These adversarial examples are usually imperceptible by human eyes but are crafted to mislead the model into making mistakes. Adversarial robustness and OOD generalization are interconnected areas as the adversarial samples can be considered from an adversarial distribution outside of the training data. Explorations in this area from a causal view are rare, but still offer valuable insights. For example, IDEA <ref type="bibr" target="#b66">[67]</ref> considers the causal features as the key to graph adversarial robustness due to their invariance across attacks. Based on this, it designs both node-based and structure-based invariance objectives to capture causally invariant node representations against adversarial attacks. Under linear causal assumptions, the defense strategy of IDEA has been proved to be causally invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fairness</head><p>Fairness in machine learning aims to eliminate the bias against any demographic groups or individuals. It is widely regarding certain sensitive features (e.g., age, gender). There have been lots of notions of fairness defined from different perspectives. In recent years, causality-based fairness has attracted more and more attention since it can track the root and path of bias in its generation process, providing explanatory and controllable approaches for mitigating potential discrimination. Counterfactual fairness <ref type="bibr" target="#b67">[68]</ref> is one of the most well-known notions in this line, which measures fairness by comparing the model prediction under one's original sensitive feature and a counterfactual case. An example question is: would a male applicant have the same chance to get a job offer if he had been a female?</p><p>A fair model must effectively handle various sensitive feature values, which are often regarded as different domains. In this sense, a fair model should also demonstrate strong generalization capabilities across these different domains. From a causal view, it requires identifying the causal path from sensitive features to other variables used in prediction, and eliminating (a part of) these paths to achieve fairness. This relates to the mitigation of environment effects on model prediction in generalization. On graphs, fairness is a more complicated task due to the bias (causally) propagated through graph links. Studies for counterfactual fairness, such as GEAR <ref type="bibr" target="#b64">[65]</ref> and RFCGNN+ <ref type="bibr" target="#b68">[69]</ref> incorporate causal reasoning in graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>In this paper, we have conducted a comprehensive review of causality-involved approaches for OOD generalization of graph machine learning. We begin by highlighting the motivations and challenges inherent to this area, providing a structured categorization of existing methodologies based on their technical approaches. Furthermore, we discuss the commonalities and differences of methods in this field, and also introduce the connections between them and related studies in other areas. Through this analysis, we have extracted valuable insights and established a robust foundation for ongoing exploration in related fields.</p><p>Looking ahead, there are several promising avenues for further studies:</p><p>‚Ä¢ Application in high-stakes domains for graph trustworthiness: Causal models in important domains such as science, finance, law, and health often incorporate domain knowledge and require a high degree of trustworthiness. These fields demand rigorous frameworks when modeling the causal relationships as well as the graph structure due to their reliance on precise and professional knowledge. Future research could focus on customizing domain-specific causality-enhanced graph models to maintain the trustworthiness and application of GML in high-stakes domains.</p><p>‚Ä¢ Uncertainty quantification of causality-involved GML: While existing GML methods have demonstrated impressive performance, it is crucial to include uncertainty quantification, especially under distribution shifts on graphs. This aspect might intersect with emerging research on conformal prediction in causal inference <ref type="bibr" target="#b69">[70]</ref> and graph learning <ref type="bibr" target="#b70">[71]</ref>. Addressing uncertainty quantification will enhance the reliability and applicability of GML in varying environments.</p><p>‚Ä¢ Graph-related AGI incorporating causal knowledge: Another exciting direction is the integration of causal knowledge into graph foundation models (GFMs). GFMs have the capability of addressing a broader range of tasks beyond traditional GML limits. While large models exhibit superior generalization, they still face challenges such as bias from training domains. Incorporating human-provided or data-driven causal insights into these foundation models has the potential to effectively improve their trustworthiness and mimic complex human reasoning processes, which represents a significant step toward achieving artificial general intelligence (AGI). But currently, this task is much more challenging than traditional causality-involved GML due to the large scale of GFMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 4 (</head><label>4</label><figDesc>Frontdoor criteron). A set of variables M satisfies the frontdoor criterion relative to T and Y if: (1) M completely mediates the effect of T on Y ; (2) There is no unblocked backdoor path from T to M ; (3) All backdoor paths from M to Y are blocked by T . Definition 5 (Instrumental variable (IV)). Given treatment T and outcome Y , a variable I can serve as an instrumental variable if it satisfies the following conditions: (1) (Relevance) I is relevant to the treatment T ; (2) (Exclusion restriction) The causal effect from I to Y is mediated by T ; (3) (Instrumental unconfoundedness) There is no unblocked backdoor path from I to Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The formal assumptions of invariant learning slightly vary in different works, but generally, there should exist invariant factors Œ¶(G) from input G, that P e (Y |Œ¶(G)) remains stable in different domain Comparison of existing causality-involved GML generalization methods in different aspects, including applicable shift type (including A=graph structure, X=node features, graph size, and node degree)</figDesc><table><row><cell>Method</cell><cell>Shift</cell><cell>Task</cell><cell cols="3">E known Single E SCM Theory</cell><cell>Application</cell></row><row><cell>EERM [18]</cell><cell>A, X</cell><cell>Node</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>These assumptions include the Modularity assumption and Positivity assumption.</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>The authors have no conflicts of interest to report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>


			<div type="funding">
<div><p><rs type="projectName">Graph No No No Yes General DIR [24] A, X Graph No No Yes Yes General GSAT [25] A, X Graph No No Yes Yes General RGCL [26] A, X Graph No No No No General CIGA [27] Size, A, X Graph No No Yes Yes General GALA [28] Size, A, X Graph No No Yes Yes General DisC [29] A, X Graph No No Yes No General IGM [30] Degree, size, A, X Graph No No No No General LECI [31] A, X Graph Yes No Yes Yes General SILD [21] (Dynamic) A, X Node/link No No No Yes Dynamic graph DIDA [22] (Dynamic) A, X Graph No No No No Spatio-temporal MoleOOD [32] Size, A Graph No No No Yes Molecular iMOLD [33] Size, A Graph No No No No Molecular CAL [34] A, X Graph No No Yes</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Yhu3HJm">
					<orgName type="project" subtype="full">Graph No No No Yes General DIR [24] A, X Graph No No Yes Yes General GSAT [25] A, X Graph No No Yes Yes General RGCL [26] A, X Graph No No No No General CIGA [27] Size, A, X Graph No No Yes Yes General GALA [28] Size, A, X Graph No No Yes Yes General DisC [29] A, X Graph No No Yes No General IGM [30] Degree, size, A, X Graph No No No No General LECI [31] A, X Graph Yes No Yes Yes General SILD [21] (Dynamic) A, X Node/link No No No Yes Dynamic graph DIDA [22] (Dynamic) A, X Graph No No No No Spatio-temporal MoleOOD [32] Size, A Graph No No No Yes Molecular iMOLD [33] Size, A Graph No No No No Molecular CAL [34] A, X Graph No No Yes</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Biographies</head><p>Jing Ma is a Timothy E. and Allison L. Schroeder Assistant Professor in the Department of Computer &amp; Data Sciences at Case Western Reserve University (CWRU). Before that, she obtained her Ph.D. in the Department of Computer Science at University of Virginia (UVA) in 2023. She obtained her master's degree and bachelor's degree at Shanghai Jiao Tong University (SJTU). She is broadly interested in machine learning and data mining. Her current research mainly focuses on trustworthy AI (generalization, explanation, fairness, robustness, etc.), causal machine learning, graph mining, AI for social good, and recently large language model. She has won KDD'22 Best Paper Award, and has been selected for the AAAI'24 New Faculty Highlights program.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A review on graph neural network methods in financial applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15367</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Molecular generative graph neural networks for drug discovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bongini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Addressing crime situation forecasting task with temporal graph convolutional neural network approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 12th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="474" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assessing the causal impact of covid-19 related policies on outbreak dynamics: A case study in the us</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mietchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2678" to="2686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.12477</idno>
		<title level="m">Survey on trustworthy graph neural networks: From a causal perspective</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning causality with graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Magazine</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Causal machine learning: A survey and open problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15475</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring causal learning through graph neural networks: an in-depth review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Job</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14994</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A survey of trustworthy graph learning: Reliability, explainability, and privacy protection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10014</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5782" to="5799" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairness in graph mining: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">602</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instrumental variable methods for causal inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baiocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2297" to="2340" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invariant node representation learning under distribution shifts with multiple latent environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flood: A flexible invariant learning framework for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1548" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectral invariant learning for dynamic graphs under distribution shifts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks under spatiotemporal distribution shift</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6074" to="6089" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12872</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable graph learning via stochastic attention mechanism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">543</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Let invariant rationale discovery inspire graph contrastive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22" to="131" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Does invariant graph learning via environment augmentation learn invariance?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24" to="934" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph invariant learning with subgraph co-mixup for out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8562" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint learning of label and environment causal independence for graph out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">978</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning invariant molecular representation in latent discrete space</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhancing out-of-distribution generalization on graphs via causal attention learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph out-of-distribution generalization via causal intervention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="850" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deconfounding to explanation evaluation in graph neural networks</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08802</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust causal graph representation learning against confounding effects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7624" to="7632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ood link prediction generalization capabilities of message-passing gnns in larger test graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="20" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from counterfactual links for link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="7328" to="7340" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing graph neural networks on out-of-distribution graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Debiased graph neural networks with agnostic label selection bias</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4411" to="4422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to reweight for graph neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.12475</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A unified causal view of domain invariant representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07922</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lov√°sz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the statistics of vision: the julesz conjecture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="138" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="100" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stable learning establishes some common ground between causal inference and machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="115" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Covariate balancing propensity score</title>
		<author>
			<persName><forename type="first">K</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="243" to="263" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph rationalization with environment-based augmentations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1069" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ci-gnn: A granger causality-inspired graph neural network for interpretable brain network-based psychiatric diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page">106147</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box: Automated decisions and the gdpr</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harv. JL &amp; Tech</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Preserving causal constraints in counterfactual explanations for machine learning classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03277</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Clear: Generative counterfactual explanations on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">907</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM web conference 2022</title>
		<meeting>the ACM web conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1018" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Idea: Invariant defense for graph adversarial robustness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="page">121171</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Toward fair graph neural networks via real counterfactual samples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Conformal inference of counterfactuals and individual treatment effects</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Cand√®s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="911" to="938" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Conformal prediction sets for graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Zargarbashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">318</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
