<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DisCover: Disentangled Music Representation Learning for Cover Song Identification</title>
				<funder ref="#_TH9GHph #_6wZTptw">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_k9XUfGC">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-07-19">19 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Xun</surname></persName>
							<email>jhxun@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
							<email>sy_zhang@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanting</forename><surname>Yang</surname></persName>
							<email>yantingyang@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
							<email>jiemingzhu@ieee.org</email>
						</author>
						<author>
							<persName><forename type="first">Liqun</forename><surname>Deng</surname></persName>
							<email>dengliqun.deng@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhaozhou@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
							<email>dongzhenhua@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Ruiqi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><forename type="middle">2023</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab Shenzhen</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab Shenzhen</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab Shenzhen</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DisCover: Disentangled Music Representation Learning for Cover Song Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-19">19 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3539618.3591664</idno>
					<idno type="arXiv">arXiv:2307.09775v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Information retrieval</term>
					<term>â€¢ Computing methodologies â†’ Artificial intelligence Cover Song Identification</term>
					<term>Disentanglement Representation</term>
					<term>Music Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the field of music information retrieval (MIR), cover song identification (CSI) is a challenging task that aims to identify cover versions of a query song from a massive collection. Existing works still suffer from high intra-song variances and inter-song correlations, due to the entangled nature of version-specific and version-invariant factors in their modeling. In this work, we set the goal of disentangling version-specific and version-invariant factors, which could make it easier for the model to learn invariant music representations for unseen query songs. We analyze the CSI task in a disentanglement view with the causal graph technique, and identify the intra-version and inter-version effects biasing the invariant learning. To block these effects, we propose the disentangled music representation learning framework (DisCover) for CSI. DisCover consists of two critical components: (1) Knowledge-guided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM), which block intra-version and inter-version biased effects, respectively. KDM minimizes the mutual information between the learned representations and version-variant factors that are identified with prior domain knowledge. GADM identifies version-variant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, online digital music platforms, such as Spotify and Apple Musiccontain a massive number of music tracks for consumption, intensifying the need of music retrieval techniques for discovering related songs. One of the key techniques for music discovery is cover song identification (CSI), which aims to retrieve the cover versions from a music collection given a query song. Specially, a cover version/song is an alternative interpretation of the original version with different musical facets (e.g. timbre, key, tempo, or structure).</p><p>In real-world scenarios, the music collection can be massive and rapidly updated, potentially amplifying the intra-song variances (c.f. Figure <ref type="figure" target="#fig_0">1</ref>) and inter-song correlations. These characteristics drive the CSI problem hard to handle due to the ubiquitous spurious correlations among songs of different collections. Intuitively, CSI requires a fine-grained analysis of music facets and semantics such that the intra-song correlations and inter-song differences can be adequately distinguished. Recently, with the development of artificial intelligence in other domains <ref type="bibr">[1, 29, 50-53, 59, 60, 65]</ref>, deep learning based CSI models have presented superior performance compared with traditional sequence matching methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. Most of them <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref> treat CSI as a classification task and utilize CNN-based architecture for music content understanding. Furthermore, some state-of-the-art works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> explore metric learning techniques to narrow the gap between different cover versions of the same song and simultaneously expand the distance among the different version groups. Despite the significant advances made by these methods, we argue that song-specific and song-sharing musical factors are highly entangled in their modeling, thus being inadequate to distinguish unseen cover versions and songs. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, the testing cover version (b) of the query song (a) shows significant differences in pitch/F0 (the orange curve in the spectrogram), timbre, or rhythm. If the model is unable to disentangle these factors and identifies them as version-variant, it might fail to generalize on this testing version and identify it as negative ones. On the other hand, if the model fails to disentangle and recognize version-invariant factors, it might falsely correlate some other songs with the given query based on the high similarity of version-specific factors. To bridge the gap, we set the goal of explicitly disentangling version-variant and version-invariant factors and thus learning invariant musical representations for unseen cover song identification.</p><p>To better understand the underlying mechanism of the disentanglement in CSI task, we resort the causal graph technique <ref type="bibr" target="#b33">[34]</ref> for illustration (c.f. Figure <ref type="figure" target="#fig_1">2</ref>). The nodes denote cause or effect factors. An edge ğ´ â†’ ğµ means the ğ´ can directly affect ğµ.</p><p>â€¢ Firstly, we illustrate the causal graph from the model's perspective in Figure <ref type="figure" target="#fig_1">2</ref>(a): ğ‘ ğ‘– denotes the set of factors that are specific to Technically, we propose a Disentangled music representation learning framework for Cover song identification, denoted as Dis-Cover, which encapsulates two key components: (1) Knowledgeguided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM) for blocking biased effects ğ‘ ğ‘– â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– and ğ‘ ğ‘— â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– , respectively. KDM employs off-the-shelf music feature extractors as the domain knowledge for disentanglement, and minimizes the mutual information (MI) between the learned representations and version-variant factors. GADM identifies version-variant factors by simulating the representation transitions between intra-song versions and adopting gradient-based adaptive masking. Since the discrete-valued mask might distort the continuity of representations in the hypersphere, it would be less effective to use MI to measure the effect ğ‘ ğ‘— â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– . Instead, GADM incorporates an adversarial distillation sub-module for distribution-based effect blocking.</p><p>The main highlights of this work are summarized as follows:</p><p>â€¢ We analyze the cover song identification problem in a disentanglement view with causal graph, a powerful tool but is seldom used in the community. We identify the bad impact of version-variant factors with two effect paths that needed to be blocked. â€¢ We propose the DisCover framework that disentangles versionvariant factors among intra-song versions and blocks two biased effect paths via knowledge-guided MI minimization and gradientbased adversarial distillation. â€¢ We conduct in-depth experimental analyses along on both quantitative and qualitative results, which have demonstrated the effectiveness and necessity of disentanglement for CSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Cover Song identification</head><p>With the increasing amount of music data on the Internet, cover song identification (CSI) has long been a popular task in the music information retrieval community. CSI aims to retrieve the cover versions of a given song in a dataset, which can also be seen as measuring the similarity between music signals without meta-information (e.g., title, author, genre). Specifically, meta-information might ease the problem but also introduce spurious correlations that many different songs have quite similar or even the same short title. Moreover, users humming the query songs might not necessarily know/provide the meta-information. Overall, CSI as a challenging task has long attracted lots of researchers due to its potential applications in music representation learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref>, retrieval <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b60">61]</ref> and recommendation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>. However, those cover songs may differ from the original song in key transposition, speed change, and structural variations, which challenges identifying the cover song. To solve these problems, <ref type="bibr" target="#b44">[45]</ref> developed music sequences alignment algorithms for version identification by measuring the similarity between time series, and <ref type="bibr" target="#b12">[13]</ref> generated fixed-length vectors for cover song identification. In addition, deep learning approaches are introduced to CSI. For instance, CNNs are utilized to measure the similarity matrix <ref type="bibr" target="#b1">[2]</ref> or learn features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>. On this basis, TPPNet <ref type="bibr" target="#b61">[62]</ref> uses a temporal pyramid pool to extract information at a different scale. CQTNet <ref type="bibr" target="#b62">[63]</ref> proposes a special CNN architecture to extract musical representations and train the network through classification strategies. Although these methods have made significant progress, they ignore the entanglement of cover song representations and may incorrectly correlate some other songs with a given query. Thus, we propose a framework that disentangles version-variant factors among intra-song versions. </p><formula xml:id="formula_0">Z 1 X 1 X 2 Z Z 2 Y 1 Y 2 â—Š â—Š Cover Version 1 Cover Version 2 (a) Cutoff Z i â†’ X i Z 1 X 1 X 2 Z Z 2 Y 1 Y 2 â—Š â—Š Cover Version 1 Cover Version 2 (b) Cutoff Z i â†’ X j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disentangled Representation Learning</head><p>Disentangled representation learning (DRL) focuses on encoding data points into separate independent embedding subspaces, where different subspaces represent different data attributes. To prevent information leakage from each other, the correlation between the two embedding parts is still required to be reduced. Some correlationreducing methods mainly focus on Mutual Information (MI) minimization, where MI is a fundamental measure of the dependence between two random variables. To accurately estimate MI upper bound, CLUB <ref type="bibr" target="#b4">[5]</ref> bridges mutual information estimation with contrastive learning. This method has gained a lot of attention and applications in scenarios such as domain adaption, style transfer, and causal inference. For instance, IDE-VC <ref type="bibr" target="#b63">[64]</ref> and VQMIVC <ref type="bibr" target="#b48">[49]</ref> achieves proper disentanglement of speech representations. MIM-DRCFR <ref type="bibr" target="#b3">[4]</ref> learns disentangled representations for counterfactual regression. In addition, as analyzed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b65">66]</ref>, the gradients of the final predicted score convey the task-discriminative information, which correctly identifies the task-relevant features. For instance, Grad-CAM <ref type="bibr" target="#b41">[42]</ref> visualizes the importance of each class by leveraging the gradient information. On the basis of this, ToAlign <ref type="bibr" target="#b53">[54]</ref> decomposes a source feature into a task-relevant one and a taskirrelevant one for performing the classification-oriented alignment. RSC <ref type="bibr" target="#b23">[24]</ref> discards the task-relevant representations associated with the higher gradients. DropClass <ref type="bibr" target="#b7">[8]</ref> uses gradient information to extract class-specific information from the entangled feature map. However, most of these works learn to disentangle representations from a single perspective. This paper blocks two biased effect paths via knowledge-guided MI minimization and gradient-based adversarial distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Music Representation Learning</head><p>An effective musical representation is essential for learning different music-related tasks, such as music classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, cover song identification <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>, music generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref>. Most of them rely on large amounts of labeled datasets to learn music representations. As the labeled datasets on which supervised learning methods require extensive manual labeling, it is often costly and time-consuming, leading to limitations in the performance of supervised learning methods. For this reason, some audio researchers have adopted a self-supervised learning approach to learning musical representations <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b66">67]</ref>. For example, MusicBERT <ref type="bibr" target="#b66">[67]</ref> models music self-representation with a multi-task learning framework. PEMR <ref type="bibr" target="#b56">[57]</ref> proposes a positive-negative frame mask for music representation with contrastive learning. Many approaches to music representation learning focus on key pieces of music, while CSI focuses more on the whole song. Prior Knowledge Selection. There are usually multiple variations of musical facets for the cover version, such as timbre, key, tempo, timing, or structure <ref type="bibr" target="#b42">[43]</ref>. Hence it meets a problem of how to select the appropriate musical facets as expert knowledge. Inspired by the common practice in the disentanglement-based voice conversion <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> and singing voice synthesis <ref type="bibr" target="#b5">[6]</ref> and other speech-related tasks <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, we consider that fundamental frequency (F0) and timbre are relatively more sensitive to cover versions among different facets since they often change when different artists perform the same piece of song/music. Therefore, we select the F0 and timbre as representatives of the prior knowledge in our work. Specifically, F0 is the musical pitch, representing the high or low notes in the song/music. Timbre describes the vocal characteristics of the artist or instrument, which strongly influences how song/music is heard by trained as well as untrained ears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Overview</head><p>To block the intra-version and inter-version biased effects for learning version-invariant representations, we propose DisCover, as shown in Figure <ref type="figure" target="#fig_4">4</ref>. DisCover consists of two modules: (1) Knowledgeguided Disentanglement Module (KDM), which mitigates the negative effect from cover information and extracting the commonness for the versions (green area in the upper left of Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>(2) Gradient-based Adversarial Disentanglement Module (GADM), which identifies the differences between versions and alleviates the negative transfer (blue area in the lower right of Figure <ref type="figure" target="#fig_4">4</ref>). The two modules are jointly trained in a parallel manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge-guided Disentanglement</head><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>(a), the Knowledge-guided Disentanglement module (KDM) aims to block the bias between intra-song versions (cutoff ğ‘ ğ‘– â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– ), which attempts to make the model more focused on the version-invariant factors ğ‘ and learn invariant representations for different cover versions. Considering that the model is hard to identify the version-specific factors entangled in the representation, as mentioned in Sec 3.1, we introduce the prior knowledge (e.g. F0 and timbre) to serve as the teacher that provides version-variant factors ğ‘ ğ‘– . In contrast to the goal of knowledge transfer, the model aims to minimize the correlation between the Here, we denote ğ’™ âˆˆ R ğ‘‘ğ‘–ğ‘š as the learned representations and ğ’› âˆˆ {ğ’, ğ’• } as the knowledge bank of version-variant factors, where ğ’ âˆˆ R ğ‘‘ğ‘–ğ‘š represents the fundamental frequency (F0) features, ğ’• âˆˆ R ğ‘‘ğ‘–ğ‘š represents the timbre representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">factors-invariant Representation Modeling.</head><p>To minimize the correlation between the learned representations ğ‘¥ and the versionvariant factors ğ‘§, we introduce mutual information (MI) to serves as the measurement, which is defined as the Kullback-Leibler (KL) divergence between their joint and marginal distributions as:</p><formula xml:id="formula_1">ğ¼ (ğ’™; ğ’›) = E ğ‘ (ğ’™,ğ’› ) [log ğ‘ (ğ’›|ğ’™) ğ‘ (ğ’™) ]<label>(1)</label></formula><p>Since the conditional distribution ğ‘ (ğ’›|ğ’™) is intractable, we adopt vCLUB <ref type="bibr" target="#b4">[5]</ref> to approximate the upper bound of MI as:</p><formula xml:id="formula_2">ğ¼ (ğ‘¥, ğ‘§) = E ğ‘ (ğ‘¥,ğ‘§ ) [log ğ‘ ğœƒ ğ‘¥,ğ‘§ (ğ‘§|ğ‘¥)] -E ğ‘ (ğ‘¥ ) E ğ‘ (ğ‘§ ) [log ğ‘ ğœƒ ğ‘¥,ğ‘§ (ğ‘§|ğ‘¥)]<label>(2)</label></formula><p>where ğ‘ ğœƒ ğ‘¥,ğ‘§ (â€¢) represents the variational estimation network between ğ’™ and ğ’›. Therefore the unbiased estimation for vCLUB between learned representation and version-variant factors can be reformulated as:</p><formula xml:id="formula_3">L ğ¼ (ğ’™;ğ’) = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [log(ğ‘ ğœƒ ğ’™,ğ’ (ğ’ ğ‘– |ğ’™ ğ‘– )) - 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 log(ğ‘ ğœƒ ğ’™,ğ’ (ğ’ ğ‘— |ğ’™ ğ‘– ))]<label>(3)</label></formula><formula xml:id="formula_4">L ğ¼ (ğ’™;ğ’• ) = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [log(ğ‘ ğœƒ ğ’™,ğ’• (ğ’• ğ‘– |ğ’™ ğ‘– )) - 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘—=1 log(ğ‘ ğœƒ ğ’™,ğ’• (ğ’• ğ‘— |ğ’™ ğ‘– ))]<label>(4)</label></formula><p>where ğ‘ represents the batch size. By minimizing the Eq. ( <ref type="formula" target="#formula_3">3</ref>) and (4), we can decrease the correlation between learned representation and version-variant factors and the total MI loss is:</p><formula xml:id="formula_5">L ğ‘€ğ¼ = L ğ¼ (ğ’™;ğ’) + L ğ¼ (ğ’™;ğ’• )<label>(5)</label></formula><p>To obtain the reliable upper bound approximation, a robust variational estimator ğ‘ ğœƒ ğ‘¥,ğ‘§ (â€¢) is required. We train the variational estimator by minimizing the log-likelihood:</p><formula xml:id="formula_6">L ğ‘ ğœƒğ’™,ğ’› = - 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [log(ğ‘ ğœƒ ğ’™,ğ’› (ğ’™ |ğ’›))], ğ’› âˆˆ {ğ’, ğ’• }<label>(6)</label></formula><p>3.3.2 knowledge tradeoff. However, we argue that vCLUB might be at risk of posterior collapse <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> due to the KL-Vanishinig. For example, if the weights of the variational estimator become randomized due to undesirable training, the introduction of prior knowledge would be meaningless. Therefore, knowledge tradeoff is the self-supervised way to relieve the posterior collapse and ensure training stability for variational estimator. Furthermore, considering knowledge extractors' ability, little beneficial versioninvariant information might still remain in the ğ’›. To address these concerns, we provide two alternatively simple methods. Firstly, we can fuse task-oriented representation ğ’† as:</p><formula xml:id="formula_7">ğ‘ = ğœ (ğ‘”(ğ’†, ğ‘(ğ’›))),<label>(7)</label></formula><formula xml:id="formula_8">ğ’† * = ğ‘ * ğ’† + (1 -ğ‘) * ğ‘(ğ’›)<label>(8)</label></formula><p>where ğœ (â€¢) denotes the sigmoid function, ğ‘”(â€¢) is the linear transformation, ğ‘(â€¢) is the shared MLP in the variational estimator, and ğ‘ âˆˆ R serves as the tradeoff between ğ’† and ğ‘(ğ’›). Secondly, we can use clustering models (e.g. k-means) to annotate the pseudo labels for ğ’› to supervise the variational estimator with classification task:</p><formula xml:id="formula_9">L ğ‘§ ğ‘ğ‘™ğ‘  = - ğ‘ âˆ‘ï¸ ğ‘–=1 ğ‘¦ ğ‘§ ğ‘– ğ‘™ğ‘œğ‘”( Å·ğ‘§ ğ‘– ) + (1 -ğ‘¦ ğ‘§ ğ‘– )ğ‘™ğ‘œğ‘”(1 -Å·ğ‘§ ğ‘– )<label>(9)</label></formula><p>where ğ‘¦ ğ‘§ ğ‘– is the pseudo label for ğ‘§ ğ‘– , and Å·ğ‘§ ğ‘– is the output of the knowledge classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gradient-based Adversarial Disentanglement</head><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>(b), the Gradient-based Adversarial Disentanglement module (GADM) aims to block the bias between inter-song versions (cutoff ğ‘ ğ‘— â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– ), which attempts to bridge the intra-group gap and avoid biased representation learning. As analyzed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b65">66]</ref>, the gradients of the predictive score contain the discriminative information for the downstream tasks. Analogously, the gradients of the transition cost between two versions might convey important information for version-variant factors. For this purpose, we randomly construct the positive query-target pairs with different versions and obtain the corresponding representation pairs (ğ’™, ğ’™ + ) with the same backbone model. GADM has three main steps: identification, decomposition, and alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Identification.</head><p>The main idea of identification is to recognize the version-variant factors that are entangled in the elements of learned representations. Since the backbone encoder maps the samples into the hyperspace, positive representation pairs ğ‘¥ and ğ‘¥ + can be regarded as two points in the same high dimensional space.</p><p>In the ideal case, different versions of the same song should have similar representations. In other words, these points should cluster together in the hyperspace. However, the distance between two points would be enlarged due to the disruption of version-variant factors that are highly entangled in the representations. Therefore, we treat the distance between query-target pair ğ‘¥ and ğ‘¥ + as the transition cost caused by entangled version-variant factors. Here, we can use metric function (e.g. Euclidean, Manhattan, or Cosine) to serve as the transitions cost C ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  âˆˆ R + between the representations of intra-song versions ğ‘¥ and ğ‘¥ + as:</p><formula xml:id="formula_10">C ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  = â„(ğ’™, ğ’™ + )<label>(10)</label></formula><p>where â„(â€¢, â€¢) denotes the metric function. Motivated by the GradCAMlike methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b65">66]</ref>, which utilize the saliency-based class information from the gradient perspective. We can obtain versionvariant information by calculating the gradients of the transition cost C ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  w.r.t. the representation ğ’™ as:</p><formula xml:id="formula_11">ğ‘” ğ’™ = ğœ• C ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  ğœ• ğ’™<label>(11)</label></formula><p>where ğ‘” ğ’™ âˆˆ R ğ‘‘ğ‘–ğ‘š denotes the gradient vector of ğ’™. Since the partial derivative operation for query ğ’™ utilizes the information from target ğ’™ + , gradient vector ğ‘” ğ’™ probably conveys the element-wise importance information of representation ğ’™ for measuring the difference to its target ğ’™ + . Specifically, as shown in the bottom right corner of Figure <ref type="figure" target="#fig_4">4</ref>, each element ğ‘” ğ’™ (ğ‘–) in ğ‘” ğ’™ represents the fusion result between query element ğ’™ (ğ‘–) and whole target representation ğ’™ + . The process allows element ğ‘” ğ’™ (ğ‘–) to automatically search for the elements of the query representation ğ’™ that are relevant to the transition cost. That's why ğ‘” ğ’™ can identify the version-variant factors hiding in the ğ’™ + . Furthermore, the value of ğ‘” ğ’™ (ğ‘–) represents the sensitivity to the changes of transition cost C ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  , where the element with the higher value is more relevant to the version-variant factors based on the nature of gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Decomposition. After identifying the version-variant factors, we attempt to decompose the version-invariant representation</head><p>x from ğ’™. Inspired by ToAlign <ref type="bibr" target="#b53">[54]</ref>, which decomposes a source feature into a task-relevant/irrelevant one with a gradient-based attention weight vector. We further exploit the numeric order in ğ‘” ğ’™ to ensure that the element with the higher gradient has the lower attention weight. Specifically, given the gradient vector ğ‘” ğ’™ , we will construct the corresponding mask vector and decompose it as:</p><formula xml:id="formula_12">ğ’ ğ’™ (ğ‘–) = ï£± ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£³ 1 - exp (ğ‘” ğ‘¥ (ğ‘–)) ğ‘˜ âˆˆ {ğ‘˜ |ğ‘” ğ‘¥ (ğ‘˜ ) â‰¥ğ‘ ğ‘ } exp (ğ‘” ğ‘¥ (ğ‘˜)) , ğ‘– ğ‘“ ğ‘” ğ‘¥ (ğ‘–) â‰¥ ğ‘ ğ‘ 1, ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’<label>(12)</label></formula><p>x = ğ’ ğ’™ âŠ™ ğ’™ (13) where ğ‘š ğ‘¥ (ğ‘–) denotes ğ‘–-th element in the mask, ğ‘ ğ‘ denotes the ğ‘-th largest percentile in ğ‘” ğ’™ , and âŠ™ denotes the hadamard product. Moreover, in view of the self-challenging method <ref type="bibr" target="#b23">[24]</ref>, the decomposition process adaptively re-weights ğ’™ based on the knowledge from ğ‘” ğ’™ and forces the backbone to lower the attention on version-specific elements, so as to obtain the version-invariant representation x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Alignment.</head><p>To alleviate the negative transfer, we adopt the adversarial distillation sub-module to align entangled representation ğ’™ to the disentangled one x. In the beginning, ğ’™ and x belong to different hyperspheres, where the ğ’™ is considered as the negative source and the x is the positive target. We use them to train the discriminator ğ· to distinguish which hypersphere the representation belongs to, with the classification loss L ğ· 1 . Meanwhile, the backbone encoder is trained to fool the discriminator to learn the version-invariant representation by minimizing task-oriented loss while maximizing L ğ· 2 :</p><formula xml:id="formula_13">L ğ· 1 = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [log ğ· ( xğ‘– ) + log (1 -ğ· (ğ‘¥ ğ‘– )]<label>(14)</label></formula><formula xml:id="formula_14">L ğ· 2 = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 [log (1 -ğ· (ğ‘¥ ğ‘– )]<label>(15)</label></formula><p>Furthermore, considering the symmetry of the query-target pair, we can similarly obtain the version-invariant target representation x+ âˆˆ R ğ‘‘ğ‘–ğ‘š . To ensure the semantic consistency between query and target, it is better to minimize transition cost as:</p><formula xml:id="formula_15">L ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  = â„( x, x+ )<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>Given the output of the task-oriented classifier Å· x , we treat CSI as the classification task, where the task-oriented learning objective can be formulated as follows:</p><formula xml:id="formula_16">L ğ‘¡ğ‘ğ‘ ğ‘˜ = - ğ‘ âˆ‘ï¸ ğ‘–=1 ğ‘¦ xğ‘™ğ‘œğ‘”( Å· x ) + (1 -ğ‘¦ x )ğ‘™ğ‘œğ‘”(1 -Å· x )<label>(17)</label></formula><p>where ğ‘¦ x is the groundtruth label. To be clear, the overall optimization objective of our proposed DisCover is summarized as follows:</p><formula xml:id="formula_17">L 1 = L ğ‘¡ğ‘ğ‘ ğ‘˜ + L ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘  + ğœ† 1 L ğ‘€ğ¼ + L ğ‘§ ğ‘ğ‘™ğ‘  -L ğ· 2 (<label>18</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">L 2 = L ğ· 1 + ğœ† 2 L ğ‘ ğœƒğ’™,ğ’›<label>(19)</label></formula><p>where L 1 and L 2 are optimized alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We analyze the DisCover framework and demonstrate its effectiveness by answering the following research questions:</p><p>â€¢ RQ1: How does DisCover perform compared with existing bestperforming cover song identification methods in different scenarios (e.g., unseen songs/versions) ? â€¢ RQ2: Do knowledge-guided disentanglement and gradient-based disentanglement all contribute to the effectiveness over various base models in a model-agnostic manner? â€¢ RQ3: How does different architecture and hyper-parameter settings will affect the performance of DisCover? â€¢ RQ4: Does DisCover disentangle the version-variant factors? , we employ three widely used metrics for evaluation, i.e., MAP (mean average precision), P@10 (precision at 10), and MR1 (mean rank of the first correctly identified cover).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison Baselines.</head><p>â€¢ 2DFM <ref type="bibr" target="#b12">[13]</ref>: 2DFM transforms a beat-synchronous chroma matrix with a 2D Fourier transformer and poses the search for cover songs as estimating the Euclidean distance. â€¢ ki-CNN <ref type="bibr" target="#b55">[56]</ref>: ki-CNN uses a key-invariant convolutional neural network robust against key transposition for classification. â€¢ TPPNet <ref type="bibr" target="#b61">[62]</ref>: TPPNet combines CNN architecture with temporal pyramid pooling to extract information on different scales and transform songs with different lengths into fixed-dimensional representations.</p><p>â€¢ CQTNet <ref type="bibr" target="#b62">[63]</ref>: CQTNet uses carefully designed kernels and dilated convolutions to extend the receptive field, which can improve the model's representation learning capacity. â€¢ PICKiNet <ref type="bibr" target="#b32">[33]</ref>: PICKiNet devises pitch class blocks to obtain the key-invariant musical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We train models on the SHS100K and Karaoke30K and report the evaluation metrics on them with different scenarios. Covers80 is used to evaluate the models trained on SHS100K since their languages are the same. We use parselmouth <ref type="foot" target="#foot_4">6</ref> and resemblyzer <ref type="foot" target="#foot_5">7</ref> to extract F0 and timbre respectively. In KDM, we apply Eq. ( <ref type="formula" target="#formula_8">8</ref>) to F0 feature and Eq. ( <ref type="formula" target="#formula_9">9</ref>) to timbre representation, where the number of the clusters for generating pseudo label ğ‘ = 100. Following the default MI-related setting in <ref type="bibr" target="#b48">[49]</ref>, we set hyper-parameters ğœ† 1 = 0.05, ğœ† 2 = 1. In GADM, we select Euclidean distance as the metric function, and the mask ratio is set to 1. Following the setting of <ref type="bibr" target="#b61">[62]</ref>, we also apply a multi-length training strategy. Adam <ref type="bibr" target="#b25">[26]</ref> is used as the optimizer for backbone, discriminator, and variational estimator. The training batch size ğ‘ is 32, initial learning rate is 4e-4, weight decay is 1e-5. Notably, Lay-erNorm is applied in DisCover to obtain normalized representation to ensure numerical stability in similarity-based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Results (RQ1)</head><p>We instantiate the proposed DisCover framework on three bestperforming CSI methods, i.e., TPPNet, CQTNet,and PICKiNet, and obtain TPPNet-Dis, CQTNet-Dis and PICKiNet-Dis. â€¢ Overall, the results across multiple evaluation metrics consistently indicate that TPPNet-Dis, CQTNet-Dis, and PICKiNet-Dis achieve better results than their base models among different datasets and scenarios. Especially, CQTNet-Dis and PICKiNet-Dis show comparable performance and outperform other bestperforming methods. We attribute the improvements to the fact that baselines succeed in learning the version-invariant representations by disentangling version-specific musical factors.</p><p>â€¢ DisCover can boost the performance of models in different scenarios, especially in scenario #1, where all test songs are unseen. It suggests that the version-variant factors have been highly disentangled. In addition, in Karaoke30k where the cover versions of a particular song are fewer, DisCover could still significantly improve the baselines. These results demonstrate the practical merits of DisCover, i.e., identifying version-variant factors with limited number of annotated versions. Note that in real-world scenarios, less popular songs constitute the majority of the music collections and have fewer cover versions. In summary, these results demonstrate the strengths of DisCover in generalization and few-shot learning, which is critical for industrial scenarios where music collections could be rapidly updated and too massive to sample the full cover versions for training. â€¢ Surprisingly, MR1 scores in scenario #1 are mostly worse than those in scenario #2 for all models, especially in SHS100K. These results might suggest that entangled training leads to spurious correlations among songs, including those testing songs seen during training. We also observe that on the SHS100K dataset, the proposed method could not beat some baselines w.r.   â€¢ Removing either KDM or GADM leads to performance degradation, while removing both modules (i.e., the base model) leads to the worst performance. These results demonstrate the effectiveness of the proposed two modules as well as the benefits of disentanglement for CSI. We attribute this superiority to the fact that the models would absorb less spurious correlations among songs and versions by learning version-invariant representations and blocking intra/inter-version biased effects. â€¢ Removing GADM leads to more performance drops than removing KDM, which indicates that introduced prior knowledge only contains the part of the version-variant factors. Therefore it is necessary to identify the remained factors that hide in the representation. These results again verify the effectiveness of the end-to-end disentanglement module GADM.  between two versions of a song. Therefore a reliable metric function is vital for identifying the version-variant factors between different versions. In this experiment, we select three commonly used distances (e.g. Euclidean, Manhattan, and Cosine) to serve as the transition cost. Surprisingly, as shown in Table <ref type="table" target="#tab_8">7</ref>, the Euclidean distance, which is less explored in the CSI literature, shows a clear advantage over other widely used metric functions. This is an interesting finding that might be potentially inspirational. We plan to further uncover the underlying mechanisms in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis (RQ4)</head><p>The above analysis quantitatively shows the effectiveness of disentanglement in cover song identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we first analyze the cover song identification problem in a disentanglement view with causal graph. We identify the bad impact of version-variant factors with two effect paths that need to be blocked. Then, we propose the disentangled music representation learning framework DisCover to block these effects. DisCover consists of two modules: (1) Knowledge-guided Disentanglement  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the data distribution in CSI task with the raw waveform (top) and CQT spectrogram (bottom). (a) and (b) are two different cover version for the same song "Don't Let It Bring You Down" in Covers80 dataset.</figDesc><graphic coords="2,176.62,185.37,106.54,106.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal graph of CSI from different perspectives. the ğ‘–-th cover version and are mostly version-variant. ğ‘‹ ğ‘– denotes the learned musical representation of the ğ‘–-th cover version. ğ‘ denotes the set of version-invariant factors. ğ‘Œ ğ‘– denotes the retrieval results (e.g. a candidate playlist) given the learned representation ğ‘‹ ğ‘– . Intuitively, during the co-training of various cover versions where different factors are highly entangled, ğ‘ 1 could have a direct effect on ğ‘‹ 1 and also ğ‘‹ 2 , which is the musical representation of the second cover version. Therefore, ğ‘Œ 1 and ğ‘Œ 2 will be indirectly affected through causal path ğ‘ 1 â†’ ğ‘‹ 1 â†’ ğ‘Œ 1 and ğ‘ 1 â†’ ğ‘‹ 2 â†’ ğ‘Œ 2 respectively, which will lead to spurious correlations and mismatching during unseen cover song identification. â€¢ Secondly, as illustrated in Figure 2(b), we further consider the causal graph from searcher's perspective. It is a relatively ideal causal graph that Xğ‘– is only affected by ğ‘ . In other words, version information has no effect on the learned music representation, such that intra-song versions can be adequately distinguished from the others. In this work, we aim to develop a disentanglement framework that could realize the transition of models' underlying behavior from Figure 2(a) to Figure 2(b) for debiased and effective cover song identification. We identify two critical challenges in achieving disentanglement in CSI: (1) Mitigating the negative effect from cover information and extracting the commonness for the versions (cutoff ğ‘ ğ‘– â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– ), which aims to make the model more focused on the version-invariant factors ğ‘ and learn invariant representations for different cover versions. (2) Identifying the differences between versions and alleviating the negative transfer (cutoff ğ‘ ğ‘– â†’ ğ‘‹ ğ‘— â†’ ğ‘Œ ğ‘— ), which attempts to bridge the intra-group gap and avoid biased representation learning. It is non-trivial to block paths ğ‘ ğ‘– â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– and ğ‘ ğ‘— â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– due to the implicit nature of version-specific factors ğ‘ ğ‘– , ğ‘ ğ‘— and the effects in deep neural networks. In this regard, we introduce a disentanglement module for identifying versionspecific factors, followed by an effect-blocking module for learning invariant representations. As for the path ğ‘ ğ‘– â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– , disentangling ğ‘ ğ‘– is challenging without supervision signals since different factors (e.g. F0 and timbre) in raw music are highly entangled. In this regard, we introduce prior domain knowledge as guidance for disentanglement. As for the path ğ‘ ğ‘— â†’ ğ‘‹ ğ‘– â†’ ğ‘Œ ğ‘– , the challenge lies in how to identify the factors ğ‘ ğ‘— in the ğ‘—-th sample that could affect the representation learning of ğ‘‹ ğ‘– . Intuitively, we regard the modified factors during the transition from ğ‘‹ ğ‘— to ğ‘‹ ğ‘– as version-variant factors that are critical to ğ‘‹ ğ‘– in the ğ‘—-th sample. Technically, we propose a Disentangled music representation learning framework for Cover song identification, denoted as Dis-Cover, which encapsulates two key components: (1) Knowledgeguided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM) for blocking biased</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interventions on causal graph of DisCover from the perspective of modelling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Following the common practice in modern cover song identification task<ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b61">62]</ref>, we formulate cover song identification as an information retrieval problem and specifically focus on music representation learning. We use ğ‘ to denote one query song and S = {ğ‘  ğ‘– } ğ‘–=1,...,| S | to denote the song collections on an online music platform. Given the query ğ‘, cover song identification aims to retrieve the most similar candidates C = {ğ‘ ğ‘– } ğ‘–=1,...,ğ‘˜ from the song collections S in a top-k manner. A deep learning-based CSI model ğ‘“ (â€¢) encodes the ğ‘ and ğ‘  ğ‘– into the fixed dimension representation ğ’’ and ğ’” ğ‘– separately. Then we use cosine distance to calculate the similarity for all the pairs ğ‘ƒ = {(ğ’’, ğ’” ğ‘– )} ğ‘–=1,...,|ğ‘† | . During testing and serving, top-k candidates C will be ranked by the similarity and displayed on the music platform in a position consistent with the rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematic illustration of DisCover framework. KDM minimizes the MI between the learned representations and version-variant factors that are identified with prior domain knowledge. GADM identifies and decomposes versionvariant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation for effect blocking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>To evaluate whether the model can learn the version-invariant and unbiased representations via disentangled learning, we visualize the t-SNE transformed embeddings. We adopt CQTNet, TPPNet and PICKiNet as baseline and equip them with DisCover framework and plot the twenty randomly sampled songs and each song has three versions with the representations encoded by the corresponding model. As shown in Figure 5, we can observe that: â€¢ Overall, different versions of a song exhibit more noticeable clusters with the help of DisCover. The base model is more likely to falsely correlate songs based on the similarity of version-variant factors. For example, the versions of song #12 in Figure 5(a) are closer to the other songs, which suggests that CQTNet fail to learn the discriminative representation for them. However, in Figure 5(b), different versions of song #12 are more compact, which demonstrates the capability of disentanglement. â€¢ Moreover, equipped with DisCover, all of CQTNet, TPPNet and PICKiNet show better performance in learning more discriminative representations compared to the baselines, which further reveals the model-agnostic capability of DisCover. â€¢ Furthermore, although the training samples for each song are limited (2 to 3 cover versions for a song), DisCover can still learn the discriminative representations for unseen songs. These results again verify the strengths of DisCover in generalization and few-shot learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Case study with t-SNE transformed embeddings derived from different baselines with our DisCover framework, where colored nodes represent the different songs. module, it mitigates the negative effect of cover information and extracts the commonness for the versions, which makes the model more focused on the version-invariant factors and learning invariant representations for different cover versions. (2) Gradient-based Adversarial Disentanglement module, it identifies the differences between versions and alleviates the negative transfer, which bridges the intra-group gap and avoids biased representation learning. Extensive comparisons with best-performing methods and in-depth analysis demonstrate the effectiveness of DisCover and the necessity of disentanglement for CSI.</figDesc><graphic coords="9,325.49,333.09,108.10,108.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statics We conduct experiments on two open source datasets commonly used in cover song identification and one self-collected real-world dataset. Statistics of these datasets are shown inTable1. â€¢ Second Hand Songs 100K (SHS100K): We downloaded raw audios through youtube-dl 2 using the URLs provided on GitHub 3 . It has 10000 songs with 104641 recordings. Notablely, there are 25% of test songs seen during model training in the setting of [62]. To further explore the generalization performance, we also construct another scenario setting where all test songs are unseen during training. For both scenarios, the ratio among the training set, validation set, and testing set is 8:1:1. â€¢ Covers80 4 : It has 80 songs with 160 recordings, where each song has 2 cover versions. Due to the small amount of data, it is commonly used only for evaluating models. â€¢ Karaoke30K: A real-world Chinese karaoke dataset collected by ourselves. It has 11500 songs with 31629 recordings, where each song has 1 to 3 cover versions. Following the SHS100K, we also construct the two scenarios with the same setting. 4.1.2 Evaluation Metrics. Following the evaluation protocol of the Mirex Audio Cover Song Identification Contest 5</figDesc><table><row><cell>Dataset</cell><cell cols="4">Songs Recordings Avg. versions Language</cell></row><row><cell>SHS100K</cell><cell>10000</cell><cell>104641</cell><cell>10.5</cell><cell>English</cell></row><row><cell cols="2">Karaoke30K 11500</cell><cell>31629</cell><cell>2.8</cell><cell>Chinese</cell></row><row><cell>Covers80</cell><cell>80</cell><cell>160</cell><cell>2.0</cell><cell>English</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Improvement over the best-performing baselines across different scenarios.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SHS100K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Covers80</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Scenario 1 :</cell><cell></cell><cell></cell><cell>Scenario 2 :</cell><cell></cell><cell></cell><cell>Scenario 1 :</cell><cell></cell><cell></cell><cell>Scenario 2 :</cell><cell></cell></row><row><cell></cell><cell cols="12">MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“</cell></row><row><cell>2DFM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.104</cell><cell>0.113</cell><cell>415</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.381</cell><cell>0.053</cell><cell>33.60</cell></row><row><cell>Ki-CNN</cell><cell>0.176</cell><cell>0.224</cell><cell cols="2">105.79 0.215</cell><cell>0.183</cell><cell>147.3</cell><cell>0.485</cell><cell>0.069</cell><cell>16.18</cell><cell>0.509</cell><cell>0.071</cell><cell>15.45</cell></row><row><cell>TPPNet</cell><cell>0.419</cell><cell>0.455</cell><cell>45.85</cell><cell>0.471</cell><cell>0.338</cell><cell>74.38</cell><cell>0.757</cell><cell>0.084</cell><cell>5.81</cell><cell>0.786</cell><cell>0.087</cell><cell>8.39</cell></row><row><cell>CQTNet</cell><cell>0.571</cell><cell>0.573</cell><cell>31.69</cell><cell>0.624</cell><cell>0.340</cell><cell>61.31</cell><cell>0.805</cell><cell>0.087</cell><cell>6.58</cell><cell>0.846</cell><cell>0.089</cell><cell>5.13</cell></row><row><cell>PICKiNet</cell><cell>0.617</cell><cell>0.602</cell><cell>38.66</cell><cell>0.626</cell><cell>0.408</cell><cell>84.12</cell><cell>0.818</cell><cell>0.085</cell><cell>7.11</cell><cell>0.858</cell><cell>0.091</cell><cell>4.27</cell></row><row><cell>TPPNet-Dis</cell><cell>0.565</cell><cell>0.567</cell><cell>41.60</cell><cell>0.561</cell><cell>0.384</cell><cell>74.26</cell><cell>0.814</cell><cell>0.091</cell><cell>7.81</cell><cell>0.849</cell><cell>0.091</cell><cell>4.74</cell></row><row><cell>CQTNet-Dis</cell><cell>0.658</cell><cell>0.627</cell><cell>37.98</cell><cell>0.640</cell><cell>0.417</cell><cell>76.41</cell><cell>0.856</cell><cell>0.091</cell><cell>5.11</cell><cell>0.912</cell><cell>0.095</cell><cell>2.43</cell></row><row><cell cols="2">PICKiNet-Dis 0.657</cell><cell>0.627</cell><cell>46.80</cell><cell>0.653</cell><cell>0.421</cell><cell>72.30</cell><cell>0.830</cell><cell>0.087</cell><cell>5.88</cell><cell>0.882</cell><cell>0.093</cell><cell>3.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparing different methods on Karaoke30K with different scenarios.</figDesc><table><row><cell>Model</cell><cell>Scenario 1 :</cell><cell>Scenario 2 :</cell><cell></cell></row><row><cell></cell><cell cols="3">MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“</cell></row><row><cell>Ki-CNN</cell><cell cols="3">0.483 0.119 52.53 0.524 0.116 52.01</cell></row><row><cell>TPPNet</cell><cell cols="3">0.760 0.165 17.65 0.777 0.154 13.86</cell></row><row><cell>CQTNet</cell><cell>0.863 0.182</cell><cell cols="2">7.84 0.831 0.161 11.93</cell></row><row><cell>PICKiNet</cell><cell>0.944 0.194</cell><cell>4.41 0.959 0.178</cell><cell>4.12</cell></row><row><cell>TPPNet-Dis</cell><cell>0.935 0.192</cell><cell>5.23 0.957 0.177</cell><cell>3.24</cell></row><row><cell cols="2">CQTNet-Dis 0.976 0.198</cell><cell cols="2">2.66 0.983 0.180 3.20</cell></row><row><cell cols="3">PICKiNet-Dis 0.974 0.198 2.61 0.973 0.179</cell><cell>3.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>and 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Dis to obtain ablation architectures, i.e., w/o. KDM, and w/o. GADM, respectively to show the model-agnostic capability of these two modules. The results are shown in Table4. We can observe that:</figDesc><table><row><cell>t. MR1.</cell></row><row><cell>SHS100K are known to have unusual audio manifestations in</cell></row><row><cell>recordings and vocal concert songs (with strong background</cell></row><row><cell>noises e.g. claps, shouts, or whistles), where MR1 scores are</cell></row><row><cell>sensitive to these noises and exhibit high variances. On the</cell></row><row><cell>Karaoke30K dataset where the manifestations in recordings are</cell></row><row><cell>closer to real-world search scenarios, we observe consistent per-</cell></row><row><cell>formance improvement brought by DisCover across all metrics.</cell></row><row><cell>4.3 Model Analysis (RQ2, RQ3)</cell></row><row><cell>4.3.1 Analysis of key building modules. knowledge-guided disen-</cell></row><row><cell>tanglement and gradient-based adversarial disentanglement are</cell></row><row><cell>two key components of DisCover framework. We conduct the abla-</cell></row><row><cell>tion study on them to reveal the efficacy of the architectures and</cell></row><row><cell>the benefits of disentangling version-variant factors. Specifically,</cell></row><row><cell>we selectively discard the KDM and GADM from CQTNet-Dis and</cell></row><row><cell>TPPNet-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies by selectively discarding the knowledge-guided disentanglement module (w/o. KDM) and gradient-based adversarial disentanglement module (w/o. GADM). We study both TPPNet-Dis and CQTNet-Dis on different datasets to reveal the model-agnostic capability of the proposed modules.</figDesc><table><row><cell>Scenario 1 :</cell><cell>SHS100K</cell><cell>Covers80</cell><cell>Karaoke30K</cell></row><row><cell cols="4">Model MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“</cell></row><row><cell cols="4">TPPNet-Dis 0.565 0.567 41.60 0.814 0.091 7.81 0.935 0.192 5.23</cell></row><row><cell cols="4">w/o. KDM 0.542 0.551 39.98 0.805 0.085 7.06 0.921 0.190 3.79</cell></row><row><cell cols="4">w/o. GADM 0.497 0.522 48.55 0.790 0.087 8.34 0.845 0.180 8.52</cell></row><row><cell>TPPNet</cell><cell cols="3">0.419 0.455 45.85 0.757 0.084 5.81 0.760 0.165 17.65</cell></row><row><cell cols="4">CQTNet-Dis 0.658 0.627 37.98 0.856 0.091 5.11 0.976 0.198 2.66</cell></row><row><cell cols="4">w/o. KDM 0.649 0.622 32.34 0.843 0.093 4.45 0.961 0.196 3.73</cell></row><row><cell cols="4">w/o. GADM 0.619 0.607 36.66 0.833 0.088 7.17 0.887 0.186 7.88</cell></row><row><cell>CQTNet</cell><cell cols="3">0.571 0.573 31.69 0.805 0.087 6.58 0.863 0.182 7.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Study of different prior knowledge. The disentanglement of both F0 and timbre can be beneficial.</figDesc><table><row><cell>Scenario 1:</cell><cell></cell><cell>SHS100K</cell><cell></cell><cell></cell><cell>Covers80</cell><cell></cell></row><row><cell>Factors</cell><cell cols="6">MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“</cell></row><row><cell>TPPNet</cell><cell>0.420</cell><cell>0.454</cell><cell>44.33</cell><cell>0.757</cell><cell>0.084</cell><cell>5.81</cell></row><row><cell>F0</cell><cell>0.457</cell><cell>0.485</cell><cell>48.20</cell><cell>0.764</cell><cell>0.086</cell><cell>7.54</cell></row><row><cell>w/. tradeoff</cell><cell>0.463</cell><cell>0.490</cell><cell>43.65</cell><cell>0.778</cell><cell>0.086</cell><cell>8.31</cell></row><row><cell>Timbre</cell><cell>0.443</cell><cell>0.475</cell><cell>55.75</cell><cell>0.772</cell><cell>0.086</cell><cell>8.38</cell></row><row><cell>w/. tradeoff</cell><cell>0.466</cell><cell>0.495</cell><cell>52.69</cell><cell>0.784</cell><cell>0.088</cell><cell>7.14</cell></row><row><cell cols="2">Timbre &amp; F0 0.469</cell><cell>0.496</cell><cell>43.96</cell><cell>0.782</cell><cell>0.086</cell><cell>8.73</cell></row><row><cell>w/. tradeoff</cell><cell>0.497</cell><cell>0.522</cell><cell>48.55</cell><cell>0.790</cell><cell>0.087</cell><cell>8.34</cell></row><row><cell>CQTNet</cell><cell>0.569</cell><cell>0.572</cell><cell>31.90</cell><cell>0.805</cell><cell>0.087</cell><cell>6.58</cell></row><row><cell>F0</cell><cell>0.585</cell><cell>0.580</cell><cell>34.52</cell><cell>0.814</cell><cell>0.093</cell><cell>4.00</cell></row><row><cell>w/. tradeoff</cell><cell>0.603</cell><cell>0.597</cell><cell>34.46</cell><cell>0.821</cell><cell>0.091</cell><cell>3.97</cell></row><row><cell>Timbre</cell><cell>0.586</cell><cell>0.585</cell><cell>38.17</cell><cell>0.816</cell><cell>0.093</cell><cell>4.59</cell></row><row><cell>w/. tradeoff</cell><cell>0.606</cell><cell>0.599</cell><cell>37.17</cell><cell>0.829</cell><cell>0.094</cell><cell>4.53</cell></row><row><cell cols="2">Timbre &amp; F0 0.590</cell><cell>0.586</cell><cell>37.83</cell><cell>0.824</cell><cell>0.089</cell><cell>5.51</cell></row><row><cell>w/. tradeoff</cell><cell>0.619</cell><cell>0.607</cell><cell>36.66</cell><cell>0.833</cell><cell>0.088</cell><cell>7.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Analysis of the number of clustering centers N for timbre in knowledge tradeoff on CQTNet and TPPNet under scenario #1. Study of different prior knowledge introduced in KDM. F0 and timbre are two commonly used features in singing voice conversion/synthesis tasks, which can reflect music pitch and voice characteristics, respectively. To further study the impact of different prior knowledge, we selectively use F0 and timbre to serve as the version-variant factors. We conduct experiments on CQTNet and TPPNet with SHS100K dataset. The results are shown in table5where we can find that:</figDesc><table><row><cell>Model</cell><cell>N</cell><cell cols="6">SHS100K MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“ Covers80</cell></row><row><cell></cell><cell>100</cell><cell>0.466</cell><cell>0.495</cell><cell>52.69</cell><cell>0.784</cell><cell>0.088</cell><cell>7.14</cell></row><row><cell>TPPNet</cell><cell>1K 5K</cell><cell>0.463 0.462</cell><cell>0.492 0.492</cell><cell>43.06 44.06</cell><cell>0.785 0.792</cell><cell>0.088 0.084</cell><cell>9.83 8.68</cell></row><row><cell></cell><cell cols="2">10K 0.467</cell><cell>0.496</cell><cell>44.46</cell><cell>0.777</cell><cell>0.086</cell><cell>8.14</cell></row><row><cell></cell><cell>100</cell><cell>0.606</cell><cell>0.599</cell><cell>37.17</cell><cell>0.829</cell><cell>0.094</cell><cell>4.53</cell></row><row><cell>CQTNet</cell><cell>1K 5K</cell><cell>0.601 0.593</cell><cell>0.594 0.590</cell><cell>32.39 38.47</cell><cell>0.827 0.832</cell><cell>0.092 0.096</cell><cell>2.95 3.32</cell></row><row><cell></cell><cell cols="2">10K 0.609</cell><cell>0.602</cell><cell>34.34</cell><cell>0.833</cell><cell>0.092</cell><cell>4.18</cell></row><row><cell cols="8">â€¢ The results are consistent across different baselines, which indi-</cell></row><row><cell cols="8">cates that the proposed two modules can easily boost the best-</cell></row><row><cell cols="8">performing CSI baselines in a plug-and-play and model-agnostic</cell></row><row><cell>manner.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">4.3.2 â€¢ Introducing either F0 or timbre can improve the baseline perfor-</cell></row><row><cell cols="8">mance and introducing both of them will achieve better results.</cell></row><row><cell cols="8">These results further demonstrate the effectiveness of minimiz-</cell></row><row><cell cols="8">ing the correlation between the learned representations and the</cell></row><row><cell cols="3">version-variant factors.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">â€¢ Different verison-variant factors play different roles in exerting a</cell></row><row><cell cols="8">bad impact on model learning. Compared with F0, disentangling</cell></row><row><cell cols="8">timbre appears to be more beneficial to the baseline models. The</cell></row><row><cell cols="8">reason might be that voice characteristic vary from person to</cell></row><row><cell cols="8">person, which leads to high intra-song variances among versions</cell></row><row><cell cols="5">that are performed by different people.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">â€¢ Learning with knowledge tradeoff leads to better performance</cell></row><row><cell cols="8">with different baselines and datasets, which suggests that this</cell></row><row><cell cols="8">technique can further exploit the useful information hiding in</cell></row><row><cell cols="8">prior knowledge and is helpful in relieving the posterior collapse</cell></row><row><cell cols="5">of variational estimator [15, 30, 39].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">4.3.3 Analysis of the number of clustering centers for timbre in</cell></row><row><cell cols="8">knowledge tradeoff. In this experiment, we analyze the impact of the</cell></row><row><cell cols="8">number (N) of clusters used to generate pseudo-labels on the model</cell></row><row><cell cols="8">performance, which uncovers the hyper-parameter sensitivity. As</cell></row><row><cell cols="8">shown in Table 6, the model performance is overall insensitive</cell></row><row><cell cols="8">to the number of clusters. In other words, the model can achieve</cell></row><row><cell cols="8">comparable performance with relatively few pseudo-labels (e.g.</cell></row></table><note><p>N = 100) and lower complexity, which is suitable for real-world scenarios to reduce resource consumption. 4.3.4 Analysis of transition simulation in GADM. As analyzed in Sec. 3.4.1, we use metric function to serve as the transition cost</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Analysis of transition simulation in GADM on CQT-Net and TPPNet under scenario #1.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="2">SHS100K MAPâ†‘ P@10â†‘ MR1â†“ MAPâ†‘ P@10â†‘ MR1â†“ Covers80</cell></row><row><cell></cell><cell cols="2">Manhattan 0.344 0.389 79.87 0.724 0.082</cell><cell>8.58</cell></row><row><cell>TPPNet</cell><cell cols="2">Euclidean 0.542 0.551 39.98 0.805 0.085</cell><cell>7.06</cell></row><row><cell></cell><cell>Cosine</cell><cell cols="2">0.473 0.495 44.08 0.730 0.084 10.35</cell></row><row><cell></cell><cell cols="2">Manhattan 0.620 0.603 34.01 0.814 0.089</cell><cell>4.41</cell></row><row><cell>CQTNet</cell><cell cols="2">Euclidean 0.649 0.622 32.34 0.843 0.093</cell><cell>4.45</cell></row><row><cell></cell><cell>Cosine</cell><cell>0.525 0.538 48.75 0.784 0.091</cell><cell>5.15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/ytdl-org/youtube-dl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/NovaFrost/SHS100K2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://labrosa.ee.columbia.edu/projects/coversongs/covers80/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.music-ir.org/mirex/wiki/2020:Audio_Cover_Song_Identification</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/YannickJadoul/Parselmouth</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/resemble-ai/Resemblyzer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://www.mindspore.cn</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">ACKNOWLEDGEMENTS</head><p>This work was supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant No.<rs type="grantNumber">61836002</rs>, <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">62222211</rs> and No.<rs type="grantNumber">62072397</rs>. We gratefully acknowledge the support of Mindspore 8 , which is a new deep learning computing framework.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k9XUfGC">
					<idno type="grant-number">61836002</idno>
				</org>
				<org type="funding" xml:id="_TH9GHph">
					<idno type="grant-number">62222211</idno>
				</org>
				<org type="funding" xml:id="_6wZTptw">
					<idno type="grant-number">62072397</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gloss Attention for Gloss-free Sign Language Translation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Aoxiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Tianyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Weike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Sungkyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juheon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyogu</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Cover Song Identification using Convolutional Neural Network</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName><surname>Vineeth N Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="839" to="847" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Disentangled Representations for Counterfactual Regression via Mutual Information Minimization</title>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinru</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1802" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Club: A contrastive log-ratio upper bound of mutual information</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1779" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis</title>
		<author>
			<persName><forename type="first">Hyeong-Seok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhyeok</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juheon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09407</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for music classification</title>
		<author>
			<persName><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GyÃ¶rgy</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International conference on acoustics, speech and signal processing</title>
		<imprint>
			<biblScope unit="page" from="2392" to="2396" />
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Debiased and Disentangled Representations for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Sanghyeok</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8355" to="8366" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cover detection using dominant melody embeddings</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Doras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
		<idno>ISMIR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bytecover2: Towards dimensionality reduction of latent embedding for efficient cover song identification</title>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bytecover: Cover song identification via multi-loss training</title>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="551" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying cover songs&apos; with chroma features and dynamic programming beat tracking</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">E</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><surname>Poliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP&apos;07</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1429</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large-scale cover song recognition using the 2d fourier transform magnitude</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertin-Mahieux</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><surname>Thierry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual and sequential user embeddings for large-scale music recommendation</title>
		<author>
			<persName><forename type="first">Casper</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Maystre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Brost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM conference on recommender systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lagging Inference Networks and Posterior Collapse in Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WideResNet with Joint Representation Learning and Data Augmentation for Cover Song Identification</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiliang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingcheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Incheon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">2022. 18-22 September 2022</date>
			<biblScope unit="page" from="4187" to="4191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-singer: Fast multi-singer singing voice vocoder with a large-scale corpus</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenye</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3945" to="3954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Singgan: Generative adversarial network for high-fidelity singing voice generation</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenye</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2525" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Make-an-audio: Textto-audio generation with prompt-enhanced diffusion models</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12661</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">Wy</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09934</idno>
		<title level="m">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gener-Speech: towards style transfer for generalizable out-of-domain text-to-speech</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenye</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10970" to="10983" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prodiff: Progressive fast diffusion model for high-quality text-to-speech</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenye</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2595" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12523</idno>
		<title level="m">TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="124" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transformer VAE: A hierarchical model for structure-aware and interpretable music representation learning</title>
		<author>
			<persName><forename type="first">Junyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gus</forename><forename type="middle">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><forename type="middle">B</forename><surname>Carlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">N</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">H</forename><surname>Miyakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sample-level Deep Convolutional Neural Networks for Music auto-tagging Using Raw Waveforms</title>
		<author>
			<persName><forename type="first">Jongpil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 14th Sound and Music Computing Conference</title>
		<imprint>
			<publisher>SMCNetwork</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Diffsinger: Singing voice synthesis via shallow diffusion mechanism</title>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11020" to="11028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09778</idno>
		<title level="m">Pseudo numerical methods for diffusion models on manifolds</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Don&apos;t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9403" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Mid-level Melody-based Representation for Calculating Audio Similarity</title>
		<author>
			<persName><forename type="first">Matija</forename><surname>Marolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR. Citeseer</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="280" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-modal music retrieval and applications: An overview of key methodologies</title>
		<author>
			<persName><forename type="first">Meinard</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Arzt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting Cover Songs with Pitch Class Key-Invariant Networks</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Ken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Hanlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 31st International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online learning to rank for sequential music recommendation</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bruno L Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><surname>Penha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nivio</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><surname>Ziviani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="237" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06654</idno>
		<title level="m">musicnn: Pre-trained convolutional neural networks for music audio tagging</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Audio feature learning with triplet-based embedding network</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised speech decomposition via triple information bottleneck</title>
		<author>
			<persName><forename type="first">Kaizhi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7836" to="7846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Preventing Posterior Collapse with delta-VAEs</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Popmag: Pop music accompaniment generation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1198" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive learning of general-purpose audio representations</title>
		<author>
			<persName><forename type="first">Aaqib</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3875" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>SerrÃ </surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>GÃ³mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<title level="m">Audio Cover Song Identification and Similarity: Background, Approaches, Evaluation, and Beyond. Advances in Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chroma binary similarity and local alignment applied to cover song identification</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>GÃ³mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1138" to="1151" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross recurrence quantification for cover song identification</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">G</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">93017</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal music information processing and retrieval: Survey and future challenges</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Simonetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Ntalampiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Avanzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 international workshop on multilayer music representation and processing (MMRP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Janne</forename><surname>Spijkervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burgoyne</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09410</idno>
		<title level="m">Contrastive learning of musical representations</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transfer learning by supervised pre-training for audio-based music classification</title>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion</title>
		<author>
			<persName><forename type="first">Disong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Ting</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Denoising implicit feedback for recommendation</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021. Deconfounded recommendation for alleviating bias amplification</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Causal Representation Learning for Out-of-Distribution Recommendation</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 3562-3571</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13834" to="13846" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised pre-training for music classification</title>
		<author>
			<persName><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Chi</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="556" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Key-invariant convolutional neural network toward efficient cover song identification</title>
		<author>
			<persName><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Contrastive Learning with Positive-Negative Frame Mask for Music Representation</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2906" to="2915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Accurate and scalable version identification using musically-motivated embeddings</title>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>SerrÃ </surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><surname>GÃ³mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">MLSLT: Towards Multilingual Sign Language Translation</title>
		<author>
			<persName><forename type="first">Aoxiong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5109" to="5119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Simulslt: End-to-end simultaneous sign language translation</title>
		<author>
			<persName><forename type="first">Aoxiong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4118" to="4127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep cross-modal correlation learning for audio and lyrics in music retrieval</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal Pyramid Pooling Convolutional Neural Network for Cover Song Identification</title>
		<author>
			<persName><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4846" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning a representation for cover song identification using convolutional neural network</title>
		<author>
			<persName><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="541" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning</title>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hifidenoise: Highfidelity denoising text to speech with adversarial networks</title>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7232" to="7236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">MusicBERT: A Selfsupervised Learning of Music Representation</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3955" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
