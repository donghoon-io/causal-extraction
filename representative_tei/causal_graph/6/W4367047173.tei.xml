<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction</title>
				<funder ref="#_z4HF8vk">
					<orgName type="full">Chinese Scientific and Technical Innovation</orgName>
				</funder>
				<funder ref="#_Ghe3Xz5">
					<orgName type="full">Tianjin Natural Science Foundation for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_E6h2sNG #_zTvNvu4">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_FqcDJ2f">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-03">3 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qijie</forename><surname>Bai</surname></persName>
							<email>qijie.bai@mail.nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Changli</forename><surname>Nie</surname></persName>
							<email>nie_cl@mail.nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Haiwei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongming</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
							<email>yuanxj@nankai.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of CS</orgName>
								<orgName type="department" key="dep2">TJ Key Lab</orgName>
								<orgName type="institution">NDST Nankai University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">College of CS</orgName>
								<orgName type="department" key="dep2">TJ Key Lab</orgName>
								<orgName type="institution">NDST Nankai University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">College of CS</orgName>
								<orgName type="department" key="dep2">TJ Key Lab</orgName>
								<orgName type="institution">NDST Nankai University Tianjin</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of CS</orgName>
								<orgName type="institution">China Mobile Communication Group Tianjin Co</orgName>
								<address>
									<settlement>Ltd Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">TJ Key Lab</orgName>
								<orgName type="institution">NDST Nankai University Tianjin</orgName>
								<address>
									<addrLine>WWW &apos;23 April 30-May 4</addrLine>
									<postCode>2023</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country>China USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-03">3 May 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583455</idno>
					<idno type="arXiv">arXiv:2304.07302v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>â€¢ Theory of computation â†’ Dynamic graph algorithms</term>
					<term>â€¢ Computing methodologies â†’ Dimensionality reduction and manifold learning</term>
					<term>Neural networks Temporal link prediction, hyperbolic geometry, graph neural network, diffusion graph convolution, dilated causal convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the other hand, the internal order of causal correlation between historical states is captured by hyperbolic dilated causal convolution (HDCC) modules. The whole model is built upon the hyperbolic spaces to preserve the hierarchical structural information in the entire data flow. To prove the superiority of HG-WaveNet, extensive experiments are conducted on six real-world graph datasets and the results show a relative improvement by up to 6.67% on AUC for temporal link prediction over SOTA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Dynamic graphs, derived from the general graphs by adding an additional temporal dimension, have attracted a lot of attention in recent years <ref type="bibr" target="#b16">[14]</ref>. It has become a general practice to abstract real-world complex systems (e.g. traffic systems <ref type="bibr" target="#b60">[58]</ref>, social networks <ref type="bibr" target="#b52">[50]</ref> and e-commerce platforms <ref type="bibr" target="#b18">[16]</ref>) to this newly data structure due to its powerful ability for modeling the temporal interactions between entities. Temporal link prediction, aiming to forecast future relationships between nodes, is of vital importance for understanding the evolution of dynamic graphs <ref type="bibr" target="#b51">[49]</ref>.</p><p>According to the expression of temporal information, dynamic graphs can be divided into two categories: continuous dynamic graphs and discrete dynamic graphs <ref type="bibr" target="#b56">[54]</ref>. Continuous dynamic graphs, also called graph streams, can be viewed as groups of edges ordered by time and each edge is associated with a timestamp. Recording topological structures of a dynamic graph at constant time intervals as snapshots, the list of snapshots is defined as a discrete dynamic graph. In comparison, continuous dynamic graphs keep all temporal information <ref type="bibr" target="#b19">[17]</ref>, but discrete dynamic graphs are more computationally efficient benefiting from the coarse-grained updating frequency <ref type="bibr" target="#b26">[24]</ref>. This paper studies temporal link prediction on discrete dynamic graphs.</p><p>Existing studies have put forward some approaches to dynamic graph embedding and temporal link prediction. Most of them focus on two main contents: the spatial topological structure and the temporal evolutionary information of dynamic graphs. For instance, CTDNE <ref type="bibr" target="#b27">[25]</ref> runs temporal random walks to capture both spatial and temporal information on graphs. TDGNN <ref type="bibr" target="#b32">[30]</ref> introduces a temporal aggregator for GNNs to aggregate historical information of neighbor nodes and edges. Hawkes process <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b62">60]</ref>  is also applied to simulate the evolution of graphs. Furthermore, DyRep <ref type="bibr" target="#b42">[40]</ref> leverages recurrent neural networks to update node representations over time. TGAT <ref type="bibr" target="#b9">[7]</ref> uses the multi-head attention mechanism <ref type="bibr" target="#b44">[42]</ref> and a sampling strategy that fixes the number of neighbor nodes involved in message-propagation to learn both spatial and temporal information.</p><p>These methods are all built upon Euclidean spaces. However, recent works <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b21">19]</ref> have noticed that most real-world graph data, such as social networks, always exhibit implicit hierarchical structures and power-law distributions (as shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>) rather than uniform grid structures which fit Euclidean spaces best. The mismatches between data distributions and space geometries severely limit the performances of Euclidean models <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b28">26]</ref>.</p><p>Hyperbolic spaces are those of constant negative curvatures, and are found to keep great representation capacity for data with hierarchical structures and power-law distributions due to the exponential expansion property (as shown in Figure <ref type="figure" target="#fig_0">1(b</ref>)) <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b31">29]</ref>. In the past few years, researchers have made some progress in representing hierarchical data in hyperbolic spaces <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b29">27,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b58">56]</ref>. HTGN <ref type="bibr" target="#b54">[52]</ref> attempts to embed dynamic graphs into hyperbolic geometry and achieves state-of-the-art performance. It adopts hyperbolic GCNs to capture spatial features and a hyperbolic temporal contextual attention module to extract the historical information. However, this method faces two major shortcomings. First, for some large graphs with long paths, the message propagation mechanism of GCNs is not effective enough because only direct neighbor nodes can be calculated in each step. Second, the temporal contextual attention module cannot handle the internal order of causal correlation in historical data and thus loses valid information for temporal evolutionary process.</p><p>To address the two aforementioned shortcomings of HTGN, in this paper, we propose a novel hyperbolic graph neural network model named HGWaveNet for temporal link prediction. In HG-WaveNet, we first project the nodes into hyperbolic spaces. Then in the aspect of spatial topology, a hyperbolic diffusion graph convolution (HDGC) module is designed to learn node representations of each snapshot effectively from both direct neighbors and indirectly connected nodes. For temporal information, recurrent neural networks always emphasize short time memory while ignoring that of long time due to the time monotonic assumption <ref type="bibr" target="#b7">[6]</ref>, and the attention mechanism ignores the causal orders in temporality. Inspired by WaveNet <ref type="bibr" target="#b43">[41]</ref> and Graph WaveNet <ref type="bibr" target="#b50">[48]</ref>, we present hyperbolic dilated causal convolution (HDCC) modules to obtain hidden cumulative states of nodes by aggregating historical information and capturing temporal dependencies. In the training process of each snapshot, the hidden cumulative states and spatial-based node representations are fed into a hyperbolic gated recurrent unit (HGRU). The outputs of HGRU are regarded as the integration of spatial topological structures and temporal evolutionary information, and are utilized for final temporal link prediction. A hyperbolic temporal consistency (HTC) <ref type="bibr" target="#b54">[52]</ref> component is also leveraged to ensure stability for tracking the evolution of graphs.</p><p>To conclude, the main contributions of this paper are as follows:</p><p>â€¢ We propose a novel hyperbolic graph neural network model named HGWaveNet for temporal link prediction, which learns both spatial hierarchical structures and temporal causal correlation of dynamic graphs. â€¢ With regard to spatial topological structures, we design a hyperbolic diffusion graph convolution (HDGC) module to fit the power-law distributions of data and aggregate information from a wider range of nodes with greater effectiveness. â€¢ For temporal evolution, we present hyperbolic dilated causal convolution (HDCC) modules to capture the internal causality between snapshots, and a hyperbolic temporal consistency (HTC) component is applied to remain stable when learning the evolution of graphs. â€¢ We conduct extensive experiments on diverse real-world dynamic graphs. The results prove the superiority of HG-WaveNet, as it has a relative improvement by up to 6.67% in terms of AUC over state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we systematically review the relevant works on temporal link prediction and hyperbolic graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Link Prediction</head><p>Temporal link prediction on dynamic graphs has attracted increasing interests in the past few years. Early methods usually use traditional algorithms or shallow neural architectures to represent structural and temporal information. For instance, CTDNE <ref type="bibr" target="#b27">[25]</ref> captures the spatial and temporal information simultaneously by adding temporal constraints to random walk. Another example of using temporal random walk is DynNode2vec <ref type="bibr" target="#b25">[23]</ref>, which updates the sampled sequences incrementally at each snapshot rather than generating them anew. DynamicTriad <ref type="bibr" target="#b61">[59]</ref> imposes the triad closure process and models the evolution of graphs by developing closed triads from open triads. Furthermore, Change2vec <ref type="bibr" target="#b3">[2]</ref> improves this process and makes it applicable for dynamic heterogeneous graphs. HTNE <ref type="bibr" target="#b62">[60]</ref> integrates the Hawkes process into graph embedding to learn the influence of historical neighbors on current neighbors. In contrast, graph neural network methods have recently received increasing attention. GCN <ref type="bibr" target="#b48">[46]</ref> provides an excellent node embedding pattern for general tasks on graphs, and most of later models take GCN or its variants, such as GraphSAGE <ref type="bibr" target="#b14">[12]</ref> and GAT <ref type="bibr" target="#b45">[43]</ref>, as basic modules for learning topological structures. GCRN <ref type="bibr" target="#b38">[36]</ref> feeds node representations learned from GCNs into a modified LSTM to obtain the temporal information. Similar ideas are explored in EvolveGCN <ref type="bibr" target="#b30">[28]</ref>, E-LSTM-D <ref type="bibr" target="#b6">[5]</ref> and NTF <ref type="bibr" target="#b49">[47]</ref>. The main difference between EvolveGCN and E-LSTM-D is that EvolveGCN could be considered as a combination of GCN and RNN while E-LSTM-D uses LSTM together with an encoder-decoder architecture. NTF takes a reverse order that characterizes the temporal interactions with LSTM before adopting the MLP for non-linearities between different latent factors, and sufficiently learns the evolving characteristics of graphs. To better blend the spatial and temporal node features, DySAT <ref type="bibr" target="#b36">[34]</ref> proposes applying multi-head self-attention. TGN <ref type="bibr" target="#b33">[31]</ref> leverages memory modules with GCN operators and significantly increases the computational efficiency. TNS <ref type="bibr" target="#b47">[45]</ref> provides an adaptive receptive neighborhood for each node at any time. VRGNN <ref type="bibr" target="#b13">[11]</ref> models the uncertainty of node embeddings by regarding each node in each snapshot as a distribution.</p><p>However, the prevailing methods are built upon Euclidean spaces, which are not isometric with the power-law distributions of realworld graphs, and may cause distortion with the graph scale grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hyperbolic Graph Representation Learning</head><p>Representation learning in hyperbolic spaces has been noticed due to their fitness to the hierarchical structures of real-world data. The significant performance advantages shown by the shallow PoincarÃ© <ref type="bibr" target="#b28">[26]</ref> and Lorentz <ref type="bibr" target="#b29">[27]</ref> models spark more attempts to this issue. Integrated with graph neural networks, HGNN <ref type="bibr" target="#b24">[22]</ref> and HGCN <ref type="bibr" target="#b5">[4]</ref> are designed for graph classification and node embedding tasks separately. HAT <ref type="bibr" target="#b58">[56]</ref> exploits attention mechanism for hyperbolic node information propagation and aggregation. LGCN <ref type="bibr" target="#b59">[57]</ref> builds the graph operations of hyperbolic GCNs with Lorentzian version and rigorously guarantees that the learned node features follow the hyperbolic geometry. The above hyperbolic GNN-based models all adopt the bi-directional transition between a hyperbolic space and corresponding tangent spaces, while a tangent space is the first-order approximation of the original space and may inevitably cause distortion. To avoid distortion, H2H-GCN <ref type="bibr" target="#b10">[8]</ref> develops a manifold-preserving graph convolution and directly works on hyperbolic manifolds. For practical application situations, HGCF <ref type="bibr" target="#b39">[37]</ref>, HRCF <ref type="bibr" target="#b55">[53]</ref> and HICF <ref type="bibr">[51]</ref> study the hyperbolic collaborative filtering for user-item recommendation systems. Through hyperbolic graph learning, HyperStockGAT <ref type="bibr" target="#b37">[35]</ref> captures the scalefree spatial and temporal dependencies in stock prices, and achieves state-of-the-art stock forecasting performance.</p><p>HVGNN <ref type="bibr" target="#b40">[38]</ref> and HTGN <ref type="bibr" target="#b54">[52]</ref> fill the gap of hyperbolic models on dynamic graphs. HVGNN generates stochastic node representations of hyperbolic normal distributions via a hyperbolic graph variational autoencoder to represent the uncertainty of dynamics. HTGN adopts a conventional model architecture that handles the spatial and temporal information with HGCNs and contextual attention modules separately, however, ignores the causal order in the graph evolution. To further improve the hyperbolic graph models especially on temporal link prediction problem, we propose our HGWaveNet in terms of discrete dynamic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we first give the formalized definitions of discrete dynamic graphs and temporal link prediction. Then some critical fundamentals about hyperbolic geometry are introduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>This paper discusses temporal link prediction on discrete dynamic graphs. Following <ref type="bibr" target="#b20">[18]</ref>, we define discrete dynamic graphs as: Definition 3.1 (Discrete Dynamic Graphs). In discrete dynamic graph modeling, dynamic graphs can be viewed as a sequence of snapshots sampled from the original evolving process at consecutive time points. Formally, discrete dynamic graphs are represented as</p><formula xml:id="formula_0">G = (G 0 , G 1 , â€¢ â€¢ â€¢ , G ğ‘‡ ), in which G ğ‘¡ is a snapshot at timestamp ğ‘¡.</formula><p>The time granularity for snapshot divisions could be hours, days, months or even years depending on specific datasets and applications.</p><p>Based on Definition 3.1, temporal link prediction is described as: Definition 3.2 (Temporal Link Prediction). The aim of temporal link prediction is to predict the links appeared in the snapshots after timestamp ğ‘¡ based on the observed snapshots before timestamp ğ‘¡. Formally, the model takes</p><formula xml:id="formula_1">(G 0 , G 1 , â€¢ â€¢ â€¢ , G ğ‘¡ -1 )</formula><p>as input in the training process, and then makes predictions on</p><formula xml:id="formula_2">(G ğ‘¡ , G ğ‘¡ +1 , â€¢ â€¢ â€¢ , G ğ‘‡ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperbolic Geometry of the PoincarÃ© Ball</head><p>The ğ‘›-dimensional hyperbolic space (H ğ‘› ğ‘ , ğ‘” ğ‘,H ) is the unique simply connected ğ‘›-dimensional complete Riemannian manifold H ğ‘› ğ‘ with a constant negative curvature -ğ‘ (ğ‘ &gt; 0), and ğ‘” ğ‘,H is the Riemannian metric. The tangent space T x H ğ‘› ğ‘ is a Euclidean, local, first-order approximation of H ğ‘› ğ‘ around the point x âˆˆ H ğ‘› ğ‘ . Similar to <ref type="bibr" target="#b28">[26]</ref> and <ref type="bibr" target="#b11">[9]</ref>, we construct our method based on the PoincarÃ© ball, one of the most widely used isometric models of hyperbolic spaces. Corresponding to (H ğ‘› ğ‘ , ğ‘” ğ‘,H ), the PoincarÃ© ball (B ğ‘› ğ‘ , ğ‘” ğ‘,B ) is defined as</p><formula xml:id="formula_3">B ğ‘› ğ‘ := x âˆˆ R ğ‘› : ğ‘ âˆ¥xâˆ¥ 2 &lt; 1 , ğ‘” ğ‘,B x := ğœ† ğ‘ x 2 ğ‘” ğ¸ , ğœ† ğ‘ x := 2 1 -ğ‘ âˆ¥xâˆ¥ 2 ,<label>(1)</label></formula><p>where ğ‘” ğ¸ = I ğ‘› is the Euclidean metric tensor.</p><p>The PoincarÃ© ball manifold B ğ‘› ğ‘ is an open ball of radius 1/ âˆš ğ‘ (see Figure <ref type="figure" target="#fig_1">2</ref>). The induced distance between two points x, y âˆˆ B ğ‘› ğ‘ is measured along a geodesic and given by<ref type="foot" target="#foot_0">foot_0</ref> </p><formula xml:id="formula_4">ğ‘‘ ğ‘ (x, y) = 2 âˆš ğ‘ â€¢ tanh -1 âˆš ğ‘ âˆ¥ -x âŠ• ğ‘ yâˆ¥ ,<label>(2)</label></formula><p>in which the MÃ¶bius addition âŠ• ğ‘ in B ğ‘› ğ‘ is defined as</p><formula xml:id="formula_5">x âŠ• ğ‘ y := 1 + 2ğ‘ âŸ¨x, yâŸ© + ğ‘ âˆ¥yâˆ¥ 2 x + 1 -ğ‘ âˆ¥xâˆ¥ 2 y 1 + 2ğ‘ âŸ¨x, yâŸ© + ğ‘ 2 âˆ¥xâˆ¥ 2 âˆ¥yâˆ¥ 2 . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>To map points between hyperbolic spaces and tangent spaces, exponential and logarithmic maps are given <ref type="bibr" target="#b11">[9]</ref>. For x, y âˆˆ B ğ‘› ğ‘ , v âˆˆ T x B ğ‘› ğ‘ , x â‰  y and v â‰  0, the exponential map exp ğ‘ x :</p><formula xml:id="formula_7">T x B ğ‘› ğ‘ â†’ B ğ‘› ğ‘ is exp ğ‘ x (v) = x âŠ• ğ‘ tanh âˆš ğ‘ğœ† ğ‘ x âˆ¥vâˆ¥ 2 v âˆš ğ‘ âˆ¥vâˆ¥ ,<label>(4)</label></formula><p>and the logarithmic map log ğ‘ x :</p><formula xml:id="formula_8">B ğ‘› ğ‘ â†’ T x B ğ‘› ğ‘ is log ğ‘ x (y) = ğ‘‘ ğ‘ (x, y) -x âŠ• ğ‘ y ğœ† ğ‘ x âˆ¥ -x âŠ• ğ‘ yâˆ¥ ,<label>(5)</label></formula><p>where ğœ† ğ‘ x and ğ‘‘ ğ‘ (x, y) are the same as in Equations ( <ref type="formula" target="#formula_3">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>). In our method, we use the origin point o as the reference point x to balance the errors in diverse directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>This section describes our proposed model HGWaveNet. First, we elaborate on the details of two core components: hyperbolic diffusion graph convolution (HDGC) and hyperbolic dilated causal convolution (HDCC). Then other key modules contributing to HG-WaveNet are introduced, including gated HDCC, hyperbolic gated recurrent unit (HGRU), hyperbolic temporal consistency (HTC) and Fermi-Dirac decoder. Finally, we summarize the overall framework of HGWaveNet and analyse the time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperbolic Diffusion Graph Convolution</head><p>Hyperbolic graph convolutional neural networks (HGCNs <ref type="bibr" target="#b5">[4]</ref>) are built analogous to traditional GNNs. A typical HGCN layer consists of three key parts: hyperbolic feature transform</p><formula xml:id="formula_9">h ğ‘™,B ğ‘– = W ğ‘™ âŠ— ğ‘ ğ‘™ -1 x ğ‘™-1,B ğ‘– âŠ• ğ‘ ğ‘™ -1 b ğ‘™ ,<label>(6)</label></formula><p>attention-based neighbor aggregation</p><formula xml:id="formula_10">y ğ‘™,B ğ‘– = Agg ğ‘ ğ‘™ (h ğ‘™,B ) ğ‘– = exp ğ‘ ğ‘™ o Att Concat ğ‘— âˆˆN (ğ‘–) log ğ‘ ğ‘™ -1 o h ğ‘™,B ğ‘— ,<label>(7)</label></formula><p>and hyperbolic activation</p><formula xml:id="formula_11">x ğ‘™,B ğ‘– = exp ğ‘ ğ‘™ o ğœ log ğ‘ ğ‘™ o y ğ‘™,B ğ‘– ,<label>(8)</label></formula><p>in which W ğ‘™ , b ğ‘™ are trainable parameters and x ğ‘™,B ğ‘– is the representation of node ğ‘– at layer ğ‘™ in manifold B ğ‘› ğ‘ ğ‘™ . Following <ref type="bibr" target="#b54">[52]</ref>, the matrix-vector multiplication âŠ— ğ‘ is defined as</p><formula xml:id="formula_12">M âŠ— ğ‘ x := exp ğ‘ o M log ğ‘ o (x) , M âˆˆ R ğ‘›Ã—ğ‘› , x âˆˆ B ğ‘› ğ‘ .<label>(9)</label></formula><p>However, the shallow HGCN can only aggregate information of direct neighbors and is not effective enough for large graphs with long paths. To overcome this shortcoming, we impose the diffusion process referring to <ref type="bibr" target="#b23">[21]</ref>. Consider a random walk process with restart probability ğ›¼ on G, and a state transition matrix P (for most situations, P is the normalized adjacent matrix). Such Markov process converges to a stationary distribution P âˆˆ R ğ‘›Ã—ğ‘› after many steps, the ğ‘–-th row of which is the likelihood of diffusion from node ğ‘–. The stationary distribution P can be calculated in the closed form <ref type="bibr" target="#b41">[39]</ref> </p><formula xml:id="formula_13">P = âˆ âˆ‘ï¸ ğ‘˜=0 ğ›¼ (1 -ğ›¼) ğ‘˜ P ğ‘˜ , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where ğ‘˜ is the diffusion step. In practice, a finite ğ¾-step truncation of the diffusion process is adopted and separate trainable weight matrices are added to each step for specific objective tasks. Then, the diffusion convolution layer on graphs is defined as</p><formula xml:id="formula_15">Z = ğ¾ âˆ‘ï¸ ğ‘˜=0 A ğ‘˜ XW ğ‘˜ ,<label>(11)</label></formula><p>in which A is the bi-direct adjacent matrix, X is the input node features, W ğ‘˜ is the weight matrix for the ğ‘˜-th diffusion step, and Z is the output node representations. On account of the sparsity of real-world data, we convert the graph convolution into equivalent spatial domains for efficiency. Do all above operations in the hyperbolic space, and replace the summation in Equation ( <ref type="formula" target="#formula_15">11</ref>) with an attention mechanism for better information aggregation. Then, the ğ‘˜-th step of hyperbolic diffusion graph convolution at layer ğ‘™ is</p><formula xml:id="formula_16">X ğ‘™ ğ‘˜ = HGCN ğ‘ ğ‘™ ğ‘˜ A ğ‘˜ , X ğ‘™-1 ,<label>(12)</label></formula><p>where HGCN ğ‘ ğ‘™ (â€¢) is the conjunctive form of Equations ( <ref type="formula" target="#formula_9">6</ref>), <ref type="bibr" target="#b9">(7)</ref>, and (8) with the adjacent matrix and node features as inputs. X ğ‘™ is calculated as</p><formula xml:id="formula_17">X ğ‘™ = exp ğ‘ ğ‘™ o Att Concat ğ‘˜ log ğ‘ ğ‘™ ğ‘˜ o X ğ‘™ ğ‘˜ .<label>(13)</label></formula><p>Hyperbolic diffusion graph convolution is finally constructed with the stack of layers defined by Equations ( <ref type="formula" target="#formula_17">13</ref>) and ( <ref type="formula" target="#formula_16">12</ref>), as sketched in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperbolic Dilated Causal Convolution</head><p>Causal convolution refers to applying a convolution filter only to data at past timestamps <ref type="bibr" target="#b43">[41]</ref>, and guarantees that the order of data modeling is not violated, which means that the prediction</p><formula xml:id="formula_18">ğ‘ (ğ‘¥ ğ‘¡ |ğ‘¥ 0 , ğ‘¥ 1 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘¡ -1</formula><p>) emitted by the model at timestamp ğ‘¡ cannot depend on future data ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘¡ +1 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‡ . For 1-D temporal information, this operation can be implemented by shifting the output of a normal convolution with kernel size ğ‘† by âŒŠğ‘†/2âŒ‹ steps, but a sufficiently large receptive field requires stacking many layers. To decrease the computational cost, dilated convolutions are adopted. By skipping the inputs with a certain step ğ‘‘, a dilated convolution applies its kernel over a larger area than its length, and with the stack of dilated convolution layers, the receptive field expands exponentially while preserving high computational efficiency. Based on the above definitions of causal convolutions and dilated convolutions, the dilated causal convolution operation in manifold B ğ‘› ğ‘ can be formalized mathematically as</p><formula xml:id="formula_19">(Z ğ‘– âŠ™ F) ğ‘¡ = exp ğ‘ o ğ‘†-1 âˆ‘ï¸ ğ‘ =0 F ğ‘  log ğ‘ o Z ğ‘–,ğ‘¡ -ğ‘‘Ã—ğ‘  ,<label>(14)</label></formula><p>where ğ‘‘ is the dilation step, Z ğ‘–,ğ‘¡ âˆˆ B ğ‘› ğ‘ (ğ‘¡ â‰¤ ğ‘‡ ) is the representation of node ğ‘– at snapshot G ğ‘¡ and F âˆˆ R ğ‘†Ã—ğ‘› denotes the kernels for all dimensions (also called channels). A stacked hyperbolic dilated causal convolution module is shown in Figure <ref type="figure">4</ref>.</p><p>Compared to attention-based historical information aggregation <ref type="bibr" target="#b54">[52]</ref>, HDCC can learn the internal order of causal correlation in temporality. Meanwhile, compared to RNNs, HDCC allows parallel computation and alleviates vanishing or exploding gradient problems due to its non-recursion <ref type="bibr" target="#b50">[48]</ref>. Considering the superiority of HDCC on sequence problems, we leverage this module in handling the temporal evolutionary information of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Key Modules in HGWaveNet</head><p>Except for HDGC and HDCC, some other modules contribute greatly to our proposed HGWaveNet. Based on HDCC, we further design gated HDCC to preserve the valid historical information. Hyperbolic gated recurrent unit (HGRU) <ref type="bibr" target="#b54">[52]</ref> is able to efficiently learn the latest node representations in PoincarÃ© ball from historical states and current spatial characteristics. Additionally, as defined in <ref type="bibr" target="#b54">[52]</ref>, hyperbolic temporal consistency (HTC) module provides a similarity constraint in temporality to ensure stability during graph evolution. Finally, a Fermi-Dirac decoder <ref type="bibr" target="#b21">[19]</ref> is used to compute the probability scores for edge reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Gated HDCC.</head><p>As shown in the left part of Figure <ref type="figure" target="#fig_5">5</ref>, we adopt a simple gating mechanism on the outputs of HDCCs. What is noteworthy is the use of logarithmic map before the Euclidean activation functions and exponential map after gating. The formulation is described as</p><formula xml:id="formula_20">H ğ‘–,ğ‘¡ = exp ğ‘ o tanh log ğ‘ o ((Z ğ‘– âŠ™ F 1 ) ğ‘¡ ) â€¢ ğœ log ğ‘ o ((Z ğ‘– âŠ™ F 2 ) ğ‘¡ ) ,<label>(15)</label></formula><p>where F 1 and F 2 are different convolution kernels for two HDCCs, H ğ‘–,ğ‘¡ âˆˆ B ğ‘› ğ‘ (ğ‘¡ â‰¤ ğ‘‡ ) is the historical hidden state of node ğ‘– at snapshot G ğ‘¡ , and â€¢ denotes element-wise product. In addition, to enable a deeper model for learning more implicit information, residual and skip connections are applied throughout the gated HDCC layers. 4.3.2 Hyperbolic Gated Recurrent Unit. HGRU <ref type="bibr" target="#b54">[52]</ref> is the hyperbolic variant of GRU <ref type="bibr" target="#b7">[6]</ref>. Performed in the tangent space, HGRU computes the output with high efficiency from historical hidden states and current input features, as</p><formula xml:id="formula_21">Z ğ‘–,ğ‘¡ = exp ğ‘ o GRU log ğ‘ o X ğ‘–,ğ‘¡ , log ğ‘ o H ğ‘–,ğ‘¡ -1 .<label>(16)</label></formula><p>In HGWaveNet, the historical hidden states are from gated HDCC module, so we use a single HGRU cell for just one step rather than linking it into recursion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Hyperbolic Temporal Consistency.</head><p>In real-world graphs, the evolution proceeds continuously. Correspondingly, the node representations are expected to change gradually in terms of temporality, which means that the node representations at two consecutive snapshots should keep a short distance. Hence, HTC <ref type="bibr" target="#b54">[52]</ref> defines a similarity constraint penalty between Z ğ‘¡ -1 and Z ğ‘¡ at snapshot G ğ‘¡ as</p><formula xml:id="formula_22">L ğ‘¡ HTC = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 ğ‘‘ ğ‘ Z ğ‘–,ğ‘¡ -1 , Z ğ‘–,ğ‘¡ ,<label>(17)</label></formula><p>where ğ‘‘ ğ‘ (â€¢) is defined in Equation ( <ref type="formula" target="#formula_4">2</ref>) and ğ‘ is the number of nodes. By adding the penalty in optimization process, HTC ensures that the node representations do not change rapidly and the stability of graph evolution is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Fermi-Dirac Decoder.</head><p>As a generalization of sigmoid, Fermi-Dirac decoder <ref type="bibr" target="#b21">[19]</ref> gives a probability score for edges between nodes ğ‘– and ğ‘—, which fits the temporal link prediction problem greatly. It is defined as</p><formula xml:id="formula_23">ğ‘ ğ¹ -ğ· (x ğ‘– , x ğ‘— ) = 1 exp ğ‘‘ ğ‘ x ğ‘– , x ğ‘— -ğ‘Ÿ /ğ‘  + 1 ,<label>(18)</label></formula><p>where ğ‘Ÿ and ğ‘  are hyper-parameters, and x ğ‘– , x ğ‘— âˆˆ B ğ‘› ğ‘ are hyperbolic representations of nodes ğ‘– and ğ‘—.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Framework of HGWaveNet</head><p>With all modules introduced above, we now summarize the overall framework of HGWaveNet in Figure <ref type="figure" target="#fig_5">5</ref>. For the snapshot G ğ‘¡ -1 âˆˆ G, we first project the input node features into the hyperbolic space with exponential map, and then an HDGC module is applied to learn the spatial topological structure characteristics denoted by X ğ‘¡ -1 . Simultaneously, the stacked gated HDCC layers calculate the latest historical hidden state H ğ‘¡ -2 from previous node representations Z ğ‘¡ -2 , Z ğ‘¡ -3 , â€¢ â€¢ â€¢ (the initial historical node representations are padded randomly at the beginning of training). Next, the spatial information X ğ‘¡ -1 and the temporal information H ğ‘¡ -2 are inputted into a single HGRU cell. The output of HGRU Z ğ‘¡ -1 is exactly the new latest node representations at snapshot G ğ‘¡ -1 , and is fed into the Fermi-Dirac decoder to make temporal link predictions for snapshot G ğ‘¡ .</p><p>To maximize the probability of connected nodes in G ğ‘¡ and minimize the probability of unconnected nodes, we use a cross-entropy  </p><formula xml:id="formula_24">like loss L ğ‘¡ ğ¶ğ¸ defined as L ğ‘¡ ğ¶ğ¸ =Avg ğ‘–âˆ¼ ğ‘¡ ğ‘— -log ğ‘ ğ¹ -ğ· Z ğ‘–,ğ‘¡ -1 , Z ğ‘—,ğ‘¡ -1 + Avg ğ‘– â€² â‰ ğ‘¡ ğ‘— â€² -log 1 -ğ‘ ğ¹ -ğ· Z ğ‘– â€² ,ğ‘¡ -1 , Z ğ‘— â€² ,ğ‘¡ -1 ,<label>(19)</label></formula><p>in which âˆ¼ ğ‘¡ denotes the connection at snapshot G ğ‘¡ between two nodes, and â‰ ğ‘¡ is the opposite. (ğ‘– â€² , ğ‘— â€² ) is the sampled negative edge for accelerating training and preventing over-smoothing. Taking the HTC module into account and summing up all snapshots, the complete loss function is</p><formula xml:id="formula_25">L = ğ‘‡ âˆ‘ï¸ ğ‘¡ =0 L ğ‘¡ ğ¶ğ¸ + ğœ†L ğ‘¡ ğ»ğ‘‡ğ¶ .<label>(20)</label></formula><p>Time Complexity Analysis. We analyse the time complexity of our proposed HGWaveNet by module for each snapshot. For the HDGC module, the computation is of O ((ğ‘ğ‘‘<ref type="foot" target="#foot_1">foot_1</ref> + |E ğ‘¡ |ğ¾ğ‘‘)ğ¿), where ğ‘‘ is the dimension of node representations, |E ğ‘¡ | is the edge number for snapshot G ğ‘¡ , ğ¾ is the truncated diffusion steps and ğ¿ is the layer number of HDGC. For gated HDCC, the time complexity is O (ğ‘† ğ· ğ‘‘ğ· â€² ), in which ğ‘† and ğ· are the kernel size and dilated depth of a single HDCC, respectively, and ğ· â€² is the layer number of the complete gated HDCC module. HGRU and HTC run only once for each snapshot, and both take the time complexity of O (ğ‘ğ‘‘). For Fermi-Dirac decoder, the time complexity is O (|E ğ‘¡ |ğ‘‘ + |E ğ‘¡ |ğ‘‘), in which |E ğ‘¡ | is the number of negative samples for snapshot G ğ‘¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND ANALYSIS</head><p>In this section, we conduct extensive experiments on six real-world graphs and prove the superiority of our proposed HGWaveNet. In addition, we also conduct ablation studies and hyper-parameter analysis to corroborate the three primary ideas of our method: the fitness between graph distributions and hyperbolic geometry, the effectiveness of spatial information from a wider range of neighbors Dynamic PoincarÃ© aggregated by HDGC, and the causality of historical information in the evolutionary process learned by HDCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>5.1.1 Datasets. We evaluate our proposed model and baselines on six real-world datasets from diverse areas, including email communication networks Enron 2 [1], academic co-author networks DBLP<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr" target="#b13">[11]</ref> and HepPh<ref type="foot" target="#foot_3">foot_3</ref>  <ref type="bibr" target="#b22">[20]</ref>, Internet router networks AS733<ref type="foot" target="#foot_4">foot_4</ref>  <ref type="bibr" target="#b22">[20]</ref>, social networks FB<ref type="foot" target="#foot_5">foot_5</ref>  <ref type="bibr" target="#b34">[32]</ref> and movie networks MovieLens<ref type="foot" target="#foot_6">foot_6</ref>  <ref type="bibr" target="#b15">[13]</ref>. The statistics of these datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. We take the same splitting ratios for training and testing as <ref type="bibr" target="#b54">[52]</ref> on all datasets except MovieLens on which a similar manner is adopted. Gromov's hyperbolicity <ref type="bibr" target="#b12">[10]</ref> ğ›¿ is used to measure the tree-likeness and hierarchical properties of graphs. A lower ğ›¿ denotes a more tree-like structure and ğ›¿ = 0 denotes a pure tree. The datasets we choose all remain implicitly hierarchical and show distinct power-law distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>Considering that HGWaveNet is constructed in the hyperbolic space for dynamic graphs, we choose seven competing baselines either in hyperbolic spaces or built for dynamic graphs to verify the superiority of our model. The baselines are summarized in Table <ref type="table" target="#tab_1">2</ref>, where HTGN on PoincarÃ© ball shows stateof-the-art performance in most evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Tasks and Metrics.</head><p>Similar to <ref type="bibr" target="#b54">[52]</ref>, our experiments consist of two different tasks: temporal link prediction and temporal new link prediction. Specifically, temporal link prediction aims to predict the edges that appear in G ğ‘¡ and other snapshots after timestamp ğ‘¡ based on the training on</p><formula xml:id="formula_26">(G 0 , G 1 , â€¢ â€¢ â€¢ , G ğ‘¡ -1 )</formula><p>, while temporal new link prediction aims to predict those in G ğ‘¡ but not in G ğ‘¡ -1 . To quantify the experimental performance, we choose the widely used metrics average precision (AP) and area under ROC curve (AUC). The datasets are split into training sets and test sets as shown in Table <ref type="table" target="#tab_0">1</ref> by snapshots to run both our proposed model and baselines on the above tasks.  <ref type="table" target="#tab_3">4</ref>.</p><p>â˜… Part of results are from <ref type="bibr" target="#b54">[52]</ref> under the same experimental setup. The same applies to Table <ref type="table" target="#tab_3">4</ref>. We implement HGWaveNet 8 with PyTorch on Ubuntu 18.04. Each experiment is repeated five times to avoid random errors, and the average results (with form average value Â± standard deviation) on the test sets are reported in Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref>. Next, we discuss the experimental results on each evaluation task separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Temporal Link Prediction.</head><p>The results on temporal link prediction are shown in Table <ref type="table" target="#tab_2">3</ref>. Obviously, our HGWaveNet outperforms all baselines including the hyperbolic state-of-the-art model HTGN both on AUC and AP. To analyse the results further, we intuitively divide the six datasets into two groups: small (Enron, DBLP, AS733) and large (FB, HepPh, MovieLens), according to the scales. It can be observed that the superiority of our model is more significant on large graphs than small ones compared with baselines, especially those Euclidean methods. This is because the representation capacity of Euclidean methods decreases rapidly with increasing graph scales, while hyperbolic models remain stable by virtue of the fitness between data distributions and space properties. 8 The code is open sourced in <ref type="url" target="https://github.com/TaiLvYuanLiang/HGWaveNet">https://github.com/TaiLvYuanLiang/HGWaveNet</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.2</head><p>Temporal New Link Prediction. The results on temporal new link prediction are shown in Table <ref type="table" target="#tab_3">4</ref>. This task aims to predict the edges unseen in the training process and evaluates the inductive ability of models. Our model shows greater advantages, and similar gaps with those on temporal link prediction between small and large graphs appear again. Another observed fact is that even for the inductive evaluation task, the two hyperbolic static methods HGCN and HAT have a relatively better performance than the Euclidean dynamic models. This observation further demonstrates the excellence of modeling real-world graphs on hyperbolic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To asses the contribution of each component, we conduct the following ablation study on three HGWaveNet variants:</p><p>â€¢ w/o HDGC: HGWaveNet without the hyperbolic diffusion graph convolution, implemented by replacing HDGC with shallow HGCN to learn from spatial characteristics. â€¢ w/o HDCC: HGWaveNet without the hyperbolic dilated causal convolution, implemented by replacing HDCC with an attention mechanism to achieve historical hidden states.</p><p>â€¢ w/o B: HGWaveNet without hyperbolic geometry, implemented by replacing all modules with corresponding Euclidean versions.</p><p>We take the same experimental setup with HGWaveNet and baselines on these three variants, and the results are reported in the last rows of Table <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">Table 4</ref>. The performance drops noticeably after removing any of these three components, which indicates their respective importance. In the following, we discuss the details of the effectiveness of these variants.</p><p>HDGC module imposes the diffusion process into hyperbolic graph convolution networks. It allows nodes to aggregate information from a wider range of neighbors and improves the efficiency of message propagation by calculating the stationary distribution of a long Markov process in a closed form. The fact that performance of w/o HDGC variant has a larger degradation on large graphs than small ones forcefully proves the effectiveness of this component for large graphs with long paths.</p><p>The purpose of HDCC module is to capture the internal order of causal correlation in sequential data. Generally, the larger a graph is, the more complex its evolution is and the richer historical information it has. The experimental results on w/o HDCC show the powerful ability of this component to capture causal order in temporal evolution process, especially on complex graphs (i.e. FB and MovieLens).</p><p>As observed in Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref>, the degradation of w/o B is much more severe than that of the other two variants, which strongly supports our primary idea that hyperbolic geometry fits the power-law distributions of real-world graphs very well. Moreover, it is noteworthy that for most results, the variant w/o B exhibits a comparable or even exceeding performance to other Euclidean baselines. This proves that our model architecture still keeps advanced even without the advantages of hyperbolic geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Analysis</head><p>We further study the influences of three hyper-parameters: representation dimension, truncated diffusion step and dilated depth.</p><p>We evaluate the performance of HGWaveNet and one of the best Euclidean models GRUGCN on two datasets FB (relatively sparse) and MovieLens (relatively dense) by setting the representation dimension into different values, as shown in Figure <ref type="figure" target="#fig_6">6</ref>. On the one hand, it is clear that with the dimension decreasing, our model remains stable compared to GRUGCN, to which the hyperbolic geometry contributes greatly. On the other hand, for HGWaveNet, the degradation of the performance on MovieLens is severer than that on FB, which is consistent with the conclusion in <ref type="bibr" target="#b57">[55]</ref> that hyperbolic models fit sparse data better than dense data.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows the results of our model for temporal link prediction on MovieLens with different values of truncated diffusion step ğ¾ in HDGC and dilated depth ğ· in HDCC. The wrap-up observation that the performance generally increases as ğ¾ and ğ· increase proves the positive influences of diffusion process and dilation convolution. Specifically, in terms of truncated diffusion step, the performance has a great improvement from ğ¾ = 1 to ğ¾ = 2. When ğ¾ &gt; 2, the benefit from increasing ğ¾ becomes too small to match the surge of computation from exponentially expanding neighbors in graphs. Dilated depth ğ· is used to control the receptive field of  causal convolution. However, as the receptive field expands, more noise is imported and may degrade the model performance. According to our extensive experiments, ğ· = 3 is a nice choice in most situations, with the causal convolution kernel size being 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a hyperbolic graph neural network HG-WaveNet for temporal link prediction. Inspired by the observed power-law distribution and implicit hierarchical structure of realworld graphs, we construct our model on the PoincarÃ© ball, one of the isometric models of hyperbolic spaces. Specifically, we design two novel modules hyperbolic diffusion graph convolution (HDGC) and hyperbolic dilated causal convolution (HDCC) to extract the spatial topological information and temporal evolutionary states, respectively. HDGC imposes the diffusion process into graph convolution and provides an efficient way to aggregate information from a wider range of neighbors. HDCC ensures that the internal order of causal correlations is not violated by applying the convolution filter only to previous data. Extensive experiments on diverse real-world datasets prove the superiority of HGWaveNet, and the ablation study further verifies the effectiveness of each component of our model. In future work, we will further generalize our model for more downstream tasks and try to use hyperbolic GNNs to capture more complex semantic information in heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Notations</head><p>Part of notations in our paper are summarized in Table <ref type="table" target="#tab_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experimental Details</head><p>For all baselines, we take the recommended parameter settings unless otherwise noted. For our proposed HGWaveNet, the parameters are set as follows: the truncated diffusion step ğ¾ = 2, the layer number of HDGC ğ¿ = 2, the dilation depth for HDCC ğ· = 3, kernel size ğ‘† = 2, the layer number for gated HDCC is valued as 4, and the hyper-parameters ğ‘Ÿ, ğ‘  in Fermi-Dirac decoder are taken to be 2 and 1 separately. All involved curvatures are initialized as 1 and keep trainable during the training process. For fairness, the representation dimension is set to 16 for all methods. All experiments are run on a machine with 2 Ã— Intel Xeon Gold 6226R 16C 2.90 GHz CPUs, 4 Ã— GeForce RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Supplementary Experiments</head><p>In Table <ref type="table" target="#tab_5">6</ref>, 7 and 8, we give out the supplementary experimental results for parameter analysis in Section 5.4. All these experiments are evaluated by AUC score on temporal link prediction task.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The degree distribution of real-world network MovieLens. Coordinate axes are logarithmic. (b) Areas of circles on hyperbolic spaces (curvature ğ‘ = -1) and Euclidean spaces, which shows that the hyperbolic spaces expand exponentially while Euclidean spaces expand polynomially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A hyperbolic manifold H and the corresponding PoincarÃ© ball model B. The distances on the manifolds are measured along geodesics.</figDesc><graphic coords="3,342.06,73.23,186.07,119.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A hyperbolic diffusion graph convolution (HDGC) module with the number of layers ğ¿ = 2 and the number of diffusion steps ğ¾ = 2. The part in red dashed rectangle is a complete HDGC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : 2 .</head><label>42</label><figDesc>Figure 4: A stacked hyperbolic dilated causal convolution (HDCC) module with dilated depth ğ· = 3 and kernel size ğ‘† = 2. Each circle refers to one node at a timestamp. Under the exponentially large receptive field, each node preserves the temporal causal correlations with the past ğ‘† ğ· snapshots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The overall framework of HGWaveNet. For G ğ‘¡ -1 , the historical evolutionary information handled by gated HDCCs and the spatial topological information handled by HDGC are fed into an HGRU module. With the constraint of HTC, the hyperbolic outputs are decoded by a Fermi-Dirac decoder and the temporal link prediction for G ğ‘¡ is made.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The influence of representation dimension on two datasets FB and MovieLens for HGWaveNet and GRUGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The influence of truncated diffusion step ğ¾ (left) and dilated depth ğ· (right) on MovieLens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="3">Enron DBLP AS733</cell><cell>FB</cell><cell cols="2">HepPh MovieLens</cell></row><row><cell># Nodes</cell><cell>184</cell><cell>315</cell><cell>6,628</cell><cell>45,435</cell><cell>15,330</cell><cell>9,746</cell></row><row><cell># Edges</cell><cell>790</cell><cell>943</cell><cell cols="3">13,512 180,011 976,097</cell><cell>997,837</cell></row><row><cell># Snapshots</cell><cell>11</cell><cell>10</cell><cell>30</cell><cell>36</cell><cell>36</cell><cell>11</cell></row><row><cell>Train : Test</cell><cell>8:3</cell><cell>7:3</cell><cell>20:10</cell><cell>33:3</cell><cell>30:6</cell><cell>8:3</cell></row><row><cell>ğ›¿</cell><cell>1.5</cell><cell>2.0</cell><cell>1.5</cell><cell>2.0</cell><cell>1.0</cell><cell>2.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Baselines.</figDesc><table><row><cell>Methods</cell><cell cols="2">Static/dynamic Manifolds</cell></row><row><cell>HGCN [4]</cell><cell>Static</cell><cell>Lorentz</cell></row><row><cell>HAT[56]</cell><cell>Static</cell><cell>PoincarÃ©</cell></row><row><cell>EvolveGCN [28]</cell><cell>Dynamic</cell><cell>Euclidean</cell></row><row><cell>GRUGCN [36]</cell><cell>Dynamic</cell><cell>Euclidean</cell></row><row><cell>TGN [31]</cell><cell>Dynamic</cell><cell>Euclidean</cell></row><row><cell>DySAT [34]</cell><cell>Dynamic</cell><cell>Euclidean</cell></row><row><cell>HTGN [52]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on temporal link prediction. Â± 0.09 93.55 Â± 0.06 89.16 Â± 0.16 91.63 Â± 0.22 96.34 Â± 0.05 93.28 Â± 0.11 86.11 Â± 0.13 83.74 Â± 0.15 90.64 Â± 0.07 88.98 Â± 0.09 84.89 Â± 0.40 77.98 Â± 0.66 HAT 94.49 Â± 0.11 94.63 Â± 0.08 89.29 Â± 0.18 90.15 Â± 0.14 96.09 Â± 0.01 93.21 Â± 0.02 84.02 Â± 0.09 83.03 Â± 0.15 90.52 Â± 0.04 89.53 Â± 0.04 86.26 Â± 0.14 84.19 Â± 0.21 EvolveGCN 90.12 Â± 0.69 92.71 Â± 0.34 83.88 Â± 0.53 87.53 Â± 0.22 92.47 Â± 0.04 95.28 Â± 0.01 76.85 Â± 0.85 80.87 Â± 0.64 76.82 Â± 1.46 81.18 Â± 0.89 55.41 Â± 1.55 69.41 Â± 1.00 GRUGCN 92.47 Â± 0.36 93.38 Â± 0.24 84.60 Â± 0.92 87.87 Â± 0.58 94.96 Â± 0.35 96.64 Â± 0.22 79.38 Â± 1.02 82.77 Â± 0.75 82.86 Â± 0.53 85.87 Â± 0.23 71.01 Â± 0.77 79.04 Â± 0.53 TGN 91.77 Â± 1.04 85.36 Â± 0.87 87.20 Â± 0.82 84.87 Â± 0.80 93.19 Â± 1.03 92.27 Â± 0.83 80.85 Â± 0.23 81.92 Â± 1.34 74.78 Â± 0.47 74.61 Â± 0.43 71.52 Â± 0.59 70.53 Â± 0.38 DySAT 93.06 Â± 0.97 93.06 Â± 1.05 87.25 Â± 1.70 90.40 Â± 1.47 95.06 Â± 0.21 96.72 Â± 0.12 76.88 Â± 0.08 80.39 Â± 0.14 81.02 Â± 0.25 84.47 Â± 0.23 70.21 Â± 0.58 78.41 Â± 0.33 HTGN 94.17 Â± 0.17 94.31 Â± 0.26 89.26 Â± 0.17 91.91 Â± 0.07 98.75 Â± 0.03 98.41 Â± 0.03 83.70 Â± 0.33 83.80 Â± 0.43 91.13 Â± 0.14 89.52 Â± 0.28 86.80 Â± 0.34 85.40 Â± 0.19 Ours HGWaveNet 96.86 Â± 0.08 97.04 Â± 0.07 89.96 Â± 0.27 92.12 Â± 0.18 98.78 Â± 0.01 98.53 Â± 0.02 89.51 Â± 0.28 86.88 Â± 0.29 92.37 Â± 0.04 91.48 Â± 0.05 91.90 Â± 0.06 90.65 Â± 0.12 HDCC 95.33 Â± 0.09 95.64 Â± 0.09 89.56 Â± 0.22 92.04 Â± 0.12 97.83 Â± 0.17 97.25 Â± 0.21 85.13 Â± 0.16 82.38 Â± 0.88 90.83 Â± 0.20 89.77 Â± 0.32 82.46 Â± 0.37 80.29 Â± 0.58 B 92.62 Â± 0.55 93.77 Â± 0.33 85.18 Â± 0.29 87.76 Â± 0.11 95.21 Â± 0.11 96.85 Â± 0.08 79.20 Â± 0.34 82.45 Â± 0.21 76.31 Â± 0.75 78.11 Â± 0.29 73.70 Â± 0.54 80.15 Â± 0.54 For evaluations on baselines and our model, the best results are in bold, and the suboptimal results are underlined. The same applies to Table</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Enron</cell><cell></cell><cell>DBLP</cell><cell></cell><cell>AS733</cell><cell></cell><cell>FB</cell><cell></cell><cell>HepPh</cell><cell></cell><cell cols="2">MovieLens</cell></row><row><cell></cell><cell>Metric</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell>Baselines</cell><cell cols="2">HGCN 93.96 Gain(%) +2.51</cell><cell>+2.55</cell><cell>+0.75</cell><cell>+0.23</cell><cell>+0.03</cell><cell>+0.12</cell><cell>+3.95</cell><cell>+3.68</cell><cell>+1.36</cell><cell>+2.18</cell><cell>+5.88</cell><cell>+6.15</cell></row><row><cell></cell><cell cols="13">w/o HDGC 94.95 Â± 0.10 95.24 Â± 0.07 89.51 Â± 0.24 91.84 Â± 0.17 97.97 Â± 0.20 97.35 Â± 0.24 85.99 Â± 0.29 82.77 Â± 0.48 90.61 Â± 0.22 89.68 Â± 0.03 84.35 Â± 0.12 82.77 Â± 0.38</cell></row><row><cell>Ablation</cell><cell>Gain(%) w/o Gain(%)</cell><cell>-1.97 -1.58</cell><cell>-1.85 -1.44</cell><cell>-0.50 -0.44</cell><cell>-0.30 -0.09</cell><cell>-0.82 -0.96</cell><cell>-1.20 -1.30</cell><cell>-3.93 -4.89</cell><cell>-4.73 -5.18</cell><cell>-1.91 -1.67</cell><cell>-1.97 -1.87</cell><cell>-8.22 -10.27</cell><cell>-8.69 -11.43</cell></row><row><cell></cell><cell>w/o Gain(%)</cell><cell>-4.38</cell><cell>-3.37</cell><cell>-5.31</cell><cell>-4.73</cell><cell>-3.61</cell><cell>-1.71</cell><cell>-11.52</cell><cell>-5.10</cell><cell>-17.39</cell><cell>-14.62</cell><cell>-19.80</cell><cell>-11.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>*  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance on temporal new link prediction. Â± 0.16 88.33 Â± 0.55 81.20 Â± 0.19 83.28 Â± 0.23 92.33 Â± 0.12 88.31 Â± 0.16 81.04 Â± 0.14 80.59 Â± 0.13 89.64 Â± 0.27 87.87 Â± 0.11 85.27 Â± 0.35 78.56 Â± 0.49 HAT 90.34 Â± 0.10 88.60 Â± 0.19 79.29 Â± 0.15 82.58 Â± 0.08 91.65 Â± 0.08 90.13 Â± 0.15 83.05 Â± 0.10 82.96 Â± 0.18 89.63 Â± 0.05 88.34 Â± 0.04 86.54 Â± 0.16 84.60 Â± 0.25 EvolveGCN 82.85 Â± 0.97 85.01 Â± 0.22 73.49 Â± 0.86 77.11 Â± 0.44 75.82 Â± 0.67 83.57 Â± 0.46 74.49 Â± 0.89 78.33 Â± 0.66 74.79 Â± 1.61 79.04 Â± 1.02 55.45 Â± 1.53 69.35 Â± 0.98 GRUGCN 87.59 Â± 0.57 88.41 Â± 0.45 75.60 Â± 1.60 78.55 Â± 1.05 83.14 Â± 1.21 88.14 Â± 0.76 77.69 Â± 1.03 81.07 Â± 0.77 81.97 Â± 0.49 84.78 Â± 0.22 71.32 Â± 0.77 79.51 Â± 0.50 TGN 86.75 Â± 0.59 81.37 Â± 0.42 81.83 Â± 1.08 81.61 Â± 0.62 87.89 Â± 0.63 84.94 Â± 0.94 78.93 Â± 0.98 76.57 Â± 0.63 74.21 Â± 0.78 76.12 Â± 0.54 72.22 Â± 0.68 70.90 Â± 0.40 DySAT 87.94 Â± 3.78 86.83 Â± 5.01 79.74 Â± 4.35 83.47 Â± 3.01 82.84 Â± 0.72 89.07 Â± 0.57 74.97 Â± 0.12 78.34 Â± 0.07 79.01 Â± 0.26 82.53 Â± 0.25 70.63 Â± 0.58 79.02 Â± 0.33 HTGN 91.26 Â± 0.27 90.62 Â± 0.34 81.74 Â± 0.56 84.06 Â± 0.41 96.62 Â± 0.22 95.52 Â± 0.25 82.21 Â± 0.41 81.70 Â± 0.46 90.11 Â± 0.14 88.18 Â± 0.31 87.06 Â± 0.25 85.82 Â± 0.29</figDesc><table><row><cell>Dataset</cell><cell>Enron</cell><cell></cell><cell>DBLP</cell><cell></cell><cell>AS733</cell><cell></cell><cell>FB</cell><cell></cell><cell>HepPh</cell><cell></cell><cell cols="2">MovieLens</cell></row><row><cell>Metric</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell>HGCN</cell><cell>90.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Notations.ğ‘˜-th step of HDGC at layer ğ‘™, timestamp ğ‘¡. H ğ‘¡ Temporal information earlier than timestamp ğ‘¡.</figDesc><table><row><cell>Symbols</cell><cell>Descriptions</cell></row><row><cell>G</cell><cell>Discrete dynamic graph represented by snapshots.</cell></row><row><cell>G ğ‘¡</cell><cell>Snapshot of G at timestamp ğ‘¡.</cell></row><row><cell>H ğ‘› ğ‘</cell><cell>ğ‘›-dimensional hyperbolic manifold with curvature -ğ‘.</cell></row><row><cell>ğ‘” ğ‘,H</cell><cell>Riemannian metric corresponding to the manifold H.</cell></row><row><cell>T x H ğ‘› ğ‘</cell><cell>Tangent space of H ğ‘› ğ‘ at point x.</cell></row><row><cell>B ğ‘› ğ‘</cell><cell>PoincarÃ© ball manifold corresponding to H ğ‘› ğ‘ .</cell></row><row><cell>ğ‘‘</cell><cell>Dimension of node representations.</cell></row><row><cell>ğ¾</cell><cell>Truncated diffusion steps in HDGC.</cell></row><row><cell>ğ¿</cell><cell>Layer number of HDGC.</cell></row><row><cell>ğ‘†</cell><cell>Convolution kernel size of HDCC.</cell></row><row><cell>ğ·</cell><cell>Dilated depth of HDCC.</cell></row><row><cell>ğ· â€²</cell><cell>Layer number of gated HDCC module.</cell></row><row><cell>A</cell><cell>Bi-direct adjacent matrix of a static graph or snapshot.</cell></row><row><cell>X ğ‘™ ğ‘¡,ğ‘˜</cell><cell></cell></row><row><cell>F</cell><cell>Kernel metrix of HDCC.</cell></row><row><cell>Z ğ‘¡</cell><cell>Node representations at timestamp ğ‘¡.</cell></row><row><cell>L ğ‘¡ HTC L ğ‘¡ CE</cell><cell>Hyperbolic temporal consistency at timestamp ğ‘¡. Cross-entropy like loss at timestamp ğ‘¡.</cell></row><row><cell>W, b</cell><cell>Trainable parameters.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The influence of representation dimension.</figDesc><table><row><cell cols="4">Dimension Enron DBLP AS733</cell><cell>FB</cell><cell cols="2">HepPh MovieLens</cell></row><row><cell>16</cell><cell>96.86</cell><cell>89.96</cell><cell>98.78</cell><cell>89.51</cell><cell>92.37</cell><cell>91.90</cell></row><row><cell>8</cell><cell>96.56</cell><cell>89.49</cell><cell>98.67</cell><cell>88.74</cell><cell>90.20</cell><cell>88.44</cell></row><row><cell>4</cell><cell>96.64</cell><cell>89.24</cell><cell>98.36</cell><cell>88.51</cell><cell>88.76</cell><cell>87.90</cell></row><row><cell>2</cell><cell>96.41</cell><cell>88.84</cell><cell>97.95</cell><cell>87.87</cell><cell>88.42</cell><cell>86.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The influence of truncated diffusion step ğ¾.</figDesc><table><row><cell cols="4">ğ¾ Enron DBLP AS733</cell><cell>FB</cell><cell cols="2">HepPh MovieLens</cell></row><row><cell>1</cell><cell>94.95</cell><cell>89.51</cell><cell>97.97</cell><cell>85.99</cell><cell>90.61</cell><cell>84.35</cell></row><row><cell>2</cell><cell>96.86</cell><cell>89.96</cell><cell>98.78</cell><cell>89.51</cell><cell>92.37</cell><cell>91.90</cell></row><row><cell>3</cell><cell>96.88</cell><cell>89.62</cell><cell>98.95</cell><cell>90.28</cell><cell>92.72</cell><cell>92.36</cell></row><row><cell>4</cell><cell>96.86</cell><cell>89.67</cell><cell>98.94</cell><cell>90.39</cell><cell>92.81</cell><cell>92.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The influence of dilated depth ğ·.</figDesc><table><row><cell cols="4">ğ· Enron DBLP AS733</cell><cell>FB</cell><cell cols="2">HepPh MovieLens</cell></row><row><cell>1</cell><cell>91.14</cell><cell>88.67</cell><cell>94.98</cell><cell>73.69</cell><cell>87.38</cell><cell>80.44</cell></row><row><cell>2</cell><cell>92.16</cell><cell>89.89</cell><cell>97.12</cell><cell>83.18</cell><cell>90.41</cell><cell>87.52</cell></row><row><cell>3</cell><cell>96.86</cell><cell>89.96</cell><cell>98.78</cell><cell>89.51</cell><cell>92.37</cell><cell>91.90</cell></row><row><cell>4</cell><cell>97.17</cell><cell>89.62</cell><cell>98.93</cell><cell>89.99</cell><cell>93.17</cell><cell>91.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>-x âŠ• ğ‘ y should be read as (-x) âŠ• ğ‘ y rather than -(x âŠ• ğ‘ y).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.cs.cornell.edu/~arb/data/email-Enron/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/VGraphRNN/VGRNN/tree/master/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://snap.stanford.edu/data/cit-HepPh.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://snap.stanford.edu/data/as-733.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://networkrepository.com/ia-facebook-wall-wosn-dir.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://grouplens.org/datasets/movielens/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by the <rs type="funder">Chinese Scientific and Technical Innovation</rs> Project <rs type="grantNumber">2030</rs> (<rs type="grantNumber">2018AAA0102100</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">U1936206</rs>, <rs type="grantNumber">62172237</rs>) and the <rs type="funder">Tianjin Natural Science Foundation for Distinguished Young Scholars</rs> (<rs type="grantNumber">22JCJQJC00150</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_z4HF8vk">
					<idno type="grant-number">2030</idno>
				</org>
				<org type="funding" xml:id="_FqcDJ2f">
					<idno type="grant-number">2018AAA0102100</idno>
				</org>
				<org type="funding" xml:id="_E6h2sNG">
					<idno type="grant-number">U1936206</idno>
				</org>
				<org type="funding" xml:id="_Ghe3Xz5">
					<idno type="grant-number">62172237</idno>
				</org>
				<org type="funding" xml:id="_zTvNvu4">
					<idno type="grant-number">22JCJQJC00150</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gain</surname></persName>
		</author>
		<idno>3.02 -2.82 -0.94 -0.39 -1.34 -1.68 -4.10 -5.15 -1.99 -2.15 -7.91 -7.99</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gain</surname></persName>
		</author>
		<idno>%) -6.83 -4.35 -9.03 -7.73 -12.52 -6.29 -12.67 -6.24 -18.83 -15.52 -19.48 -11.20</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simplicial closure and higher-order link prediction</title>
		<author>
			<persName><forename type="first">Rediet</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="11221" to="E11230" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network embedding and change modeling in dynamic heterogeneous networks</title>
		<author>
			<persName><forename type="first">Ranran</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Sing Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gillian</forename><surname>Dobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Divoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="861" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">E-lstm-d: A deep learning framework for dynamic network link prediction</title>
		<author>
			<persName><forename type="first">Jinyin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenbo</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3699" to="3712" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã‡aglar</forename><surname>GÃ¼lÃ§ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on temporal graphs</title>
		<author>
			<persName><surname>Da Xu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJeW1yHYwH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>evren korpeoglu, sushant kumar, and kannan achan</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hyperbolic-to-hyperbolic graph convolutional network</title>
		<author>
			<persName><forename type="first">Jindou</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="154" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>BÃ©cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperbolic groups, Essays in group theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MSRI Publ</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="75" to="264" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational graph recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm transactions on interactive intelligent systems (tiis)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal networks</title>
		<author>
			<persName><forename type="first">Petter</forename><surname>Holme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jari</forename><surname>SaramÃ¤ki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">519</biblScope>
			<biblScope unit="page" from="97" to="125" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal Heterogeneous Information Network Embedding</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1470" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Who you would like to share with? a study of share recommendation in social e-commerce</title>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojian</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Connectivity and inference problems for temporal networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-second annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-second annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Shima</forename><surname>Khoshraftar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aijun</forename><surname>An</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01855</idno>
		<title level="m">A Survey on Graph Representation Learning Methods</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MariÃ¡n</forename><surname>BogunÃ¡</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">dynnode2vec: Scalable dynamic network embedding</title>
		<author>
			<persName><forename type="first">Sedigheh</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shima</forename><surname>Khoshraftar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aijun</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on big data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3762" to="3765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A guide to temporal networks</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Lambiotte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous-time dynamic network embeddings</title>
		<author>
			<persName><forename type="first">Giang</forename><surname>Hoang Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion proceedings of the the web conference 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PoincarÃ© embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperbolic Deep Neural Networks: A Survey</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Varanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous-time link prediction via temporal dependent graph neural network</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaisheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3026" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal Graph Networks for Deep Learning on Dynamic Graphs</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The network data repository with interactive graph analytics and visualization</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4460" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dysat: Deep neural representation learning on dynamic graphs via self-attention networks</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on web search and data mining</title>
		<meeting>the 13th international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the scale-free nature of stock markets: Hyperbolic graph learning for algorithmic trading</title>
		<author>
			<persName><forename type="first">Ramit</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MichaÃ«l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on neural information processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hgcf: Hyperbolic graph convolution networks for collaborative filtering</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Zuberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hyperbolic variational graph neural network for modeling dynamic graphs</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongbao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4375" to="4383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable algorithms for data and network analysis</title>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and TrendsÂ® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="274" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dyrep: Learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjeet</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th ISCA Speech Synthesis Workshop</title>
		<meeting><address><addrLine>Sunnyvale, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">2016. September 2016</date>
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hyperbolic heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5337" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09845</idno>
		<title level="m">Time-Aware Neighbor Sampling for Temporal Graph Networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">J. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural tensor factorization for temporal interaction learning</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM international conference on web search and data mining</title>
		<meeting>the Twelfth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="537" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th International Joint Conference on Artificial Intelligence (IJCAI). International Joint Conferences on Artificial Intelligence Organization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Few-shot Link Prediction in Dynamic Networks</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xumeng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1245" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relation learning on social networks with multi-modal graph edge variational autoencoders</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiou</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="699" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">HICF: Hyperbolic Informative Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2212" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Discrete-time temporal network embedding via implicit hierarchical learning in hyperbolic space</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kalander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="1975">2021. 1975-1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">HRCF: Enhancing collaborative filtering via hyperbolic geometric regularization</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2462" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Comprehensive survey on dynamic graph models</title>
		<author>
			<persName><forename type="first">Aya</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doaa</forename><surname>Hegazy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safaa</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Where are we in embedding spaces</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hyperbolic graph attention network</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Fanny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lorentzian graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1249" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">T-gcn: A temporal graph convolutional network for traffic prediction</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3848" to="3858" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic network embedding by modeling triadic closure process</title>
		<author>
			<persName><forename type="first">Lekui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Embedding temporal network via neighborhood formation</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2857" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
