<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-07-30">30 Jul 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yan</forename><surname>Jin</surname></persName>
							<email>y.jin@qub.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Ding</surname></persName>
							<email>dingfei@hnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chongfeng</forename><surname>Wei</surname></persName>
							<email>c.wei@qub.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical and Aerospace Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Advanced Design and Manufacturing Technology for Vehicle</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Belfast Belfast</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Mechanical and Ve- hicle Engineering</orgName>
								<orgName type="institution">Hunan University</orgName>
								<address>
									<postCode>410082</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution" key="instit1">James Watt</orgName>
								<orgName type="institution" key="instit2">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-07-30">30 Jul 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2507.23080v1[cs.MA]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal Disentanglement Representation Learning</term>
					<term>Reinforcement Learning</term>
					<term>Graph Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the advent of autonomous driving technology, it has experienced remarkable progress over the last decade. However, most existing research still struggles to address the challenges posed by environments where multiple vehicles have to interact seamlessly. This study aims to integrate causal learning with reinforcement learning-based methods by leveraging causal disentanglement representation learning (CDRL) to identify and extract causal features that influence optimal decision-making in autonomous vehicles. These features are then incorporated into graph neural network-based reinforcement learning algorithms to enhance decision-making in complex traffic scenarios. By using causal features as inputs, the proposed approach enables the optimization of vehicle behavior at an unsignalized intersection. Experimental results demonstrate that our proposed method achieves the highest average reward during training and our approach significantly outperforms other learning-based methods in several key metrics such as collision rate and average cumulative reward during testing. This study provides a promising direction for advancing multi-agent autonomous driving systems and make autonomous vehicles' navigation safer and more efficient in complex traffic environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>information from neighboring agents, resulting in data inefficiency and limited scalability <ref type="bibr" target="#b0">[1]</ref>. This limitation arises these methods heavily rely on the characteristics of the training data, which often fail to encompass the full variability encountered in diverse operational scenarios. <ref type="bibr" target="#b1">[2]</ref>. Secondly, GRL algorithms typically construct relationships between vehicles based on observed correlations <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Since GNN excels at aggregating and propagating information across graph-structured data by leveraging node features and graph connectivity, they primarily rely on correlation-based relationships <ref type="bibr" target="#b5">[6]</ref>. Therefore, incorporating CDRL into GRL offers a promising solution to address these above-mentioned challenges. By leveraging CDRL, the model is able to disentangle and identify underlying causal factors that govern interactions among vehicles, rather than relying solely on observed correlations. This supports the extraction of features that more accurately model the causal influence of surrounding agents on autonomous decision-making, thereby mitigating data inefficiency, promoting scalability, and enabling autonomous vehicles to learn effectively from fewer interactions <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Currently, there is no method that effectively combining RL-based algorithms with causal learning algorithms to improve the efficiency and quality of policy generation in multi-agent environments <ref type="bibr" target="#b8">[9]</ref>. Even though GRL excels at modeling interactions among agents and causal models uncover underlying causal structures, the two models have not unified into a single framework to guide autonomous vehicles in generating more optimal policies in complex multi-agent traffic scenarios. We model the decision-making problem in a multi-agent traffic environment and propose a causality-inspired graph reinforcement learning (CGRL) framework. Our method seeks to learn causally disentangled representations within the Variational Graph Auto-Encoders (VGAE) framework <ref type="bibr" target="#b9">[10]</ref>. Specifically, we use CDRL to identify and separate causal features from observed data, which are then fed into the designed GRL algorithm to enhance decision-making performance. To achieve this, we use a GNN-based encoder in VGAE to learn latent representations. Additionally, using information-theoretic methods to extract causal features in the latent space that influence autonomous vehicle decision-making. This process enables the separation of invariant (causal) features from variant (non-causal) ones, based on the premise that invariant representations correspond to causal factors directly related to correct decision-making. Precisely, the contributions of this paper are as follows: 1. We propose a CDRL method that can be effectively applied into the VGAE framework. 2. This research introduces an innovative multi-agent decision-making framework CGRL that incorporates CDRL into GRL. This framework discerns the causal features which can causally influence the optimal decision-making of autonomous vehicles. 3. The innovative CGRL algorithm is implemented and validated in a simulator. Our approach achieves superior performance in the unsignalized intersection scenario, as demonstrated by the results. The following section are organized as follows: Section II provides a review of related work on Graph Reinforcement Learning, Causal Reinforcement Learning algorithms, and Causal Disentanglement Representation Learning. Section III defines an unsignalized intersection scenario and assigns brief identifiers to two kinds of vehicle. Section IV introduces our developed CGRL algorithm employed in this study. Section V demonstrates the establishment of experimental scenarios and introduces implementation details. Section VI analyzes experimental results and provides further discussion. Section VII presents the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Graph Reinforcement Learning</head><p>Deep reinforcement learning (DRL) has shown great promise in the domain of autonomous driving. However, as the number of agents increases, their interactions may grow exponentially in complexity. Consequently, DRL-based approaches in multi-agent systems still face several fundamental challenges. Recently, the integration of GNN with DRL, particularly in multi-agent systems, has gained significant attention in graph-structured environments. GNN are inherently designed to capture topological relationships, making them well-suited for learning the interactions between vehicles in a graph. Additionally, GNN excels at capturing multi-agent relationships compared to other approaches <ref type="bibr" target="#b10">[11]</ref>. Therefore, the combination of DRL and GNN allows for optimizing complex problems while generalizing effectively to unseen topologies <ref type="bibr" target="#b0">[1]</ref>. Recently, a graph convolution-based DRL algorithm was presented that combines Graph Convolution Network (GCN) with Deep Q-Networks (DQN) to achieve improved decisionmaking in graph-structured environments, such as highway ramping, lane-changing scenarios <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> and intersection <ref type="bibr" target="#b15">[16]</ref>. The rise of Graph Attention Network (GAT) is primarily attributed to their attention mechanism, which dynamically assigns importance weights to neighboring nodes rather than relying on fixed aggregation rules. Therefore, some studies proposed GAT-based DRL algorithms <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Despite its greater potential in multi-agent environments, the combination of GNN and DRL still struggles with sample inefficiency <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Causal Reinforcement Learning</head><p>Over the past few decades, both causality and reinforcement learning have made significant theoretical and technical advancements independently. However, these two fields have yet to be fully reconciled and integrated. Combining causality with reinforcement learning has the potential to enhance generalization capabilities and improve the sample efficiency of reinforcement learning models. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Currently, causal reinforcement learning can be broadly classified into two categories. The first category relies on prior causal information, where methods typically assume that the causal structure of the environment or task is provided in advance by experts. The second category, on the other hand, deals with unknown causal information, where the relevant causal relationships must be learned through interaction with the environment to inform the policy. In the context of causal reinforcement learning with given prior causal information, Méndez-Molina et al., <ref type="bibr" target="#b19">[20]</ref> integrated causal knowledge into Q-learning to enhance the agent's ability to learn effectively from its environment. By leveraging this prior knowledge, agents can make informed decisions in complex tasks. In <ref type="bibr" target="#b20">[21]</ref>, the study emphasizes the advantages of incorporating prior causal knowledge to guide the learning process in reinforcement learning settings, showing that this approach can improve performance in a comparison of traditional methods that do not account for causal relationships. However, the complex traffic environment is highly dynamic and involves unpredictable factors, such as road conditions and interactions with other vehicles, making it difficult to fully capture these variables with predefined causal structures. A causal reinforcement learning approach based on unknown causal information can be applied to such traffic scenarios. The authors proposed a runtime method called Counterfactual Behavior Policy Evaluation which focuses on the application of counterfactual reasoning within the context of autonomous driving to enhance decision-making processes <ref type="bibr" target="#b21">[22]</ref>. This study introduces a novel framework called Causality-driven Hierarchical RL aimed at improving the discovery of hierarchical structures in RL tasks, particularly in complex environments <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Causal Disentanglement Representation Learning</head><p>Currently, capturing the causal structure of relevant variables and extracting causal information to train RL models is a challenging task. A traditional method for discovering causal relationships relies on interventions or randomized experiments, which are often too expensive, time-consuming, or even impractical to carry out in many cases. As a result, causal discovery through the analysis of purely observational data has garnered significant attention <ref type="bibr" target="#b23">[24]</ref>. However, current causal discovery methods typically assume that units in the system are random variables and these variables are connected through a causal graph. In real-world complex traffic scenarios, observational data are not always naturally divided into such independent causal units, and the causal discovery approaches are incapable of dealing with high dimensional and complex data <ref type="bibr" target="#b24">[25]</ref>. To address the issues mentioned in causal discovery, CDRL is proposed. This approach focuses on identifying and isolating independent causal factors within the data <ref type="bibr" target="#b25">[26]</ref>. In the context of supervised CDRL, a framework called ICM-VAE was proposed to learn causally disentangled representations by utilizing causally related observed labels. This enables the model to leverage supervision from known causal relationships within the data. The authors <ref type="bibr" target="#b26">[27]</ref> introduced a novel method called Disentangled Generative Causal Representation aimed at learning disentangled representations that account for causal relationships among latent variables. In addition, the approach employs weakly supervised information, meaning that it utilizes some level of supervision regarding ground-truth factors and their causal structure without requiring fully labeled datasets. This allows the model to learn effectively even with limited supervision. For unsupervised manner, a VAE-based CDRL framework was proposed by <ref type="bibr" target="#b27">[28]</ref> introducing a novel Structural Causal Model (SCM) layer, enabling the recovery of latent factors with semantic meaning and structural relationships through a causal directed acyclic graph (DAG). The authors proposed a Concept-free Causal VGAE model by incorporating a novel causal disentanglement layer into VGAE, and it presents a significant advancement in causal inference methodology to achieve concept-free causal disentanglement, thereby enhancing our ability to analyze complex systems with minimal prior assumptions <ref type="bibr" target="#b28">[29]</ref>. The authors proposed a CaDeT approach, which significantly advances trajectory prediction for autonomous driving by incorporating causal reasoning into the learning process. The model enhances its predictive reliability in challenging and dynamic conditions by disentangling causal features and spurious features and utilizing a targeted intervention mechanism <ref type="bibr" target="#b29">[30]</ref>. This study addresses the decision-making issue in autonomous driving at an unsignalized intersection, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The scenario involves a single autonomous ego vehicle (AEV) interacting with surrounding HVs. We demonstrate that the sequential decision-making process of the AEV can be effectively modeled as a Markov Decision Making (MDP). MDP stands as a pivotal mathematical construct for modeling complex decision-making scenarios. This framework performs well in environments where outcomes are influenced by both stochastic elements and deliberate actions taken by an agent, and it is widely employed in the field of autonomous driving to model various sequential decision-making problems. Furthermore, MDP is typically defined as a four-element tuple ⟨S av , A av , R av , P av ⟩. At each time step t, an autonomous vehicle interacts with their surrounding traffic environment and takes an action a t based on the current state s t . Its action leads to a transition to the next state s t+1 according to transition probabilities P av (s t+1 |s t , a t ), resulting in a reward r t . In our study, the number of HVs is predefined, with each appearing at random starting positions and assigned different destinations. In contrast, the AEV has fixed starting and destination points. We model the scenario as a graph, with nodes representing vehicles and edges capturing their pairwise interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CAUSALITY-INSPIRED GRAPH REINFORCEMENT LEARNING</head><p>In this section, we propose a novel multi-agent decisionmaking framework, CGRL, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. CGRL integrates CDRL with GRL to model the decision-making process of the AEV in multi-agent interactions at an unsignalized intersection scenario. Specifically, we leverage CDRL within VGAE to extract causal features from complex graphstructured data. These features are then input into the GRL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning Implementation</head><p>In our experimental scenario, we primarily utilize an undirected and unweighted graph G = (V, E) to model the interactions among vehicles, and in this graph, each vehicle is represented as a node, while the interaction between each pair of nodes is represented by an undirected edge. Here, V = {v i , ...v N } represents the set of vehicles present in the current state. E is the set of edges between vertices in the graph. An undirected edge is present between two vehicles if they are within a certain predefined distance from each other, specifically when the relative x-distance is less than 10 meters and the relative y-distance is less than 30 meters. The state of the AEV S av = [F, A] consists of the feature matrix F ∈ R N ×d and the adjacency matrix A ∈ R N ×N . F captures the attributes of each surrounding vehicle, while A represents the interactions between vehicles within a predefined distance of each other.</p><p>1) Feature Matrix: The feature matrix is composed of the attribute of each vehicle in the simulation, and the rows of F represent the number of vehicles, while the columns correspond to the feature dimensions for each vehicle. To distinguish the AEV from other vehicles, the first row of the features matrix is designated to represent the features of the AEV.</p><formula xml:id="formula_0">F = [e i , x i , y i , v i x , v i y , cos i h , sin i h ]<label>(1)</label></formula><p>Where e i denotes the presence or absence of the i th vehicle, being set to 1 if the vehicle exists and 0 otherwise. x i denotes the position of the i th vehicle on the x-axis; y i denotes the position of the i th vehicle on the y-axis. v i x denotes the i th velocity component in the x-direction. v i y denotes the i th velocity component in the y-direction; (cos i h , sin i h ) denotes the trigonometric components of the i th vehicle's heading angle, where cos i h and sin i h correspond to the cosine and sine of its heading angle, respectively. 2) Adjacency Matrix: The adjacency matrix is a square matrix that represents vehicle interactions. Each element e ij in the matrix indicates whether a pair of vehicles i and j is adjacent. If vehicles i and j are within the specified range and interacting, e ij = 1; otherwise, if there is no interaction or proximity, e ij = 0.</p><formula xml:id="formula_1">A =           e 11 e 12 • • • • • • e 1n e 21 e 22 • • • • • • e 2n . . . . . . . . . . . . e ij . . . . . . . . . e n1 e n2 • • • • • • e nn          <label>(2)</label></formula><p>3) Action Space (A av ): During interactions, the AEV operates within a discrete action space defined in our scenario. This action space allows it to perform three primary operations: accelerate, decelerate, or maintain a constant speed, corresponding to specific adjustments in its throttle and brake controls. At each time step, the AEV can choose one action from this space to navigate the intersection effectively.</p><formula xml:id="formula_2">A av = {constant, accelerated, decelerated}<label>(3)</label></formula><p>4) Reward Function (R av ): Each state-action pair is associated with a numerical reward signal. This reward R av represents the immediate benefit or cost of taking a specific action in a particular state. The AEV is designed with the goal of maximizing long-term cumulative rewards.</p><formula xml:id="formula_3">R av = ω or t * r or t * (ω c t * r c t + ω hs t * r hs t + ω tc t * r tc t ) (4)</formula><p>where r c t , r hs t , r or t , and r tc t represent a collision penalty term, a high speed reward term, a staying on road reward term, and a task completion term, respectively. The weights ω correspond to their respective rewards, where ω c t , ω hs t , ω or t and ω tc t are 1 predefined as respectively.</p><p>• The component r c t is a reward signal related to collisions, indicating whether the vehicle has collided with another object or vehicle, This reward is penalizing to discourage the agent from engaging in unsafe behaviors that lead to collisions. Therefore, it is a critical component for teaching the AEV to drive safely and avoid accidents.</p><formula xml:id="formula_4">r c t = -2, if collision 0, otherwise.<label>(5)</label></formula><p>• The component r hs t is a reward signal that encourages the vehicle to maintain a high speed within a desired range. Moreover, this reward incentivizes the AEV to drive efficiently and reach its destination quickly, while still adhering to speed limits or safety constraints.</p><formula xml:id="formula_5">r hs t = y 0 + (v -x 0 ) • (y 1 -y 0 ) x 1 -x 0<label>(6)</label></formula><p>Where v refers to the current velocity where the AEV has reached, The speed of the vehicle can be mapped from [x 0 , x 1 ] to [y 0 , y 1 ], which is then used to calculate the high-speed reward. • The component r or t is a binary reward signal that indicates whether the vehicle is on the road. This reward encourages the agent to stay on the road, which is a fundamental requirement for safe and effective driving. This reward is crucial for ensuring that the AEV learns to follow the road and avoid dangerous situations.</p><formula xml:id="formula_6">r or t = 1, if on road 0, otherwise.<label>(7)</label></formula><p>• The component r tc t is the reward to encourage efficient task completion, when the vehicle successfully arrives the target location. As such, It serves as a strong incentive for the AEV to complete its mission successfully.</p><formula xml:id="formula_7">r tc t = 1, if arrive the destination 0, otherwise.<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Neural Network Module</head><p>We integrate the GAT with the GCN to process graphstructured data and use the dueling network as the GRL policy network. The absence of the GCN impairs the policy network's ability to capture the chain reactions inherent in complex multi-agent interaction scenarios. Likewise, without the GAT, the network is unable to effectively prioritize the relative influence of each vehicle's interactions. Firstly, we mainly use two-layer GCN GCN2, which apply convolutional operations on graph-structure data. The equation is defined as:</p><formula xml:id="formula_8">x (l+1) = (1 -α) Ãx (l) + αx (0) ((1 -β l )I + β l W) (9)</formula><p>Where Ã = D -1/2 ÂD -1/2 is the symmetrically normalized adjacency matrix, Â = A + I is the adjacency matrix with self-loops, D ii = j Âij is diagonal degree matrix. x (0) is the initial node features, x (l) is the feature of layer l. α is the fraction of initial node features, and β l is the hyperparameter to tune the strength of identity mapping. It is defined by</p><formula xml:id="formula_9">β l = log( λ l + 1) ≈ λ l</formula><p>, where λ is a tunable hyperparameter. β ensures that the decay of the weight matrix progressively and adaptively intensifies as the network depth increases.</p><p>After the GCN2 layers, input processed data into two-layer GATv2 <ref type="bibr" target="#b30">[31]</ref>, which perform attention-based aggregation of node features.</p><formula xml:id="formula_10">α ij = exp(a ⊤ σ((W [x i ∥ x j ])) k∈N (i) exp(a ⊤ σ (W [x i ∥ x k ]) ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where || is the concatenation operation; W is a learnable weight vector; σ(•) is the LeakyReLU activation function. α ij quantifies the significance of node j's features for node i.</p><p>Use the attention weights α ij to aggregate the features of neighboring nodes and update the representation of node i</p><formula xml:id="formula_12">x ′ i = σ   j∈N (i) α ij Wx j  <label>(11)</label></formula><p>where σ(•) is applied component-wise, The set N i includes node i along with its neighboring nodes.</p><p>In order to enhance the expressive power of the model, GATv2 utilizes a multi-head attention mechanism, concatenating the results from M attention heads.</p><formula xml:id="formula_13">x ′′ i = M m=1 σ   j∈N (i) α (m) ij W (m) x j  <label>(12)</label></formula><p>Where α</p><formula xml:id="formula_14">(m) ij</formula><p>is the weight of the m-th attention head. W (m) is the weight matrix of the m-th attention head. ∥ Represents the concatenation of outputs from different heads. Capturing global features that aggregate information from all nodes is essential. After processing through the GATv2 layers, global average pooling is applied to compute the mean feature representation across all nodes in the graph.</p><formula xml:id="formula_15">h = 1 N N i=1 x ′′ i<label>(13)</label></formula><p>Finally, the global features are mapped to the hidden layer through two fully connected layers, and the transformed global features are used as input for the dueling layer to calculate the Q-value. In the dueling architecture, the state value estimation and advantage calculation are processed through separate pathways, both utilizing shared features extracted by a common backbone network. The Q-value formula is given by:</p><formula xml:id="formula_16">Q(s, a) = V ′ (s) + A ′ (s, a) - 1 |A av | a ′ A ′ (s, a ′ )<label>(14)</label></formula><p>Where V ′ (s) is the state value computed by the value network. A ′ (s, a) is the advantage for action a at state s.</p><p>1 |Aav| a ′ A ′ (s, a ′ ) normalizes the advantages across all possible actions to ensure that the advantage reflects the relative improvement of action a over others. |A av | is the number of possible actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Reinforcement Learning Module</head><p>The deep reinforcement learning component of our GRL architecture is implemented using the Dueling Double Deep Q-Network (D3QN) algorithm <ref type="bibr" target="#b31">[32]</ref>, combining it with GNN module to model the decision-making process in autonomous driving. Firstly, D3QN can enhance the original DQN by integrating the principles of Double Q-learning, reducing the overestimation bias inherent in DQN while maintaining its ability to handle reinforcement learning problems in highdimensional state and action spaces. Secondly, similar to DQN, D3QN uses a deep neural network Q(s, a; θ) to approximate the state-action values, where θ is the parameter of the network. D3QN modifies the temporal difference learning process by decoupling action selection and evaluation, which helps mitigate overestimation bias. Originally, the optimal actionvalue function in D3QN is updated using the following rule:</p><formula xml:id="formula_17">Q(s t , a t ) ← Q(s t , a t )+α•[r t + γQ target (s t+1 , a max ) -Q(s t , a t )]<label>(15</label></formula><p>) where a max = arg max a Q online (s t+1 , a; θ) is determined by the online network, and the value is evaluated using the target network Q target (s t+1 , a max ; θ -).</p><p>The true target values of D3QN in our algorithm can be expressed as:</p><formula xml:id="formula_18">y = R av +γQ target (S ′ av , arg max a∈Aav(S ′ av ) Q online (S ′ av , a; θ); θ -)<label>(16</label></formula><p>) Here, θ represents the trainable parameters of the online network, and θ -corresponds to the fixed parameters of the target network.</p><p>The loss function is designed to minimize the temporal difference error while leveraging the decoupled action selection and evaluation to improve stability:</p><formula xml:id="formula_19">L = E (y -Q online (S av , A av ; θ)) 2<label>(17)</label></formula><p>Gradient descent is used to iteratively update the parameters θ of the online network in the direction that decreases the loss function. The loss function gradients with respect to the network parameters are derived as follows:</p><formula xml:id="formula_20">∇ θ L = E [(y -Q online (S av , A av ; θ)) ∇ θ Q online (S av , A av ; θ)]<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Causal Disentangled Representation Learning with VGAE</head><p>In this subsection, we employ the CDRL method within the VGAE framework. Unlike traditional causal discovery methods, CDRL is an emerging field of research that seeks to address the challenge of extracting causal features from complex, high-dimensional data.</p><p>1) Causal Graphical Model: We utilize a directed acyclic graph as a Causal Graphical Model to describe the causal relationships among multiple variables and construct a Structural Causal Model (SCM). This includes graph data G, two isolated features derived from the hidden space in VGAE: causal features Z c and spurious features Z s , as well as the optimal decision-making of the autonomous vehicle A * av , where the cause-effect relationship is listed as</p><formula xml:id="formula_21">Z c ← G → Z s → A * av .</formula><p>Below is a detailed explanation of the components of this causal graphical model: 2) Graph-based Generative Model: After constructing a Causal Graphical Model, we use VGAE to map abovementioned high-dimensional graph-structured data, including the feature and adjacency matrices, into a low-dimensional latent space to generate low-dimensional feature matrix Z ∈ R N ×(L) . Additionally, applying CDRL within VGAE to extract the causal features, which serve as inputs for the GRL model. Our VGAE model comprises two main components: an encoder and a decoder. We employ a two-layer GCN as the encoder for the VGAE. The encoder processes the graph data and outputs the latent variable Z, which captures the underlying representations of the graph structure and feature features. The first GCN layer generates a lower-dimensional feature matrix. It is defined as:</p><formula xml:id="formula_22">• Z c ← G → Z s .</formula><formula xml:id="formula_23">F = GCN(F, A) = ReLU( ÃF W 0 )<label>(19)</label></formula><p>Where Ã = D -1 2 AD -1 2 is the symmetrically normalized adjacency matrix, while D is degree matrix of A. F is feature Outputs after applying the first GCN layer. W 0 is Learnable weight matrix of the first GCN layer, mapping input features to a lower-dimensional space. The second GCN layer generates µ and σ, as in the following equations:</p><formula xml:id="formula_24">µ = GCN µ (F, A) = Ã F W 1 log(σ 2 ) = GCN σ (F, A) = Ã F W 1 (<label>20</label></formula><formula xml:id="formula_25">)</formula><p>Where µ is the mean vector matrix, σ is the variance matrix, log σ and µ share the weight W 1 . Then we can obtain latent vector Z.</p><formula xml:id="formula_26">q(Z | F, A) = N i=1 q(z i | F, A) q(z i | F, A) = N (z i | µ i , diag(σ 2 i ))<label>(21)</label></formula><p>Where Z ∈ R N ×L be the latent representation, with N denoting the number of vehicles and L specifying the dimensionality of the latent representation. Additionally, the latent representation for the i th vehicle v i is denoted as z i .</p><p>In VGAE, we can utilize a multi-layer perception paired with an inner product decoder. Its generative model can be formulated as:</p><formula xml:id="formula_27">p(A | Z) = N i=1 N j=1 p(a ij | z i , z j ) p(a ij = 1 | z i , z j ) = σ(z ⊤ i z j )<label>(22)</label></formula><p>The overall objective of VGAE is including optimizing the evidence lower bound (ELBO) of VAE while simultaneously minimizing the total correlation, which can be formulated as</p><formula xml:id="formula_28">L VGAE = E q(Z|F,A) [logp(A|Z)] -KL[q(Z|F, A)||p(Z)]<label>(23)</label></formula><p>Where KL is the Kullback-Leibler divergence.</p><p>3) Causal Filter: After generate low-dimensional feature matrix Z, the challenge is how to identify and extract causal features. To address this challenge, we employ state-of-theart information-theoretic frameworks to quantify and optimize causal relationships within the system <ref type="bibr" target="#b32">[33]</ref>, and informationtheoretics have been utilized in GNN <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. To find causal features Z c , we first ensure that Z c and Z s are independent, Subsequently, causal intervention techniques are applied to maximize the causal influence between Z c and A * av . This approach effectively removes the spurious features, uncovering the true causal relationship and extracting the natural causal features Z c . Mutual information (MI) I (Z c ; Z s ) is used to ensure whether the causal features Z c and the spurious feature features Z s are independent, as MI measures the statistical dependence between two random variables between Z c and Z s , such as</p><formula xml:id="formula_29">I (Z c ; Z s ) = 0 means Z c is independent of Z s .</formula><p>The estimation method of MI is originally defined as:</p><formula xml:id="formula_30">I(Z c ; Z s ) = E P (Zc,Zs) log P (Z c , Z s ) P (Z c )P (Z s )<label>(24)</label></formula><p>However, the method based on probability distribution requires knowledge of the joint distribution P(Z c ,Z s ) and marginal distributions P(Z c ) and P(Z s ), which are often unavailable in practice, and direct estimation of MI is difficult for highdimensional variables, where exact probability distributions are unknown.</p><p>Other methods use entropic graph techniques have been developed that eliminate the need for explicit distribution estimation. include the k-nearest-neighbour <ref type="bibr" target="#b35">[36]</ref> and the MI Neural Estimators <ref type="bibr" target="#b36">[37]</ref>. However, the MI Neural Estimator presents several challenges. The first one is instability during training, joint optimization of the neural network can lead to issues like unstable convergence or poor local minima, and the logarithmic terms and exponentiation can result in numerical instability. The second one is negative MI values, In some cases, especially when the model is undertrained, the estimator may output negative MI values, which are not valid since MI is always non-negative. The third one is high computational cost, training the neural network can be computationally expensive, particularly for large datasets or high-dimensional inputs.</p><p>For the k-nearest-neighbour estimator, its non-differentiability prevents the use of traditional optimization algorithms, such as gradient descent, which rely on the differentiability of the objective function for parameter adjustment. Therefore, Sánchez Giraldo et al. <ref type="bibr" target="#b37">[38]</ref> introduced a matrixbased functional to compute Rényi α-order entropy, characterizing both entropy and MI through the normalized eigenvalues of the Gram matrix. This Gram matrix, a Hermitian operator, is derived by mapping the data into a reproducing kernel Hilbert space. This approach is more effective for MI estimation, as matrix-based methods are naturally suited for high-dimensional data and do not rely on neural network optimization. This avoids issues such as unstable training, sensitivity to hyperparameters, and ensures reproducible results. Additionally, it directly computes MI from the eigenspectrum of similarity matrices, bypassing the need for joint and marginal distribution estimation. Therefore, this method is especially useful in scenarios where stability and computational efficiency are crucial. I (Z c ; Z s ) can be represented as:</p><formula xml:id="formula_31">I α (Z c ; Z s ) = S α (Z c ) + S α (Z s ) -S α (Z c , Z s )<label>(25)</label></formula><p>The α-order Rényi entropy for one matrix Z c or Z s is defined as:</p><formula xml:id="formula_32">S α (Z c ) = 1 1 -α log 2 tr((Z c ) α ) = 1 1 -α log 2 n i=1 λ i (Z c ) α<label>(26)</label></formula><p>where λ i (Z c ) are the eigenvalues of the matrix Z c , and tr((Z c ) α ) is the trace of (Z c ) α . The parameter α is used to define different orders of entropy.</p><p>For two matrices Z c and Z s , the joint Rényi entropy can be defined similarly:</p><formula xml:id="formula_33">S α (Z c , Z s ) = 1 1 -α log 2 tr(D α )<label>(27)</label></formula><p>Where D = Z c ⊗ Z s denotes the Kronecker product of matrices Z c and Z s . Due to the incorrect decision-making A * av based on spurious features Z s instead of causal features Z c , it is crucial to eliminate the backdoor path. To identify direct causal connections between Z c and A * av . we can apply the do-calculus on the variable Z s to block the backdoor path. Accordingly, we propose an intervention mechanism that leverages a conditional MI method to quantify causal influence and identify the causal relationship by intervening on Z s .</p><formula xml:id="formula_34">I (Z c → A * av | do (Z s )) = I (Z c ; A * av |Z s )<label>(28)</label></formula><p>We also use the Matrix-based Rényi entropy method to directly estimate the conditional MI.</p><formula xml:id="formula_35">I α (Z c ; A * av |Z s ) = S α (Z c |Z s ) + S α (A * av |Z s ) -S α (Z c , A * av |Z s ) = S α (Z c , Z s ) + S α (A * av |Z s ) -S α (Z s ) -S α (Z c , A * av , Z s )<label>(29)</label></formula><p>The joint Rényi entropy for three variables can be defined as:</p><formula xml:id="formula_36">S α (Z c , Z s , A * av ) = 1 1 -α log 2 tr((K 3 ) α )<label>(30)</label></formula><p>Where K 3 where is the Gram matrix constructed from the joint distribution of Z c , Z s and A * av . This step involves using a decoder to map the extracted causal features to a causal adjacency matrix, which then serves as the state input for training the CGRL decision-making algorithm.</p><formula xml:id="formula_37">A c = σ(Z c Z T c )<label>(31)</label></formula><p>4) Objective function: To obtain causal features Z c from the latent feature matrix Z, the overall loss function can be defined based on <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_38">min -I α (Z c ; A * av |Z s ) + I α (Z c ; Z s ) + λ 1 L VGAE + λ 2 ∥Ac∥1 ∥A∥1<label>(32)</label></formula><p>where λ i (i ∈ {1, 2}) controls the associated regularizer terms. The term -I α (Z c ; A * av |Z s ) is minimized to maximize the causal influence between Z c and A * av . This process involves removing the spurious influence of Z s and ensuring that the causal pathway between Z c and A * av remains strong and direct, unaffected by any extraneous confounding factors. Minimizing I α (Z c ; Z s ) is designed to ensure the independence between the causal features Z c and the spurious feature Z s . This term is crucial because it forces the model to eliminate any potential correlation or indirect dependency between the two feature sets. L VGAE is the negative evidence lower bound (ELBO) loss term, and minimize it can help approximate the true posterior distribution of latent variables, improves the approximation of the posterior distribution of the latent variables, which leads to better representations of graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimizing ∥Ac∥1</head><p>∥A∥1 is to minimize the number of nodes and edges necessary to represent the key information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT A. Driving Scenario Setup</head><p>To assess the performance of our CGRL approach, Our experiment employs the highway-env simulator <ref type="bibr" target="#b38">[39]</ref> to create an unsignalized intersection scenario with 15 human-driven vehicles, which is an open-sourced simulation platform designed for developing and testing decision-making algorithms in autonomous driving systems. The experimental setup involves a four-way intersection, where each road measures 4 meters in width and extends 30 meters in length in all directions. The main road usually has a higher priority, allowing vehicles to pass without slowing down, while vehicles on the secondary road need to observe the main road traffic before entering the intersection and then proceed when it is safe. In the simulation, we employ intelligent driver model (IDM) <ref type="bibr" target="#b39">[40]</ref> as a fundamental traffic flow model for vehicle dynamics. Vehicles are modeled to follow this car-following behavior, adjusting their speed based on the relative distance to the preceding vehicle. This allows for the modeling of realistic interactions between vehicles at the unsignalized intersection. In terms of IDM model, it is a car-following model, simulates how vehicles adjust their speed based on the car ahead.</p><p>It considers factors like desired velocity v 0 , the gap distance between the vehicle and the leading vehicle s, the maximum acceleration of the vehicle a max , and the speed of the vehicle v to replicate realistic driving behaviors.</p><formula xml:id="formula_39">a = a max 1 - v v 0 δ - s * (v, ∆v) s 2<label>(33)</label></formula><p>where s * (v, ∆v) is the safety distance function, which depends on the current speed vT and the speed difference ∆v.</p><formula xml:id="formula_40">s * (v, ∆v) = s 0 + vT + v∆v 2 √ a max b<label>(34)</label></formula><p>Where s 0 is the minimum jam distance to the vehicle ahead.</p><p>vT represents the distance corresponding to the desired time gap. ∆v represents the difference between the vehicle's speed and the speed of the vehicle ahead. b is the comfortable deceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details 1) Training details:</head><p>The proposed decision-making method is employed to train an optimal behavior policy for the autonomous vehicle. Specifically, a PyTorch-based decisionmaking framework is utilized to train the model over 1000 episodes. Each episode concludes once the autonomous vehicle successfully arrives at the target destination or collides with any other road user. Additionally, the reward values for each episode are recorded, making it convenient to conduct model testing. Simulation hyperparameters are summarized in TABLE <ref type="table" target="#tab_2">II</ref>. </p><p>2) Network Architecture: The network architecture of our proposed CGRL algorithm is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. First, the input node features pass through two GCN2 layers to extract local structural information, followed by two GATv2 layers to capture global attention relationships . The first twolayer GATv2 takes processed features as input and produces embeddings, which are further processed by the two-layer GATv2. ReLU activation and Layer Normalization is applied after the first GCN2 and the first GATv2 layer. The outputs from the second GATv2 layer are aggregated through a mean pooling operation across all nodes, resulting in a single vector representation of the entire graph. This aggregated graph embedding is then passed through two fully connected layers to generate latent representations, with ReLU activation applied once again to enhance non-linearity. The dueling network splits the latent representation into two streams: an advantage network and a value network. The  <ref type="bibr" target="#b11">[12]</ref>, GAT-D3QN <ref type="bibr" target="#b17">[18]</ref>, and GCN-GAT-D3QN <ref type="bibr" target="#b40">[41]</ref> are used as baseline methods to evaluate the effectiveness of integrating graph-based representations with various DQN variants. These provide a basis for comparison against the proposed method. GCN-DQN: The method combines the GCN with DQN to capture the relational structure of agents or entities in a graph-based environment, enabling more informed and coordinated decision-making. GCN-Double-DQN: The method combines the GCN with Double DQN to enable more effective learning in multi-agent interaction environments. GCN-Dueling-DQN: The method combines the GCN with Dueling DQN to improve decision-making in graph-structured environments by extracting relational features among agents and separating value and advantage estimations for more stable and efficient learning. GCN-D3QN: This approach integrates GCN with D3QN to enhance decision-making in autonomous vehicle interaction scenarios, utilizing GNN to extract scenario features and the D3QN framework to generate driving behaviors. GAT-D3QN: The method combines the GAT with D3QN to achieve improved autonomous vehicles decision-making in interactive scenarios. In this approach, it employs GAT to extract features from interactive scenarios, utilizing attention mechanisms to emphasize the significance of inter-vehicle relationships. These features are then fed into the D3QN framework, which generates optimal decisions for autonomous vehicles in dynamic and interactive environments. GCN-GAT-D3QN: The method combines the GCN, GAT, and D3QN to capture both global structural information and finegrained relational importance among agents, while improving the stability and efficiency of value-based reinforcement learning in graph-structured multi-agent environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>With the necessary setup completion, this section involves initiating the simulation and training loop to execute the specified number of episode. During this process, the CGRL algorithm's performance in the unsignalized intersection en- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Process</head><p>This subsection outlines the training procedures for all decision-making algorithms across three driving tasks: turning left, turning right, and going straight. In Figure <ref type="figure" target="#fig_2">3</ref>, it shows that the average rewards obtained by each algorithm progressively increase and eventually converge to a stable range across all tasks. During training, the average reward and episode length increase progressively until a stable convergence state is reached. Among the compared algorithms, the CGRL algorithm demonstrates superior learning efficiency over other baseline algorithms.</p><p>The integration of causal learning with GRL is essential for effectively capturing causal features and enhancing overall system performance. This efficiency is further enhanced by CGRL's ability to extract causal representations from the interaction states of AEVs, while disregarding non-causal features that may impair decision-making. Moreover, by isolating these causal features, CGRL facilitates the learning of invariant representations, thereby mitigating data inefficiency challenges commonly encountered in GRL frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation</head><p>Table III reveals that the left-turning and straight-driving tasks exhibit higher collision rates across all decision-making algorithms when compared to the right-turning scenario. This can be attributed to the increased complexity of these maneuvers, which require the AEV to carefully observe its surroundings, assess traffic dynamics, and select appropriate actions to navigate through more intricate interactions. In contrast, right-turning is a relatively simpler maneuver, as it typically involves fewer conflicts with other vehicles, thereby reducing the likelihood of collisions. The proposed CGRL algorithm outperforms all baseline methods across multiple performance metrics over 2000 testing episodes, demonstrating its effectiveness in complex intersection navigation scenarios. One key indicator of this performance is the average velocity, which reflects the trade-off between driving efficiency and safety. CGRL achieves wellbalanced average velocities of 7.81 m/s for left turns, 7.84 m/s for straight driving, and 8.46 m/s for right turns. While these values is not able to represent the highest speeds among all compared models, they indicate that CGRL maintains an optimal balance, allowing the AEV to make appropriate, safe decisions while ensuring smooth and timely intersection crossings. Another critical metric is the average reward, which captures the algorithm's overall effectiveness in achieving the driving task objectives, such as minimizing intersection traversal time and promoting smooth, efficient navigation. CGRL attains the highest average reward across all tested scenarios, left-turning, right-turning, and going straight. This superior performance demonstrates CGRL's ability to learn optimal control policies that balance speed and safety, highlighting its effectiveness in maintaining efficient traffic flow while ensuring that the AEV operates within safe behavioral margins. CGRL also achieves significantly lower collision rates compared to the baseline models, underscoring its strong emphasis on safety. Specifically, it achieves the lowest collision rates for all maneuver types: 13.50 percent for left turns, 11.35 percent for straight driving, and 7.00 percent for right turns. These results suggest that CGRL enables the AEV to make safer decisions, particularly in high-interaction, multi-agent environments. Therefore, the effectiveness of CGRL in achieving both safety and efficiency can be attributed to its unique integration of causal learning and GRL. By explicitly modeling interactions among surrounding vehicles using graph structures, CGRL enables a fine-grained understanding of inter-agent relationships and dynamics. The decision-making framework leverages this structural information to extract causal features, thereby facilitating more informed and reliable decisions. As a result, CGRL prioritizes collision avoidance while minimizing disruptions to traffic flow, ultimately achieving superior performance across a range of challenging driving tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this study, we propose a multi-agent decision-making framework CGRL, which combines causal learning with GRL, successfully identifies and leverages causal features that influence optimal decision-making in autonomous vehicles. This framework systematically identifies and exploits causal features that influence its decision-making processes, thereby facilitating more informed and optimized behaviors within multi-agent environments. Empirical evaluations reveal that the method attains superior average rewards throughout training and markedly surpasses baseline approaches across critical performance metrics, including collision rates and average cumulative rewards during testing. These findings underscore the potential of CGRL algorithm to enhance the safety and efficiency of autonomous vehicles in complex multi-agent environments. Future work will implement real-world validation trials to examine the framework's performance and extend the current methodology to incorporate complex environmental interactions, particularly focusing on heterogeneous traffic participants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: an unsignalized intersection scenario.</figDesc><graphic coords="3,48.96,367.49,251.06,171.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The framework of CGRL algorithm.</figDesc><graphic coords="4,69.53,56.06,472.95,406.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Training Rewards and Loss Curves Across Three Driving Tasks: Left Turn, Straight, and Right Turn. The reward and loss values for the left-turn task are shown in subfigures (a) and (d); for the straight-driving task in (b) and (e); and for the right-turn task in (c) and (f).</figDesc><graphic coords="9,55.75,197.25,164.49,109.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>or noise and does not have a direct causal relationship with A * av , but this influence is often spurious in nature. Since Z c and Z s naturally coexist in graph data G, these causal effects are established.• Z s → A * av . In this context, the spurious feature Z s acts as a confounder between Z c and A * av , because Z s is causally connected to both Z c and A * av , potentially creating a spurious association between Z c and A * av if Z s is not properly accounted for.</figDesc><table /><note><p>The causal features Z c serves as a key descriptor of the graph data G, accurately reflecting its intrinsic properties and exerting a direct causal influence on the optimal decision-making of the autonomous vehicle A * av . The spurious feature Z s , typically caused by data biases</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>IDM parameters used for training.</figDesc><table><row><cell>Keyword</cell><cell>Value</cell><cell>Unit</cell></row><row><cell>Maximum acceleration a max</cell><cell>6</cell><cell>m/s 2</cell></row><row><cell>Acceleration argument δ</cell><cell>4</cell><cell>/</cell></row><row><cell>Desired time gap T</cell><cell>1.5</cell><cell>s</cell></row><row><cell>Minimum jam distance s 0</cell><cell>5</cell><cell>m</cell></row><row><cell>Comfortable deceleration b</cell><cell>-5</cell><cell>m/s 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Hyperparameter used for training.</figDesc><table><row><cell>Parameters</cell><cell>Symbols</cell><cell>Value</cell></row><row><cell>Discounted factor</cell><cell>γ</cell><cell>0.95</cell></row><row><cell>Replay memory size</cell><cell>M replay</cell><cell>100000</cell></row><row><cell>Mini-batch size</cell><cell>M mini</cell><cell>64</cell></row><row><cell>Learning rate</cell><cell>η</cell><cell>0.0001</cell></row><row><cell>Epsilon</cell><cell>ϵ</cell><cell>0.1</cell></row><row><cell>Target update frequency</cell><cell>N update</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Policy Evaluation Results</figDesc><table><row><cell>Models</cell><cell></cell><cell>Turn Left</cell><cell></cell><cell></cell><cell cols="2">Go Straight</cell><cell></cell><cell>Turn Right</cell><cell></cell></row><row><cell></cell><cell>C.R. (%)</cell><cell>A.R.</cell><cell>A.V. (m/s)</cell><cell>C.R. (%)</cell><cell>A.R.</cell><cell>A.V. (m/s)</cell><cell>C.R. (%)</cell><cell>A.R.</cell><cell>A.V. (m/s)</cell></row><row><cell>GCN-DQN</cell><cell>27.40</cell><cell>4.79</cell><cell>7.71</cell><cell>15.45</cell><cell>5.48</cell><cell>7.69</cell><cell>12.40</cell><cell>5.86</cell><cell>8.60</cell></row><row><cell>GCN-Double-DQN</cell><cell>24.65</cell><cell>4.96</cell><cell>7.72</cell><cell>18.40</cell><cell>5.27</cell><cell>7.75</cell><cell>12.05</cell><cell>6.05</cell><cell>8.45</cell></row><row><cell>GCN-Dueling-DQN</cell><cell>23.25</cell><cell>5.05</cell><cell>7.70</cell><cell>16.60</cell><cell>5.39</cell><cell>7.71</cell><cell>9.80</cell><cell>6.74</cell><cell>8.37</cell></row><row><cell>GCN-D3QN</cell><cell>22.35</cell><cell>5.13</cell><cell>7.76</cell><cell>14.20</cell><cell>5.56</cell><cell>7.73</cell><cell>9.20</cell><cell>6.98</cell><cell>8.41</cell></row><row><cell>GAT-D3QN</cell><cell>19.40</cell><cell>5.25</cell><cell>7.74</cell><cell>17.00</cell><cell>5.35</cell><cell>7.78</cell><cell>11.35</cell><cell>6.21</cell><cell>8.28</cell></row><row><cell>GCN-GAT-D3QN</cell><cell>20.15</cell><cell>5.36</cell><cell>7.87</cell><cell>14.45</cell><cell>5.64</cell><cell>7.85</cell><cell>10.25</cell><cell>6.46</cell><cell>8.31</cell></row><row><cell>CGRL(ours)</cell><cell>13.50</cell><cell>5.96</cell><cell>7.81</cell><cell>11.35</cell><cell>6.12</cell><cell>7.84</cell><cell>7.00</cell><cell>7.42</cell><cell>8.46</cell></row><row><cell cols="5">vironment is assessed on three distinct driving tasks: left</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">turn, straight traversal, and right turn. Episodic rewards are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">recorded to assess performance during training. Additionally,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">comparative tests are conducted to statistically analyze the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">performance of all of decision-making algorithms. The testing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">metrics, including average reward, collision rate, and average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">velocity, are used for a comprehensive evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph neural networks and reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Adibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shoushtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Challenges and opportunities in deep reinforcement learning with graph neural networks: A comprehensive review of algorithms and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Munikoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halappanavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring causal learning through graph neural networks: An in-depth review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Job</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2311.14994</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">When graph neural network meets causality: Opportunities, methodologies and an outlook</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on causal reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<idno>abs/2302.05209</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal multi-agent reinforcement learning: Review and open problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Grimbly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pretorius</surname></persName>
		</author>
		<idno>abs/2111.06721</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1611.07308</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Challenges and opportunities in deep reinforcement learning with graph neural networks: A comprehensive review of algorithms and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Munikoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halappanavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph convolution-based deep reinforcement learning for multi-agent decision-making in mixed traffic environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<idno>abs/2201.12776</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized single-vehiclebased graph reinforcement learning for decision-making in autonomous driving</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2022">2022</date>
			<pubPlace>Basel, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-agent decision-making modes in uncertain interactive traffic scenarios via graph convolution-based deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2022">2022</date>
			<pubPlace>Basel, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Labi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Civil and Infrastructure Engineering</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="838" to="857" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cooperative behavioral planning for automated driving using graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klimke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Völz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buchholz</surname></persName>
		</author>
		<idno>abs/2202.11376</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient connected and automated driving system with multi-agent graph reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miranda-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dq-gat: Towards safe and efficient autonomous driving with deep q-learning and graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="21" to="102" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Drl-gat-sa: Deep reinforcement learning for autonomous driving planning based on graph attention networks and simplex architecture</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sysarc.2022.102505</idno>
		<ptr target="https://doi.org/10.1016/j.sysarc.2022.102505" />
	</analytic>
	<monogr>
		<title level="j">J. Syst. Archit</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">C</biblScope>
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal based q-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Méndez-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Feliciano-Avelino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient reinforcement learning with prior causal knowledge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEaR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Counterfactual policy evaluation for decisionmaking in autonomous driving</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causality-driven hierarchical structure discovery for reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2210.06964</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Constructing bayesian network models of gene expression networks from microarray data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aimale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Wimberly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-Channel Causal Variational Autoencoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balelli</surname></persName>
		</author>
		<ptr target="https://hal.science/hal-04666466" />
		<imprint>
			<date type="published" when="2024-08">Aug. 2024</date>
		</imprint>
	</monogr>
	<note>working paper or preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On causally disentangled representations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised disentangled generative causal representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causalvae: Disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9588" to="9597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concept-free causal disentanglement with variational graph auto-encoder</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2311.10638</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cadet: A causal disentanglement approach for robust trajectory prediction in autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pourkeshavarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="14" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno>abs/2105.14491</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information flows in causal networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Polani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Complex Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="17" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ci-gnn: A granger causality-inspired graph neural network for interpretable brain network-based psychiatric diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks : the official journal of the International Neural Network Society</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page">106147</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orphicx: A causality-inspired latent variable model for interpreting graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">728</biblScope>
			<biblScope unit="page" from="13" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimation of renyi entropy and mutual information based on generalized nearest-neighbor graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mine: Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1801.04062</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Measures of entropy from data using infinitely divisible kernels</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G S</forename><surname>Giraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Príncipe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="535" to="548" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An environment for autonomous driving decision-making</title>
		<author>
			<persName><forename type="first">E</forename><surname>Leurent</surname></persName>
		</author>
		<ptr target="https://github.com/eleurent/highway-env" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Congested traffic states in empirical observations and microscopic simulations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hennecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physical review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1805" to="1824" />
		</imprint>
	</monogr>
	<note>Pt A</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reasoning graph-based reinforcement learning to cooperate mixed connected and autonomous traffic at unsignalized intersections</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
